<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4597 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4597</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4597</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-99.html">extraction-schema-99</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <p><strong>Paper ID:</strong> paper-cb27c931ded5d41d991916b6cb7ce74cf2ee53f6</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/cb27c931ded5d41d991916b6cb7ce74cf2ee53f6" target="_blank">A sequence-to-sequence approach for document-level relation extraction</a></p>
                <p><strong>Paper Venue:</strong> Workshop on Biomedical Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> This paper develops a sequence-to-sequence approach, seq2rel, that can learn the subtasks of DocRE end- to-end, replacing a pipeline of task-specific components, and demonstrates that, under the model, an end-To-end approach outperforms a pipeline-based approach.</p>
                <p><strong>Paper Abstract:</strong> Motivated by the fact that many relations cross the sentence boundary, there has been increasing interest in document-level relation extraction (DocRE). DocRE requires integrating information within and across sentences, capturing complex interactions between mentions of entities. Most existing methods are pipeline-based, requiring entities as input. However, jointly learning to extract entities and relations can improve performance and be more efficient due to shared parameters and training steps. In this paper, we develop a sequence-to-sequence approach, seq2rel, that can learn the subtasks of DocRE (entity extraction, coreference resolution and relation extraction) end-to-end, replacing a pipeline of task-specific components. Using a simple strategy we call entity hinting, we compare our approach to existing pipeline-based methods on several popular biomedical datasets, in some cases exceeding their performance. We also report the first end-to-end results on these datasets for future comparison. Finally, we demonstrate that, under our model, an end-to-end approach outperforms a pipeline-based approach. Our code, data and trained models are available at https://github.com/johngiorgi/seq2rel. An online demo is available at https://share.streamlit.io/johngiorgi/seq2rel/main/demo.py.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4597.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4597.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>seq2rel</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>seq2rel: a sequence-to-sequence approach for document-level relation extraction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A seq2seq, encoder-decoder model that jointly performs entity extraction, coreference resolution and document-level relation extraction by linearizing relations into target strings and using a copy mechanism with a restricted target vocabulary to avoid hallucinations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>seq2rel</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Formulates document-level relation extraction as a text-generation task. Input: paragraph/document text (optionally prepended entity 'hints'). Encoder: pretrained transformer (PubMedBERT for biomedical corpora, BERT_BASE for DocRED) producing contextual token embeddings. Decoder: single-layer LSTM autoregressive decoder with multi-head cross-attention to encoder outputs. Important components: (1) a linearization schema that encodes entities (with semicolon-separated coreferent mentions and entity-type tokens) and relations (terminated with relation-type tokens) so n-ary and cross-sentence relations are representable; (2) a restricted target vocabulary containing only special tokens, forcing non-special tokens to be copied from the source via a copy mechanism; (3) constrained decoding rules (optional) to enforce syntactic validity of linearization; (4) relation sorting by first mention to mitigate permutation sensitivity of cross-entropy loss; (5) training by maximizing sequence log-likelihood with teacher forcing, fine-tuning the pretrained encoder and training decoder from scratch. The model can operate end-to-end (no entity input) or with entity hinting to mimic pipeline inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>PubMedBERT (biomedical experiments) and BERT_BASE (DocRED experiments) as pretrained encoders; decoder is an LSTM trained from scratch. Copy mechanism per Gu et al. 2016.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biomedical literature (chemical-disease, gene-disease, drug-gene-mutation) and general-domain (Wikipedia DocRED)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>CDR: 1,500 abstracts; GDA: 30,192 titles/abstracts (30k); DGM: 4,606 articles; DocRED: ~5,000 documents (used splits as described)</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Semantic relations between biomedical entities (e.g., chemical-induced disease (CID), gene-disease association (GDA), n-ary drug-gene-mutation (DGM) relations) — i.e., structured relational knowledge, not numeric physical laws.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Linearized textual tuples: semicolon-separated entity mentions, entity-type tokens (e.g. @GENE@), and relation-type tokens (e.g. @GDA@). Parsed to structured lists/dictionaries of entities and relation tuples.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Evaluation on held-out benchmark datasets (CDR, GDA, DGM, DocRED) using micro-precision/recall/F1; both strict and relaxed entity-matching criteria; comparisons to prior pipeline and end-to-end baselines. Ablation studies and analysis of gold/silver/entity-hint settings.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>CDR (entity hinting): P=68.2%, R=66.2%, F1=67.2%. GDA (entity hinting): P=84.4%, R=85.3%, F1=84.9%. DGM (entity hinting): P=84.0%, R=84.8%, F1=84.4%. Seq2rel end-to-end (CDR strict): F1=40.2% (relaxed: 52.4%). DocRED end-to-end: P=44.0%, R=33.8%, F1=38.2% (relaxed: F1=46.7%).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to multiple pipeline-based methods: on GDA seq2rel (entity hinting) outperformed prior best reported (Zhou et al. 2021: F1=83.9) with F1=84.9; on CDR it was competitive but below best (best prior F1=69.4; seq2rel 67.2). On DGM, seq2rel (entity hinting) substantially outperformed Jia et al. (2019) (68.9% -> 84.4% F1). Compared to end-to-end JEREX on DocRED: JEREX F1=40.4 vs seq2rel F1=38.2.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Permutation-sensitive sequence cross-entropy (requires relation ordering heuristic); decoder hallucination potential mitigated by restricted vocab + copy but remains a concern; decoder (LSTM) struggles with very long target sequences when documents contain many relations (recall drop); pretrained encoder input length limit (512 tokens) restricts full-document extraction to paragraph-length; underperforms in low-data regimes; decoder not pretrained (only encoder is pretrained).</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A sequence-to-sequence approach for document-level relation extraction', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4597.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4597.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CopyRE / CopyMTL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CopyRE and CopyMTL: seq2seq copy-mechanism models for joint entity and relation extraction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Seq2seq encoder-decoder approaches that incorporate a copy mechanism to enable generation (copying) of entity mentions from input text and jointly perform entity and relation extraction; CopyRE is restricted to binary intra-sentence relations, CopyMTL extends multi-token entity decoding and multi-task objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Extracting relational facts by an end-to-end neural model with copy mechanism</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>CopyRE / CopyMTL</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Encoder-decoder architectures (originally RNN-based seq2seq) with an explicit copy mechanism (Gu et al. 2016) enabling tokens from the source to be copied into the generated target. CopyRE frames relation extraction as generation where each relation is emitted via a small fixed decoding pattern (original CopyRE limited to three timesteps per relation, thus binary single-token entities). CopyMTL extends this to handle multi-token entities and uses multi-task losses to jointly learn entity spans and relations.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Original models used RNN-based seq2seq encoders/decoders with copy mechanism; later follow-ups may combine with pretrained encoders but specifics are not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General NLP relation extraction (sentence-level datasets); applied across domains in prior literature (not document-level).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Intra-sentence semantic relations between entities (binary relations) — structured relational facts rather than numeric laws.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Short linearized relation strings per relation (tokens copied from input + relation/entity special tokens); parsed into relation tuples.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Prior work evaluated on sentence-level RE benchmarks; not evaluated on document-level datasets in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not reported in this paper (prior publications report sentence-level metrics on their chosen benchmarks).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Mentioned as prior seq2seq baselines for joint extraction; these methods are limited to intra-sentence/binary settings and thus less expressive than seq2rel for document-level and n-ary relations.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Original CopyRE limited to binary single-token entities due to fixed decoding steps; prior methods do not model cross-sentence/coreference or n-ary relations; potential hallucination without copy restrictions.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A sequence-to-sequence approach for document-level relation extraction', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4597.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4597.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GenerativeRE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GenerativeRE: incorporating a novel copy mechanism and pretrained model for joint entity and relation extraction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A generative seq2seq approach that proposes a refined copy mechanism and leverages pretrained models to improve joint entity and relation extraction, with improved handling of multi-token entities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>GenerativeRE: Incorporating a novel copy mechanism and pretrained model for joint entity and relation extraction</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>GenerativeRE</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Seq2seq-based generative formulation for relation extraction that introduces a copy mechanism variant tailored to multi-token entity generation and pairs it with pretrained language models to boost performance on joint extraction tasks. Positions relation extraction as text generation so that entities are produced as text segments copied from the input.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Uses a pretrained model (paper indicates pairing with pretrained architectures) but precise model names/sizes are not specified in this paper's discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General NLP relation extraction; designed for intra-sentence and multi-token entity extraction tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Semantic entity-relation extraction (structured relational facts), not numeric laws.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Generated textual linearizations of entity mentions and relations; parsed into structured tuples.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Evaluated on sentence-level joint entity and relation extraction benchmarks in the original work; this paper only cites it as related work.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Cited as state-of-the-art among generative approaches for multi-token entities at time of publication; not quantitatively compared within this paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Prior work limited to intra-sentence relations; not designed for document-level cross-sentence/coreference-heavy extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A sequence-to-sequence approach for document-level relation extraction', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4597.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4597.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>JEREX</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>JEREX: an end-to-end model for entity-level relation extraction using multi-instance learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An end-to-end system that extends BERT with four task-specific components for mention localization, coreference resolution, entity classification and relation classification, trained jointly.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>An end-to-end model for entity-level relation extraction using multiinstance learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>JEREX</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Uses BERT as encoder and adds four specialized modules that operate on BERT outputs to perform (1) entity mention localization, (2) coreference resolution to aggregate mentions into entities, (3) entity classification, and (4) relation classification (global or multi-instance variants). Trained end-to-end with multi-instance learning to handle multiple mentions per entity; evaluated on DocRED for document-level RE.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>BERT_BASE as the pretrained encoder (same encoder used for fair comparison in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General-domain document-level relation extraction (DocRED Wikipedia-based dataset).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>DocRED: training set of ~3,008 documents (used split as in the paper); validation/test sizes per DocRED splits.</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Entity-level semantic relations (document-level relation extraction), not numeric/scientific laws.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Structured entity-level relation tuples produced via classifier outputs aggregated across mentions.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Evaluated on DocRED held-out test set with precision/recall/F1; compared to seq2rel in this paper (JEREX F1=40.4 vs seq2rel F1=38.2).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>On DocRED (reported in this paper): JEREX (Eberts & Ulges 2021) P=42.8%, R=38.2%, F1=40.4%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared directly to seq2rel on DocRED in this paper; JEREX slightly outperforms seq2rel in F1 on DocRED.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Relies on multiple task-specific components (complexity), may be sensitive to training and mention aggregation heuristics; recall can be impacted by long documents with many relations.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A sequence-to-sequence approach for document-level relation extraction', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4597.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4597.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>REBEL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>REBEL: Relation extraction by end-to-end language generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A concurrent work that extends seq2seq generation approaches to document-level relation extraction using end-to-end language generation, reportedly achieving strong performance on DocRED.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>REBEL: Relation extraction by end-to-end language generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>REBEL</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Seq2seq generation-based relation extraction framing; extends language-generation paradigms to relation extraction at document level. Described as concurrent to seq2rel and reported to achieve strong performance on DocRED; details not evaluated within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Not detailed in this paper (REBEL is cited as concurrent work).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General-domain document-level relation extraction (DocRED).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Document-level semantic relations (entity-entity relations) rather than numeric laws.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Generated textual linearizations parsed to relation tuples.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Reported strong performance on DocRED in the REBEL paper (not reproduced here).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Cited as concurrent strong seq2seq approach; not directly compared in experiments here.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Not discussed in this paper (only cited).</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A sequence-to-sequence approach for document-level relation extraction', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4597.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4597.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Text-to-text / T5 framework</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Text-to-text transfer Transformer (T5) / text-to-text framework</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A unified text-to-text modeling paradigm where tasks are cast so that both inputs and outputs are text strings, enabling the same encoder-decoder architecture and training procedure for many NLP tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Exploring the limits of transfer learning with a unified text-to-text transformer</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Text-to-text/T5 framework (applied to extraction tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>General framework (T5) that formulates diverse NLP tasks as text-in/text-out problems and pretrains encoder-decoder transformer models on large corpora with denoising objectives; downstream tasks (including relation extraction and other information extraction tasks) can be fine-tuned by designing input->output textual formats (linearizations). The paper positions seq2rel as an instantiation of this text-to-text approach for document-level RE.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>T5-family encoder-decoder transformers (various sizes depending on instantiation); specifics not used in seq2rel experiments but cited as conceptual framework. Related biomedical instantiations (Scifive) exist.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General NLP; has been applied to biomedical literature extraction tasks in other works.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>When applied to literature mining, typically used to extract structured relations/facts (semantic relations), not inherently numeric laws unless specifically formulated.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Textual linearizations (task-specific) that can encode relations, answers, or structured information.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Depends on downstream task; in general environments validated by held-out datasets and standard metrics for each task.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not reported in this paper for specific extraction tasks; T5 and text-to-text approaches have known high performance on many benchmarks per original T5 paper.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Cited as a general framework; seq2rel is framed as an application of text-to-text ideas to DocRE.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Design of linearization format and decoding constraints critical; sequence cross-entropy permutation sensitivity persists if output is unordered; computational costs of large encoder-decoder models.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A sequence-to-sequence approach for document-level relation extraction', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4597.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4597.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SciFive / Scifive</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SciFive: a text-to-text transformer model for biomedical literature</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A biomedical-domain instantiation of the text-to-text transformer, pretrained on PubMed/PubMedCentral data to improve performance on biomedical text-generation and extraction tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Scifive: a text-to-text transformer model for biomedical literature</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>SciFive (biomedical text-to-text transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Pretrained text-to-text transformer (T5-like) trained on biomedical corpora (PubMed, PMC) to provide a domain-adapted encoder-decoder backbone suitable for biomedical information extraction and generation tasks. Supports casting extraction as text generation using linearizations analogous to seq2rel's schema.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>T5-style encoder-decoder pretrained on biomedical corpora (model sizes vary by release).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biomedical literature (PubMed, PMC).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Designed to extract structured biomedical knowledge (entity relations, QA, NER), not specifically numeric/scientific laws unless posed as such.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Textual outputs per text-to-text formulation; can be parsed into structured relations.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Evaluated in its own work on biomedical downstream tasks; cited here as related text-to-text application to biomedical extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Cited as relevant domain-adapted text-to-text pretrained model that could be applied to DocRE tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Pretraining large encoder-decoder models is expensive; inference/decoding costs can be high for long documents.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A sequence-to-sequence approach for document-level relation extraction', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4597.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e4597.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PubTator</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PubTator: a web-based text mining tool for assisting biocuration</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A practical NER/pipeline service that provides automated entity annotations (genes, diseases, chemicals, etc.) for PubMed records; used in this paper to create 'silver' entity hints for pipeline-style evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Pubtator: a web-based text mining tool for assisting biocuration</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>PubTator (used as NER provider / silver hints)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>External, production text-mining pipeline that annotates PubMed documents with entity mentions using state-of-the-art models; in this paper it supplies 'silver' entity hints which are prepended to input documents to simulate pipeline extraction settings and to evaluate seq2rel performance when upstream NER is imperfect.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>PubTator aggregates specialized ML/NLP components for entity recognition (not specified as a single LLM in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biomedical literature (PubMed abstracts/articles).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Provides entity annotations (semantic entities) rather than laws; used to support relation extraction evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Entity mention annotations (spans and types) attached to PubMed records; used as prepended entity hints.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Not validated within this paper beyond empirical experiments comparing gold/silver/entity-hint settings; generally PubTator is curated/benchmarked elsewhere.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not reported here; using PubTator (silver hints) resulted in lower seq2rel F1 relative to gold hints (example: CDR with silver hints F1=39.7 strict vs gold hints F1=67.2).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Used to simulate realistic upstream NER; comparisons in paper show pipelines using silver hints degrade performance, while end-to-end seq2rel recovers better.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>NER errors (false positives/negatives) propagate to relation extraction in pipeline settings; silver hints degrade performance relative to gold annotated entities.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A sequence-to-sequence approach for document-level relation extraction', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Extracting relational facts by an end-to-end neural model with copy mechanism <em>(Rating: 2)</em></li>
                <li>Copymtl: Copy mechanism for joint extraction of entities and relations with multi-task learning <em>(Rating: 2)</em></li>
                <li>GenerativeRE: Incorporating a novel copy mechanism and pretrained model for joint entity and relation extraction <em>(Rating: 2)</em></li>
                <li>An end-to-end model for entity-level relation extraction using multiinstance learning <em>(Rating: 2)</em></li>
                <li>REBEL: Relation extraction by end-to-end language generation <em>(Rating: 2)</em></li>
                <li>Exploring the limits of transfer learning with a unified text-to-text transformer <em>(Rating: 2)</em></li>
                <li>Scifive: a text-to-text transformer model for biomedical literature <em>(Rating: 2)</em></li>
                <li>Document-level n-ary relation extraction with multiscale representation learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4597",
    "paper_id": "paper-cb27c931ded5d41d991916b6cb7ce74cf2ee53f6",
    "extraction_schema_id": "extraction-schema-99",
    "extracted_data": [
        {
            "name_short": "seq2rel",
            "name_full": "seq2rel: a sequence-to-sequence approach for document-level relation extraction",
            "brief_description": "A seq2seq, encoder-decoder model that jointly performs entity extraction, coreference resolution and document-level relation extraction by linearizing relations into target strings and using a copy mechanism with a restricted target vocabulary to avoid hallucinations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "seq2rel",
            "method_description": "Formulates document-level relation extraction as a text-generation task. Input: paragraph/document text (optionally prepended entity 'hints'). Encoder: pretrained transformer (PubMedBERT for biomedical corpora, BERT_BASE for DocRED) producing contextual token embeddings. Decoder: single-layer LSTM autoregressive decoder with multi-head cross-attention to encoder outputs. Important components: (1) a linearization schema that encodes entities (with semicolon-separated coreferent mentions and entity-type tokens) and relations (terminated with relation-type tokens) so n-ary and cross-sentence relations are representable; (2) a restricted target vocabulary containing only special tokens, forcing non-special tokens to be copied from the source via a copy mechanism; (3) constrained decoding rules (optional) to enforce syntactic validity of linearization; (4) relation sorting by first mention to mitigate permutation sensitivity of cross-entropy loss; (5) training by maximizing sequence log-likelihood with teacher forcing, fine-tuning the pretrained encoder and training decoder from scratch. The model can operate end-to-end (no entity input) or with entity hinting to mimic pipeline inputs.",
            "llm_model_used": "PubMedBERT (biomedical experiments) and BERT_BASE (DocRED experiments) as pretrained encoders; decoder is an LSTM trained from scratch. Copy mechanism per Gu et al. 2016.",
            "scientific_domain": "Biomedical literature (chemical-disease, gene-disease, drug-gene-mutation) and general-domain (Wikipedia DocRED)",
            "number_of_papers": "CDR: 1,500 abstracts; GDA: 30,192 titles/abstracts (30k); DGM: 4,606 articles; DocRED: ~5,000 documents (used splits as described)",
            "type_of_quantitative_law": "Semantic relations between biomedical entities (e.g., chemical-induced disease (CID), gene-disease association (GDA), n-ary drug-gene-mutation (DGM) relations) — i.e., structured relational knowledge, not numeric physical laws.",
            "extraction_output_format": "Linearized textual tuples: semicolon-separated entity mentions, entity-type tokens (e.g. @GENE@), and relation-type tokens (e.g. @GDA@). Parsed to structured lists/dictionaries of entities and relation tuples.",
            "validation_method": "Evaluation on held-out benchmark datasets (CDR, GDA, DGM, DocRED) using micro-precision/recall/F1; both strict and relaxed entity-matching criteria; comparisons to prior pipeline and end-to-end baselines. Ablation studies and analysis of gold/silver/entity-hint settings.",
            "performance_metrics": "CDR (entity hinting): P=68.2%, R=66.2%, F1=67.2%. GDA (entity hinting): P=84.4%, R=85.3%, F1=84.9%. DGM (entity hinting): P=84.0%, R=84.8%, F1=84.4%. Seq2rel end-to-end (CDR strict): F1=40.2% (relaxed: 52.4%). DocRED end-to-end: P=44.0%, R=33.8%, F1=38.2% (relaxed: F1=46.7%).",
            "baseline_comparison": "Compared to multiple pipeline-based methods: on GDA seq2rel (entity hinting) outperformed prior best reported (Zhou et al. 2021: F1=83.9) with F1=84.9; on CDR it was competitive but below best (best prior F1=69.4; seq2rel 67.2). On DGM, seq2rel (entity hinting) substantially outperformed Jia et al. (2019) (68.9% -&gt; 84.4% F1). Compared to end-to-end JEREX on DocRED: JEREX F1=40.4 vs seq2rel F1=38.2.",
            "challenges_limitations": "Permutation-sensitive sequence cross-entropy (requires relation ordering heuristic); decoder hallucination potential mitigated by restricted vocab + copy but remains a concern; decoder (LSTM) struggles with very long target sequences when documents contain many relations (recall drop); pretrained encoder input length limit (512 tokens) restricts full-document extraction to paragraph-length; underperforms in low-data regimes; decoder not pretrained (only encoder is pretrained).",
            "requires_human_in_loop": false,
            "fully_automated": true,
            "uuid": "e4597.0",
            "source_info": {
                "paper_title": "A sequence-to-sequence approach for document-level relation extraction",
                "publication_date_yy_mm": "2022-04"
            }
        },
        {
            "name_short": "CopyRE / CopyMTL",
            "name_full": "CopyRE and CopyMTL: seq2seq copy-mechanism models for joint entity and relation extraction",
            "brief_description": "Seq2seq encoder-decoder approaches that incorporate a copy mechanism to enable generation (copying) of entity mentions from input text and jointly perform entity and relation extraction; CopyRE is restricted to binary intra-sentence relations, CopyMTL extends multi-token entity decoding and multi-task objectives.",
            "citation_title": "Extracting relational facts by an end-to-end neural model with copy mechanism",
            "mention_or_use": "mention",
            "method_name": "CopyRE / CopyMTL",
            "method_description": "Encoder-decoder architectures (originally RNN-based seq2seq) with an explicit copy mechanism (Gu et al. 2016) enabling tokens from the source to be copied into the generated target. CopyRE frames relation extraction as generation where each relation is emitted via a small fixed decoding pattern (original CopyRE limited to three timesteps per relation, thus binary single-token entities). CopyMTL extends this to handle multi-token entities and uses multi-task losses to jointly learn entity spans and relations.",
            "llm_model_used": "Original models used RNN-based seq2seq encoders/decoders with copy mechanism; later follow-ups may combine with pretrained encoders but specifics are not detailed in this paper.",
            "scientific_domain": "General NLP relation extraction (sentence-level datasets); applied across domains in prior literature (not document-level).",
            "number_of_papers": null,
            "type_of_quantitative_law": "Intra-sentence semantic relations between entities (binary relations) — structured relational facts rather than numeric laws.",
            "extraction_output_format": "Short linearized relation strings per relation (tokens copied from input + relation/entity special tokens); parsed into relation tuples.",
            "validation_method": "Prior work evaluated on sentence-level RE benchmarks; not evaluated on document-level datasets in this paper.",
            "performance_metrics": "Not reported in this paper (prior publications report sentence-level metrics on their chosen benchmarks).",
            "baseline_comparison": "Mentioned as prior seq2seq baselines for joint extraction; these methods are limited to intra-sentence/binary settings and thus less expressive than seq2rel for document-level and n-ary relations.",
            "challenges_limitations": "Original CopyRE limited to binary single-token entities due to fixed decoding steps; prior methods do not model cross-sentence/coreference or n-ary relations; potential hallucination without copy restrictions.",
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4597.1",
            "source_info": {
                "paper_title": "A sequence-to-sequence approach for document-level relation extraction",
                "publication_date_yy_mm": "2022-04"
            }
        },
        {
            "name_short": "GenerativeRE",
            "name_full": "GenerativeRE: incorporating a novel copy mechanism and pretrained model for joint entity and relation extraction",
            "brief_description": "A generative seq2seq approach that proposes a refined copy mechanism and leverages pretrained models to improve joint entity and relation extraction, with improved handling of multi-token entities.",
            "citation_title": "GenerativeRE: Incorporating a novel copy mechanism and pretrained model for joint entity and relation extraction",
            "mention_or_use": "mention",
            "method_name": "GenerativeRE",
            "method_description": "Seq2seq-based generative formulation for relation extraction that introduces a copy mechanism variant tailored to multi-token entity generation and pairs it with pretrained language models to boost performance on joint extraction tasks. Positions relation extraction as text generation so that entities are produced as text segments copied from the input.",
            "llm_model_used": "Uses a pretrained model (paper indicates pairing with pretrained architectures) but precise model names/sizes are not specified in this paper's discussion.",
            "scientific_domain": "General NLP relation extraction; designed for intra-sentence and multi-token entity extraction tasks.",
            "number_of_papers": null,
            "type_of_quantitative_law": "Semantic entity-relation extraction (structured relational facts), not numeric laws.",
            "extraction_output_format": "Generated textual linearizations of entity mentions and relations; parsed into structured tuples.",
            "validation_method": "Evaluated on sentence-level joint entity and relation extraction benchmarks in the original work; this paper only cites it as related work.",
            "performance_metrics": "Not reported in this paper.",
            "baseline_comparison": "Cited as state-of-the-art among generative approaches for multi-token entities at time of publication; not quantitatively compared within this paper's experiments.",
            "challenges_limitations": "Prior work limited to intra-sentence relations; not designed for document-level cross-sentence/coreference-heavy extraction.",
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4597.2",
            "source_info": {
                "paper_title": "A sequence-to-sequence approach for document-level relation extraction",
                "publication_date_yy_mm": "2022-04"
            }
        },
        {
            "name_short": "JEREX",
            "name_full": "JEREX: an end-to-end model for entity-level relation extraction using multi-instance learning",
            "brief_description": "An end-to-end system that extends BERT with four task-specific components for mention localization, coreference resolution, entity classification and relation classification, trained jointly.",
            "citation_title": "An end-to-end model for entity-level relation extraction using multiinstance learning",
            "mention_or_use": "mention",
            "method_name": "JEREX",
            "method_description": "Uses BERT as encoder and adds four specialized modules that operate on BERT outputs to perform (1) entity mention localization, (2) coreference resolution to aggregate mentions into entities, (3) entity classification, and (4) relation classification (global or multi-instance variants). Trained end-to-end with multi-instance learning to handle multiple mentions per entity; evaluated on DocRED for document-level RE.",
            "llm_model_used": "BERT_BASE as the pretrained encoder (same encoder used for fair comparison in this paper).",
            "scientific_domain": "General-domain document-level relation extraction (DocRED Wikipedia-based dataset).",
            "number_of_papers": "DocRED: training set of ~3,008 documents (used split as in the paper); validation/test sizes per DocRED splits.",
            "type_of_quantitative_law": "Entity-level semantic relations (document-level relation extraction), not numeric/scientific laws.",
            "extraction_output_format": "Structured entity-level relation tuples produced via classifier outputs aggregated across mentions.",
            "validation_method": "Evaluated on DocRED held-out test set with precision/recall/F1; compared to seq2rel in this paper (JEREX F1=40.4 vs seq2rel F1=38.2).",
            "performance_metrics": "On DocRED (reported in this paper): JEREX (Eberts & Ulges 2021) P=42.8%, R=38.2%, F1=40.4%.",
            "baseline_comparison": "Compared directly to seq2rel on DocRED in this paper; JEREX slightly outperforms seq2rel in F1 on DocRED.",
            "challenges_limitations": "Relies on multiple task-specific components (complexity), may be sensitive to training and mention aggregation heuristics; recall can be impacted by long documents with many relations.",
            "requires_human_in_loop": false,
            "fully_automated": true,
            "uuid": "e4597.3",
            "source_info": {
                "paper_title": "A sequence-to-sequence approach for document-level relation extraction",
                "publication_date_yy_mm": "2022-04"
            }
        },
        {
            "name_short": "REBEL",
            "name_full": "REBEL: Relation extraction by end-to-end language generation",
            "brief_description": "A concurrent work that extends seq2seq generation approaches to document-level relation extraction using end-to-end language generation, reportedly achieving strong performance on DocRED.",
            "citation_title": "REBEL: Relation extraction by end-to-end language generation",
            "mention_or_use": "mention",
            "method_name": "REBEL",
            "method_description": "Seq2seq generation-based relation extraction framing; extends language-generation paradigms to relation extraction at document level. Described as concurrent to seq2rel and reported to achieve strong performance on DocRED; details not evaluated within this paper.",
            "llm_model_used": "Not detailed in this paper (REBEL is cited as concurrent work).",
            "scientific_domain": "General-domain document-level relation extraction (DocRED).",
            "number_of_papers": null,
            "type_of_quantitative_law": "Document-level semantic relations (entity-entity relations) rather than numeric laws.",
            "extraction_output_format": "Generated textual linearizations parsed to relation tuples.",
            "validation_method": "Reported strong performance on DocRED in the REBEL paper (not reproduced here).",
            "performance_metrics": "Not reported in this paper.",
            "baseline_comparison": "Cited as concurrent strong seq2seq approach; not directly compared in experiments here.",
            "challenges_limitations": "Not discussed in this paper (only cited).",
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4597.4",
            "source_info": {
                "paper_title": "A sequence-to-sequence approach for document-level relation extraction",
                "publication_date_yy_mm": "2022-04"
            }
        },
        {
            "name_short": "Text-to-text / T5 framework",
            "name_full": "Text-to-text transfer Transformer (T5) / text-to-text framework",
            "brief_description": "A unified text-to-text modeling paradigm where tasks are cast so that both inputs and outputs are text strings, enabling the same encoder-decoder architecture and training procedure for many NLP tasks.",
            "citation_title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "mention_or_use": "mention",
            "method_name": "Text-to-text/T5 framework (applied to extraction tasks)",
            "method_description": "General framework (T5) that formulates diverse NLP tasks as text-in/text-out problems and pretrains encoder-decoder transformer models on large corpora with denoising objectives; downstream tasks (including relation extraction and other information extraction tasks) can be fine-tuned by designing input-&gt;output textual formats (linearizations). The paper positions seq2rel as an instantiation of this text-to-text approach for document-level RE.",
            "llm_model_used": "T5-family encoder-decoder transformers (various sizes depending on instantiation); specifics not used in seq2rel experiments but cited as conceptual framework. Related biomedical instantiations (Scifive) exist.",
            "scientific_domain": "General NLP; has been applied to biomedical literature extraction tasks in other works.",
            "number_of_papers": null,
            "type_of_quantitative_law": "When applied to literature mining, typically used to extract structured relations/facts (semantic relations), not inherently numeric laws unless specifically formulated.",
            "extraction_output_format": "Textual linearizations (task-specific) that can encode relations, answers, or structured information.",
            "validation_method": "Depends on downstream task; in general environments validated by held-out datasets and standard metrics for each task.",
            "performance_metrics": "Not reported in this paper for specific extraction tasks; T5 and text-to-text approaches have known high performance on many benchmarks per original T5 paper.",
            "baseline_comparison": "Cited as a general framework; seq2rel is framed as an application of text-to-text ideas to DocRE.",
            "challenges_limitations": "Design of linearization format and decoding constraints critical; sequence cross-entropy permutation sensitivity persists if output is unordered; computational costs of large encoder-decoder models.",
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4597.5",
            "source_info": {
                "paper_title": "A sequence-to-sequence approach for document-level relation extraction",
                "publication_date_yy_mm": "2022-04"
            }
        },
        {
            "name_short": "SciFive / Scifive",
            "name_full": "SciFive: a text-to-text transformer model for biomedical literature",
            "brief_description": "A biomedical-domain instantiation of the text-to-text transformer, pretrained on PubMed/PubMedCentral data to improve performance on biomedical text-generation and extraction tasks.",
            "citation_title": "Scifive: a text-to-text transformer model for biomedical literature",
            "mention_or_use": "mention",
            "method_name": "SciFive (biomedical text-to-text transformer)",
            "method_description": "Pretrained text-to-text transformer (T5-like) trained on biomedical corpora (PubMed, PMC) to provide a domain-adapted encoder-decoder backbone suitable for biomedical information extraction and generation tasks. Supports casting extraction as text generation using linearizations analogous to seq2rel's schema.",
            "llm_model_used": "T5-style encoder-decoder pretrained on biomedical corpora (model sizes vary by release).",
            "scientific_domain": "Biomedical literature (PubMed, PMC).",
            "number_of_papers": null,
            "type_of_quantitative_law": "Designed to extract structured biomedical knowledge (entity relations, QA, NER), not specifically numeric/scientific laws unless posed as such.",
            "extraction_output_format": "Textual outputs per text-to-text formulation; can be parsed into structured relations.",
            "validation_method": "Evaluated in its own work on biomedical downstream tasks; cited here as related text-to-text application to biomedical extraction.",
            "performance_metrics": "Not reported in this paper.",
            "baseline_comparison": "Cited as relevant domain-adapted text-to-text pretrained model that could be applied to DocRE tasks.",
            "challenges_limitations": "Pretraining large encoder-decoder models is expensive; inference/decoding costs can be high for long documents.",
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4597.6",
            "source_info": {
                "paper_title": "A sequence-to-sequence approach for document-level relation extraction",
                "publication_date_yy_mm": "2022-04"
            }
        },
        {
            "name_short": "PubTator",
            "name_full": "PubTator: a web-based text mining tool for assisting biocuration",
            "brief_description": "A practical NER/pipeline service that provides automated entity annotations (genes, diseases, chemicals, etc.) for PubMed records; used in this paper to create 'silver' entity hints for pipeline-style evaluations.",
            "citation_title": "Pubtator: a web-based text mining tool for assisting biocuration",
            "mention_or_use": "use",
            "method_name": "PubTator (used as NER provider / silver hints)",
            "method_description": "External, production text-mining pipeline that annotates PubMed documents with entity mentions using state-of-the-art models; in this paper it supplies 'silver' entity hints which are prepended to input documents to simulate pipeline extraction settings and to evaluate seq2rel performance when upstream NER is imperfect.",
            "llm_model_used": "PubTator aggregates specialized ML/NLP components for entity recognition (not specified as a single LLM in this paper).",
            "scientific_domain": "Biomedical literature (PubMed abstracts/articles).",
            "number_of_papers": null,
            "type_of_quantitative_law": "Provides entity annotations (semantic entities) rather than laws; used to support relation extraction evaluation.",
            "extraction_output_format": "Entity mention annotations (spans and types) attached to PubMed records; used as prepended entity hints.",
            "validation_method": "Not validated within this paper beyond empirical experiments comparing gold/silver/entity-hint settings; generally PubTator is curated/benchmarked elsewhere.",
            "performance_metrics": "Not reported here; using PubTator (silver hints) resulted in lower seq2rel F1 relative to gold hints (example: CDR with silver hints F1=39.7 strict vs gold hints F1=67.2).",
            "baseline_comparison": "Used to simulate realistic upstream NER; comparisons in paper show pipelines using silver hints degrade performance, while end-to-end seq2rel recovers better.",
            "challenges_limitations": "NER errors (false positives/negatives) propagate to relation extraction in pipeline settings; silver hints degrade performance relative to gold annotated entities.",
            "requires_human_in_loop": false,
            "fully_automated": true,
            "uuid": "e4597.7",
            "source_info": {
                "paper_title": "A sequence-to-sequence approach for document-level relation extraction",
                "publication_date_yy_mm": "2022-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Extracting relational facts by an end-to-end neural model with copy mechanism",
            "rating": 2
        },
        {
            "paper_title": "Copymtl: Copy mechanism for joint extraction of entities and relations with multi-task learning",
            "rating": 2
        },
        {
            "paper_title": "GenerativeRE: Incorporating a novel copy mechanism and pretrained model for joint entity and relation extraction",
            "rating": 2
        },
        {
            "paper_title": "An end-to-end model for entity-level relation extraction using multiinstance learning",
            "rating": 2
        },
        {
            "paper_title": "REBEL: Relation extraction by end-to-end language generation",
            "rating": 2
        },
        {
            "paper_title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "rating": 2
        },
        {
            "paper_title": "Scifive: a text-to-text transformer model for biomedical literature",
            "rating": 2
        },
        {
            "paper_title": "Document-level n-ary relation extraction with multiscale representation learning",
            "rating": 1
        }
    ],
    "cost": 0.02035725,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>A sequence-to-sequence approach for document-level relation extraction</h1>
<p>John Giorgi ${ }^{1,4,5, \text {, }}$ Gary D. Bader ${ }^{1,2,4,6,7, \dagger}$ Bo Wang ${ }^{1,3,5,8, \dagger}$<br>${ }^{1}$ Department of Computer Science, University of Toronto<br>${ }^{2}$ Department of Molecular Genetics, University of Toronto<br>${ }^{3}$ Department of Laboratory Medicine and Pathobiology, University of Toronto<br>${ }^{4}$ Terrence Donnelly Centre for Cellular \&amp; Biomolecular Research<br>${ }^{5}$ Vector Institute for Artificial Intelligence<br>${ }^{6}$ The Lunenfeld-Tanenbaum Research Institute, Sinai Health System<br>${ }^{7}$ Princess Margaret Cancer Centre, University Health Network<br>${ }^{8}$ Peter Munk Cardiac Center, University Health Network<br>${ }^{9}$ Corresponding author ${ }^{\dagger}$ Equal contribution<br>{john.giorgi, gary.bader}@mail.utoronto.ca<br>bowang@vectorinstitute.ai</p>
<h4>Abstract</h4>
<p>Motivated by the fact that many relations cross the sentence boundary, there has been increasing interest in document-level relation extraction (DocRE). DocRE requires integrating information within and across sentences, capturing complex interactions between mentions of entities. Most existing methods are pipelinebased, requiring entities as input. However, jointly learning to extract entities and relations can improve performance and be more efficient due to shared parameters and training steps. In this paper, we develop a sequence-tosequence approach, seq2rel, that can learn the subtasks of DocRE (entity extraction, coreference resolution and relation extraction) end-toend, replacing a pipeline of task-specific components. Using a simple strategy we call entity hinting, we compare our approach to existing pipeline-based methods on several popular biomedical datasets, in some cases exceeding their performance. We also report the first end-to-end results on these datasets for future comparison. Finally, we demonstrate that, under our model, an end-to-end approach outperforms a pipeline-based approach. Our code, data and trained models are available at https: //github.com/johngiorgi/seq2rel. An online demo is available at https://share.streamlit. io/johngiorgi/seq2rel/main/demo.py.</p>
<h2>1 Introduction</h2>
<p>PubMed, the largest repository of biomedical literature, contains over 30 million publications and is adding more than two papers per minute. Accurate, automated text mining and natural language processing (NLP) methods are needed to maximize discovery and extract structured information from
this massive volume of text. An important step in this process is relation extraction (RE), the task of identifying groups of entities within some text that participate in a semantic relationship. In the domain of biomedicine, relations of interest include chemical-induced disease, protein-protein interactions, and gene-disease associations.</p>
<p>Many methods have been proposed for RE, ranging from rule-based to machine learning-based (Zhou et al., 2014; Liu et al., 2016). Most of this work has focused on intra-sentence binary RE, where pairs of entities within a sentence are classified as belonging to a particular relation (or none). These methods often ignore commonly occurring complexities like nested or discontinuous entities, coreferent mentions (words or phrases in the text that refer to the same entity), inter-sentence and $n$-ary relations (see Figure 1 for examples). The decision not to model these phenomena is a strong assumption. In GENIA (Kim et al., 2003), a corpus of PubMed articles labelled with around 100,000 biomedical entities, $\sim 17 \%$ of all entities are nested within another entity. Discontinuous entities are particularly common in clinical text, where $\sim 10 \%$ of mentions in popular benchmark corpora are discontinuous (Wang et al., 2021). In the CDR corpus (Li et al., 2016b), which comprises 1500 PubMed articles annotated for chemical-induced disease relations, $\sim 30 \%$ of all relations are inter-sentence. Some relations, like drug-gene-mutation interactions, are difficult to model with binary RE (Zhou et al., 2014).</p>
<p>In response to some of these shortcomings, there has been a growing interest in document-level RE (DocRE). DocRE aims to model inter-sentence re-</p>
<p>Figure 1: Examples of complexities in entity and relation extraction and the proposed linearization schema to model them. CID: chemical-induced disease. GDA: gene-disease association. DGM: drug-gene-mutation.</p>
<table>
<thead>
<tr>
<th>Complexities</th>
<th>Example</th>
<th>Comment</th>
</tr>
</thead>
<tbody>
<tr>
<td>Discontinuous mentions</td>
<td>Induction by paracetamol of bladder and liver tumours.</td>
<td>Discontinuous mention of bladder tumours.</td>
</tr>
<tr>
<td></td>
<td>paracetamol @DRUG@ bladder tumours @DISEASE@ @CID@ paracetamol @DRUG@ liver tumours @DISEASE@ @CID@</td>
<td></td>
</tr>
<tr>
<td>Coreferent mentions</td>
<td>Proto-oncogene HER2 (also known as erbB-2 or neu) plays an important role in the carcinogenesis and the prognosis of breast cancer.</td>
<td>Two coreferent mentions of HER2.</td>
</tr>
<tr>
<td></td>
<td>her2 ; erbb-2 ; neu @GENE@ breast cancer @DISEASE@ @GDA@</td>
<td></td>
</tr>
<tr>
<td>$n$-ary, intersentence</td>
<td>The deletion mutation on exon-19 of EGFR gene was present in 16 patients, while the L858E point mutation on exon-21 was noted in 10. All patients were treated with gefitinib and showed a partial response.</td>
<td>Ternary DGM relationship crosses a sentence boundary.</td>
</tr>
<tr>
<td></td>
<td>gefitinib @DRUG@ egfr @GENE@ 1858e @MUTATION@ @DGM@</td>
<td></td>
</tr>
</tbody>
</table>
<p>lations between coreferent mentions of entities in a document. A popular approach involves graphbased methods, which have the advantage of naturally modelling inter-sentence relations (Peng et al., 2017; Song et al., 2018; Christopoulou et al., 2019; Nan et al., 2020; Minh Tran et al., 2020). However, like all pipeline-based approaches, these methods assume that the entities within the text are known. As previous work has demonstrated, and as we show in $\S 5.2$, jointly learning to extract entities and relations can improve performance (Miwa and Sasaki, 2014; Miwa and Bansal, 2016; Gupta et al., 2016; Li et al., 2016a, 2017; Nguyen and Verspoor, 2019a; Yu et al., 2020) and may be more efficient due to shared parameters and training steps. Existing end-to-end methods typically combine taskspecific components for entity detection, coreference resolution, and relation extraction that are trained jointly. Most approaches are restricted to intra-sentence RE (Bekoulis et al., 2018; Luan et al., 2018; Nguyen and Verspoor, 2019b; Wadden et al., 2019; Giorgi et al., 2019) and have only recently been extended to DocRE (Eberts and Ulges, 2021). However, they still focus on binary relations. Ideally, DocRE methods would be capable of modelling the complexities mentioned above without strictly requiring entities to be known.</p>
<p>A less popular end-to-end approach is to frame RE as a generative task with sequence-to-sequence (seq2seq) learning (Sutskever et al., 2014). This framing simplifies RE by removing the need for task-specific components and explicit negative training examples, i.e. pairs of entities that do not express a relation. If the information to extract is appropriately linearized to a string, seq2seq methods are flexible enough to model all complexities
discussed thus far. However, existing work stops short, focusing on intra-sentence binary relations (Zeng et al., 2018; Zhang et al., 2020; Nayak and Ng, 2020; Zeng et al., 2020). In this paper, we extend work on seq2seq methods for RE to the document level, with several important contributions:</p>
<ul>
<li>We propose a novel linearization schema that can handle complexities overlooked by previous seq2seq approaches, like coreferent mentions and $n$-ary relations (§3.1).</li>
<li>Using this linearization schema, we demonstrate that a seq2seq approach is able to learn the subtasks of DocRE (entity extraction, coreference resolution and relation extraction) jointly, and report the first end-to-end results on several popular biomedical datasets (§5.1).</li>
<li>We devise a simple strategy, referred to as "entity hinting" (§3.3), to compare our model to existing pipeline-based approaches, in some cases exceeding their performance (§5.1).</li>
</ul>
<h2>2 Task definition: document-level relation extraction</h2>
<p>Given a source document of $S$ tokens, a model must extract all tuples corresponding to a relation, $R$, expressed between the entities, $E$ in the document, $\left(E_{1}, \ldots, E_{n}, R\right)$ where $n$ is the number of participating entities, or arity, of the relation. Each entity $E_{i}$ is represented as the set of its coreferent mentions $\left{e_{j}^{i}\right}$ in the document, which are often expressed as aliases, abbreviations or acronyms. All entities appearing in a tuple have at least one mention in the document. The mentions that express a given relation are not necessarily contained within</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 2: A sequence-to-sequence model for document-level relation extraction. Special tokens are generated by the decoder. Entity mentions are copied from the input via a copy mechanism (not shown). Decoding is initiated by a @START@ token and terminated when the model generates the @END@ token. Attention connections shown only for the second timestep to reduce clutter. CID: chemical-induced disease.
the same sentence. Commonly, $E$ is assumed to be known and provided as input to a model. We will refer to these methods as "pipeline-based". In this paper, we are primarily concerned with the situation where $E$ is not given and must be predicted by a model, which we will refer to as "end-to-end".</p>
<h2>3 Our approach: seq2rel</h2>
<h3>3.1 Linearization</h3>
<p>To use seq2seq learning for RE, the information to be extracted must be linearized to a string. This linearization should be expressive enough to model the complexities of entity and relation extraction without being overly verbose. We propose the following schema, illustrated with an example:
$X$ : Variants in the estrogen receptor alpha (ESR1) gene and its mRNA contribute to risk for schizophrenia.
$Y$ : estrogen receptor alpha ; ESR1 @GENE@ schizophrenia @DISEASE@ @GDA@</p>
<p>The input text $X$, expresses a gene-disease association (GDA) between ESR1 and schizophrenia. In the corresponding target string $Y$, each relation begins with its constituent entities. A semicolon separates coreferent mentions (;), and entities are terminated with a special token denoting their type (e.g. @GENE@). Similarly, relations are terminated with a special token denoting their type (e.g. @GDA@). Two or more entities can be included before the special relation token to support $n$-ary extraction. Entities can be ordered if they serve specific roles as head or tail of a relation. For each document, multiple relations can be included in the target string. Entities may be nested or discontinuous in the input text. In Figure 1, we provide examples of how this
schema can be used to model various complexities, like coreferent entity mentions and $n$-ary relations.</p>
<h3>3.2 Model</h3>
<p>The model follows a canonical seq2seq setup. An encoder maps each token in the input to a contextual embedding. An autoregressive decoder generates an output, token-by-token, attending to the outputs of the encoder at each timestep (Figure 2). Decoding proceeds until a special "end-of-sequence" token (@END@) is generated, or a maximum number of tokens have been generated. Formally, $X$ is the source sequence of length $S$, which is some text we would like to extract relations from. $Y$ is the corresponding target sequence of length $T$, a linearization of the relations contained in the source. We model the conditional probability</p>
<p>$$
p(Y \mid X)=\prod_{t=1}^{T} p\left(y_{t} \mid X, y_{&lt;t}\right)
$$</p>
<p>During training, we optimize over the model parameters $\theta$ the sequence cross-entropy loss</p>
<p>$$
\ell(\theta)=-\sum_{t=1}^{T} \log p\left(y_{t} \mid X, y_{&lt;t} ; \theta\right)
$$</p>
<p>maximizing the log-likelihood of the training data. ${ }^{1}$
The main problems with this setup for RE are: 1) The model might "hallucinate" by generating entity mentions that do not appear in the source text. 2) It may generate a target string that does not follow the linearization schema and therefore cannot</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>be parsed. 3) The loss function is permutationssensitive, enforcing an unnecessary decoding order. To address 1) we use two modifications: a restricted target vocabulary (§3.2.1) and a copy mechanism (§3.2.2). To address 2) we experiment with several constraints applied during decoding (§3.2.3). Finally, to address 3) we sort relations according to their order of appearance in the source text (§3.2.4).</p>
<h3>3.2.1 Restricted target vocabulary</h3>
<p>To prevent the model from "hallucinating" (generating entity mentions that do not appear in the source text), the target vocabulary is restricted to the set of special tokens needed to model entities and relations (e.g. ; and @DRUG@). All other tokens must be copied from the input using a copy mechanism (see §3.2.2). The embeddings of these special tokens are initialized randomly and learned jointly with the rest of the model's parameters.</p>
<h3>3.2.2 Copy mechanism</h3>
<p>To enable copying of input tokens during decoding, we use a copying mechanism (Gu et al., 2016a). The mechanism works by effectively extending the target vocabulary with the tokens in the source sequence $X$, allowing the model to "copy" these tokens into the output sequence, $Y$. Our use of the copy mechanism is similar to previous seq2seqbased approaches for RE (Zeng et al., 2018, 2020).</p>
<h3>3.2.3 Constrained decoding</h3>
<p>We experimented with several constraints applied to the decoder during test time to reduce the likelihood of generating syntactically invalid target strings (strings that do not follow the linearization schema). These constraints are applied by setting the predicted probabilities of invalid tokens to a tiny value at each timestep. The full set of constraints is depicted in Appendix A. In practice, we found that a trained model rarely generates invalid target strings, so these constraints have little effect on final performance (see §5.3). We elected not to apply them in the rest of our experiments.</p>
<h3>3.2.4 Sorting relations</h3>
<p>The relations to extract from a given document are inherently unordered. However, the sequence crossentropy loss (Equation 2) is permutation-sensitive with respect to the predicted tokens. During training, this enforces an unnecessary decoding order and may make the model prone to overfit frequent token combinations in the training set (Vinyals
et al., 2016; Yang et al., 2019). To partially mitigate this, we sort relations within the target strings according to their order of appearance in the source text, providing the model with a consistent decoding order. The position of a relation is determined by the first occurring mention of its head entity. The position of a mention is determined by the sum of its start and end character offsets. In the case of ties, we then sort by the first mention of its tail entity (and so on for $n$-ary relations).</p>
<h3>3.3 Entity hinting</h3>
<p>Although the proposed model can jointly extract entities and relations from unannotated text, most existing DocRE methods provide the entities as input. Therefore, to more fairly compare to existing methods, we also provide entities as input, using a simple strategy that we will refer to as "entity hinting". This involves prepending entities to the source text as they appear in the target string. Taking the example from $\S 3.1$, entity hints would be added as follows:</p>
<div class="codehilite"><pre><span></span><code><span class="n">X</span><span class="o">:</span><span class="w"> </span><span class="n">estrogen</span><span class="w"> </span><span class="n">receptor</span><span class="w"> </span><span class="n">alpha</span><span class="w"> </span><span class="o">;</span><span class="w"> </span><span class="n">ESR1</span><span class="w"> </span><span class="err">@</span><span class="n">GENE</span><span class="err">@</span>
<span class="n">schizophrenia</span><span class="w"> </span><span class="err">@</span><span class="n">DISEASE</span><span class="err">@</span><span class="w"> </span><span class="err">@</span><span class="n">SEP</span><span class="err">@</span><span class="w"> </span><span class="n">Variants</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">estrogen</span>
<span class="n">receptor</span><span class="w"> </span><span class="n">alpha</span><span class="w"> </span><span class="o">(</span><span class="n">ESR1</span><span class="o">)</span><span class="w"> </span><span class="n">gene</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">its</span><span class="w"> </span><span class="n">mRNA</span><span class="w"> </span><span class="n">contribute</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">risk</span>
<span class="k">for</span><span class="w"> </span><span class="n">schizophrenia</span><span class="o">.</span>
</code></pre></div>

<p>where the special @SEP@ token demarcates the end of the entity hint. ${ }^{2}$ We experimented with the common approach of inserting marker tokens before and after each entity mention (Zhou and Chen, 2021) but found this to perform worse. Our approach adds fewer extra tokens to the source text and provides a location for the copy mechanism to focus, i.e. tokens left of @SEP@. In our experiments, we use entity hinting when comparing to methods that provide ground truth entity annotations as input (§5.1.1). In $\S 5.2$, we use entity hinting to compare pipeline-based and end-to-end approaches.</p>
<h2>4 Experimental setup</h2>
<h3>4.1 Datasets</h3>
<p>We evaluate our approach on several biomedical, DocRE datasets. We also include one nonbiomedical dataset, DocRED. In Appendix B, we list relevant details about their annotations.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>CDR (Li et al., 2016b) The BioCreative V CDR task corpus is manually annotated for chemicals, diseases and chemical-induced disease (CID) relations. It contains the titles and abstracts of 1500 PubMed articles and is split into equally sized train, validation and test sets. Given the relatively small size of the training set, we follow Christopoulou et al. (2019) and others by first tuning the model on the validation set and then training on the combination of the train and validation sets before evaluating on the test set. Similar to prior work, we filter negative relations with disease entities that are hypernyms of a corresponding true relations disease entity within the same abstract (see Appendix C).</p>
<p>GDA (Wu et al., 2019) The gene-disease association corpus contains 30,192 titles and abstracts from PubMed articles that have been automatically labelled for genes, diseases and gene-disease associations via distant supervision. The test set is comprised of 1000 of these examples. Following Christopoulou et al. (2019) and others, we hold out a random $20 \%$ of the remaining abstracts as a validation set and use the rest for training.</p>
<p>DGM (Jia et al., 2019) The drug-gene-mutation corpus contains 4606 PubMed articles that have been automatically labelled for drugs, genes, mutations and ternary drug-gene-mutation relationships via distant supervision. The dataset is available in three variants: sentence, paragraph, and documentlength text. We train and evaluate our model on the paragraph-length inputs. Since the test set does not contain relation annotations on the paragraph level, we report results on the validation set. We hold out a random $20 \%$ of training examples to form a new validation set for tuning.</p>
<p>DocRED (Yao et al., 2019) DocRED includes over 5000 human-annotated documents from Wikipedia. There are six entity and 96 relation types, with $\sim 40 \%$ of relations crossing the sentence boundary. We use the same split as previous end-to-end methods (Eberts and Ulges, 2021), which has 3,008 documents in the training set, 300 in the validation set and 700 in the test set ${ }^{3}$.</p>
<h3>4.2 Evaluation</h3>
<p>We evaluate our model using the micro F1-score by extracting relation tuples from the decoder's output (see Appendix D). Similar to prior work, we use a "strict" criteria. A predicted relation is considered</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>correct if the relation type and its entities match a ground truth relation. An entity is considered correct if the entity type and its mentions match a ground truth entity. However, since the aim of DocRE is to extract relations at the entity-level (as opposed to the mention-level), we also report performance using a relaxed criterion (denoted "relaxed"), where predicted entities are considered correct if more than $50 \%$ of their mentions match a ground truth entity (see Appendix E).</p>
<p>Existing methods that evaluate on CDR, GDA and DGM use the ground truth entity annotations as input. This makes it difficult to directly compare with our end-to-end approach, which takes only the raw text as input. To make the comparison fairer, we use entity hinting (§3.3) so that our model has access to the ground truth entity annotations. We also report the performance of our method in the end-to-end setting on these corpora to facilitate future comparison. To compare to existing end-toend approaches, we use DocRED.</p>
<h3>4.3 Implementation, training and hyperparameters</h3>
<p>Implementation We implemented our model in PyTorch (Paszke et al., 2017) using AllenNLP (Gardner et al., 2018). As encoder, we use a pretrained transformer, implemented in the Transformers library (Wolf et al., 2020), which is fine-tuned during training. When training and evaluating on biomedical corpora, we use PubMedBERT (Gu et al., 2020), and BERT ${ }_{\text {BASE }}$ (Devlin et al., 2019) otherwise. In both cases, we use the default hyperparameters of the pretrained model. As decoder, we use a single-layer LSTM (Hochreiter and Schmidhuber, 1997) with randomly initialized weights. We use multi-head attention (Vaswani et al., 2017) as the cross-attention mechanism between encoder and decoder. Select hyperparameters were tuned on the validation sets, see Appendix F for details.</p>
<p>Training All parameters are trained jointly using the AdamW optimizer (Loshchilov and Hutter, 2019). Before training, we re-initialize the top $L$ layers of the pretrained transformer encoder, which has been shown to improve performance and stability during fine-tuning (Zhang et al., 2021b). During training, the learning rate is linearly increased for the first $10 \%$ of training steps and linearly decayed to zero afterward. Gradients are scaled to a vector norm of 1.0 before backpropagating. During each forward propagation, the hidden state of the LSTM</p>
<p>Table 1: Comparison to existing pipeline-based methods. Performance reported as micro-precision, recall and F1scores (\%) on the CDR and GDA test sets. Results below the horizontal line are not comparable to existing methods. Bold: best scores.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">CDR</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">GDA</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Method</td>
<td style="text-align: center;">P</td>
<td style="text-align: center;">R</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">P</td>
<td style="text-align: center;">R</td>
<td style="text-align: center;">F1</td>
</tr>
<tr>
<td style="text-align: left;">Christopoulou et al. (2019)</td>
<td style="text-align: center;">62.1</td>
<td style="text-align: center;">65.2</td>
<td style="text-align: center;">63.6</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">81.5</td>
</tr>
<tr>
<td style="text-align: left;">Nan et al. (2020)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">64.8</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">82.2</td>
</tr>
<tr>
<td style="text-align: left;">Minh Tran et al. (2020)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">66.1</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">82.8</td>
</tr>
<tr>
<td style="text-align: left;">Lai and Lu (2021)</td>
<td style="text-align: center;">64.9</td>
<td style="text-align: center;">67.1</td>
<td style="text-align: center;">66.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Xu et al. (2021)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">68.7</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">83.7</td>
</tr>
<tr>
<td style="text-align: left;">Zhou et al. (2021)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\mathbf{6 9 . 4}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">83.9</td>
</tr>
<tr>
<td style="text-align: left;">seq2rel (entity hinting)</td>
<td style="text-align: center;">68.2</td>
<td style="text-align: center;">66.2</td>
<td style="text-align: center;">67.2</td>
<td style="text-align: center;">84.4</td>
<td style="text-align: center;">85.3</td>
<td style="text-align: center;">$\mathbf{8 4 . 9}$</td>
</tr>
<tr>
<td style="text-align: left;">seq2rel (entity hinting, relaxed)</td>
<td style="text-align: center;">68.2</td>
<td style="text-align: center;">66.2</td>
<td style="text-align: center;">67.2</td>
<td style="text-align: center;">84.5</td>
<td style="text-align: center;">85.4</td>
<td style="text-align: center;">85.0</td>
</tr>
<tr>
<td style="text-align: left;">seq2rel (end-to-end)</td>
<td style="text-align: center;">43.5</td>
<td style="text-align: center;">37.5</td>
<td style="text-align: center;">40.2</td>
<td style="text-align: center;">55.0</td>
<td style="text-align: center;">55.4</td>
<td style="text-align: center;">55.2</td>
</tr>
<tr>
<td style="text-align: left;">seq2rel (end-to-end, relaxed)</td>
<td style="text-align: center;">56.6</td>
<td style="text-align: center;">48.8</td>
<td style="text-align: center;">52.4</td>
<td style="text-align: center;">70.3</td>
<td style="text-align: center;">70.8</td>
<td style="text-align: center;">70.5</td>
</tr>
</tbody>
</table>
<p>decoder is initialized with the mean of token embeddings output by the encoder. The decoder is regularized by applying dropout (Srivastava et al., 2014) with probability 0.1 to its inputs, and DropConnect (Wan et al., 2013) with probability 0.5 to the hidden-to-hidden weights. As is common, we use teacher forcing, feeding previous ground truth inputs to the decoder when predicting the next token in the sequence. During test time, we generate the output using beam search (Graves, 2012). Beams are ranked by mean token log probability after applying a length penalty. ${ }^{4}$ Models were trained and evaluated on a single NVIDIA Tesla V100. ${ }^{5}$</p>
<h2>5 Results</h2>
<h3>5.1 Comparison to existing methods</h3>
<p>In the following sections, we compare our model to existing DocRE methods on several benchmark corpora. We compare to existing pipeline-based methods (§5.1.1), including $n$-ary methods (§5.1.2), and end-to-end methods (§5.1.3). Details about these methods are provided in Appendix G.</p>
<h3>5.1.1 Existing pipeline-based methods</h3>
<p>In Table 1, we use entity hinting to compare our method to existing pipeline-based methods on CDR and GDA. We also report end-to-end performance, which is not comparable to existing pipeline-based methods but will facilitate future comparisons.</p>
<p>The large performance improvement when using entity hinting ( $+27-29 \%$ ) confirms that the model</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: Effect of training set size on performance. Performance reported as the median micro F1-score obtained over five runs with different random seeds on the CDR and GDA validation sets, with and without entity hinting. Error bands correspond to the standard deviation over the five runs. The absolute number of training examples are displayed for each corpus. Some labels are excluded to reduce clutter.
exploits the entity annotations. The fact that relaxed entity matching makes a large difference in the end-to-end setting ( $+12-15 \%$ ) suggests that a significant portion of the model's mistakes occur during coreference resolution. Although our method is designed for end-to-end RE, we find that it outperforms existing pipeline-based methods when using entity hinting on GDA. Our method is competitive with existing methods when using entity hinting on the CDR corpus but ultimately underperforms state-of-the-art results. Given that GDA is 46 X larger, we speculated that our method might be underperforming in the low-data regime. To determine if this is a contributing factor, we artificially reduce the size of the CDR and GDA training sets and plot the performance as a curve (Figure 3). In all cases besides GDA with entity hinting, performance increases monotonically with dataset size. There is no obvious plateau on CDR even when using all 500 training examples. Together, these results suggest that our seq2seq based approach can outperform existing pipeline-based methods when there are sufficient training examples but underperforms relative to existing methods in the low-data regime.</p>
<h3>5.1.2 $n$-ary relation extraction</h3>
<p>In Table 2 we compare to existing $n$-ary methods on the DGM corpus. With entity hinting, our method significantly outperforms the existing method. The difference in encoders partially explains this large performance gap. Where Jia et al. (2019) use a BiLSTM that is trained from scratch, we use PubMedBERT, a much larger model that has been pretrained on abstracts and full-text ar-</p>
<p>Table 2: Comparison to existing $n$-ary methods. Performance reported as micro-precision, recall and F1-scores (\%) on the DGM validation set. Results below the horizontal line are not comparable to existing methods. Bold: best scores. ${ }^{\dagger}$ Jia et al. 2019 do not report results on the validation set, so we re-run their paragraph-level model.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">P</th>
<th style="text-align: center;">R</th>
<th style="text-align: center;">F1</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Jia et al. (2019) ${ }^{\dagger}$</td>
<td style="text-align: center;">62.9</td>
<td style="text-align: center;">76.2</td>
<td style="text-align: center;">68.9</td>
</tr>
<tr>
<td style="text-align: left;">seq2rel (entity hinting)</td>
<td style="text-align: center;">$\mathbf{8 4 . 0}$</td>
<td style="text-align: center;">$\mathbf{8 4 . 8}$</td>
<td style="text-align: center;">$\mathbf{8 4 . 4}$</td>
</tr>
<tr>
<td style="text-align: left;">seq2rel (entity hinting, relaxed)</td>
<td style="text-align: center;">84.1</td>
<td style="text-align: center;">84.9</td>
<td style="text-align: center;">84.5</td>
</tr>
<tr>
<td style="text-align: left;">seq2rel (end-to-end)</td>
<td style="text-align: center;">68.9</td>
<td style="text-align: center;">65.9</td>
<td style="text-align: center;">67.4</td>
</tr>
<tr>
<td style="text-align: left;">seq2rel (end-to-end, relaxed)</td>
<td style="text-align: center;">78.3</td>
<td style="text-align: center;">74.9</td>
<td style="text-align: center;">76.6</td>
</tr>
</tbody>
</table>
<p>ticles from PubMedCentral. ${ }^{6}$ However, this does not completely account for the improvement in performance, as recent work that has replaced the BiLSTM encoder of (Jia et al., 2019) with PubMedBERT found that it improves performance by approximately $2-4 \%$ on the task of drug-genemutation prediction (Zhang et al., 2021a). ${ }^{7}$ Our results on the DGM corpus suggest that our linearization schema effectively models $n$-ary relations without requiring changes to the model architecture or training procedure.</p>
<h3>5.1.3 End-to-end methods</h3>
<p>In Table 3 we compare to an existing end-to-end approach on DocRED, JEREX (Eberts and Ulges, 2021). To make the comparison fair, we use the same pretrained encoder (BERT ${ }_{\text {BASE }}$ ). We find that although our model is arguably simpler (JEREX contains four task-specific sub-components, each with its own loss) it only slightly underperforms JEREX, mainly due to recall. We speculate that one reason for this is a large number of relations per document, which leads to longer target strings and, therefore, more decoding steps. The median length of the target strings in DocRED, using our linearization, is 110, whereas the next largest is 19 in GDA. Improving the decoder's ability to process long sequences, e.g. switching the LSTM for a transformer or modifying the linearization schema to produce shorter target strings, may improve recall and close the gap with existing methods.</p>
<h3>5.2 Pipeline vs. End-to-end</h3>
<p>In $\S 5.1 .1$ and $\S 5.1 .2$, we provide gold-standard entity annotations from each corpus as input to</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 3: Comparison to existing end-to-end methods. Performance reported as micro-precision, recall and F1scores (\%) on the DocRED test set. Results below the horizontal line are not comparable to existing methods. Bold: best scores.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">P</th>
<th style="text-align: center;">R</th>
<th style="text-align: center;">F1</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">JEREX (Eberts and Ulges, 2021)</td>
<td style="text-align: center;">42.8</td>
<td style="text-align: center;">$\mathbf{3 8 . 2}$</td>
<td style="text-align: center;">$\mathbf{4 0 . 4}$</td>
</tr>
<tr>
<td style="text-align: left;">seq2rel (end-to-end)</td>
<td style="text-align: center;">$\mathbf{4 4 . 0}$</td>
<td style="text-align: center;">33.8</td>
<td style="text-align: center;">38.2</td>
</tr>
<tr>
<td style="text-align: left;">seq2rel (end-to-end, relaxed)</td>
<td style="text-align: center;">53.7</td>
<td style="text-align: center;">41.3</td>
<td style="text-align: center;">46.7</td>
</tr>
</tbody>
</table>
<p>Table 4: Comparison of pipeline-based and end-to-end approaches. Gold hints use gold-standard entity annotations to insert entity hints in the source text. Silver hints use the entity annotations provided by PubTator. Pipeline is identical to silver entity hints, except that we filter out entity mentions predicted by our model that PubTator does not predict. The end-to-end model only has access to the unannotated source text as input. Performance reported as micro-precision, recall and F1scores (\%) on the CDR test set, with strict and relaxed entity matching criteria. Bold: best scores.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Strict</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Relaxed</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">P</td>
<td style="text-align: center;">R</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">P</td>
<td style="text-align: center;">R</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Gold hints</td>
<td style="text-align: center;">68.2</td>
<td style="text-align: center;">66.2</td>
<td style="text-align: center;">67.2</td>
<td style="text-align: center;">68.2</td>
<td style="text-align: center;">66.2</td>
<td style="text-align: center;">67.2</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Silver hints</td>
<td style="text-align: center;">42.4</td>
<td style="text-align: center;">37.3</td>
<td style="text-align: center;">39.7</td>
<td style="text-align: center;">53.0</td>
<td style="text-align: center;">46.7</td>
<td style="text-align: center;">49.7</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Pipeline</td>
<td style="text-align: center;">$\mathbf{4 5 . 0}$</td>
<td style="text-align: center;">16.9</td>
<td style="text-align: center;">24.6</td>
<td style="text-align: center;">$\mathbf{6 2 . 5}$</td>
<td style="text-align: center;">23.5</td>
<td style="text-align: center;">34.1</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">End-to-end</td>
<td style="text-align: center;">43.5</td>
<td style="text-align: center;">$\mathbf{3 7 . 5}$</td>
<td style="text-align: center;">$\mathbf{4 0 . 2}$</td>
<td style="text-align: center;">56.6</td>
<td style="text-align: center;">$\mathbf{4 8 . 8}$</td>
<td style="text-align: center;">$\mathbf{5 2 . 4}$</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>our model via entity hinting (referred to as "gold" hints from here on, see §3.3). This allowed us to compare to existing methods that also provide these annotations as input. However, gold-standard entity annotations are (almost) never available in real-world settings, such as large-scale extraction on PubMed. In this setting, there are two strategies: pipeline-based, where independent systems perform entity and relation extraction, and end-toend, where a single model performs both tasks. To compare these approaches under our model, we perform evaluations where a named entity recognition (NER) system is used to determine entity hints (referred to as "silver" hints from here on) and when no entity hints are provided (end-to-end). ${ }^{8}$ However, this alone does not create a true pipeline, as our model can recover from both false negatives and false positives in the NER step. To mimic error propagation in the pipeline setting, we filter any entity mention predicted by our model that was not predicted by the NER system. In Table 4, we</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>Table 5: Ablation study results. Performance reported as the micro-precision, recall and F1-scores (\%) on the CDR and DocRED validation sets. $\Delta$ : difference to the complete models F1-score. Bold: best scores.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">CDR</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">DocRED</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">P</td>
<td style="text-align: center;">R</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">$\Delta$</td>
<td style="text-align: center;">P</td>
<td style="text-align: center;">R</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">$\Delta$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">seq2rel (end-to-end)</td>
<td style="text-align: center;">$\mathbf{4 1 . 0}$</td>
<td style="text-align: center;">35.1</td>
<td style="text-align: center;">37.8</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">46.9</td>
<td style="text-align: center;">$\mathbf{3 6 . 1}$</td>
<td style="text-align: center;">$\mathbf{4 0 . 8}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">- pretraining</td>
<td style="text-align: center;">9.4</td>
<td style="text-align: center;">6.9</td>
<td style="text-align: center;">8.0</td>
<td style="text-align: center;">-29.8</td>
<td style="text-align: center;">18.5</td>
<td style="text-align: center;">7.7</td>
<td style="text-align: center;">10.8</td>
<td style="text-align: center;">-30.0</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">- fine-tuning</td>
<td style="text-align: center;">24.3</td>
<td style="text-align: center;">20.5</td>
<td style="text-align: center;">22.2</td>
<td style="text-align: center;">-15.6</td>
<td style="text-align: center;">42.4</td>
<td style="text-align: center;">15.5</td>
<td style="text-align: center;">22.7</td>
<td style="text-align: center;">-18.1</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">- vocab restriction</td>
<td style="text-align: center;">39.6</td>
<td style="text-align: center;">32.2</td>
<td style="text-align: center;">35.5</td>
<td style="text-align: center;">-2.3</td>
<td style="text-align: center;">45.2</td>
<td style="text-align: center;">35.5</td>
<td style="text-align: center;">39.7</td>
<td style="text-align: center;">-1.1</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">- sorting relations</td>
<td style="text-align: center;">36.1</td>
<td style="text-align: center;">29.2</td>
<td style="text-align: center;">32.3</td>
<td style="text-align: center;">-5.6</td>
<td style="text-align: center;">$\mathbf{5 2 . 9}$</td>
<td style="text-align: center;">17.4</td>
<td style="text-align: center;">26.2</td>
<td style="text-align: center;">-14.7</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">+ constrained decoding</td>
<td style="text-align: center;">40.8</td>
<td style="text-align: center;">$\mathbf{3 5 . 6}$</td>
<td style="text-align: center;">$\mathbf{3 8 . 0}$</td>
<td style="text-align: center;">+0.2</td>
<td style="text-align: center;">46.8</td>
<td style="text-align: center;">35.9</td>
<td style="text-align: center;">40.6</td>
<td style="text-align: center;">-0.2</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>present the results of all four settings (gold and silver entity hints, pipeline and end-to-end) on CDR.</p>
<p>We find that using gold entity hints significantly outperforms all other settings. This is expected, as the gold-standard entity annotations are highquality labels produced by domain experts. Using silver hints significantly drops performance, likely due to a combination of false positive and false negatives from the NER step. In the pipeline setting, where there is no recovery from false negatives, performance falls by another $15 \%$. The end-to-end setting significantly outperforms the pipeline setting (due to a large boost in recall) and performs comparably to using silver hints. Together, our results suggest that performance reported using gold-standard entity annotations may be overly optimistic and corroborates previous work demonstrating the benefits of jointly learning entity and relation extraction (Miwa and Sasaki, 2014; Miwa and Bansal, 2016; Gupta et al., 2016; Li et al., 2016a, 2017; Nguyen and Verspoor, 2019a; Yu et al., 2020).</p>
<h3>5.3 Ablation</h3>
<p>In Table 5, we present the results of an ablation study. We perform the analysis twice, once on the biomedical corpus CDR and once on the general domain corpus DocRED. Unsurprisingly, we find that fine-tuning a pretrained encoder greatly impacts performance. Training the same encoder from scratch (- pretraining) reduces performance by $\sim 30 \%$. Using the pretrained weights without fine-tuning (- fine-tuning) drops performance by 15.6-18.1\%. Restricting the target vocabulary (vocab restriction, see $\S 3.2 .1$ ) has a small positive impact, boosting performance by $1.1 \%-2.3 \%$. Deliberately ordering the relations within each target string (- sorting relations, see $\S 3.2 .4$ ) has a large positive impact, boosting performance by $5.6 \%$ $14.7 \%$. This effect is larger on DocRED, likely because it has more relations per document on average than CDR, so ordering becomes more impor-
tant. Finally, adding constraints to the decoding process (+ constrained decoding) has little impact on performance, suggesting that a trained model rarely generates invalid target strings (see §3.2.3).</p>
<h2>6 Discussion</h2>
<h3>6.1 Related work</h3>
<p>Seq2seq learning for RE has been explored in prior work. CopyRE (Zeng et al., 2018) uses an encoder-decoder architecture with a copy mechanism, similar to our approach, but is restricted to intra-sentence relations. Additionally, because CopyRE's decoding proceeds for exactly three timesteps per relation, the model is limited to generating binary relations between single token entities. The ability to decode multi-token entities was addressed in follow-up work, CopyMTL (Zeng et al., 2020). A similar approach was published concurrently but was again limited to intra-sentence binary relations (Nayak and Ng, 2020). Most recently, GenerativeRE (Cao and Ananiadou, 2021) proposed a novel copy mechanism to improve performance on multi-token entities. None of these approaches deal with the complexities of DocRE, where many relations cross the sentence boundary, and coreference resolution is critical. ${ }^{9}$</p>
<p>More generally, our paper is related to a recently proposed "text-to-text" framework (Raffel et al., 2020). In this framework, a task is formulated so that the inputs and outputs are both text strings, enabling the use of the same model, loss function and even hyperparameters across many seq2seq, classification and regression tasks. This framework has recently been applied to biomedical literature to perform named entity recognition, relation extraction (binary, intra-sentence), natural language inference, and question answering (Phan et al., 2021). Our work can be seen as an attempt to formulate the task of DocRE within this framework.</p>
<h3>6.2 Limitations and future work</h3>
<p>Permutation-sensitive loss Our approach adopts the sequence cross-entropy loss (Equation 2), which is sensitive to the order of predicted tokens, enforcing an unnecessary decoding order on the inherently unordered relations. To partially mitigate this problem, we order relations within the</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>target string according to order of appearance in the source text, providing the model with a consistent decoding order that can be learned (see §3.2.4, §5.3). Previous work has addressed this issue with various strategies, including reinforcement learning (Zeng et al., 2019), unordered-multi-tree decoders (Zhang et al., 2020), and non-autoregressive decoders (Sui et al., 2020). However, these works are limited to binary intra-sentence relation extraction, and their suitability for DocRE has not been explored. A promising future direction would be to modify our approach such that the arbitrary order of relations is not enforced during training.</p>
<p>Input length restriction Due to the pretrained encoder's input size limit (512 tokens), our experiments are conducted on paragraph-length text. Our model could be extended to full documents by swapping its encoder with any of the recently proposed "efficient transformers" (Tay et al., 2021). Future work could evaluate such a model's ability to extract relations from full scientific papers.</p>
<p>Pretraining the decoder In our model, the encoder is pretrained, while the decoder is trained from scratch. Several recent works, such as T5 (Raffel et al., 2020) and BART (Lewis et al., 2020), have proposed pretraining strategies for entire encoder-decoder architectures, which can be fine-tuned on downstream tasks. An interesting future direction would be to fine-tune such a model on DocRE using our linearization schema.</p>
<h2>7 Conclusion</h2>
<p>In this paper, we extend generative, seq2seq methods for relation extraction to the document level. We propose a novel linearization schema that can handle complexities overlooked by previous seq2seq approaches, like coreferent mentions and $n$-ary relations. We compare our approach to existing pipeline-based and end-to-end methods on several benchmark corpora, in some cases exceeding their performance. In future work, we hope to extend our method to full scientific papers and develop strategies to improve performance in the low-data regime and in cases where there are many relations per document.</p>
<h2>Acknowledgements</h2>
<p>This research was enabled in part by support provided by Compute Ontario (www.computeontario.ca), Compute Canada
(www.computecanada.ca) and the CIFAR AI Chairs Program and partially funded by the US National Institutes of Health (NIH) [U41 HG006623, U41 HG003751).</p>
<h2>References</h2>
<p>Takuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and Masanori Koyama. 2019. Optuna: A nextgeneration hyperparameter optimization framework. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \&amp; Data Mining, KDD 2019, Anchorage, AK, USA, August 4-8, 2019, pages 2623-2631. ACM.</p>
<p>Giannis Bekoulis, Johannes Deleu, Thomas Demeester, and Chris Develder. 2018. Joint entity recognition and relation extraction as a multi-head selection problem. Expert Systems with Applications, 114:34-45.</p>
<p>James Bergstra, Rémi Bardenet, Yoshua Bengio, and Balázs Kégl. 2011. Algorithms for hyper-parameter optimization. In Advances in Neural Information Processing Systems 24: 25th Annual Conference on Neural Information Processing Systems 2011. Proceedings of a meeting held 12-14 December 2011, Granada, Spain, pages 2546-2554.</p>
<p>Jiarun Cao and Sophia Ananiadou. 2021. GenerativeRE: Incorporating a novel copy mechanism and pretrained model for joint entity and relation extraction. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2119-2126, Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Fenia Christopoulou, Makoto Miwa, and Sophia Ananiadou. 2019. Connecting the dots: Document-level neural relation extraction with edge-oriented graphs. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 49254936, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Markus Eberts and Adrian Ulges. 2021. An end-to-end model for entity-level relation extraction using multiinstance learning. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 3650-3660, Online. Association for Computational Linguistics.</p>
<p>Matt Gardner, Joel Grus, Mark Neumann, Oyvind Tafjord, Pradeep Dasigi, Nelson F. Liu, Matthew Peters, Michael Schmitz, and Luke Zettlemoyer. 2018. AllenNLP: A deep semantic natural language processing platform. In Proceedings of Workshop for NLP Open Source Software (NLP-OSS), pages 1-6, Melbourne, Australia. Association for Computational Linguistics.</p>
<p>John Giorgi, Xindi Wang, Nicola Sahar, Won Young Shin, Gary D Bader, and Bo Wang. 2019. End-toend named entity recognition and relation extraction using pre-trained language models. ArXiv preprint, abs/1912.13415.</p>
<p>Alex Graves. 2012. Sequence transduction with recurrent neural networks. ArXiv preprint, abs/1211.3711.</p>
<p>Jiatao Gu, Zhengdong Lu, Hang Li, and Victor O.K. Li. 2016a. Incorporating copying mechanism in sequence-to-sequence learning. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1631-1640, Berlin, Germany. Association for Computational Linguistics.</p>
<p>Jinghang Gu, Longhua Qian, and Guodong Zhou. 2016b. Chemical-induced disease relation extraction with various linguistic features. Database: The Journal of Biological Databases and Curation, 2016.</p>
<p>Jinghang Gu, Fuqing Sun, Longhua Qian, and Guodong Zhou. 2017. Chemical-induced disease relation extraction via convolutional neural network. Database: The Journal of Biological Databases and Curation, 2017.</p>
<p>Yu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto Usuyama, Xiaodong Liu, Tristan Naumann, Jianfeng Gao, and Hoifung Poon. 2020. Domain-specific language model pretraining for biomedical natural language processing. ArXiv preprint, abs/2007.15779.</p>
<p>Pankaj Gupta, Hinrich Schütze, and Bernt Andrassy. 2016. Table filling multi-task recurrent neural network for joint entity and relation extraction. In Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 2537-2547, Osaka, Japan. The COLING 2016 Organizing Committee.</p>
<p>Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory. Neural Computation, 9:17351780 .</p>
<p>Pere-Lluís Huguet Cabot and Roberto Navigli. 2021. REBEL: Relation extraction by end-to-end language generation. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 23702381, Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Sarthak Jain, Madeleine van Zuylen, Hannaneh Hajishirzi, and Iz Beltagy. 2020. SciREX: A challenge dataset for document-level information extraction. In</p>
<p>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 75067516, Online. Association for Computational Linguistics.</p>
<p>Robin Jia, Cliff Wong, and Hoifung Poon. 2019. Document-level n-ary relation extraction with multiscale representation learning. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 3693-3704, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Jin-Dong Kim, T. Ohta, Yuka Tateisi, and Jun'ichi Tsujii. 2003. Genia corpus - a semantically annotated corpus for bio-textmining. Bioinformatics, 19 Suppl 1:i1802.</p>
<p>Po-Ting Lai and Zhiyong Lu. 2021. Bert-gt: Crosssentence n-ary relation extraction with bert and graph transformer. Bioinformatics.</p>
<p>Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871-7880, Online. Association for Computational Linguistics.</p>
<p>Fei Li, Meishan Zhang, Guohong Fu, and Donghong Ji. 2017. A neural joint model for entity and relation extraction from biomedical text. BMC bioinformatics, 18(1):198.</p>
<p>Fei Li, Yue Zhang, Meishan Zhang, and Donghong Ji. 2016a. Joint models for extracting adverse drug events from biomedical text. In Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence, IJCAI 2016, New York, NY, USA, 915 July 2016, pages 2838-2844. IJCAI/AAAI Press.</p>
<p>Jiao Li, Yueping Sun, Robin J. Johnson, Daniela Sciaky, Chih-Hsuan Wei, Robert Leaman, Allan Peter Davis, Carolyn J. Mattingly, Thomas C. Wiegers, and Zhiyong Lu. 2016b. Biocreative v cdr task corpus: a resource for chemical disease relation extraction. ArXiv preprint, abs/d.</p>
<p>Feifan Liu, Jinying Chen, Abhyuday Jagannatha, and Hong Yu. 2016. Learning for biomedical information extraction: Methodological review of recent advances. ArXiv preprint, abs/1606.07993.</p>
<p>Ilya Loshchilov and Frank Hutter. 2019. Decoupled weight decay regularization. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net.</p>
<p>Yi Luan, Luheng He, Mari Ostendorf, and Hannaneh Hajishirzi. 2018. Multi-task identification of entities, relations, and coreference for scientific knowledge</p>
<p>graph construction. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3219-3232, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Hieu Minh Tran, Minh Trung Nguyen, and Thien Huu Nguyen. 2020. The dots have their values: Exploiting the node-edge connections in graph-based neural models for document-level relation extraction. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4561-4567, Online. Association for Computational Linguistics.</p>
<p>Makoto Miwa and Mohit Bansal. 2016. End-to-end relation extraction using LSTMs on sequences and tree structures. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1105-1116, Berlin, Germany. Association for Computational Linguistics.</p>
<p>Makoto Miwa and Yutaka Sasaki. 2014. Modeling joint entity and relation extraction with table representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1858-1869, Doha, Qatar. Association for Computational Linguistics.</p>
<p>Guoshun Nan, Zhijiang Guo, Ivan Sekulic, and Wei Lu. 2020. Reasoning with latent structure refinement for document-level relation extraction. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1546-1557, Online. Association for Computational Linguistics.</p>
<p>Tapas Nayak and Hwee Tou Ng. 2020. Effective modeling of encoder-decoder architecture for joint entity and relation extraction. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pages 8528-8535. AAAI Press.</p>
<p>Dat Quoc Nguyen and Karin Verspoor. 2019a. End-toend neural relation extraction using deep biaffine attention. In Advances in Information Retrieval, pages 729-738, Cham. Springer, Springer International Publishing.</p>
<p>Dat Quoc Nguyen and Karin Verspoor. 2019b. End-to-end neural relation extraction using deep biaffine attention. In European Conference on Information Retrieval, pages 729-738. Springer.</p>
<p>Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. 2017. Automatic differentiation in PyTorch. In NIPS Autodiff Workshop.</p>
<p>Nanyun Peng, Hoifung Poon, Chris Quirk, Kristina Toutanova, and Wen-tau Yih. 2017. Cross-sentence n-ary relation extraction with graph LSTMs. Transactions of the Association for Computational Linguistics, 5:101-115.</p>
<p>Long N Phan, James T Anibal, Hieu Tran, Shaurya Chanana, Erol Bahadroglu, Alec Peltekian, and Grégoire Altan-Bonnet. 2021. Scifive: a text-to-text transformer model for biomedical literature. ArXiv preprint, abs/2106.03598.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21:167.</p>
<p>Linfeng Song, Yue Zhang, Zhiguo Wang, and Daniel Gildea. 2018. N-ary relation extraction using graphstate LSTM. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2226-2235, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Nitish Srivastava, Geoffrey E. Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: a simple way to prevent neural networks from overfitting. J. Mach. Learn. Res., 15:19291958.</p>
<p>Dianbo Sui, Yubo Chen, Kang Liu, Jun Zhao, Xiangrong Zeng, and Shengping Liu. 2020. Joint entity and relation extraction with set prediction networks. ArXiv preprint, abs/2011.01675.</p>
<p>Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada, pages 3104-3112.</p>
<p>Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. 2021. Long range arena : A benchmark for efficient transformers. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 5998-6008.</p>
<p>Patrick Verga, Emma Strubell, and Andrew McCallum. 2018. Simultaneously self-attending to all mentions for full-abstract biological relation extraction. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 872-884, New Orleans, Louisiana. Association for Computational Linguistics.</p>
<p>Oriol Vinyals, Samy Bengio, and Manjunath Kudlur. 2016. Order matters: Sequence to sequence for sets. In 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings.</p>
<p>David Wadden, Ulme Wennberg, Yi Luan, and Hannaneh Hajishirzi. 2019. Entity, relation, and event extraction with contextualized span representations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 57845789, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Li Wan, Matthew D. Zeiler, Sixin Zhang, Yann LeCun, and Rob Fergus. 2013. Regularization of neural networks using dropconnect. In Proceedings of the 30th International Conference on Machine Learning, ICML 2013, Atlanta, GA, USA, 16-21 June 2013, volume 28 of JMLR Workshop and Conference Proceedings, pages 1058-1066. JMLR.org.</p>
<p>Yucheng Wang, Bowen Yu, Hongsong Zhu, Tingwen Liu, Nan Yu, and Limin Sun. 2021. Discontinuous named entity recognition as maximal clique discovery. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 764-774, Online. Association for Computational Linguistics.</p>
<p>Chih-Hsuan Wei, Hung-Yu Kao, and Zhiyong Lu. 2013. Pubtator: a web-based text mining tool for assisting biocuration. Nucleic acids research, 41(W1):W518W522.</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38-45, Online. Association for Computational Linguistics.</p>
<p>Ye Wu, Ruibang Luo, Henry C. M. Leung, Hing-Fung Ting, and Tak-Wah Lam. 2019. Renet: A deep learning approach for extracting gene-disease associations from literature. In Research in Computational Molecular Biology, pages 272-284, Cham. Springer International Publishing.</p>
<p>Benfeng Xu, Quan Wang, Yajuan Lyu, Yong Zhu, and Zhendong Mao. 2021. Entity structure within and throughout: Modeling mention dependencies for document-level relation extraction. In $A A A I$.</p>
<p>Pengcheng Yang, Fuli Luo, Shuming Ma, Junyang Lin, and Xu Sun. 2019. A deep reinforced sequence-to-set
model for multi-label classification. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5252-5258, Florence, Italy. Association for Computational Linguistics.</p>
<p>Yuan Yao, Deming Ye, Peng Li, Xu Han, Yankai Lin, Zhenghao Liu, Zhiyuan Liu, Lixin Huang, Jie Zhou, and Maosong Sun. 2019. DocRED: A large-scale document-level relation extraction dataset. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 764-777, Florence, Italy. Association for Computational Linguistics.</p>
<p>Bowen Yu, Zhenyu Zhang, Xiaobo Shu, Tingwen Liu, Yubin Wang, Bin Wang, and Sujian Li. 2020. Joint extraction of entities and relations based on a novel decomposition strategy. In ECAI 2020, pages 22822289. IOS Press.</p>
<p>Daojian Zeng, Haoran Zhang, and Qianying Liu. 2020. Copymtl: Copy mechanism for joint extraction of entities and relations with multi-task learning. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pages 95079514. AAAI Press.</p>
<p>Xiangrong Zeng, Shizhu He, Daojian Zeng, Kang Liu, Shengping Liu, and Jun Zhao. 2019. Learning the extraction order of multiple relational facts in a sentence with reinforcement learning. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 367-377, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Xiangrong Zeng, Daojian Zeng, Shizhu He, Kang Liu, and Jun Zhao. 2018. Extracting relational facts by an end-to-end neural model with copy mechanism. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 506-514, Melbourne, Australia. Association for Computational Linguistics.</p>
<p>Ranran Haoran Zhang, Qianying Liu, Aysa Xuemo Fan, Heng Ji, Daojian Zeng, Fei Cheng, Daisuke Kawahara, and Sadao Kurohashi. 2020. Minimize exposure bias of Seq2Seq models in joint entity and relation extraction. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 236-246, Online. Association for Computational Linguistics.</p>
<p>Sheng Zhang, Cliff Wong, Naoto Usuyama, Sarthak Jain, Tristan Naumann, and Hoifung Poon. 2021a. Modular self-supervision for document-level relation extraction. In Proceedings of the 2021 Conference</p>
<p>on Empirical Methods in Natural Language Processing, pages 5291-5302, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Tianyi Zhang, Felix Wu, Arzoo Katiyar, Kilian Q. Weinberger, and Yoav Artzi. 2021b. Revisiting fewsample BERT fine-tuning. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net.</p>
<p>Deyu Zhou, Dayou Zhong, and Yulan He. 2014. Biomedical relation extraction: from binary to complex. Computational and mathematical methods in medicine, 2014.</p>
<p>Wenxuan Zhou and Muhao Chen. 2021. An improved baseline for sentence-level relation extraction. ArXiv, abs/2102.01373.</p>
<p>Wenxuan Zhou, Kevin Huang, Tengyu Ma, and Jing Huang. 2021. Document-level relation extraction with adaptive thresholding and localized context pooling. In $A A A I$.</p>
<h2>A Constrained decoding</h2>
<p>In Figure 4, we illustrate the rules used to constrain decoding. At each timestep $t$, given the prediction of the previous timestep $t-1$, the predicted class probabilities of tokens that would generate a syntactically invalid target string are set to a tiny value. In practice, we found that a model rarely generates invalid target strings, so these constraints have little effect on final performance (see §3.2.3 and §5.3).</p>
<h2>B Details about dataset annotations</h2>
<p>In Table 6, we list which complexities (e.g. nested $\&amp;$ discontinuous mentions, $n$-ary relations) are contained within each dataset used in our evaluations. We also report the fraction of relations in the test set that are inter-sentence. We consider a relation intra-sentence if any sentence in the document contains at least one mention of each entity in the relation, and inter-sentence otherwise. This produces an estimate that matches previously reported numbers for CDR ( $\sim 30 \%$ ). In Yao et al. (2019), the fraction of inter-sentence relations in DocRED is reported as $\sim 40.7 \%$. We can reproduce this value if we consider relations intra-sentence when all mentions of an entity exist within a single sentence and inter-sentence otherwise.</p>
<h2>C Hypernym filtering</h2>
<p>The CDR dataset is annotated for chemical-induced disease (CID) relationships between the most
specific chemical and disease mentions in an abstract. Take the following example from the corpus:</p>
<p>Carbamazepine-induced cardiac dysfunction [...] A patient with sinus bradycardia and atrioventricular block, induced by carbamazepine, prompted an extensive literature review of all previously reported cases.</p>
<p>In this example (PMID: 1728915), only (carbamazepine, bradycardia) and (carbamazepine, atrioventricular block) are labelled as true relations. The relation (carbamazepine, cardiac dysfunction), although true, is not labelled as cardiac dysfunction is a hypernym of both bradycardia and atrioventricular block. This can harm evaluation performance, as the prediction (carbamazepine, cardiac dysfunction) will be considered a false positive. Therefore, we follow previous work (Gu et al., 2016b, 2017; Verga et al., 2018; Christopoulou et al., 2019; Zhou et al., 2021) by filtering negative relations like these, with disease entities that are hypernyms of a corresponding true relations disease entity within the same abstract, according to the hierarchy in the MeSH vocabulary. ${ }^{10}$</p>
<h2>D Parsing the models output</h2>
<p>At test time, our model autoregressively generates an output, token-by-token, using beam search decoding (see §3.2). In order to extract the predicted relations from this output, we apply the following steps. First, predicted token ids are converted to a string. We use the decode ()$^{11}$ method of the HuggingFace Transformers tokenizer (Wolf et al., 2020) to do this. For example, after calling decode () on the predicted token ids, this string might look like:
monoamine oxidase b ; maob @GENE@ parkinson's disease ; pd @DISEASE@ @GDA@</p>
<p>We then use regular expressions to extract any relations from this string that match our linearization schema (see §3.1), which produces a dictionary of nested lists, keyed by relation class:</p>
<div class="codehilite"><pre><span></span><code>{
    &quot;GDA&quot;: [
        [
</code></pre></div>

<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: A diagram depicting syntactically valid predictions during decoding at each timestep $t$. The log probabilities of all other possible predictions are set to a tiny value to prevent the model from producing a syntactically invalid target string. BOS is the special beginning-of-sequence token, COPY denotes any token copied from the source text, and COREF is the special token used to separate coreferent mentions (i.e. ;). ENTITY is any special entity token (e.g. @GENE@) and RELATION any special relation token (e.g. @GDA@ for gene-disease association). $\hat{n}<em _ents="{ents" _text="\text">{\text {ents }}$ denotes the number of entities predicted by the current timestep and $n</em>$ the expected arity of the relation. The special end-of-sequence token (not shown) is always considered valid and its log probability is never modified.}</p>
<p>Table 6: Evaluation datasets used in this paper with details about their annotations. Inter-sentence relations (\%) are the fraction of relations in the test set that cross sentence boundaries. We consider a relation intra-sentence if any sentence in the document contains at least one mention of each entity in the relation, and inter-sentence otherwise. *This differs from the estimate in Yao et al. (2019), see Appendix B.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Corpus</th>
<th style="text-align: center;">Nested Mentions?</th>
<th style="text-align: center;">Discontinuous Mentions?</th>
<th style="text-align: center;">Coreferent mentions?</th>
<th style="text-align: center;">$\alpha$-ary relations?</th>
<th style="text-align: center;">Inter-sentence relations (\%)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">CDR (Li et al., 2016b)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">29.8</td>
</tr>
<tr>
<td style="text-align: left;">GDA (Wu et al., 2019)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">15.6</td>
</tr>
<tr>
<td style="text-align: left;">DGM (Jia et al., 2019)</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">63.5</td>
</tr>
<tr>
<td style="text-align: left;">DocRED (Yao et al., 2019)</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$12.5^{*}$</td>
</tr>
</tbody>
</table>
<div class="codehilite"><pre><span></span><code>    [[&quot;monoamine oxidase b&quot;, &quot;maob&quot;], &quot;GENE&quot;],
        [[&quot;parkinson&#39;s disease&quot;, &quot;pd&quot;], &quot;DISEASE&quot;]
    ]
}
</code></pre></div>

<p>Finally, we apply some normalization steps to the entity mentions. Namely, we strip leading and trailing white space characters, sort entity mentions lexicographically (as their order is not important), and remove duplicate mentions. Similarly, we remove duplicate relations. These steps are applied to both target and model output strings. The F1-score can then be computed by tallying true positives, false positives and false negatives.</p>
<h2>E Relaxed entity matching</h2>
<p>The aim of DocRE is to extract relations at the entity-level. However, it is common to evaluate these methods with a "strict" matching criteria, where a predicted entity $\mathcal{P}$ is considered correct if and only if all its mentions exactly match a corresponding gold entities mentions, i.e. $\mathcal{P}=\mathcal{G}$. This penalizes model predictions that miss even a single coreferent mention, but are otherwise correct. A relaxed
criteria, proposed in prior work (Jain et al., 2020) considers $\mathcal{P}$ to match $\mathcal{G}$ if more than $50 \%$ of $\mathcal{P}$ 's mentions belong to $\mathcal{G}$, that is</p>
<p>$$
\frac{|\mathcal{P} \cap \mathcal{G}|}{|\mathcal{P}|}&gt;0.5
$$</p>
<p>In this paper, alongside the strict criteria, we report performance using this relaxed entity matching strategy, denoted "relaxed".</p>
<h2>F Hyperparameters</h2>
<p>In Table 7, we list the hyperparameter values used during evaluation on each corpus, with and without entity hinting. Select hyperparameters were tuned using Optuna (Akiba et al., 2019). The tuning process selects the best hyperparameters according to the validation set micro F1-score using the TPE (Tree-structured Parzen Estimator) algorithm (Bergstra et al., 2011). ${ }^{12}$ During tuning, we use greedy decoding (i.e. beam size of one). Once opti-</p>
<p><sup id="fnref8:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 7: Hyperparameter values used for each corpus. Hyperparameters values when using entity hinting, if they differ from the values used without entity hinting, are shown in parentheses. Tuned indicates whether or not the hyperparameters were tuned on the validation sets.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Hyperparameter</th>
<th style="text-align: center;">Tuned?</th>
<th style="text-align: center;">CDR</th>
<th style="text-align: center;">GDA</th>
<th style="text-align: center;">DGM</th>
<th style="text-align: center;">DocRED</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Batch size</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">4</td>
</tr>
<tr>
<td style="text-align: left;">Training epochs</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$130(70)$</td>
<td style="text-align: center;">$30(25)$</td>
<td style="text-align: center;">$30(45)$</td>
<td style="text-align: center;">50</td>
</tr>
<tr>
<td style="text-align: left;">Encoder learning rate</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$2 \mathrm{e}-5$</td>
<td style="text-align: center;">$2 \mathrm{e}-5$</td>
<td style="text-align: center;">$2 \mathrm{e}-5$</td>
<td style="text-align: center;">$2 \mathrm{e}-5$</td>
</tr>
<tr>
<td style="text-align: left;">Encoder weight decay</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">0.01</td>
<td style="text-align: center;">0.01</td>
<td style="text-align: center;">0.01</td>
<td style="text-align: center;">0.01</td>
</tr>
<tr>
<td style="text-align: left;">Encoder re-initialized top $L$ layers</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$1(2)$</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: left;">Decoder learning rate</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$1.21 \mathrm{e}-4(1.13 \mathrm{e}-4)$</td>
<td style="text-align: center;">$5 \mathrm{e}-4(4 \mathrm{e}-4)$</td>
<td style="text-align: center;">$8 \mathrm{e}-4(1.5 \mathrm{e}-5)$</td>
<td style="text-align: center;">$7.8 \mathrm{e}-5$</td>
</tr>
<tr>
<td style="text-align: left;">Decoder input dropout</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.1</td>
</tr>
<tr>
<td style="text-align: left;">Decoder hidden-to-hidden weights dropout</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">0.5</td>
</tr>
<tr>
<td style="text-align: left;">Target embedding size</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">256</td>
<td style="text-align: center;">256</td>
<td style="text-align: center;">256</td>
<td style="text-align: center;">256</td>
</tr>
<tr>
<td style="text-align: left;">No. heads in multi-head cross-attention</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">6</td>
</tr>
<tr>
<td style="text-align: left;">Beam size</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$3(2)$</td>
<td style="text-align: center;">$4(1)$</td>
<td style="text-align: center;">$3(2)$</td>
<td style="text-align: center;">8</td>
</tr>
<tr>
<td style="text-align: left;">Length penalty</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$1.4(0.2)$</td>
<td style="text-align: center;">$0.8(1.0)$</td>
<td style="text-align: center;">$0.2(0.8)$</td>
<td style="text-align: center;">1.4</td>
</tr>
<tr>
<td style="text-align: left;">Max decoding steps</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">128</td>
<td style="text-align: center;">96</td>
<td style="text-align: center;">96</td>
<td style="text-align: center;">400</td>
</tr>
</tbody>
</table>
<p>mal hyperparameters are found, we tune the beam size (bs) and length penalty $(\alpha)$ using a grid search over the values $\mathrm{bs}={2 \ldots 10}$, with a step size of 1 , and $\alpha={0.2 \ldots 2.0}$, with a step size of 0.2 .</p>
<h2>G Baselines</h2>
<p>This section contains detailed descriptions of all methods we compare to in this paper.</p>
<h2>G. 1 Pipeline-based methods</h2>
<p>These methods are pipeline-based, assuming the entities are provided as input. Many of them construct a document-level graph using dependency parsing, heuristics, or structured attention and then update node and edge representations using propagation.</p>
<ul>
<li>Christopoulou et al. (2019) propose EoG, an edge-orientated graph neural model. The nodes of the graph are constructed from mentions, entities, and sentences. Edges between nodes are initially constructed using heuristics. An iterative algorithm is then used to generate edges between nodes in the graph. Finally, a classification layer takes the representation of entity-to-entity edges as input to determine whether those entities express a relation or not. We compare to EoG in the pipeline-based setting on the CDR and GDA corpora.</li>
<li>Nan et al. (2020) propose LSR (Latent Structure Refinement). A "node constructor" encodes each sentence of an input document and outputs contextual representations. Representations that correspond to mentions and tokens on the shortest dependency path in a sentence
are extracted as nodes. A "dynamic reasoner" is then applied to induce a document-level graph based on the extracted nodes. The classifier uses the final representations of nodes for relation classification. We compare to LSR in the pipeline-based setting on the CDR and GDA corpora.</li>
<li>Lai and Lu (2021) propose BERT-GT, which combines BERT with a graph transformer. Both BERT and the graph transformer accept the document text as input, but the graph transformer requires the neighbouring positions for each token, and the self-attention mechanism is replaced with a neighbour-attention mechanism. The hidden states of the two transformers are aggregated before classification. We compare to BERT-GT in the pipeline-based setting on the CDR and GDA corpora.</li>
<li>Minh Tran et al. (2020) propose EoGANE (EoG model Augmented with Node Representations), which extends the edge-orientated model proposed by Christopoulou et al. (2019) to include explicit node representations which are used during relation classification. We compare to EoGANE in the pipeline-based setting on the CDR and GDA corpora.</li>
<li>SSAN (Xu et al., 2021) propose SSAN (Structured Self-Attention Network), which inherits the architecture of the transformer encoder (Vaswani et al., 2017) but adds a novel structured self-attention mechanism to model the coreference and co-occurrence dependencies between an entities mentions. We compare</li>
</ul>
<p>to SSAN in the pipeline-based setting on the CDR and GDA corpora.</p>
<ul>
<li>Zhou et al. (2021) propose ALTOP (Adaptive Thresholding and Localized cOntext Pooling), which extends BERT with two modifications. Adaptive thresholding, which learns an optimal threshold to apply to the relation classifier. Localized context pooling, which uses the pretrained self-attention layers of BERT to create an entity embedding from its mentions and their context. We compare to ALTOP in the pipeline-based setting on the CDR and GDA corpora.</li>
</ul>
<h1>G. 2 n-ary relation extraction</h1>
<p>These methods are explicitly designed for the extraction of $n$-ary relations, where $n&gt;2$.</p>
<ul>
<li>Jia et al. (2019) propose a multiscale neural architecture, which combines representations learned over text spans of varying scales and for various sub-relations. We compare to Jia et al. (2019) in the pipeline-based setting on the $n$-ary DGM corpus.</li>
</ul>
<h2>G. 3 End-to-end methods</h2>
<p>These methods are capable of performing the subtasks of DocRE in an end-to-end fashion with only the document text as input.</p>
<ul>
<li>Eberts and Ulges (2021) propose JEREX, which extends BERT with four task-specific components that use BERTs outputs to perform entity mention localization, coreference resolution, entity classification, and relation classification. They present two versions of their relation classifier, denoted "global relation classifier" (GRC) and "multi-instance relation classifier" (MRC). We compare to JEREX-MRC in the end-to-end setting on the DocRED corpus.</li>
</ul>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{12}$ https://optuna.readthedocs.io/en/stable/ reference/generated/optuna.samplers.TPESampler. html&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref8:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{8}$ Specifically, we use PubTator (Wei et al., 2013). PubTator provides up-to-date entity annotations for PubMed using state-of-the-art machine learning systems.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4708 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4708</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4708</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-102.html">extraction-schema-102</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <p><strong>Paper ID:</strong> paper-39238a92de090c104936a4f78375b95600e42ce5</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/39238a92de090c104936a4f78375b95600e42ce5" target="_blank">NumGLUE: A Suite of Fundamental yet Challenging Mathematical Reasoning Tasks</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> NumGLUE is proposed, a multi-task benchmark that evaluates the performance of AI systems on eight different tasks, that at their core require simple arithmetic understanding and it is shown that this benchmark is far from being solved with neural models including state-of-the-art large-scale language models performing significantly worse than humans.</p>
                <p><strong>Paper Abstract:</strong> Given the ubiquitous nature of numbers in text, reasoning with numbers to perform simple calculations is an important skill of AI systems. While many datasets and models have been developed to this end, state-of-the-art AI systems are brittle; failing to perform the underlying mathematical reasoning when they appear in a slightly different scenario. Drawing inspiration from GLUE that was proposed in the context of natural language understanding, we propose NumGLUE, a multi-task benchmark that evaluates the performance of AI systems on eight different tasks, that at their core require simple arithmetic understanding. We show that this benchmark is far from being solved with neural models including state-of-the-art large-scale language models performing significantly worse than humans (lower by 46.4 %). Further, NumGLUE promotes sharing knowledge across tasks, especially those with limited training data as evidenced by the superior performance (average gain of 3.4 % on each task) when a model is jointly trained on all the tasks as opposed to task-specific modeling. Finally, we hope that NumGLUE will encourage systems that perform robust and general arithmetic reasoning within language, a first step towards being able to perform more complex mathematical reasoning.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4708.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4708.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 3</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large autoregressive transformer language model (Brown et al., 2020) used in this paper in zero-/few-shot and fine-tuned settings; evaluated as an end-to-end model for arithmetic reasoning across NumGLUE tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language models are few-shot learners</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (including GPT3-13B fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive transformer (decoder-only) pre-trained on web-scale corpora. Multiple sizes exist; paper reports using API zero/few-shot GPT-3 and a fine-tunable 13B parameter variant (GPT3-13B) provided by OpenAI for finetuning experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Addition, subtraction, multiplication, unit arithmetic, single-step and simple multi-step arithmetic embedded in word problems, numeric span extraction, quantitative comparison</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>Per authors: leverages broad pretraining (memorized facts/patterns) and few-shot in-context learning to map textual patterns to numeric outputs; likely pattern-matching/memorization of templates and statistical mapping rather than explicit algorithmic arithmetic.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>GPT-3 (few-shot and fine-tuned) performs relatively well on small-data tasks and tasks requiring commonsense/domain knowledge (tasks 1-3), suggesting benefit from pretraining; GPT-3-13B fine-tuned multi-task achieves substantially higher scores on several tasks (e.g., Task3 71.2 F1) than zero-shot, indicating effective adaptation from pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Zero-shot and few-shot experiments show inconsistent answers across minor format changes (supplementary examples, Figures 7-11); error analysis shows 56% of GPT-3 errors are incorrect calculations and nontrivial rates of copying numbers or redundant text, indicating failure to apply reliable arithmetic algorithms.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Zero-shot GPT3 overall NumGLUE score 4.9; GPT3-Instruct zero-shot 8.1. Few-shot (task-specific) GPT3 NumGLUE 37.4. Fine-tuned GPT3-13B (multi-task) NumGLUE score 32.7. Per-task examples (GPT3 few-shot task-specific): Task1 44, Task2 42, Task3 46, Task4 40, Task5 10, Task6 42, Task7 35, Task8 40 (F1 percentages from Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>Ablations: GPT-3 performs poorly with single-part inputs; few-shot and multi-task conditioning degrade when mixed-task in-context examples are used. Error analysis: 7% invalid outputs, 3% copy-number, 56% incorrect calculation, 34% redundant text (reported for best GPT-3 task-specific model variant).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>Inconsistent across superficial format variants; frequent incorrect arithmetic (wrong operation or numeric parsing), copying numbers instead of computing, producing extraneous textual output; better at few-shot/small-data but still far below humans; brittle to task presentation and expects distributional cues.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Compared to Ex-NumNet (neuro-symbolic), GPT-3 is stronger on small datasets and tasks needing external commonsense knowledge (tasks 1-3), while Ex-NumNet outperforms on large-data tasks (e.g., DROP-based tasks). Fine-tuned GPT3-13B outperforms other baselines on tasks 1-3 but not uniformly across all tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NumGLUE: A Suite of Fundamental yet Challenging Mathematical Reasoning Tasks', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4708.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4708.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT3-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3 (instruct-tuned variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An instruction-tuned variant of GPT-3 (released by OpenAI) evaluated in zero- and few-shot modes; reported to produce inconsistent basic arithmetic answers across format variants in the paper's supplement.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT3-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A fine-tuned/instruction-following variant of GPT-3 provided by OpenAI (no formal paper cited in the text); used in zero-shot and few-shot experiments with default API parameters and tuned prompt settings.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Same as GPT-3: addition, subtraction, multiplication, unit arithmetic, simple multi-step arithmetic in word problems and format variants (NLI, fill-in-the-blank, comparison, RC).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>Similar to GPT-3: relies on pretraining plus instruction-tuning to better follow prompts, but still performs arithmetic via pattern matching/statistical mapping rather than reliable algorithmic computation.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>GPT3-Instruct improves over raw GPT-3 in some zero-shot/few-shot settings (e.g., zero-shot Task6 = 29 vs GPT3 zero-shot Task6 = 17), indicating instruction tuning helps prompt-following and extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Supplementary examples (Figures 7-11) show GPT3-Instruct 'could not solve basic arithmetic questions reliably' even with parameter tuning; error distribution and inconsistent outputs across simple format changes highlight lack of robust arithmetic algorithm.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Zero-shot GPT3-Instruct NumGLUE score 8.1. Few-shot task-specific GPT3-Instruct NumGLUE 35.9. Per-task zero/few-shot numbers in Table 2 (e.g., few-shot task-specific: Task1 40, Task2 39, Task3 51, Task4 33...).</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>Authors tuned temperature, stop conditions, penalties, and prompt engineering but still observed unreliable arithmetic; averaged few-shot results over 5 runs to estimate variance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>Same brittle behaviors as GPT-3: inconsistent answers when problem phrasing changes, incorrect numerical computation, copying numbers, and producing extraneous text; instruction tuning reduces some but does not eliminate these failures.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>GPT3-Instruct often slightly outperforms vanilla GPT-3 in zero-shot/few-shot but remains below fine-tuned GPT3-13B and the best Ex-NumNet+CIR multi-task setting on aggregate NumGLUE score.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NumGLUE: A Suite of Fundamental yet Challenging Mathematical Reasoning Tasks', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4708.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4708.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Ex-NumNet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Extended NumNet (memory-augmented neuro-symbolic model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neuro-symbolic, memory-augmented architecture extending NumNet+v2 that converts tasks to a reading-comprehension format and (optionally) retrieves numeric commonsense/domain facts from a MATH KB to assist arithmetic reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Ex-NumNet (with variants: multi-task, multi-task+IR, multi-task+CIR, single-task, Q-only, C-only, oversampling)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Neuro-symbolic model built on NumNet+v2 ideas, uses a reading-comprehension converter, a heuristic task detector, what the authors call a 'math KB' retrieval module (IR/CIR) to append facts/formulae, and a memory-augmented reasoning component to perform numeric operations; trained and evaluated in task-specific and multi-task regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Same set of arithmetic tasks in NumGLUE (single-step arithmetic, unit conversions, multiplication, subtraction, comparison, fill-in-the-blank, RC numeric extraction).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>Per design: combines symbolic-style explicit numeric reasoning (NumNet-style numerical reasoning modules) with retrieved factual knowledge; arithmetic performed by explicit neural modules that reason over parsed quantities and small symbolic-like operations rather than pure surface pattern matching.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Multi-task training improved Ex-NumNet average by ~3.4% over single-task, and the addition of conditional IR (CIR) that supplies relevant facts improved performance on tasks requiring external knowledge (tasks 1,2,4), supporting the role of explicit knowledge injection combined with neuro-symbolic reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Error analysis shows Ex-NumNet still commits high rates of incorrect calculation (71% of examined errors) and produces invalid outputs (16%), indicating that the neuro-symbolic pipeline and parsing still fail frequently; IR sometimes harms performance if irrelevant info is retrieved (task 3 drop unless made conditional).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Single-task Ex-NumNet NumGLUE score 43.4. Multi-task Ex-NumNet 46.8. Multi-task + IR 46.6. Multi-task + CIR 48.8 (best Ex-NumNet variant). Per-task example (Multi-task+CIR): Task1 7.4, Task2 38.8, Task3 58.0, Task4 36.8, Task5 69.2, Task6 70.8, Task7 85.8, Task8 23.6 (F1 percentages from Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>Ablations: Q-only and C-only finetuning performed poorly (Q-only 13.3 NumGLUE, C-only 8.8), showing the model requires both question and context; adding IR and then conditioning IR (CIR) selectively improved tasks needing external facts; oversampling improved low-resource tasks but hurt high-resource ones.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>High rate of incorrect calculation (dominant error source), invalid outputs (answer type mismatches), copying numbers from question instead of computing, sensitivity to irrelevant retrieved knowledge (IR must be conditional), poorer performance on small-data tasks relative to GPT-3.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Outperforms GPT-3 few-shot in multi-task settings on large-data tasks (e.g., DROP-related tasks) and achieves best overall among neuro-symbolic baselines; when combined with CIR it attains the paper's strongest non-GPT3 baseline (NumGLUE 48.8) but is still well below human performance (~95 F1).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NumGLUE: A Suite of Fundamental yet Challenging Mathematical Reasoning Tasks', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4708.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4708.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NumNet+v2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NumNet (version 2) numerical reasoning architecture</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior neuro-symbolic architecture for numeric reasoning in reading comprehension that Ex-NumNet extends; used as a foundation for explicit numerical reasoning modules in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Numnet: Machine reading comprehension with numerical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>NumNet+v2 (referenced baseline architecture)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Neuro-symbolic model that represents numbers and performs discrete operations to answer reading-comprehension style numerical questions; the paper extends this core with memory augmentation and IR.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Numeric reasoning for reading comprehension: arithmetic operations on quantities mentioned in text (addition, subtraction, comparison, unit handling).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>Operates by building explicit numeric representations and relational graphs over quantities, enabling symbolic-like operations instead of purely distributional mapping.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Authors build Ex-NumNet by extending NumNet+v2 and observe that neuro-symbolic approaches (NumNet family) perform well on larger, many-shot datasets in NumGLUE, indicating that explicit numeric representation aids scaling.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Despite explicit numeric components, Ex-NumNet (NumNet-based) still makes many calculation errors and produces invalid outputs, indicating that practical instantiations still struggle with robust arithmetic reasoning and parsing.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not directly evaluated as a standalone baseline in the paper; Ex-NumNet (the extension) results demonstrate strengths and limitations inherited from NumNet family.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>The paper's ablations and variants (Q-only/C-only/IR/CIR/oversampling) are applied to Ex-NumNet, the NumNet+v2 extension, revealing sensitivity to retrieval and multi-task training.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>Parsing errors, incorrect operation selection, numeric representation mismatches, and generalization to varied linguistic formats remain weaknesses even with explicit numeric modules.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>NumNet-style models (Ex-NumNet) outperform GPT-3 on large-data tasks but underperform GPT-3 on small-data, knowledge-heavy tasks; combining neuro-symbolic and large pretrained LMs is suggested as a promising direction.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NumGLUE: A Suite of Fundamental yet Challenging Mathematical Reasoning Tasks', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4708.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4708.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MATH KB</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MATH Knowledge Base (authors' plain-text facts and formula store)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A curated plain-text knowledge base of numerical commonsense facts and mathematical formulae created by the authors (e.g., 'a human has 2 hands', 'distance = speed * time') used by Ex-NumNet's IR module to supply external facts during reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MATH KB (retrieval store used by Ex-NumNet IR/CIR)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A small, plain-text collection of factual numerical knowledge and formulas indexed with Elasticsearch; used to retrieve up to v statements per question, filtered by semantic-textual-similarity thresholds to avoid redundancy.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Supplies facts needed for arithmetic reasoning: unit facts, object property counts, chemical stoichiometry facts, simple formulae for speed/distance/time, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>Providing explicit external numeric facts allows the neuro-symbolic model to combine factual knowledge with explicit numerical computation, reducing failures caused by missing commonsense/domain facts.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Adding IR from MATH KB improves performance on tasks that require external knowledge (tasks 1,2,4). Conditional IR (CIR), which retrieves only for tasks that need it, gives the best Ex-NumNet results (multi-task+CIR NumGLUE 48.8).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Retrieving unnecessary or irrelevant facts can hurt performance (task 3 score drops when IR applied indiscriminately), showing that retrieval must be targeted and that adding text alone does not guarantee improved arithmetic reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Ex-NumNet Multi-task baseline 46.8 vs Multi-task+IR 46.6 vs Multi-task+CIR 48.8 (NumGLUE aggregated F1 scores), illustrating conditional retrieval benefit.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>Authors experiment with IR hyperparameters (Z=50, v=10, th=0.75, b=0.1) and a STS-based redundancy filter; conditional retrieval yields best trade-off between helpfulness and distractor noise.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>KB must be curated and small; IR adds noise if applied to tasks that do not need it; the KB approach does not fix core calculation/parsing errorsâ€”only addresses missing factual knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NumGLUE: A Suite of Fundamental yet Challenging Mathematical Reasoning Tasks', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language models are few-shot learners <em>(Rating: 2)</em></li>
                <li>Numnet: Machine reading comprehension with numerical reasoning <em>(Rating: 2)</em></li>
                <li>Analysing mathematical reasoning abilities of neural models <em>(Rating: 2)</em></li>
                <li>Measuring mathematical problem solving with the math dataset <em>(Rating: 1)</em></li>
                <li>DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning over Paragraphs <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4708",
    "paper_id": "paper-39238a92de090c104936a4f78375b95600e42ce5",
    "extraction_schema_id": "extraction-schema-102",
    "extracted_data": [
        {
            "name_short": "GPT-3",
            "name_full": "Generative Pre-trained Transformer 3",
            "brief_description": "A large autoregressive transformer language model (Brown et al., 2020) used in this paper in zero-/few-shot and fine-tuned settings; evaluated as an end-to-end model for arithmetic reasoning across NumGLUE tasks.",
            "citation_title": "Language models are few-shot learners",
            "mention_or_use": "use",
            "model_name": "GPT-3 (including GPT3-13B fine-tuned)",
            "model_description": "Autoregressive transformer (decoder-only) pre-trained on web-scale corpora. Multiple sizes exist; paper reports using API zero/few-shot GPT-3 and a fine-tunable 13B parameter variant (GPT3-13B) provided by OpenAI for finetuning experiments.",
            "arithmetic_task_type": "Addition, subtraction, multiplication, unit arithmetic, single-step and simple multi-step arithmetic embedded in word problems, numeric span extraction, quantitative comparison",
            "mechanism_hypothesis": "Per authors: leverages broad pretraining (memorized facts/patterns) and few-shot in-context learning to map textual patterns to numeric outputs; likely pattern-matching/memorization of templates and statistical mapping rather than explicit algorithmic arithmetic.",
            "evidence_for_mechanism": "GPT-3 (few-shot and fine-tuned) performs relatively well on small-data tasks and tasks requiring commonsense/domain knowledge (tasks 1-3), suggesting benefit from pretraining; GPT-3-13B fine-tuned multi-task achieves substantially higher scores on several tasks (e.g., Task3 71.2 F1) than zero-shot, indicating effective adaptation from pretraining.",
            "evidence_against_mechanism": "Zero-shot and few-shot experiments show inconsistent answers across minor format changes (supplementary examples, Figures 7-11); error analysis shows 56% of GPT-3 errors are incorrect calculations and nontrivial rates of copying numbers or redundant text, indicating failure to apply reliable arithmetic algorithms.",
            "performance_metrics": "Zero-shot GPT3 overall NumGLUE score 4.9; GPT3-Instruct zero-shot 8.1. Few-shot (task-specific) GPT3 NumGLUE 37.4. Fine-tuned GPT3-13B (multi-task) NumGLUE score 32.7. Per-task examples (GPT3 few-shot task-specific): Task1 44, Task2 42, Task3 46, Task4 40, Task5 10, Task6 42, Task7 35, Task8 40 (F1 percentages from Table 2).",
            "probing_or_intervention_results": "Ablations: GPT-3 performs poorly with single-part inputs; few-shot and multi-task conditioning degrade when mixed-task in-context examples are used. Error analysis: 7% invalid outputs, 3% copy-number, 56% incorrect calculation, 34% redundant text (reported for best GPT-3 task-specific model variant).",
            "limitations_and_failure_modes": "Inconsistent across superficial format variants; frequent incorrect arithmetic (wrong operation or numeric parsing), copying numbers instead of computing, producing extraneous textual output; better at few-shot/small-data but still far below humans; brittle to task presentation and expects distributional cues.",
            "comparison_to_other_models": "Compared to Ex-NumNet (neuro-symbolic), GPT-3 is stronger on small datasets and tasks needing external commonsense knowledge (tasks 1-3), while Ex-NumNet outperforms on large-data tasks (e.g., DROP-based tasks). Fine-tuned GPT3-13B outperforms other baselines on tasks 1-3 but not uniformly across all tasks.",
            "uuid": "e4708.0",
            "source_info": {
                "paper_title": "NumGLUE: A Suite of Fundamental yet Challenging Mathematical Reasoning Tasks",
                "publication_date_yy_mm": "2022-04"
            }
        },
        {
            "name_short": "GPT3-Instruct",
            "name_full": "GPT-3 (instruct-tuned variant)",
            "brief_description": "An instruction-tuned variant of GPT-3 (released by OpenAI) evaluated in zero- and few-shot modes; reported to produce inconsistent basic arithmetic answers across format variants in the paper's supplement.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT3-Instruct",
            "model_description": "A fine-tuned/instruction-following variant of GPT-3 provided by OpenAI (no formal paper cited in the text); used in zero-shot and few-shot experiments with default API parameters and tuned prompt settings.",
            "arithmetic_task_type": "Same as GPT-3: addition, subtraction, multiplication, unit arithmetic, simple multi-step arithmetic in word problems and format variants (NLI, fill-in-the-blank, comparison, RC).",
            "mechanism_hypothesis": "Similar to GPT-3: relies on pretraining plus instruction-tuning to better follow prompts, but still performs arithmetic via pattern matching/statistical mapping rather than reliable algorithmic computation.",
            "evidence_for_mechanism": "GPT3-Instruct improves over raw GPT-3 in some zero-shot/few-shot settings (e.g., zero-shot Task6 = 29 vs GPT3 zero-shot Task6 = 17), indicating instruction tuning helps prompt-following and extraction.",
            "evidence_against_mechanism": "Supplementary examples (Figures 7-11) show GPT3-Instruct 'could not solve basic arithmetic questions reliably' even with parameter tuning; error distribution and inconsistent outputs across simple format changes highlight lack of robust arithmetic algorithm.",
            "performance_metrics": "Zero-shot GPT3-Instruct NumGLUE score 8.1. Few-shot task-specific GPT3-Instruct NumGLUE 35.9. Per-task zero/few-shot numbers in Table 2 (e.g., few-shot task-specific: Task1 40, Task2 39, Task3 51, Task4 33...).",
            "probing_or_intervention_results": "Authors tuned temperature, stop conditions, penalties, and prompt engineering but still observed unreliable arithmetic; averaged few-shot results over 5 runs to estimate variance.",
            "limitations_and_failure_modes": "Same brittle behaviors as GPT-3: inconsistent answers when problem phrasing changes, incorrect numerical computation, copying numbers, and producing extraneous text; instruction tuning reduces some but does not eliminate these failures.",
            "comparison_to_other_models": "GPT3-Instruct often slightly outperforms vanilla GPT-3 in zero-shot/few-shot but remains below fine-tuned GPT3-13B and the best Ex-NumNet+CIR multi-task setting on aggregate NumGLUE score.",
            "uuid": "e4708.1",
            "source_info": {
                "paper_title": "NumGLUE: A Suite of Fundamental yet Challenging Mathematical Reasoning Tasks",
                "publication_date_yy_mm": "2022-04"
            }
        },
        {
            "name_short": "Ex-NumNet",
            "name_full": "Extended NumNet (memory-augmented neuro-symbolic model)",
            "brief_description": "A neuro-symbolic, memory-augmented architecture extending NumNet+v2 that converts tasks to a reading-comprehension format and (optionally) retrieves numeric commonsense/domain facts from a MATH KB to assist arithmetic reasoning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Ex-NumNet (with variants: multi-task, multi-task+IR, multi-task+CIR, single-task, Q-only, C-only, oversampling)",
            "model_description": "Neuro-symbolic model built on NumNet+v2 ideas, uses a reading-comprehension converter, a heuristic task detector, what the authors call a 'math KB' retrieval module (IR/CIR) to append facts/formulae, and a memory-augmented reasoning component to perform numeric operations; trained and evaluated in task-specific and multi-task regimes.",
            "arithmetic_task_type": "Same set of arithmetic tasks in NumGLUE (single-step arithmetic, unit conversions, multiplication, subtraction, comparison, fill-in-the-blank, RC numeric extraction).",
            "mechanism_hypothesis": "Per design: combines symbolic-style explicit numeric reasoning (NumNet-style numerical reasoning modules) with retrieved factual knowledge; arithmetic performed by explicit neural modules that reason over parsed quantities and small symbolic-like operations rather than pure surface pattern matching.",
            "evidence_for_mechanism": "Multi-task training improved Ex-NumNet average by ~3.4% over single-task, and the addition of conditional IR (CIR) that supplies relevant facts improved performance on tasks requiring external knowledge (tasks 1,2,4), supporting the role of explicit knowledge injection combined with neuro-symbolic reasoning.",
            "evidence_against_mechanism": "Error analysis shows Ex-NumNet still commits high rates of incorrect calculation (71% of examined errors) and produces invalid outputs (16%), indicating that the neuro-symbolic pipeline and parsing still fail frequently; IR sometimes harms performance if irrelevant info is retrieved (task 3 drop unless made conditional).",
            "performance_metrics": "Single-task Ex-NumNet NumGLUE score 43.4. Multi-task Ex-NumNet 46.8. Multi-task + IR 46.6. Multi-task + CIR 48.8 (best Ex-NumNet variant). Per-task example (Multi-task+CIR): Task1 7.4, Task2 38.8, Task3 58.0, Task4 36.8, Task5 69.2, Task6 70.8, Task7 85.8, Task8 23.6 (F1 percentages from Table 2).",
            "probing_or_intervention_results": "Ablations: Q-only and C-only finetuning performed poorly (Q-only 13.3 NumGLUE, C-only 8.8), showing the model requires both question and context; adding IR and then conditioning IR (CIR) selectively improved tasks needing external facts; oversampling improved low-resource tasks but hurt high-resource ones.",
            "limitations_and_failure_modes": "High rate of incorrect calculation (dominant error source), invalid outputs (answer type mismatches), copying numbers from question instead of computing, sensitivity to irrelevant retrieved knowledge (IR must be conditional), poorer performance on small-data tasks relative to GPT-3.",
            "comparison_to_other_models": "Outperforms GPT-3 few-shot in multi-task settings on large-data tasks (e.g., DROP-related tasks) and achieves best overall among neuro-symbolic baselines; when combined with CIR it attains the paper's strongest non-GPT3 baseline (NumGLUE 48.8) but is still well below human performance (~95 F1).",
            "uuid": "e4708.2",
            "source_info": {
                "paper_title": "NumGLUE: A Suite of Fundamental yet Challenging Mathematical Reasoning Tasks",
                "publication_date_yy_mm": "2022-04"
            }
        },
        {
            "name_short": "NumNet+v2",
            "name_full": "NumNet (version 2) numerical reasoning architecture",
            "brief_description": "A prior neuro-symbolic architecture for numeric reasoning in reading comprehension that Ex-NumNet extends; used as a foundation for explicit numerical reasoning modules in this work.",
            "citation_title": "Numnet: Machine reading comprehension with numerical reasoning",
            "mention_or_use": "mention",
            "model_name": "NumNet+v2 (referenced baseline architecture)",
            "model_description": "Neuro-symbolic model that represents numbers and performs discrete operations to answer reading-comprehension style numerical questions; the paper extends this core with memory augmentation and IR.",
            "arithmetic_task_type": "Numeric reasoning for reading comprehension: arithmetic operations on quantities mentioned in text (addition, subtraction, comparison, unit handling).",
            "mechanism_hypothesis": "Operates by building explicit numeric representations and relational graphs over quantities, enabling symbolic-like operations instead of purely distributional mapping.",
            "evidence_for_mechanism": "Authors build Ex-NumNet by extending NumNet+v2 and observe that neuro-symbolic approaches (NumNet family) perform well on larger, many-shot datasets in NumGLUE, indicating that explicit numeric representation aids scaling.",
            "evidence_against_mechanism": "Despite explicit numeric components, Ex-NumNet (NumNet-based) still makes many calculation errors and produces invalid outputs, indicating that practical instantiations still struggle with robust arithmetic reasoning and parsing.",
            "performance_metrics": "Not directly evaluated as a standalone baseline in the paper; Ex-NumNet (the extension) results demonstrate strengths and limitations inherited from NumNet family.",
            "probing_or_intervention_results": "The paper's ablations and variants (Q-only/C-only/IR/CIR/oversampling) are applied to Ex-NumNet, the NumNet+v2 extension, revealing sensitivity to retrieval and multi-task training.",
            "limitations_and_failure_modes": "Parsing errors, incorrect operation selection, numeric representation mismatches, and generalization to varied linguistic formats remain weaknesses even with explicit numeric modules.",
            "comparison_to_other_models": "NumNet-style models (Ex-NumNet) outperform GPT-3 on large-data tasks but underperform GPT-3 on small-data, knowledge-heavy tasks; combining neuro-symbolic and large pretrained LMs is suggested as a promising direction.",
            "uuid": "e4708.3",
            "source_info": {
                "paper_title": "NumGLUE: A Suite of Fundamental yet Challenging Mathematical Reasoning Tasks",
                "publication_date_yy_mm": "2022-04"
            }
        },
        {
            "name_short": "MATH KB",
            "name_full": "MATH Knowledge Base (authors' plain-text facts and formula store)",
            "brief_description": "A curated plain-text knowledge base of numerical commonsense facts and mathematical formulae created by the authors (e.g., 'a human has 2 hands', 'distance = speed * time') used by Ex-NumNet's IR module to supply external facts during reasoning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "MATH KB (retrieval store used by Ex-NumNet IR/CIR)",
            "model_description": "A small, plain-text collection of factual numerical knowledge and formulas indexed with Elasticsearch; used to retrieve up to v statements per question, filtered by semantic-textual-similarity thresholds to avoid redundancy.",
            "arithmetic_task_type": "Supplies facts needed for arithmetic reasoning: unit facts, object property counts, chemical stoichiometry facts, simple formulae for speed/distance/time, etc.",
            "mechanism_hypothesis": "Providing explicit external numeric facts allows the neuro-symbolic model to combine factual knowledge with explicit numerical computation, reducing failures caused by missing commonsense/domain facts.",
            "evidence_for_mechanism": "Adding IR from MATH KB improves performance on tasks that require external knowledge (tasks 1,2,4). Conditional IR (CIR), which retrieves only for tasks that need it, gives the best Ex-NumNet results (multi-task+CIR NumGLUE 48.8).",
            "evidence_against_mechanism": "Retrieving unnecessary or irrelevant facts can hurt performance (task 3 score drops when IR applied indiscriminately), showing that retrieval must be targeted and that adding text alone does not guarantee improved arithmetic reasoning.",
            "performance_metrics": "Ex-NumNet Multi-task baseline 46.8 vs Multi-task+IR 46.6 vs Multi-task+CIR 48.8 (NumGLUE aggregated F1 scores), illustrating conditional retrieval benefit.",
            "probing_or_intervention_results": "Authors experiment with IR hyperparameters (Z=50, v=10, th=0.75, b=0.1) and a STS-based redundancy filter; conditional retrieval yields best trade-off between helpfulness and distractor noise.",
            "limitations_and_failure_modes": "KB must be curated and small; IR adds noise if applied to tasks that do not need it; the KB approach does not fix core calculation/parsing errorsâ€”only addresses missing factual knowledge.",
            "uuid": "e4708.4",
            "source_info": {
                "paper_title": "NumGLUE: A Suite of Fundamental yet Challenging Mathematical Reasoning Tasks",
                "publication_date_yy_mm": "2022-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 2
        },
        {
            "paper_title": "Numnet: Machine reading comprehension with numerical reasoning",
            "rating": 2
        },
        {
            "paper_title": "Analysing mathematical reasoning abilities of neural models",
            "rating": 2
        },
        {
            "paper_title": "Measuring mathematical problem solving with the math dataset",
            "rating": 1
        },
        {
            "paper_title": "DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning over Paragraphs",
            "rating": 2
        }
    ],
    "cost": 0.013393249999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>NUMGLUE: A Suite of Fundamental yet Challenging Mathematical Reasoning Tasks</h1>
<p>Swaroop Mishra ${ }^{1}$ Arindam Mitra ${ }^{2}$ Neeraj Varshney ${ }^{1}$ Bhavdeep Sachdeva ${ }^{1}$ Peter Clark ${ }^{3}$ Chitta Baral ${ }^{1}$ Ashwin Kalyan ${ }^{3}$<br>${ }^{1}$ Arizona State University ${ }^{2}$ Microsoft Research ${ }^{3}$ Allen Institute for AI</p>
<h4>Abstract</h4>
<p>Given the ubiquitous nature of numbers in text, reasoning with numbers to perform simple calculations is an important skill of AI systems. While many datasets and models have been developed to this end, state-of-the-art AI systems are brittle; failing to perform the underlying mathematical reasoning when they appear in a slightly different scenario. Drawing inspiration from GLUE (Wang et al., 2018) that was proposed in the context of natural language understanding, we propose NUMGLUE, a multi-task benchmark that evaluates the performance of AI systems on eight different tasks, that at their core require simple arithmetic understanding. We show that this benchmark is far from being solved with neural models including state-of-the-art large-scale language models performing significantly worse than humans (lower by $46.4 \%$ ). Further, NUMGLUE promotes sharing knowledge across tasks, especially those with limited training data as evidenced by the superior performance (average gain of $3.4 \%$ on each task) when a model is jointly trained on all the tasks as opposed to taskspecific modeling. Finally, we hope that NUMGLUE will encourage systems that perform robust and general arithmetic reasoning within language, a first step towards being able to perform more complex mathematical reasoning ${ }^{1}$.</p>
<h2>1 Introduction</h2>
<p>Reasoning with numbers is an important skill that occurs in various day-to-day scenarios and not surprisingly, numbers are ubiquitous in textual data. To train AI reasoning systems that can perform simple mathematical reasoning, many tasks have been proposed (Dua et al., 2019b; Ravichander et al., 2019; Koncel-Kedziorski et al., 2016). Despite these efforts, current state-of-the-art AI</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>Original Word Problem</h2>
<p>John had 5 apples. He gave 3 to Peter. How many apples does John have now?</p>
<h2>Fill In The Blanks Format</h2>
<p>John had 5 apples. He gave 3 to Peter. John has
$\qquad$ apples now.</p>
<h2>NLI Format</h2>
<p>Premise: John had 5 apples. He gave 3 apples to Peter. Hypothesis: John has 2 apples now. Does the hypothesis entail, contradict or is neutral to the premise?</p>
<h2>Comparison Format</h2>
<p>John had 5 apples. He gave 3 to Peter. Who has more apples?</p>
<p>Figure 1: A system that can robustly perform numeric reasoning over language should be able to solve problems such as the above, regardless of how the problem is posed. However, we observe existing systems are brittle; producing inconsistent solutions to such minor stylistic variations.
systems are brittle and fail when problems involving similar mathematical reasoning is posed in a slightly different manner. For instance, presenting a word problem in a different manner as shown in fig. 1, while hardly affecting human performance, is sufficient to confuse state-of-the-art AI systems ${ }^{2}$. This brittleness in reasoning indicates that the models latch on to spurious signals in the specific dataset resulting in "solving" the dataset while not truly understanding the underlying reasoning skill of simple arithmetic. Further, we believe that building AI systems that can truly understand and apply simple arithmetic reasoning is a mandatory first step towards successfully tackling complex</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>mathematical reasoning skills (Saxton et al., 2019; Hendrycks et al., 2020, 2021).</p>
<p>NumGLUE. To this end, we propose NumGLUE, a multi-task benchmark consisting of eight different tasks that at their core test for arithmetic reasoning skills. For example, as discussed in fig. 1, tasks can involve word problems presented in a slightly different manner or can involve additional reasoning strategies like commonsense reasoning or reading comprehension to be combined with the core skill of simple arithmetic. Our benchmark consists of four new tasks in addition to four existing ones; with $\sim 100 K$ problems spread across eight differet tasks. The motivation behind NumGLUE is similar to GLUE (Wang et al., 2018, 2019), a multi-task benchmark that aimed at models that demonstrated superior language understanding by learning the underlying linguistic features. NumGLUE is designed with goal of progressing towards AI systems that are capable of performing arithmetic reasoning in a general setting; achieving superior performance on our benchmark requires the ability to correctly identify and perform the underlying arithmetic reasoning without relying on task or dataset-specific signals. Finally, we hope that NumGLUE will encourage systems that perform robust and general numeric reasoning within language, a first step towards being able to perform more complex mathematical reasoning.</p>
<h2>Contributions.</h2>
<ol>
<li>We introduce NumGLUE- a multi-task benchmark consisting of eight different tasks, including 4 new ones, whose solution at its core requires an understanding of simple arithmetic.</li>
<li>We demonstrate that NumGLUE is a challenging benchmark even for state-of-the-art large scale language models, obtaining poor scores not only in zero or few shot settings but also after fine-tuning. This indicates a fundamental barrier for AI systems; one that needs to be breached before complex mathematical challenges can be successfully tackled.</li>
<li>Finally, we propose a memory-augmented neural model to demonstrate the utility of such a multi-task meta dataset. Our proposed model when trained on the entirety of NumGLUE obtains an average improvement of $3.4 \%$ on each task as opposed to task-specific training - in-
dicating that joint training leads to beneficial transfer owing to the common theme of arithmetic reasoning.</li>
</ol>
<h2>2 Related Work</h2>
<p>Datasets for Numerical reasoning. Quantitative reasoning has been a challenging problem for a long time. Small question answering datasets were proposed to understand the quantitative aspect of natural language such as the template-based dataset which solved questions with equations as parameters (Kushman et al., 2014), additionsubtraction dataset (Hosseini et al., 2014) and arithmetic problems dataset (Koncel-Kedziorski et al., 2015). Difficulty of questions were increased in subsequent datasets (Roy and Roth, 2016), (Upadhyay et al., 2016). Later, larger datasets were created to facilitate deep learning research (Ling et al., 2017; Dua et al., 2019b). Several other maths datasets have been proposed to improve explainability (Amini et al., 2019), diversity (Miao et al., 2020), scale information in language embeddings (Zhang et al.) and hardness of math questions (Hendrycks et al., 2021).</p>
<p>One of the motivations behind creating this benchmark is to test for simple arithmetic reasoning independent of the context or the presentation style of the problem. Further, To the best of our knowledge, our work is the first to consider multiple tasks in the numerical reasoning space.</p>
<p>Multi-Task Benchmarks. With increased success of deep learning based models on individual tasks, there has been a significant push both in the NLP community and in the broader AI community towards general purpose models that excel at multiple tasks. Naturally, various benchmarks and challenges that test for such understanding have been proposed. For instance, the BAbI dataset (Weston et al., 2015), GLUE (Wang et al., 2019) and the subsequent harder SuperGLUE (Wang et al., 2019) were proposed to both evaluate and drive progress in language understanding via shared linguistic knowledge across tasks. McCann (McCann et al., 2018) build a multi-task dataset via a novel approach - formatting each task as that of question-answering. In the more restricted setting of reading comprehension, Dua et al. (2019a) and Downey and Rumshisky build a meta-dataset that spans multiple domains</p>
<p>and reasoning skills.</p>
<p>Multi-task Models. With the growing interest towards models that go beyond specific datasets, various neural models that can perform mutliple tasks have been proposed. When the underlying reasoning is similar - eg. commonsense reasoning, problem decomposition or linguistic understanding - it has been found that training on multi-task datasets yields more robust and accurate models. For instance, the Multi-task Question Answering Network (McCann et al., 2018), T5 (Raffel et al., 2019), GPT3 (Brown et al., 2020) and GPT3-Instruct models aim to build general purpose language models that are capable of transferring linguistic understanding across tasks. A similar approach is taken by Khashabi et al. (2020) in the setting of question-answering and Lourie et al. (2021) in the scope of commonsense reasoning. Further, Muppet (Aghajanyan et al., 2021) adds an additional step of pre-finetuning between pretraining and finetuning that improves generalization to multiple tasks.</p>
<h2>3 NUMGLUE</h2>
<p>As mentioned previously, our NUMGLUE benchmark consists of both new and already existing arithmetic reasoning tasks. We first begin by introducing the novel datasets curated by us before providing a brief overview of existing tasks that are part of NumGLUE. Finally, in this section, we provide an analysis of the datasets demonstrating that it contains interesting and diverse linguistic and mathematical properties.</p>
<p>NumGLUE Benchmark. Our proposed NumGLUE benchmark is a collection of eight different tasks that together include $\sim 100 \mathrm{~K}$ questions. The tasks may either be self-contained or require additional background knowledge (e.g.commonsense reasoning) to arrive at the final solution; however, all the tasks, at their core, involve arithmetic reasoning. Table 1 shows an example question belonging to each task along with indicating the total number of data points associated with each task. It is important to note that tasks are imbalanced with only $\sim 400$ examples for Task 1 and nearly $50 K$ questions under Task 5. While we could have under-sampled the questions to create a balanced suite, we retain the imbalanced dataset in order
to mimic the real world - for instance, arithmetic word problems are more abundant as opposed to word problems that may require commonsense reasoning in addition to arithmetic reasoning.</p>
<p>Data Partition and Evaluation. We randomly partition data in each task into training (70\%), development ( $10 \%$ ) and test ( $20 \%$ ) sets. In the case of reading comprehension tasks (Task 5 and 6 ), we assign all questions corresponding to a passage to the same split - we do this in order to discourage any data leakage and thereby, allowing models to potentially rely on memorization to arrive at the correct answer.</p>
<p>For each task, we report the F1 measure and as an aggregate measure of performance on the NUMGLUE benchmark similar to Dua et al. (2019b), we report the (unweighted) average of the F1 scores corresponding to each task.</p>
<h3>3.1 Novel Datasets</h3>
<p>The novel tasks proposed as part of NumGLUE are a combination of both freshly collected data and intelligent modifications of already existing datasets. The four novel arithmetic reasoning tasks introduced are as follows ${ }^{3}$ :</p>
<p>Task 1: Commonsense + Arithmetic Reasoning. Consider the following question - How many faces do 10 dice have? Answering this not only requires simple arithmetic i.e.multiplying the number of faces in a die by ten but also requires knowing that a standard die has six faces. We collect this dataset by first asking the annotator to write down a numerical commonsense fact (e.g.a human has 2 hands, a day has 24 hours etc.) and then use frame a question that requires using this numerical fact as part of a simple arithmetic calculation.</p>
<p>Task 2: Domain Specific + Arithmetic Reasoning. How many units of hydrogen are required to produce 10 units of water? This question, similar to the previously introduced task of arithmetic reasoning questions, requires additional domain-specific knowledge - specifically, that each unit of water contains two units of hydrogen. We</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Question Setting</th>
<th style="text-align: center;">Size</th>
<th style="text-align: center;">Example</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">TASK 1</td>
<td style="text-align: center;">Commonsense + Arithmetic</td>
<td style="text-align: center;">404</td>
<td style="text-align: center;">Question: A man can lift one box in each of his hands. How many boxes can a group of 5 people hold in total? Answer: 10</td>
</tr>
<tr>
<td style="text-align: center;">TASK 2</td>
<td style="text-align: center;">Domain specific + Arithmetic</td>
<td style="text-align: center;">1620</td>
<td style="text-align: center;">Question: How many units of $H_{2}$ are required to react with 2 units of $C_{2} H_{4}$ to form 2 units of $C_{2} H_{6}$ ? Answer: 2</td>
</tr>
<tr>
<td style="text-align: center;">TASK 3</td>
<td style="text-align: center;">Commonsense + Quantitative</td>
<td style="text-align: center;">807</td>
<td style="text-align: center;">Question: A person wants to get shopping done quickly. They know that they can get through the check-out at big store in 5 minutes whereas it can take 20 minutes at small store. The store they go to finish quickly is? (A) big store (B) small store? Answer: big store</td>
</tr>
<tr>
<td style="text-align: center;">TASK 4</td>
<td style="text-align: center;">Fill-in-the-blanks</td>
<td style="text-align: center;">1100</td>
<td style="text-align: center;">Question: Joan found 70 seashells on the beach. She gave Sam some of her seashells. She has 27 seasshells left. She gave $\qquad$ seashells to Sam? Answer: 43</td>
</tr>
<tr>
<td style="text-align: center;">TASK 5</td>
<td style="text-align: center;">RC + Explicit Numerical Reasoning</td>
<td style="text-align: center;">54212</td>
<td style="text-align: center;">Passage: $&lt;&gt;. \text { Question: How many counties were added in } 1887 ?$ Answer: 2</td>
</tr>
<tr>
<td style="text-align: center;">TASK 6</td>
<td style="text-align: center;">RC + Implicit Numerical Reasoning</td>
<td style="text-align: center;">32724</td>
<td style="text-align: center;">Passage: $&lt;&gt;. \text { Question: Which player kicked the shortest field }$ goal? Answer: David Akers</td>
</tr>
<tr>
<td style="text-align: center;">TASK 7</td>
<td style="text-align: center;">Quantitative NLI</td>
<td style="text-align: center;">9702</td>
<td style="text-align: center;">Statement 1: James took a 3 - hour bike ride, Statement 2: James took a more than 1 - hour bike ride, Options: Entailment or contradiction or neutral?, Answer: Entailment</td>
</tr>
<tr>
<td style="text-align: center;">TASK 8</td>
<td style="text-align: center;">Arithmetic word problems</td>
<td style="text-align: center;">1266</td>
<td style="text-align: center;">Question: Joe had 50 toy cars. If he gives away 12 cars, how many cars will he have remaining?, Answer: 38</td>
</tr>
</tbody>
</table>
<p>Table 1: Size and example of each task in the NumGLUE benchmark. RC: Reading Comrehension
curate a dataset of such questions that require both domain-specific knowledge and arithmetic reasoning motivated by the finding that QA systems perform poorly on the ARC dataset Clark et al. (2018) consisting of grade-school level science questions. Specifically, the dataset collected by us requires understanding of a small set of chemistry (conservation of mass in chemical reactions) and physics principles (speed $=$ distance $/$ time $)$.</p>
<p>Task 3: Commonsense + Quantitative Comparison. A golf ball weighs 40 g and a baseball weighs 150g. Which has a higher gravitational force? Answering this question requires both knowing that mass is directly proportional to gravitational force and a numerical comparison via subtraction. We collect such quantitative comparison questions by using the QuaRel dataset (Tafjord et al., 2019) containing questions from diverse fields such as physics and economics as the starting point. The annotator chooses a subset of these questions that involve numerically comparable quantities (for instance, in this example, mass of the objects involved) to create the required task of quantitative comparison questions.</p>
<p>Task 4: Fill-in-the-blanks Format. Unlike the previously proposed tasks that require external in-
formation (e.g.commonsense knowledge) in addition to simple arithmetic reasoning, this task is selfcontained but a stylistic variant of existing math word problems. We source word problems from the Arithmetic Word Problem repository (Roy and Roth, 2016, 2017, 2018) and convert them into the fill-in-the-blanks format. For an example of such a conversion, refer to fig. 1.</p>
<h3>3.2 Existing Datasets</h3>
<p>We now review existing datasets while discussing any modifications made when including them in NUMGLUE. In general, for all the datasets included, we perform a filtering step to clean and control for the quality of the data points being included. This step includes - a) discarding questions that do not have answer annotations b) eliminating questions with high lexical overlap with the remainder of the dataset and c) fixing any type mismatches present in the data (e.g." 7.0 students" $\rightarrow$ " 7 students").</p>
<p>Task 5: Reading Comprehension (RC) + Explicit Numerical Reasoning. We select a subset from the DROP (Dua et al., 2019b) dataset to create this task. Specifically, the selected questions involve reading comprehension and numerical reasoning but importantly, the required</p>
<p>answer is also a number.</p>
<p>Task 6: Reading Comprehension (RC) + Implicit Numerical Reasoning. Consider the following question based on a relevant passage Which state has the highest income tax rate? Here, while the final answer is a name, arriving at it requires performing comparison (i.e.subtraction). We classify such questions in the DROP dataset as a separate task in NUMGLUE.</p>
<p>Task 7: Quantitative NLI EQUATE (Ravichander et al., 2019) introduces quantitative NLI questions that require simple arithmetic calculations to be performed in order to accurately classify the relationship between the provided premise and the hypothesis. As noted in fig. 1, many word problems can also be easily converted to this format and is therefore, a diverse and interesting task for evaluating arithmetic reasoning skills of AI systems.</p>
<p>Task 8: Arithmetic Word Problems Finally, we arrive at one of the earliest and extensively studied class of arithmetic reasoning problems i.e.word problems. The specific dataset included as part of our NUMGLUEbenchmark is a combination of multiple datasets proposed by Koncel-Kedziorski et al. (2016), (Koncel-Kedziorski et al., 2015) and Kushman et al. (2014). Further, to ensure that the benchmark as a whole is diverse, we eliminate questions that have a high sentence similarity with questions from the fill-in-the-blanks task.</p>
<h3>3.3 Data Quality Analysis:</h3>
<p>In order to ensure a high-quality test set, three independent annotators evaluate each question in the test set across all tasks. A tiny porton of the data marked as invalid or with disagreement between the annotators was excluded, resulting in a verified, high-quality NUMGLUE evaluation suite. We also perform a variety of analysis and find that the novel question tasks we created (task 1-4) have higher quality than the existing question tasks since they have higher average vocabulary (number of unique words per number of samples), higher number of unique nouns, verbs and other POS tags and have less semantic textual similarity among each other (indicating lower repetition). Detailed analysis can be found in the supplementary material: Data Quality Analysis of NUMGLUE.</p>
<h2>4 Experiments</h2>
<p>In this section, we establish multiple baselines on our benchmark and discuss their performance.</p>
<h3>4.1 Baselines</h3>
<p>We evaluate several baselines on our benchmark - (i) Heuristic, (ii) Zero-shot, (iii) Few-shot, (iv) Fine-tuning and (v) Human. We use two kinds of model architectures (i) Neuro-symbolic, a memory augmented novel architecture that extends Numnet+v2 (Ran et al., 2019) and (ii) End-to-end, GPT3 (Brown et al., 2020).</p>
<p>Architectures. In the multi-task setting where the same model is trained on all the NUMGLUE tasks, we use Reading Comprehension (RC) as the common format - converting each task to RC format via a set of hand-coded rules ${ }^{4}$. In addition to being capable of faithfully representing all the constituent tasks, the RC format also allows us to inject additional context in the IR setting without affecting the rest of the pipeline ${ }^{5}$. On the other hand, GPT3 being a generative model does not require such modifications. Importantly, note that both models are inputted the exact same information for the multi-task experiments.</p>
<p>Heuristic Baselines with Task Oracle. For this baseline, we assume a task oracle that knows the task a particular question belongs (in a multitask setting) - we use this to make our heuristic baselines more competitive. The first heuristic baseline is random: we randomly select one of the options in case the question has multiple options (task 3 and 7), a number between 0 to 100 for questions having a numerical answer and a random entity present in the passage for questions having a text segment from the passage as the answer. In the majority baseline, we select the most frequent answer for each task such as "Entailment" for NLI questions and similarly, the most frequent number for questions having numerical answer and the major entity present in the passage for questions having span based answer. As the task information is known, we include these baselines under task-specific baselines when discussing results.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 2: Performance of zeroshot, fewshot and finetuning baselines (Section 4) across NumGLUE. There is a significant gap between the highest performing model and the human baseline. ZS: Zeroshot, GPT3I: GPT3-Instruct, MT: Multi-task, TS: Task-specific, QO: Question Only, CO: Context Only, EXNN: Ex-NumNet,FS: Fewshot, OS: Oversampling, IR: Information Retrieval, CIR: Conditional Information Retrieval.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: Our proposed memory-augmented model that detects the type of task (1-8), uses Information Retrieval from <em>MATH KB</em> and append the information that gets fed to Ex-NumNet.</p>
<p>Zeroshot and Fewshot Baselines. We use GPT3 (Brown et al., 2020) and the more recent GPT3-Instruct<sup>6</sup>. We have two types of few shot baseline (i) task specific and (ii) multi-task. In case of task-specific fewshot baseline, instances of the same task are used as in-context examples (Brown et al., 2020) whereas in case of multitask few shot baseline, instances from all tasks are used to condition the model. Multitask fewshot is naturally a harder setting as it is task-agnostic. We use default parameters in GPT3 and GPT3-Instruct. In few-shot setting, we experiment after feeding as many examples as it can fit within the tokensize. For few shot experiments, we randomly select examples and averaged the results over 5 runs.</p>
<p>Fine-tuning Baselines. We first consider variations of the fine-tuning baselines in the context of our neuro-symbolic model, Ex-NumNet.</p>
<p>We use it as bias-checking baseline â€“ to ensure that solving the benchmark correctly requires considering all of the information presented to it. To this end, we evaluate the performance of our model when finetuned only on the question (Q-only) or the context (C-only). Next, we present task-specific and multi-task baselines where Ex-NumNet is fine-tuned on individual tasks and the entire NumGLUE benchmark respectively. With the goal of addressing the data imbalance across the tasks, we include an oversampling</p>
<p><sup>6</sup>newly released by OpenAI as part of the GPT3 finetuned series</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Learning</th>
<th style="text-align: center;">Baseline category</th>
<th style="text-align: center;">Baseline name</th>
<th style="text-align: center;">Task 1</th>
<th style="text-align: center;">Task 2</th>
<th style="text-align: center;">Task 3</th>
<th style="text-align: center;">Task 4</th>
<th style="text-align: center;">Task 5</th>
<th style="text-align: center;">Task 6</th>
<th style="text-align: center;">Task 7</th>
<th style="text-align: center;">Task 8</th>
<th style="text-align: center;">NumGLUE Score</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Heuristic</td>
<td style="text-align: center;">Task-specific</td>
<td style="text-align: center;">Random</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">46.9</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">3.4</td>
<td style="text-align: center;">33</td>
<td style="text-align: center;">0.4</td>
<td style="text-align: center;">10.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Task-specific</td>
<td style="text-align: center;">Majority</td>
<td style="text-align: center;">1.2</td>
<td style="text-align: center;">13.9</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">7.4</td>
<td style="text-align: center;">3.8</td>
<td style="text-align: center;">36.5</td>
<td style="text-align: center;">1.2</td>
<td style="text-align: center;">14.3</td>
</tr>
<tr>
<td style="text-align: center;">ZERO-SHOT</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">GPT3</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">17</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">4.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">GPT3-Instruct</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">29</td>
<td style="text-align: center;">17</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">8.1</td>
</tr>
<tr>
<td style="text-align: center;">FEW-SHOT</td>
<td style="text-align: center;">Task-specific</td>
<td style="text-align: center;">GPT3</td>
<td style="text-align: center;">44</td>
<td style="text-align: center;">42</td>
<td style="text-align: center;">46</td>
<td style="text-align: center;">40</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">42</td>
<td style="text-align: center;">35</td>
<td style="text-align: center;">40</td>
<td style="text-align: center;">37.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Task-specific</td>
<td style="text-align: center;">GPT3-Instruct</td>
<td style="text-align: center;">40</td>
<td style="text-align: center;">39</td>
<td style="text-align: center;">51</td>
<td style="text-align: center;">33</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">43</td>
<td style="text-align: center;">35</td>
<td style="text-align: center;">33</td>
<td style="text-align: center;">35.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Multi-task</td>
<td style="text-align: center;">GPT3</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">27</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">28</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">12.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Multi-task</td>
<td style="text-align: center;">GPT3-Instruct</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">37</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">35</td>
<td style="text-align: center;">31</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">15.1</td>
</tr>
<tr>
<td style="text-align: center;">FINE-TUNING</td>
<td style="text-align: center;">Multi-task</td>
<td style="text-align: center;">GPT3-13B</td>
<td style="text-align: center;">21.5</td>
<td style="text-align: center;">40.7</td>
<td style="text-align: center;">71.2</td>
<td style="text-align: center;">11.1</td>
<td style="text-align: center;">6.3</td>
<td style="text-align: center;">48.2</td>
<td style="text-align: center;">48.0</td>
<td style="text-align: center;">14.2</td>
<td style="text-align: center;">32.7</td>
</tr>
<tr>
<td style="text-align: center;">FINE-TUNING</td>
<td style="text-align: center;">Multi-task (Q-only)</td>
<td style="text-align: center;">Ex-NumNet</td>
<td style="text-align: center;">1.2</td>
<td style="text-align: center;">13.2</td>
<td style="text-align: center;">25.1</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">6.1</td>
<td style="text-align: center;">25.1</td>
<td style="text-align: center;">32.8</td>
<td style="text-align: center;">2.4</td>
<td style="text-align: center;">13.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Multi-task (C-only)</td>
<td style="text-align: center;">Ex-NumNet</td>
<td style="text-align: center;">1.2</td>
<td style="text-align: center;">14.2</td>
<td style="text-align: center;">22.8</td>
<td style="text-align: center;">19.1</td>
<td style="text-align: center;">0.6</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">9.5</td>
<td style="text-align: center;">8.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Single-task</td>
<td style="text-align: center;">Ex-NumNet</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">37.8</td>
<td style="text-align: center;">50.8</td>
<td style="text-align: center;">22.2</td>
<td style="text-align: center;">66.6</td>
<td style="text-align: center;">71.6</td>
<td style="text-align: center;">85.9</td>
<td style="text-align: center;">12.2</td>
<td style="text-align: center;">43.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Multi-task</td>
<td style="text-align: center;">Ex-NumNet</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">37.5</td>
<td style="text-align: center;">58</td>
<td style="text-align: center;">31.4</td>
<td style="text-align: center;">68.2</td>
<td style="text-align: center;">70.2</td>
<td style="text-align: center;">85.7</td>
<td style="text-align: center;">23.2</td>
<td style="text-align: center;">46.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Multi-task + IR</td>
<td style="text-align: center;">Ex-NumNet</td>
<td style="text-align: center;">5.6</td>
<td style="text-align: center;">37.5</td>
<td style="text-align: center;">46.6</td>
<td style="text-align: center;">36.4</td>
<td style="text-align: center;">68.6</td>
<td style="text-align: center;">69.6</td>
<td style="text-align: center;">85.9</td>
<td style="text-align: center;">22.4</td>
<td style="text-align: center;">46.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Multi-task + CIR</td>
<td style="text-align: center;">Ex-NumNet</td>
<td style="text-align: center;">7.4</td>
<td style="text-align: center;">38.8</td>
<td style="text-align: center;">58</td>
<td style="text-align: center;">36.8</td>
<td style="text-align: center;">69.2</td>
<td style="text-align: center;">70.8</td>
<td style="text-align: center;">85.8</td>
<td style="text-align: center;">23.6</td>
<td style="text-align: center;">48.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Multi-task + OS</td>
<td style="text-align: center;">Ex-NumNet</td>
<td style="text-align: center;">7.4</td>
<td style="text-align: center;">38.8</td>
<td style="text-align: center;">47.8</td>
<td style="text-align: center;">35.9</td>
<td style="text-align: center;">44.3</td>
<td style="text-align: center;">53.7</td>
<td style="text-align: center;">85.4</td>
<td style="text-align: center;">22.4</td>
<td style="text-align: center;">42.0</td>
</tr>
<tr>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Human</td>
<td style="text-align: center;">94.4</td>
<td style="text-align: center;">94.5</td>
<td style="text-align: center;">97.8</td>
<td style="text-align: center;">95</td>
<td style="text-align: center;">94.7</td>
<td style="text-align: center;">96.1</td>
<td style="text-align: center;">96.5</td>
<td style="text-align: center;">92.8</td>
<td style="text-align: center;">95.2</td>
</tr>
</tbody>
</table>
<p>Table 2: F1 performance of various baselines on the NumGLUE test set across various tasks 1-8. Human performance was calculated on 100 samples of each task ( 81 of Task 1) [*IR = Information Retrieval, CIR=Conditional Information Retrieval, OS=Oversampling, Q. Only: Question Only, C. Only: Context Only ].
baseline that oversamples data from tasks with limited data so as to ensure that the model sees the same number of examples from each constituent task.</p>
<p>In addition, we propose a new architectural modification to Ex-NumNet. Noting that our baseline model Ex-NumNet does not take into account external knowledge, we create a new enhanced architecture in the form of a memory-augmented model that does Information Retrieval (IR) (Khot et al., 2019) with respect to a knowledge base we create, $M A T H ~ K B$ to identify the needed knowledge. This is inspired by the observation that formula book and mathematical knowledge make the task easier for humans while solving math questions of various types. We then use this knowledge in the Ex-NumNet setting. Figure 3 illustrates our approach which leverages our newly created knowledge base MATH KB. Conditional IR model is different from the regular IR model in the sense that, IR is performed only for questions of task 1,2 and 4 , since they require external knowledge to get answered. More details about the model and the IR process can be found in supplementary material: Proposed Memory-Augmented Model (A. 5 and A.6).</p>
<p>Finally, we discuss fine-tuning baselines in the context of end-to-end models, specifically GPT3. We finetune the GPT3-13B model (for which the finetuning capability has been recently
provided by OpenAI ${ }^{7}$ ) in the multi-task setting i.e. the desired setting of the NUMGLUE benchmark.</p>
<p>Human Baseline. Human baseline was calculated on 100 test set samples of each task ( 81 of Task 1) by averaging the scores of four annotators.</p>
<h2>5 Results and Discussion</h2>
<p>Table 2 shows the performance of various baseline models on the test set of our benchmark. Note that the performance of all baseline models is significantly lesser than the human baseline (Figure 2). We now discuss various insights based on these results.</p>
<p>Does the benchmark contain bias that a model can exploit? A challenging dataset requires the model to ideally consider all the information provided to it before arriving at an answer. To ensure that this is indeed the case, we perform ablations where only one portion of the input is provided i.e. either the question or the context. Both these "bias-checking" baselines perform poorly even in task-specific setting indicating that both the benchmark and constituent tasks are challenging.</p>
<p>Which Tasks are Hard to Solve? Our results show that task 1 which requires numerical commonsense knowledge, is the hardest task to solve. Similarly, tasks 2, 4 and 8 appear to be</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>comparatively harder from the rest. One pattern among these tasks is that all of them expect the answer to be numeric. Numeric answer requires accurate calculation. So, models might have difficulty in learning the task directly from data. This hypothesis is also justified from the slight drop in human performance in these tasks..
On the other hand, task 7 has the best performance among all. Further, we see that performance on task 6 is slightly better than task 5 - although both tasks are sourced from the same dataset, we observe that models answer span based questions better as compared to numeric answers. Relatively higher performance for task 3 suggests that models find it easier to answer in an MCQ setting.</p>
<p>Does IR Help? Results show that knowledge help in improving performance of tasks 1 , 2 and 4 - where indeed, external knowledge like commonsense or domain-specific knowledge is needed in addition to arithmetic reasoning to arrive at the correct answer. However, task 3 is an exception to this trend and in fact registers a drop in the score when provided with (unnecessary) additional information; we find that this shortcoming is fixed when using conditional information retrieval (CIR) which in fact leads to the strongest baseline presented in this work.</p>
<p>Does Oversampling help overcome data imbalance across tasks? Even though oversampling results in higher performance in certain tasks (in comparison with the multitask baseline), specifically the ones with smaller training data, it results in significant drop in performance in the other extreme, i.e tasks with bigger training data. Also, it never performs better than the Conditional IR module in multitask setting.</p>
<h3>5.1 Error Analysis</h3>
<p>We now present an analysis of the errors made by our baselines to indicate potential avenues for future research.</p>
<p>We analyze errors associated with 50 samples each of the 8 tasks and find that there are mainly 4 categories of error models make: (1) producing invalid output (e.g. answering text where the answer is supposed to be a number, answering a text different from the classes allowed in a classification problem), (2) copying a number</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Error</th>
<th style="text-align: left;">Ex-NumNet</th>
<th style="text-align: left;">GPT3</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Invalid output</td>
<td style="text-align: left;">$16 \%$</td>
<td style="text-align: left;">$7 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Copy number</td>
<td style="text-align: left;">$5 \%$</td>
<td style="text-align: left;">$3 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Incorrect calculation</td>
<td style="text-align: left;">$71 \%$</td>
<td style="text-align: left;">$56 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Redundant text</td>
<td style="text-align: left;">$8 \%$</td>
<td style="text-align: left;">$34 \%$</td>
</tr>
</tbody>
</table>
<p>Table 3: Error analysis for the best Ex-NumNet Multi-task+CIR and GPT3 Task-specific model
from the question instead of calculating the answer, (3) incorrect calculation - this can be due to multiple reasons including (i) using an incorrect operation e.g. subtraction in place of addition, (ii) incorrect parsing of numbers or (iii) incorrect knowledge of numerical commonsense facts. (4) producing redundant text after producing correct answer. Based on error distribution in Table 3, we observe that the majority of errors come from incorrect calculation. Further, GPT3 is better than Ex NumNet+v2 in producing valid outputs, but it produces more redundant text.</p>
<p>Future Directions: Bigger model, more data or ...? Table 2 shows that fine-tuned GPT3-13B outperforms other baselines on task 1, 2 and 3. Recall that these tasks require external knowledge and perhaps, this is the reason why GPT3, already pre-trained on a diverse web-scale text corpus has an edge over other baselines on these tasks. In case of the smaller Ex-NumNet, it is interesting that multitask baselines are higher than the single task baselines by $3.4 \%$ on average and that information retrieval helps in tasks that require external knowledge. Also notice that, GPT-3 is better on smaller datasets and NumNet is better on large datasets. This may indicate that GPT-3 is a better few-shot learner but not necessarily a better many-shot learner. This non-overlapping performance of GPT-3 and Ex-numnet, end-to-end and neuro-symbolic models respectively, indicates that a potential future direction for research is to combine the best of both the models.</p>
<h2>6 Conclusion</h2>
<p>We propose NumGLUE, a multi-task benchmark to test for arithmetic understanding. Our benchmark consists of eight tasks including four new ones. While some of the tasks require external knowledge like commonsense or domain-specific information in addition to arithmetic reasoning, some are self-contained e.g. arithmetic word problems. Further, we demonstrate that our benchmark</p>
<p>is far from being solved - with state-of-the-art large scale models achieving considerably lower performance than humans. This indicates that current AI systems are incapable of performing simple arithmetic reasoning in a general setting - indicating a fundamental hurdle towards AI systems that understand complex mathematical concepts like differential equations or combinatorics. Finally, we present various baselines including a novel architecture (memory augmented Ex-NumNet) that demonstrate the advantages of various modeling choices (e.g. end-to-end vs neuro-symbolic models). Specifically, we show that training in the multi-task setting leads to meaningful sharing of knowledge across tasks as evidenced by an average gain of $3.4 \%$ on tasks compared to task-specific modeling. Finally, we hope that our benchmark not only leads to AI systems that are capable of performing simple arithmetic reasoning in a fairly general setting but also results in progress towards more complex mathematical reasoning capability.</p>
<h2>Acknowledgements</h2>
<p>We thank OpenAI for providing academic access to the GPT3 API, the Aristo team at AI2 for helpful input, the Beaker team for their support with experiments and the anonymous reviewers for their insightful feedback. The support of DARPA SAILON, DARPA CHESS program is gratefully acknowledged.</p>
<h2>Ethical Considerations</h2>
<p>We have verified that all licenses of source datasets used in this paper allow for their use, modification, and redistribution in a research context. The dataset will be distributed in a manner similar to SuperGLUE (Wang et al., 2019) i.e. give full credit assignment to the original data and task creators.</p>
<h2>References</h2>
<p>Armen Aghajanyan, Anchit Gupta, Akshat Shrivastava, Xilun Chen, Luke Zettlemoyer, and Sonal Gupta. 2021. Muppet: Massive multi-task representations with pre-finetuning. arXiv preprint arXiv:2101.11038.</p>
<p>Aida Amini, Saadia Gabriel, Peter Lin, Rik KoncelKedziorski, Yejin Choi, and Hannaneh Hajishirzi. 2019. Mathqa: Towards interpretable math word problem solving with operation-based formalisms. arXiv preprint arXiv:1905.13319.</p>
<p>Anjana Arunkumar, Swaroop Mishra, Bhavdeep Sachdeva, Chitta Baral, and Chris Bryan. 2020. Real-time visual feedback for educative benchmark creation: A human-and-metric-in-the-loop workflow.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel HerbertVoss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877-1901. Curran Associates, Inc.</p>
<p>Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457.</p>
<p>Anna Rogers Olga Kovaleva Matthew Downey and Anna Rumshisky. Getting closer to ai complete question answering: A set of prerequisite real tasks.</p>
<p>Dheeru Dua, Ananth Gottumukkala, Alon Talmor, Sameer Singh, and Matt Gardner. 2019a. Orb: An open reading benchmark for comprehensive evaluation of machine reading comprehension. arXiv preprint arXiv:1912.12598.</p>
<p>Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. 2019b. Drop: A reading comprehension benchmark requiring discrete reasoning over paragraphs. arXiv preprint arXiv:1903.00161.</p>
<p>Suchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel R Bowman, and Noah A Smith. 2018. Annotation artifacts in natural language inference data. arXiv preprint arXiv:1803.02324.</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring massive multitask language understanding. In International Conference on Learning Representations.</p>
<p>Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874.</p>
<p>Mohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, and Nate Kushman. 2014. Learning to solve arithmetic word problems with verb categorization. In In Conference on Empirical Methods in Natural Language Processing (EMNLP.</p>
<p>Daniel Khashabi, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, and Hannaneh Hajishirzi. 2020. Unifiedqa: Crossing format boundaries with a single qa system. arXiv preprint arXiv:2005.00700.</p>
<p>Tushar Khot, Ashish Sabharwal, and Peter Clark. 2019. What's missing: A knowledge gap guided approach for multi-hop question answering. arXiv preprint arXiv:1909.09253.</p>
<p>Rik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish Sabharwal, Oren Etzioni, and Siena Dumas Ang. 2015. Parsing algebraic word problems into equations. Transactions of the Association for Computational Linguistics, 3:585-597.</p>
<p>Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi. 2016. Mawps: A math word problem repository. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1152-1157.</p>
<p>Nate Kushman, Yoav Artzi, Luke Zettlemoyer, and Regina Barzilay. 2014. Learning to automatically solve algebra word problems. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 271-281.</p>
<p>Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. 2017. Program induction by rationale generation: Learning to solve and explain algebraic word problems. arXiv preprint arXiv:1705.04146.</p>
<p>Nicholas Lourie, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2021. Unicorn on rainbow: A universal commonsense reasoning model on a new multitask benchmark. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages $13480-13488$.</p>
<p>Bryan McCann, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. 2018. The natural language decathlon: Multitask learning as question answering. arXiv preprint arXiv:1806.08730.</p>
<p>Shen-yun Miao, Chao-Chun Liang, and Keh-Yih Su. 2020. A diverse corpus for evaluating and developing English math word problem solvers. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 975-984, Online. Association for Computational Linguistics.</p>
<p>Swaroop Mishra, Anjana Arunkumar, Bhavdeep Sachdeva, Chris Bryan, and Chitta Baral. 2020. Dqi: Measuring data quality in nlp. arXiv preprint arXiv:2005.00816.</p>
<p>Swaroop Mishra and Bhavdeep Singh Sachdeva. 2020. Do we need to create big datasets to learn a task? In Proceedings of SustaiNLP: Workshop on Simple and Efficient Natural Language Processing, pages 169173, Online. Association for Computational Linguistics.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2019. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv preprint arXiv:1910.10683.</p>
<p>Qiu Ran, Yankai Lin, Peng Li, Jie Zhou, and Zhiyuan Liu. 2019. Numnet: Machine reading comprehension with numerical reasoning. arXiv preprint arXiv:1910.06701.</p>
<p>Abhilasha Ravichander, Aakanksha Naik, Carolyn Rose, and Eduard Hovy. 2019. Equate: A benchmark evaluation framework for quantitative reasoning in natural language inference. arXiv preprint arXiv:1901.03735.</p>
<p>Subhro Roy and Dan Roth. 2016. Solving general arithmetic word problems. arXiv preprint arXiv:1608.01413.</p>
<p>Subhro Roy and Dan Roth. 2017. Unit dependency graph and its application to arithmetic word problem solving. In Thirty-First AAAI Conference on Artificial Intelligence.</p>
<p>Subhro Roy and Dan Roth. 2018. Mapping to declarative knowledge for word problem solving. Transactions of the Association for Computational Linguistics, 6:159-172.</p>
<p>David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. 2019. Analysing mathematical reasoning abilities of neural models. arXiv preprint arXiv:1904.01557.</p>
<p>Swabha Swayamdipta, Roy Schwartz, Nicholas Lourie, Yizhong Wang, Hannaneh Hajishirzi, Noah A Smith, and Yejin Choi. 2020. Dataset cartography: Mapping and diagnosing datasets with training dynamics. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 9275-9293.</p>
<p>Oyvind Tafjord, Peter Clark, Matt Gardner, Wen-tau Yih, and Ashish Sabharwal. 2019. Quarel: A dataset and models for answering questions about qualitative relationships. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 7063-7071.</p>
<p>Shyam Upadhyay, Ming-Wei Chang, Kai-Wei Chang, and Wen-tau Yih. 2016. Learning from explicit and implicit supervision jointly for algebra word problems. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 297-306.</p>
<p>Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2019. Superglue: A stickier benchmark for general-purpose language understanding systems. In Advances in Neural Information Processing Systems, pages 3261-3275.</p>
<p>Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. 2018. Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461.</p>
<p>Jason Weston, Antoine Bordes, Sumit Chopra, Alexander M Rush, Bart van MerriÃ«nboer, Armand Joulin, and Tomas Mikolov. 2015. Towards ai-complete question answering: A set of prerequisite toy tasks. arXiv preprint arXiv:1502.05698.</p>
<p>Xikun Zhang, Deepak Ramachandran, Ian Tenney, Yanai Elazar, and Dan Roth. Do language embeddings capture scales?</p>
<h2>A Supplemental Material</h2>
<h2>A. 1 NumGLUE vs Other Datasets:</h2>
<p>As figure 4 shows, we select each task from one of the clusters of numerical reasoning datasets (except the multi-model reasoning cluster since we wanted to limit our dataset to text only).</p>
<h2>A. 2 Construction of NumGLUE :</h2>
<p>Figure 5 and 6 illustrate detailed data creation process for task 1 , task 2 , task 3 and task 4 questions with the help of an example for each task. We follow the same procedure for creating other examples within the task.</p>
<h2>A. 3 GPT3-Instruct's Response</h2>
<p>We used GPT3-Instruct on various forms of a simple arithmetic question. An expert did tuning of various parameteres such as temperature, stop condition, presence penalty, engine, maximum token size. However, GPT3-Instruct still could not solve the basic aritmetic questions reliabily (Figures 711).</p>
<h2>A. 4 Data Quality Analysis of NumGLUE</h2>
<p>In this section, we discuss various linguistic and statistical properties of our benchmark; ones that we believe result in the quality, diversity and challenging nature (Gururangan et al., 2018; Mishra et al., 2020; Mishra and Sachdeva, 2020; Swayamdipta et al., 2020; Arunkumar et al., 2020) of the proposed NumGLUE benchmark.</p>
<p>Vocabulary Size. First, we calculate vocabulary size of each task by finding the number of unique words across all questions. Since our dataset is unbalanced in terms of question task, we find the average vocabulary size by dividing vocabulary size with number of data in that task.</p>
<p>Which Data has Higher Average Vocabulary? As illustrated in Figure 12a, most of the tasks belonging to the novel dataset category have relatively better average vocabulary size. This implies questions in those tasks have less repetitiveness. Furthermore, we expand our vocabulary analysis to understand Figure 12a better. We dive deep to analyze different parts of speech. Figure 12b summarises our analysis. Most of the novel datasets have more average number of nouns, verbs and adjectives implying there are more varieties of entities, actions and
attributes. This further means that datasets belonging to the novel category are more diverse in nature.</p>
<p>Sentence Similarity Analysis We extend our analysis to reinforce our inference from the word vocabulary analysis. We find Semantic Textual Similarity (STS) of a sentence with every other sentence.</p>
<p>Which Data Consists of Most Dissimilar Sentences? As depicted by Figure 12c-12f, most questions in QuaRel have high similarity value with other questions indicating the repetitiveness of data. Same is true for majority of EQUATE data. DROP also has high similarity among questions. However, similarity among questions in our dataset is significantly less. Some similarity boxes can be seen in the chart. They are mostly due to task 2 data, and partly due to task 3 data. Lesser similarity implies that our dataset is far less repetitive than others. Also, the repetition in our dataset is sparse and is not equally distributed among the whole dataset unlike others. This way, our dataset is more diverse.</p>
<p>Note that question in Task 2 have lower vocabulary and further, a higher similarity as well. As a small set of chemistry and physics principles are used to generate questions, the result is a fairly templated or uniform-looking dataset - leading to the observed reversal of trends in this particular task.</p>
<h2>A. 5 Ex-NumNet</h2>
<p>Figure 13 illustrates our baseline model: ExNumNet. This contains a Reading Comprehension Converter module which converts each task of question to reading comprehension format. Figure 14 illustrates various examples of how each task of questions get converted to the reading comprehension format. We add a task converter module to detect task of a question. We design task converter heuristically based on the features associated with questions (e.g. NLI contains "Sentence 1" and "Sentence 2" whereas completion contains a blank). We convert each of the tasks to RC format. For NLI questions, we use the premise sentence as passage, hypothesis as the question and append the string "Entailment, contradiction or neutral?"</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: Our dataset NumGLUE (center in the yellow circle) has been positioned with respect to existing datasets. T1-T8 represents 8 tasks. Note that, NumGLUE contains the feature of being format invariant unlike other datasets. Position of datasets within clusters is done based on their semantic category, for example T1 Numerical Commonsense QA is closer to the cluster of Commonsense Reasoning + Knowledge of Facts; its position reflects the same
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: Step by step data creation process for task 1, 2 and 4 questions
to the question so that it has a span based answer. For other questions, we tokenize the question string into its constituent sentences and use a heuristic approach to split the question string into passage and question. Furthermore, for option based questions, we append all the options at the end of the question.</p>
<h2>A. 6 Proposed Memory-Augmented Model</h2>
<p>Figure 13 illustrates our baseline model ExNumNet. We add an IR mechanism as described in Algorithm 1 and illustrated in Figure 3 of the</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6: Step by step data creation process for task 3 questions
main paper. As mentioned in the 'Baselines' subsection (Experiments section) of the main paper, we convert each task to RC format in our baseline and append the knowledge retrieved using IR from $M A T H ~ K B$ at the end of the passage. In our experiments, we use the following hyperparameters in the IR process: $Z=50, v=10, t h=0.75$ and $b=0.1$.</p>
<p>Formalization Let $D$ represents dataset, $s$ represents sample, $K$ represent the $M A T H ~ K B, v$ represents the number of knowledge statements retrieved for each sample, $t h$ is the cut off STS (Semantic Textual Similarity) value above which knowledge statements are treated redundant and removed, $b$ is the reduction we do iteratively on $t h$ until $v$ statements remain.</p>
<p>We create a knowledge base, $M A T H ~ K B$ by accumulating all tasks of external knowledge which are needed to solve questions of various tasks (e.g. human has 2 hands, cow has 4 legs, there are 24 hours in a day etc..). We also add math formulae required to solve questions in our benchmark (e.g. the formula of speed in terms of distance and time). We add alll these in the form of plain text separated by new line. We use Elasticsearch to retrieve relevant knowledge sentences. We further filter them using a heuristic threshold of relevance. We append this knowledge in the beginning of the passage so that continuity is not broken between passage and question. Figure 3 of the main paper illustrates our approach.</p>
<div class="codehilite"><pre><span></span><code><span class="nx">Algorithm</span><span class="w"> </span><span class="mi">1</span><span class="p">:</span><span class="w"> </span><span class="nx">Our</span><span class="w"> </span><span class="nx">Information</span><span class="w"> </span><span class="nx">Retrieval</span>
<span class="nx">Approach</span>
<span class="mi">1</span><span class="p">}</span><span class="err">\</span><span class="nx">mathrm</span><span class="p">{</span><span class="w"> </span><span class="nx">Dataset</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">D</span><span class="err">\</span><span class="p">),</span><span class="w"> </span><span class="nx">MATH</span><span class="w"> </span><span class="nx">KB</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">K</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">Hyper</span><span class="o">-</span><span class="nx">Parameters</span><span class="p">:</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">Z</span><span class="p">,</span><span class="w"> </span><span class="nx">v</span><span class="err">\</span><span class="p">),</span>
<span class="w">    </span><span class="err">\</span><span class="p">(</span><span class="nx">t</span><span class="w"> </span><span class="nx">h</span><span class="p">,</span><span class="w"> </span><span class="nx">b</span><span class="w"> </span><span class="nx">v</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">Knowledge</span><span class="w"> </span><span class="nx">sentences</span><span class="w"> </span><span class="k">forall</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">s</span><span class="w"> </span><span class="err">\</span><span class="k">in</span><span class="w"> </span><span class="nx">D</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">do</span>
<span class="w">    </span><span class="nx">Concat</span><span class="w"> </span><span class="nx">Question</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">Answer</span><span class="w"> </span><span class="p">;</span>
<span class="w">    </span><span class="nx">Generate</span><span class="w"> </span><span class="nx">Query</span><span class="w"> </span><span class="nx">by</span><span class="w"> </span><span class="nx">retaining</span><span class="w"> </span><span class="nx">only</span><span class="w"> </span><span class="nx">verbs</span><span class="p">,</span>
<span class="w">        </span><span class="nx">adjectives</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">adverbs</span><span class="p">;</span>
<span class="w">        </span><span class="k">forall</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">j</span><span class="w"> </span><span class="err">\</span><span class="k">in</span><span class="w"> </span><span class="nx">K</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">do</span>
<span class="w">            </span><span class="nx">Create</span><span class="w"> </span><span class="nx">Index</span><span class="w"> </span><span class="nx">using</span><span class="w"> </span><span class="nx">Elastic</span><span class="w"> </span><span class="nx">Search</span><span class="w"> </span><span class="p">;</span>
<span class="w">            </span><span class="nx">Retrieve</span><span class="w"> </span><span class="nx">top</span><span class="w"> </span><span class="nx">Z</span><span class="w"> </span><span class="nx">sentences</span><span class="w"> </span><span class="nx">from</span><span class="w"> </span><span class="nx">MATH</span><span class="w"> </span><span class="nx">KB</span><span class="p">.</span>
<span class="w">        </span><span class="nx">end</span>
<span class="w">        </span><span class="k">while</span><span class="w"> </span><span class="nx">size</span><span class="p">(</span><span class="nx">Z</span><span class="p">)</span><span class="w"> </span><span class="err">\</span><span class="p">(&gt;</span><span class="nx">v</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">do</span>
<span class="w">            </span><span class="k">forall</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">k</span><span class="w"> </span><span class="err">\</span><span class="k">in</span><span class="w"> </span><span class="nx">Z</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">do</span>
<span class="w">                </span><span class="k">forall</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">u</span><span class="w"> </span><span class="err">\</span><span class="k">in</span><span class="w"> </span><span class="nx">k</span><span class="o">-</span><span class="mi">1</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">do</span>
<span class="w">                    </span><span class="k">if</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">S</span><span class="w"> </span><span class="nx">T</span><span class="w"> </span><span class="nx">S</span><span class="p">(</span><span class="nx">Z</span><span class="p">(</span><span class="nx">u</span><span class="p">),</span><span class="w"> </span><span class="nx">Z</span><span class="p">(</span><span class="nx">k</span><span class="p">))&gt;</span><span class="nx">t</span><span class="w"> </span><span class="nx">h</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">then</span>
<span class="w">                    </span><span class="nx">Delete</span><span class="w"> </span><span class="nx">k</span><span class="p">;</span>
<span class="w">                </span><span class="nx">end</span>
<span class="w">                </span><span class="nx">end</span>
<span class="w">                </span><span class="nx">end</span>
<span class="w">                </span><span class="nx">th</span><span class="w"> </span><span class="err">\</span><span class="p">(=</span><span class="nx">t</span><span class="w"> </span><span class="nx">h</span><span class="o">-</span><span class="nx">b</span><span class="w"> </span><span class="p">;</span><span class="err">\</span><span class="p">)</span>
<span class="w">            </span><span class="nx">end</span>
</code></pre></div>

<p>1 end</p>
<h2>A. 7 Hyper Parameters Used</h2>
<p>All the experiments were ran with the following hyper parameters, batch size was kept at 16 where as the eval batch size was 5 . The maximum number of epoch ran for the experiments were 5 with the warm-up kept at 0.06 . The learning rate used was $1.5 \mathrm{e}-5$ and the weight decay was 0.01 .</p>
<p>All above hyper parameters were selected using a grid search; we kept rest of the hyper parameters unaltered. All the experiments were performed on "TeslaV100-SXM2-16GB", with which the model takes 24hrs to train on nearly 100k samples.</p>
<h2>A. 8 Additional Examples</h2>
<p>We provide additional examples of task 1, 2, 3 and 4 questions here to better illustrate the novel datasets we have created as part of our NumGLUE.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 7: GPT3-Instruct's response to a simple numerical reasoning question.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 8: GPT3-Instruct's response to a simple numerical reasoning question expressed in fill in the blanks format.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 9: GPT3-Instruct's response to a simple numerical reasoning question expressed in fill in the blanks format where numbers are changed.</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 10: GPT3-Instruct's response to a simple numerical reasoning question expressed in comparison format.</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 11: GPT3-Instruct's response to a simple numerical reasoning question expressed in NLI format.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Question</th>
<th style="text-align: left;">Knowledge Required</th>
<th style="text-align: center;">Answer</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Ella and Lily are play- <br> ing a game that re- <br> quires 10 die. Find <br> out the total number of <br> faces in 10 die.</td>
<td style="text-align: left;">A die has 6 <br> faces</td>
<td style="text-align: center;">60</td>
</tr>
<tr>
<td style="text-align: left;">Jacob and Lillian are <br> running a km long <br> race. Jacob finished <br> the race when Lil- <br> lian was 190 meters <br> from the finish line. <br> How many meters did <br> Lillian cover till that <br> time?</td>
<td style="text-align: left;">1000 meters <br> make a km</td>
<td style="text-align: center;">810</td>
</tr>
<tr>
<td style="text-align: left;">A man can lift one box <br> in each of his hands. <br> How many boxes can <br> a group of 5 people <br> hold in total?</td>
<td style="text-align: left;">A human <br> being has 2 <br> hands</td>
<td style="text-align: center;">10</td>
</tr>
</tbody>
</table>
<p>Table 4: Example questions where numerical knowledge required to answer is not explicitly provided in the question.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Question</th>
<th style="text-align: left;">Knowledge Required</th>
<th style="text-align: center;">Answer</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Find the mass percent- <br> age of H in C6H6</td>
<td style="text-align: left;">Mass of C is <br> 12 units and <br> mass of H is <br> 1 unit</td>
<td style="text-align: center;">7.69</td>
</tr>
<tr>
<td style="text-align: left;">How many units of <br> H2 are required to re- <br> act with 2 units of <br> C2H4 to form 2 units <br> of C2H6</td>
<td style="text-align: left;">H2 + C2H4 = <br> C2H6</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: left;">A car covers 912 me- <br> ters in 19 seconds. If <br> bike's speed is one <br> fourth of the car. Find <br> the distance covered <br> by the bike in 4 sec- <br> onds.</td>
<td style="text-align: left;">distance trav- <br> elled = speed <br> * time</td>
<td style="text-align: center;">48</td>
</tr>
</tbody>
</table>
<p>Table 5: Example questions where domain knowledge is required to answer a question.</p>
<p><img alt="img-10.jpeg" src="img-10.jpeg" />
(a) Average vocabulary represents the average number of unique words across various tasks. On an average, novel datasets (task 1-4) have higher vocabulary.
<img alt="img-11.jpeg" src="img-11.jpeg" />
(c) STS plot for the QuaReL
(d) STS plot for the EQUATE dataset shows significant repetition across samples
<img alt="img-12.jpeg" src="img-12.jpeg" />
(e) STS plot for the DROP dataset shows repetitions for most part of the data.
(f) STS plot for the novel datasets show relatively lower repetition than other datasets</p>
<p>Figure 12: Data quality analysis of NumGLUE across various tasks of data. On an average, novel datasets have higher quality than the others since they have higher average vocabulary, higher average POS tag numbers and lower Semantic Textual Similarity (STS) among each other. X-axis and Y-axis represents samples ordered in the same way, an ideal high quality dataset would have a bright line in the diagonal and rest of the places it should be dark signifying lower repetition across instances.
<img alt="img-13.jpeg" src="img-13.jpeg" /></p>
<p>Figure 13: Architecture of Ex-NumNet
<img alt="img-14.jpeg" src="img-14.jpeg" /></p>
<p>Figure 14: Conversion of various tasks to reading comprehension format</p>
<table>
<thead>
<tr>
<th style="text-align: left;">QuaRel Question</th>
<th style="text-align: left;">Transformed Question</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">A person wants to get <br> shopping done quickly. <br> They know that they can <br> get through the checkout <br> at big store faster than they <br> can at small store. The <br> store they go to to finish <br> quickly is <br> (A) big store (B) small <br> store</td>
<td style="text-align: left;">A person wants to get <br> shopping done quickly. <br> They know that they can <br> get through the checkout <br> at big store in 5 min- <br> utes whereas it can take <br> 20 mintues at small store. <br> The store they go to to fin- <br> ish quickly is <br> (A) big store (B) small <br> store</td>
</tr>
<tr>
<td style="text-align: left;">Tina is racing her two <br> dogs. Her greyhound <br> is slim, her rottweiler is <br> heavy. The dog that gets <br> faster more quickly is the <br> (A) rottweiler (B) grey- <br> hound</td>
<td style="text-align: left;">Tina is racing her two <br> dogs. Her greyhound <br> weighs 88 lbs and her rot- <br> tweiler weighs 79 lbs. The <br> dog that gets faster more <br> quickly is the <br> (A) rottweiler (B) grey- <br> hound</td>
</tr>
<tr>
<td style="text-align: left;">A golf ball has a smaller <br> mass then a baseball. <br> Which item has a weaker <br> gravitational field? <br> (A) golf ball (B) baseball</td>
<td style="text-align: left;">A golf ball has a mass of <br> 78 grams and a baseball <br> has a mass of 0.159 Kg . <br> Which item has a weaker <br> gravitational field? <br> (A) golf ball (B) baseball</td>
</tr>
</tbody>
</table>
<p>Table 6: Examples showing conversion of QuaRel questions to quantitative comparison questions</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Arithmetic Word Problem</th>
<th style="text-align: left;">Transformed Question</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Joan found 70 seashells <br> on the beach. She gave <br> Sam some of her seashells. <br> She has 27 seashell left. <br> How many seashells did <br> she give to Sam ? 43</td>
<td style="text-align: left;">Joan found 70 seashells <br> on the beach . She gave <br> Sam some of her seashells <br> She has 27 seashells left. <br> To Sam. 43</td>
</tr>
<tr>
<td style="text-align: left;">Last week Tom had 74 dol- <br> lars. He washed cars over <br> the weekend and now has <br> 86 dollars. How much <br> money did he make wash- <br> ing cars ? 12</td>
<td style="text-align: left;">Last week Tom had 74 dol- <br> lars. He washed cars over <br> the weekend and made an- <br> other 86 dollars. Tom has <br> _____ dollars now . 160</td>
</tr>
</tbody>
</table>
<p>Table 7: Examples showing MAWPS questions and corresponding questions in Completion format</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{7}$ https://beta.openai.com/docs/guides/fine-tuning&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{2}$ The recently released GPT3-Instruct, a fine-tuned model with 175B parameters produces inconsistent answers for these questions. See supplementary material: GPT3-Instruct's Response for more details.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
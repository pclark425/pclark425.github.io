<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2117 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2117</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2117</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-54.html">extraction-schema-54</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <p><strong>Paper ID:</strong> paper-276902702</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2503.05822v3.pdf" target="_blank">Unlocking the Potential of AI Researchers in Scientific Discovery: What Is Missing?</a></p>
                <p><strong>Paper Abstract:</strong> The potential of AI researchers in scientific discovery remains largely untapped. Over the past decade, AI for Science (AI4Science) publications in 145 Nature Index journals have increased fifteen-fold, yet they still account for less than 3% of the total publications. Drawing upon the Diffusion of Innovation theory, we project AI4Science's share of total publications to rise from 2.72% in 2024 to approximately 20% by 2050. Achieving this shift requires fully harnessing the potential of AI researchers, as nearly 95% of AI-driven research in these journals is led by experimental scientists. To facilitate this, we propose structured workflows and strategic interventions to position AI researchers at the forefront of scientific discovery. Specifically, we identify three critical pathways: equipping experimental scientists with accessible AI tools to amplify the impact of AI researchers, bridging cognitive and methodological gaps to enable more direct involvement in scientific discovery, and proactively fostering a thriving AI-driven scientific ecosystem. By addressing these challenges, we aim to empower AI researchers as key drivers of future scientific breakthroughs.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2117.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2117.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hybrid LLM-human classification workflow</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hybrid automated reasoning models with human verification classification workflow</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-stage pipeline combining multiple smaller reasoning LLMs, a larger LLM arbitrator, voting, prompt/search-term optimization, and targeted human verification to classify large corpora of research articles and affiliations with high accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Hybrid LLM-human classification workflow (this study)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Three 20B–32B reasoning models (Qwen3-32B, GPT-OSS-20B, DeepSeek R1-32B) classify title+abstract; if disagreement occurs a 120B model (GPT-OSS-120B) adjudicates; persistent disagreements are resolved by humans; iterative optimization of prompts/search terms and sampling-based accuracy checks are used to calibrate the pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>meta-science / bibliometrics / NLP for scientific literature</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>hybrid</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Validation consisted of (1) monitoring disagreement rates between small models (3.2% reported) and routing those cases to the large model or to human annotators, (2) random-sampling 400 LLM classifications for human assessment, and (3) performing four rounds of optimization (search terms, prompts, model usage) until classification accuracy exceeded a target threshold.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_sufficiency</strong></td>
                            <td>The paper argues this hybrid human-in-the-loop approach is sufficient for large-scale classification tasks in bibliometrics, and explicitly used a >95% accuracy threshold as an operational sufficiency criterion; domain norms favor human verification for edge cases.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_accuracy</strong></td>
                            <td>>95% classification accuracy on a random sample of 400 LLM-determined classifications after four rounds of optimization; 3.2% of cases showed significant discrepancies requiring human verification.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_details</strong></td>
                            <td>No physical experiments; validation was annotation-based (human checking) of model outputs on sampled records. The authors describe the sampling size (400) and iterative optimization but do not report further per-class error matrices.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_comparison</strong></td>
                            <td>The workflow explicitly compares outputs from small ensemble models versus a larger LLM arbitrator and human labels; the paper reports disagreement rate (3.2%) between models and that involving human verification improved final accuracy to >95%. No quantitative comparison to purely-human or purely-automated pipelines is provided beyond these figures.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_failures</strong></td>
                            <td>The authors note a non-zero disagreement fraction (3.2%) where small models conflicted and required escalation; they emphasize LLM hallucination risk and hence human oversight. No specific misclassification examples are detailed.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_cases</strong></td>
                            <td>Achieved final classification accuracy >95% on sampled checks after iterative optimization; the multi-model voting + large-model arbitration + human verification process is presented as a successful operational validation strategy.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_comparison</strong></td>
                            <td>Human annotations on a random sample (n=400) served as ground truth for accuracy estimation; no large-scale independent gold-standard dataset is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_replication</strong></td>
                            <td>The paper describes methods and sampling procedure but does not report independent replication by an external party. The authors provide workflows and meta-prompts in supplemental information to support reproducibility.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Not quantified in monetary terms; the paper notes human verification was applied only for a small fraction (3.2%) of cases, implying reduced human labor compared to full manual classification. No wall-clock or compute-cost numbers provided.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_norms</strong></td>
                            <td>For bibliometric classification, the paper treats sampled human verification and high annotation accuracy (>95%) as acceptable validation; highlights domain preference for human oversight to mitigate hallucinations.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Uncertainty is operationalized via inter-model disagreement rates; the 3.2% disagreement threshold triggered human review. No probabilistic error bars beyond sample accuracy reported.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Limited to sampled checks (n=400) for final accuracy estimation; potential remaining systematic errors outside the sample are possible. The approach relies on availability of human experts for edge cases.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_details</strong></td>
                            <td>Combines ensemble of smaller LLMs (voting) with a larger LLM arbitrator and targeted human verification on disagreements; iterative prompt/search-term optimization based on sampled accuracy checks.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2117.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2117.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-OSS-120B extraction pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-OSS-120B-based scientific term extraction and curation pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A rule-guided extraction pipeline that uses a large reasoning LLM (GPT-OSS-120B) to pull scientific terms from titles and abstracts followed by multi-step automated curation and clustering to produce a cleaned term set.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GPT-OSS-120B extraction + curation pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>GPT-OSS-120B was run with temperature 0.0 (raised to 0.3 for difficult cases) to extract terms; outputs were then cleaned (delimiter fixes, filtering malformed entries), tokenized, normalized (capitalization, punctuation removal), clustered by shared prefixes, filtered by length, and aggregated by frequency.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>scientometrics / NLP for scientific texts</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>hybrid</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Validation of extraction quality performed via multi-step curation and heuristic filters; extraction yielded 20,636 initial terms reduced to 16,598 after clustering and curation. High-frequency terms underwent minor manual adjustments (merging synonyms, capitalization).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_sufficiency</strong></td>
                            <td>Authors present the curation pipeline as sufficient to produce a reliable term set for downstream analysis; no formal precision/recall metrics are reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_accuracy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_details</strong></td>
                            <td>No wet-lab experiments; validation was computational and manual curation-based. The pipeline's output counts (pre- and post-filtering) are reported but no accuracy against a gold standard.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_comparison</strong></td>
                            <td>No explicit quantitative comparison to other extraction methods provided; the paper cites related work on LLM-based extraction improvements but does not benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_failures</strong></td>
                            <td>The authors note that some entries required raising temperature from 0.0 to 0.3 to extract appropriate terms, indicating edge-case extraction failures at stricter settings; no concrete failure rates provided.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_cases</strong></td>
                            <td>Substantial reduction from 20,636 to 16,598 clustered terms and production of high-frequency term lists used in downstream analyses (wordclouds, domain mapping).</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_comparison</strong></td>
                            <td>No explicit ground-truth dataset or reference lexicon comparisons are reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_replication</strong></td>
                            <td>Meta-prompts and pipeline steps are described and supplemental information is noted; no independent replication is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Not quantified; manual adjustments were applied only to high-frequency terms, implying targeted human effort rather than full manual curation.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_norms</strong></td>
                            <td>For large-scale term extraction from literature, multi-step automated curation plus manual spot-checking is presented as an acceptable approach in lieu of full manual annotation.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not explicitly quantified; the need to adjust temperature for some entries is discussed as an operational uncertainty control.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>No precision/recall or error-rate reporting; potential remaining noisy or missing entities outside the curated high-frequency set.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_details</strong></td>
                            <td>Automated LLM extraction followed by deterministic curation rules and limited manual post-hoc adjustments for high-frequency items.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2117.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2117.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AlphaFold</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Highly accurate protein structure prediction with AlphaFold</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A deep learning system for protein structure prediction that substantially improved computational prediction accuracy and efficiency compared to traditional physics-based methods.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Highly accurate protein structure prediction with AlphaFold</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AlphaFold</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A deep learning model for predicting protein 3D structure from amino-acid sequence, cited here as an exemplar of AI transforming structural biology.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>structural biology / computational biology</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>experimental</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>The paper references AlphaFold's improved accuracy relative to physics-based methods — implying validation through benchmarking predicted structures against experimental structural data and community CASP-style evaluation; however this survey paper does not enumerate the exact validation protocols or metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td>first-principles-approximating data-driven (high accuracy in many cases relative to prior computational methods)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_sufficiency</strong></td>
                            <td>Within structural biology norms, validation against experimentally-determined structures (X-ray/cryo-EM/NMR) and community benchmarks is the accepted standard; the survey asserts AlphaFold meets these standards but does not present new validation data.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_accuracy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_details</strong></td>
                            <td>Not performed in this survey; AlphaFold's experimental benchmarking is only cited from the original AlphaFold work (no numerical metrics reproduced here).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_comparison</strong></td>
                            <td>Paper states AlphaFold dramatically improves accuracy and efficiency compared to traditional physics-based methods, but provides no numerical comparisons itself.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_failures</strong></td>
                            <td>This survey does not report specific failure cases for AlphaFold; it simply cites its transformative impact.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_cases</strong></td>
                            <td>Cited as a successful exemplar of AI-driven validation against structural biology benchmarks and as driving adoption in the field.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_comparison</strong></td>
                            <td>Implied: comparisons against experimentally-determined structures and community benchmarks; the survey does not extract specific outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_replication</strong></td>
                            <td>Not discussed in this survey beyond citation.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_norms</strong></td>
                            <td>Structural biology expects computational predictions to be validated against experimental structural data and community benchmarks; the paper indicates AlphaFold aligns with these norms.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not described within this survey's discussion of AlphaFold.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>This paper does not detail AlphaFold limitations; it acknowledges AlphaFold as improving accuracy but does not discuss domains where it may fail.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2117.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2117.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ESMFold</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ESMFold (evolutionary-scale protein folding model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large language-model-like approach for protein structure prediction cited as another advance alongside AlphaFold improving structure prediction capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Evolutionary-scale prediction of atomic-level protein structure with a language model</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ESMFold</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Transformer-based / language-model-style system that predicts protein structure at scale, presented as complementary to AlphaFold in revolutionizing protein structure prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>structural biology / computational biology</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>experimental</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>The survey references ESMFold's gains in protein structure prediction; the original method is typically validated by comparing predictions to experimentally-determined structures and benchmark datasets, but this paper does not provide per-method validation details.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td>data-driven first-principles-approximating (high empirical fidelity reported by original authors)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_sufficiency</strong></td>
                            <td>Implied to meet community validation norms (benchmarking vs experimental structures); the survey does not present direct validation numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_accuracy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_details</strong></td>
                            <td>Not performed in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_comparison</strong></td>
                            <td>Mentioned broadly alongside AlphaFold as improving accuracy and efficiency compared to prior physics-based methods; no quantitative comparison in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_failures</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_cases</strong></td>
                            <td>Cited as part of the transformative advances in protein structure prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_comparison</strong></td>
                            <td>Not detailed in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_replication</strong></td>
                            <td>Not discussed in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Not discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_norms</strong></td>
                            <td>As above, experimental structural benchmarks are the norm; survey implies compliance via citation.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not described in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2117.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2117.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Virtual Lab (Swanson et al. 2025)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>The Virtual Lab of AI agents designs new SARS-CoV-2 nanobodies</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-PI-led, human-in-the-loop agent team that used computational design pipelines (ESM–AlphaFold–Rosetta) to propose nanobody sequences and then validated designs experimentally, yielding functional binders.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The Virtual Lab of AI agents designs new SARS-CoV-2 nanobodies</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Virtual Lab (LLM-PI-led agent team)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>An integrated agent approach combining large models and structure-design pipelines (ESM, AlphaFold, Rosetta) to design protein binders (nanobodies) with human-in-the-loop selection and downstream experimental testing.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>protein engineering / molecular biology / therapeutics</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>experimental</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Designed 92 nanobodies computationally and then performed wet-lab experiments to assess binding/function; experiments validated the presence of functional binders among the designs, including two improved binders against specific viral variants (JN.1/KP.3).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td>computational design/structure-prediction pipelines used (data-driven structure prediction coupled with energy-based design via Rosetta); experimental validation is the gold standard.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_sufficiency</strong></td>
                            <td>The paper highlights experimental validation as the ultimate safeguard; this Virtual Lab example is cited as a case where computational design was followed by experimental confirmation, aligning with domain norms that require wet-lab validation.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_accuracy</strong></td>
                            <td>Not expressed as a percent; counts given: 92 designs were produced and experiments validated functional binders among them, with at least two showing improved activity against named variants.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_details</strong></td>
                            <td>Wet-lab testing of designed nanobodies confirmed functional binders; the survey reports the numbers (92 designs, validated functional binders including two improved against variants) but does not provide detailed protocols, binding affinities, or assay types in this text.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_comparison</strong></td>
                            <td>This example is used to illustrate successful hybrid computational design followed by experimental validation. No direct numerical comparison of computational-only vs. experimentally-validated performance is provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_failures</strong></td>
                            <td>No specific failed designs or false-positive rates are detailed in this survey; the implication is that not all designs were functional but some were validated.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_cases</strong></td>
                            <td>Successful identification of experimentally-validated functional binders from computationally-designed sequences (including two with improved properties), demonstrating end-to-end validation.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_comparison</strong></td>
                            <td>Experimental assays served as ground truth for binding/function; the paper does not report benchmark metrics vs prior binders.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_replication</strong></td>
                            <td>Not discussed in this survey aside from citation of the Virtual Lab work.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Not quantified; wet-lab validation implies higher resource/time cost than purely computational steps, but specifics are not given.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_norms</strong></td>
                            <td>Protein design requires experimental assays to confirm binding/function — the paper stresses experimental validation as the ultimate safeguard in most disciplines.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not reported here for this example.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Survey does not report specific assay limitations or false-positive rates; notes general requirement for experimental validation in biology.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_details</strong></td>
                            <td>Computational design pipeline (ESM → AlphaFold → Rosetta) produced candidates which were then experimentally tested in wet-lab assays; human-in-the-loop agents mediated the process.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2117.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2117.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Autonomous laboratory (Szymanski et al. 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>An autonomous laboratory for the accelerated synthesis of novel materials</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A robotized autonomous lab that executes high-throughput synthesis and characterization workflows, enabling accelerated materials discovery with automated experimental validation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>An autonomous laboratory for the accelerated synthesis of novel materials</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Autonomous laboratory (robotics + ML)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Integration of AI-driven planning, robotics for experiment execution, and automated characterization to close the design–make–test loop for materials discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>materials science / automated experimentation</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>experimental</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Autonomous execution of synthesis and characterization experiments serves as the experimental validation step for candidate materials; the survey cites this work as demonstrating the potential of robot-driven validation but does not detail assays or metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_sufficiency</strong></td>
                            <td>In materials science, direct experimental synthesis and characterization is the accepted validation norm; autonomous labs perform these experiments to provide domain-appropriate validation.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_accuracy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_details</strong></td>
                            <td>The cited autonomous-lab work carries out real-world syntheses and characterizations; this survey references it to support claims about automated experimental validation but does not reproduce experimental protocols or results.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_comparison</strong></td>
                            <td>No explicit numeric comparison in this survey between autonomous vs. human-executed experiments; autonomous labs are presented as enabling higher throughput and reproducibility.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_failures</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_cases</strong></td>
                            <td>Cited as an initial application demonstrating transformative potential in materials discovery when coupled with AI-driven planning.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_comparison</strong></td>
                            <td>Not discussed in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_replication</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Survey suggests autonomous labs reduce manual labor and may increase reproducibility and throughput but gives no quantitative cost/time figures.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_norms</strong></td>
                            <td>Materials science expects synthesis and physical characterization as validation; autonomous labs perform those steps in an automated manner.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not discussed in this survey for this example.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Survey notes autonomous labs are in early stages; does not enumerate specific technical limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_details</strong></td>
                            <td>Autonomous labs typically combine computational planning (AI) with physical execution (robotics) and automated measurements — a hybrid computational+experimental validation loop.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2117.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2117.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SHAP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SHapley Additive exPlanations (SHAP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model-agnostic explainability method that attributes feature contributions to individual predictions and has been widely adopted in scientific domains to extract interpretable hypotheses from ML models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A Unified Approach to Interpreting Model Predictions</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SHAP explanation method</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Computes Shapley-value-based attributions to quantify each input feature's contribution to a model's output, enabling interpretation that can be used to form hypotheses or identify important variables in domain datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>cross-disciplinary (biology, materials, environmental science, chemistry)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>computational_proof</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>SHAP outputs are used to quantify contributions of metabolites to disease risk, feature importance in nanomaterial–plant–soil interactions, and ingredient contributions in materials strength tasks; validation is via consistency with domain knowledge and downstream empirical studies cited in the literature rather than independent experimental protocols presented here.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td>empirical model explanation (model-agnostic, not physics-based simulation)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_sufficiency</strong></td>
                            <td>The paper notes that for experimental scientists, explanation (e.g., SHAP) can be more valuable than marginal performance gains because it yields testable hypotheses which then require domain-standard experimental validation; SHAP alone is not a substitute for wet-lab validation.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_accuracy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_details</strong></td>
                            <td>The survey cites applications where SHAP explanations informed domain analyses; however, the survey does not present direct experimental confirmation tied to SHAP outputs within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_comparison</strong></td>
                            <td>No formal comparison between explainability methods presented here; SHAP is presented as widely adopted due to availability of packages and interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_failures</strong></td>
                            <td>Not discussed in this survey; the authors caution that explanation is a tool to generate hypotheses and requires subsequent experimental follow-up.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_cases</strong></td>
                            <td>Applications cited where SHAP was used to identify important features in metabolomics, nanomaterial interactions, and materials strength, enabling domain-relevant insights.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_comparison</strong></td>
                            <td>Not explicitly described here; explainability outputs are compared qualitatively to domain expectations in cited works but no benchmark numbers given in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_replication</strong></td>
                            <td>SHAP has standardized implementations (package link cited) facilitating reproducibility of explanations; the paper highlights the availability of the package as aiding adoption.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Computationally inexpensive relative to experimental validation; no numeric costs provided.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_norms</strong></td>
                            <td>Authors state that in experimental domains, model explanation can drive experiments but experimental/wet-lab validation remains the domain norm for confirming hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>SHAP values provide per-feature contribution scores but do not by themselves quantify statistical uncertainty of causal claims; the survey emphasizes need for downstream validation.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Explainability does not prove causality; SHAP identifies associations and contributions to model outputs but can be misleading without careful experimental follow-up.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2117.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2117.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPTCelltype / GPT-4 cell annotation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Assessing GPT-4 for cell type annotation in single-cell RNA-seq analysis</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLM-based approaches (e.g., GPT-4/GPTCelltype) applied to automate cell-type annotation from single-cell RNA-seq data, reducing expert labor required for annotation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Assessing GPT-4 for cell type annotation in single-cell RNA-seq analysis</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GPTCelltype / GPT-4-based cell-type annotation</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>An approach or R package that leverages GPT-4 to suggest cell-type labels from single-cell RNA-seq profiles or marker gene lists.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>computational biology / single-cell transcriptomics</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>computational_proof</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Implied validation by comparing automated annotations to expert or reference annotations; the survey cites the tool as automating accurate cell-type annotation but provides no metrics here.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td>data-driven annotation (empirical performance evaluated against curated annotations)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_sufficiency</strong></td>
                            <td>Single-cell community norms favor benchmarking against curated reference annotations and expert labels; the survey presents the approach as promising but does not detail sufficiency of validation.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_accuracy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_details</strong></td>
                            <td>No wet-lab experiments; validation is computational comparison to reference annotations in cited work (details not reproduced in this survey).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_comparison</strong></td>
                            <td>Not presented in this paper; only cited as an example of LLM utility in biomedical data analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_failures</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_cases</strong></td>
                            <td>Described qualitatively as automating accurate annotation and reducing required expertise.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_comparison</strong></td>
                            <td>Implicit (comparisons vs expert-labeled annotations) but not quantitatively given in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_replication</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Not discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_norms</strong></td>
                            <td>Benchmarking against curated datasets and expert annotations is the domain norm; the survey implies this is how such tools are assessed.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not described in this survey for the tool.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Authors note LLMs can generate false information and rely on outdated data, implying potential pitfalls in annotation without careful retrieval-augmentation and validation.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2117.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e2117.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PathCha / pathology copilot</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PathCha / A multimodal generative AI copilot for human pathology</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A vision-language AI assistant tailored for pathology that integrates a vision encoder with a pretrained LLM to support diagnosis and user workflows, reported to show strong diagnostic accuracy and user preference.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A multimodal generative AI copilot for human pathology</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>PathCha (vision–language pathology AI assistant)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Combines a specialized visual encoder and an LLM to provide diagnostic assistance in pathology, evaluated on diagnostic accuracy and user preference metrics in the cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>pathology / medical imaging / clinical AI</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>experimental</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>The survey reports PathCha 'excels in diagnostic accuracy and user preference' — implying validation through diagnostic accuracy testing (likely against labeled pathology cases) and human user studies, though the survey does not provide numerical results or protocol details.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_sufficiency</strong></td>
                            <td>Clinical AI norms require evaluation on held-out clinical datasets and human-subject studies for user preference; the survey implies such evaluations were done in the cited work but does not reproduce details.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_accuracy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_details</strong></td>
                            <td>Not provided in this survey; cited work presumably includes diagnostic accuracy experiments and user preference studies.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_comparison</strong></td>
                            <td>Not detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_failures</strong></td>
                            <td>Not discussed in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_cases</strong></td>
                            <td>Reported qualitatively as excelling in diagnostic accuracy and user preference in cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_comparison</strong></td>
                            <td>Implied comparison to labeled diagnostic ground truth in pathology datasets; not quantified in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_replication</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Not discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_norms</strong></td>
                            <td>Medical diagnostics require rigorous dataset-based evaluation and human factors studies prior to clinical deployment; the survey highlights PathCha as evaluated in these dimensions in its original paper.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not described in this survey's summary.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Survey warns generally about LLM hallucinations and outdated knowledge which can impact clinical systems; PathCha-specific limitations are not enumerated here.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>The Virtual Lab of AI agents designs new SARS-CoV-2 nanobodies <em>(Rating: 2)</em></li>
                <li>An autonomous laboratory for the accelerated synthesis of novel materials <em>(Rating: 2)</em></li>
                <li>Closed-loop transfer enables artificial intelligence to yield chemical knowledge <em>(Rating: 2)</em></li>
                <li>Extracting accurate materials data from research papers with conversational language models and prompt engineering <em>(Rating: 2)</em></li>
                <li>Highly accurate protein structure prediction with AlphaFold <em>(Rating: 2)</em></li>
                <li>A Unified Approach to Interpreting Model Predictions <em>(Rating: 2)</em></li>
                <li>Assessing GPT-4 for cell type annotation in single-cell RNA-seq analysis <em>(Rating: 1)</em></li>
                <li>A multimodal generative AI copilot for human pathology <em>(Rating: 1)</em></li>
                <li>Detecting hallucinations in large language models using semantic entropy <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2117",
    "paper_id": "paper-276902702",
    "extraction_schema_id": "extraction-schema-54",
    "extracted_data": [
        {
            "name_short": "Hybrid LLM-human classification workflow",
            "name_full": "Hybrid automated reasoning models with human verification classification workflow",
            "brief_description": "A multi-stage pipeline combining multiple smaller reasoning LLMs, a larger LLM arbitrator, voting, prompt/search-term optimization, and targeted human verification to classify large corpora of research articles and affiliations with high accuracy.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Hybrid LLM-human classification workflow (this study)",
            "system_description": "Three 20B–32B reasoning models (Qwen3-32B, GPT-OSS-20B, DeepSeek R1-32B) classify title+abstract; if disagreement occurs a 120B model (GPT-OSS-120B) adjudicates; persistent disagreements are resolved by humans; iterative optimization of prompts/search terms and sampling-based accuracy checks are used to calibrate the pipeline.",
            "scientific_domain": "meta-science / bibliometrics / NLP for scientific literature",
            "validation_type": "hybrid",
            "validation_description": "Validation consisted of (1) monitoring disagreement rates between small models (3.2% reported) and routing those cases to the large model or to human annotators, (2) random-sampling 400 LLM classifications for human assessment, and (3) performing four rounds of optimization (search terms, prompts, model usage) until classification accuracy exceeded a target threshold.",
            "simulation_fidelity": null,
            "validation_sufficiency": "The paper argues this hybrid human-in-the-loop approach is sufficient for large-scale classification tasks in bibliometrics, and explicitly used a &gt;95% accuracy threshold as an operational sufficiency criterion; domain norms favor human verification for edge cases.",
            "validation_accuracy": "&gt;95% classification accuracy on a random sample of 400 LLM-determined classifications after four rounds of optimization; 3.2% of cases showed significant discrepancies requiring human verification.",
            "experimental_validation_performed": false,
            "experimental_validation_details": "No physical experiments; validation was annotation-based (human checking) of model outputs on sampled records. The authors describe the sampling size (400) and iterative optimization but do not report further per-class error matrices.",
            "validation_comparison": "The workflow explicitly compares outputs from small ensemble models versus a larger LLM arbitrator and human labels; the paper reports disagreement rate (3.2%) between models and that involving human verification improved final accuracy to &gt;95%. No quantitative comparison to purely-human or purely-automated pipelines is provided beyond these figures.",
            "validation_failures": "The authors note a non-zero disagreement fraction (3.2%) where small models conflicted and required escalation; they emphasize LLM hallucination risk and hence human oversight. No specific misclassification examples are detailed.",
            "validation_success_cases": "Achieved final classification accuracy &gt;95% on sampled checks after iterative optimization; the multi-model voting + large-model arbitration + human verification process is presented as a successful operational validation strategy.",
            "ground_truth_comparison": "Human annotations on a random sample (n=400) served as ground truth for accuracy estimation; no large-scale independent gold-standard dataset is reported.",
            "reproducibility_replication": "The paper describes methods and sampling procedure but does not report independent replication by an external party. The authors provide workflows and meta-prompts in supplemental information to support reproducibility.",
            "validation_cost_time": "Not quantified in monetary terms; the paper notes human verification was applied only for a small fraction (3.2%) of cases, implying reduced human labor compared to full manual classification. No wall-clock or compute-cost numbers provided.",
            "domain_validation_norms": "For bibliometric classification, the paper treats sampled human verification and high annotation accuracy (&gt;95%) as acceptable validation; highlights domain preference for human oversight to mitigate hallucinations.",
            "uncertainty_quantification": "Uncertainty is operationalized via inter-model disagreement rates; the 3.2% disagreement threshold triggered human review. No probabilistic error bars beyond sample accuracy reported.",
            "validation_limitations": "Limited to sampled checks (n=400) for final accuracy estimation; potential remaining systematic errors outside the sample are possible. The approach relies on availability of human experts for edge cases.",
            "hybrid_validation_approach": true,
            "hybrid_validation_details": "Combines ensemble of smaller LLMs (voting) with a larger LLM arbitrator and targeted human verification on disagreements; iterative prompt/search-term optimization based on sampled accuracy checks.",
            "uuid": "e2117.0"
        },
        {
            "name_short": "GPT-OSS-120B extraction pipeline",
            "name_full": "GPT-OSS-120B-based scientific term extraction and curation pipeline",
            "brief_description": "A rule-guided extraction pipeline that uses a large reasoning LLM (GPT-OSS-120B) to pull scientific terms from titles and abstracts followed by multi-step automated curation and clustering to produce a cleaned term set.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "GPT-OSS-120B extraction + curation pipeline",
            "system_description": "GPT-OSS-120B was run with temperature 0.0 (raised to 0.3 for difficult cases) to extract terms; outputs were then cleaned (delimiter fixes, filtering malformed entries), tokenized, normalized (capitalization, punctuation removal), clustered by shared prefixes, filtered by length, and aggregated by frequency.",
            "scientific_domain": "scientometrics / NLP for scientific texts",
            "validation_type": "hybrid",
            "validation_description": "Validation of extraction quality performed via multi-step curation and heuristic filters; extraction yielded 20,636 initial terms reduced to 16,598 after clustering and curation. High-frequency terms underwent minor manual adjustments (merging synonyms, capitalization).",
            "simulation_fidelity": null,
            "validation_sufficiency": "Authors present the curation pipeline as sufficient to produce a reliable term set for downstream analysis; no formal precision/recall metrics are reported in this paper.",
            "validation_accuracy": null,
            "experimental_validation_performed": false,
            "experimental_validation_details": "No wet-lab experiments; validation was computational and manual curation-based. The pipeline's output counts (pre- and post-filtering) are reported but no accuracy against a gold standard.",
            "validation_comparison": "No explicit quantitative comparison to other extraction methods provided; the paper cites related work on LLM-based extraction improvements but does not benchmark.",
            "validation_failures": "The authors note that some entries required raising temperature from 0.0 to 0.3 to extract appropriate terms, indicating edge-case extraction failures at stricter settings; no concrete failure rates provided.",
            "validation_success_cases": "Substantial reduction from 20,636 to 16,598 clustered terms and production of high-frequency term lists used in downstream analyses (wordclouds, domain mapping).",
            "ground_truth_comparison": "No explicit ground-truth dataset or reference lexicon comparisons are reported in this paper.",
            "reproducibility_replication": "Meta-prompts and pipeline steps are described and supplemental information is noted; no independent replication is reported.",
            "validation_cost_time": "Not quantified; manual adjustments were applied only to high-frequency terms, implying targeted human effort rather than full manual curation.",
            "domain_validation_norms": "For large-scale term extraction from literature, multi-step automated curation plus manual spot-checking is presented as an acceptable approach in lieu of full manual annotation.",
            "uncertainty_quantification": "Not explicitly quantified; the need to adjust temperature for some entries is discussed as an operational uncertainty control.",
            "validation_limitations": "No precision/recall or error-rate reporting; potential remaining noisy or missing entities outside the curated high-frequency set.",
            "hybrid_validation_approach": true,
            "hybrid_validation_details": "Automated LLM extraction followed by deterministic curation rules and limited manual post-hoc adjustments for high-frequency items.",
            "uuid": "e2117.1"
        },
        {
            "name_short": "AlphaFold",
            "name_full": "Highly accurate protein structure prediction with AlphaFold",
            "brief_description": "A deep learning system for protein structure prediction that substantially improved computational prediction accuracy and efficiency compared to traditional physics-based methods.",
            "citation_title": "Highly accurate protein structure prediction with AlphaFold",
            "mention_or_use": "mention",
            "system_name": "AlphaFold",
            "system_description": "A deep learning model for predicting protein 3D structure from amino-acid sequence, cited here as an exemplar of AI transforming structural biology.",
            "scientific_domain": "structural biology / computational biology",
            "validation_type": "experimental",
            "validation_description": "The paper references AlphaFold's improved accuracy relative to physics-based methods — implying validation through benchmarking predicted structures against experimental structural data and community CASP-style evaluation; however this survey paper does not enumerate the exact validation protocols or metrics.",
            "simulation_fidelity": "first-principles-approximating data-driven (high accuracy in many cases relative to prior computational methods)",
            "validation_sufficiency": "Within structural biology norms, validation against experimentally-determined structures (X-ray/cryo-EM/NMR) and community benchmarks is the accepted standard; the survey asserts AlphaFold meets these standards but does not present new validation data.",
            "validation_accuracy": null,
            "experimental_validation_performed": null,
            "experimental_validation_details": "Not performed in this survey; AlphaFold's experimental benchmarking is only cited from the original AlphaFold work (no numerical metrics reproduced here).",
            "validation_comparison": "Paper states AlphaFold dramatically improves accuracy and efficiency compared to traditional physics-based methods, but provides no numerical comparisons itself.",
            "validation_failures": "This survey does not report specific failure cases for AlphaFold; it simply cites its transformative impact.",
            "validation_success_cases": "Cited as a successful exemplar of AI-driven validation against structural biology benchmarks and as driving adoption in the field.",
            "ground_truth_comparison": "Implied: comparisons against experimentally-determined structures and community benchmarks; the survey does not extract specific outcomes.",
            "reproducibility_replication": "Not discussed in this survey beyond citation.",
            "validation_cost_time": "Not discussed here.",
            "domain_validation_norms": "Structural biology expects computational predictions to be validated against experimental structural data and community benchmarks; the paper indicates AlphaFold aligns with these norms.",
            "uncertainty_quantification": "Not described within this survey's discussion of AlphaFold.",
            "validation_limitations": "This paper does not detail AlphaFold limitations; it acknowledges AlphaFold as improving accuracy but does not discuss domains where it may fail.",
            "hybrid_validation_approach": null,
            "hybrid_validation_details": null,
            "uuid": "e2117.2"
        },
        {
            "name_short": "ESMFold",
            "name_full": "ESMFold (evolutionary-scale protein folding model)",
            "brief_description": "A large language-model-like approach for protein structure prediction cited as another advance alongside AlphaFold improving structure prediction capabilities.",
            "citation_title": "Evolutionary-scale prediction of atomic-level protein structure with a language model",
            "mention_or_use": "mention",
            "system_name": "ESMFold",
            "system_description": "Transformer-based / language-model-style system that predicts protein structure at scale, presented as complementary to AlphaFold in revolutionizing protein structure prediction.",
            "scientific_domain": "structural biology / computational biology",
            "validation_type": "experimental",
            "validation_description": "The survey references ESMFold's gains in protein structure prediction; the original method is typically validated by comparing predictions to experimentally-determined structures and benchmark datasets, but this paper does not provide per-method validation details.",
            "simulation_fidelity": "data-driven first-principles-approximating (high empirical fidelity reported by original authors)",
            "validation_sufficiency": "Implied to meet community validation norms (benchmarking vs experimental structures); the survey does not present direct validation numbers.",
            "validation_accuracy": null,
            "experimental_validation_performed": null,
            "experimental_validation_details": "Not performed in this survey.",
            "validation_comparison": "Mentioned broadly alongside AlphaFold as improving accuracy and efficiency compared to prior physics-based methods; no quantitative comparison in this paper.",
            "validation_failures": "Not discussed here.",
            "validation_success_cases": "Cited as part of the transformative advances in protein structure prediction.",
            "ground_truth_comparison": "Not detailed in this survey.",
            "reproducibility_replication": "Not discussed in this survey.",
            "validation_cost_time": "Not discussed.",
            "domain_validation_norms": "As above, experimental structural benchmarks are the norm; survey implies compliance via citation.",
            "uncertainty_quantification": "Not described in this survey.",
            "validation_limitations": "Not discussed here.",
            "hybrid_validation_approach": null,
            "hybrid_validation_details": null,
            "uuid": "e2117.3"
        },
        {
            "name_short": "Virtual Lab (Swanson et al. 2025)",
            "name_full": "The Virtual Lab of AI agents designs new SARS-CoV-2 nanobodies",
            "brief_description": "An LLM-PI-led, human-in-the-loop agent team that used computational design pipelines (ESM–AlphaFold–Rosetta) to propose nanobody sequences and then validated designs experimentally, yielding functional binders.",
            "citation_title": "The Virtual Lab of AI agents designs new SARS-CoV-2 nanobodies",
            "mention_or_use": "mention",
            "system_name": "Virtual Lab (LLM-PI-led agent team)",
            "system_description": "An integrated agent approach combining large models and structure-design pipelines (ESM, AlphaFold, Rosetta) to design protein binders (nanobodies) with human-in-the-loop selection and downstream experimental testing.",
            "scientific_domain": "protein engineering / molecular biology / therapeutics",
            "validation_type": "experimental",
            "validation_description": "Designed 92 nanobodies computationally and then performed wet-lab experiments to assess binding/function; experiments validated the presence of functional binders among the designs, including two improved binders against specific viral variants (JN.1/KP.3).",
            "simulation_fidelity": "computational design/structure-prediction pipelines used (data-driven structure prediction coupled with energy-based design via Rosetta); experimental validation is the gold standard.",
            "validation_sufficiency": "The paper highlights experimental validation as the ultimate safeguard; this Virtual Lab example is cited as a case where computational design was followed by experimental confirmation, aligning with domain norms that require wet-lab validation.",
            "validation_accuracy": "Not expressed as a percent; counts given: 92 designs were produced and experiments validated functional binders among them, with at least two showing improved activity against named variants.",
            "experimental_validation_performed": true,
            "experimental_validation_details": "Wet-lab testing of designed nanobodies confirmed functional binders; the survey reports the numbers (92 designs, validated functional binders including two improved against variants) but does not provide detailed protocols, binding affinities, or assay types in this text.",
            "validation_comparison": "This example is used to illustrate successful hybrid computational design followed by experimental validation. No direct numerical comparison of computational-only vs. experimentally-validated performance is provided here.",
            "validation_failures": "No specific failed designs or false-positive rates are detailed in this survey; the implication is that not all designs were functional but some were validated.",
            "validation_success_cases": "Successful identification of experimentally-validated functional binders from computationally-designed sequences (including two with improved properties), demonstrating end-to-end validation.",
            "ground_truth_comparison": "Experimental assays served as ground truth for binding/function; the paper does not report benchmark metrics vs prior binders.",
            "reproducibility_replication": "Not discussed in this survey aside from citation of the Virtual Lab work.",
            "validation_cost_time": "Not quantified; wet-lab validation implies higher resource/time cost than purely computational steps, but specifics are not given.",
            "domain_validation_norms": "Protein design requires experimental assays to confirm binding/function — the paper stresses experimental validation as the ultimate safeguard in most disciplines.",
            "uncertainty_quantification": "Not reported here for this example.",
            "validation_limitations": "Survey does not report specific assay limitations or false-positive rates; notes general requirement for experimental validation in biology.",
            "hybrid_validation_approach": true,
            "hybrid_validation_details": "Computational design pipeline (ESM → AlphaFold → Rosetta) produced candidates which were then experimentally tested in wet-lab assays; human-in-the-loop agents mediated the process.",
            "uuid": "e2117.4"
        },
        {
            "name_short": "Autonomous laboratory (Szymanski et al. 2023)",
            "name_full": "An autonomous laboratory for the accelerated synthesis of novel materials",
            "brief_description": "A robotized autonomous lab that executes high-throughput synthesis and characterization workflows, enabling accelerated materials discovery with automated experimental validation.",
            "citation_title": "An autonomous laboratory for the accelerated synthesis of novel materials",
            "mention_or_use": "mention",
            "system_name": "Autonomous laboratory (robotics + ML)",
            "system_description": "Integration of AI-driven planning, robotics for experiment execution, and automated characterization to close the design–make–test loop for materials discovery.",
            "scientific_domain": "materials science / automated experimentation",
            "validation_type": "experimental",
            "validation_description": "Autonomous execution of synthesis and characterization experiments serves as the experimental validation step for candidate materials; the survey cites this work as demonstrating the potential of robot-driven validation but does not detail assays or metrics.",
            "simulation_fidelity": null,
            "validation_sufficiency": "In materials science, direct experimental synthesis and characterization is the accepted validation norm; autonomous labs perform these experiments to provide domain-appropriate validation.",
            "validation_accuracy": null,
            "experimental_validation_performed": true,
            "experimental_validation_details": "The cited autonomous-lab work carries out real-world syntheses and characterizations; this survey references it to support claims about automated experimental validation but does not reproduce experimental protocols or results.",
            "validation_comparison": "No explicit numeric comparison in this survey between autonomous vs. human-executed experiments; autonomous labs are presented as enabling higher throughput and reproducibility.",
            "validation_failures": "Not discussed here.",
            "validation_success_cases": "Cited as an initial application demonstrating transformative potential in materials discovery when coupled with AI-driven planning.",
            "ground_truth_comparison": "Not discussed in this survey.",
            "reproducibility_replication": "Not discussed here.",
            "validation_cost_time": "Survey suggests autonomous labs reduce manual labor and may increase reproducibility and throughput but gives no quantitative cost/time figures.",
            "domain_validation_norms": "Materials science expects synthesis and physical characterization as validation; autonomous labs perform those steps in an automated manner.",
            "uncertainty_quantification": "Not discussed in this survey for this example.",
            "validation_limitations": "Survey notes autonomous labs are in early stages; does not enumerate specific technical limitations.",
            "hybrid_validation_approach": true,
            "hybrid_validation_details": "Autonomous labs typically combine computational planning (AI) with physical execution (robotics) and automated measurements — a hybrid computational+experimental validation loop.",
            "uuid": "e2117.5"
        },
        {
            "name_short": "SHAP",
            "name_full": "SHapley Additive exPlanations (SHAP)",
            "brief_description": "A model-agnostic explainability method that attributes feature contributions to individual predictions and has been widely adopted in scientific domains to extract interpretable hypotheses from ML models.",
            "citation_title": "A Unified Approach to Interpreting Model Predictions",
            "mention_or_use": "mention",
            "system_name": "SHAP explanation method",
            "system_description": "Computes Shapley-value-based attributions to quantify each input feature's contribution to a model's output, enabling interpretation that can be used to form hypotheses or identify important variables in domain datasets.",
            "scientific_domain": "cross-disciplinary (biology, materials, environmental science, chemistry)",
            "validation_type": "computational_proof",
            "validation_description": "SHAP outputs are used to quantify contributions of metabolites to disease risk, feature importance in nanomaterial–plant–soil interactions, and ingredient contributions in materials strength tasks; validation is via consistency with domain knowledge and downstream empirical studies cited in the literature rather than independent experimental protocols presented here.",
            "simulation_fidelity": "empirical model explanation (model-agnostic, not physics-based simulation)",
            "validation_sufficiency": "The paper notes that for experimental scientists, explanation (e.g., SHAP) can be more valuable than marginal performance gains because it yields testable hypotheses which then require domain-standard experimental validation; SHAP alone is not a substitute for wet-lab validation.",
            "validation_accuracy": null,
            "experimental_validation_performed": false,
            "experimental_validation_details": "The survey cites applications where SHAP explanations informed domain analyses; however, the survey does not present direct experimental confirmation tied to SHAP outputs within this paper.",
            "validation_comparison": "No formal comparison between explainability methods presented here; SHAP is presented as widely adopted due to availability of packages and interpretability.",
            "validation_failures": "Not discussed in this survey; the authors caution that explanation is a tool to generate hypotheses and requires subsequent experimental follow-up.",
            "validation_success_cases": "Applications cited where SHAP was used to identify important features in metabolomics, nanomaterial interactions, and materials strength, enabling domain-relevant insights.",
            "ground_truth_comparison": "Not explicitly described here; explainability outputs are compared qualitatively to domain expectations in cited works but no benchmark numbers given in this survey.",
            "reproducibility_replication": "SHAP has standardized implementations (package link cited) facilitating reproducibility of explanations; the paper highlights the availability of the package as aiding adoption.",
            "validation_cost_time": "Computationally inexpensive relative to experimental validation; no numeric costs provided.",
            "domain_validation_norms": "Authors state that in experimental domains, model explanation can drive experiments but experimental/wet-lab validation remains the domain norm for confirming hypotheses.",
            "uncertainty_quantification": "SHAP values provide per-feature contribution scores but do not by themselves quantify statistical uncertainty of causal claims; the survey emphasizes need for downstream validation.",
            "validation_limitations": "Explainability does not prove causality; SHAP identifies associations and contributions to model outputs but can be misleading without careful experimental follow-up.",
            "hybrid_validation_approach": false,
            "hybrid_validation_details": null,
            "uuid": "e2117.6"
        },
        {
            "name_short": "GPTCelltype / GPT-4 cell annotation",
            "name_full": "Assessing GPT-4 for cell type annotation in single-cell RNA-seq analysis",
            "brief_description": "LLM-based approaches (e.g., GPT-4/GPTCelltype) applied to automate cell-type annotation from single-cell RNA-seq data, reducing expert labor required for annotation.",
            "citation_title": "Assessing GPT-4 for cell type annotation in single-cell RNA-seq analysis",
            "mention_or_use": "mention",
            "system_name": "GPTCelltype / GPT-4-based cell-type annotation",
            "system_description": "An approach or R package that leverages GPT-4 to suggest cell-type labels from single-cell RNA-seq profiles or marker gene lists.",
            "scientific_domain": "computational biology / single-cell transcriptomics",
            "validation_type": "computational_proof",
            "validation_description": "Implied validation by comparing automated annotations to expert or reference annotations; the survey cites the tool as automating accurate cell-type annotation but provides no metrics here.",
            "simulation_fidelity": "data-driven annotation (empirical performance evaluated against curated annotations)",
            "validation_sufficiency": "Single-cell community norms favor benchmarking against curated reference annotations and expert labels; the survey presents the approach as promising but does not detail sufficiency of validation.",
            "validation_accuracy": null,
            "experimental_validation_performed": false,
            "experimental_validation_details": "No wet-lab experiments; validation is computational comparison to reference annotations in cited work (details not reproduced in this survey).",
            "validation_comparison": "Not presented in this paper; only cited as an example of LLM utility in biomedical data analysis.",
            "validation_failures": "Not discussed here.",
            "validation_success_cases": "Described qualitatively as automating accurate annotation and reducing required expertise.",
            "ground_truth_comparison": "Implicit (comparisons vs expert-labeled annotations) but not quantitatively given in this survey.",
            "reproducibility_replication": "Not discussed here.",
            "validation_cost_time": "Not discussed.",
            "domain_validation_norms": "Benchmarking against curated datasets and expert annotations is the domain norm; the survey implies this is how such tools are assessed.",
            "uncertainty_quantification": "Not described in this survey for the tool.",
            "validation_limitations": "Authors note LLMs can generate false information and rely on outdated data, implying potential pitfalls in annotation without careful retrieval-augmentation and validation.",
            "hybrid_validation_approach": false,
            "hybrid_validation_details": null,
            "uuid": "e2117.7"
        },
        {
            "name_short": "PathCha / pathology copilot",
            "name_full": "PathCha / A multimodal generative AI copilot for human pathology",
            "brief_description": "A vision-language AI assistant tailored for pathology that integrates a vision encoder with a pretrained LLM to support diagnosis and user workflows, reported to show strong diagnostic accuracy and user preference.",
            "citation_title": "A multimodal generative AI copilot for human pathology",
            "mention_or_use": "mention",
            "system_name": "PathCha (vision–language pathology AI assistant)",
            "system_description": "Combines a specialized visual encoder and an LLM to provide diagnostic assistance in pathology, evaluated on diagnostic accuracy and user preference metrics in the cited work.",
            "scientific_domain": "pathology / medical imaging / clinical AI",
            "validation_type": "experimental",
            "validation_description": "The survey reports PathCha 'excels in diagnostic accuracy and user preference' — implying validation through diagnostic accuracy testing (likely against labeled pathology cases) and human user studies, though the survey does not provide numerical results or protocol details.",
            "simulation_fidelity": null,
            "validation_sufficiency": "Clinical AI norms require evaluation on held-out clinical datasets and human-subject studies for user preference; the survey implies such evaluations were done in the cited work but does not reproduce details.",
            "validation_accuracy": null,
            "experimental_validation_performed": null,
            "experimental_validation_details": "Not provided in this survey; cited work presumably includes diagnostic accuracy experiments and user preference studies.",
            "validation_comparison": "Not detailed here.",
            "validation_failures": "Not discussed in this survey.",
            "validation_success_cases": "Reported qualitatively as excelling in diagnostic accuracy and user preference in cited work.",
            "ground_truth_comparison": "Implied comparison to labeled diagnostic ground truth in pathology datasets; not quantified in this survey.",
            "reproducibility_replication": "Not discussed here.",
            "validation_cost_time": "Not discussed.",
            "domain_validation_norms": "Medical diagnostics require rigorous dataset-based evaluation and human factors studies prior to clinical deployment; the survey highlights PathCha as evaluated in these dimensions in its original paper.",
            "uncertainty_quantification": "Not described in this survey's summary.",
            "validation_limitations": "Survey warns generally about LLM hallucinations and outdated knowledge which can impact clinical systems; PathCha-specific limitations are not enumerated here.",
            "hybrid_validation_approach": null,
            "hybrid_validation_details": null,
            "uuid": "e2117.8"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "The Virtual Lab of AI agents designs new SARS-CoV-2 nanobodies",
            "rating": 2
        },
        {
            "paper_title": "An autonomous laboratory for the accelerated synthesis of novel materials",
            "rating": 2
        },
        {
            "paper_title": "Closed-loop transfer enables artificial intelligence to yield chemical knowledge",
            "rating": 2
        },
        {
            "paper_title": "Extracting accurate materials data from research papers with conversational language models and prompt engineering",
            "rating": 2
        },
        {
            "paper_title": "Highly accurate protein structure prediction with AlphaFold",
            "rating": 2
        },
        {
            "paper_title": "A Unified Approach to Interpreting Model Predictions",
            "rating": 2
        },
        {
            "paper_title": "Assessing GPT-4 for cell type annotation in single-cell RNA-seq analysis",
            "rating": 1
        },
        {
            "paper_title": "A multimodal generative AI copilot for human pathology",
            "rating": 1
        },
        {
            "paper_title": "Detecting hallucinations in large language models using semantic entropy",
            "rating": 1
        }
    ],
    "cost": 0.02203925,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Unlocking the Potential of AI Researchers in Scientific Discovery: What Is Missing?</p>
<p>Hengjie Yu 
School of Engineering
Westlake University
310030HangzhouZhejiangChina</p>
<p>Institute of Advanced Technology
Westlake Institute for Advanced Study
310024HangzhouZhejiangChina</p>
<p>Shuya Liu 
School of Engineering
Westlake University
310030HangzhouZhejiangChina</p>
<p>Haiyun Yang 
School of Engineering
Westlake University
310030HangzhouZhejiangChina</p>
<p>Yuping Yan 
Institute of Advanced Technology
Westlake Institute for Advanced Study
310024HangzhouZhejiangChina</p>
<p>Maozhen Qu 
School of Engineering
Westlake University
310030HangzhouZhejiangChina</p>
<p>Department of Chemistry
National University of Singapore
3 Science Drive 3117543Singapore, Singapore</p>
<p>Yaochu Jin jinyaochu@westlake.edu.cn 
School of Engineering
Westlake University
310030HangzhouZhejiangChina</p>
<p>Institute of Advanced Technology
Westlake Institute for Advanced Study
310024HangzhouZhejiangChina</p>
<p>Unlocking the Potential of AI Researchers in Scientific Discovery: What Is Missing?
38E6DB9AAC3AC4BCD076BB6B03288AA4AI for ScienceScientific discoveryDiffusion of InnovationResearch paradigmAI researcher
The potential of AI researchers in scientific discovery remains largely untapped.Over the past decade, AI for Science (AI4Science) publications in 145 Nature Index journals have increased fifteen-fold, yet they still account for less than 3% of the total publications.Drawing upon the Diffusion of Innovation theory, we project AI4Science's share of total publications to rise from 2.72% in 2024 to approximately 20% by 2050.Achieving this shift requires fully harnessing the potential of AI researchers, as nearly 95% of AI-driven research in these journals is led by experimental scientists.To facilitate this, we propose structured workflows and strategic interventions to position AI researchers at the forefront of scientific discovery.Specifically, we identify three critical pathways: equipping experimental scientists with accessible AI tools to amplify the impact of AI researchers, bridging cognitive and methodological gaps to enable more direct involvement in scientific discovery, and proactively fostering a thriving AI-driven scientific ecosystem.By addressing these challenges, we aim to empower AI researchers as key drivers of future scientific breakthroughs.</p>
<p>Introduction</p>
<p>Over the past decade, AI has become a powerful catalyst for scientific discovery (Gil et al. 2014;Xu et al. 2021;Wang et al. 2023).Since 2015, its adoption and impact have expanded rapidly across scientific disciplines, driving unprecedented growth (Gao and Wang 2024).This surge has fueled the emergence of AI for Science (AI4Science), a field that leverages machine learning techniques to tackle complex data challenges and uncover insights that were previously beyond reach.The impact of AI4Science is exemplified by the success of deep learning models in structural biology, chemistry, and biomedical research.For instance, AlphaFold (Jumper et al. 2021) and ESMFold (Lin et al. 2023) have revolutionized protein structure prediction, dramatically improving the accuracy and efficiency of computational modeling compared to traditional physics-based methods.Similarly, in molecular representation learning, models like MolCLR (Wang et al. 2022) leverage contrastive learning to enhance the predictive power of molecular property predictions, facilitating drug discovery and materials science applications.Beyond structural and molecular biology, AI, especially large language models (LLMs), has also demonstrated remarkable utility in biomedical data analysis.GPTCelltype (Hou and Ji 2024), an R package utilizing GPT-4, automates accurate cell type annotation from single-cell RNA sequencing data, greatly reducing the effort and expertise needed for this task.PathCha (Lu et al. 2024), a visionlanguage AI assistant tailored for human pathology, excels in diagnostic accuracy and user preference by integrating a specialized vision encoder with a pretrained LLM.AI has been recognized as a transformative technology and powerful paradigm in various scientific fields (Xu et al. 2021), such as health and medicine (Rajpurkar et al. 2022), materials (Choudhary et al. 2022), biology (Angermueller et al. 2016), chemistry (Baum et al. 2021), and the environment (Konya and Nematzadeh 2024).</p>
<p>Transformative technologies have periodically reshaped scientific discovery.Computational modeling and numerical simulation, such as density functional theory (Huang et al. 2023), revolutionized research in physics, chemistry, and engineering by enabling large-scale predictive simulations.The 1990s brought high-throughput experimental techniques like next-generation sequencing (Satam et al. 2023) and mass spectrometry (Heuckeroth et al. 2024), shifting biological science toward data-driven exploration.The early 2000s saw the rise of statistical learning and data science (Edfeldt et al. 2024), enhancing pattern recognition and predictive modeling across disciplines.As AI continues to reshape scientific discovery, it is essential to assess both the scope of its contributions and the challenges that remain.Moreover, "I sensed anxiety and frustration at NeurIPS'24," remarked Kyunghyun Cho (Cho 2024), receiving great attention and reflecting on the intense competition in AI research.Doctoral candidates nearing graduation and early-career AI researchers face an increasingly challenging academic and industrial landscape and struggle to secure positions due to the rapid saturation of talent in core AI fields.To ensure the long-term sustainability of the AI talent pipeline, it is crucial to broaden the application of AI across diverse fields.AI has already demonstrated significant value in accelerating scientific discovery.This article explores two fundamental questions: To what extent has AI advanced scientific discovery, and what potential remains for further expansion and innovation?Additionally, what gaps still exist in the engagement of AI researchers with scientific discovery, and how can these be bridged?</p>
<p>To answer these questions, this research examines AI-related research papers published in leading natural and health science journals over the past decade.By analyzing these publications and their author affiliations, we aim to understand the current progress of AI4Science within high-impact research and identify pathways for AI researchers to engage in this field.Furthermore, we offer practical guidance and a structured workflow to support AI researchers in embarking on scientific discovery.By highlighting key entry points and methodological approaches, we aim to lower the barriers for AI researchers seeking to contribute to scientific advancements.Ultimately, we hope this survey serves as both an inspiration and a resource, facilitating broader integration of AI into scientific discovery while simultaneously expanding the frontiers of human knowledge, fostering mutual benefits for both AI researchers and the broader scientific community.</p>
<p>Results and discussion</p>
<p>AI4Science: A rapidly growing, yet relatively minor component of scientific discovery</p>
<p>The 145 Nature Index journals, selected by an expert panel of active scientists for their reputation and impact, have long been central venues for groundbreaking research in the natural and health sciences.To examine the role of AI in high-impact scientific discovery, we compiled a dataset of 20,603 AI-related research articles published in these journals over the past decade, drawing from the Web of Science Core Collection.Although our search terms targeted AI-related studies, not all retrieved articles fell into the category of AI for Science.A classification of research types was therefore essential.Here, we defined four categories of research: Typical AI Research refers to studies focused on the development or analysis of AI methods, such as machine learning, neural networks, deep learning, algorithms, or hardware, without direct application to natural or health sciences; AI for Science encompasses studies that apply AI techniques to address scientific challenges in fields such as materials science, chemistry, physics, biology, and medicine, where AI is explicitly used to enhance discovery, prediction, data analysis, or scientific understanding; Science for AI denotes research that leverages findings from natural or cognitive sciences to inspire or improve AI models, architectures, or hardware, with the goal of advancing AI itself rather than solving scientific problems; Not Applicable covers studies outside these domains, including those unrelated to AI, those applying AI in non-scientific contexts, or those where AI is mentioned only peripherally.</p>
<p>Manual classification is both time-consuming and impractical, given the need to categorize more than 100,000 entries, first by research type, and subsequently by institutional affiliations and scientific terms.The increasing amount of literature creates major challenges for analysis, as traditional methods like reviews and meta-analyses are limited to a small number of studies (Gurevitch et al. 2018).With the rapid growth of publications, AI tools, especially LLMs, provide an effective solution by analyzing large volumes of research (Lin et al. 2024;Luo et al. 2024;Ahad et al. 2024).Models like ChatGPT can handle complex tasks, such as recognizing textual entities, and have shown remarkable abilities, making them ideal for efficiently summarizing knowledge and identifying research gaps (Singhal et al. 2023;Hagendorff et al. 2023).LLMs were employed here for basic classification tasks, without requiring complex understanding or detailed outputs.However, precautions are necessary to mitigate the inherent risk of hallucinations associated with these models (Farquhar et al. 2024).</p>
<p>To balance efficiency with accuracy, we developed a hybrid workflow that combines automated reasoning models with human verification (Fig. 1).This approach integrates large and small reasoning models, a voting mechanism, and optimization approaches for search terms, prompts, and model usage, ensuring the accuracy of LLM classifications.For cases with significant discrepancies in large models (only 3.2%), human verification was applied.Through random sampling of 400 LLM classification results, we conducted four rounds of optimization on search terms, model usage, and prompts, achieving a classification accuracy of over 95%.Ultimately, 72.7% of the articles were categorized under AI for Science.The process begins with the use of three powerful 20B-32B small reasoning models (Qwen3-32B, GPT-OSS-20B, and DeepSeek R1-32B) released in 2025.These models classify the input articles-based on their title and abstract-into four categories: AI for Science, Science for AI, Typical AI Research, and Not Applicable.If all three models consistently classify a paper as AI for Science, it is directly categorized as such.For entries with conflicting results, a 120B large reasoning model (GPT-OSS-120B) is used.If the large model's classification aligns with the majority of the small models' results, the paper is assigned to that category.In cases of further disagreement, human classification is applied.A random sample of 400 AI-determined classifications is assessed for accuracy.If the accuracy is below 95%, improvements are made to the search terms, prompts, models, and evaluation strategies.The displayed results represent the final classifications.</p>
<p>Our analysis reveals that the number of AI-related articles exhibits a rapid increase over the last ten years, with both the absolute number and proportion rising across 145 Nature Index journals (Fig. 2A and Fig. S1).The growth in AI-related research is especially evident in recent years, with the number of such articles in 2024 being fifteen times higher than in 2015, and the relative proportion of AI-related articles has also surged, reaching twelve times the level observed in 2015.This significant rise highlights the escalating adoption of AI methodologies across various scientific disciplines.Notably, 21 journals each published over 50 AI-related articles in 2024 alone, indicating a broad integration of AI.Furthermore, 16 journals had more than 5% of their total publications focused on AI, with five surpassing 9%.Among these, Nature Methods stands out, with AI-related articles comprised 15.87% of its total output in 2024, reflecting the pivotal role of AI in advancing specific fields.However, they still represent only 2.72% of total publications, indicating that AI is still in the early stages of widespread adoption in science.</p>
<p>The Diffusion of Innovation theory (Rogers et al. 2014), developed by Everett Rogers, describes how new ideas and technologies spread through a population over time.It posits that adoption follows an S-curve, with innovations initially embraced by a small group of innovators and early adopters, followed by the early majority, late majority, and finally laggards.Based on this pattern, we anticipate that AI-related research will continue to grow rapidly over the next few decades, with its expansion gradually slowing as it reaches maturity (Fig. 2B).As the innovation matures, its adoption rate increases rapidly, then slows as it becomes widely accepted and integrated into everyday practices.AI's application in scientific discovery is likely following a similar pattern.Early adopters in fields like computational biology, materials science, and datadriven research have led the charge, demonstrating the transformative potential of AI.As more researchers integrate AI into their work and the technology becomes more accessible, adoption will spread to the early majority and late majority, solidifying its place as a central tool in scientific research.By 2050, we project that AI-related research will stabilize at around 20% of the total Nature Index journal publications, reflecting AI's fully integrated role in modern research.Projected growth trend of AI-related research in Nature Index journals based on a logistic growth model.This estimate follows the S-shaped curve derived from the Diffusion of Innovation theory and is calibrated using historical data from the past decade.The projection assumes that AI for Science will account for 20% of Nature Index publications by 2050.The shaded area represents the upper and lower bounds of this 20% projection.</p>
<p>Experimental scientists lead AI4Science research</p>
<p>To elucidate the roles of AI researchers in the current AI for Science wave, we analyzed 61,759 author affiliations from 20,603 AI-related articles.These affiliations were categorized into three groups: AI Institute, Science Institute, or Not Applicable (for those not clearly affiliated with either AI or Science).The reasoning LLM-human collaboration classification workflow is depicted in Fig. S2, which is similar to the workflow for research type (Fig. 1) but with slight adjustments.</p>
<p>The trends in author affiliations across these 20,603 AI-related research articles from 2015 to 2024 are illustrated in Fig. 3. AI institutions are increasingly involved in scientific discovery, yet they largely maintain a supportive role despite their growing significance.The average number of AI institutions per paper has risen from 0.16 in 2015 to 0.38 in 2024, a 1.38-fold increase (Fig. 3A).In contrast, the average number of institutions per research article and the total number of scientific institutions have remained relatively stable, experiencing a decline between 2018 and 2022, followed by a slight upward trend in recent years.The proportion of papers with AI institutions as author affiliations has grown from 13.33% to 25.30%, a 0.90-fold increase (Fig. 3B).Additionally, AI institute-led research saw a notable rise in 2016, followed by a gradual and uneven upward trend in the subsequent years (Fig. 3C), with the average percentage over the past three years at 5.66%.Similarly, the average ranking of AI institutions as first authors has improved from 6.97 in the first year to 3.68 over the past nine years.However, scientific institutions continue to dominate AI-driven scientific discovery, with the average ranking of the first-listed scientific institution remaining stable at around 1.1, shifting slightly from 1.09 in the first three years to 1.12 in the latest three years (Fig. 3D).Recognizing that most of these journals may not be as familiar to AI researchers, we have also conducted analysis on their participation in five well-known interdisciplinary journals that are also familiar within the AI research community, including Nature, Nature Communications, Science, Science Advances, and PNAS.The five journals contribute 3,262 articles, accounting for 21.7% of the overall AI for Science publications.This trend is unsurprising, as AI researchers are more actively engaged in these interdisciplinary journals compared to the broader Nature Index journals.In 2015, the average number of AI-affiliated institutions per article, the proportion of articles involving AI institutions, and the percentage of articles with AI institutions as the first affiliation were all twice as high as the average for Nature Index journals (Fig. S3).By 2024, AI institutions were involved in 36.3% of published articles, but only 10.7% of articles listing an AI institution as the first affiliation.Furthermore, after 2021, AI institutions' participation reached a plateau, suggesting a stabilization phase in their contribution to these journals.This stagnation may indicate that AI's integration into specific scientific fields has transitioned from a phase of rapid expansion to one of consolidation.</p>
<p>Unlocking the potential of AI researchers in scientific discovery: Three key directions</p>
<p>AI has been and will continue to accelerate the process of scientific discovery.While experimental scientists are increasingly leveraging AI tools in their research, the full potential of AI researchers in driving scientific breakthroughs remains largely untapped (Fig. 3).Achieving the projected growth from 2.72% to 20% in AI-driven scientific contributions will require the active and extensive involvement of AI researchers.We identify three key directions, which are both ongoing and yet to be fully defined, that are essential to realizing this transformation, as illustrated in Fig. 4. Developing user-friendly AI tools for scientific discovery presents a significant opportunity for AI researchers, enhancing the impact and practical value of their work.Moreover, such advancements hold the potential to bridge the gap between AI technology and real-world applications, facilitating the transition from research to practical products.Equipping experimental scientists with AI tools-such as data collection (Polak and Morgan 2024), modeling and analysis (Krenn et al. 2022a), and experiment design and conduction (Gottweis et al. 2025)-will accelerate the discovery process across a range of scientific fields.In fact, early adopters among experimental scientists have already begun utilizing AI tools in their research.As shown in Fig. 3B and Fig. 3C, 75% of AI-related articles are currently independently authored by experimental scientists, who also lead the majority of collaborative research efforts.The next key step in advancing the application of AI is the development of more userfriendly tools and platforms, which will be discussed in Section 2.4.This is crucial for further driving the adoption of AI among experimental scientists.</p>
<p>The success of AI4Science hinges not only on experimental scientists adopting AI tools but also on the development of proactive AI researchers who are deeply embedded in scientific discovery.These researchers will bridge the gap between AI and domainspecific knowledge, developing novel AI algorithms and models tailored to specific scientific challenges, thus driving the next generation of AI-powered discoveries.The proportion of AI researchers leading AI-assisted scientific discoveries has increased (Fig. 3C).However, it is also observed that the rate of increase has stagnated in recent years, and the overall number remains relatively small.We attribute this limitation to two main gaps: a cognitive gap (regarding what scientific discoveries AI can be applied to) and a methodological gap (in terms of the workflow in which AI researchers lead scientific discoveries).Discussions on bridging these gaps will be presented in Section 2.5 and Section 2.6, respectively.</p>
<p>Collaboration is a key driver of AI4Science research, and this collaboration extends beyond individual studies to a broader research ecosystem.A quintessential example of this is the SHapley Additive exPlanations (SHAP) (Lundberg and Lee 2017;Lundberg et al. 2020), a model interpretation method, initially proposed by AI researchers and widely adopted across various scientific disciplines.For instance, SHAP has been used to quantify the contributions of individual metabolites to disease risk, identifying key metabolites influencing the risk of 24 investigated diseases (Buergel et al. 2022).It has also been applied to interpret nanomaterial-plantenvironment interactions, providing insights into the factors affecting the root uptake of metal-oxide nanoparticles and their interactions within the soil (Yu et al. 2022).Additionally, SHAP was employed to analyze the contributions of raw ingredients and their interactions, offering valuable insights into the factors influencing the compressive strength of alkali-activated materials (Zheng et al. 2023).This is also facilitated by the development of an easily accessible package by the authors (https://shap.readthedocs.io/),which can be considered a key factor in enabling the widespread use of AI tools by experimental scientists.Another reason is that, for experimental scientists, model explanation is often more important than marginal improvements in model performance, as it enables the extraction of hypotheses that can inform subsequent research.Such an ecosystem would significantly accelerate the progress of AI4Science, creating fertile ground for innovation and driving advancements in scientific discovery.Moreover, the establishment of standards and best practices in AI implementation across scientific discovery will ensure reproducibility, transparency, and broader adoption.</p>
<p>Four approaches to equipping experimental scientists with AI</p>
<p>AI will not replace scientists because scientific discovery requires human intuition, creativity, and the ability to navigate uncertainty-capabilities that AI lacks (Makarov et al. 2021;Messeri and Crockett 2024).However, scientists who leverage AI can process vast amounts of data, automate complex analyses, and generate insights more efficiently, giving them a significant advantage in accelerating breakthroughs and expanding the frontiers of knowledge (Oviedo et al. 2022;Kapoor et al. 2024).Here, we discuss four key approaches, both established and emerging, that enable researchers to harness AI effectively, lowering barriers to adoption while accelerating scientific discovery (Fig. 5).These approaches not only illustrate how experimental scientists can harness AI tools but also present significant opportunities for AI and software researchers.Building user-friendly platforms of this kind, akin to existing statistical analysis software but enhanced with AI capabilities, can bridge the gap between advanced machine learning and practical scientific applications, enabling broader adoption and innovation.User-friendly AI platforms can streamline data-driven research by automating model selection, training, and model explanation (Fig. 5A).Scientists provide raw data, specify prediction targets, and define constraints such as time and computational resources.The platform then constructs an appropriate model, performs analysis, and generates an explainable report.Model explanation and the resulting reports are critical components as they directly contribute to scientific understanding, which is one of the primary objectives of scientific research (Krenn et al. 2022b).Besides, many scientific fields lack sufficient experimental data, yet AI can still provide meaningful insights by integrating domain knowledge with LLMs (Fig. 5B).Although powerful, LLMs face challenges such as generating false information, relying on outdated data, and lacking clear logical reasoning.A promising solution to address these issues is retrievalaugmented generation, which incorporates information from external repositories to enhance the accuracy of outputs, particularly in knowledge-intensive tasks (Lewis et al. 2020;Lewis et al. 2020).Researchers can input relevant prompts and background information into a platform, which then constructs a domain-specific vector database.By leveraging retrieval-augmented inference, the system generates predictions in zeroor few-shot learning scenarios.This approach enables scientists to explore hypotheses and obtain preliminary insights even in data-scarce environments.In fact, experimental scientists are increasingly adept at using AI-driven methods to tackle both data-rich (Pyzer-Knapp et al. 2022;Wong et al. 2024) and data-scarce (Chen et al. 2024;Liu et al. 2024) scientific problems.User-friendly platforms will further accelerate this process by making these tools more accessible, encouraging broader adoption among scientists, and ultimately empowering AI to play a larger role in driving scientific discovery.</p>
<p>Data is essential for driving machine learning advancements in scientific discovery, as its quality and quantity directly influence the accuracy and reliability of predictions (Liu et al. 2023).A vast amount of historical data is embedded in published literature, with an ever-growing volume emerging in scientific journal articles.However, its unstructured format, including both natural language text and figures, presents significant obstacles for immediate use by modern informatics systems that rely on structured datasets (Gupta et al. 2024).Therefore, the extraction of usable data from these articles is crucial for AI-assisted scientific discovery.While LLMs have advanced the way data is extracted from literature (Polak et al. 2024), their ability to extract complex text data remains limited.Additionally, their capacity to collect data from images and related research documents, such as supplementary tables, is still underdeveloped.However, with the development of more powerful LLMs, especially those fine-tuned specifically for data extraction (Ai et al. 2024), and the rise of multimodal LLMs, we anticipate a significant enhancement in data extraction capabilities.Researchers specify target articles and parameters of interest, and the platform retrieves, preprocesses, and extracts relevant information using LLMs (Fig. 5C).The resulting dataset is automatically curated and formatted, accelerating the process of literature-based data collection and synthesis.This approach facilitates metaanalyses, comparative studies, and data-driven hypothesis generation by efficiently aggregating knowledge from published research.Furthermore, AI can further revolutionize scientific experimentation by autonomously designing and executing experiments (Fig. 5D).Researchers define hypotheses, control variables, and desired outputs, while an AI-powered platform-enhanced by knowledge-augmented reasoning-develops an optimized experimental plan.Robotics then execute the experiments, and a validation module assesses the reliability and quality of the data.Although AI-and robot-driven automated laboratories are still in their early stages, initial applications have already emerged (Szymanski et al. 2023;Angello et al. 2024), demonstrating remarkable potential to transform scientific research (Angelopoulos et al.).This approach minimizes manual labor, increases reproducibility, and enables high-throughput hypothesis testing, ultimately accelerating scientific progress.</p>
<p>Scientific domain "ocean" with AI application potential</p>
<p>AI offers immense potential in scientific discovery, yet many opportunities remain underexplored by AI researchers.Currently, the application of AI in science is largely concentrated in fields with abundant structured datasets, such as foundational life sciences and medical research.Prominent examples include protein structure and function prediction, single-cell annotation, and drug discovery (Fig. 6A).The success of AI in these areas is largely driven by the availability of well-curated, large-scale datasets and clearly defined machine learning tasks.However, scientific discovery extends far beyond these domains, and numerous critical challenges in other fields remain largely untapped by AI researchers.</p>
<p>Fig. 6 Research areas with high AI involvement and strong application potential. (A)</p>
<p>Hot topics in AI-driven scientific research.These are prominent research areas identified through an analysis of leading AI-for-Science publications.The fields listed are currently benefiting from abundant, well-structured datasets and clearly defined machine learning tasks, such as protein prediction and drug discovery.(B) Scientific domains with broad AI application potential.This "ocean" of domains represents a forward-looking perspective on the vast areas where AI is poised to make a significant impact.These domains were curated by our team to highlight complex, multidisciplinary problems where new AI approaches are needed to handle unstructured data and integrate diverse knowledge sources.(C) High-frequency scientific terms from titles and abstracts.These terms were extracted using a LLM (GPT-OSS-120B) from a dataset of fifteen thousand AI-related research articles.This provides a data-driven view of the current focus of AI applications in science, highlighting the fields where AI is already actively being used to address fundamental research questions.</p>
<p>Beyond life sciences, AI holds transformative potential in broader scientific domains such as materials design, material-biology-environment interactions, biosynthesis and chemical synthesis, environmental assessment and remediation, climate prediction, and industrial process optimization (Fig. 6B).These areas pose complex, multidimensional problems that require AI-driven approaches for data integration, modeling, prediction, and explanation.Despite their importance, these domains have not received comparable attention from the AI research community, often due to the lack of easily accessible datasets or the need for specialized domain knowledge to frame AI-driven solutions effectively.</p>
<p>To map the current landscape of AI-driven scientific discovery, we analyzed the titles and abstracts of AI-related research articles published in high-impact scientific journals.</p>
<p>To extract scientific terms from research articles, we applied a large reasoning LLM model (GPT-OSS-120B) to titles and abstracts.Each identified term was classified into one of three categories using a LLM-human collaboration workflow: AI, Science, or Not Applicable.By identifying high-frequency scientific entities (Fig. 6C), we highlight the fields where AI is already being employed by experimental scientists to address fundamental research questions.AI researchers can expand their impact by developing advanced algorithms tailored to complex, unstructured scientific data.Additionally, AI-driven approaches to data acquisition, experimental design, and realtime analysis can accelerate discovery by optimizing research workflows.</p>
<p>AI4Science workflow for AI researchers: From tool providers to key contributors</p>
<p>There are many AI4Science workflows for scientific discovery in various fields, such as proteomics (Mann et al. 2021), materials (Pyzer-Knapp et al. 2022), biomedicine (Gao et al. 2024), and medical imaging (Najjar 2023), but most are designed for researchers in specific fields rather than for AI researchers.We propose an AI4Science workflow for AI researchers (Fig. 7).</p>
<p>The first and most crucial step is to identify suitable scientific problems, which can be guided by the researchers' interests, experience, and available resources.Areas where AI has already been applied may be a good starting point (Fig. 6C), as experimental scientists may have used basic machine learning methods to address such problems.Literature-based-discovery (LBD) has been used to bridge the growing gap in scientific knowledge by systematically uncovering hidden relationships between disparate research areas (Henry and McInnes 2017).As scientific literature grows exponentially, the application of LBD and interdisciplinary knowledge transfer techniques (Cunningham et al. 2025) is becoming increasingly important for synthesizing knowledge across fields, facilitating interdisciplinary collaboration, and accelerating the pace of discovery.Understanding the chosen scientific domain is a prerequisite for conducting efficient research, and with the development of LLM-based agents (Schmidgall et al. 2025), this will become easier.LLMs have significantly enhanced LBD by offering advanced natural language processing capabilities that enable the automated generation of novel scientific ideas (Hu et al. 2024) and scientific hypotheses (Yang et al. 2024).Moreover, researcher agents based on LLMs have been developed to aid scientific discovery.For instance, Google DeepMind's AlphaEvolve combines the creativity of an LLM with algorithms that refine and filter mathematical and computer science solutions (Novikov et al. 2025).This system allows for both innovative idea generation and solution improvement.Similarly, SciMaster's X-Master is a general-purpose AI agent designed to interact flexibly with external tools during reasoning (Chai et al. 2025).By mimicking the dynamic problem-solving process of human researchers, X-Master creates a feedback loop: tool outputs refine the agent's reasoning, while better reasoning leads to more effective tool use, facilitating smarter scientific discovery.These models can analyze large volumes of text, identify latent patterns, and suggest potential connections between previously unrelated areas of research.LLM-based agents have been used in scientific discovery.For example, the Virtual Lab, an LLM-PI-led, human-in-the-loop agent team, used an ESM-AlphaFold-Rosetta pipeline to design 92 nanobodies, with experiments validating functional binders, including two improved against JN.1/KP.3variants (Swanson et al. 2025).</p>
<p>Overall, AI plays three main roles across different fields: prediction, comprehension, and innovation (discovery or design).These roles are logically connected.For example, in the case of proteins, one can first predict the structure or functional annotation of a protein (Gligorijević et al. 2021), then understand the relationship between the sequence or structure and function (Wang et al. 2025), and finally, based on this understanding, design new proteins to achieve specific functions (Jiang et al. 2024).In data-rich fields, one can skip the understanding step, but in many cases, understanding is closely tied to scientific discovery.Formal experiments can generally be divided into three steps: data collection, modeling and analysis, and experimental validation.For AI researchers, we do not recommend collecting data through experiments or simulations, as this increases the difficulty of conducting research.Instead, we suggest focusing on fields with available datasets or those where data can be gathered from literature.In data-rich fields, the competition is more intense.For many fields, there is a lack of publicly available high-quality data, and extracting data from literature can be an effective way to enhance competitiveness.Existing LLM and AI tools (Ansari and Moosavi 2024;Polak and Morgan 2024) also make data collection from literature easier.Modeling and analysis are areas AI researchers are familiar with, so we will not elaborate further, but we emphasize the importance of model explanation and causal inference.Because scientific understanding is one of the main aims of science (Krenn et al. 2022b).Finally, we recommend experimental validation of the results obtained through computational methods.Many experiments have standardized procedures, and collaborating with other labs, shared experimental platforms, or commercialized research service institutions can help complete this step.Furthermore, the advancement of automated laboratories powered by LLMs and robotics is set to streamline experimental validation (M.Bran et al. 2024;Angelopoulos et al.), making it as accessible as the use of AI tools by experimental scientists.</p>
<p>Limitations and outlook</p>
<p>While this study is limited to AI-related research in 145 expert-selected Nature Index journals, we make this choice because the study focuses on the natural and health sciences, and these journals constitute the primary venues for scientific discovery in those fields.Our binary affiliation scheme (AI institute versus science institute) and the use of first affiliation to infer primary contribution are operational simplifications that cannot fully capture interdisciplinary contribution structures (e.g., co-first authorship, joint labs, field-specific norms), yet they enable a scalable analysis that yields a macrolevel quantitative portrait and action-oriented guidance.Despite these limitations, the study offers a fresh lens that reframes AI researchers not merely as tool providers but as potential co-leaders of scientific discovery, quantifies the current participation gap, contextualizes growth via Diffusion of Innovation projections, and presents practical workflows and strategic interventions to empower both AI researchers and experimental scientists.</p>
<p>Unlocking the potential of AI researchers is not about replacing experimental scientists but about achieving a more effective division of labor.Many scientific areas depend on making predictions and extracting insights from complex data, tasks where AI researchers excel, while problem formulation, experimental design, and mechanistic validation rely on the deep domain expertise of experimental scientists.Moreover, LLM-based research agents are already being applied to real-world scientific discovery (M.Bran et al. 2024;Swanson et al. 2025;Penadé s et al. 2025), underscoring an inevitable trend that makes deeper AI-researcher involvement not only beneficial but necessary.At present, AI-researcher-led scientific discoveries represent roughly 5% of AI4Science publication in these journals; raising this share could catalyze a healthier cross-disciplinary ecosystem.In such an ecosystem, AI researchers would better understand scientific questions, experimental scientists would acquire foundational AI literacy, and more cross-disciplinary institutes, joint centers, and dual-PI teams would emerge to tackle a focused set of critical scientific problems.This structural shift can accelerate breakthroughs while creating more sustainable career pathways for earlycareer AI researchers at the intersection of AI and the sciences.</p>
<p>Conclusion</p>
<p>AI4Science has seen rapid growth over the past decade, yet its full potential remains untapped, particularly the contributions of AI researchers themselves.While AI is increasingly integrated into scientific research, its application is still largely driven by experimental scientists, with AI researchers often playing a supportive role.This imbalance limits the depth of AI's impact on scientific discovery.To accelerate progress, this work highlights the need to unlock the expertise of AI researchers, positioning them as active contributors rather than just tool developers.By fostering deeper collaboration and bridging cognitive and methodological gaps, AI researchers can drive more transformative advancements and reshape the landscape of scientific discovery.Looking ahead, the future of AI4Science depends on a well-defined human-AI collaboration paradigm: one that leverages AI's analytical power while ensuring human researchers remain in control of scientific reasoning and validation.Addressing challenges such as AI hallucinations and setting clear boundaries for AI applications will be essential.Moreover, experimental validation stands as the ultimate safeguard of AI-driven discoveries across most disciplines, upholding scientific rigor.With these strategic shifts, AI4Science can expand from its current 2.72% share of publications in Nature Index journal to 20% by 2050, unlocking unprecedented opportunities for scientific discovery.</p>
<p>Methods</p>
<p>Data collection</p>
<p>AI-related research articles published in 145 Nature Index journals over the past decade were retrieved from the Web of Science Core Collection.The search query used was [SO="Journal" AND PY=2015-2024 AND DT=Article AND TS=("machine learning" OR "deep learning" OR "artificial intelligence" OR "AI" OR "data-driven" OR "neural network" OR "convolutional network" OR "residual network" OR "long short-term memory" OR "multilayer perceptron" OR "transformer architecture" OR "transformer model" OR "vision transformer" OR "foundation model" OR "pretrained model" OR "ensemble learning" OR "gradient boosting" OR "XGBoost" OR "LightGBM" OR "CatBoost" OR "random forest" OR "decision tree" OR "tree-based model" OR "support vector machine" OR "k-nearest neighbors" OR "naive bayes" OR "bayesian modeling" OR "bayesian learning" OR "generative adversarial network" OR "autoencoder" OR "genetic algorithm" OR "optimization algorithm" OR "reinforcement learning" OR "transfer learning" OR "few-shot learning" OR "zero-shot learning" OR "semi-supervised learning" OR "self-supervised learning" OR "unsupervised learning" OR "federated learning" OR "attention mechanism" OR "selfattention" OR "cross-attention" OR "multihead attention" OR "language model" OR "natural language processing" OR "computer vision" OR "image processing" OR "speech recognition" OR "audio processing")], which yielded a total of 20,603 AIrelated research articles.The retrieved data included information such as titles, abstracts, publication years, author affiliations, and journal names.To obtain the total number of research articles published in these journals during the same period, we used the search query (SO="Journal name" AND PY=2015-2024 AND DT=Article), which yielded a total of 1.16 million research articles.</p>
<p>Research type classification</p>
<p>Although the search terms targeted AI-related studies, not all retrieved articles were classified as AI for Science, necessitating a classification of research types.We defined four categories of research: Typical AI Research, AI for Science, Science for AI, and Not Applicable.Given the scale of the dataset, manual classification was impractical.Therefore, we employed a hybrid workflow that combines automated reasoning models with human verification (Fig. 1).Large and small reasoning models, a voting mechanism, and optimization of search terms, prompts, and model usage were integrated to ensure classification accuracy.The meta prompt is provided in Supplemental Information.The process begins with three state-of-the-art 20-32B reasoning models, Qwen3-32B (Yang et al. 2025), GPT-OSS-20B, and DeepSeek R1-32B (Guo et al. 2025), released in 2025.Articles (title and abstract) unanimously classified as AI for Science by all three models were directly assigned.In cases of disagreement, a larger 120B model (GPT-OSS-120B) was introduced.If the large model's output aligned with the majority decision of the smaller models, the article was categorized accordingly.To address LLMs' discrepancies in classification, human verification was applied.A random sample of 400 LLM classifications was assessed for accuracy, leading to four rounds of optimization, which resulted in a classification accuracy exceeding 95%.</p>
<p>Author affiliation classification</p>
<p>To classify author affiliations, we applied the similar hybrid workflow used in Research Type Classification, utilizing both large and small reasoning models (Fig. S2).The meta prompt is provided in Supplemental Information.Affiliations were categorized into three groups: AI Institute, Science Institute, and Not Applicable.AI Institute includes institutions explicitly focused on AI, machine learning, or related fields.Science Institute refers to institutions dedicated to research in natural or health sciences, like biology, chemistry, or medicine, without reference to AI.Not Applicable includes non-research organizations, comprehensive research institutions, or institutions where AI or scientific research is not the primary focus.Unlike Research Type Classification, which prioritizes the accuracy of classifying AI for Science papers, Author Affiliation Classification equally emphasizes all categories.Each affiliation was considered independently, with discrepancies resolved through the use of a large reasoning model or human classification when necessary.</p>
<p>Scientific entity extraction and curation</p>
<p>The GPT-OSS-120B was deployed to extract terms from the titles and abstracts of research articles.The meta prompt is provided in Supplemental Information.Following extraction, we implemented a multi-step curation pipeline to ensure the reliability of the term dataset (shown in Fig. S5).First, inconsistent delimiter usage (e.g., excessive commas in place of semicolons) was corrected, and entries with abnormally short or malformed strings were filtered out.Terms were then split into individual entities and stripped of surrounding whitespace and punctuation.We standardized capitalization (first-letter only) and aggregated terms by frequency.To unify semantically similar entries, we normalized terms by removing case, punctuation, and hyphenation, then clustered those with shared prefixes.Finally, high-frequency terms were filtered by character length and word count.Remaining terms were clustered into groups of similar expressions and ranked by total frequency.The resulting list was exported for downstream analysis.</p>
<p>Scientific entity classification and word cloud plotting</p>
<p>Using the LLM-human collaboration workflow (shown in Fig. S6), each identified term was classified into one of three categories based on a structured prompt: AI (terms directly related to artificial intelligence technologies, such as machine learning, neural networks, and deep learning), Science (terms associated with natural or health sciences, such as physics, biology, or medicine), or Not Applicable (general or unrelated terms not specific to either domain).The meta prompt is provided in Supplemental Information.The high-frequency terms were subject to minor manual adjustments, including the merging of obvious synonyms and standardization of capitalization, to enhance clarity and consistency in the visualization.After classification, we utilized the wordcloud package to visualize the high-frequency terms, providing a clear representation of the most commonly mentioned entities in the dataset.) was used to extract scientific terms from titles and abstracts.The initial temperature was set to 0.0.For entries where appropriate scientific terms could not be extracted, the temperature was adjusted to 0.3.A total of 20,636 terms were initially extracted, which was subsequently reduced to 16,598 after clustering.#Output Format# Only output one of the following three categories: "Typical AI Research", "AI for Science", "Science for AI", or "Not Applicable".DO NOT provide any explanation, commentary, or any other text.</p>
<p>""" messages = [ {"role": "system", "content": SYSTEM_PROMPT}, {"role": "user", "content": f"Journal Name: {journal_name}\n" f"Article Title: {article_title}\n" f"Abstract: {abstract}"} ] *[Reasoning: high] is only used for GPT-OSS models.research focus, non-research commercial organizations, and institutions whose research does not explicitly indicate a link to AI or scientific domains.Additionally, some institutions that house both AI and science research teams, such as certain research labs or large multi-disciplinary institutions, should be classified as Not Applicable if they do not explicitly focus on either AI or science in isolation.</p>
<h1>Output Format# Only output one of the following three categories per input: "AI Institute", "Science Institute", or "Not Applicable".Do not provide any explanation, justification, or additional text.</h1>
<p>""" messages = [ {"role": "system", "content": SYSTEM_PROMPT}, {"role": "user", "content": f"#Input address: {address}"} ] *[Reasoning: high] is only used for GPT-OSS models.</p>
<p>Fig. 1
1
Fig. 1 Workflow for classifying research articles in collaboration with reasoning models and human oversight.The process begins with the use of three powerful 20B-32B small reasoning models (Qwen3-32B, GPT-OSS-20B, and DeepSeek R1-32B) released in 2025.These models classify the input articles-based on their title and abstract-into four categories: AI for Science, Science for AI, Typical AI Research, and Not Applicable.If all three models consistently classify a paper as AI for Science, it is directly categorized as such.For entries with conflicting results, a 120B large reasoning model (GPT-OSS-120B) is used.If the large model's classification aligns with the majority of the small models' results, the paper is assigned to that category.In cases of further disagreement, human classification is applied.A random sample of 400</p>
<p>Fig. 2
2
Fig. 2 Publication and projected growth trends of AI-related research articles in 145 Nature Index journals.(A) Publication trends of 20,603 AI-related research articles in Nature Index journals (2015-2024).Publication proportions are shown in Fig. S1.(B) Projected growth trend of AI-related research in Nature Index journals based on a logistic growth model.This estimate follows the S-shaped curve derived from the Diffusion of Innovation theory and is calibrated using historical data from the past decade.The projection assumes that AI for Science will account for 20% of Nature Index publications by 2050.The shaded area represents the upper and lower bounds of this 20% projection.</p>
<p>Fig. 3
3
Fig. 3 Trends in author affiliations of 20,603 AI-related research articles (2015-2024).(A) Number of institutions per article by institution type.(B) Number and proportion of articles involving AI-related institutions.(C) Number and proportion of articles with</p>
<p>Fig. 4
4
Fig. 4 Schematic representation of three key strategies for unlocking the potential of AI researchers in scientific discovery: (1) equipping experimental scientists with userfriendly AI tools, (2) enabling AI researchers to take a more direct and active role in scientific discovery, and (3) fostering a thriving AI-driven scientific ecosystem to sustain long-term innovation.</p>
<p>Fig. 5
5
Fig. 5 Schematic diagram of four approaches to equipping experimental scientists with AI. (A) User-friendly platforms facilitate modeling and explanation when researchers have access to sufficient data.(B) Domain-enhanced LLMs enable zero-shot and fewshot inference in data-scarce scenarios.(C) The integration of LLMs, multimodal systems, and multi-task frameworks makes automated data extraction from literature increasingly feasible.(D) Advances in LLMs and robotics are driving the development of autonomous experimental platforms, freeing researchers from labor-intensive experiments.</p>
<p>Fig. 7
7
Fig. 7 AI4Science workflow for AI researchers.</p>
<p>Fig. S1
S1
Fig. S1 Publication ratios of fifteen thousand AI-related research articles in 145 Nature Index journals (2015-2024).</p>
<p>Fig. S2
S2
Fig. S2 Workflow for classifying author affiliations in collaboration with reasoning models and human oversight.</p>
<p>Fig. S3
S3
Fig. S3 Trends in publication count and author affiliations of 3,262 AI-related research articles published in five well-known multidisciplinary journals (2015-2024), including Nature, Nature Communications, Science, Science Advances, and PNAS.(A) Number of publications by year.(B) Number of institutions per article by institution type.(C) Number and proportion of articles involving AI-related institutions.(D) Number and proportion of articles with AI-related institutions as the first affiliation.(E) Rank of the first-affiliated AI and Science institution.The shaded area represents the 95% confidence interval.</p>
<p>Fig. S4
S4
Fig. S4 High-frequency AI terms in the titles and abstracts of fifteen thousand AIrelated research articles.</p>
<p>Fig. S5
S5
Fig. S5Workflow for the extraction and curation of scientific terms.A 120B large reasoning model (GPT-OSS-120B) was used to extract scientific terms from titles and abstracts.The initial temperature was set to 0.0.For entries where appropriate scientific terms could not be extracted, the temperature was adjusted to 0.3.A total of 20,636 terms were initially extracted, which was subsequently reduced to 16,598 after clustering.</p>
<p>Fig. S6
S6
Fig. S6 Workflow for classifying scientific terms in collaboration with reasoning models and human oversight.</p>
<p>AcknowledgementsThis work is supported by Westlake Education Foundation under the Grant No. 103110846022301 and China Postdoctoral Science Foundation under the Grant No. 2024M762941.Declarations#Task# Given one affiliation string from a scientific paper, classify each affiliation into one of the following categories:1. <strong>AI Institute</strong>: The affiliation explicitly includes terms or names related to Artificial Intelligence (AI), its subfields, or broader intelligence-related fields.This includes departments, centers, labs, or institutes whose primary focus is on artificial intelligence, machine learning, deep learning, neural networks, computer science, reinforcement learning, meta-learning, big data, data science, natural language processing, computer vision, or other intelligent interdisciplinary research.Any institution with renowned names or organizations typically associated with AI-such as "Turing," "DeepMind," "OpenAI," "Mila," "FAIR," or similar-should be classified as AI-focused.Additionally, institutions that integrate AI research with other domains, such as AI in healthcare, bioinformatics, intelligent robotics, or interdisciplinary AI studies, should also be classified as AI-focused, as these fields heavily rely on AI technology.2. <strong>Science Institute</strong>: The affiliation is primarily engaged in research within the natural sciences or health sciences.This includes fields such as biology, chemistry, physics, environmental science, pharmacy, medicine, engineering, agricultural science, and related disciplines.Institutions classified as Science Institutes typically focus on understanding the natural world, biological systems, or the physical and chemical properties of matter.Typical terms in the affiliation may include "Faculty of Science," "Department of Chemistry," "Institute of Biomedicine," "School of Engineering," "Department of Physics," "Institute of Environmental Science," "College of Medicine," or similar.Additionally, these institutions will not include any terms or references related to Artificial Intelligence (AI) or its subfields within the affiliation address.These institutions are characterized by their focus on empirical, experimental, and applied research in scientific fields that do not explicitly focus on AI. 4. <em>Hyphenation and Abbreviations</em>: Follow the common rules for using hyphens in academic writing (e.g., "data-driven", "state-of-the-art", "long-term").Retain full original terminology if the abbreviation is not well-known or commonly used in the field.5. <em>Avoid Generic Terms</em>: Do not include overly broad or generic terms that do not contribute to understanding the specific scientific focus of the paper.3. <em>Not Applicable</em> Terms that do not fall under AI or Science.This category includes general terms, tools, or concepts that don't directly relate to either field, such as generic methods, statistical terms, or other unrelated technical phrases.#Output Format# Only output one of the following categories: "AI", "Science", or "Not Applicable".DO NOT provide any explanation, commentary, or any other text."""messages = [ {"role": "system", "content": SYSTEM_PROMPT}, {"role": "user", "content": f"#Input term: {term}"} ] *[Reasoning: high] is only used for GPT-OSS models.
Empowering Meta-Analysis: Leveraging Large Language Models for Scientific Synthesis. J I Ahad, R M Sultan, A Kaikobad, F Rahman, M R Amin, N Mohammed, S Rahman, 2024 IEEE International Conference on Big Data (BigData). Washington, DC, USAIEEE2024</p>
<p>Extracting structured data from organic synthesis procedures using a fine-tuned large language model. Q Ai, F Meng, J Shi, B Pelkie, C W Coley, 10.1039/D4DD00091ADigital Discovery. 392024</p>
<p>Closed-loop transfer enables artificial intelligence to yield chemical knowledge. N H Angello, D M Friday, C Hwang, S Yi, A H Cheng, T C Torres-Flores, E R Jira, W Wang, A Aspuru-Guzik, M D Burke, C M Schroeder, Y Diao, Jackson Ne, 10.1038/s41586-024-07892-1Nature. 63380292024</p>
<p>Alterovitz R Transforming science labs into automated factories of discovery. A Angelopoulos, J F Cahoon, 10.1126/scirobotics.adm6991Science Robotics. 9956991</p>
<p>Deep learning for computational biology. C Angermueller, T Pä Rnamaa, L Parts, O Stegle, 10.15252/msb.20156651Molecular Systems Biology. 1278782016</p>
<p>Agent-based learning of materials datasets from the scientific literature. M Ansari, S M Moosavi, 10.1039/D4DD00252KDigital Discovery. 3122024</p>
<p>Artificial Intelligence in Chemistry: Current Trends and Future Directions. Z J Baum, X Yu, P Y Ayala, Y Zhao, S P Watkins, Q Zhou, 10.1021/acs.jcim.1c00619J Chem Inf Model. 6172021</p>
<p>Metabolomic profiles predict individual multidisease outcomes. T Buergel, J Steinfeldt, G Ruyoga, M Pietzner, D Bizzarri, D Vojinovic, Upmeier Zu Belzen, J Loock, L Kittner, P Christmann, L Hollmann, N Strangalies, H Braunger, J M Wild, B Chiesa, S T Spranger, J Klostermann, F Van Den Akker, E B Trompet, S Mooijaart, S P Sattar, N Jukema, J W Lavrijssen, B Kavousi, M Ghanbari, M Ikram, M A Slagboom, E Kivimaki, M Langenberg, C Deanfield, J Eils, R Landmesser, U , 10.1038/s41591-022-01980-3Nat Med. 28112022</p>
<p>J Chai, S Tang, R Ye, Y Du, X Zhu, M Zhou, Y Wang, E W Zhang, Y Zhang, L Chen, S , 10.48550/ARXIV.2507.05241SciMaster: Towards General-Purpose Scientific AI Agents, Part I. X-Master as Foundation: Can We Lead on Humanity's Last Exam. 2025</p>
<p>ProBID-Net: a deep learning model for protein-protein binding interface design. Z Chen, M Ji, J Qian, Z Zhang, X Zhang, H Gao, H Wang, R Wang, Y Qi, 10.1039/D4SC02233EChem Sci. 15472024</p>
<p>2024) I sensed anxiety and frustration at NeurIPS'24. K Cho, 5 Feb 2025</p>
<p>Recent advances and applications of deep learning methods in materials science. K Choudhary, B Decost, C Chen, A Jain, F Tavazza, R Cohn, C W Park, A Choudhary, A Agrawal, Sjl Billinge, E Holm, S P Ong, C Wolverton, 10.1038/s41524-022-00734-6npj Comput Mater. 81592022</p>
<p>Facilitating interdisciplinary knowledge transfer with research paper recommender systems. E Cunningham, B Smyth, D Greene, 10.1162/qss.a.9Quantitative Science Studies :1-22. 2025</p>
<p>A data science roadmap for open science organizations engaged in early-stage drug discovery. K Edfeldt, A M Edwards, O Engkvist, J Günther, M Hartley, D G Hulcoop, A R Leach, B D Marsden, A Menge, L Misquitta, S Müller, D R Owen, K T Schütt, N Skelton, A Steffen, A Tropsha, E Vernet, Y Wang, J Wellnitz, T M Willson, D-A Clevert, B Haibe-Kains, L H Schiavone, M Schapira, 10.1038/s41467-024-49777-xNat Commun. 15156402024</p>
<p>Detecting hallucinations in large language models using semantic entropy. S Farquhar, J Kossen, L Kuhn, Y Gal, 10.1038/s41586-024-07421-0Nature. 63080172024</p>
<p>Quantifying the use and potential benefits of artificial intelligence in scientific research. J Gao, D Wang, 10.1038/s41562-024-02020-5Nat Hum Behav. 8122024</p>
<p>Empowering biomedical discovery with AI agents. S Gao, A Fang, Y Huang, V Giunchiglia, A Noori, J R Schwarz, Y Ektefaie, J Kondic, M Zitnik, 10.1016/j.cell.2024.09.022Cell. 187222024</p>
<p>Amplify scientific discovery with artificial intelligence. Y Gil, M Greaves, J Hendler, H Hirsh, 10.1126/science.1259439Science. 34662062014</p>
<p>Structure-based protein function prediction using graph convolutional networks. V Gligorijević, P D Renfrew, T Kosciolek, J K Leman, D Berenberg, T Vatanen, C Chandler, B C Taylor, I M Fisk, H Vlamakis, R J Xavier, R Knight, K Cho, R Bonneau, 10.1038/s41467-021-23303-9Nat Commun. 12131682021</p>
<p>. J Gottweis, W-H Weng, A Daryin, T Tu, A Palepu, P Sirkovic, A Myaskovsky, F Weissenberger, K Rong, R Tanno, K Saab, D Popovici, J Blum, F Zhang, K Chou, A Hassidim, B Gokturk, A Vahdat, P Kohli, Y Matias, A Carroll, K Kulkarni, N Tomasev, Y Guan, V Dhillon, E D Vaishnav, B Lee, Trd Costa, Penadé S Jr, G Peltz, Y Xu, A Pawlosky, A Karthikesalingam, V Natarajan, 10.48550/ARXIV.2502.18864Towards an AI co-scientist. 2025</p>
<p>DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning. D Guo, D Yang, H Zhang, J Song, P Wang, Q Zhu, R Xu, R Zhang, S Ma, X Bi, X Zhang, X Yu, Y Wu, Z F Wu, Z Gou, Z Shao, Z Li, Z Gao, A Liu, B Xue, B Wang, B Wu, B Feng, C Lu, C Zhao, C Deng, C Ruan, D Dai, D Chen, Ji D Li, E Lin, F Dai, F Luo, F Hao, G Chen, G Li, G Zhang, H Xu, H Ding, H Gao, H Qu, H Li, H Guo, J Li, J Chen, J Yuan, J Tu, J Qiu, J Li, J Cai, J L Ni, J Liang, J Chen, J Dong, K Hu, K You, K Gao, K Guan, K Huang, K Yu, K Wang, L Zhang, L Zhao, L Wang, L Zhang, L Xu, L Xia, L Zhang, M Zhang, M Tang, M Zhou, M Li, M Wang, M Li, M Tian, N Huang, P Zhang, P Wang, Q Chen, Q Du, Q Ge, R Zhang, R Pan, R Wang, R Chen, R J Jin, R L Chen, R Lu, S Zhou, S Chen, S Ye, S Wang, S Yu, S Zhou, S Pan, S Li, S S Zhou, S Wu, S Yun, T Pei, T Sun, T Wang, T Zeng, W Liu, W Liang, W Gao, W Yu, W Zhang, W , Xiao Wl , An W Liu, X Wang, X Chen, X Nie, X Cheng, X Liu, X Xie, X Liu, X Yang, X Li, X Su, X Lin, X Li, X Q , Jin X Shen, X Chen, X Sun, X Wang, X Song, X Zhou, X Wang, X Shan, X Li, Y K Wang, Y Q Wei, Y X Zhang, Y Xu, Y Li, Y Zhao, Y Sun, Y Wang, Y Yu, Y Zhang, Y Shi, Y Xiong, Y Zz, Z Ren, Z Sha, Z Fu, Z Xu, Z Xie, Z Zhang, Z Hao, Z Ma, Z Yan, Z Wu, Z Gu, Z Zhu, Z Liu, Z Li, Z Xie, Z Song, Z Pan, Z Huang, Z Xu, Z Zhang, Z Zhang, 10.1038/s41586-025-09422-zNature. 64580812025</p>
<p>Data extraction from polymer literature using large language models. S Gupta, A Mahmood, P Shetty, A Adeboye, R Ramprasad, 10.1038/s43246-024-00708-9Commun Mater. 512692024</p>
<p>Meta-analysis and the science of research synthesis. J Gurevitch, J Koricheva, S Nakagawa, G Stewart, 10.1038/nature25753Nature. 55576952018</p>
<p>Human-like intuitive behavior and reasoning biases emerged in large language models but disappeared in ChatGPT. T Hagendorff, S Fabi, M Kosinski, 10.1038/s43588-023-00527-xNat Comput Sci. 3102023</p>
<p>Literature Based Discovery: Models, methods, and trends. S Henry, B T Mcinnes, 10.1016/j.jbi.2017.08.011Journal of Biomedical Informatics. 742017</p>
<p>Reproducible mass spectrometry data processing and compound annotation in MZmine 3. S Heuckeroth, T Damiani, A Smirnov, O Mokshyna, C Brungs, A Korf, J D Smith, P Stincone, N Dreolin, L-F Nothias, T Hyötyläinen, M Orešič, U Karst, P C Dorrestein, D Petras, X Du, Jjj Van Der Hooft, R Schmid, T Pluskal, 10.1038/s41596-024-00996-yNat Protoc. 1992024</p>
<p>Assessing GPT-4 for cell type annotation in single-cell RNA-seq analysis. W Hou, Ji Z , 10.1038/s41592-024-02235-4Nat Methods. 2182024</p>
<p>X Hu, H Fu, J Wang, Y Wang, Z Li, R Xu, Y Lu, Jin Y Pan, L Lan, Z , 10.48550/ARXIV.2410.14255An Iterative Planning and Search Approach to Enhance Novelty and Diversity of LLM Generated Ideas. Nova2024</p>
<p>The central role of density functional theory in the AI age. B Huang, Von Rudorff, G F , Von Lilienfeld, O A , 10.1126/science.abn3445Science. 38166542023</p>
<p>PocketFlow is a data-and-knowledge-driven structure-based molecular generative model. Y Jiang, G Zhang, J You, H Zhang, R Yao, H Xie, L Zhang, Z Xia, M Dai, Y Wu, L Li, S Yang, 10.1038/s42256-024-00808-8Nat Mach Intell. 632024</p>
<p>Highly accurate protein structure prediction with AlphaFold. J Jumper, R Evans, A Pritzel, T Green, M Figurnov, O Ronneberger, K Tunyasuvunakool, R Bates, A Žídek, A Potapenko, A Bridgland, C Meyer, Saa Kohl, A J Ballard, A Cowie, B Romera-Paredes, S Nikolov, R Jain, J Adler, T Back, S Petersen, D Reiman, E Clancy, M Zielinski, M Steinegger, M Pacholska, T Berghammer, S Bodenstein, D Silver, O Vinyals, A W Senior, K Kavukcuoglu, P Kohli, D Hassabis, 10.1038/s41586-021-03819-2Nature. 59678732021</p>
<p>REFORMS: Consensus-based Recommendations for Machine-learning-based. S Kapoor, E M Cantrell, K Peng, T H Pham, C A Bail, O E Gundersen, J M Hofman, J Hullman, M A Lones, M M Malik, P Nanayakkara, R A Poldrack, I D Raji, M Roberts, M J Salganik, M Serra-Garcia, B M Stewart, G Vandewiele, A Narayanan, 10.1126/sciadv.adk3452Science. Sci Adv. 101834522024</p>
<p>Recent applications of AI to environmental disciplines: A review. A Konya, P Nematzadeh, 10.1016/j.scitotenv.2023.167705Science of The Total Environment. 9061677052024</p>
<p>On scientific understanding with artificial intelligence. M Krenn, R Pollice, S Y Guo, M Aldeghi, A Cervera-Lierta, P Friederich, Passos Dos, G Gomes, F Hä Se, A Jinich, A Nigam, Z Yao, A Aspuru-Guzik, 10.1038/s42254-022-00518-3Nat Rev Phys. 4122022a</p>
<p>On scientific understanding with artificial intelligence. M Krenn, R Pollice, S Y Guo, M Aldeghi, A Cervera-Lierta, P Friederich, Passos Dos, G Gomes, F Hä Se, A Jinich, A Nigam, Z Yao, A Aspuru-Guzik, 10.1038/s42254-022-00518-3Nat Rev Phys. 4122022b</p>
<p>Rocktä schel T, others (2020) Retrieval-augmented generation for knowledge-intensive nlp tasks. P Lewis, E Perez, A Piktus, F Petroni, V Karpukhin, N Goyal, H Küttler, M Lewis, W Yih, Advances in neural information processing systems. 33</p>
<p>Evolutionary-scale prediction of atomic-level protein structure with a language model. Z Lin, H Akin, R Rao, B Hie, Z Zhu, W Lu, N Smetanin, R Verkuil, O Kabeli, Y Shmueli, Dos Santos Costa, A Fazel-Zarandi, M Sercu, T Candido, S Rives, A , 10.1126/science.ade2574Science. 37966372023</p>
<p>Large language models reveal big disparities in current wildfire research. Z Lin, A Chen, X Wang, Z Liu, S Piao, 10.1038/s43247-024-01341-7Commun Earth Environ. 511682024</p>
<p>A prompt-engineered large language model, deep learning workflow for materials classification. S Liu, T Wen, Asls Pattamatta, D J Srolovitz, 10.1016/j.mattod.2024.08.028Materials Today. 802024</p>
<p>Data quantity governance for machine learning in materials science. Y Liu, Z Yang, X Zou, S Ma, D Liu, M Avdeev, S Shi, 10.1093/nsr/nwad125National Science Review. 1071252023</p>
<p>A multimodal generative AI copilot for human pathology. M Y Lu, B Chen, Dfk Williamson, R J Chen, M Zhao, A K Chow, K Ikemura, A Kim, D Pouli, A Patel, A Soliman, C Chen, T Ding, J J Wang, G Gerber, I Liang, L P Le, A V Parwani, L L Weishaupt, F Mahmood, 10.1038/s41586-024-07618-3Nature. 63480332024</p>
<p>S Lundberg, S-I Lee, 10.48550/ARXIV.1705.07874A Unified Approach to Interpreting Model Predictions. 2017</p>
<p>From local explanations to global understanding with explainable AI for trees. S M Lundberg, G Erion, H Chen, A Degrave, J M Prutkin, B Nair, R Katz, J Himmelfarb, N Bansal, S-I Lee, 10.1038/s42256-019-0138-9Nat Mach Intell. 212020</p>
<p>Potential Roles of Large Language Models in the Production of Systematic Reviews and Meta-Analyses. X Luo, F Chen, D Zhu, L Wang, Z Wang, H Liu, M Lyu, Y Wang, Q Wang, Y Chen, 10.2196/56780J Med Internet Res. 26e567802024</p>
<p>Augmenting large language models with chemistry tools. M Bran, A Cox, S Schilter, O Baldassari, C White, A D Schwaller, P , 10.1038/s42256-024-00832-8Nat Mach Intell. 652024</p>
<p>Best practices for artificial intelligence in life sciences research. V A Makarov, T Stouch, B Allgood, C D Willis, N Lynch, 10.1016/j.drudis.2021.01.017Drug Discovery Today. 2652021</p>
<p>Artificial intelligence for proteomics and biomarker discovery. M Mann, C Kumar, W-F Zeng, M T Strauss, 10.1016/j.cels.2021.06.006Cell Systems. 1282021</p>
<p>Artificial intelligence and illusions of understanding in scientific research. L Messeri, M J Crockett, 10.1038/s41586-024-07146-0Nature. 6272024</p>
<p>Redefining Radiology: A Review of Artificial Intelligence Integration in Medical Imaging. R Najjar, 10.3390/diagnostics13172760Diagnostics. 131727602023</p>
<p>AlphaEvolve: A coding agent for scientific and algorithmic discovery. A Novikov, N Vũ, M Eisenberger, E Dupont, P-S Huang, A Z Wagner, S Shirobokov, B Kozlovskii, Fjr Ruiz, A Mehrabian, M P Kumar, See A Chaudhuri, S Holland, G Davies, A Nowozin, S Kohli, P Balog, M , 10.48550/ARXIV.2506.131312025</p>
<p>Interpretable and Explainable Machine Learning for Materials Science and Chemistry. F Oviedo, J L Ferres, T Buonassisi, K T Butler, 10.1021/accountsmr.1c00244Acc Mater Res. 362022</p>
<p>AI mirrors experimental science to uncover a mechanism of gene transfer crucial to bacterial evolution. J R Penadé S, J Gottweis, L He, J B Patkowski, A Daryin, W-H Weng, T Tu, A Palepu, A Myaskovsky, A Pawlosky, V Natarajan, A Karthikesalingam, Trd Costa, 10.1016/j.cell.2025.08.018S00928674250097302025</p>
<p>Flexible, model-agnostic method for materials data extraction from text using general purpose language models. M P Polak, S Modi, A Latosinska, J Zhang, C-W Wang, S Wang, A D Hazra, D Morgan, 10.1039/D4DD00016ADigital Discovery. 362024</p>
<p>Extracting accurate materials data from research papers with conversational language models and prompt engineering. M P Polak, D Morgan, 10.1038/s41467-024-45914-8Nat Commun. 15115692024</p>
<p>Accelerating materials discovery using artificial intelligence, high performance computing and robotics. E O Pyzer-Knapp, J W Pitera, Pwj Staar, S Takeda, T Laino, D P Sanders, J Sexton, J R Smith, A Curioni, 10.1038/s41524-022-00765-zComput Mater. 81842022</p>
<p>AI in health and medicine. P Rajpurkar, E Chen, O Banerjee, E J Topol, 10.1038/s41591-021-01614-0Nat Med. 2812022</p>
<p>Diffusion of innovations. E M Rogers, A Singhal, M M Quinlan, An integrated approach to communication theory and research. Routledge2014</p>
<p>Next-Generation Sequencing Technology: Current Trends and Advancements. H Satam, K Joshi, U Mangrolia, S Waghoo, G Zaidi, S Rawool, R P Thakare, S Banday, A K Mishra, G Das, S K Malonia, 10.3390/biology12070997Biology. 1279972023</p>
<p>S Schmidgall, Y Su, Z Wang, X Sun, J Wu, X Yu, J Liu, Z Liu, E Barsoum, 10.48550/ARXIV.2501.04227Agent Laboratory: Using LLM Agents as Research Assistants. 2025</p>
<p>Large language models encode clinical knowledge. K Singhal, S Azizi, T Tu, S S Mahdavi, J Wei, H W Chung, N Scales, A Tanwani, H Cole-Lewis, S Pfohl, P Payne, M Seneviratne, P Gamble, C Kelly, A Babiker, N Schä Rli, A Chowdhery, P Mansfield, D Demner-Fushman, Agüera Y Arcas, B Webster, D Corrado, G S Matias, Y Chou, K Gottweis, J Tomasev, N Liu, Y Rajkomar, A Barral, J Semturs, C Karthikesalingam, A Natarajan, V , 10.1038/s41586-023-06291-2Nature. 62079722023</p>
<p>The Virtual Lab of AI agents designs new SARS-CoV-2 nanobodies. K Swanson, W Wu, N L Bulaong, J E Pak, J Zou, 10.1038/s41586-025-09442-9Nature. 2025</p>
<p>An autonomous laboratory for the accelerated synthesis of novel materials. N J Szymanski, B Rendy, Y Fei, R E Kumar, T He, D Milsted, M J Mcdermott, M Gallant, E D Cubuk, A Merchant, H Kim, A Jain, C J Bartel, K Persson, Y Zeng, G Ceder, 10.1038/s41586-023-06734-wNature. 62479902023</p>
<p>Scientific discovery in the age of artificial intelligence. H Wang, T Fu, Y Du, W Gao, K Huang, Z Liu, P Chandak, S Liu, P Van Katwyk, A Deac, A Anandkumar, K Bergen, C P Gomes, S Ho, P Kohli, J Lasenby, J Leskovec, T-Y Liu, A Manrai, D Marks, B Ramsundar, L Song, J Sun, J Tang, P Veličković, M Welling, L Zhang, C W Coley, Y Bengio, M Zitnik, 10.1038/s41586-023-06221-2Nature. 62079722023</p>
<p>DPFunc: accurately predicting protein function via deep learning with domain-guided structure information. W Wang, Y Shuai, M Zeng, Fan W Li, M , 10.1038/s41467-024-54816-8Nat Commun. 161702025</p>
<p>Molecular contrastive learning of representations via graph neural networks. Y Wang, J Wang, Z Cao, Barati Farimani, A , 10.1038/s42256-022-00447-xNat Mach Intell. 432022</p>
<p>Discovery of a structural class of antibiotics with explainable deep learning. F Wong, E J Zheng, J A Valeri, N M Donghia, M N Anahtar, S Omori, A Li, A Cubillos-Ruiz, A Krishnan, Jin W Manson, A L Friedrichs, J Helbig, R Hajian, B Fiejtek, D K Wagner, F F Soutter, H H Earl, A M Stokes, J M Renner, L D Collins, J J , 10.1038/s41586-023-06887-8Nature. 62679972024</p>
<p>Artificial intelligence: A powerful paradigm for scientific research. Y Xu, X Liu, X Cao, C Huang, E Liu, S Qian, X Liu, Y Wu, F Dong, C-W Qiu, J Qiu, K Hua, W Su, J Wu, H Xu, Y Han, C Fu, Z Yin, M Liu, R Roepman, S Dietmann, M Virta, F Kengara, Z Zhang, L Zhang, T Zhao, J Dai, J Yang, L Lan, M Luo, Z Liu, T An, B Zhang, X He, S Cong, X Liu, W Zhang, J P Lewis, J M Tiedje, Q Wang, An Z Wang, F Zhang, L Huang, T Lu, C Cai, Z Wang, F Zhang, J , 10.1016/j.xinn.2021.100179The Innovation. 241001792021</p>
<p>. A Yang, A Li, B Yang, B Zhang, B Hui, B Zheng, B Yu, C Gao, C Huang, C Lv, C Zheng, D Liu, F Zhou, F Huang, F Hu, H Ge, H Wei, H Lin, J Tang, J Yang, J Tu, J Zhang, J Yang, J Yang, J Zhou, J Zhou, J Lin, K Dang, K Bao, K Yang, L Yu, L Deng, M Li, M Xue, M Li, P Zhang, P Wang, Q Zhu, R Men, R Gao, S Liu, S Luo, T Li, T Tang, W Yin, X Ren, X Wang, X Zhang, X Ren, Y Fan, Y Su, Y Zhang, Y Zhang, Y Wan, Y Liu, Z Wang, Z Cui, Z Zhang, Z Zhou, Z Qiu, 10.48550/ARXIV.2505.093882025Qwen3 Technical Report</p>
<p>MOOSE-Chem: Large Language Models for Rediscovering Unseen Chemistry Scientific Hypotheses. Z Yang, W Liu, B Gao, T Xie, Y Li, W Ouyang, S Poria, E Cambria, D Zhou, 10.48550/ARXIV.2410.070762024</p>
<p>Interpretable machine learning for investigating complex nanomaterial-plant-soil interactions. H Yu, Z Zhao, D Luo, F Cheng, 10.1039/D2EN00181KEnviron Sci: Nano. 9112022</p>
<p>A data-driven approach to predict the compressive strength of alkali-activated materials and correlation of influencing parameters using SHapley Additive exPlanations (SHAP) analysis. X Zheng, Y Xie, X Yang, M N Amin, S Nazar, S A Khan, F Althoey, A F Deifalla, 10.1016/j.jmrt.2023.06.207Journal of Materials Research and Technology. 252023</p>            </div>
        </div>

    </div>
</body>
</html>
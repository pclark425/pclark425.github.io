<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5348 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5348</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5348</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-111.html">extraction-schema-111</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <p><strong>Paper ID:</strong> paper-2ca2332434413affd2747697c4b7c4ffda139471</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/2ca2332434413affd2747697c4b7c4ffda139471" target="_blank">Path-LLM: A Shortest-Path-based LLM Learning for Unified Graph Representation</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> A novel Path-LLM model to efficiently learn unified graph representation, which leverages a powerful large language model (LLM) to incorporate the authors' proposed path features and theoretically analyze the algorithm complexity of the Path-LLM approach.</p>
                <p><strong>Paper Abstract:</strong> Unified graph representation learning aims to generate node embeddings, which can be applied to multiple downstream applications of graph analytics. However, existing studies based on graph neural networks and language models either suffer from the limitations of numerous training needs toward specific downstream predictions, poor generalization, or shallow semantic features. In this work, we propose a novel Path-LLM model to efficiently learn unified graph representation, which leverages a powerful large language model (LLM) to incorporate our proposed path features. Our Path-LLM framework consists of four well-designed techniques. First, we develop a new mechanism of long-to-short shortest path (L2SP) selection, which can cover key connections between different dense groups. An in-depth analysis and comparison of different path selections is conducted to justify the rationale behind our designed L2SP method. Next, we design path textualization to obtain L2SP-based training texts with key phrase selection from node text attributes. We then feed the texts into a self-supervised LLM training process to align next node/edge generation in L2SP with next token generation in causal language modeling for graph representation learning and finally extract the unified graph embeddings. We theoretically analyze the algorithm complexity of our Path-LLM approach. Extensive experiments on large-scale graph benchmarks validate the superiority of Path-LLM against state-of-the-art methods WalkLM, GraphGPT, OFA, and GraphTranslator on two classical graph learning tasks (node classification and edge validation) and one NP-hard graph query processing task (keyword search). Compared with WalkLM, our approach saves more than 90% of training paths on millions-scale graphs and runs at most 35x faster.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5348.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5348.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Path-LLM L2SP Textualization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Path-LLM long-to-short shortest-path (L2SP) path textualization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph->text representation used in Path-LLM that (1) selects long shortest paths across the graph, (2) cuts them into short shortest-path segments (L2SP), and (3) converts each short path into a compact textual sequence by cleaning node texts, extracting keyphrases (PositionRank), and concatenating node and edge attributes with simple templates suited for homogeneous or heterogeneous TAGs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>L2SP-based path textualization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Two-stage process: (A) L2SP selection — sample b source nodes, BFS to find target nodes at distance >= L, get k long shortest paths between sampled pairs, then cut each long path of length L into overlapping/adjacent short shortest paths of max length ℓ via the L2SP conversion formula; (B) Path textualization Φ(·) — for each short path, clean node text attributes, extract keyphrases with PositionRank to form new node text attributes X_v^t, and assemble a text sequence by concatenating node tokens and edge attributes with a simple template: for homogeneous graphs a '〈paper with content ...〉' style template is used (citation networks), for heterogeneous graphs the text is formed as 〈X_v1^t | X_e1,2 | X_v2^t | ...〉. The produced texts align node/edge ordering with token ordering for causal LM training.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Text-Attributed Graphs (TAGs): homogeneous citation networks and heterogeneous biomedical/knowledge graphs</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Alignment with causal language modeling (next-token ≈ next-node/edge generation); compactness (short paths, keyphrase extraction reduces long texts); robustness (shortest paths avoid noisy detours and cycles vs random walks); better coverage of bridge/ inter-community edges thanks to sampling long paths and cutting into short segments; sample-efficiency (far fewer training paths than random-walk-based methods); requires pre-cleaning and keyphrase extraction (PositionRank). The representation trades off very long-range context (cutting long paths) for better local path faithfulness and LLM-fit.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Used to train/fine-tune LLMs and produce node embeddings evaluated on node classification, edge validation (link existence), and a keyword-search (Steiner tree) task.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Node classification: Macro-F1 and Micro-F1 (e.g., PubMed Macro-F1=0.7595, Micro-F1=0.7674; Cora Macro-F1=0.7524, Micro-F1=0.7773; ARXIV Micro-F1=0.6616); Edge validation: AUC and Accuracy (e.g., PubMed AUC=0.7497, Acc=0.7111; Cora AUC=0.9244, Acc=0.8476); Keyword search: average answer distances (lower is better) (|Q|=2 Path-LLM=0.93 vs WalkLM=1.27); Efficiency: training-path savings ~91.09% on average vs WalkLM, average training speed ~12× faster and up to ~35× faster on PubMed (Path-LLM 380s vs WalkLM 13,370s); robustness: ~4% performance drop with 20% added noisy tokens in Cora.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Compared to random-walk-based textualization (WalkLM) and other LM/LLM baselines (WalkLM, GraphGPT, OFA, GraphTranslator, Llama2), L2SP textualization + causal-LM fine-tuning (Path-LLM) produced consistently better embeddings across tasks. Example: PubMed node classification Macro-F1 Path-LLM=0.7595 vs WalkLM=0.2871; edge validation AUC PubMed Path-LLM=0.7497 vs WalkLM=0.5962. L2SP also uses far fewer training paths (~9% of WalkLM's paths) and yields much faster training.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Requires preprocessing (text cleaning and keyphrase extraction) which can remove useful information if over-aggressive; cutting long paths into short segments can lose long-range dependencies and create parameter choices (L, k, ℓ) that must be tuned (ablation showed sensitivity to L and k); relies on availability of informative node/edge text attributes; computational cost depends on LLM training (black-box B_LLM complexity) and attention cost grows with text length (but L2SP keeps texts short); possible information loss where graph structure beyond sampled shortest paths is important.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Path-LLM: A Shortest-Path-based LLM Learning for Unified Graph Representation', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5348.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5348.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WalkLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>WalkLM: A Uniform Language Model Fine-tuning Framework for Attributed Graph Embedding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior graph->text approach that samples random walks on the graph and converts walks into textual sequences to fine-tune language models (RoBERTa) to produce node embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>WalkLM: A Uniform Language Model Fine-tuning Framework for Attributed Graph Embedding</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Random-walk-based path textualization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Random walks are sampled from the graph (many walks per node), nodes' text attributes along each walk are concatenated into textual sequences which are used to fine-tune a language model (WalkLM used RoBERTa in prior work); training uses masked-LM style objectives in that work (in contrast to causal LM alignment).</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Text-Attributed Graphs (TAGs)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Simple and straightforward serialization via walks; tends to produce long and irregular sequences; vulnerable to noisy nodes and cycles; commonly stays inside dense subgraphs and thus may miss bridge edges; not well-matched to causal next-token generation because next-node in random walk is often unpredictable.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Used as a baseline for unified graph embedding evaluation tasks in this paper: node classification and edge validation and keyword search comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Examples reported in this paper as baseline: Node classification (PubMed Macro-F1=0.2871, Micro-F1=0.4070); Edge validation (PubMed AUC=0.5962, Acc=0.5684); training paths required (e.g., PubMed 295,512 RW paths vs Path-LLM 19,670 L2SP paths).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Path-LLM's L2SP textualization outperforms WalkLM both in embedding quality and efficiency; Path-LLM requires much fewer training paths (~9% of WalkLM's on average) and trains orders of magnitude faster (avg ~12×, up to ~35× on some datasets).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Random walks: noisy, contain cycles, limited coverage of inter-community bridge edges, and misaligned with causal LLM generation; requires many sampled paths (large |N|) and produces longer texts, increasing LM compute cost.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Path-LLM: A Shortest-Path-based LLM Learning for Unified Graph Representation', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5348.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5348.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OFA (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>One For All: Towards Training One Graph Model For All Classification Tasks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline that uses natural-language descriptions of nodes (textual node descriptions) encoded by a language model to produce node embeddings for downstream graph tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>One For All: Towards Training One Graph Model For All Classification Tasks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Natural-language node descriptions</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Nodes are represented by natural language textual descriptions (node attributes expressed in NL) and encoded by an LM to get node embeddings; minimal explicit structural serialization is described in this paper (OFA focuses on training a single graph model across tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Text-Attributed Graphs</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Leverages LM semantic understanding of node descriptions; simple to implement; may underrepresent explicit graph topology unless augmented with structural features.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Serves as a baseline for node classification and edge validation in the experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported in paper: Node classification (PubMed Macro-F1=0.6520, Micro-F1=0.6761); Edge validation (PubMed AUC=0.6061, Acc=0.5719).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Path-LLM outperforms OFA across datasets in both node classification and edge validation, indicating that path-aware textualization plus self-supervised LLM training better captures structure+semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Descriptive node texts may not encode relational structure; performance depends on quality/consistency of node descriptions; additional steps needed to inject topology.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Path-LLM: A Shortest-Path-based LLM Learning for Unified Graph Representation', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5348.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5348.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GraphGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GraphGPT: Graph instruction tuning for large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that adapts instruction tuning/self-supervised tuning of LLMs for graph tasks, with emphasis on QA and reasoning rather than producing unified node embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Graphgpt: Graph instruction tuning for large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Instruction-tuned graph textualization (QA-focused)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Graph information presented to LLMs via instruction-style prompts and tuned examples (self-supervised instruction tuning) to improve question answering/reasoning over graphs; representation is designed for instruction/QA tasks more than for deriving per-node unified embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Various graphs used for QA/reasoning tasks (mentioned as LLM adaptation for graphs)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Optimized for instruction-following and reasoning; not primarily designed as a serialization for embedding extraction; may excel at QA but less directly comparable for unified embedding tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Included as a strong LLM-based baseline in node classification and edge validation comparisons in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Examples in this paper: Node classification (PubMed Macro-F1=0.7088, Micro-F1=0.7202); Edge validation (PubMed AUC=0.7134, Acc=0.6631).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Path-LLM outperforms GraphGPT on the unified embedding tasks reported here (node classification, edge validation), indicating specialized L2SP textualization plus causal LM pretraining is better suited for unified embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>GraphGPT focuses on instruction tuning for QA/reasoning; adapting it to produce compact per-node embeddings for multiple downstream tasks may require additional design.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Path-LLM: A Shortest-Path-based LLM Learning for Unified Graph Representation', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5348.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5348.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GraphTranslator</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GraphTranslator: Aligning graph model to large language model for open-ended tasks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline method that aligns graph models with LLMs for open-ended tasks by learning graph-text alignment; used as comparison in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Graphtranslator: Aligning graph model to large language model for open-ended tasks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Graph-text alignment (graph→text embeddings)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Method aligns graph model outputs with LLM textual representations to support open-ended tasks; the paper references it as a baseline rather than detailing conversion rules.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Text-Attributed Graphs / general graphs for open-ended tasks</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Focuses on alignment between graph encoders and LLM text representations; may combine structural and semantic alignment losses.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Used as a baseline for node classification and edge validation in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Examples from this paper: Node classification (PubMed Macro-F1=0.4917, Micro-F1=0.5396); Edge validation (PubMed AUC=0.5939, Acc=0.5618).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Path-LLM achieves better downstream performance across datasets; GraphTranslator performs worse on the unified embedding tasks reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>As presented here, alignment methods may require careful loss/design to capture both structure and semantics; details not elaborated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Path-LLM: A Shortest-Path-based LLM Learning for Unified Graph Representation', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>WalkLM: A Uniform Language Model Fine-tuning Framework for Attributed Graph Embedding <em>(Rating: 2)</em></li>
                <li>Graphgpt: Graph instruction tuning for large language models <em>(Rating: 2)</em></li>
                <li>One For All: Towards Training One Graph Model For All Classification Tasks <em>(Rating: 1)</em></li>
                <li>Graphtranslator: Aligning graph model to large language model for open-ended tasks <em>(Rating: 1)</em></li>
                <li>Positionrank: An unsupervised approach to keyphrase extraction from scholarly documents <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5348",
    "paper_id": "paper-2ca2332434413affd2747697c4b7c4ffda139471",
    "extraction_schema_id": "extraction-schema-111",
    "extracted_data": [
        {
            "name_short": "Path-LLM L2SP Textualization",
            "name_full": "Path-LLM long-to-short shortest-path (L2SP) path textualization",
            "brief_description": "A graph-&gt;text representation used in Path-LLM that (1) selects long shortest paths across the graph, (2) cuts them into short shortest-path segments (L2SP), and (3) converts each short path into a compact textual sequence by cleaning node texts, extracting keyphrases (PositionRank), and concatenating node and edge attributes with simple templates suited for homogeneous or heterogeneous TAGs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "L2SP-based path textualization",
            "representation_description": "Two-stage process: (A) L2SP selection — sample b source nodes, BFS to find target nodes at distance &gt;= L, get k long shortest paths between sampled pairs, then cut each long path of length L into overlapping/adjacent short shortest paths of max length ℓ via the L2SP conversion formula; (B) Path textualization Φ(·) — for each short path, clean node text attributes, extract keyphrases with PositionRank to form new node text attributes X_v^t, and assemble a text sequence by concatenating node tokens and edge attributes with a simple template: for homogeneous graphs a '〈paper with content ...〉' style template is used (citation networks), for heterogeneous graphs the text is formed as 〈X_v1^t | X_e1,2 | X_v2^t | ...〉. The produced texts align node/edge ordering with token ordering for causal LM training.",
            "graph_type": "Text-Attributed Graphs (TAGs): homogeneous citation networks and heterogeneous biomedical/knowledge graphs",
            "representation_properties": "Alignment with causal language modeling (next-token ≈ next-node/edge generation); compactness (short paths, keyphrase extraction reduces long texts); robustness (shortest paths avoid noisy detours and cycles vs random walks); better coverage of bridge/ inter-community edges thanks to sampling long paths and cutting into short segments; sample-efficiency (far fewer training paths than random-walk-based methods); requires pre-cleaning and keyphrase extraction (PositionRank). The representation trades off very long-range context (cutting long paths) for better local path faithfulness and LLM-fit.",
            "evaluation_task": "Used to train/fine-tune LLMs and produce node embeddings evaluated on node classification, edge validation (link existence), and a keyword-search (Steiner tree) task.",
            "performance_metrics": "Node classification: Macro-F1 and Micro-F1 (e.g., PubMed Macro-F1=0.7595, Micro-F1=0.7674; Cora Macro-F1=0.7524, Micro-F1=0.7773; ARXIV Micro-F1=0.6616); Edge validation: AUC and Accuracy (e.g., PubMed AUC=0.7497, Acc=0.7111; Cora AUC=0.9244, Acc=0.8476); Keyword search: average answer distances (lower is better) (|Q|=2 Path-LLM=0.93 vs WalkLM=1.27); Efficiency: training-path savings ~91.09% on average vs WalkLM, average training speed ~12× faster and up to ~35× faster on PubMed (Path-LLM 380s vs WalkLM 13,370s); robustness: ~4% performance drop with 20% added noisy tokens in Cora.",
            "comparison_to_other_representations": "Compared to random-walk-based textualization (WalkLM) and other LM/LLM baselines (WalkLM, GraphGPT, OFA, GraphTranslator, Llama2), L2SP textualization + causal-LM fine-tuning (Path-LLM) produced consistently better embeddings across tasks. Example: PubMed node classification Macro-F1 Path-LLM=0.7595 vs WalkLM=0.2871; edge validation AUC PubMed Path-LLM=0.7497 vs WalkLM=0.5962. L2SP also uses far fewer training paths (~9% of WalkLM's paths) and yields much faster training.",
            "limitations_or_challenges": "Requires preprocessing (text cleaning and keyphrase extraction) which can remove useful information if over-aggressive; cutting long paths into short segments can lose long-range dependencies and create parameter choices (L, k, ℓ) that must be tuned (ablation showed sensitivity to L and k); relies on availability of informative node/edge text attributes; computational cost depends on LLM training (black-box B_LLM complexity) and attention cost grows with text length (but L2SP keeps texts short); possible information loss where graph structure beyond sampled shortest paths is important.",
            "uuid": "e5348.0",
            "source_info": {
                "paper_title": "Path-LLM: A Shortest-Path-based LLM Learning for Unified Graph Representation",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "WalkLM",
            "name_full": "WalkLM: A Uniform Language Model Fine-tuning Framework for Attributed Graph Embedding",
            "brief_description": "A prior graph-&gt;text approach that samples random walks on the graph and converts walks into textual sequences to fine-tune language models (RoBERTa) to produce node embeddings.",
            "citation_title": "WalkLM: A Uniform Language Model Fine-tuning Framework for Attributed Graph Embedding",
            "mention_or_use": "use",
            "representation_name": "Random-walk-based path textualization",
            "representation_description": "Random walks are sampled from the graph (many walks per node), nodes' text attributes along each walk are concatenated into textual sequences which are used to fine-tune a language model (WalkLM used RoBERTa in prior work); training uses masked-LM style objectives in that work (in contrast to causal LM alignment).",
            "graph_type": "Text-Attributed Graphs (TAGs)",
            "representation_properties": "Simple and straightforward serialization via walks; tends to produce long and irregular sequences; vulnerable to noisy nodes and cycles; commonly stays inside dense subgraphs and thus may miss bridge edges; not well-matched to causal next-token generation because next-node in random walk is often unpredictable.",
            "evaluation_task": "Used as a baseline for unified graph embedding evaluation tasks in this paper: node classification and edge validation and keyword search comparisons.",
            "performance_metrics": "Examples reported in this paper as baseline: Node classification (PubMed Macro-F1=0.2871, Micro-F1=0.4070); Edge validation (PubMed AUC=0.5962, Acc=0.5684); training paths required (e.g., PubMed 295,512 RW paths vs Path-LLM 19,670 L2SP paths).",
            "comparison_to_other_representations": "Path-LLM's L2SP textualization outperforms WalkLM both in embedding quality and efficiency; Path-LLM requires much fewer training paths (~9% of WalkLM's on average) and trains orders of magnitude faster (avg ~12×, up to ~35× on some datasets).",
            "limitations_or_challenges": "Random walks: noisy, contain cycles, limited coverage of inter-community bridge edges, and misaligned with causal LLM generation; requires many sampled paths (large |N|) and produces longer texts, increasing LM compute cost.",
            "uuid": "e5348.1",
            "source_info": {
                "paper_title": "Path-LLM: A Shortest-Path-based LLM Learning for Unified Graph Representation",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "OFA (baseline)",
            "name_full": "One For All: Towards Training One Graph Model For All Classification Tasks",
            "brief_description": "Baseline that uses natural-language descriptions of nodes (textual node descriptions) encoded by a language model to produce node embeddings for downstream graph tasks.",
            "citation_title": "One For All: Towards Training One Graph Model For All Classification Tasks",
            "mention_or_use": "use",
            "representation_name": "Natural-language node descriptions",
            "representation_description": "Nodes are represented by natural language textual descriptions (node attributes expressed in NL) and encoded by an LM to get node embeddings; minimal explicit structural serialization is described in this paper (OFA focuses on training a single graph model across tasks).",
            "graph_type": "Text-Attributed Graphs",
            "representation_properties": "Leverages LM semantic understanding of node descriptions; simple to implement; may underrepresent explicit graph topology unless augmented with structural features.",
            "evaluation_task": "Serves as a baseline for node classification and edge validation in the experiments.",
            "performance_metrics": "Reported in paper: Node classification (PubMed Macro-F1=0.6520, Micro-F1=0.6761); Edge validation (PubMed AUC=0.6061, Acc=0.5719).",
            "comparison_to_other_representations": "Path-LLM outperforms OFA across datasets in both node classification and edge validation, indicating that path-aware textualization plus self-supervised LLM training better captures structure+semantics.",
            "limitations_or_challenges": "Descriptive node texts may not encode relational structure; performance depends on quality/consistency of node descriptions; additional steps needed to inject topology.",
            "uuid": "e5348.2",
            "source_info": {
                "paper_title": "Path-LLM: A Shortest-Path-based LLM Learning for Unified Graph Representation",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "GraphGPT",
            "name_full": "GraphGPT: Graph instruction tuning for large language models",
            "brief_description": "An approach that adapts instruction tuning/self-supervised tuning of LLMs for graph tasks, with emphasis on QA and reasoning rather than producing unified node embeddings.",
            "citation_title": "Graphgpt: Graph instruction tuning for large language models",
            "mention_or_use": "use",
            "representation_name": "Instruction-tuned graph textualization (QA-focused)",
            "representation_description": "Graph information presented to LLMs via instruction-style prompts and tuned examples (self-supervised instruction tuning) to improve question answering/reasoning over graphs; representation is designed for instruction/QA tasks more than for deriving per-node unified embeddings.",
            "graph_type": "Various graphs used for QA/reasoning tasks (mentioned as LLM adaptation for graphs)",
            "representation_properties": "Optimized for instruction-following and reasoning; not primarily designed as a serialization for embedding extraction; may excel at QA but less directly comparable for unified embedding tasks.",
            "evaluation_task": "Included as a strong LLM-based baseline in node classification and edge validation comparisons in the paper.",
            "performance_metrics": "Examples in this paper: Node classification (PubMed Macro-F1=0.7088, Micro-F1=0.7202); Edge validation (PubMed AUC=0.7134, Acc=0.6631).",
            "comparison_to_other_representations": "Path-LLM outperforms GraphGPT on the unified embedding tasks reported here (node classification, edge validation), indicating specialized L2SP textualization plus causal LM pretraining is better suited for unified embeddings.",
            "limitations_or_challenges": "GraphGPT focuses on instruction tuning for QA/reasoning; adapting it to produce compact per-node embeddings for multiple downstream tasks may require additional design.",
            "uuid": "e5348.3",
            "source_info": {
                "paper_title": "Path-LLM: A Shortest-Path-based LLM Learning for Unified Graph Representation",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "GraphTranslator",
            "name_full": "GraphTranslator: Aligning graph model to large language model for open-ended tasks",
            "brief_description": "A baseline method that aligns graph models with LLMs for open-ended tasks by learning graph-text alignment; used as comparison in experiments.",
            "citation_title": "Graphtranslator: Aligning graph model to large language model for open-ended tasks",
            "mention_or_use": "use",
            "representation_name": "Graph-text alignment (graph→text embeddings)",
            "representation_description": "Method aligns graph model outputs with LLM textual representations to support open-ended tasks; the paper references it as a baseline rather than detailing conversion rules.",
            "graph_type": "Text-Attributed Graphs / general graphs for open-ended tasks",
            "representation_properties": "Focuses on alignment between graph encoders and LLM text representations; may combine structural and semantic alignment losses.",
            "evaluation_task": "Used as a baseline for node classification and edge validation in this paper.",
            "performance_metrics": "Examples from this paper: Node classification (PubMed Macro-F1=0.4917, Micro-F1=0.5396); Edge validation (PubMed AUC=0.5939, Acc=0.5618).",
            "comparison_to_other_representations": "Path-LLM achieves better downstream performance across datasets; GraphTranslator performs worse on the unified embedding tasks reported here.",
            "limitations_or_challenges": "As presented here, alignment methods may require careful loss/design to capture both structure and semantics; details not elaborated in this paper.",
            "uuid": "e5348.4",
            "source_info": {
                "paper_title": "Path-LLM: A Shortest-Path-based LLM Learning for Unified Graph Representation",
                "publication_date_yy_mm": "2024-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "WalkLM: A Uniform Language Model Fine-tuning Framework for Attributed Graph Embedding",
            "rating": 2
        },
        {
            "paper_title": "Graphgpt: Graph instruction tuning for large language models",
            "rating": 2
        },
        {
            "paper_title": "One For All: Towards Training One Graph Model For All Classification Tasks",
            "rating": 1
        },
        {
            "paper_title": "Graphtranslator: Aligning graph model to large language model for open-ended tasks",
            "rating": 1
        },
        {
            "paper_title": "Positionrank: An unsupervised approach to keyphrase extraction from scholarly documents",
            "rating": 2
        }
    ],
    "cost": 0.017993,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Path-LLM: A Shortest-Path-based LLM Learning for Unified Graph Representation</h1>
<p>Wenbo Shang<br>Hong Kong Baptist University<br>Hong Kong, China<br>cswbshang@comp.hkbu.edu.hk</p>
<p>Xuliang Zhu<br>Shanghai Jiao Tong University<br>Shanghai, China<br>zhu.xl@sjtu.edu.cn</p>
<p>Xin Huang<br>Hong Kong Baptist University<br>Hong Kong, China<br>xinhuang@comp.hkbu.edu.hk</p>
<h2>ABSTRACT</h2>
<p>Unified graph representation learning aims to generate node embeddings, which can be applied to multiple downstream applications of graph analytics. However, existing studies based on graph neural networks and language models either suffer from the limitations of numerous training needs toward specific downstream predictions, poor generalization, or shallow semantic features. In this work, we propose a novel Path-LLM model to efficiently learn unified graph representation, which leverages a powerful large language model (LLM) to incorporate our proposed path features. Our PathLLM framework consists of four well-designed techniques. First, we develop a new mechanism of long-to-short shortest path (L2SP) selection, which can cover key connections between different dense groups. An in-depth analysis and comparison of different path selections is conducted to justify the rationale behind our designed L2SP method. Next, we design path textualization to obtain L2SP-based training texts with key phrase selection from node text attributes. We then feed the texts into a self-supervised LLM training process to align next node/edge generation in L2SP with next token generation in causal language modeling for graph representation learning and finally extract the unified graph embeddings. We theoretically analyze the algorithm complexity of our Path-LLM approach. Extensive experiments on large-scale graph benchmarks validate the superiority of Path-LLM against state-of-the-art methods WalkLM, GraphGPT, OFA, and GraphTranslator on two classical graph learning tasks (node classification and edge validation) and one NP-hard graph query processing task (keyword search). Compared with WalkLM, our approach saves more than $90 \%$ of training paths on millions-scale graphs and runs at most $35 \times$ faster. Besides the quality and efficiency evaluations, we have also conducted several ablation studies, case studies, and embedding visualizations to show the effectiveness of Path-LLM.</p>
<h2>1 INTRODUCTION</h2>
<p>Graphs play a crucial role in various real-world application scenarios, including academic networks, biomedical graphs, social networks, financial networks, and so on [4, 19, 65, 77, 90]. These graph data associated with textual information, such as the attributes of nodes and edges, representing complex and diverse semantics, are commonly known as text-attributed graphs (TAGs) [41, 47, 82]. A wide range of graph representation learning models has been proposed, including successful graph neural networks (GNNs) [25, $40,72,87]$. However, these models often require sufficient training toward specific downstream predictions [69, 92] and focus more on processing graph structure, ignoring rich semantics.</p>
<p>For obtaining graph representation with richer semantics, existing path-based works [69] have recently proposed to integrate LMs
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: A comparison of existing works and our work.
and random walks to derive unified graph embeddings for multiple graph learning tasks, e.g., node classification and edge validation. However, it still suffers from the following three limitations. 1) The sampled random walks can hardly cover the bridge edges between different dense groups, as the path of the random walk has a high probability of falling within one dense subgraph; 2) random walks can easily involve noisy nodes to damage the embedding quality. Meanwhile, the node texts of irregular paths cannot match the linguistic rules, bringing more difficulty for learning models, and 3) the transferability of existing path-based works to LLMs with causal language modeling is limited, as it is primarily designed for small LMs with masked language modeling, such as BERT [13] and DeBERTa [28], which generally have weaker performance than the recent LLMs [18].</p>
<p>To tackle the above limitations, we propose a novel graph representation learning model of Path-LLM, as shown in Figure 1. Our developed Path-LLM utilizes powerful LLMs with causal language modeling, well-designed shortest-path-based graph features and a shortest-path-based self-supervised learning method to learn unified graph embeddings for several downstream tasks, including node classification, edge validation, and one typical NP-hard graph analytics task of keyword search [45, 73]. Path-LLM enjoys two major advantages based on LLMs and shortest paths. First, the advent of LLMs (e.g., GPT [21, 61] and Llama [16]) has revolutionized the field of language processing. LLMs possess billions of parameters and have been trained on extensive corpora, enabling them to exhibit powerful semantic representation capabilities and strong generalization abilities [17, 85, 93]. Second, the shortest path is the closest path between two nodes, which can avoid unnecessary detours through noisy nodes and cycles, in contrast to random walks. The shortest paths usually indicate regular paths to follow in a complex graph, indicating good-quality learning features for LLMs. Third,</p>
<p>shortest paths are suitable for causal language modeling in LLMs, as searching the next node based on previous nodes is more aligned with generating next tokens based on prefix tokens, against masked language modeling in existing works.</p>
<p>It is challenging to achieve a diverse and high-quality set of shortest paths and enable LLMs to understand complex attributes and graph structures in self-supervised manners. To tackle it, we construct a new path extraction mechanism to select training features, called the long-to-short shortest paths. It first samples a few long shortest paths to capture both cross-group connections and internal group structures and then cuts them into short paths in designed ways for Path-LLM effective and efficient learning. In addition, we design a path textualization function to transform L2SP-based structural information into L2SP-based texts for PathLLM learning. To ease the Path-LLM learning process, we conduct data cleaning and key phrase selection based on PositionRank [20] to reconstruct new text attributes of nodes during path textualization. To embed graph structures into the semantic space, we feed L2SP-based texts into Path-LLM for a self-supervised pre-training process. During the training process, we align L2SP-based shortest paths with the LLM-learned linguistic rules. As Path-LLM learns the order of tokens in the L2SP-based text, it also learns the order of nodes and edges within the L2SP, thereby learning graph structures. Finally, we derive an integrated embedding for all nodes from the frozen Path-LLM, which is effective for several downstream tasks. To summarize, we make the following contributions:</p>
<ul>
<li>We propose a novel Path-LLM model for generating unified graph embeddings, which can effectively handle both homogeneous and heterogeneous graphs for multiple downstream graph analytics tasks. (Section 4)</li>
<li>We first propose the long-to-short shortest paths (L2SP) and design a path textualization function to construct L2SPbased texts. Moreover, we conduct a comprehensive analysis by comparing our L2SP method against different path selections to show the advantages and suitability of short paths and L2SP selection in Path-LLM learning. (Section 5)</li>
<li>We propose a new Path-LLM graph feature learning method for graph embedding learning, which aligns next node/edge generation in L2SP with next token generation in causal language modeling. We then extract Path-LLM embeddings for downstream tasks. Furthermore, we give a complexity analysis of Path-LLM, and provide theoretical proof demonstrating that Path-LLM can learn the graph structure through self-supervised learning of L2SP-based texts. (Section 6)</li>
<li>To illustrate the usefulness of our learned embedding results, we revisit a useful but challenging graph task of keyword search [81], which finds a subgraph covering all keywords with tight closeness of topology structure and node semantics. We develop a method of weighted TAG construction based on the Path-LLM embedding vectors and introduce approximate solutions. (Section 7)</li>
<li>We conduct extensive experiments to validate the effectiveness of Path-LLM outperforming the state-of-the-art WalkLM method on four real-world benchmarks. Our PathLLM model only uses $9 \%$ of WalkLM's training paths on
average. We also conduct a case study of keyword search on PubMed and embedding visualization to show the usefulness of our Path-LLM model. (Section 8)
We give preliminaries in Section 3. We review related work in Section 2 and conclude the paper in Section 9.</li>
</ul>
<h2>2 RELATED WORK</h2>
<p>In recent years, various graph representation learning approaches have been studied [37, 49, 50, 66, 74].
GNN-based graph representation learning. Graph neural networks (GNNs) have been employed to learn node representations by aggregating information from neighboring nodes on graphs [22, 27, 40, 46, 72, 76, 89, 91]. Some studies [33, 36, 57, 84] have utilized self-supervision techniques to pre-train a sophisticated GNN model for unified graph embedding learning, such as contrastive learning. However, this method predominantly focuses on the graph structure, leading to shallow and rough alignment of semantic information and insufficient integration of structural and semantic information.
LM-based graph representation learning. Several studies have employed diverse methodologies to generate graph embeddings using Language Models (LM) [29, 31, 48, 52, 62, 69, 75]. Current methodologies for LLM-based graph representation can be classified into two main categories: supervised and self-supervised graph representation methods. Numerous cutting-edge supervised techniques are specifically designed for single graph learning tasks [83], such as node classification, exemplified by TAPE [29] and GraphAdapter [35]. Furthermore, further supervised approaches train their models across one more task [10], such as OFA [53] and GraphTranslator [86]. Unlike supervised methods, which require extensive labeled data and are specifically trained for particular downstream tasks, self-supervised methods do not need labeled data or human manual resources. Currently, the state-of-the-art approach, WalkLM [69], still utilizes LM to generate embeddings, which is markedly inferior to that of LLM [17, 18], such as GPT-4 [61] and Llama [16]. Although GraphGPT [70] employs a self-supervised learning approach to adapt LLMs for various downstream tasks, it primarily focuses on enhancing question-answering and reasoning capabilities rather than deriving unified graph embeddings. Different from most existing studies, our emphasis is to generate unified embeddings for each node to capture both its topological and semantic features. Our learning model leverages advanced LLMs and well-developed shortest path selection to tackle graph learning tasks and one NP-hard graph querying task of keyword search.</p>
<h2>3 PRELIMINARY</h2>
<p>In this section, we introduce the causal language modeling in LLMs, notations, and the objective of our Path-LLM model.</p>
<h3>3.1 Causal Language Modeling in LLMs</h3>
<p>For the pre-training process, LLMs mainly use Causal Language Modeling (CLM) training techniques, which are trained to predict the next token $x_{i}$ in a sequence $x=\left{x_{1}, x_{2}, \ldots, x_{q}\right}$ based on prefix tokens $x_{&lt;i}=\left{x_{1}, x_{2}, \ldots, x_{i-1}\right}$. CLM is commonly used to train LLMs like GPT [21, 61] and Llama [16]. LLM is typically trained to optimize a conditional probability distribution $p\left(x_{i} \mid x_{&lt;i}\right)$ [29],</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Our proposed Path-LLM framework involves four key components: (1) Long-to-short shortest path (L2SP) selection, converting sampled long shortest paths to L2SP-based shortest paths. (2) Path textualization, transforming L2SP-based shortest paths to L2SP-based texts. (3) Path-LLM pre-training, learning graph structures from L2SP-based texts. (4) Path-LLM embedding generation, deriving the final embeddings from the frozen Path-LLM.
which assigns a probability to each possible $x_{i}^{\prime} \in \mathcal{D}$ given prefix tokens $x_{&lt;i}$, where $\mathcal{D}$ is the LLM vocabulary. Thus, the probability of the output sequence $x[29,64,79]$ can be formulated as :</p>
<p>$$
p(x)=\prod_{i=1}^{9} p\left(x_{i} \mid x_{&lt;i}\right)
$$</p>
<p>Notably, the probability of generating token $x_{i}$ depends only on the prefix tokens $x_{<i}$, showing that LLMs are blind to the following tokens $x_{>i}$ after $x_{i}$.</p>
<h3>3.2 Problem Formulation</h3>
<p>Text-attributed graphs. A text-attributed graph (TAG) can be represented as $G=\left(V, E, \mathcal{X}<em e="e">{v}, \mathcal{X}</em>}\right)$, where $V$ and $E$ denote the set of nodes and edges. $V=\left{v_{1}, v_{2}, \ldots, v_{n}\right}$ is the set of $n$ nodes paired with raw text attributes $\mathcal{X<em v__1="v_{1">{v}=\left{\mathcal{X}</em>}}, \mathcal{X<em 2="2">{v</em>}}, \ldots, \mathcal{X<em n="n">{v</em>}}\right} . E=\left{e_{i, j}\right}$ is the set of edges where $e_{i, j}$ is an edge from $v_{i}$ to $v_{j}$ and is paired with raw text attributes $\mathcal{X<em e__i_="e_{i," j="j">{e}=\left{\mathcal{X}</em>=t$ ) that over all possible paths between these two nodes with the fewest edges.
Objective of Path-LLM. Given a text-attributed graph $G$, the goal of Path-LLM is to generate unified graph embeddings $\xi$ integrating complex graph structures and text attribute semantics in $G$, and then improve task performances on multiple downstream tasks (e.g., node classification, edge validation and keyword search). Specifically, each node $v_{i} \in V$ is paired with the embedding $\xi_{v_{i}}$ extracted from Path-LLM. Note that, different from supervised methods, we
focus on self-supervised graph representation learning methods, where models learn by solving pretext tasks, with supervision signals automatically derived from the data itself [54, 55, 78].}}\right}$. The path in a TAG can be defined as $\mathcal{P}=\left\langle v_{1}, v_{2}, \ldots, v_{\ell}\right\rangle$, such that $v_{i}$ is adjacent to $v_{i+1}$ for $1 \leq i&lt;\ell$. Such a path $\mathcal{P}$ is called a path of length $\ell$ from $v_{1}$ to $v_{\ell}$. Furthermore, the shortest path from $s$ to $t$ is the path $\mathcal{P}=\left\langle v_{1}, v_{2}, \ldots, v_{\ell}\right\rangle$ (where $v_{1}=s$ and $v_{\ell</p>
<h2>4 AN OVERVIEW OF PATH-LLM</h2>
<p>In this section, we introduce our Path-LLM model, which learns graph structures through our proposed mechanism of L2SP selection and LLM self-supervised pre-training. Figure 2 depicts the framework of Path-LLM with four key components in different phases.</p>
<ul>
<li>Phase-I: L2SP selection. To capture comprehensive graph properties, we first sample a few shortest paths of long length widely across the whole network and then cut them into short paths. We construct these long-to-short shortest paths (L2SP) as a base set of important path-based features for Path-LLM.</li>
<li>Phase-II: Path textualization. Next, we employ the path textual function to obtain L2SP-based texts, incorporating the properties of L2SP and key phrases in text attributes, with each text representing a single L2SP within the graph.</li>
<li>Phase-III: L2SP-based graph feature learning. These L2SPbased texts form a comprehensive dataset and are subsequently fed into the Path-LLM for self-supervised learning. As the Path-LLM acquires the ability to generate L2SP-based texts, it inherently learns to generate L2SP-based shortest paths.</li>
<li>Phase-IV: Path-LLM embedding generation. Finally, we utilize the frozen Path-LLM model to extract Path-LLM embeddings that combine shortest path-based graph features with deep semantic information derived from node text attributes.</li>
</ul>
<p>Algorithm 1 L2SP Selection
Input: A text-attributed graph $G=\left(V, E, X_{0}, X_{e}\right)$, the number of sampling nodes $b$, the minimum long path length $L$, the number of sampling shortest paths $k$, the maximum short path length $\ell$.
Output: $\mathbb{P}<em i="i">{\text {short }}$ shortest paths that have no length greater than $\ell$.
1: Randomly select a $b$-sized node set $\mathcal{S}$;
2: for $s</em>$ do
3: $\mathcal{T}} \in \mathcal{S<em i="i">{i} \leftarrow\left{\tau</em> \in V}$ by BFS;
4: $\quad$ Randomly select a target node $\tau_{i}$ in $\mathcal{T}} \mid\right.$ dist $\left(s_{i}, \tau_{i}\right) \geq L, \tau_{i<em _long="{long" _text="\text">{i}$;
5: $\quad \mathbb{P}</em>$ by BFS;
6: for $\mathcal{P}}} \leftarrow$ Select $k$ shortest paths from $s_{i}$ to $\tau_{i<em 1="1">{\text {long }}\left\langle v</em>}, v_{2}, \ldots, v_{L}\right\rangle \in \mathbb{P<em _short="{short" _text="\text">{\text {long }}$ do
7: $\quad \mathcal{P}</em>\right\rangle\right} ;$
8: $\quad \mathbb{P}}}=\left{\left\langle v_{i+1}, \ldots, v_{i+\ell}\right\rangle \mid i=\alpha(\ell-1), 0 \leq \alpha&lt;\left\lfloor\frac{L}{\ell-1}\right\rfloor-1\right} \cup$ $\left{\left\langle v_{\left(\left\lfloor\frac{L}{\ell-1}\right\rfloor-1\right)(\ell-1)+1}, \ldots, v_{L<em _short="{short" _text="\text">{\text {short }} \leftarrow \mathbb{P}</em>}} \cup \mathcal{P<em _short="{short" _text="\text">{\text {short }} ;$
9: return $\mathbb{P}</em>$;}</p>
<h2>5 L2SP SELECTION AND TEXTUALIZATION</h2>
<p>This section emphasizes the methodology for transforming the graph into a textual format, which provides Path-LLM with a highquality graph training corpus. This conversion maintains the structural features and essential semantic information from the text attributes of nodes and edges.</p>
<h3>5.1 Phase-I: L2SP Selection</h3>
<p>Generally, a graph has complex structures composed of dense groups and interconnections among them. Capturing features of these dense groups and connections is vital. Hence, to effectively obtain the critical structure of dense groups and their cross-over connections, we first sample long paths $\mathcal{P}<em _long="{long" _text="\text">{\text {long }}$ from $G$ and then cut $\mathcal{P}</em>}}$ into small ones for easy learning by Path-LLM in Algorithm 1. Long-path sampling. Randomly selecting a pair of nodes with a long shortest path between them is highly challenging, as many graphs have a small-world property. To effectively obtain $\mathcal{P<em i="i">{\text {long }}$, we propose a fast long-path sampling algorithm. We first randomly select a set $\mathcal{S}=\left{s</em>\right}<em i="i">{i=1}^{b}$ of $b$ nodes as the source nodes (line 1). A full-graph breadth-first search (BFS) [43] is then conducted for each source node $s</em>}$ to identify nodes in $G$ at a distance greater than $L$ edges from $s_{i}$ (lines 2-3). These identified nodes collectively form a candidate set $\mathcal{T<em i="i" j="j">{i}=\left{\tau</em>\right}<em i="i">{j=1}^{b^{\prime}}$ of target nodes, with the size of $\mathcal{T}</em>}$ being $b^{\prime}$. In other words, each node in $\mathcal{T<em i="i">{i}$ is beyond the $L$-hop distance from $s</em>}$. Within this candidate set $\mathcal{T<em i="i">{i}$, a node is randomly chosen as the target node, denoted as $\tau</em>}$. The BFS algorithm is then employed to find a shortest path union cover between $s_{i}$ and $\tau_{i}$. To reduce the overlapping of these long paths, a $k$-size subset of all long shortest paths with the same $s_{i}$ and $\tau_{i}$ is randomly selected to constitute the result $\mathbb{P<em _long="{long" _text="\text">{\text {long }}$ (lines 4-5).
Long-to-short path conversion. To convert sampled long shortestpaths $\mathcal{P}</em>}}$ into short ones, we propose the L2SP conversion method to cut them into short shortest-paths (lines 6-8). Given one long shortest path $\mathcal{P<em 1="1">{\text {long }}=\left\langle v</em>}, v_{2}, \ldots, v_{L}\right\rangle$ and a parameter of maximum length $\ell$, i.e., $\mathcal{P<em i_1="i+1">{\text {short }}=\left{\left\langle v</em>}, \ldots, v_{i+\ell}\right\rangle \mid i=\alpha(\ell-1), 0 \leq\right.$ $\left.\alpha&lt;\left\lfloor\frac{L}{\ell-1}\right\rfloor-1\right} \cup\left{\left\langle v_{\left(\left\lfloor\frac{L}{\ell-1}\right\rfloor-1\right)(\ell-1)+1}, \ldots, v_{L}\right\rangle\right}$. For example, assume that a $\mathcal{P<em 1="1">{\text {long }}=\left\langle v</em>}, v_{2}, v_{3}, v_{4}, v_{5}, v_{6}, v_{7}, v_{8}, v_{9}, v_{10}\right\rangle$ and $\ell=3$, the output of $\Gamma\left(\mathcal{P<em _short="{short" _text="\text">{\text {long }}\right)$ is a set of five shortest paths as $\mathcal{P}</em>\right\rangle\right}$. Note that
}}=$ $\left{\left\langle v_{1}, v_{2}, v_{3}\right\rangle,\left\langle v_{3}, v_{4}, v_{5}\right\rangle,\left\langle v_{5}, v_{6}, v_{7}\right\rangle,\left\langle v_{7}, v_{8}, v_{9}\right\rangle,\left\langle v_{9}, v_{10<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Comparison between random walks, randomly sampled short shortest paths, and L2SP-based paths.
these five short paths can be connected via the cutting nodes, e.g., $v_{3}, v_{5}, v_{7}$, and $v_{9}$.</p>
<h3>5.2 Comparing Path Selections in Phase-I</h3>
<p>We compare different choices of path selection and analyze their pros and cons, including 1) random walk v.s. shortest paths, 2) long paths v.s. short paths, and 3) random short paths v.s. L2SP-based paths. Random walk vs. Shortest paths. Previous work [69] applies random walk to sample paths, while it is less effective than our L2SP method in LLM due to three major reasons, noisy and fragile, mismatching with LLM, and hardly covering bridge edges.</p>
<p>First, random walk can easily involve noisy nodes and cycles, rendering them less robust. In contrast, the shortest path focuses on direct connections, avoiding unnecessary detours through noisy nodes and cycles. Thus, our methods demonstrate greater resilience to noisy nodes and other disruptions.</p>
<p>Second, random patterns may not match the order of language sequences in LLM. LLM may have limited ability to learn graph features from irregular random walks. Most LLMs use causal language modeling (CLM) training techniques. LLMs generate tokens one by one according to the linguistic rules. The next token prediction is only based on the prefix tokens, not the following ones. Considering the prefix nodes in random walks, the next node is essentially random and unpredictable. Conversely, the next node selection in the shortest path is also based on the prefix nodes, making it more in line with the generation pattern of LLMs.</p>
<p>Last, random walk may hardly cover bridge edges, i.e., the edges across distinct dense groups. Consider a bridge formed by $w_{1}-w_{2}-$ $w_{3}$ to connect two dense groups $G_{1}$ and $G_{2}$ shown in Figure 3. For a random source node $s$ in a dense group, the generated random walk paths tend to stay within its dense groups, which misses the bridge $w_{1}-w_{2}-w_{3}$ as shown in Figure 3(a). On the other hand, L2SP-based shortest paths are converted from a long shortest path, where the source and target nodes that span a long distance may have a high probability of appearing in different dense groups, as shown in Figure 3(c). This easily enables to cover the bridge between dense groups by our L2SP-based path selection.
Long paths vs. Short paths. We compare two kinds of shortest paths in terms of different lengths, called long paths and short paths, respectively. Given a long path $\mathcal{P}=\left\langle v_{1}, v_{2}, \ldots, v_{\ell}\right\rangle$, the distance between the source node $v_{1}$ and the target node $v_{\ell}$ is $\ell$. LLM tends to learn from all the prefix nodes $\left\langle v_{1}, \ldots, v_{\ell-1}\right\rangle$ and generate the next node $v_{\ell}$. A pair of nodes $\left(v_{1}, v_{\ell}\right)$ more than $\ell$ hops apart in a graph has a very weak relationship. However, the node embedding of $v_{\ell}$ still may be affected by the weakly associated node $v_{1}$ because LLM</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Path textualization of homogeneous graphs on citation networks.
generates the next token based on all forward nodes. Therefore, compared to long paths, we chose short shortest paths instead to avoid such irrelevant propagation.
Random short paths vs. L2SP-based paths. Similar to random walk, most randomly sampled shortest paths tend to fall within dense groups in Figure 3(b). It is also challenging to cover bridge edges with random shortest paths. Differently, our L2SP method could cover not only the paths in dense groups but also the interconnections between diverse dense groups. Overall, L2SP-based shortest paths have more representative abilities as graph features.</p>
<h3>5.3 Phase-II: Path Textualization of L2SP</h3>
<p>To facilitate the Path-LLM's learning of our L2SP structural and semantic features, we utilize a path textual function to represent structural paths into textual sequences, denoted as $\Phi(\cdot)$. Path textual function $\Phi(\cdot)$ can automatically process and concatenate node text attributes within the path $\mathcal{P}<em _short="{short" _text="\text">{\text {short }}$ in designed ways to form the text sequence $T$, which can be formulated as $T=\Phi\left(\mathcal{P}</em>\right)$. $\Phi(\cdot)$ has different processing ways for two cases: homogeneous and heterogeneous text-attributed graphs, as shown in Figure 4 and 5.}</p>
<p>For homogeneous text-attributed graphs, we first conduct data cleaning for node text attributes, removing noisy characters and links. Excessively long text attributes and overly complex sentence structures (e.g., abstracts) can weaken the path structure properties in L2SP-based texts. This complexity makes it more difficult for the following Path-LLM to learn the path structure effectively. To address it, we utilize the Positionrank algorithm [20] to extract key phrases from the text attributes as new node attributes $\mathcal{X}<em _short="{short" _text="\text">{0}^{t}$. This simplification retains the primary semantic information while making path structures more accessible for Path-LLM. We design the 〈paper with content...〉 template for citation networks to assist Path-LLM in distinguishing the text attributes of different nodes. $\Phi(\cdot)$ integrates templates and different node and edge text attributes into textual paths, as shown in Figure 4. Formally, $\Phi\left(\mathcal{P}</em>}}\right)=\operatorname{template}\left(\left\langle\mathcal{X<em e="e">{0}^{t} \mid \mathcal{X}</em>}\right\rangle\right)$, where $v, e \in \mathcal{P<em _short="{short" _text="\text">{\text {short }}$. For heterogeneous text-attributed graphs, data cleaning and key phrase selection are also applicable. Given a path $\mathcal{P}</em>}}=\left\langle v_{1}, v_{2}, \ldots, v_{t}\right\rangle$, $\Phi(\cdot)$ concatenates the text attributes of each node on the path as $\Phi\left(\mathcal{P<em v__1="v_{1">{\text {short }}\right)=\left\langle\mathcal{X}</em>}}^{t} \mid \mathcal{X<em 1_2="1,2">{e</em>}} \mid \mathcal{X<em 2="2">{v</em>}}^{t} \mid \ldots\right\rangle$ to derive L2SP-based texts, as shown in Figure 5. Thus, we obtain the L2SP-based text by $T=$ $\Phi\left(\mathcal{P<em 1="1">{\text {short }}\right)$, where $T=\left{t</em>\right}$ and the length is $|T|=5$.
}, \ldots, t_{5<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Path textualization of heterogeneous graphs on biomedical networks with four types of nodes: genes, diseases, chemicals and species [80].</p>
<p>Discussion. Different from WalkLM [69], our proposed path textual function $\Phi(\cdot)$ can handle homogeneous graphs with abundant texts, as shown in Figure 4. For nodes with excessively long text attributes, we conduct data cleaning and key phrase selection, selecting essential tokens as new text attributes of nodes. Moreover, for heterogeneous graphs, WalkLM employs a complicated template unsuitable for LLM path structure learning.</p>
<h3>5.4 Complexity Analysis</h3>
<p>We analyze the time complexities of L2SP selection in Phase-I and path textualization in Phase-II, respectively. For a graph $G(V, E)$, the size of nodes and edges are $n=|V|$ and $m=|E|$, respectively. W.L.O.G., we assume that $n-1 \leq m$ for a connected graph $G$, i.e., $O(n) \subseteq O(m)$.</p>
<p>Theorem 1. Algorithm 1 of L2SP selection in Phase-I takes $O(b m)$ time and $O\left(m+b L_{\text {max }}\right)$ space, where $b$ is the number of sampled nodes and $L_{\text {max }}$ is the maximum length of paths.</p>
<p>Proof. First, in the path selection process, we randomly select $b$ nodes. For each sampled node, we conduct BFS on $G$, taking $O(b(n+m)) \subseteq O(b m)$ time. It then randomly selects one node in a candidate set $\mathcal{T}$, and then conducts BFS again between a source node and a target node, which is $O(n+m) \subseteq O(m)$. Following this, we convert long shortest paths into L2SP-based shortest paths using $O\left(b \Sigma\left|\mathcal{P}<em _short="{short" _text="\text">{\text {short }}\right|\right) \subseteq O(b m)$ time, where $\mathcal{P}</em>$. Therefore, the overall time complexity of Algorithm 1 is $O(b m)$.}} \in \mathbb{P}_{\text {short }</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Path-LLM self-supervised pre-training and embedding generation.</p>
<p>Next, we analyze the space complexity. Algorithm 1 takes $O(m+$ $n) \subseteq O(m)$ to store $G$ and apply BFS. It then takes $O\left(b L_{\max }\right)$ space to save the long shortest paths, where $L_{\max }$ is the maximum length of the long shortest paths. Thus, the space complexity of Algorithm 1 is $O\left(m+b L_{\max }\right)$.</p>
<p>Theorem 2. Path textualization in Phase-II takes $O\left(\sum_{v \in \mathbb{P}} \operatorname{Pos}(\right.$ At$\operatorname{tr}(v))$ ) time, where Pos $(\cdot)$ denotes the time complexity of PositionRank algorithm [20] and Attr $(v)$ denotes the number of characters in the text-attribute of $v$.</p>
<p>Proof. In the path textualization phase, we first clean the text attributes of each node $v \in \mathbb{P}<em _in="\in" _mathbb_P="\mathbb{P" v="v">{\text {short }}$, leading to a time complexity of $O\left(\sum</em>(v))\right)$.}} \operatorname{Attr}(v)\right)$, where Attr $(v)$ denotes the number of characters in the text attribute of $v$. For key phrase selection, we employ the PositionRank algorithm [20] to extract key phrases from each cleaned text attribute, incurring a complexity of $O\left(\sum_{v \in \mathbb{P}} \operatorname{Pos}(\operatorname{Token}(v))\right)$, where Token $(v)$ denotes the token number of cleaned text attributes and $\operatorname{Pos}(\operatorname{Token}(v))$ ) denotes the time complexity of PositionRank for one node $v$. In the worst case, every token only contains one character, i.e., $O(\operatorname{Token}(v)) \subseteq O(\operatorname{Attr}(v))$. Thus, we can obtain that $O\left(\sum_{v \in \mathbb{P}} \operatorname{Pos}(\operatorname{Token}(v))\right) \subseteq O\left(\sum_{v \in \mathbb{P}} \operatorname{Pos}(\operatorname{Attr}(v))\right)$. Furthermore, based on the PositionRank algorithm, $O\left(\sum_{v \in \mathbb{P}} \operatorname{Attr}(v)\right) \subseteq$ $O\left(\sum_{v \in \mathbb{P}} \operatorname{Pos}(\operatorname{Attr}(v))\right)$. Therefore, the overall time complexity of path textualization is $O\left(\sum_{v \in \mathbb{P}} \operatorname{Pos}(\operatorname{Attr</p>
<h2>6 PATH-LLM EMBEDDING LEARNING FROM L2SP BASED GRAPH FEATURES</h2>
<p>This section introduces the Path-LLM self-supervised learning process, designed to learn both inter-group connections and intragroup structures from L2SP-based texts. The trained Path-LLM derives graph embeddings based on node text attributes and learned graph structure features, which are then applied to subsequent downstream tasks (node classification, edge validation, keyword search). The detailed whole process is shown in Figure 6.</p>
<h3>6.1 Phase-III: L2SP-based graph feature learning</h3>
<p>Based on the powerful semantic representation capabilities of LLM, Path-LLM integrates essential graph features inherent in L2SP into its own deep semantic space. To achieve this, Path-LLM learns
to generate L2SP-based texts constructed above through the selfsupervised pre-training process. Specifically, we transform the order of nodes in the L2SP to the order of text tokens. As Path-LLM learns the order of tokens, it also learns the order of nodes and edges within the L2SP. Typically, the pre-training task for LLM is to generate the next token based on prefix tokens [21], similar to Eq. 1. In this paper, we align the next token generation with the next node or edge generation. When Path-LLM generates L2SP-based texts, the pre-training task for Path-LLM is to generate the next node $v_{i}$ or edge $e_{i, j}$ on the corresponding L2SP based on prefix nodes and edges $\left{e_{&lt;i}, v_{&lt;i}\right}$, as shown in Figure 6. This process is then iteratively repeated until the whole L2SP is generated.</p>
<p>Formally, the L2SP-based text can be denoted as $T=\Phi\left(\mathcal{P}<em v__i="v_{i">{\text {short }}\right)=$ $\left\langle\ldots . .\left|\mathcal{X}</em>}}^{\prime}\right| \mathcal{X<em i_="i," j="j">{e</em>}}\left|\mathcal{X<em j="j">{v</em>}}^{\prime}\right| \ldots\right\rangle$, where $1 \leq i, j \leq \ell . \mathcal{X<em i="i">{v</em>}}^{\prime}$ is the processed new text attribute of the node $v_{i}, \mathcal{X<em i_="i," j="j">{e</em>}}$ is the text attribute of edge $e_{i, j}$ and $\ell$ denotes the length of $\mathcal{P<em i="i">{\text {short }}$. Based on Eq. 1, the probability of Path-LLM predicting the next node $v</em>}$ can be formulated as $p\left(\mathcal{X<em i="i">{v</em>}}^{\prime}\left|\left\langle\mathcal{X<em _i="&lt;i">{v</em>}}^{\prime}, \mathcal{X<em _i="&lt;i">{e</em>}}\right\rangle\right)\right.$ and the probability of predicting the next edge $e_{i, j}$ is $p\left(\mathcal{X<em i_="i," j="j">{e</em>}}\left|\left\langle\mathcal{X<em _i="&lt;i">{v</em>}}^{\prime}, \mathcal{X<em _i="&lt;i">{e</em>}}\right\rangle\right) .\left\langle\mathcal{X<em _i="&lt;i">{v</em>}}^{\prime}, \mathcal{X<em _i="&lt;i">{e</em>$ is}}\right\rangle\right.$ symbolizes the token sequence composed of the nodes and edges preceding $v_{i}$. Thus, the probability of generating the whole shortest path $\mathcal{P}_{\text {short }</p>
<p>$$
\prod_{i=1}^{\ell} p\left(\mathcal{X}<em i="i">{v</em>}}^{\prime}\left|\left\langle\mathcal{X<em _i="&lt;i">{v</em>}}^{\prime}, \mathcal{X<em _wzxhzdk:10_i="<i}}\right\rangle\right) p\left(\mathcal{X}_{e_{i, i+1}} \mid\left\langle\mathcal{X}_{v_{>i">{e</em>}}^{\prime}, \mathcal{X<em _i="&lt;i">{e</em>\right\rangle\right)\right.
$$}</p>
<p>where $p\left(\mathcal{X}<em i_="i," i_1="i+1">{e</em>}} \mid\left\langle\mathcal{X<em _i="&lt;i">{v</em>}}^{\prime}, \mathcal{X<em _i="&lt;i">{e</em>}}\right\rangle\right)=1$ if $i=\ell$. The probability of generating $\mathcal{P<em i="1">{\text {short }}$ is consistent with the probability of generating tokens, which is $p(t)=\prod</em>\right)$, as proved in Theorem 3. Thus, PathLLM learns the path structure through self-supervised learning, thereby understanding the graph structure. Consequently, when Path-LLM generates unified graph embeddings, it can integrate the learned graph structure with its inherent deep semantic representation capabilities.}^{S} p\left(t_{i} \mid t_{&lt;i</p>
<p>We use the cross-entropy loss function [88] during the selfsupervised training process of Path-LLM. The L2SP-based text can also be denoted as $T=\left{t_{1}, \ldots t_{S}\right}$, where $S$ denotes the length of $T$. Path-LLM starts generating from the first token $t_{1}$ and calculates the loss for generating the next token $t_{j}$ based on prefix tokens $t_{&lt;j}$.</p>
<p>Therefore, the loss for generating one L2SP-based text is:</p>
<p>$$
\mathcal{L}<em j="1">{\Theta}=-\frac{1}{S} \sum</em>^{}^{S} \sum_{k=1}^{|\mathcal{D}|} y_{k} \log \frac{\exp \left(P_{t_{k<em>}}\left(t_{j} \mid t_{&lt;j}\right)\right)}{\sum_{t_{k}^{</em>} \in \mathcal{D}} \exp \left(P_{t_{k}^{*}} \mid t_{j} \mid t_{&lt;j}\right))}
$$</p>
<p>where $\Theta$ is the learnable parameters of Path-LLM, $j$ is the position in $T$ and $k$ is the position in the LLM vocabulary $\mathcal{D} . P_{t_{k}^{<em>}}$ represents the probability that the $k$-th token $t_{k}^{</em>}$ in $\mathcal{D}$ is the next token. $|\mathcal{D}|$ denotes the size of the LLM vocabulary. $y_{k}$ is the label of whether $t_{k}^{*}$ in $\mathcal{D}$ is the token at position $j$. For the whole dataset composed of $N$ L2SP-based texts, the final loss function is:</p>
<p>$$
\mathcal{L}<em N="N">{\text {pretrain }}=\frac{1}{N} \sum</em>
$$} \mathcal{L}_{\Theta</p>
<p>Rationales of Pre-training in Phase-III. During the Path-LLM pre-training process, L2SP-based text generation can be regarded as L2SP generation. Formally, tokens of node and edge attributes can be denoted as $\left{t_{j}^{v_{i}}\right}$ and $\left{t_{j}^{e_{i, t+1}}\right}$ separately, where $1 \leq j \leq S$ and $1 \leq i \leq t$. $j$ represents the token index in $T$, while $i$ represents the node index in the corresponding path. $S$ is the length of the text, and $\ell$ is the length of the corresponding L2SP. Tokens associated to node $v_{i}$ can be denoted as $\left{t_{j^{\prime} \leq j \leq j}^{v_{i}}\right}$, where $t_{j^{\prime}}^{v_{i}}$ is the first token in $\mathcal{X}<em i="i">{v</em>}}^{\ell}$ and $t_{j}^{v_{i}}$ is the last token in $\mathcal{X<em i="i">{v</em>\right)$. While, the probability of the corresponding L2SP generation can be formulated as Eq. 2.}}^{\ell}$. The probability of Path-LLM generating tokens of node $v_{i}$ [79][64] is $\prod_{j=j^{\prime}}^{j} p\left(t_{j}^{v_{i}} \mid t_{&lt;j}^{v_{&lt;i}}, t_{&lt;j}^{e_{&lt;i}}\right)$, where $t_{&lt;j}^{v_{&lt;i}}$ represents a token list satisfying the token index being less than $j$ and the node index being less than or equal to $i$, while $t_{&lt;j}^{e_{&lt;i}}$ represents the tokens associated with the edges preceding $v_{i}$. Similarly, tokens associated to edge $e_{i, t+1}$ can be denoted as $\left{t_{j^{\prime} \leq j \leq j^{\prime 1}}^{e_{i, t+1}}\right}$ and the probability of generating tokens of edge $e_{i, t+1}$ is $\prod_{j=j^{\prime}} p\left(t_{j}^{e_{i, t+1}} \mid t_{&lt;j}^{v_{&lt;i}}, t_{&lt;j}^{e_{&lt;i}}\right)$. Then, we can formulate the probability of generating the L2SP-based text $T$ as $\prod_{i=1}^{t}\left[\prod_{j=j^{\prime}}^{j} p\left(t_{j}^{v_{i}} \mid t_{&lt;j}^{v_{&lt;i}}, t_{&lt;j}^{e_{&lt;i}}\right) \prod_{j=j^{\prime}}^{j^{\prime \prime}} p\left(t_{j}^{e_{i, t+1}} \mid t_{&lt;j}^{v_{&lt;i}}, t_{&lt;j}^{e_{&lt;i}}\right)\right]$, which equals $p(t)=\prod_{i=1}^{N} p\left(t_{i} \mid t_{&lt;i</p>
<p>Lemma 1. During the Path-LLM self-supervised pre-training process, the generation of tokens associated with nodes in L2SP-based texts can be regarded as the generation of nodes in L2SPs. Formally,</p>
<p>$$
\prod_{j=j^{\prime}}^{j} p\left(t_{j}^{v_{i}} \mid t_{&lt;j}^{v_{&lt;i}}, t_{&lt;j}^{e_{&lt;i}}\right)=p\left(\mathcal{X}<em i="i">{v</em>}}^{\ell} \mid\left\langle\mathcal{X<em _i="&lt;i">{v</em>}}^{\ell}, \mathcal{X<em _i="&lt;i">{e</em>\right\rangle\right)
$$}</p>
<p>Proof. Based on the chain rule of conditional probability [8], we can derive that</p>
<p>$$
\prod_{j=j^{\prime}}^{j} p\left(t_{j}^{v_{i}} \mid t_{&lt;j}^{v_{&lt;i}}, t_{&lt;j}^{e_{&lt;i}}\right)=p\left(\left{t_{j^{\prime} \leq j \leq j}^{v_{i}}\right} \mid t_{&lt;j^{\prime \prime}}^{v_{&lt;i}}, t_{&lt;j^{\prime \prime}}^{e_{&lt;i}}\right)
$$</p>
<p>Due to $\mathcal{X}<em i="i">{v</em>}}^{\ell}=\left{t_{j^{\prime} \leq j \leq j}^{v_{i}}\right}$ and $\left\langle\mathcal{X<em _i="&lt;i">{v</em>}}^{\ell}, \mathcal{X<em _i="&lt;i">{e</em>\right}$, we can know that:}}\right\rangle=\left{t_{&lt;j^{\prime \prime}}^{v_{&lt;i}}, t_{&lt;j^{\prime \prime}}^{e_{&lt;i}</p>
<p>$$
\prod_{j=j^{\prime}}^{j} p\left(t_{j}^{v_{i}} \mid t_{&lt;j}^{v_{&lt;i}}, t_{&lt;j}^{e_{&lt;i}}\right)=p\left(\mathcal{X}<em i="i">{v</em>}}^{\ell} \mid\left\langle\mathcal{X<em _i="&lt;i">{v</em>}}^{\ell}, \mathcal{X<em _i="&lt;i">{e</em>\right\rangle\right)
$$}</p>
<p>Table 1: Complexity analysis of WalkLM and Path-LLM in two stages: fine-tuning and embedding deriving. $B_{L M}(\cdot)$ denotes the time complexity taken by a LM in WalkLM. $|N|$ denotes the number of random walks. $l_{\text {avg }}$ denotes their average length. Path-LLM significantly faster than WalkLM in two reasons: $\left|\mathbb{P}<em _avg="{avg" _text="\text">{\text {short }}\right|&lt;&lt;|N|$ and $\hat{S}&lt;&lt;l</em>$.}</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: left;">Fine-tuning</th>
<th style="text-align: left;">Embedding-deriving</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">WalkLM</td>
<td style="text-align: left;">$O\left(B_{L M}\left(I,</td>
<td style="text-align: left;">N</td>
</tr>
<tr>
<td style="text-align: left;">Path-LLM</td>
<td style="text-align: left;">$O\left(B_{L L M}\left(I,\left</td>
<td style="text-align: left;">\mathbb{P}_{\text {short }} \mid, \hat{S}, d\right)\right)\right.$</td>
</tr>
</tbody>
</table>
<p>Lemma 2. During the Path-LLM self-supervised pre-training process, the generation of tokens associated with edges in L2SP-based texts can be regarded as the generation of edges in L2SPs. Formally,</p>
<p>$$
\prod_{j=j^{\prime}}^{j^{\prime \prime}} p\left(t_{j}^{e_{i, t+1}} \mid t_{&lt;j}^{v_{&lt;i}}, t_{&lt;j}^{e_{&lt;i}}\right)=p\left(\mathcal{X}<em i_="i," t_1="t+1">{e</em>}}\left\langle\left\langle\mathcal{X<em _i="&lt;i">{v</em>}}^{\ell}, \mathcal{X<em _i="&lt;i">{e</em>\right\rangle\right)\right.
$$}</p>
<p>Proof. Similar to the proof of Lemma 1.
Theorem 3. During the Path-LLM self-supervised pre-training process, L2SP-based text generation can be regarded as the L2SP generation. Formally,</p>
<p>$$
\begin{aligned}
&amp; \prod_{i=1}^{\ell}\left[\prod_{j=j^{\prime}}^{j} p\left(t_{j}^{v_{i}} \mid t_{&lt;j}^{v_{&lt;i}}, t_{&lt;j}^{e_{&lt;i}}\right) \prod_{j=j^{\prime}}^{j^{\prime \prime}} p\left(t_{j}^{e_{i, t+1}} \mid t_{&lt;j}^{v_{&lt;i}}, t_{&lt;j}^{e_{&lt;i}}\right)\right] \
= &amp; \prod_{i=1}^{\ell} p\left(\mathcal{X}<em i="i">{v</em>}}^{\ell} \mid\left\langle\mathcal{X<em _i="&lt;i">{v</em>}}^{\ell}, \mathcal{X<em _i="&lt;i">{e</em>}}\right\rangle\right) p\left(\mathcal{X<em i_="i," t_1="t+1">{e</em>}} \mid\left\langle\mathcal{X<em _i="&lt;i">{v</em>}}^{\ell}, \mathcal{X<em _i="&lt;i">{e</em>\right\rangle\right)
\end{aligned}
$$}</p>
<p>Proof. By Lemmas 1 and 2, we can prove Theorem 3.</p>
<h3>6.2 Phase-IV: Path-LLM Embedding Generation</h3>
<p>The node text attribute $\mathcal{X}<em i="i">{v</em>}}$ can be a few words, a sentence, or a paragraph, all of which can be transformed into Path-LLM embeddings. Specifically, after cleaning $\mathcal{X<em i="i">{v</em>}}$ and selecting key phrases as new text attributes $\mathcal{X<em i="i">{v</em>}}^{\ell}$, we input $\mathcal{X<em i="i">{v</em>\right}}}^{\ell}$ into the frozen Path-LLM and then extract all token embeddings $\left{c_{i<em i="i">{i=1}^{\ell}$ associated with the same node $v</em>$ integrates both the graph structure learned by Path-LLM and its inherent semantic understanding capabilities.
Complexity Analysis. We analyze the time complexity of PhaseIII and Phase-IV. Due to the complex structure of the neural networkbased learning model, we assume that a commonly used LLM is a black-box model taking a function of time complexity, denoted by $B_{L L M}(\cdot)$. Thus, our Path-LLM takes $O\left(B_{L L M}\left(I,\left|\mathbb{P}}$, where $r$ is the number of token embeddings from the same node. We extract embeddings from the last layer of the Path-LLM, where $c_{i} \in \mathbb{R}^{d}$ and $d$ is the embedding dimension. To obtain the Path-LLM embedding for node $v_{i}$, the token embeddings derived from the text attribute of $v_{i}$ are averaged, i.e., for a node $v_{i}$, the Path-LLM embedding $\xi_{v_{i}} \in \mathbb{R}^{d}$ is calculated as $\xi_{v_{i}}=\frac{1}{r} \sum_{i=1}^{r} c_{i}$. The embedding $\xi_{v_{i}<em _short="{short" _text="\text">{\text {short }} \mid, \hat{S}, d\right)\right)\right.$, where $I$ is the number of iterations for Path-LLM training, $\left|\mathbb{P}</em>$ is the average length of L2SP-based texts and $d$}}\right|$ is the number of training samples, which equals the total number of L2SPs, $\hat{S</p>
<p>is the dimension of embedding. When parameters $I, \mathbb{P}<em L="L" M="M">{\text {short }}, \hat{S}, d$ increase, the complexity $O\left(B</em>}\left(I, \mathbb{P<em L="L" M="M">{\text {short }}, \hat{S}, d\right)\right)$ also increases. Notably, due to the attention mechanism of texts generally used in LLMs, $O\left(B</em>}\left(I, \mathbb{P<em L="L" M="M">{\text {short }}, \hat{S}, d\right)\right)$ is positively correlated with $\hat{S}^{2}$. For Path-LLM embedding generation, the time complexity of PathLLM deriving embeddings is related to the node size $|V|=n$, the length of node text-attributes $r$, where $r&lt;\hat{S}$ and the dimension of embedding $d$. After deriving embeddings, the time complexity of the average operation is $O(n r d)$. Therefore, the total time complexity of Phase-III and Phase-IV is $O\left(B</em>, d\right)+n r d\right)$, where the complexity is positively correlated with all input parameters.}\left(I, \mathbb{P}_{\text {short }} \mid, n, \hat{S</p>
<h3>6.3 Path-LLM Algorithm</h3>
<p>We propose Path-LLM to derive unified graph embeddings from a given text-attributed graph, which is detailed in Algorithm 2. We first sample L2SP-based shortest paths in Phase-I (line 1). Then, in Phase II (lines 2-10), we perform path textualization to convert the L2SP-based shortest paths into graph-attributed texts. The resulting graph-attributed texts, referred to as L2SP-based texts, serve as the training corpus for Path-LLM. In Phase III, we train Path-LLM utilizing a self-supervised learning approach (lines 11-19). $\mathcal{B}$ in line 13 denotes one training batch. The number of iterations can be calculated as $I_{\max }=\frac{1 \cdot\left|\mathcal{P}<em v="v">{\text {short }}\right|}{|\mathcal{B}|} \cdot|\mathcal{B}|$ in line 17 represents the batch size. We demonstrate in Theorem 3 that generating the subsequent token within L2SP-based texts (line 15) can also be interpreted as generating the next node or edge in L2SP-based shortest paths. Finally, in Phase IV (lines 20-23), Path-LLM extracts unified graph embeddings, with $f\left(\mathcal{X}</em>$ into the Path-LLM function $f(\cdot)$.}^{\prime}\right)$ denoting the embeddings produced by inputting $\mathcal{X}_{v}^{\prime</p>
<p>Theorem 4. Path-LLM in Algorithm 2 takes $O\left(b m+\sum_{v \in \mathbb{P}} \operatorname{Pos}\left(A t-\right.\right.$ $\operatorname{tr}(v))+B_{L L M}\left(I, \mathbb{P}_{\text {short }} \mid, n, \hat{S}, d\right)+n r d)$ time.</p>
<p>Proof. We conclude the complexity by integrating the complexity results by Theorems 1, 2, and the analysis of Phase-III and Phase-IV in Section 6.</p>
<h2>7 PATH-LLM BASED KEYWORD SEARCH</h2>
<p>To illustrate the usefulness of Path-LLM, we investigate the application of our learned Path-LLM embeddings to help tackle one typical graph analytics task of keyword search.
Keyword search. Keyword search is a classical graph query processing task widely applied in databases and recommendation systems [45, 73], particularly retrieval-augmented generation [30]. Given a graph $G$ associated with node keywords and a set of query keywords $Q$, the goal of keyword search is to find the best subgraph $H$ of $G$ such that $H$ covers all keywords $Q$ and has the smallest edge weights. Following [81], we formulate the problem as follows. Given a weighted graph $G=(V, E, W)$ a set of query keywords $Q=\left{q_{1}, \ldots, q_{m}\right}$, the problem returns a tree structure $T \subseteq G$, such that
(1) $V_{T}$ covers all keywords in $Q$;
(2) the edge weight $f(T)$ is minimized among all feasible choices, where $f(T)=\sum_{e_{i, j} \in E_{T}} w_{i, j}$.
However, in many real-world applications of keyword search, it strictly follows the original structure of given graphs but neglects</p>
<h2>Algorithm 2 Path-LLM Framework</h2>
<p>Input: A text-attributed graph $G=\left(V, E, \mathcal{X}<em e="e">{v}, \mathcal{X}</em>\right)$.
Output: Path-LLM graph embeddings $\xi_{v}$.
1: Phase-I: Generate a series of shortest paths $\mathbb{P}<em _short="{short" _text="\text">{\text {short }}$ on $G$ by the L2SP selection in Algorithm 1;
2: Phase-II: Transform $\mathbb{P}</em>$ into graph-attributed texts
3: for $\mathcal{P}}<em _short="{short" _text="\text">{\text {short }} \in \mathbb{P}</em>$ do
4: for $v \in \mathcal{P}}<em v="v">{\text {short }}$ do
5: $\quad$ Remove noisy characters and links from $\mathcal{X}</em>$;
6: Select key phrases from $\mathcal{X}<em v="v">{v}$ as new $\mathcal{X}</em>$;
7: if $G$ is a homogeneous graph then
$7=\Phi\left(\mathcal{P}}^{\prime<em v="v">{\text {short }}\right)=$ template $\left(\left\langle\mathcal{X}</em>}^{\prime} \mid \mathcal{X<em _short="{short" _text="\text">{e}\right\rangle\right)$ by Fig. 4;
8: if $G$ is a heterogeneous graph then
$7=\Phi\left(\mathcal{P}</em>}}\right)=\left\langle\ldots \mid \mathcal{X<em e="e">{v}^{\prime} \mid \mathcal{X}</em>\right| \ldots\rangle$ by Fig. 5;
11: Phase-III: Path-LLM self-supervised pre-training
12: for $1 \leq$ iteration $\leq I_{\max }$ do
13: for $T \in \mathcal{B}$ do
14: $\quad$ for $t \in T$ do
15: $\quad$ Generate next token $t_{j} \in T$;
16: Compute loss $\mathcal{L}<em _batch="{batch" _text="\text">{\Theta}$ as Eq. 3;
17: Compute $\mathcal{L}</em>}}=\frac{1}{|\mathcal{B}|} \sum_{|\mathcal{B}|} \mathcal{L<em _batch="{batch" _text="\text">{\Theta}$;
18: Differentiate the loss function $\mathcal{L}</em>$ and use the chain rule to backpropagate the gradient to calculate gradient of the parameters $g$;
19: Update Path-LLM parameters $w \leftarrow w-\eta g$, where $\eta$ is the adaptive learning rate that adjusts the parameter updates;
20: Phase-IV: Path-LLM embedding generation
21: for $v_{i} \in V$ do
22: $\quad\left{c_{i}\right}}<em v="v">{i=1}^{T} \leftarrow f\left(\mathcal{X}</em>\right)$ from frozen Path-LLM;
23: $\quad \xi_{v_{i}}=\frac{1}{T} \sum_{i=1}^{t} c_{i} ;$
24: $\quad \xi_{v} \leftarrow \xi_{v} \cup\left{\xi_{v_{i}}\right}$
25: return $\xi_{v}$;
the semantic connections of node weights, which limits the discovery of real close subgraph patterns w.r.t. the given keywords. Even worse, in some cases, the edge weights are not given in advance for the querying graph. In this paper, we leverage the Path-LLM embedding to reconstruct a new weighted graph $G^{}^{\prime<em>}\left(V, E^{</em>}, W^{<em>}\right)$ and search for an integrated keyword answer with close connections in terms of both topology structure and node semantics.
Graph structure construction. Given a TAG $G(V, E)$, we construct a new structure of graph $G^{</em>}$ based on the Path-LLM node embedding. Specifically, we first collect all isolated nodes $V$ and then add edges to them as follows. For an edge $\left(v_{i}, v_{j}\right) \in E$, we measure its importance by calculating cosine-similarity [68] based on their embedding vectors $\xi_{i}, \xi_{j}$ derived from Path-LLM, i.e.,</p>
<p>$$
\psi_{i, j}=\frac{\xi_{i} \cdot \xi_{j}}{\left|\xi_{i}\right| | \xi_{j} |}, \text { where } \psi_{i, j} \in[-1,1]
$$</p>
<p>For $\psi_{i, j}&lt;0$, we consider $\xi_{i}$ and $\xi_{j}$ to be dissimilar, which is treated as the minimum non-negative edge weight of 0 . Thus, we design a mapping function $f^{*}$ as:</p>
<p>$$
f^{*}\left(e_{i, j}\right)=\left{\begin{array}{cc}
0 &amp; \text { for } \quad \psi_{i, j} \leq 0 \
\psi_{i, j} &amp; \text { for } \quad \psi_{i, j}&gt;0
\end{array}\right.
$$</p>
<p>Only for a positive weight $\psi_{i, j}$, we add an edge connection between $v_{i}$ and $v_{j}$. Thus, the new edge set is $E^{<em>}=\left{e_{i, j}: v_{i}, v_{j} \in V, f^{</em>}\left(e_{i, j}\right)&gt;\right.$</p>
<p>Table 2: Statistics of tested datasets.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Statistics of graphs</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">The number of training paths</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Dataset</td>
<td style="text-align: center;">#Nodes</td>
<td style="text-align: center;">#Edges</td>
<td style="text-align: center;">Graph type</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">#RW-paths [69]</td>
<td style="text-align: center;">#L2SP-paths</td>
<td style="text-align: center;">The ratio of saving paths</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">PubMed [80]</td>
<td style="text-align: center;">63,109</td>
<td style="text-align: center;">244,986</td>
<td style="text-align: center;">heterogeneous</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">295,512</td>
<td style="text-align: center;">19,670</td>
<td style="text-align: center;">93.35\%</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Freebase [3]</td>
<td style="text-align: center;">180,098</td>
<td style="text-align: center;">1,057,688</td>
<td style="text-align: center;">heterogeneous</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">300,640</td>
<td style="text-align: center;">28,041</td>
<td style="text-align: center;">90.67\%</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Cora [58]</td>
<td style="text-align: center;">2,708</td>
<td style="text-align: center;">5,429</td>
<td style="text-align: center;">homogeneous</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">100,000</td>
<td style="text-align: center;">12,932</td>
<td style="text-align: center;">87.07\%</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Citeseer [24]</td>
<td style="text-align: center;">3,312</td>
<td style="text-align: center;">8,554</td>
<td style="text-align: center;">homogeneous</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">100,000</td>
<td style="text-align: center;">11,504</td>
<td style="text-align: center;">88.50\%</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Facebook [44]</td>
<td style="text-align: center;">4,039</td>
<td style="text-align: center;">88,234</td>
<td style="text-align: center;">homogeneous</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">124,881</td>
<td style="text-align: center;">9,010</td>
<td style="text-align: center;">92.78\%</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">OGB-ARXIV [32]</td>
<td style="text-align: center;">169,343</td>
<td style="text-align: center;">1,166,243</td>
<td style="text-align: center;">homogeneous</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">350,000</td>
<td style="text-align: center;">20,523</td>
<td style="text-align: center;">94.14\%</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 3: Mean results of node classification on six datasets in self-supervised settings: PubMed, Freebase, Cora, ARXIV, Citeseer and Facebook. The best performances are in bold, and the second-best are shaded in gray.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Datasets</th>
<th style="text-align: center;">PubMed</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Freebase</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Cora</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">ARXIV</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Citeseer</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Facebook</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Metrics</td>
<td style="text-align: center;">Macro-F1</td>
<td style="text-align: center;">Micro-F1</td>
<td style="text-align: center;">Macro-F1</td>
<td style="text-align: center;">Micro-F1</td>
<td style="text-align: center;">Macro-F1</td>
<td style="text-align: center;">Micro-F1</td>
<td style="text-align: center;">Macro-F1</td>
<td style="text-align: center;">Micro-F1</td>
<td style="text-align: center;">Macro-F1</td>
<td style="text-align: center;">Micro-F1</td>
<td style="text-align: center;">Macro-F1</td>
<td style="text-align: center;">Micro-F1</td>
</tr>
<tr>
<td style="text-align: center;">GCN [40]</td>
<td style="text-align: center;">0.2593</td>
<td style="text-align: center;">0.3570</td>
<td style="text-align: center;">0.0411</td>
<td style="text-align: center;">0.1197</td>
<td style="text-align: center;">0.3628</td>
<td style="text-align: center;">0.4040</td>
<td style="text-align: center;">0.1681</td>
<td style="text-align: center;">0.5199</td>
<td style="text-align: center;">0.4627</td>
<td style="text-align: center;">0.4903</td>
<td style="text-align: center;">0.1542</td>
<td style="text-align: center;">0.2567</td>
</tr>
<tr>
<td style="text-align: center;">GraphSage [27]</td>
<td style="text-align: center;">0.2140</td>
<td style="text-align: center;">0.2430</td>
<td style="text-align: center;">0.0486</td>
<td style="text-align: center;">0.1041</td>
<td style="text-align: center;">0.3684</td>
<td style="text-align: center;">0.4140</td>
<td style="text-align: center;">0.1962</td>
<td style="text-align: center;">0.5581</td>
<td style="text-align: center;">0.4903</td>
<td style="text-align: center;">0.5240</td>
<td style="text-align: center;">0.2641</td>
<td style="text-align: center;">0.36</td>
</tr>
<tr>
<td style="text-align: center;">GATv2 [5]</td>
<td style="text-align: center;">0.2501</td>
<td style="text-align: center;">0.2870</td>
<td style="text-align: center;">0.0859</td>
<td style="text-align: center;">0.1468</td>
<td style="text-align: center;">0.3706</td>
<td style="text-align: center;">0.4740</td>
<td style="text-align: center;">0.1199</td>
<td style="text-align: center;">0.4443</td>
<td style="text-align: center;">0.4816</td>
<td style="text-align: center;">0.5530</td>
<td style="text-align: center;">0.4354</td>
<td style="text-align: center;">0.2573</td>
</tr>
<tr>
<td style="text-align: center;">WalkLM [69]</td>
<td style="text-align: center;">0.2871</td>
<td style="text-align: center;">0.4070</td>
<td style="text-align: center;">0.2014</td>
<td style="text-align: center;">0.5437</td>
<td style="text-align: center;">0.4031</td>
<td style="text-align: center;">0.5336</td>
<td style="text-align: center;">0.0872</td>
<td style="text-align: center;">0.3823</td>
<td style="text-align: center;">0.5761</td>
<td style="text-align: center;">0.6616</td>
<td style="text-align: center;">0.3327</td>
<td style="text-align: center;">0.51</td>
</tr>
<tr>
<td style="text-align: center;">Llama 2 [16]</td>
<td style="text-align: center;">0.7167</td>
<td style="text-align: center;">0.7246</td>
<td style="text-align: center;">0.5141</td>
<td style="text-align: center;">0.7087</td>
<td style="text-align: center;">0.6608</td>
<td style="text-align: center;">0.6946</td>
<td style="text-align: center;">0.3484</td>
<td style="text-align: center;">0.5748</td>
<td style="text-align: center;">0.6739</td>
<td style="text-align: center;">0.7281</td>
<td style="text-align: center;">0.4053</td>
<td style="text-align: center;">0.5366</td>
</tr>
<tr>
<td style="text-align: center;">OFA [53]</td>
<td style="text-align: center;">0.6520</td>
<td style="text-align: center;">0.6761</td>
<td style="text-align: center;">0.5376</td>
<td style="text-align: center;">0.7178</td>
<td style="text-align: center;">0.6720</td>
<td style="text-align: center;">0.7045</td>
<td style="text-align: center;">0.3199</td>
<td style="text-align: center;">0.5792</td>
<td style="text-align: center;">0.6280</td>
<td style="text-align: center;">0.6987</td>
<td style="text-align: center;">0.4053</td>
<td style="text-align: center;">0.5366</td>
</tr>
<tr>
<td style="text-align: center;">GraphTranslator [86]</td>
<td style="text-align: center;">0.4917</td>
<td style="text-align: center;">0.5396</td>
<td style="text-align: center;">0.4417</td>
<td style="text-align: center;">0.6768</td>
<td style="text-align: center;">0.5998</td>
<td style="text-align: center;">0.6503</td>
<td style="text-align: center;">0.3431</td>
<td style="text-align: center;">0.5651</td>
<td style="text-align: center;">0.6445</td>
<td style="text-align: center;">0.7077</td>
<td style="text-align: center;">0.3997</td>
<td style="text-align: center;">0.5533</td>
</tr>
<tr>
<td style="text-align: center;">GraphGPT [70]</td>
<td style="text-align: center;">0.7088</td>
<td style="text-align: center;">0.7202</td>
<td style="text-align: center;">0.5715</td>
<td style="text-align: center;">0.7359</td>
<td style="text-align: center;">0.6766</td>
<td style="text-align: center;">0.7097</td>
<td style="text-align: center;">0.3610</td>
<td style="text-align: center;">0.5811</td>
<td style="text-align: center;">0.6785</td>
<td style="text-align: center;">0.7335</td>
<td style="text-align: center;">0.4166</td>
<td style="text-align: center;">0.5400</td>
</tr>
<tr>
<td style="text-align: center;">Path-LLM (Ours)</td>
<td style="text-align: center;">0.7595</td>
<td style="text-align: center;">0.7674</td>
<td style="text-align: center;">0.6101</td>
<td style="text-align: center;">0.7419</td>
<td style="text-align: center;">0.7524</td>
<td style="text-align: center;">0.7773</td>
<td style="text-align: center;">0.4529</td>
<td style="text-align: center;">0.6616</td>
<td style="text-align: center;">0.7034</td>
<td style="text-align: center;">0.7545</td>
<td style="text-align: center;">0.4409</td>
<td style="text-align: center;">0.5600</td>
</tr>
</tbody>
</table>
<p>0). Therefore, we obtain the topology structure $V$ and $E^{<em>}$ of new TAG $G^{</em>}$ without weights.
Edge importance weights assignment in $G^{<em>}$. To find the most important subgraph $T$ satisfying the requirements of both with minimum edges and semantically strongest related, covering all keywords in $Q$, we assign edge weights based on edge importance values. Considering one subgraph $T$, the importance of $T$ is measured as the product of all edge importance values in $T$, i.e., $\prod_{e_{i, j} \in E_{T}} f^{</em>}\left(e_{i, j}\right)$. Next, we convert the edge importance to the edge weight as $w_{i, j}^{<em>}=-\log f^{</em>}\left(e_{i, j}\right)$, thereby transforming our keyword search task into the traditional keyword search task as follows,</p>
<p>$$
\min <em T="T">{E</em> \in E^{<em>}} \sum w_{i, j}^{</em>}=\min <em T="T">{E</em> \in E^{<em>}} \sum-\log f^{</em>}\left(e_{i, j}\right) \Leftrightarrow \max <em T="T">{E</em> \in E^{<em>}} \prod f^{</em>}\left(e_{i, j}\right)
$$</p>
<p>The goal of finding traditional Steiner Tree is $\min \sum_{e_{i, j} \in E_{T}} w_{i, j}^{<em>}$, the same as finding the most important subtree $T, \max \prod_{e_{i, j} \in E_{T}} f^{</em>}\left(e_{i, j}\right)$. As a result, we finally obtain a new weighted TAG $G^{<em>}\left(V, E^{</em>}, W^{<em>}\right)$. Across this TAG, we can search the subtree that satisfies both the fewest edges and semantically strongest related.
Our solution of keyword search over $G^{</em>}$. We first consider one simple case of keyword search with $|Q|=2$ for two query nodes $v_{i}, v_{j}$. We use Dijkstra's algorithm [14] to search for the shortest path between $v_{i}$ and $v_{j}$ based on the new weight $W^{*}$. For a general keyword search with $|Q| \geq 3$, we adopt a 2-approximation greedy algorithm [59] for finding a Steiner Tree to cover all keywords $Q$ to tackle this NP-hard problem.</p>
<h2>8 EXPERIMENTS</h2>
<p>Datasets. We conduct evaluations on six diverse datasets in Table 2, encompassing graphs of varying scales and types: PubMed [80],</p>
<p>Freebase [3], Cora [58], Citeseer [24], Facebook [44], and ARXIV [32]. PubMed is a biomedical network containing four types of nodes: genes, diseases, chemicals, and species [80]. Freebase is a multidomain knowledge graph with eight types of nodes, ranging from books to business [3]. Facebook is a social network with textual user attributes [44]. The other three datasets are citation networks with text attributes like title, abstract, and so on. Raw text data of Cora and Citeseer are collected from [11]. Table 2 also reports the number of training paths for WalkLM and Path-LLM, respectively. Our Path-LLM, as an effective learning model, uses significantly fewer L2SP-based shortest paths compared to the number of random-walk-based paths in WalkLM [69], saving the number of training paths by an average of $91.09 \%$ across six datasets.
Competitive methods. We compare our Path-LLM with the SOTA GNN-, LM- and LLM-based methods as follows.</p>
<ul>
<li>Three classical graph learning models: We test three methods, GCN [40], GraphSage [27], and GATv2 [5]. The GATv2 [5] proposes a dynamic graph attention variant and captures more expressive graph structures.</li>
<li>WalkLM [69]: WalkLM is the state-of-the-art LM-based method, integrating random walks and RoBERTa [56] for unified graph representation learning.</li>
<li>Llama 2-7B [16]: Llama 2-7B is the widely used large language model for graph embeddings with exceptionally powerful semantic representation abilities.</li>
<li>OFA [53]: OFA uses natural language to describe nodes and then encodes texts to feature vectors as node embeddings.</li>
<li>GraphTranslator [86]: GraphTranslator proposes the graphtext alignment method to derive node embeddings.</li>
</ul>
<p>Table 4: The results of edge validation on six datasets in self-supervised settings: PubMed, Freebase, Cora, ARXIV, Citeseer and Facebook. The best performances are in bold, and the second-best are shaded in gray. Corresponding std are provided in Appendix.</p>
<table>
<thead>
<tr>
<th>Datasets</th>
<th>PubMed</th>
<th></th>
<th>Freebase</th>
<th></th>
<th>Cora</th>
<th></th>
<th>ARXIV</th>
<th></th>
<th>Citeseer</th>
<th></th>
<th>Facebook</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Metrics</td>
<td>AUC</td>
<td>Accuracy</td>
<td>AUC</td>
<td>Accuracy</td>
<td>AUC</td>
<td>Accuracy</td>
<td>AUC</td>
<td>Accuracy</td>
<td>AUC</td>
<td>Accuracy</td>
<td>AUC</td>
<td>Accuracy</td>
</tr>
<tr>
<td>GCN [40]</td>
<td>0.5155</td>
<td>0.5426</td>
<td>0.5012</td>
<td>0.0144</td>
<td>0.6680</td>
<td>0.7018</td>
<td>0.5216</td>
<td>0.4415</td>
<td>0.6392</td>
<td>0.6052</td>
<td>0.4354</td>
<td>0.2573</td>
</tr>
<tr>
<td>GraphSage [27]</td>
<td>0.5133</td>
<td>0.5211</td>
<td>0.5426</td>
<td>0.6519</td>
<td>0.7445</td>
<td>0.4052</td>
<td>0.5110</td>
<td>0.3120</td>
<td>0.7822</td>
<td>0.5227</td>
<td>0.5157</td>
<td>0.3900</td>
</tr>
<tr>
<td>GATv2 [5]</td>
<td>0.5204</td>
<td>0.5011</td>
<td>0.6065</td>
<td>0.5390</td>
<td>0.5143</td>
<td>0.4488</td>
<td>0.2550</td>
<td>0.6120</td>
<td>0.7488</td>
<td>0.5986</td>
<td>0.2117</td>
<td>0.39</td>
</tr>
<tr>
<td>WalkLM [69]</td>
<td>0.5962</td>
<td>0.5684</td>
<td>0.7039</td>
<td>0.6519</td>
<td>0.8581</td>
<td>0.7746</td>
<td>0.8799</td>
<td>0.7923</td>
<td>0.9149</td>
<td>0.8424</td>
<td>0.5665</td>
<td>0.5251</td>
</tr>
<tr>
<td>Llama 2 [16]</td>
<td>0.7144</td>
<td>0.6665</td>
<td>0.8183</td>
<td>0.7481</td>
<td>0.8568</td>
<td>0.7809</td>
<td>0.9157</td>
<td>0.8379</td>
<td>0.9290</td>
<td>0.8550</td>
<td>0.6454</td>
<td>0.5891</td>
</tr>
<tr>
<td>OFA [53]</td>
<td>0.6061</td>
<td>0.5719</td>
<td>0.8203</td>
<td>0.7517</td>
<td>0.8473</td>
<td>0.7585</td>
<td>0.9091</td>
<td>0.8291</td>
<td>0.8979</td>
<td>0.8125</td>
<td>0.5739</td>
<td>0.5380</td>
</tr>
<tr>
<td>GraphTranslator [86]</td>
<td>0.5939</td>
<td>0.5618</td>
<td>0.7842</td>
<td>7157</td>
<td>0.8473</td>
<td>0.7585</td>
<td>0.9091</td>
<td>0.8291</td>
<td>0.8979</td>
<td>0.8124</td>
<td>0.6049</td>
<td>0.5891</td>
</tr>
<tr>
<td>GraphGPT [70]</td>
<td>0.7134</td>
<td>0.6631</td>
<td>0.8381</td>
<td>0.7683</td>
<td>0.8679</td>
<td>0.7857</td>
<td>0.9221</td>
<td>0.8430</td>
<td>0.9329</td>
<td>0.8570</td>
<td>0.6921</td>
<td>0.6357</td>
</tr>
<tr>
<td>Path-LLM (Ours)</td>
<td>0.7497</td>
<td>0.7111</td>
<td>0.8456</td>
<td>0.7794</td>
<td>0.9244</td>
<td>0.8476</td>
<td>0.9655</td>
<td>0.9060</td>
<td>0.9627</td>
<td>0.8966</td>
<td>0.7019</td>
<td>0.6423</td>
</tr>
</tbody>
</table>
<ul>
<li>GraphGPT [70]: GraphGPT proposes to encode graph structures and utilizes a self-supervised instruction tuning method for Vicuna-7B-v1.5 training, which is the state-of-the-art LLMbased baseline for graph representation. Experimental settings. We mainly compare eight advanced baselines under the setting of self-supervised graph learning. Most experimental settings follow WalkLM, including train-test data split ratio 8:2, five-fold cross-validation, and the one-layer MLP, except for epoch settings and supplementary models. To assess the effectiveness of different graph embeddings, we use a simple but challenging setting designed to better differentiate embedding performance while saving cost and time. For node classification, the one-layer MLP training epoch is set to 50, compared to 2000 epochs in WalkLM. For edge validation, the MLP training epoch is set to 100 without any additional models, compared to 2000 epochs in WalkLM with the LMNN as supplementary models. We choose Llama 2-7B [16] as the base LLM in our Path-LLM method. All models are optimized using the Adam optimizer [39] with an initial learning rate of $2 \mathrm{e}-4$. The rank and scaling factors of the LoRA adapter [15] are set to 16 and 32 , respectively. The minimum length of the long shortest path is set to $L=10$ for particular social networks with an average distance of 4 to 7 . For other parameters, we set $k=5, b=1000$ and $\ell=3$. All experiments are implemented with an NVIDIA A100 (80G) GPU. Exp-1: Effectiveness on node classification task. Node classification task is to assign labels to nodes in a graph based on node embeddings integrating the attributes of nodes and the relationships between them. For node classification, we train a separate one-layer MLP classifier based on all unified graph embeddings and evaluate graph embeddings derived from all methods with Macro-F1 (across all labels) and Micro-F1 (across all nodes) [26]. Note that for GNNs, we train GNNs through contrastive learning to generate unified graph embeddings and then feed them into MLP for downstream tasks. As shown in Table 3, our proposed Path-LLM showcases substantial performance enhancements over WalkLM and other GNN-based and LLM-based competitors on six diverse datasets. For PubMed, Path-LLM achieves a remarkable 174.56\% relative improvement in macro-F1 and 104.24\% in micro-F1 over WalkLM. Moreover, Path-LLM outperforms WalkLM by achieving a $72.58 \%$ relative improvement in macro-F1 and $36.82 \%$ in micro-F1 on</li>
</ul>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Efficiency evaluation. Training time comparison of Path-LLM and WalkLM across six datasets (PubMed, Freebase, Cora, Citeseer, ARXIV, and Facebook).</p>
<p>Cora. Even on large-scale graphs, Path-LLM outperforms WalkLM by achieving a $419.38 \%$ relative improvement in macro-F1 and $73.06 \%$ in micro-F1 on ARXIV. In addition, it demonstrates that the shortest path structure can significantly enhance the effectiveness of Path-LLM, with an average $4.66 \%$ gain of the pure LLM method Llama 2 [16] on PubMed and an average $12.89 \%$ gain of Llama 2 [16] on Cora, verifying that the structural information learned by Path-LLM is beneficial for node classification. Furthermore, it also demonstrates that our self-supervised approach can learn graph structure features better for unified graph embeddings compared to the existing state-of-the-art LLM self-supervised method. Compared to GraphGPT [70], Path-LLM outperforms GraphGPT across all six datasets. On large-scale graphs, Path-LLM achieves relative gains of $25.46 \%$ in macro-F1 on ARXIV and $6.75 \%$ on Freebase. Additionally, we also show results of node classification in the setting of WalkLM in Table 6. Exp-2: Effectiveness on edge validation task. The edge validation task involves determining whether an edge exists between two given nodes (refer to link prediction in [69][51][70][53][10]). Formally, given two nodes $u$ and $v$, verify whether $(u, v) \in E$ or $(u, v) \notin E$. We verify the existence of $(u, v)$ based on the learned graph embeddings. Specifically, we use the Hadamard function to construct feature vectors for node pairs and train a two-layer MLP classifier on the selected links. We evaluate graph embeddings with AUC (area under the ROC curve) and Accuracy [34]. Note that for GNNs, we use GNNs to generate graph embeddings and then construct feature vectors for node pairs. Table 4 shows that our</p>
<p>Table 5: Ablation study: Experimental results of different path structures involving 1-hop neighbors, random walks (RW), $(\alpha, \beta, \gamma)$ RW, randomly sampled short shortest paths (Random Short) and long shortest paths.</p>
<table>
<thead>
<tr>
<th>Datasets</th>
<th>PubMed</th>
<th></th>
<th></th>
<th></th>
<th>Cora</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Tasks</td>
<td>Node classification</td>
<td></td>
<td>Edge Validation</td>
<td></td>
<td>Node classification</td>
<td></td>
<td>Edge Validation</td>
<td></td>
</tr>
<tr>
<td>Metrics</td>
<td>Macro-F1</td>
<td>Micro-F1</td>
<td>AUC</td>
<td>Accuracy</td>
<td>Macro-F1</td>
<td>Micro-F1</td>
<td>AUC</td>
<td>Accuracy</td>
</tr>
<tr>
<td>LLM</td>
<td>$0.716(-3.1 \%)$</td>
<td>$0.724(-3.1 \%)$</td>
<td>$0.714(-3.5 \%)$</td>
<td>$0.666(-4.5 \%)$</td>
<td>$0.660(-9.2 \%)$</td>
<td>$0.694(-8.3 \%)$</td>
<td>$0.856(-6.8 \%)$</td>
<td>$0.780(-6.7 \%)$</td>
</tr>
<tr>
<td>LLM+1hop</td>
<td>$0.721(-2.6 \%)$</td>
<td>$0.737(-1.8 \%)$</td>
<td>$0.653(-9.6 \%)$</td>
<td>$0.612(-9.9 \%)$</td>
<td>$0.673(-7.9 \%)$</td>
<td>$0.704(-7.1 \%)$</td>
<td>$0.870(-5.4 \%)$</td>
<td>$0.791(-5.6 \%)$</td>
</tr>
<tr>
<td>LLM+RW</td>
<td>$0.723(-2.4 \%)$</td>
<td>$0.735(-2.0 \%)$</td>
<td>$0.642(-10.7 \%)$</td>
<td>$0.604(-10.7 \%)$</td>
<td>$0.668(-8.4 \%)$</td>
<td>$0.701(-7.8 \%)$</td>
<td>$0.872(-5.2 \%)$</td>
<td>$0.789(-5.8 \%)$</td>
</tr>
<tr>
<td>LLM+ $(\alpha, \beta, \gamma)$ RW [51]</td>
<td>$0.721(-2.6 \%)$</td>
<td>$0.737(-1.8 \%)$</td>
<td>$0.663(-8.6 \%)$</td>
<td>$0.624(-8.7 \%)$</td>
<td>$0.675(-7.7 \%)$</td>
<td>$0.706(-7.1 \%)$</td>
<td>$0.875(-4.9 \%)$</td>
<td>$0.798(-4.9 \%)$</td>
</tr>
<tr>
<td>LLM+Random Short</td>
<td>$0.728(-1.9 \%)$</td>
<td>$0.742(-1.3 \%)$</td>
<td>$0.655(-9.4 \%)$</td>
<td>$0.614(-9.7 \%)$</td>
<td>$0.674(-7.8 \%)$</td>
<td>$0.708(-6.9 \%)$</td>
<td>$0.886(-3.8 \%)$</td>
<td>$0.806(-4.1 \%)$</td>
</tr>
<tr>
<td>LLM+Long SP</td>
<td>$0.729(-1.8 \%)$</td>
<td>$0.744(-1.1 \%)$</td>
<td>$0.658(-9.1 \%)$</td>
<td>$0.619(-9.2 \%)$</td>
<td>$0.669(-8.3 \%)$</td>
<td>$0.705(-7.2 \%)$</td>
<td>$0.877(-4.7 \%)$</td>
<td>$0.800(-4.7 \%)$</td>
</tr>
<tr>
<td>LLM + L2SP</td>
<td>0.747</td>
<td>0.755</td>
<td>0.749</td>
<td>0.711</td>
<td>0.752</td>
<td>0.777</td>
<td>0.924</td>
<td>0.847</td>
</tr>
</tbody>
</table>
<p>Path-LLM achieves remarkable performance in uncovering latent associations among nodes in text-attributed graphs. This implies that Path-LLM possesses a more precise grasp of graph structure through semantic information integration, thus conferring advantages in edge validation tasks. Path-LLM consistently outperforms WalkLM, achieving a notable $25.74 \%$ relative improvement in AUC and $25.11 \%$ in Accuracy on PubMed. Meanwhile, Path-LLM achieves a notable $7.73 \%$ relative improvement in AUC and $9.42 \%$ in Accuracy over WalkLM on Cora. On the large-scale citation network AROIV, Path-LLM shows a relative performance gain of $9.73 \%$ in AUC and $14.35 \%$ in Accuracy over WalkLM. Compared to pure LLM, the structural advantages displayed by the L2SP on PubMed have an average $5.82 \%$ increase and a $7.90 \%$ increase in AUC on Cora. Table 4 shows that our Path-LLM self-supervised method of learning graph structure from L2SP is significantly effective, outperforming GraphGPT [70] on all six datasets, with an average relative gain of $4.54 \%$ in AUC and $6.39 \%$ in Accuracy. Additionally, we also show results of edge validation in the setting of WalkLM in Table 6. Exp-3: Training duration across six datasets. We compare the training times of WalkLM and Path-LLM. Even though WalkLM utilizes the smaller language model distill-roberta-66M, Path-LLM demonstrates significantly faster performance on various graphs, as illustrated in Figure 7. On average, Path-LLM is 12 times faster than WalkLM across these datasets and 11 times faster across millionsscale graphs. Notably, on PubMed, Path-LLM takes only 380 seconds, while WalkLM takes 13,370 seconds, making Path-LLM approximately 35 times faster. Based on the analysis of complexity, two main reasons are: (1) the average text length $\hat{S}$ in Path-LLM is much smaller than $\hat{S}$ in WalkLM, and (2) the number of training paths $\left|{ }<em _LLM="{LLM" _text="\text">{s b o r t}\right|$ in Path-LLM is much less than $|N|$ in WalkLM. Additionally, the number of L2SP paths employed for training PathLLM is significantly lower than the number of Random Walk paths used for training WalkLM, resulting in an average reduction of $91.09 \%$ in training data across six datasets. Efficiency results shown in Figure 7 verify that the average text length is a critical factor affecting training time, represented by $\hat{S}$ in the time complexity $O\left(B</em>, d\right)\right)$ of the Path-LLM training process. L2SPbased texts in PubMed are much shorter than those in the other three citation networks. As a result, although there are more L2SP paths in PubMed than in Cora and Citeseer, the model training is significantly faster in PubMed.}}\left(I,\left|{ }_{s b o r t}\right|, \hat{S</p>
<p>Table 6: Results in the setting of WalkLM.</p>
<table>
<thead>
<tr>
<th>Datasets</th>
<th>PubMed</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Tasks</td>
<td>Node classification</td>
<td></td>
<td>Edge validation</td>
<td></td>
</tr>
<tr>
<td>Metrics</td>
<td>Macro-F1</td>
<td>Micro-F1</td>
<td>AUC</td>
<td>Accuracy</td>
</tr>
<tr>
<td>WalkLM</td>
<td>0.6044</td>
<td>0.6210</td>
<td>0.8359</td>
<td>0.7754</td>
</tr>
<tr>
<td>Path-LLM</td>
<td>0.7622</td>
<td>0.7674</td>
<td>0.8640</td>
<td>0.7983</td>
</tr>
</tbody>
</table>
<p>Table 7: Ablation study of hyperparameters $L$, when $k=10$.</p>
<table>
<thead>
<tr>
<th>Datasets</th>
<th>PubMed</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Tasks</td>
<td>Node classification</td>
<td></td>
<td>Edge validation</td>
<td></td>
</tr>
<tr>
<td>Metrics</td>
<td>Macro-F1</td>
<td>Micro-F1</td>
<td>AUC</td>
<td>Accuracy</td>
</tr>
<tr>
<td>$L=4$</td>
<td>0.7285</td>
<td>0.7422</td>
<td>0.6829</td>
<td>0.6424</td>
</tr>
<tr>
<td>$L=6$</td>
<td>0.7308</td>
<td>0.7445</td>
<td>0.7085</td>
<td>0.6734</td>
</tr>
<tr>
<td>$L=8$</td>
<td>0.7372</td>
<td>0.7466</td>
<td>$\mathbf{0 . 7 5 6 0}$</td>
<td>$\mathbf{0 . 7 1 0 4}$</td>
</tr>
<tr>
<td>$L=10$</td>
<td>$\mathbf{0 . 7 5 0 1}$</td>
<td>$\mathbf{0 . 7 5 3 2}$</td>
<td>0.7362</td>
<td>0.6941</td>
</tr>
<tr>
<td>$L=12$</td>
<td>0.7277</td>
<td>0.7357</td>
<td>0.6995</td>
<td>0.6595</td>
</tr>
</tbody>
</table>
<p>Table 8: Ablation study of hyperparameters $k$, when $L=10$.</p>
<table>
<thead>
<tr>
<th>Datasets</th>
<th>PubMed</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Tasks</td>
<td>Node classification</td>
<td></td>
<td>Edge validation</td>
<td></td>
</tr>
<tr>
<td>Metrics</td>
<td>Macro-F1</td>
<td>Micro-F1</td>
<td>AUC</td>
<td>Accuracy</td>
</tr>
<tr>
<td>$k=1$</td>
<td>$\mathbf{0 . 7 5 2 9}$</td>
<td>$\mathbf{0 . 7 6 4 4}$</td>
<td>0.6866</td>
<td>0.6451</td>
</tr>
<tr>
<td>$k=3$</td>
<td>0.7334</td>
<td>0.7467</td>
<td>0.6967</td>
<td>0.6571</td>
</tr>
<tr>
<td>$k=5$</td>
<td>0.7471</td>
<td>0.7555</td>
<td>0.7497</td>
<td>$\mathbf{0 . 7 1 1 1}$</td>
</tr>
<tr>
<td>$k=7$</td>
<td>0.7383</td>
<td>0.7511</td>
<td>0.7453</td>
<td>0.7063</td>
</tr>
<tr>
<td>$k=10$</td>
<td>0.7501</td>
<td>0.7532</td>
<td>0.7362</td>
<td>0.6941</td>
</tr>
<tr>
<td>$k=12$</td>
<td>0.7186</td>
<td>0.7379</td>
<td>$\mathbf{0 . 7 5 1 0}$</td>
<td>0.7059</td>
</tr>
</tbody>
</table>
<p>Exp-4: Ablation studies. We perform an ablation study to verify the analysis in Section 5.2, demonstrating the effectiveness of our proposed L2SP structure compared to other path structures. To conduct a comprehensive evaluation, we propose several baselines for ablation study: LLM with 1-hop neighbors, LLM with random walk, LLM with new $(\alpha, \beta, \gamma)$ random walk [51], LLM with long shortest path and LLM with randomly sampled short shortest path. Particularly, when reproducing $(\alpha, \beta, \gamma)$ random walk, we utilize Sentence-BERT [63] to process node embeddings. Here, we</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Robustness evaluation of Path-LLM.
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: Performance comparison of Path-LLM and other LLM-based methods after adding 20\% noisy data in Cora.</p>
<p>Table 9: Performance comparison of different LLMs on node classification (NC) and edge validation (EV).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Datasets</th>
<th style="text-align: center;">PubMed</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Cora</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Tasks</td>
<td style="text-align: center;">NC</td>
<td style="text-align: center;">EV</td>
<td style="text-align: center;">NC</td>
<td style="text-align: center;">EV</td>
</tr>
<tr>
<td style="text-align: left;">Metrics</td>
<td style="text-align: center;">Micro-F1</td>
<td style="text-align: center;">AUC</td>
<td style="text-align: center;">Micro-F1</td>
<td style="text-align: center;">AUC</td>
</tr>
<tr>
<td style="text-align: left;">Llama 1 [71]</td>
<td style="text-align: center;">0.7158</td>
<td style="text-align: center;">0.6887</td>
<td style="text-align: center;">0.6990</td>
<td style="text-align: center;">0.8439</td>
</tr>
<tr>
<td style="text-align: left;">Llama 1+L2SP</td>
<td style="text-align: center;">$\mathbf{0 . 7 4 4 5}$</td>
<td style="text-align: center;">$\mathbf{0 . 7 3 2 2}$</td>
<td style="text-align: center;">$\mathbf{0 . 7 6 4 7}$</td>
<td style="text-align: center;">$\mathbf{0 . 9 1 7 9}$</td>
</tr>
<tr>
<td style="text-align: left;">Llama 2 [16]</td>
<td style="text-align: center;">0.7246</td>
<td style="text-align: center;">0.7144</td>
<td style="text-align: center;">0.6946</td>
<td style="text-align: center;">0.8568</td>
</tr>
<tr>
<td style="text-align: left;">Path-LLM (Ours)</td>
<td style="text-align: center;">$\mathbf{0 . 7 5 5 5}$</td>
<td style="text-align: center;">$\mathbf{0 . 7 4 9 7}$</td>
<td style="text-align: center;">$\mathbf{0 . 7 7 7 3}$</td>
<td style="text-align: center;">$\mathbf{0 . 9 2 4 4}$</td>
</tr>
</tbody>
</table>
<p>present our findings of L2SP-based shortest paths and Path-LLM from PubMed, Freebase, Cora, ARXIV, Citeseer and Facebook.</p>
<p>Table 10: Results of average answer distance among query keywords. Lower distance shows better performance.</p>
<p>| Keywords Number | $|Q|=2$ | $|Q|=3$ | $|Q|=4$ |
| :--: | :--: | :--: | :--: |
| uniform weight | 4.0 | 6.16 | 8.73 |
| WalkLM [69] | 1.27 | 1.91 | 2.72 |
| GraphGPT [70] | 1.24 | 1.87 | 2.63 |
| Path-LLM | $\mathbf{0 . 9 3}$ | $\mathbf{1 . 4 6}$ | $\mathbf{2 . 0 4}$ |</p>
<p>Ablation-study-1: L2SP-based shortest paths outperform random walks. Table 5 indicates that the L2SP-based shortest path structure outperforms the random walk structure on node classification and edge validation. We compare L2SP-based shortest paths with two random walk algorithms, which are the random walk in WalkLM and the new $(\alpha, \beta, \gamma)$ random walk. Path-LLM is superior to the new $(\alpha, \beta, \gamma)$ random walk with an average increase of $3.03 \%$ on PubMed and $10.74 \%$ on Cora for node classification. While in edge validation, RW-based LLM even decreased by $10.08 \%$ of AUC after training with random walks compared to pure LLM. Our L2SP-based LLM outperforms the RW-based LLM with an average increase of $17.19 \%$ on PubMed and $6.66 \%$ on Cora, demonstrating
that L2SP represents a better path for capturing graph structure compared to Random Walk.
Ablation-study-2: L2SP-based shortest paths outperform other different path structures. Table 5 also compares the effectiveness of different graph embeddings with distinct path structures on node classification and edge validation. It is evident that the L2SPbased shortest path structure outperforms all other path structures. Especially in edge validation, which evaluates the graph structure understanding of LLMs, the results show that different path structures capture different graph features, which influence LLMs' learning of the graph structure. Our proposed L2SP-based structure yields the best results among all other different path structures, with $12.31 \%$ performance gains in AUC and $12.95 \%$ accuracy over randomly sampled shortest paths on PubMed while an average of $5.62 \%$ performance gains over original long shortest paths on Cora. Ablation-study-3: Different hyperparameters analysis. We conduct ablation studies varying hyperparameters $L$ and $k$, testing $L$ in the interval $[4,12]$ and $k$ in the interval $[1,12] . L$ represents the minimum length of the long shortest path in the L2SP selection phase. $k$ denotes the number of random samples of long shortest paths with the same source and target nodes in the L2SP selection phase. Results shown in Table 7 show that Path-LLM performs well in downstream tasks for $L$ in [8,10], and we choose $L=10$ in experiments. Table 8 shows that $k=5$ is a good trade-off choice. Ablation-study-4: Our Path-LLM shows strong robustness even after adding 20\% noisy data. For each text attribute in Cora, we add $2 \%-20 \%$ noisy and irrelevant words in random positions. We then use Path-LLM to generate embeddings based on noisy Cora. Results in Figure 8 show the strong robustness of our Path-LLM with a slight decrease of $4 \%$. In addition, we compare with other LLM-based baselines on Cora after adding 20\% noisy data. Results in Figure 9 show that Path-LLM outperforms all baselines, once again verifying the strong robustness of Path-LLM.
Ablation-study-5: Our L2SP-based self-supervised learning consistently enhances the performance of different LLMs. In Table 9, we evaluate Llama 1 and Llama 2. After L2SP-based selfsupervised training, LLMs exhibit an average absolute improvement of $3 \%$ in node classification and $4-5 \%$ in edge validation.
Exp-5: Keyword search evaluation. We quantitatively evaluate the effectiveness of keyword search. We use the sum distance among given keywords as our evaluation metric. The lower the distance, the closer the connection among keywords, and the better the result. We test three groups of keyword queries on PubMed, i.e., 2-keyword, 3-keyword, and 4-keyword. For each group, we randomly test 100 queries and get the average result. The results in Table 10 show that Path-LLM can find answers with the smallest distance.
Exp-6: Case study of keyword search. We conduct a case study of keyword search by comparing three graph weighting methods, including the uniform weights, WalkLM-based weights, and our Path-LLM-based weights. This aims to evaluate the effectiveness of Path-LLM weights to find tight groups in terms of graph structure and node semantics. Our comprehensive case study thoroughly explores the inherent benefits of Path-LLM concerning both graph structure and semantics on PubMed. Figure 10 shows Steiner trees based on four given keywords $Q=\left(q_{1}\right.$ : hepatocellular, $q_{2}$ : craniofacial anomalies, $q_{3}$ : attention impairment, $q_{4}$ : intra-uterine $}$. From</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10: Case study of keyword search with a query $Q=\left{q_{1}\right.$ : hepatocellular, $q_{2}$ : craniofacial anomalies, $q_{3}$ : attention impairment, $q_{4}$ : intra-uterine $}$. Path-LLM finds a semantically meaningful answer with the smallest number of five edges.
<img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 11: Embedding visualization of Cora.
<img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 12: Embedding visualization of PubMed.
the graph structure perspective, the Path-LLM-based weight obtains the fewest five edges and the closest ties between nodes. In contrast, graphs generated with uniform and WalkLM-based weights are sparser, with longer and less tight connections between nodes, like the association between the nodes $\left{w_{5}\right.$ : acromegaloid} and $\left{w_{6}\right.$ : impaired glucose homeostasis $}$. From the node semantics perspective, the Path-LLM-based weighted subtree contains rich and medically proven associations. To elaborate, the occurrence of $\left(q_{1}\right)$ hepatocellular issues along with symptoms of $\left(q_{2}\right)$ craniofacial anomalies is highly indicative of the presence of $\left(w_{5}\right)$ acromegaly $[1,7]$. $\left(w_{5}\right)$ may lead to $\left(w_{6}\right)$ impaired glucose homeostasis [6, 9] with potential risks of $\left(q_{3}\right)$ attention impairment caused by brain damage $[2,38,67]$ and $\left(q_{4}\right)$ intra-uterine growth retardation $[23,42,60]$. In contrast, subtrees derived from uniform
and WalkLM-based weights exhibit weaker semantic associations between nodes and lack strong medical evidence.
Exp-7: Visualization of Path-LLM embeddings. For an intuitive comparison, we visualize the embedding space of different types of nodes learned by WalkLM and our proposed Path-LLM on Cora and PubMed, respectively. The embeddings are further transformed into the 2-dimensional Euclidean space via the t-SNE [12]. The nodes are colored according to their types. Figures 11 and 12 show that node representations derived from our Path-LLM are more discriminative from different classes than WalkLM.</p>
<h2>9 CONCLUSIONS</h2>
<p>In this paper, we propose a novel Path-LLM model for unified graph representation learning. The key design of our model involves a sampled selection of our proposed L2SP-based shortest paths to represent the whole network. Then, we utilize a large language model to integrate graph structure into deep semantic embedding space by learning our L2SP-based texts. We develop techniques to construct a new weighted graph based on Path-LLM-based embedding and tackle an NP-hard graph querying task of keyword search by finding better answers. Comprehensive experiments validate the effectiveness of our Path-LLM model and embeddings.</p>
<h2>REFERENCES</h2>
<p>[1] Amit Akirov, Hiba Masri-Iraqi, Idit Dotan, and Ilan Shimon. 2021. The biochemical diagnosis of acromegaly. Journal of clinical medicine 10, 5 (2021), 1147.
[2] ROLAND N Auer. 1986. Progress review: hypoglycemic brain damage. Stroke 17, 4 (1986), 699-708.
[3] Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: a collaboratively created graph database for structuring human knowledge. In Proceedings of the 2008 ACM SIGMOD international conference on Management of data. 1247-1250.
[4] Andrei Broder, Ravi Kumar, Farzin Maghool, Prabhakar Raghavan, Sridhar Rajagopalan, Raymie Stata, Andrew Tomkins, and Janet Wiener. 2000. Graph structure in the web. Computer networks 33, 1-6 (2000), 309-320.
[5] Shaked Brody, Uri Alon, and Eran Yahav. 2022. How Attentive are Graph Attention Networks?: In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022.
[6] John D Carmichael, Vivien S Bonert, James M Mirocha, and Shlomo Melmed. 2009. The utility of oral glucose tolerance testing for diagnosis and assessment of treatment outcomes in 166 patients with acromegaly. The Journal of Clinical Endocrinology \&amp; Metabolism 94, 2 (2009), 523-527.</p>
<p>[7] John D Carmichael, Michael S Broder, Dasha Cherepanov, Eunice Chang, Adam Mamelak, Qaiyjim Said, Maureen P Neary, and Vivien Bonert. 2017. The association between biochemical control and cardiovascular risk factors in acromegaly. BMC endocrine disorders 17 (2017), 1-6.
[8] Rudolf Carnap. 1962. Logical foundations of probability. Vol. 2. Citeseer.
[9] Beatrix M Chang-DeMoranville and Ivor MD Jackson. 1992. Diagnosis and endocrine testing in acromegaly. Endocrinology and metabolism clinics of North America 21, 3 (1992), 649-668.
[10] Runjin Chen, Tong Zhao, AJAY KUMAR JAISWAL, Neil Shah, and Zhangyang Wang. 2024. LLaGA: Large Language and Graph Assistant. In Forty-first International Conference on Machine Learning.
[11] Zhikai Chen, Haitao Mao, Hang Li, Wei Jin, Hongzhi Wen, Xiaochi Wei, Shuaiqiang Wang, Dawei Yin, Wensp Fan, Hui Liu, and Jiliang Tang. 2023. Exploring the Potential of Large Language Models (LLMs)in Learning on Graphs. SIGKDD Explor. 25, 2 (2023), 42-61.
[12] Laurens Van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-sne journal of machine learning research. Journal of Machine Learning Research 9 (2008), 2579-2605.
[13] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). 4171-4186.
[14] Edsger W. Dijkstra. 1959. A note on two problems in connexion with graphs. Numer. Math. 1 (1959), 269-271.
[15] Edward J. Hu et al. 2022. LoRA: Low-Rank Adaptation of Large Language Models. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022.
[16] Hugo Touvron et al. 2023. Llama 2: Open Foundation and Fine-Tuned Chat Models. CoRR abs/2307.09288 (2023). arXiv:2307.09288
[17] Long Ouyang et al. 2022. Training language models to follow instructions with human feedback. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems.
[18] Lichao Sun et al. 2024. TrustLLM: Trustworthiness in Large Language Models. CoRR abs/2401.05561 (2024). arXiv:2401.05561
[19] Yixiang Fang, Wensheng Luo, and Chenhao Ma. 2022. Densest subgraph discovery on large graphs: Applications, challenges, and techniques. Proceedings of the VLDB Endowment 15, 12 (2022), 3766-3769.
[20] Corina Florescu and Cornelia Caragea. 2017. Positionrank: An unsupervised approach to keyphrase extraction from scholarly documents. In Proceedings of the 55th annual meeting of the association for computational linguistics (volume 1: long papers). 1105-1115.
[21] Luciano Floridi and Massimo Chiriatti. 2020. GPT-3: Its nature, scope, limits, and consequences. Minds and Machines 30 (2020), 681-694.
[22] Jun Gao, Jiazun Chen, Zhao Li, and Ji Zhang. 2021. ICS-GNN: lightweight interactive community search via graph neural network. Proceedings of the VLDB Endowment 14, 6 (2021), 1006-1018.
[23] Maria G-Femes, Sofia Asim Rahman, Ritika R Kapoor, Sarah Flanagan, Jayne AL Houghton, Shivani Misra, Nick Oliver, Mehul Tulsidas Dattani, and Pratik Shah. 2020. Hyperinsulinemic hypoglycemia in children and adolescents: recent advances in understanding of pathophysiology and management. Reviews in Endocrine and Metabolic Disorders 21 (2020), 577-597.
[24] C Lee Giles, Kurt D. Bollacker, and Steve Lawrence. 1998. CiteSeer: An Automatic Citation Indexing System. In Proceedings of the 3rd ACM International Conference on Digital Libraries, June 23-26, 1998, Pittsburgh, PA, USA. ACM, 89-98.
[25] Justin Gilmer, Samuel S Schwenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. 2017. Neural message passing for quantum chemistry. In International Conference on Machine Learning. PMLR, 1263-1272.
[26] Cyril Gourte and Eric Gaussier. 2005. A Probabilistic Interpretation of Precision, Recall and F-Score, with Implication for Evaluation. In Advances in Information Retrieval, 27th European Conference on IR Research (Lecture Notes in Computer Science), Vol. 3408. Springer, 345-359.
[27] William L. Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive Representation Learning on Large Graphs. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA. 1024-1034.
[28] Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. 2020. DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION. In International Conference on Learning Representations.
[29] Xiaoxin He, Xavier Bresson, Thomas Laurent, Adam Perold, Yann LeCun, and Bryan Hooi. 2023. Harnessing Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed Graph Representation Learning. In The Twelfth International Conference on Learning Representations.
[30] Xiaoxin He, Yijun Tian, Yifei Sun, Nitesh Chawla, Thomas Laurent, Yann LeCun, Xavier Bresson, and Bryan Hooi. 2024. G-rertriever: Retrieval-augmented generation for textual graph understanding and question answering. Advances in Neural Information Processing Systems 37 (2024), 132876-132907.
[31] Roei Herzig, Alon Mendelson, Leonid Karlinsky, Assaf Arbelle, Rogério Feris, Trevor Darrell, and Amir Globerson. 2023. Incorporating Structured Representations into Pretrained Vision \&amp; Language Models Using Scene Graphs. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 14077-14098.
[32] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. 2020. Open Graph Benchmark: Datasets for Machine Learning on Graphs. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems.
[33] Zinix Hu, Yuxiao Dong, Kuansan Wang, Kai-Wei Chang, and Yizhou Sun. 2020. GPT-GNN: Generative Pre-Training of Graph Neural Networks. In The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event, CA, USA, August 23-27, 2020. ACM, 1857-1867.
[34] Jin Huang and Charles X. Ling. 2005. Using AUC and Accuracy in Evaluating Learning Algorithms. IEEE Trans. Knowl. Data Eng. 17, 3 (2005), 299-310.
[35] Xuanwen Huang, Kaiqiao Han, Yang Yang, Dezheng Bao, Quanjin Tao, Ziwei Chai, and Qi Zhu. 2024. Can GNN be Good Adapter for LLMs?. In Proceedings of the ACM on Web Conference 2024. 893-904.
[36] Xunqiang Jiang, Tianrui Jia, Yuan Fang, Chuan Shi, Zhe Lin, and Hui Wang. 2021. Pre-training on Large-Scale Heterogeneous Graph. In The 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event, Singapore, August 14-18, 2021, Feida Zhu, Beng Chin Ooi, and Chunyan Miao (Eds.). ACM, $756-766$.
[37] Wei Ju, Zheng Fang, Yiyang Gu, Zequn Liu, Qingqing Long, Ziyue Qiao, Yifang Qin, Jianhao Shen, Fang Sun, Zhiping Xiao, Junwei Tang, Jingyang Yuan, Yusheng Zhao, Yifan Wang, Xiao Luo, and Ming Zhang. 2024. A Comprehensive Survey on Deep Graph Representation Learning. Neural Networks 173 (2024), 106207.
[38] Sanjay Kalra, Jagat Jyoti Mukherjee, Subramanian Venkataraman, Ganapathi Bantwal, Shehla Shaikh, Banshi Saboo, Ashok Kumar Das, and Ambady Ramachandran. 2013. Hypoglycemia: The neglected complication. Indian journal of endocrinology and metabolism 17, 3 (2013), 819-834.
[39] Diederik P. Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Optimization. In 3rd International Conference on Learning Representations, Yoshua Bengio and Yann LeCun (Eds.).
[40] Thomas N. Kipf and Max Welling. 2017. Semi-Supervised Classification with Graph Convolutional Networks. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings.
[41] Adrian Kochsiek and Rainer Gemulla. 2021. Parallel training of knowledge graph embedding models: a comparison of techniques. Proceedings of the VLDB Endowment 15, 3 (2021), 633-645.
[42] Oded Langer, Karla Damus, Mitchell Maiman, Michael Divon, Judith Levy, and William Bauman. 1986. A link between relative hypoglycemia-hypoinsulinemia during oral glucose tolerance tests and intrauterine growth retardation. American journal of obstetrics and gynecology 155, 4 (1986), 711-716.
[43] C. Y. Lee. 1961. An Algorithm for Path Connections and Its Applications. IRE Trans. Electron. Comput. 10, 3 (1961), 346-365.
[44] Jure Leskovec and Julian Mcauley. 2012. Learning to discover social circles in ego networks. Advances in neural information processing systems 25 (2012).
[45] Guoliang Li, Beng Chin Ooi, Jianhua Feng, Jianyong Wang, and Lizhu Zhou. 2008. Ease: an effective 3-in-1 keyword search method for unstructured, semistructured and structured data. In Proceedings of the 2008 ACM SIGMOD international conference on Management of data. 903-914.
[46] Pan Li, Yanbang Wang, Hongwei Wang, and Jure Leskovec. 2020. Distance encoding: Design provably more powerful neural networks for graph representation learning. Advances in Neural Information Processing Systems 35 (2020), $4465-4478$.
[47] Fengfei Li, Tong Zhang, Wensjing Wei, Rong Zhu, Bolin Ding, Jingren Zhou, Shuxian Hu, and Hua Lu. 2025. GRELA: Exploiting graph representation learning in effective approximate query processing. The VLDB Journal 34, 3 (2025), 35.
[48] Yichuan Li, Kaize Ding, and Kyumin Lee. 2023. GRENADE: Graph-Centric Language Model for Self-Supervised Representation Learning on Text-Attributed Graphs. In Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023. Association for Computational Linguistics. 2745-2757.
[49] Yuhan Li, Zhixun Li, Peisong Wang, Jia Li, Xiangguo Sun, Hong Cheng, and Jeffrey Xu Yu. 2023. A survey of graph meets large language model: Progress and future directions. arXiv preprint arXiv:2311.12399 (2023).
[50] Yuhan Li, Peisong Wang, Xiao Zhu, Aochuan Chen, Haiyun Jiang, Deng Cai, Victor Wai Kim Chan, and Jia Li. 2024. GLBench: A Comprehensive Benchmark for Graph with Large Language Models. arXiv preprint arXiv:2407.07457 (2024).
[51] Yiran Li, Renchi Yang, and Jieming Shi. [n.d.]. Efficient and Effective Attributed Hypergraph Clustering via K-Nearest Neighbor Augmentation. Proc. ACM Manag. Data 1, 2 ([n. d.]), 116:1-116:23.
[52] Yuhan et al. Li. 2024. Zeroq: Investigating cross-dataset zero-shot transferability in graphs. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 1725-1735.</p>
<p>[53] Hao Liu, Jiarui Feng, Lecheng Kong, Ningyue Liang, Dacheng Tao, Yixin Chen, and Muhan Zhang. 2024. One For All: Towards Training One Graph Model For All Classification Tasks. In The Twelfth International Conference on Learning Representations.
[54] Xiao Liu, Fanjie Zhang, Zhenyu Hou, Li Mian, Zhaoyu Wang, Jing Zhang, and Jie Tang. 2021. Self-supervised learning: Generative or contrastive. IEEE transactions on knowledge and data engineering 35, 1 (2021), 857-876.
[55] Yixin Liu, Ming Jiu, Shirui Fan, Chuan Zhou, Yu Zheng, Feng Xia, and S Yu Philip. 2022. Graph self-supervised learning: A survey. IEEE transactions on knowledge and data engineering 35, 6 (2022), 5879-5900.
[56] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692 (2019).
[57] Xiao Luo, Wei Ju, Yiyang Gu, Zhengyang Mao, Luchen Liu, Yuhui Yuan, and Ming Zhang. 2024. Self-supervised Graph-level Representation Learning with Adversarial Contrastive Learning. ACM Trans. Knowl. Discov. Data 18, 2 (2024), 34:1-34:23.
[58] Andrew Kachites McCallum, Kamal Nigam, Jason Rennie, and Kristie Seymore. 2000. Automating the Construction of Internet Portals with Machine Learning. Inf. Retr. 3, 2 (2000), 127-163.
[59] Kurt Mehlhorn. 1988. A faster approximation algorithm for the Steiner problem in graphs. Inform. Process. Lett. 27, 3 (1988), 125-128.
[60] Edward S Ogata, Mary E Bussey, Andrew Labarbera, and Sandra Finley. 1985. Altered growth, hypoglycemia, hypoalminemia, and ketonemia in the young rat: postnatal consequences of intrauterine growth retardation. Pediatric research 19, 1 (1985), 32-37.
[61] OpenAI. 2023. GPT-4 Technical Report. CoRR abs/2303.08774 (2023). arXiv:2303.08774
[62] James Jie Pan, Jianguo Wang, and Guoliang Li. 2024. Survey of vector database management systems. The VLDB Journal 33, 5 (2024), 1591-1615.
[63] Nils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. 3982-3992.
[64] James Requeima, John F Bronskill, Dami Choi, Richard E Turner, and David Duvenaud. 2024. LLM Processes: Numerical Predictive Distributions Conditioned on Natural Language. In ICML 2024 Workshop on In-Context Learning.
[65] Pritbviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Gallagher, and Tina Eliassi-Rad. 2008. Collective classification in network data. AI magazine 29, 5 (2008), 93-93.
[66] Weeibo Shang and Xin Huang. 2024. A Survey of Large Language Models on Generative Graph Analytics: Query, Learning, and Applications. arXiv preprint arXiv:2404.14809 (2024).
[67] Jun Su and Li Wang. 2012. Research advances in neonatal hypoglycemic brain injury. Translational pediatrics 1, 2 (2012), 108.
[68] Pang-Ning Tan, Michael S. Steinbach, and Vipin Kumar. 2005. Introduction to Data Mining. Addison-Wesley.
[69] Yanchao Tan, Zihao Zhou, Hang Lv, Weiming Liu, and Carl Yang. 2023. WalkLM: A Uniform Language Model Fine-tuning Framework for Attributed Graph Embedding. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems.
[70] Jiabin Tang, Yuhao Yang, Wei Wei, Lei Shi, Lixin Su, Suqi Cheng, Dawei Yin, and Chao Huang. 2024. Graphgpt: Graph instruction tuning for large language models. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval. 491-500.
[71] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 (2023).
[72] Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. 2017. Graph attention networks. arXiv preprint arXiv:1710.10903 (2017).
[73] Haixun Wang and Charu C Aggarwal. 2010. A survey of algorithms for keyword search on graph data. Managing and mining graph data (2010), 249-273.
[74] Xiao Wang, Deyu Bo, Chuan Shi, Shaohua Fan, Yanfang Ye, and Philip S. Yu. 2023. A Survey on Heterogeneous Graph Embedding: Methods, Techniques, Applications and Sources. IEEE Trans. Big Data 9, 2 (2023), 415-436.
[75] Yujie Wang, Hu Zhang, Jiye Liang, and Ru Li. 2023. Dynamic HeterogeneousGraph Reasoning with Language Models and Knowledge Representation Learning for Commonsense Question Answering. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, 14048-14063.
[76] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and Philip S. Yu. 2021. A Comprehensive Survey on Graph Neural Networks. IEEE Trans. Neural Networks Learn. Syst. 32, 1 (2021), 4-24.
[77] Zhenqin Wu, Bharath Ramsundar, Evan N Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S Pappu, Karl Leswing, and Vijay Pande. 2018. MoleculeNet:
a benchmark for molecular machine learning. Chemical science 9, 2 (2018), $513-530$.
[78] Yaochen Xie, Zhao Xu, Jingtun Zhang, Zhengyang Wang, and Shuiwang Ji. 2022. Self-supervised learning of graph neural networks: A unified review. IEEE transactions on pattern analysis and machine intelligence 45, 2 (2022), 2412-2429.
[79] Yasin Abbasi Yadkori, Ilja Kuzborskij, Andrǎo György, and Csaba Szepesvàri. 2024. To Believe or Not to Believe Your LLM. arXiv preprint arXiv:2406.02543 (2024).
[80] Carl Yang, Yuxin Xiao, Yu Zhang, Yizhou Sun, and Jiawei Han. 2022. Heterogeneous Network Representation Learning: A Unified Framework With Survey and Benchmark. IEEE Trans. Knowl. Data Eng. 34, 10 (2022), 4854-4873.
[81] Jianye Yang, Wu Yao, and Wenjie Zhang. 2021. Keyword Search on Large Graphs: A Survey. Data Sci. Eng. 6, 2 (2021), 142-162.
[82] Renchi Yang, Jieming Shi, Xiaohui Xiao, Yin Yang, Juncheng Liu, Sourav S Bhowmick, et al. 2020. Scaling attributed network embedding to massive graphs. Proceedings of the VLDB Endowment 14, 1 (2020), 37-49.
[83] Ruosong Ye, Caiqi Zhang, Ruzhui Wang, Shuyuan Xu, and Yongfeng Zhang. 2024. Language is all a graph needs. In Findings of the Association for Computational Linguistics. EACL 2024. 1955-1973.
[84] Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and Yang Shen. 2020. Graph contrastive learning with augmentations. Advances in Neural Information Processing Systems 33 (2020), 5812-5823.
[85] Biao Zhang, Barry Haddow, and Alexandra Birch. 2023. Prompting large language model for machine translation: A case study. In International Conference on Machine Learning. PMLR, 41092-41110.
[86] Mengmei Zhang, Mingwei Sun, Feng Wang, Shen Fan, Yanhu Mo, Xiaoxiao Xu, Hong Liu, Cheng Yang, and Chuan Shi. 2024. Graphtranslator: Aligning graph model to large language model for open-ended tasks. In Proceedings of the ACM Web Conference 2024. 1003-1014.
[87] Rui Zhang, Bayu Distiawan Triwolya, Miao Li, Yong Jiang, and Jianzhong Qi. 2022. A benchmark and comprehensive survey on knowledge graph entity alignment via representation learning. The VLDB Journal 31, 5 (2022), 1143-1168.
[88] Zhilu Zhang and Mert Sabuncu. 2018. Generalized cross entropy loss for training deep neural networks with noisy labels. Advances in Neural Information Processing Systems 31 (2018).
[89] Jie Zhou, Ganqu Cui, Shengding Hu, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng Wang, Changcheng Li, and Maosong Sun. 2020. Graph neural networks: A review of methods and applications. AI Open 1 (2020), 57-81.
[90] Xuanhe Zhou, Ji Sun, Guoliang Li, and Jianhua Feng. 2020. Query performance prediction for concurrent queries using graph embedding. Proceedings of the VLDB Endowment 15, 9 (2020), 1416-1428.
[91] Yanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang. 2020. Deep graph contrastive representation learning. arXiv preprint arXiv:2006.04131 (2020).
[92] Yanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang. 2021. Graph contrastive learning with adaptive augmentation. In Proceedings of the web conference 2021. 2069-2080.
[93] Yuchen Zhuang, Yue Yu, Kuan Wang, Haotian Sun, and Chao Zhang. 2024. Toolqa: A dataset for lfm question answering with external tools. Advances in Neural Information Processing Systems 36 (2024).</p>            </div>
        </div>

    </div>
</body>
</html>
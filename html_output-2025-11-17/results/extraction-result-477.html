<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-477 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-477</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-477</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-17.html">extraction-schema-17</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <p><strong>Paper ID:</strong> paper-258947120</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2305.16334v1.pdf" target="_blank">OlaGPT: Empowering LLMs With Human-like Problem-Solving Abilities</a></p>
                <p><strong>Paper Abstract:</strong> In most current research, large language models (LLMs) are able to perform reasoning tasks by generating chains of thought through the guidance of specific prompts. However, there still exists a significant discrepancy between their capability in solving complex reasoning problems and that of humans. At present, most approaches focus on chains of thought (COT) and tool use, without considering the adoption and application of human cognitive frameworks. It is well-known that when confronting complex reasoning challenges, humans typically employ various cognitive abilities, and necessitate interaction with all aspects of tools, knowledge, and the external environment information to accomplish intricate tasks. This paper introduces a novel intelligent framework, referred to as OlaGPT. OlaGPT carefully studied a cognitive architecture framework, and propose to simulate certain aspects of human cognition. The framework involves approximating different cognitive modules, including attention, memory, reasoning, learning, and corresponding scheduling and decision-making mechanisms. Inspired by the active learning mechanism of human beings, it proposes a learning unit to record previous mistakes and expert opinions, and dynamically refer to them to strengthen their ability to solve similar problems. The paper also outlines common effective reasoning frameworks for human problem-solving and designs Chain-of-Thought (COT) templates accordingly. A comprehensive decision-making mechanism is also proposed to maximize model accuracy. The efficacy of OlaGPT has been stringently evaluated on multiple reasoning datasets, and the experimental outcomes reveal that OlaGPT surpasses state-of-the-art benchmarks, demonstrating its superior performance. Our implementation of OlaGPT is available on GitHub: \url{https://github.com/oladata-team/OlaGPT}.</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e477.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e477.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OlaGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OlaGPT (Human-like Problem-Solving Framework for LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A modular framework that augments frozen LLMs with human-inspired cognitive modules (Intention Enhance, Memory, Active Learning, Reasoning, Controller, Voting) to combine template-driven symbolic reasoning/examples with neural chain-of-thought generation and retrieval-based memory.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>OlaGPT</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>OlaGPT is a hybrid reasoning system introduced in this paper that assembles declarative artifacts (fact/tool/notes/thinking libraries stored in a Faiss vector DB, human-authored thinking templates and example 'notes' for hard cases) with imperative neural components (frozen LLMs, e.g., GPT-3.5-turbo used to execute templates and generate chain-of-thought). The architecture is modular: Intention Enhance (input rewriting/attention), Memory (short-term/lang-term vector stores for facts/tools/notes/thinking), Active Learning (identify recurring failure cases and store expert-corrected notes), Reasoning (multiple human-inspired thinking templates instantiated as agents), Controller (retrieval and dynamic selection/integration of libraries into template agents), and Voting (ensemble aggregation either by regex majority or by an LLM voting procedure). The system is not end-to-end trained; integration is achieved at the prompting/retrieval/control level, with asynchronous execution of multiple template-agents and downstream ensemble selection.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>External indexed libraries (Faiss vector DB) containing: (1) Facts library (factual knowledge, common-sense), (2) Tools library (APIs, calculators — text I/O), (3) Notes library (expert-corrected stepwise solutions for hard cases stored as structured JSON examples), (4) Thinking library (human-authored problem-solving templates / Chain-of-Thought templates). These act as symbolic / rule-like declarative elements (few-shot examples, template prefixes, and structured notes).</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Frozen large language models (neural sequence models; experiments used GPT-3.5-turbo) that generate chain-of-thought, produce answers, and execute template agents; retrieval via Sentence-BERT embeddings and Faiss; prompt-engineered programmatic orchestration (controller) that dynamically retrieves and injects declarative content into prompts — overall procedural execution is orchestrated but not learned end-to-end.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Modular, retrieval-then-invoke prompting pipeline: the Controller retrieves relevant declarative artifacts (facts, notes, tools, thinking templates) and fills template prompts to spawn multiple LLM 'agents' which run asynchronously to produce candidate reasoning chains and answers; outputs are aggregated by a Voting module (regex majority or LLM-based voting). Active Learning updates the notes library (declarative store) from identified hard cases. Integration is achieved via prompt templates + dynamic retrieval (Faiss) rather than gradient-based joint training or differentiable symbolic layers.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Improved multi-path reasoning via ensemble of diverse thinking templates; active correction and faster adaptation on recurring hard cases (learning-from-mistakes) without retraining; increased robustness and accuracy through voting across heterogeneous reasoning styles; better use of external (long-term) knowledge via memory retrieval; apparent human-like flexibility in selecting different reasoning strategies per question.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Mathematical word problems (AQuA) and knowledge-intensive analogical reasoning (E-KAR Chinese), both multiple-choice reasoning datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td>Best reported OlaGPT results: AQuA accuracy up to 0.7047 (OlaGPT-llm-vote, reported best configuration) and 0.6772 (OlaGPT-regex-vote); E-KAR (Chinese) best reported: 0.4716 (OlaGPT-regex-vote) and 0.4507 (OlaGPT-llm-vote). (Accuracy metrics as reported in Table 3 of the paper.)</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td>GPT-3.5-turbo baseline (imperative-only, zero-shot) reported roughly ~0.3228 on AQuA and ~0.3762 on E-KAR (Chinese) in the paper's baseline rows (these are the base LLM zero-shot accuracies reported).</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Qualitatively improved robustness and cross-template coverage: ensemble of templates reduces variance and increases upper-bound accuracy on held-out test sets; Active Learning (notes) improves performance on classes of problems where the LLM previously failed repeatedly. The paper does not provide explicit out-of-distribution or formal compositional-generalization tests, so claims about OOD generalization are qualitative rather than quantitatively proven.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Improved interpretability relative to black-box LLM output because (1) declarative notes contain stepwise expert solutions that can be inspected; (2) thinking templates elicit Chain-of-Thought rationales; (3) Voting via llm-vote produces rationales for its choice. However, the LLM-generated rationales are still model outputs and not guaranteed to be faithful explanations; the notes library provides the most concrete human-interpretable artifacts.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Limitations noted in the paper: (1) Tool library integration was implemented but not exercised due to dataset/tool limitations; (2) Notes retrieval can be token-limited (too many examples cannot be injected); (3) Question-type detection and templating can be coarse (e.g., analogy templates less customizable), yielding suboptimal template matches; (4) Ensemble regex voting can select a majority incorrect answer if most templates err similarly; (5) Some modules (e.g., human-feedback submodule, verification thinking, integrative DIY mode) were not used in experiments; (6) sensitivity to number and retrieval strategy of notes/examples (hyperparameter tuning required).</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Grounded in cognitive-architecture analogies: the framework is explicitly motivated by human cognitive modules (attention, memory, learning, reasoning, action selection). The proposed explanatory principle is a division-of-labor / complementarity argument: declarative stores + symbolic templates provide stable, inspectable problem schemas and corrective examples, while imperative neural LLMs provide open-ended generative and stepwise reasoning; combining them via modular retrieval and ensemble voting yields better performance than either approach alone.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e477.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e477.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DUAL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DUAL (Distributed Unit for Assembling Learning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid cognitive architecture that aims to balance symbolic processing and connectionist (neural) approaches via a central executive and multiple collateral processing units.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>DUAL (Distributed Unit for Assembling Learning)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Mentioned in the related-work section as an exemplar hybrid cognitive architecture that combines symbolic and connectionist processing via a central executive agent plus multiple collateral units to address diverse cognitive tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Symbolic processing elements (production-like units and structured representations) as described in the brief related-work summary.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Connectionist/parallel distributed processing (neural-like) collateral units providing graded/associative computation.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Central executive that coordinates multiple collateral units (a hybrid of symbolic control plus distributed subunits) — described at high level in the paper; no implementation details provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Claimed in the literature (and referenced) to provide a balance between symbolic expressivity and neural robustness; the paper only mentions its hybrid nature without giving empirical emergent properties.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Paper does not report empirical generalization properties for DUAL; only cited as a conceptual hybrid architecture.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Not detailed in this paper; historically hybrid architectures like DUAL aim to retain interpretable symbolic structure plus distributed representations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Described as a hybrid architecture that seeks to combine strengths of symbolic production systems and connectionist networks via a central executive — cited as part of the cognitive-architecture literature motivating OlaGPT.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e477.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e477.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SOAR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SOAR (Soar cognitive architecture)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A production-system-based cognitive architecture integrating problem solving, learning, and knowledge representation in a unified system.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Soar: An architecture for general intelligence</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SOAR</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Referenced as an earlier cognitive architecture that used production rules (condition-action) to represent reasoning and integrated learning and problem solving within a single system.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Production rules / condition-action rule system (symbolic rule-based representation).</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Procedural control and mechanisms for learning and decision making (procedural modules and control policies); historically also supports mechanisms like chunking for learning.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Unified production-system framework where procedural decision-making invokes rule firings and learning updates (conceptual integrated symbolic system rather than neural differentiable integration).</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Unified problem solving and learning behaviors emergent from iterative rule application and chunking; paper only cites SOAR conceptually and does not present empirical measures.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Not quantified in this paper; SOAR historically aimed for generality across cognitive tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Production rules give inspectable, symbolic traces of reasoning, aiding interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Not discussed in the current paper.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Production-system theory: cognition as rule firing, integrated with procedural learning mechanisms.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e477.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e477.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ACT-R</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ACT-R (Adaptive Control of Thought—Rational)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A symbolic cognitive architecture emphasizing modular memory processes (declarative and procedural) and rule-based symbolic processing used widely to model human cognition.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>How can the human mind occur in the physical universe?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ACT-R</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Mentioned as an influential cognitive architecture that emphasizes symbolic processing and memory processes (declarative/working memory) and has been applied to modeling a range of cognitive tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Declarative memory of chunks (symbolic structured representations) and production rules (symbolic procedural knowledge).</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Procedural module that executes productions, and mechanisms for activation/buffering that produce stepwise behavior; historically not connectionist, though some hybrid variants exist.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Symbolic modules interacting through buffers and production rules; integration is architectural via module interfaces rather than end-to-end learning.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Predictive cognitive modeling of human reaction times and error patterns; paper only cites ACT-R conceptually.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>High interpretability via explicit symbolic chunks and production traces.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Symbolic cognitive modeling with explicit separation of declarative and procedural memory; used as conceptual inspiration.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e477.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e477.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sigma</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sigma cognitive architecture</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cognitive architecture composed of perception, working memory, long-term memory, production memory, decision networks and learning mechanisms, cited as a modern cognitive-architecture unification effort.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The Sigma cognitive architecture and system: Towards functionally elegant grand unification.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Sigma</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Cited as an example cognitive architecture that includes multiple components (perception, working memory, long-term memory, production memory, subgoals, decision networks, learning) and thus acts as a blueprint for modular cognitive systems.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Production memory and structured long-term memory (symbolic forms indicated in the description).</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Decision network and perception modules that implement procedural computations; learning mechanisms integrate information over time.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Modular architecture combining symbolic memories with decision networks and learning subsystems — described at high level in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Not empirically discussed here; cited as having broad cognitive capabilities via modular integration.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Not quantified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Modular design supports inspectability of subcomponents; specific interpretability properties not given in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Architectural unification of multiple cognitive components to obtain broad cognitive functionality.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e477.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e477.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PAL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PAL (Program-Aided Language Models / Program-Aided Language models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A paradigm that decomposes symbolic, mathematical or algorithmic reasoning tasks into intermediate steps expressed as executable programs (e.g., Python) generated by an LLM, then executed to yield exact outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>PAL: Program-aided Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>PAL (Program-Aided Language Models)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Mentioned as a hybrid approach where LLMs are prompted to synthesize small programs (imperative code) representing reasoning steps; the programs are executed to produce exact symbolic results (e.g., arithmetic). The architecture couples neural generation with program execution as an external symbolic/procedural engine.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Executable program fragments (Python code) and the program execution environment act as a symbolic/procedural declarative artifact that ensures deterministic computation.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Neural LLM that generates the program (sequence model) and produces intermediate natural-language reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Generate-then-execute: the LLM generates code representing reasoning steps which are executed by a separate runtime; results are fed back into prompts; integration is modular (not end-to-end differentiable).</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Allows exact arithmetic and algorithmic correctness beyond raw language-model token prediction; reduces arithmetic and symbolic errors of LLMs by offloading deterministic computation to a program executor.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Symbolic and mathematical reasoning tasks (e.g., algebraic word problems); PAL cited as relevant prior work rather than evaluated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Paper cites PAL as improving robustness on arithmetic/symbolic reasoning but does not provide cross-task generalization metrics here.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Program code is inspectable and provides an interpretable, executable trace of the solution.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Not discussed in detail in this paper; typical limitations (from cited literature) include brittleness of generated code and dependence on reliable execution environment.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Division of labor: neural generation of high-level reasoning/programs, deterministic execution by symbolic runtime.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e477.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e477.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Toolformer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Toolformer (Language models can teach themselves to use tools)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework that trains a model to decide when to call external APIs/tools and how to incorporate the results into token prediction, effectively combining model generation and tool-use.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Toolformer: Language models can teach themselves to use tools</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Toolformer</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Mentioned as related work: Toolformer is an approach where models are trained (or prompted) to call tools/APIs at appropriate times and incorporate the returned outputs into the generation process, combining neural sequence modeling with external procedural tool execution.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Tool APIs and their specifications (explicit, structured interfaces); not a symbolic knowledge base per se but external procedural interfaces that can be considered declarative resources (available actions).</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Neural language model that learns when/how to call tools and uses tool outputs within its generation pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Model-mediated API call insertion and post-processing integration; Toolformer trains the model to annotate when to call tools and then uses those calls during generation (modular integration).</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Improved factuality and ability to perform tasks that require external computation or retrieval by delegating to tools; not evaluated in this paper beyond citation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Not discussed here; cited as enabling dynamic access to external capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Calls to tools produce explicit action traces that are inspectable.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Tool-use as modular augmentation of neural generation; combination of action selection and language modeling.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e477.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e477.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ART</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ART (Automatic Reasoning and Tool-use)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework that uses frozen LLMs to automatically generate intermediate reasoning steps as programs and select demonstrations of multi-step reasoning and tool use from a task library.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ART (Automatic Reasoning and Tool-use)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Mentioned in related work as a framework that leverages frozen LLMs to produce intermediate steps expressed as programs and to select demonstrations from a library, coupling programmatic intermediate representations with neural generation.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Programmatic intermediate representations and a task/demo library used as external structured resources.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Frozen LLMs that generate the intermediate programs and reasoning chains.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Modular: LLMs generate program-like intermediate steps which can be executed or used as structured demonstrations; selection from a task library provides declarative exemplars.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Enables automatic composition of reasoning and tool usage patterns; claimed to help leverage external modules without fine-tuning, but the current paper only cites ART conceptually.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Intermediate programmatic steps increase traceability compared to raw text outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Program-as-intermediate-representation paradigm coupling reasoning and action.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e477.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e477.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>REACT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>REACT (Synergizing reasoning and acting in language models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that combines reasoning (chain-of-thought) and action (tool or environment interaction) to improve language-model performance on reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>React: Synergizing reasoning and acting in language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>REACT</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Cited as related work exploring the synergy of reasoning and acting: combine chain-of-thought style reasoning with calls to tools or environment interactions so that the model alternates between internal reasoning and external action.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>External tool/environment interfaces and (optionally) structured buffers of observations (procedural/declarative resources).</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Neural LLM performing interleaved reasoning and action steps.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Interleaving LLM-generated reasoning steps with executed actions; modular orchestration rather than end-to-end differentiable fusion.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Enables dynamic problem-solving that leverages external actions to support internal reasoning; paper only cites the approach without experimental details.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Action traces and stepwise reasoning provide inspectable intermediate states.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Interleaving of reasoning and acting as a mechanism for richer problem solving.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e477.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e477.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>STaR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>STaR (Self-Taught Reasoner / STaR: Bootstrapping Reasoning With Reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A bootstrapping approach where generated rationales that yield correct answers are collected and used for further fine-tuning, effectively bootstrapping reasoning capability from generated explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>STaR: Bootstrapping Reasoning With Reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>STaR</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Cited as a prior technique that loops: generate rationales and answers, if some rationales yield correct answers then use those rationales as supervision to fine-tune or improve the model. This couples neural generation with an iterative symbolic/structured supervision process.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Collected rationales and high-quality reasoning traces used as (symbolic-like) supervision examples.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Neural LLM generating rationales and answers; optionally fine-tuning steps.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Iterative dataset augmentation: use model-generated structured rationales that produce correct outputs as additional training data (supervised fine-tuning in some implementations).</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Bootstrapped improvement in reasoning accuracy by harvesting correct chain-of-thought traces; referenced in paper as related work without experimental numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Not quantified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Generated rationales provide explanatory artifacts that can be inspected and reused.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Bootstrapping by iterative harvesting of correct reasoning traces to provide structured supervision.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e477.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e477.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KokinovHybridAnalogy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>A hybrid model of reasoning by analogy (Boicho Kokinov, 1994)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited classic work describing a hybrid architecture for analogy reasoning that mixes connectionist and symbolic mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A hybrid model of reasoning by analogy</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Hybrid analogy model (Kokinov)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Cited as historical prior work demonstrating how analogy can be modeled by a combination of symbolic mapping and connectionist computations; referenced to motivate hybrid approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Symbolic mappings/relational structures used in analogy mapping.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Connectionist associative computations supporting retrieval and mapping processes.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Hybrid coupling of symbolic mapping with distributed, associative processes (historical literature — not implemented here).</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Provided a model of analogy that leverages the strengths of both paradigms; paper only cites this work.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Symbolic mappings are interpretable; connectionist components less so.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Hybrid symbolic-connectionist theory for analogy reasoning.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>A hybrid model of reasoning by analogy <em>(Rating: 2)</em></li>
                <li>Soar: An architecture for general intelligence <em>(Rating: 2)</em></li>
                <li>The Soar cognitive architecture <em>(Rating: 2)</em></li>
                <li>How can the human mind occur in the physical universe? <em>(Rating: 2)</em></li>
                <li>The atomic components of thought <em>(Rating: 2)</em></li>
                <li>The Sigma cognitive architecture and system: Towards functionally elegant grand unification. <em>(Rating: 2)</em></li>
                <li>PAL: Program-aided Language Models <em>(Rating: 2)</em></li>
                <li>Toolformer: Language models can teach themselves to use tools <em>(Rating: 2)</em></li>
                <li>React: Synergizing reasoning and acting in language models <em>(Rating: 2)</em></li>
                <li>STaR: Bootstrapping Reasoning With Reasoning <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-477",
    "paper_id": "paper-258947120",
    "extraction_schema_id": "extraction-schema-17",
    "extracted_data": [
        {
            "name_short": "OlaGPT",
            "name_full": "OlaGPT (Human-like Problem-Solving Framework for LLMs)",
            "brief_description": "A modular framework that augments frozen LLMs with human-inspired cognitive modules (Intention Enhance, Memory, Active Learning, Reasoning, Controller, Voting) to combine template-driven symbolic reasoning/examples with neural chain-of-thought generation and retrieval-based memory.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "OlaGPT",
            "system_description": "OlaGPT is a hybrid reasoning system introduced in this paper that assembles declarative artifacts (fact/tool/notes/thinking libraries stored in a Faiss vector DB, human-authored thinking templates and example 'notes' for hard cases) with imperative neural components (frozen LLMs, e.g., GPT-3.5-turbo used to execute templates and generate chain-of-thought). The architecture is modular: Intention Enhance (input rewriting/attention), Memory (short-term/lang-term vector stores for facts/tools/notes/thinking), Active Learning (identify recurring failure cases and store expert-corrected notes), Reasoning (multiple human-inspired thinking templates instantiated as agents), Controller (retrieval and dynamic selection/integration of libraries into template agents), and Voting (ensemble aggregation either by regex majority or by an LLM voting procedure). The system is not end-to-end trained; integration is achieved at the prompting/retrieval/control level, with asynchronous execution of multiple template-agents and downstream ensemble selection.",
            "declarative_component": "External indexed libraries (Faiss vector DB) containing: (1) Facts library (factual knowledge, common-sense), (2) Tools library (APIs, calculators — text I/O), (3) Notes library (expert-corrected stepwise solutions for hard cases stored as structured JSON examples), (4) Thinking library (human-authored problem-solving templates / Chain-of-Thought templates). These act as symbolic / rule-like declarative elements (few-shot examples, template prefixes, and structured notes).",
            "imperative_component": "Frozen large language models (neural sequence models; experiments used GPT-3.5-turbo) that generate chain-of-thought, produce answers, and execute template agents; retrieval via Sentence-BERT embeddings and Faiss; prompt-engineered programmatic orchestration (controller) that dynamically retrieves and injects declarative content into prompts — overall procedural execution is orchestrated but not learned end-to-end.",
            "integration_method": "Modular, retrieval-then-invoke prompting pipeline: the Controller retrieves relevant declarative artifacts (facts, notes, tools, thinking templates) and fills template prompts to spawn multiple LLM 'agents' which run asynchronously to produce candidate reasoning chains and answers; outputs are aggregated by a Voting module (regex majority or LLM-based voting). Active Learning updates the notes library (declarative store) from identified hard cases. Integration is achieved via prompt templates + dynamic retrieval (Faiss) rather than gradient-based joint training or differentiable symbolic layers.",
            "emergent_properties": "Improved multi-path reasoning via ensemble of diverse thinking templates; active correction and faster adaptation on recurring hard cases (learning-from-mistakes) without retraining; increased robustness and accuracy through voting across heterogeneous reasoning styles; better use of external (long-term) knowledge via memory retrieval; apparent human-like flexibility in selecting different reasoning strategies per question.",
            "task_or_benchmark": "Mathematical word problems (AQuA) and knowledge-intensive analogical reasoning (E-KAR Chinese), both multiple-choice reasoning datasets.",
            "hybrid_performance": "Best reported OlaGPT results: AQuA accuracy up to 0.7047 (OlaGPT-llm-vote, reported best configuration) and 0.6772 (OlaGPT-regex-vote); E-KAR (Chinese) best reported: 0.4716 (OlaGPT-regex-vote) and 0.4507 (OlaGPT-llm-vote). (Accuracy metrics as reported in Table 3 of the paper.)",
            "declarative_only_performance": null,
            "imperative_only_performance": "GPT-3.5-turbo baseline (imperative-only, zero-shot) reported roughly ~0.3228 on AQuA and ~0.3762 on E-KAR (Chinese) in the paper's baseline rows (these are the base LLM zero-shot accuracies reported).",
            "has_comparative_results": true,
            "generalization_properties": "Qualitatively improved robustness and cross-template coverage: ensemble of templates reduces variance and increases upper-bound accuracy on held-out test sets; Active Learning (notes) improves performance on classes of problems where the LLM previously failed repeatedly. The paper does not provide explicit out-of-distribution or formal compositional-generalization tests, so claims about OOD generalization are qualitative rather than quantitatively proven.",
            "interpretability_properties": "Improved interpretability relative to black-box LLM output because (1) declarative notes contain stepwise expert solutions that can be inspected; (2) thinking templates elicit Chain-of-Thought rationales; (3) Voting via llm-vote produces rationales for its choice. However, the LLM-generated rationales are still model outputs and not guaranteed to be faithful explanations; the notes library provides the most concrete human-interpretable artifacts.",
            "limitations_or_failures": "Limitations noted in the paper: (1) Tool library integration was implemented but not exercised due to dataset/tool limitations; (2) Notes retrieval can be token-limited (too many examples cannot be injected); (3) Question-type detection and templating can be coarse (e.g., analogy templates less customizable), yielding suboptimal template matches; (4) Ensemble regex voting can select a majority incorrect answer if most templates err similarly; (5) Some modules (e.g., human-feedback submodule, verification thinking, integrative DIY mode) were not used in experiments; (6) sensitivity to number and retrieval strategy of notes/examples (hyperparameter tuning required).",
            "theoretical_framework": "Grounded in cognitive-architecture analogies: the framework is explicitly motivated by human cognitive modules (attention, memory, learning, reasoning, action selection). The proposed explanatory principle is a division-of-labor / complementarity argument: declarative stores + symbolic templates provide stable, inspectable problem schemas and corrective examples, while imperative neural LLMs provide open-ended generative and stepwise reasoning; combining them via modular retrieval and ensemble voting yields better performance than either approach alone.",
            "uuid": "e477.0"
        },
        {
            "name_short": "DUAL",
            "name_full": "DUAL (Distributed Unit for Assembling Learning)",
            "brief_description": "A hybrid cognitive architecture that aims to balance symbolic processing and connectionist (neural) approaches via a central executive and multiple collateral processing units.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "DUAL (Distributed Unit for Assembling Learning)",
            "system_description": "Mentioned in the related-work section as an exemplar hybrid cognitive architecture that combines symbolic and connectionist processing via a central executive agent plus multiple collateral units to address diverse cognitive tasks.",
            "declarative_component": "Symbolic processing elements (production-like units and structured representations) as described in the brief related-work summary.",
            "imperative_component": "Connectionist/parallel distributed processing (neural-like) collateral units providing graded/associative computation.",
            "integration_method": "Central executive that coordinates multiple collateral units (a hybrid of symbolic control plus distributed subunits) — described at high level in the paper; no implementation details provided here.",
            "emergent_properties": "Claimed in the literature (and referenced) to provide a balance between symbolic expressivity and neural robustness; the paper only mentions its hybrid nature without giving empirical emergent properties.",
            "task_or_benchmark": null,
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Paper does not report empirical generalization properties for DUAL; only cited as a conceptual hybrid architecture.",
            "interpretability_properties": "Not detailed in this paper; historically hybrid architectures like DUAL aim to retain interpretable symbolic structure plus distributed representations.",
            "limitations_or_failures": "Not discussed in this paper.",
            "theoretical_framework": "Described as a hybrid architecture that seeks to combine strengths of symbolic production systems and connectionist networks via a central executive — cited as part of the cognitive-architecture literature motivating OlaGPT.",
            "uuid": "e477.1"
        },
        {
            "name_short": "SOAR",
            "name_full": "SOAR (Soar cognitive architecture)",
            "brief_description": "A production-system-based cognitive architecture integrating problem solving, learning, and knowledge representation in a unified system.",
            "citation_title": "Soar: An architecture for general intelligence",
            "mention_or_use": "mention",
            "system_name": "SOAR",
            "system_description": "Referenced as an earlier cognitive architecture that used production rules (condition-action) to represent reasoning and integrated learning and problem solving within a single system.",
            "declarative_component": "Production rules / condition-action rule system (symbolic rule-based representation).",
            "imperative_component": "Procedural control and mechanisms for learning and decision making (procedural modules and control policies); historically also supports mechanisms like chunking for learning.",
            "integration_method": "Unified production-system framework where procedural decision-making invokes rule firings and learning updates (conceptual integrated symbolic system rather than neural differentiable integration).",
            "emergent_properties": "Unified problem solving and learning behaviors emergent from iterative rule application and chunking; paper only cites SOAR conceptually and does not present empirical measures.",
            "task_or_benchmark": null,
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Not quantified in this paper; SOAR historically aimed for generality across cognitive tasks.",
            "interpretability_properties": "Production rules give inspectable, symbolic traces of reasoning, aiding interpretability.",
            "limitations_or_failures": "Not discussed in the current paper.",
            "theoretical_framework": "Production-system theory: cognition as rule firing, integrated with procedural learning mechanisms.",
            "uuid": "e477.2"
        },
        {
            "name_short": "ACT-R",
            "name_full": "ACT-R (Adaptive Control of Thought—Rational)",
            "brief_description": "A symbolic cognitive architecture emphasizing modular memory processes (declarative and procedural) and rule-based symbolic processing used widely to model human cognition.",
            "citation_title": "How can the human mind occur in the physical universe?",
            "mention_or_use": "mention",
            "system_name": "ACT-R",
            "system_description": "Mentioned as an influential cognitive architecture that emphasizes symbolic processing and memory processes (declarative/working memory) and has been applied to modeling a range of cognitive tasks.",
            "declarative_component": "Declarative memory of chunks (symbolic structured representations) and production rules (symbolic procedural knowledge).",
            "imperative_component": "Procedural module that executes productions, and mechanisms for activation/buffering that produce stepwise behavior; historically not connectionist, though some hybrid variants exist.",
            "integration_method": "Symbolic modules interacting through buffers and production rules; integration is architectural via module interfaces rather than end-to-end learning.",
            "emergent_properties": "Predictive cognitive modeling of human reaction times and error patterns; paper only cites ACT-R conceptually.",
            "task_or_benchmark": null,
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Not discussed in this paper.",
            "interpretability_properties": "High interpretability via explicit symbolic chunks and production traces.",
            "limitations_or_failures": "Not discussed in this paper.",
            "theoretical_framework": "Symbolic cognitive modeling with explicit separation of declarative and procedural memory; used as conceptual inspiration.",
            "uuid": "e477.3"
        },
        {
            "name_short": "Sigma",
            "name_full": "Sigma cognitive architecture",
            "brief_description": "A cognitive architecture composed of perception, working memory, long-term memory, production memory, decision networks and learning mechanisms, cited as a modern cognitive-architecture unification effort.",
            "citation_title": "The Sigma cognitive architecture and system: Towards functionally elegant grand unification.",
            "mention_or_use": "mention",
            "system_name": "Sigma",
            "system_description": "Cited as an example cognitive architecture that includes multiple components (perception, working memory, long-term memory, production memory, subgoals, decision networks, learning) and thus acts as a blueprint for modular cognitive systems.",
            "declarative_component": "Production memory and structured long-term memory (symbolic forms indicated in the description).",
            "imperative_component": "Decision network and perception modules that implement procedural computations; learning mechanisms integrate information over time.",
            "integration_method": "Modular architecture combining symbolic memories with decision networks and learning subsystems — described at high level in the paper.",
            "emergent_properties": "Not empirically discussed here; cited as having broad cognitive capabilities via modular integration.",
            "task_or_benchmark": null,
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Not quantified in this paper.",
            "interpretability_properties": "Modular design supports inspectability of subcomponents; specific interpretability properties not given in this paper.",
            "limitations_or_failures": "Not discussed in this paper.",
            "theoretical_framework": "Architectural unification of multiple cognitive components to obtain broad cognitive functionality.",
            "uuid": "e477.4"
        },
        {
            "name_short": "PAL",
            "name_full": "PAL (Program-Aided Language Models / Program-Aided Language models)",
            "brief_description": "A paradigm that decomposes symbolic, mathematical or algorithmic reasoning tasks into intermediate steps expressed as executable programs (e.g., Python) generated by an LLM, then executed to yield exact outputs.",
            "citation_title": "PAL: Program-aided Language Models",
            "mention_or_use": "mention",
            "system_name": "PAL (Program-Aided Language Models)",
            "system_description": "Mentioned as a hybrid approach where LLMs are prompted to synthesize small programs (imperative code) representing reasoning steps; the programs are executed to produce exact symbolic results (e.g., arithmetic). The architecture couples neural generation with program execution as an external symbolic/procedural engine.",
            "declarative_component": "Executable program fragments (Python code) and the program execution environment act as a symbolic/procedural declarative artifact that ensures deterministic computation.",
            "imperative_component": "Neural LLM that generates the program (sequence model) and produces intermediate natural-language reasoning.",
            "integration_method": "Generate-then-execute: the LLM generates code representing reasoning steps which are executed by a separate runtime; results are fed back into prompts; integration is modular (not end-to-end differentiable).",
            "emergent_properties": "Allows exact arithmetic and algorithmic correctness beyond raw language-model token prediction; reduces arithmetic and symbolic errors of LLMs by offloading deterministic computation to a program executor.",
            "task_or_benchmark": "Symbolic and mathematical reasoning tasks (e.g., algebraic word problems); PAL cited as relevant prior work rather than evaluated in this paper.",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Paper cites PAL as improving robustness on arithmetic/symbolic reasoning but does not provide cross-task generalization metrics here.",
            "interpretability_properties": "Program code is inspectable and provides an interpretable, executable trace of the solution.",
            "limitations_or_failures": "Not discussed in detail in this paper; typical limitations (from cited literature) include brittleness of generated code and dependence on reliable execution environment.",
            "theoretical_framework": "Division of labor: neural generation of high-level reasoning/programs, deterministic execution by symbolic runtime.",
            "uuid": "e477.5"
        },
        {
            "name_short": "Toolformer",
            "name_full": "Toolformer (Language models can teach themselves to use tools)",
            "brief_description": "A framework that trains a model to decide when to call external APIs/tools and how to incorporate the results into token prediction, effectively combining model generation and tool-use.",
            "citation_title": "Toolformer: Language models can teach themselves to use tools",
            "mention_or_use": "mention",
            "system_name": "Toolformer",
            "system_description": "Mentioned as related work: Toolformer is an approach where models are trained (or prompted) to call tools/APIs at appropriate times and incorporate the returned outputs into the generation process, combining neural sequence modeling with external procedural tool execution.",
            "declarative_component": "Tool APIs and their specifications (explicit, structured interfaces); not a symbolic knowledge base per se but external procedural interfaces that can be considered declarative resources (available actions).",
            "imperative_component": "Neural language model that learns when/how to call tools and uses tool outputs within its generation pipeline.",
            "integration_method": "Model-mediated API call insertion and post-processing integration; Toolformer trains the model to annotate when to call tools and then uses those calls during generation (modular integration).",
            "emergent_properties": "Improved factuality and ability to perform tasks that require external computation or retrieval by delegating to tools; not evaluated in this paper beyond citation.",
            "task_or_benchmark": null,
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Not discussed here; cited as enabling dynamic access to external capabilities.",
            "interpretability_properties": "Calls to tools produce explicit action traces that are inspectable.",
            "limitations_or_failures": "Not discussed in this paper.",
            "theoretical_framework": "Tool-use as modular augmentation of neural generation; combination of action selection and language modeling.",
            "uuid": "e477.6"
        },
        {
            "name_short": "ART",
            "name_full": "ART (Automatic Reasoning and Tool-use)",
            "brief_description": "A framework that uses frozen LLMs to automatically generate intermediate reasoning steps as programs and select demonstrations of multi-step reasoning and tool use from a task library.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "ART (Automatic Reasoning and Tool-use)",
            "system_description": "Mentioned in related work as a framework that leverages frozen LLMs to produce intermediate steps expressed as programs and to select demonstrations from a library, coupling programmatic intermediate representations with neural generation.",
            "declarative_component": "Programmatic intermediate representations and a task/demo library used as external structured resources.",
            "imperative_component": "Frozen LLMs that generate the intermediate programs and reasoning chains.",
            "integration_method": "Modular: LLMs generate program-like intermediate steps which can be executed or used as structured demonstrations; selection from a task library provides declarative exemplars.",
            "emergent_properties": "Enables automatic composition of reasoning and tool usage patterns; claimed to help leverage external modules without fine-tuning, but the current paper only cites ART conceptually.",
            "task_or_benchmark": null,
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": null,
            "interpretability_properties": "Intermediate programmatic steps increase traceability compared to raw text outputs.",
            "limitations_or_failures": null,
            "theoretical_framework": "Program-as-intermediate-representation paradigm coupling reasoning and action.",
            "uuid": "e477.7"
        },
        {
            "name_short": "REACT",
            "name_full": "REACT (Synergizing reasoning and acting in language models)",
            "brief_description": "A method that combines reasoning (chain-of-thought) and action (tool or environment interaction) to improve language-model performance on reasoning tasks.",
            "citation_title": "React: Synergizing reasoning and acting in language models",
            "mention_or_use": "mention",
            "system_name": "REACT",
            "system_description": "Cited as related work exploring the synergy of reasoning and acting: combine chain-of-thought style reasoning with calls to tools or environment interactions so that the model alternates between internal reasoning and external action.",
            "declarative_component": "External tool/environment interfaces and (optionally) structured buffers of observations (procedural/declarative resources).",
            "imperative_component": "Neural LLM performing interleaved reasoning and action steps.",
            "integration_method": "Interleaving LLM-generated reasoning steps with executed actions; modular orchestration rather than end-to-end differentiable fusion.",
            "emergent_properties": "Enables dynamic problem-solving that leverages external actions to support internal reasoning; paper only cites the approach without experimental details.",
            "task_or_benchmark": null,
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": null,
            "interpretability_properties": "Action traces and stepwise reasoning provide inspectable intermediate states.",
            "limitations_or_failures": null,
            "theoretical_framework": "Interleaving of reasoning and acting as a mechanism for richer problem solving.",
            "uuid": "e477.8"
        },
        {
            "name_short": "STaR",
            "name_full": "STaR (Self-Taught Reasoner / STaR: Bootstrapping Reasoning With Reasoning)",
            "brief_description": "A bootstrapping approach where generated rationales that yield correct answers are collected and used for further fine-tuning, effectively bootstrapping reasoning capability from generated explanations.",
            "citation_title": "STaR: Bootstrapping Reasoning With Reasoning",
            "mention_or_use": "mention",
            "system_name": "STaR",
            "system_description": "Cited as a prior technique that loops: generate rationales and answers, if some rationales yield correct answers then use those rationales as supervision to fine-tune or improve the model. This couples neural generation with an iterative symbolic/structured supervision process.",
            "declarative_component": "Collected rationales and high-quality reasoning traces used as (symbolic-like) supervision examples.",
            "imperative_component": "Neural LLM generating rationales and answers; optionally fine-tuning steps.",
            "integration_method": "Iterative dataset augmentation: use model-generated structured rationales that produce correct outputs as additional training data (supervised fine-tuning in some implementations).",
            "emergent_properties": "Bootstrapped improvement in reasoning accuracy by harvesting correct chain-of-thought traces; referenced in paper as related work without experimental numbers.",
            "task_or_benchmark": null,
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Not quantified in this paper.",
            "interpretability_properties": "Generated rationales provide explanatory artifacts that can be inspected and reused.",
            "limitations_or_failures": "Not discussed here.",
            "theoretical_framework": "Bootstrapping by iterative harvesting of correct reasoning traces to provide structured supervision.",
            "uuid": "e477.9"
        },
        {
            "name_short": "KokinovHybridAnalogy",
            "name_full": "A hybrid model of reasoning by analogy (Boicho Kokinov, 1994)",
            "brief_description": "A cited classic work describing a hybrid architecture for analogy reasoning that mixes connectionist and symbolic mechanisms.",
            "citation_title": "A hybrid model of reasoning by analogy",
            "mention_or_use": "mention",
            "system_name": "Hybrid analogy model (Kokinov)",
            "system_description": "Cited as historical prior work demonstrating how analogy can be modeled by a combination of symbolic mapping and connectionist computations; referenced to motivate hybrid approaches.",
            "declarative_component": "Symbolic mappings/relational structures used in analogy mapping.",
            "imperative_component": "Connectionist associative computations supporting retrieval and mapping processes.",
            "integration_method": "Hybrid coupling of symbolic mapping with distributed, associative processes (historical literature — not implemented here).",
            "emergent_properties": "Provided a model of analogy that leverages the strengths of both paradigms; paper only cites this work.",
            "task_or_benchmark": null,
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": null,
            "interpretability_properties": "Symbolic mappings are interpretable; connectionist components less so.",
            "limitations_or_failures": null,
            "theoretical_framework": "Hybrid symbolic-connectionist theory for analogy reasoning.",
            "uuid": "e477.10"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "A hybrid model of reasoning by analogy",
            "rating": 2,
            "sanitized_title": "a_hybrid_model_of_reasoning_by_analogy"
        },
        {
            "paper_title": "Soar: An architecture for general intelligence",
            "rating": 2,
            "sanitized_title": "soar_an_architecture_for_general_intelligence"
        },
        {
            "paper_title": "The Soar cognitive architecture",
            "rating": 2,
            "sanitized_title": "the_soar_cognitive_architecture"
        },
        {
            "paper_title": "How can the human mind occur in the physical universe?",
            "rating": 2,
            "sanitized_title": "how_can_the_human_mind_occur_in_the_physical_universe"
        },
        {
            "paper_title": "The atomic components of thought",
            "rating": 2,
            "sanitized_title": "the_atomic_components_of_thought"
        },
        {
            "paper_title": "The Sigma cognitive architecture and system: Towards functionally elegant grand unification.",
            "rating": 2,
            "sanitized_title": "the_sigma_cognitive_architecture_and_system_towards_functionally_elegant_grand_unification"
        },
        {
            "paper_title": "PAL: Program-aided Language Models",
            "rating": 2,
            "sanitized_title": "pal_programaided_language_models"
        },
        {
            "paper_title": "Toolformer: Language models can teach themselves to use tools",
            "rating": 2,
            "sanitized_title": "toolformer_language_models_can_teach_themselves_to_use_tools"
        },
        {
            "paper_title": "React: Synergizing reasoning and acting in language models",
            "rating": 2,
            "sanitized_title": "react_synergizing_reasoning_and_acting_in_language_models"
        },
        {
            "paper_title": "STaR: Bootstrapping Reasoning With Reasoning",
            "rating": 2,
            "sanitized_title": "star_bootstrapping_reasoning_with_reasoning"
        }
    ],
    "cost": 0.02344775,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>OlaGPT: Empowering LLMs With Human-like Problem-Solving Abilities</p>
<p>Yuanzhen Xie 
Platform and Content Group
GuangdongTencent ShenzhenChina</p>
<p>Tao Xie 
Platform and Content Group
GuangdongTencent ShenzhenChina</p>
<p>Mingxiong Lin 
Platform and Content Group
GuangdongTencent ShenzhenChina</p>
<p>Wentao Wei 
Platform and Content Group
GuangdongTencent ShenzhenChina</p>
<p>Chenglin Li 
Platform and Content Group
GuangdongTencent ShenzhenChina</p>
<p>Beibei Kong 
Platform and Content Group
GuangdongTencent ShenzhenChina</p>
<p>Lei Chen 
Platform and Content Group
GuangdongTencent ShenzhenChina</p>
<p>Chengxiang Zhuo 
Platform and Content Group
GuangdongTencent ShenzhenChina</p>
<p>Bo Hu 
Platform and Content Group
GuangdongTencent ShenzhenChina</p>
<p>Zang Li 
Platform and Content Group
GuangdongTencent ShenzhenChina</p>
<p>OlaGPT: Empowering LLMs With Human-like Problem-Solving Abilities</p>
<p>In most current research, large language models (LLMs) are able to perform reasoning tasks by generating chains of thought through the guidance of specific prompts. However, there still exists a significant discrepancy between their capability in solving complex reasoning problems and that of humans. At present, most approaches focus on chains of thought (COT) and tool use, without considering the adoption and application of human cognitive frameworks. It is well-known that when confronting complex reasoning challenges, humans typically employ various cognitive abilities, and necessitate interaction with all aspects of tools, knowledge, and the external environment information to accomplish intricate tasks. This paper introduces a novel intelligent framework, referred to as OlaGPT. OlaGPT carefully studied a cognitive architecture framework, and propose to simulate certain aspects of human cognition. The framework involves approximating different cognitive modules, including attention, memory, reasoning, learning, and corresponding scheduling and decision-making mechanisms. Inspired by the active learning mechanism of human beings, it proposes a learning unit to record previous mistakes and expert opinions, and dynamically refer to them to strengthen their ability to solve similar problems. The paper also outlines common effective reasoning frameworks for human problem-solving and designs Chain-of-Thought (COT) templates accordingly. A comprehensive decision-making mechanism is also proposed to maximize model accuracy. The efficacy of OlaGPT has been stringently evaluated on multiple reasoning datasets, and the experimental outcomes reveal that OlaGPT surpasses state-of-the-art benchmarks, demonstrating its superior performance. Our implementation of OlaGPT is available on GitHub: https://github.com/oladata-team/OlaGPT.</p>
<p>INTRODUCTION</p>
<p>In the past few years, large language models (LLMs) have developed the ability to process contextual information and generate fluent human language. As we encounter their outputs that sound natural and confident, we quickly assume that they have acquired the long-awaited thinking abilities, such as reasoning, communication, or collaboration, which are highly complex human skills. However, after in-depth understanding of LLM, we find that this reproduction based on high-probability language patterns is still far from the artificial general intelligence we expected. The most obvious gaps include the following: one is that LLMs in some cases produce content that is meaningless or deviates from human value preferences, or even dangerous suggestions with high confidence; secondly, the knowledge of LLMs is limited to the concepts and facts explicitly encountered in their training data. As a result, when faced with more complex problems, LLMs struggle to truly emulate human intelligence by understanding the ever-changing environment, collecting existing knowledge or tools, reflecting on historical lessons, decomposing problems, and using the thinking patterns that humans have summed up in the long-term evolution (such as Analogy, Inductive Reasoning and Deductive Reasoning, etc.) to effectively solve task. One way to solve the first problem is to introduce Reinforcement Learning from Human Feedback (RLHF), which has been recently implemented in ChatGPT [3]. It attempts to explicitly encode human expression preferences into the training process: experts will be asked to rank the answers given by the model according to human common sense and ethical requirements. Obviously, this method is still an idea based on selection or injecting constraints, which can avoid the generation of toxic information to a certain extent, but still cannot reduce the gap with human reasoning ability.</p>
<p>Naturally, inspired by the attempts to use a combination of data weights and bias to mimic how neurons work, we hope to draw on the cognitive model of the human brain and the thinking models developed in the long-term evolution process more comprehensively, and design corresponding system components to endow these cognitive structures or thinking processes to LLM, so as to approximately align the reasoning processes of humans and LLMs, and expect LLMs to be able to solve complex problems more effectively. Some recent works have tried to partially solve some problems, such as using vector databases to store and retrieve external domain knowledge in real time, hoping to improve the memory of LLMs and the ability to capture real-time knowledge; other works such as langchain 1 and toolfomer [27] are designed to be able to leverage available tools, etc. However, the process of mimicking the human brain to deal with problems still faces many systematic challenges:</p>
<p>Challenge 1: How to systematically imitate and encode the main modules in the human cognitive framework, and at the same time schedule the modules according to the general human reasoning patterns in a realizable way. As mentioned before, existing works [27,34] do not comprehensively attempt to align human and LLM reasoning pipelines.</p>
<p>Challenge 2: How to motivate LLMs to perform active learning like humans, that is, learn and evolve from historical mistakes or expert solutions to difficult problems? Encoding the corrected answers by retraining model might be feasible, but it is obviously costly and inflexible. The common in-context learning [3,6,23,36] is more to explain instructions or patterns in a few-shot way. The large model still lacks a human-like thinking framework for wrongly answered questions or historical lessons, such as "reflection-memorizing-reference-reasoning" mental model. Challenge 3. How can a LLM flexibly be able to leverage the diverse thinking patterns that human beings have evolved so as to improve its reasoning performance? It is hard to adapt to various problems by designing a fixed and general thinking model. Just like human beings usually choose different thinking methods flexibly when facing different types of problems, such as analogical reasoning, deductive reasoning and so on.</p>
<p>In order to solve challenge 1, we carefully studied humans' cognitive architecture framework [16], designed some functions to approximate these thinking modules, such as understanding of intention, memory and comprehensive decision-making, etc., and designed the corresponding scheduling mechanism. To address Challenge 2, we propose a concept called "difficult question notes" with the aim of recording cases where the model frequently answers incorrectly. Notes for difficult cases are collected either through manual corrections or by following prompts until a self-correction is made. When answering questions, LLMs can dynamically review the note pool, identify solutions for similar problems, and use them as a point of reference. For the third challenge, we summarize the most effective reasoning frameworks for humans in problem solving, and design corresponding Chain-Of-Thought templates accordingly. We also designed a comprehensive decision-making mechanism to summarize the answers given by each COT reasoning, so as to maximize the accuracy of the model. The main contributions of this work can be summarized as follows:</p>
<p>• As far as we know, this is the first work that attempts to systematically enhance LLMs' problem-solving abilities by learning from a human cognitive processing framework. Results show that this alignment approach can boost the LLMs' performance in many aspects, such as understanding of intention, accuracy of knowledge, correctness of reasoning, etc. • This work tries to summarize various methods of human reasoning into Chain-of-Thought (CoT) templates, so as to maximize the LLMs' reasoning effect in different scenarios. The article also innovatively designs an efficient active learning mechanism and vote mechanism to improve the accuracy and robustness of solving complex cases. • We conduct comprehensive experiments on two datasets and evaluate each module of the proposed method. The experimental results demonstrate that OlaGPT outperforms stateof-the-art baselines, indicating its superior performance.</p>
<p>RELATED WORK 2.1 Augmented Language Model</p>
<p>Augmented language model usually refers to the enhancement of a large language model's reasoning skills and the ability to use tools. The former is defined as decomposing a potentially complex task into simpler subtasks while the latter consists in calling external modules. Chain of Thought. Among prompt engineering methods, Chainof-Thought (CoT) prompting [32] is a popular technique that does not require fine-tuning model parameters. It is particularly effective in improving the model's performance in complex reasoning questions by simply changing the input. [6] references uncertaintybased active learning to mine most uncertain questions, which are then manually annotated and iteratively selected to stimulate reasoning ability as much as possible. To reduce the cost of manual annotation, a fully automated pipeline named Automate-CoT (Automatic Prompt Augmentation and Selection with Chain-of-Thought) is proposed in [28], which uses a variance-reduced policy gradient strategy to estimate the significance of each example in LLM. Another automatic CoT prompting method: Auto-CoT [36] samples questions with diversity and generates reasoning chains to construct prompt. Additionally, a solution is proposed in [30] to improve the results by using a voting strategy to select the most consistent answer output based on the results generated from different reasoning paths. Automatic Reasoning and Tool-use (ART) [23] uses frozen LLMs to automatically generate intermediate reasoning steps as a program. This framework selects demonstrations of multi-step reasoning and tool use from a task library. Self-Taught Reasoner (STaR) [35] relies on a simple loop: if the generated answers are wrong, try again to generate a rationale given the correct answer; fine-tune on all the rationales that ultimately yielded correct answers. Few-shot prompting struggles as task complexity increases. Consequently, many recent works employ in-context learning to decompose complex problems into sub-problems and effectively teach these sub-problems via separate prompts, such as SeqZero [33], Decomposed Prompting [13].</p>
<p>Tool Use. Although recent LLMs are able to correctly decompose many problems, they are still prone to errors when dealing with performing complex arithmetics. Program-Aided Language models (PAL) [8] decompose symbolic reasoning, mathematical reasoning, or algorithmic tasks into intermediate steps along with python code for each step. Similarly, [7] prompts Codex [5] to generate executable code-based solutions to university-level problems. furthermore, [27] introduces Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction.</p>
<p>Our work aims to enhance the performance of large models by drawing inspiration from human-like problem-solving capabilities, with a comparison to some related works presented in Table 1. </p>
<p>Cognitive Architecture</p>
<p>Cognitive architecture is a subset of general artificial intelligence research that began in the 1950s with the ultimate goal of modeling the human mind, bringing us closer to building human-level artificial intelligence. An early approach to cognitive architectures consisted of production systems, which used condition-action rules to represent and perform reasoning [22]. One notable cognitive architecture resulting from this line of research is SOAR, which combined problem-solving, learning, and knowledge representation within a unified system [19]. SOAR has evolved into a comprehensive framework for modeling various cognitive processes such as decision-making, planning, and natural language understanding [18]. Another influential cognitive architecture is ACT-R (Adaptive Control of Thought-Rational), which emphasizes symbolic processing and focuses on memory processes [2]. ACT-R has been applied to extensive cognitive tasks, such as problem-solving, language comprehension, and learning [1]. DUAL (Distributed Unit for Assembling Learning) is a hybrid cognitive architecture that strives to balance symbolic processing and connectionist approaches [15]. By employing a central executive agent and multiple collateral units, DUAL manages a diverse range of cognitive tasks. The Sigma architecture [26] is composed of several key components, such as perception, working memory, long-term memory, production memory, subgoals, decision network, and learning mechanism. [16] highlights the core cognitive abilities of these architectures and their practical applications in various domains. This paper will follow the framework of establishing an analogy between LLM and Cognitive Architectures.</p>
<p>METHOD</p>
<p>To address the challenges mentioned, we present a novel framework denoted as OlaGPT, which is a human-like problem-solving framework to empower LLMs.</p>
<p>Overview</p>
<p>Our approach draws on the theory of cognitive architecture [16], which suggests that the core capabilities of the cognitive framework include Attention, Memory, Learning, Reasoning, Action OlaGPT enhances the user's question intention; the second step is to select multiple thinking templates, tools, wrong question notes, and factual knowledge that may be used; In the third step, the previously obtained wrong notes(used as examples), factual knowledge(pre-knowledge), thinking(guidance) are combined to complete the construction of multiple template agents, and then execute these agents to obtain preliminary answers; The fourth step is to vote on the answers to get the final answer. Selection 2 . We fine-tuned this framework according to the needs of implementation and proposed a process suitable for LLM to solve complex problems. It includes six modules: the Intention Enhance module (corresponding to Attention), Memory module (Memory), Active Learning module (Learning), Reasoning module (Reasoning), Controller module (Action Selection), and Voting module.</p>
<p>The functional profile of each module is as follows:</p>
<p>Intention Enhance. According to [16], attention is an important component of human cognition, as it facilitates the identification of pertinent information and filters out irrelevant data. Similarly, we design corresponding attention modules for LLMs, namely Intention Enhance, aiming to extract the most relevant information and establish stronger associations between user input and the model's linguistic patterns.</p>
<p>Memory. The Memory module serves a crucial function in storing information in various libraries. Recent studies [21,27] have highlighted the limitations of current LLMs in comprehending the latest factual data. In light of this issue, we have designed the Memory module to focus on consolidating the existing knowledge that the model has not yet solidified and storing it in external libraries as long-term memory. During querying, the module's retrieval function can extract relevant knowledge from these libraries. There are four types of memory libraries involved in our paper: facts, tools, notes, and thinking.</p>
<p>Learning. The capability to learn is essential for humans to continuously improve their performance. Essentially, all forms of learning depend on experience. In particular, we found one way to quickly improve LLMs' reasoning ability is to let them learn from the mistakes it has made before. Firstly, we identify problems that cannot be resolved by LLMs. Next, we record the insights and explanations provided by experts in the notes library. Finally, we select relevant notes to facilitate LLMs' learning and enable them to handle similar questions more effectively.</p>
<p>Reasoning. The Reasoning module is designed to create multiple agents based on the human reasoning process, thereby stimulating the potential thought capacity of LLMs to effectively solve reasoning problems. The module incorporates various thinking templates that reference specific thinking types, such as lateral, sequential, critical, and integrative thinking, to facilitate reasoning tasks.</p>
<p>Controller. The Controller module is designed to handle relevant action selection, corresponding to the Action selection discussed in [16]. Specifically, the action selection in this paper involves the internal planning of the model for tasks such as selecting certain modules to execute and choosing from libraries of facts, tools, notes, and thinking.</p>
<p>Voting. The Voting module enables collective decision-making by leveraging the strengths of multiple thinking templates. As different thinking templates may be better suited for different types of questions, we have designed the Voting module to facilitate ensemble calibration among multiple thinking templates. The module is responsible for generating the best answer by employing various voting strategies to improve performance.</p>
<p>After introducing each module, we begin with an outline of the intelligent simulator with human-like problem-solving abilities (OlaGPT) followed by a more detailed description of each component. As depicted in Fig 2, once the user inputs a query, the Intention Enhance module generates a more understandable QA format for the LLMs. The Controller module then retrieves tools, notes, factual knowledge, and multiple thinking templates based on the user's intention. The relevant tools are integrated into the retrieved thinking templates, while notes and factual knowledge serve as supplementary information. (In our implementation, we build an index for the tool library and dynamically retrieve the relevant tools for each template from the index. It is worth noting that this feature has been implemented in the code. However, due to the lack of suitable tools in the current datasets, we did not utilize this feature in the experiment.)</p>
<p>Obviously, utilizing a singular cognitive framework is insufficient for addressing the diverse range of complex problems effectively. Thus, it is essential to adopt a variety of thinking template in order to derive more holistic and precise solutions, which is widely employed in model ensembles to reduce the variance in model outputs. More specifically, we execute multiple templates simultaneously and then employ various voting strategies in the Voting module to get the final answer.</p>
<p>Currently, LLMs have not yet reached a level of performance where they can surpass experts in various fields simultaneously. Nonetheless, by amalgamating the versatility of LLMs and the proficiency of specialists, it is possible to attain superior outcomes. As depicted in Fig 2, human feedback can be integrated into multiple aspects of the proposed framework. These aspects may include incorporating human feedback tools during the implementation of the thinking template, curating the notes library, and making selections in the Controller module. Such an approach ensures that the overall performance is enhanced through the combined expertise of humans and the framework itself. Intuitively, the inclusion of human experts to express clearly ambiguous content can notably improve the performance of LLMs when tackling intricate and multifaceted problems.</p>
<p>By leveraging the insights and expertise of human domain experts, LLMs can better understand the nuances of complex problems and generate more relevant and contextually appropriate responses. It is worth noting that the human feedback sub-module can be disabled in our design to achieve end-to-end reasoning. In this experiment, in order to reduce the cost of labor and resource, we choose the end-to-end approach for the experiment.</p>
<p>The specific content of each module is described in detail below.</p>
<p>Intention Enhance Module</p>
<p>Intention Enhance Module can be regarded as an optimized converter from user expression habits to model expression habits. A more suitable intent enhancement statement is designed for LLMs. Specifically, we get the type of questions by LLMs through specific prompts (see Table 5 in Appendix) in advance and then restructure the way the question is asked. As Fig 3 shows, the sentence--"Now give you the XX(question type produced by LLM model, the prompt see Table 5 in appendix) question and choices:" is added at the beginning of the question. To facilitate the analysis and processing of the results, the sentence--"The answer must end with JSON format: Answer: one of options[A,B,C,D,E]. ") is added at the end of the content. Additionally, we are currently trying to build an automated intention enhance module, set up a seed dataset and related instructions, and call the GPT interface to generate a batch of training data. Using the open-source LLaMA model [29] and Lora [10] technology, fine-tuning is performed using the data generated by the instruction. Intended to implement a module that automates user input enhancements. This module is still under development and experimentation, and relevant experiment results will be released in the future if there is an effect.</p>
<p>Memory Module</p>
<p>The memory module mainly stores relevant knowledge and dialogues. We use the memory function provided by langchain for short-term memory, and long-term memory is implemented by a Faiss-based vector database [11]. There are four main categories of knowledge in our approach: facts, tools, notes, and thinking library. We briefly describe these knowledge libraries as follows:</p>
<p>Facts library. Facts are real-world information like common sense and other knowledge that everyone accepts.</p>
<p>Tools library. In order to solve some defects of LLMs, a tool library that contains search engines, calculators and Wikipedia libraries, is introduced to assist the LLMs to complete some work without fine-tuning. The input and output of the tool should be in text format.</p>
<p>Notes library. Notes mainly record some hard cases and their problem-solving steps. Examples of Notes can be found in Fig 3. Thinking library. Thinking library mainly stores human problemsolving thinking templates written by experts that can be humans or models.</p>
<p>Active Learning Module</p>
<p>Intuitively, it is known that large language models (LLMs) have limitations in some specific tasks. The Active Learning module has been developed to identify challenging cases and provide expert answers. By doing so, LLMs can use expert answers as a reference when encountering similar tasks in the future. In essence, this module aims to facilitate the active learning process of LLMs, enabling them to acquire knowledge on the types of questions they typically struggle with.</p>
<p>Learn from mistakes. A common approach to improving the performance of language models in specific scenarios is to retrain them with annotated answers [24]. However, with the emergence of LLMs, this method is not practical as it requires enormous resources and cannot be updated in real-time. Drawing inspiration from how humans use notes to record their mistakes, we propose to introduce the note library for models to correct stubborn mistakes. When the LLM encounters a difficult question, it can dynamically refer to the note library to find similar problems and their solutions as references as shown in Fig 4. This approach can quickly improve the LLM's problem-solving abilities through prompts engineering. The specific notes format can be seen in Fig 3. The creation of of notes. When constructing notes library, it is recommended to first have LLMs answer a batch of questions repeatedly as shown in Fig 4. The questions that LLMs consistently answer incorrectly can then be recorded. Subsequently, experts can ensure the question types, detailed problem-solving steps, and general problem-solving ideas to form the notes library. Although the notes can be written by either experts or LLMs, it is generally preferable to have experts write the notes first and then have LLMs refine them. The final dataset format is processed as json: {"question":"x", "answer":"x", "error_reason":"x", "model_expert":"x", "explanation":"x", llm_task_type :"x"}. The type of task is generated with LLMs by specific prompt as shown in Table 5. The strategies of retrieving notes. In the Controller module, there are various ways to implement note retrieval. One intuitive approach is to find the most similar question type as an example reference, which has been found to be highly beneficial for LLMs. However, the number of questions of the same type is often too large to be used as additional knowledge input to LLMs (exceeding the number of tokens). Thus we have implemented multiple strategies, including Random Selection, Dual Retrieval, and Combine, denoting as , and respectively. indicates the method that randomly selects questions from the notes library.</p>
<p>represents a dual retrieval strategy that first retrieves the most similar question type using LLM and then retrieves the top-n relevant notes from the notes of such question type. However, text similarity does not always indicate question similarity. Hence, for , we use a random retrieval method to increase diversity in the second retrieval stage of . Last but not least, we use zero-shot to identify the zero-shot strategy, which means disabling the Active Learning module from the framework.</p>
<p>Reasoning Module</p>
<p>The diverse cognitive processes that humans possess are of importance for accurate problem-solving and achieving excellence in various tasks. To fully leverage the model's reasoning ability, we have designed multiple human reasoning approaches in the Reasoning module to assist the model in reasoning. These approaches are designed based on four thinking types, including Analogical, Sequential, Critical, and Synthesis thinking, which show in Fig 5. The following introduces some thinking principles and implementation methods.</p>
<p>• Lateral Thinking : Lateral Thinking is a creative problemsolving approach that involves looking at situations from unconventional perspectives. It includes techniques such as challenging assumptions, seeking alternative solutions with analogy, and embracing ambiguity.</p>
<p>-Analogical Thinking: Analogy Thinking is a cognitive process that involves finding similarities between two  distinct entities or concepts in order to gain a deeper understanding of a problem or situation, which is one of the most effective tools to generate innovative ideas [14]. The prompts can be seen in Table 6. • Sequential Thinking: Sequential Thinking (also named Linear Thinking) is a problem-solving approach that involves breaking down complex tasks or problems into smaller, more manageable parts and addressing them in a logical, step-bystep manner [9].</p>
<p>-Decomposition Thinking: Decomposition Thinking involves breaking down complex problems into smaller subproblems and guiding task decomposition.</p>
<p>-Plan Thinking: Plan Thinking involves creating a detailed roadmap or strategy for achieving a specific goal or completing a task.</p>
<p>-Step Thinking:</p>
<p>Step Thinking is a problem-solving approach that involves breaking down complex tasks into smaller, manageable steps, allowing for more efficient and organized solutions. • Critical Thinking: Critical thinking is the process of objectively analyzing and evaluating information to form reasoned judgments. It includes the component skills of analyzing arguments, making inferences using inductive or deductive reasoning, judging or evaluating, and making decisions or solving problems [17].</p>
<p>-Verification Thinking: In real-life problem-solving scenarios, humans often employ one approach to solve a problem and then use another approach or seek the input of another person to verify the solution and improve its accuracy. This collaborative and iterative process enables more reliable and well-rounded outcomes. Based on this observation, we designed the verification thinking, which involves using an LLM to generate an output and then allowing the same model to provide feedback on its own output from multiple perspectives. The model can subsequently refine its previously generated output based on this feedback. It has not been used in experiments so far. • Integrative Thinking: Integrative Thinking involves combining multiple approaches in various ways, which seems to be the core component in models of adult cognitive development [12]. In our specific implementation, we guide the model to use a variety of thought approaches in a free DIY(Do It Yourself) mode.</p>
<p>As of yet, it has not been utilized in any experiments.</p>
<p>Collectively, these thinking methods are designed to enable models to reason more efficiently and provide more accurate answers. The thinking introduced above is not exhaustive, and the common ones include Concrete Thinking, Abstract Thinking, Vertical Thinking, etc. In actual use, different types of thinking templates can be designed according to the situation. In this module, appropriate thinking templates can be incorporated based on the specific requirements of different tasks. Alternatively, LLMs can generate new templates by combining and selecting from an array of existing thinking templates. This flexibility allows the model to adapt and engage more effectively with the diverse challenges it encounters. In the experiment, the paper explores analogy (Lateral Thinking), decomposition (Sequential Thinking), plan (Sequential Thinking), and step (Sequential Thinking) thinking.</p>
<p>Controller Module</p>
<p>In the Controller module, relevant facts, tools, notes, and thinking are first retrieved and matched (as shown in the second step of Fig  2). The retrieved content is subsequently integrated into a template agent, requiring the LLMs to furnish a response under a single template in an asynchronous manner (as shown in the third step of Fig  2). Just as humans may struggle to identify all relevant information at the outset of reasoning, it is difficult to expect LLMs to do so. Therefore, dynamic retrieval is implemented based on the user's question and intermediate reasoning progress.</p>
<p>As mentioned earlier, the Faiss method [11] has been employed to create embedding indices for all four libraries, with retrieval strategies differing among them. Specifically, information retrieval serves as a tool that can be accessed at any time during the LLM model's problem decomposition and reasoning process from external knowledge bases. In terms of tool selection, we provide a list of tools, enabling the LLM to make real-time choices during the reasoning process. The retrieval strategy for notes differs from the others, as we have designed the three methods described in Section 3.4. As for the retrieval of thinking, we design prompts to allow the LLM for judging and selecting relevant thinking templates. But currently, we use all thinking templates together to perform a comprehensive analysis in the experiment.</p>
<p>Voting Module</p>
<p>Following the third step depicted in Fig 2, we acquire the answers of various thinking templates through LLMs. Intuitively, distinct thinking templates may be more suitable for different types of questions. Therefore, instead of relying on a single template, we adopt a voting mechanism to aggregate the answers from multiple templates and improve the final performance of our model. This approach also enhances the robustness of the model's results, as it takes into consideration the variability in the efficacy of different templates when applied to distinct questions. There are several voting methods available: 1) vote by LLM: Instruct the LLM model to choose the most consistent answer among several given options by providing an output answer along with a rationale through prompts. 2) vote by regex: Extract the answer by regex expression to get the majority vote.</p>
<p>EXPERIMENTAL</p>
<p>We conducted a series of experiments on a range of public datasets and compared the proposed OlaGPT with existing approaches. Our findings indicate that our approach consistently outperforms the baselines on every dataset considered, demonstrating its robustness and effectiveness.</p>
<p>Experimental Setup</p>
<p>Our experiments are designed to address the followings:</p>
<p>• RQ1. How does OlaGPT perform compared to the state-ofthe-art baselines? • RQ2. How effective are sub-modules in the overall model design? • RQ3. How do hyperparameters affect experimental results?</p>
<p>DataSets. To evaluate the proposed framework in a more comprehensive manner, we utilize several publicly available datasets for experimentation. AQuA [20] (Algebra QA with Rationales) is a dataset comprising approximately 100,000 algebraic word problems and 254 test questions. For our labeled training set, we randomly sample 200 training problems. E-KAR [4] is the first interpretable knowledge-intensive analogical reasoning dataset consisting of 1,655 (Chinese) and 1,251 (English) questions from the Chinese Civil Service Exam. In our experiment, we utilize the Chinese version to explore the performance with the Chinese language.</p>
<p>The statistical results of the dataset are presented in Table 2. We identified the questions that the large model answered incorrectly 3-5 times from the training set as its error books. However, it is too expensive to achieve this on the training set of AQuA due to its large size. To address this issue, we utilized Sentence-BERT [25] for embedding and then clustered the training set into 20 groups using K-means. Finally, 211 questions were randomly selected from each group based on their weights.</p>
<p>Evaluation Metrics. The current metric for measurement is the accuracy rate, and most of the questions are in multiple-choice format. The answer is considered correct only when it exactly matches the provided answer options. To compute the model's accuracy rate, we initially use regular expressions to match answers and subsequently perform manual inspection and correction of the assessed results.</p>
<p>The use of thinking templates. In the main experiment, we utilized five thinking templates and the original base model (GPT-3.5-turbo). The prompts of each thinking template can be found in Table 6. Specifically, E-KAR employed all six thinking templates, including origin, Analogy Thinking (AT), Decomposition Thinking 1 (DT), Decomposition Thinking 2 (DST), Plan Thinking (PT), and Sequential Thinking (ST). For AQuA, we used the other five templates excluding AT.</p>
<p>Baselines. We select conventionally and recently published</p>
<p>Augmented LLM baselines for model comparison, which can be briefly categorized into three groups: (1) base LLM model (GPT-3.5turbo); (2) Augmented LLM based on prompt engineering (Auto-CoT); (3) Augmented LLM based on process optimization (SC). To ensure a fair comparison, all baseline methods use the same intention enhancement or voting mechanism.</p>
<p>• GPT-3.5-turbo: A base LLM model that builds upon the GPT-3 architecture and incorporates additional training data and techniques to improve performance on natural language processing tasks. It achieves state-of-the-art results on several benchmark datasets, demonstrating its effectiveness in language modeling and downstream tasks. The temperature is set to 0 in the experiment. • Auto-CoT [36] An automatic CoT (Chain of Thought) prompting method. This approach involves sampling diverse questions and generating reasoning chains to construct demonstrations. • SC [31] A new decoding strategy called self-consistency for chain-of-thought prompting. It samples diverse reasoning paths and selects the most consistent answer by marginalizing the sampled paths. This approach acknowledges that complex reasoning problems have multiple correct ways of thought.</p>
<p>The Model Performance Comparison</p>
<p>In order to evaluate the effectiveness of our augmented LLM framework for inference tasks, we conducted comprehensive experimental comparisons on two types of inference datasets. The results of the experiments are summarized in Table 3. The best performance is indicated in bold, and "Improv" indicates the improvement over the best baseline. Our experiments yielded several findings:</p>
<p>• The performance of SC is better than GPT-3.5-turbo, suggesting that employing ensemble methods to a certain extent can indeed contribute to enhancing the effectiveness of largescale models. • The performance of our approach surpasses that of SC (SC adopts the same majority voting extracted by employing regular expressions as our method), which, to a certain extent, demonstrates the effectiveness of our thinking template strategy. The answers of different thinking templates exhibit considerable variation, and conducting voting at different thinking templates ultimately yields better results than simply running multiple rounds and voting. • The effects of different thinking templates are different, as shown in Table 3. Relatively speaking, ST and DT have better effects. This can be attributed to the fact that this kind of stepby-step solution may be more suitable for reasoning-type questions. • The results presented in Table 3 demonstrate that the Active Learning module yields significantly better performance compared to the zero-shot approach. Specifically, the random, retrieval, and combine columns exhibit superior performance. These findings suggest that incorporating challenging cases as a note library is a viable strategy. • Different Retrieval schemes work differently on different datasets. Overall, the Combine strategy works better. • Our method is significantly better than other solutions, thanks to the rational design of the overall framework, the specific reasons are as follows: 1) The effective design of the Active Learning module; 2) The thinking template has achieved the adaptation of different models as expected, and the results under different thinking templates are different; 3) The Controller module plays a good control role and selects the content that better matches the required content; 4) The way of ensembles with different thinking templates designed by the Voting module is effective.</p>
<p>Ablation Study</p>
<p>To investigate the impact of each sub-module, we conducted an ablation analysis on our framework.</p>
<p>the Performance of Active Learning module. In order to verify the effectiveness of the Active Learning module, we compare whether to use this module or not. The results shown in Fig  6 indicate that the notes' performance surpasses that of the zeroshot. Taking the similarly weak topics of large models as hints can significantly improve performance. The underlying rationale is straightforward: humans learn through accumulating experiences, especially incorrect ones. By providing LLM with the correct problem-solving process for similar problems and allowing it to understand and imitate the process, we can enable the LLM to learn from past mistakes.</p>
<p>the Performance of different retrieval strategies in Controller module. For different notes retrieval strategies, we also conducted experiments and the experimental results are shown in Fig 6 and Fig 9. From the results of different _ ℎ _ values in the table 6, we can see that: the strategy has the best effect, Random is the second one, and the worst one is zero-shot. Intuitively, finding the most similar question type as an example reference has the greatest gain for LLMs. Due to the large scale of the notes and the large number of questions depending on the type of questions, it is unrealistic to introduce them all as few shots. Both and strategies perform a second search after retrieving the question type. The strategy uses similarity search the second time and retrieval uses random search. The strategy is better than because random retrieval introduces greater diversity. Compared with random retrieval, similarity retrieval cannot necessarily satisfy the similarity of topic types, only the similarity in characters. This conclusion is similar to that of Auto-CoT. The most essential thing is to find the most relevant questions.</p>
<p>The best result of E-KAR (chinese) is the strategy. There are several reasons here: 1) The types of questions are not detailed enough. Because of this kind of analogical reasoning questions, the type of questions generated by the model is almost an analogy question, unlike the question types of math questions that can be more detailed. 2) Templates are not customizable. Except for AT for analogy thinking, other thinking templates are made for mathematics questions.</p>
<p>the Performance of Different Thoughts in Reasoning module. This section mainly discusses the effect of different thinking templates on different tasks. From Table 3, it is evident that different thinking templates exhibit varying effects across distinct datasets and retrieval strategies. However, all of them surpass the turbo's results, thereby validating the notion presented in the article that a single template cannot effectively address multiple tasks. Currently, the top three strategies yielding superior results on AQuA are ST-retrieval, DST-retrieval, and DT-combine. For E-KAR, the more effective strategies include ST-combine, DT-retrieval, and DST-random.</p>
<p>After posterior analysis, the final correct rate of some templates has little difference as shown in the table 3, but their consistency is not high. For this very reason, designing a voting module can help select as many correct answers as possible. We have also examined the possible maximum accuracy that can be achieved by different voting strategies. Please refer to Table 8 in the appendix.  c=2  66  56  45  61  42  45  44  40  c=3  95  71  79  73  94  94  91  92  c=4  63  48  63  61  102  82  81  86  c=5  29  78  67  57  64  74  71  75  c=6  ----16 16 16 16 the Performance of Voting module. We explored different voting strategies and the experimental situation of voting under different template numbers. In this experiment, if it is not specified, the regex results will be used for discussion.</p>
<p>• As shown in Fig 7, the performance of the model after voting is significantly improved compared to that of individual thinking templates, which also validates the effectiveness of the voting approach. Different thinking templates are appropriate for different types of questions, and the voting strategy can consolidate the strengths of different thinking templates to further enhance the framework's performance. We consider all correct results as the upper bound and all incorrect results as the lower bound with the regex-vote (majority vote by regex expression) method. The specific values are presented in Table 8 in the Appendix. • As shown in Fig 6, the accuracy rate increases with the growth in the number of templates. As the number of templates increases, the variety of answer combinations generated by different templates also expands, thereby offering the voting module a broader potential upper limit for performance improvement.</p>
<p>• The regex-vote strategy can ensure the majority voting mechanism, while the llm-vote strategy relies on the explanation of the candidate's answers, which may have a larger degree of uncertainty. Consider an example with five templates, where there are 2 correct and 3 incorrect answers. The regex-vote strategy will always select the incorrect. The llm-vote strategy still has the possibility of selecting the correct answer, based on the different confidence of weights on different answers. Consequently, the potential accuracy upper bound of the llm-vote strategy may be even higher.</p>
<p>In summary, we can observe the improvement of the overall recommendation performance whenever we incrementally add a new module or feature on top of the previous model, which illustrates the effectiveness of the Active Learning, Controller, Thought, and Voting modules.</p>
<p>Hyperparameter Study</p>
<p>In this subsection, we investigate the model's hyperparameters.</p>
<p>To determine the optimal number of notes to use as the example reference, we conduct an experiment on the number of examples. Taking the experimental results of the AQuA dataset under the ST template as an example, the outcomes are displayed in Fig 9. It can be found that the optimal value for the number of notes varies among different retrieval strategies. The strategy exhibits consistent improvement, while the and strategies first increase and then decrease. When more sample questions are included, additional noise may be introduced. Thus, it is important to find the appropriate number of examples, neither too few nor too many, making a trade-off.</p>
<p>CONCLUSION</p>
<p>This paper designs an enhanced LLM cognition framework (OlaGPT), aiming to solve difficult reasoning problems with human-like problemsolving abilities. Specifically, referring to the theory of human cognition, OlaGPT proposes to approximate cognitive modules, such as attention (for entention enhancement), Memory, Learning, Reasoning, action selection (Controller) and decision making. The user's query undergoes refinement through the Intention Enhancement module, achieving a more precise expression. Then, the Controller module controls and utilizes this expression to select the required library content, and completes the filling of multiple thinking templates. After acquiring the results of asynchronously executing multiple reasoning templates, a more robust effect is achieved using the Voting module. We conduct experiments on two real inference datasets and show that the OlaGPT method outperforms existing methods in answering inference questions. In addition, we also demonstrate the effectiveness of the design of each part in the model. Most of the modules are designed to be pluggable, and the required modules can be determined according to the needs of different scenarios.</p>
<p>In the follow-up work, we will continue to optimize and improve the functions of each sub-module, and it is expected to complete an easy-to-use enhanced large-scale model framework with human thinking ability. First, more diverse datasets and baselines will be added for experimental testing. In addition, we will continue to optimize the design of each sub-module and conduct more experiments to testify our ideas.         </p>
<p>A APPENDIX A.1 Prompts</p>
<p>In this section, we mainly put the related Prompts design in the experiment, including the prompts of generating question types as shown in Table 5 and thinking templates as shown in Table 6.</p>
<p>A.2 Cognitive Architecture</p>
<p>The framework structure of this paper mainly refers to the article [16], and the key modules listed in the article will be described in detail below.</p>
<p>Attention. The attention section of the essay primarily focuses on the selective mechanisms utilized by cognitive architectures for prioritizing relevant information and filtering out extraneous data. The main idea is to explore how attentional processes help individuals efficiently allocate cognitive resources to specific stimuli or tasks, as well as to discuss the techniques and models applied to modulate attention in various contexts. In LLM, the primary objective of attention is to interpret the input prompt and discern the intent underlying the words.</p>
<p>Action Selection. In the action selection section, the main idea revolves around examining the decision-making mechanisms employed by cognitive architectures to choose appropriate actions in response to external stimuli or internal states. This part covers key computational models, methods, and algorithmic logic responsible for determining and selecting goal-directed actions based on the available information and environmental context.</p>
<p>Memory. The memory section of the essay explores the concepts and models related to the storage and retrieval of information within cognitive architectures. The main idea is to investigate the fundamental features of short-term (or working) and long-term memory, as well as to present the mechanisms by which cognitive systems encode, maintain, and retrieve important information and experiences. We focus on reinforcing the existing knowledge that the model has not yet firmly established. Memory stores the consolidated information in external libraries, effectively functioning as long-term memory for the model.</p>
<p>Learning. The learning section delves into the processes that help cognitive architectures acquire new information, adapt to new situations, and generalize from previous experiences. The main idea is to examine the different learning paradigms, such as supervised, unsupervised, and reinforcement learning, and their applications in equipping cognitive architectures with the ability to modify and improve their knowledge structures, representations, and decisionmaking processes. By updating the few-shot content in the prompt, this task can be easily accomplished. This paper achieves learning in the Large Language Model (LLM) by updating the note library, allowing the model to acquire new knowledge and adapt to new information.</p>
<p>Reasoning. The reasoning section of the essay emphasizes the cognitive processes underlying problem-solving, decision-making, and inference within cognitive architectures. The main idea is to present various approaches and models that demonstrate logical and probabilistic reasoning, as well as to discuss the mechanisms for generating predictions, explanations, and strategies based on available information and knowledge. The module incorporates various templates that enable the model to approach problem-solving situations more effectively and generate well-structured solutions.</p>
<p>A.3 Examples</p>
<p>This section mainly introduces the specific execution text samples. In the Fig 10 and 11, we give the full-text content in different datasets using the strategy to retrieve the notes when asking questions about LLMs(gpt-turbo-3.5) with the DT thinking template.</p>
<p>A.4 Templates analysis</p>
<p>The performance of each template is presented in Table 3. According to Table 7, there are fluctuations in each template when executed on different datasets, which suggests that utilizing model ensembling may lead to improved performance. Consequently, this work employs a voting mechanism to capitalize on the strengths of various templates while addressing their individual limitations.</p>
<p>On the AQuA dataset, the best-performing template is DT, which demonstrates the highest average accuracy, reaching 0.5807. In the E-KAR Chinese dataset, the top-performing template is ST, achieving an accuracy of 0.4015. These results highlight the effectiveness of using different templates tailored to the specificities of each dataset in order to maximize performance.</p>
<p>A.5 Vote analysis</p>
<p>In this study, we investigate voting methods in model ensembles, employing two distinct approaches for fusing the results derived from different models. The first approach entails extracting candidate answers using regular expressions and selecting the most frequently occurring answer as the ensemble's output. The second approach feeds the predicted outputs and their analyses from diverse models into GPT-3.5-turbo, which subsequently generates the final answer.</p>
<p>Considering the voting method utilizing regular expressions for answer extraction, a supremum and infimum inherently exist. To clarify this, the study introduces two metrics evaluating the answers produced by various models for identical questions: accuracy and incorrect consistency. Accuracy represents the proportion of models delivering the correct answer, while incorrect consistency refers to the proportion associated with the most frequently recurring answer, excluding the correct one.</p>
<p>The supremum corresponds to the proportion of questions where accuracy either surpasses or equals consistency. In contrast, the infimum signifies the proportion where accuracy exceeds consistency. The GPT-3.5-turbo-based voting method results reveal an average increase of 0.0561 and 0.0366 in comparison to the infimum derived through regular expression answer extraction for the AQuA and E-KAR (Chinese) datasets, respectively. When compared to the supremum, accuracy remains an average of 0.0325 lower for the AQuA dataset and 0.0485 lower for the E-KAR (Chinese) dataset. Consequently, the outcomes acquired through LLM voting exhibit a higher degree of robustness.</p>
<p>Furthermore, the heatmap shown in Fig 12 demonstrates that answers derived from different templates tend to cluster, indicating Table 5: the prompts of generating question type.</p>
<p>E-KAR datasets:</p>
<p>You are the examiner of the Chinese Civil Service Examination, and you need to judge the specific question types of the following analogy questions and don't give an explanation. Question: {question} Answer: The output must only be in a strict JSON format: "task_type": "question type".</p>
<p>Math datasets:</p>
<p>As a mathematics professor, you need to judge the type of the following question and don't give an explanation Question: {question} Answer: The output must only be in a strict JSON format: "task_type": "question type". Table 6: the main prompts of some thinking templates.</p>
<p>Analogical Thinking (AT): For the problem of analogical reasoning, it is completed in three steps. First conduct an inductive analysis of the given sample data, considering the similarity of the relationship between words; Next, judge whether the sample to be selected is satisfied; Finally check the validity of the mapping and explain if the mapping is correct.</p>
<p>Decomposition Thinking: 1) DT: The following questions can be disassembled into multiple sub-questions to solve, the steps and answers of each sub-question are given, and finally the answer to the following question is given. 2) DST: Disassemble the following complex problems to solve them step by step</p>
<p>Plan Thinking (PT):</p>
<p>Think carefully about the problem to be solved and make a detailed plan to solve it.</p>
<p>Step Thinking (ST): Let's think step by step.     </p>
<p>Figure 1 :
1The overall structure of the OlaGPT model.</p>
<p>Figure 2 :
2The overall flow chart of the OlaGPT model. First,</p>
<p>Figure 3 :
3The notes are combined as examples. The tem-plates_prefix is template-specific content.</p>
<p>Figure 4 :
4The overall of the Active Learning module.</p>
<p>Figure 5 :
5The example of some thinking.</p>
<p>Figure 6 :
6The performance under the different number of templates and notes retrieval modes.</p>
<p>Figure 7 :
7The performance before and after voting.</p>
<p>[</p>
<p>Figure 8 :
8The performance before and after voting.</p>
<p>Figure 9 :
9The performance of ST under the different number of different notes retrieval examples.</p>
<p>Figure 10 :
10one example contents for AQuA.</p>
<p>Figure 11 :
11one example contents for E-KAR.</p>
<p>Figure 12 :
12The Accuracy and Incorrect Consistency of AQuA and E-KAR (Chinese).</p>
<p>Table 1 :
1Comparing OlaGPT with related approaches.Features 
CoT Auto-CoT Toolformer OlaGPT 
Multi-step reasoning ✓ 
✓ 
✓ 
Limited supervision 
✓ 
✓ 
✓ 
Tool use 
✓ 
✓ 
Extendable libraries 
✓ 
Cross-task transfer 
✓ 
✓ 
✓ 
Human feedback 
✓ 
✓ 
Active learning 
✓ </p>
<p>Table 2 :
2Statistics of Datasets.Domain 
Datasets #testing #training #error books </p>
<p>mathematical reasoning AQuA 
254 
97467 
81 </p>
<p>analogical reasoning 
E-KAR 
335 
1155 
632 </p>
<p>Table 3 :
3Performance comparison of different methods on two datasets.Type 
Datasets 
AQuA 
E-KAR (Chinese) 
_ 
_ 
zero-shot 
zero-shot </p>
<p>baselines </p>
<p>turbo 
0.3228 
0.5236 
0.5315 
0.5039 
0.3762 
0.3731 
0.403 
0.3701 
auto_cot 
-
0.5748 
-
-
-
0.3791 
-
-
sc 
0.3189 
0.6142 
0.5394 
0.5906 
0.3761 
0.4179 
0.4119 
0.3851 </p>
<p>thinking templates </p>
<h2>AT</h2>
<h2>-</h2>
<p>-
0.3851 
0.3821 
0.3910 
0.3881 
DT 
0.5591 
0.5866 
0.5827 
0.5945 
0.3612 
0.4119 
0.4269 
0.3881 
DST 
0.5079 
0.5236 
0.5984 
0.5945 
0.3552 
0.4030 
0.3761 
0.4000 
PT 
0.5512 
0.5472 
0.5827 
0.5512 
0.3851 
0.3552 
0.4119 
0.4060 
ST 
0.5197 
0.5787 
0.6102 
0.5669 
0.3373 
0.4179 
0.4119 
0.4388 </p>
<p>Our Framework 
OlaGPT-regex-vote 
0.5945 
0.6496 
0.6732 
0.6772 
0.4209 
0.4597 
0.4567 
0.4716 
OlaGPT-llm-vote 
0.5984 
0.6417 
0.6654 
0.7047 
0.4000 
0.4418 
0.4358 
0.4507 
%Improv. 
85.38% 
5.76% 
24.81% 
19.32% 
11.82% 
10.00% 
10.88% 
22.46% </p>
<p>Table 4 :
4Consistency analysis of the thinking templates.#consistency 
AQuA 
E-KAR (Chinese) 
zero-shot 
zero-shot </p>
<p>Table 7 :
7Precision analysis of the thinking templates.AQuAE-KAR (Chinese) zero-shot random retrieval combine zero-shot random retrieval combine that these templates produce analogous judgments for the same questions.Received 20 February 2007; revised 12 March 2009; accepted 5 June 2009Range 
0.0512 
0.0630 
0.0275 
0.0433 
0.0478 
0.0627 
0.0508 
0.0507 
Mean 
0.5345 
0.5590 
0.5935 
0.5768 
0.3648 
0.3940 
0.4036 
0.4042 </p>
<p>Table 8 :
8Explore the high accuracy of the theory.Combination 
AQuA 
E-KAR (Chinese) 
zero-shot 
zero-shot </p>
<p>regex-upper 
0.6457 
0.6614 
0.7047 
0.7283 
0.4537 
0.5015 
0.4866 
0.4806 
regex-lower 
0.5315 
0.5906 
0.6614 
0.6024 
0.3552 
0.4179 
0.4030 
0.4060 
llm-vote 
0.5984 
0.6417 
0.6654 
0.7047 
0.4000 
0.4418 
0.4358 
0.4507 
reg-vote 
0.5945 
0.6496 
0.6732 
0.6772 
0.4209 
0.4597 
0.4567 
0.4716 </p>
<p>https://python.langchain.com/en/latest/modules/agents/how_to_guides.html arXiv:2305.16334v1 [cs.CL] 23 May 2023
Relevant content is described in Appendix A.2.</p>
<p>How can the human mind occur in the physical universe?. John R Anderson, Oxford University PressJohn R Anderson. 2009. How can the human mind occur in the physical universe? Oxford University Press.</p>
<p>The atomic components of thought. R John, Christian J Anderson, Lebiere, Psychology PressJohn R Anderson and Christian J Lebiere. 2014. The atomic components of thought. Psychology Press.</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 33Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems 33 (2020), 1877-1901.</p>
<p>E-KAR: A Benchmark for Rationalizing Natural Language Analogical Reasoning. Jiangjie Chen, Rui Xu, Ziquan Fu, Wei Shi, Zhongqiao Li, Xinbo Zhang, Changzhi Sun, Lei Li, Yanghua Xiao, Hao Zhou, Findings of the Association for Computational Linguistics: ACL 2022. Dublin, IrelandAssociation for Computational LinguisticsJiangjie Chen, Rui Xu, Ziquan Fu, Wei Shi, Zhongqiao Li, Xinbo Zhang, Changzhi Sun, Lei Li, Yanghua Xiao, and Hao Zhou. 2022. E-KAR: A Benchmark for Ratio- nalizing Natural Language Analogical Reasoning. In Findings of the Association for Computational Linguistics: ACL 2022. Association for Computational Linguistics, Dublin, Ireland, 3941-3955. https://aclanthology.org/2022.findings-acl.311</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, arXiv:2107.03374Ilya Sutskever, and Wojciech Zaremba. 2021. Evaluating Large Language Models Trained on Code. Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec RadfordDave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang; Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlishcs.LGMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fo- tios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shan- tanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. Evaluating Large Language Models Trained on Code. arXiv:2107.03374 [cs.LG]</p>
<p>Active Prompting with Chain-of-Thought for Large Language Models. Shizhe Diao, Pengcheng Wang, Yong Lin, Tong Zhang, arXiv:2302.12246arXiv preprintShizhe Diao, Pengcheng Wang, Yong Lin, and Tong Zhang. 2023. Active Prompting with Chain-of-Thought for Large Language Models. arXiv preprint arXiv:2302.12246 (2023).</p>
<p>A neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level. Iddo Drori, Sarah Zhang, Reece Shuttleworth, Leonard Tang, Albert Lu, Elizabeth Ke, Kevin Liu, Linda Chen, Sunny Tran, Newman Cheng, Roman Wang, Nikhil Singh, Taylor L Patti, Jayson Lynch, Avi Shporer, Nakul Verma, Eugene Wu, Gilbert Strang, 10.1073/pnas.2123433119Proceedings of the National Academy of Sciences. 11932Iddo Drori, Sarah Zhang, Reece Shuttleworth, Leonard Tang, Albert Lu, Elizabeth Ke, Kevin Liu, Linda Chen, Sunny Tran, Newman Cheng, Roman Wang, Nikhil Singh, Taylor L. Patti, Jayson Lynch, Avi Shporer, Nakul Verma, Eugene Wu, and Gilbert Strang. 2022. A neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level. Proceedings of the National Academy of Sciences 119, 32 (aug 2022). https: //doi.org/10.1073/pnas.2123433119</p>
<p>Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, Graham Neubig, arXiv:2211.10435PAL: Program-aided Language Models. cs.CLLuyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023. PAL: Program-aided Language Models. arXiv:2211.10435 [cs.CL]</p>
<p>Linking linear/nonlinear thinking style balance and managerial ethical decision-making. Kevin Groves, Charles Vance, Yongsun Paik, Journal of Business Ethics. 80Kevin Groves, Charles Vance, and Yongsun Paik. 2008. Linking linear/nonlinear thinking style balance and managerial ethical decision-making. Journal of Busi- ness Ethics 80 (2008), 305-325.</p>
<p>J Edward, Yelong Hu, Phillip Shen, Zeyuan Wallis, Yuanzhi Allen-Zhu, Shean Li, Lu Wang, Weizhu Wang, Chen, arXiv:2106.09685Lora: Low-rank adaptation of large language models. arXiv preprintEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 (2021).</p>
<p>Billion-scale similarity search with gpus. Jeff Johnson, Matthijs Douze, Hervé Jégou, IEEE Transactions on Big Data. 7Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2019. Billion-scale similarity search with gpus. IEEE Transactions on Big Data 7, 3 (2019), 535-547.</p>
<p>Integrative thinking is the key: An evaluation of current research into the development of adult thinking. Eeva Kallio, Theory &amp; Psychology. 21Eeva Kallio. 2011. Integrative thinking is the key: An evaluation of current research into the development of adult thinking. Theory &amp; Psychology 21, 6 (2011), 785-801.</p>
<p>Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, arXiv:2210.02406Peter Clark, and Ashish Sabharwal. 2023. Decomposed Prompting: A Modular Approach for Solving Complex Tasks. cs.CLTushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Pe- ter Clark, and Ashish Sabharwal. 2023. Decomposed Prompting: A Modular Approach for Solving Complex Tasks. arXiv:2210.02406 [cs.CL]</p>
<p>Analogical thinking for generation of innovative ideas: An exploratory study of influential factors. Eunyoung Kim, Hideyuki Horii, Interdisciplinary Journal of Information, Knowledge, and Management. 11201Eunyoung Kim and Hideyuki Horii. 2016. Analogical thinking for generation of innovative ideas: An exploratory study of influential factors. Interdisciplinary Journal of Information, Knowledge, and Management 11 (2016), 201.</p>
<p>A hybrid model of reasoning by analogy. Boicho Kokinov, Advances in connectionist and neural computation theory. 2Boicho Kokinov. 1994. A hybrid model of reasoning by analogy. Advances in connectionist and neural computation theory 2 (1994), 247-318.</p>
<p>40 years of cognitive architectures: core cognitive abilities and practical applications. Iuliia Kotseruba, John K Tsotsos, Artificial Intelligence Review. 53Iuliia Kotseruba and John K Tsotsos. 2020. 40 years of cognitive architectures: core cognitive abilities and practical applications. Artificial Intelligence Review 53, 1 (2020), 17-94.</p>
<p>Critical thinking: A literature review. R Emily, Lai, Pearson's Research Reports. 6Emily R Lai. 2011. Critical thinking: A literature review. Pearson's Research Reports 6, 1 (2011), 40-41.</p>
<p>The Soar cognitive architecture. E John, Laird, MIT pressJohn E Laird. 2019. The Soar cognitive architecture. MIT press.</p>
<p>Soar: An architecture for general intelligence. E John, Allen Laird, Paul S Newell, Rosenbloom, Artificial intelligence. 33John E Laird, Allen Newell, and Paul S Rosenbloom. 1987. Soar: An architecture for general intelligence. Artificial intelligence 33, 1 (1987), 1-64.</p>
<p>Program induction by rationale generation: Learning to solve and explain algebraic word problems. Wang Ling, Dani Yogatama, Chris Dyer, Phil Blunsom, arXiv:1705.04146arXiv preprintWang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. 2017. Program induction by rationale generation: Learning to solve and explain algebraic word problems. arXiv preprint arXiv:1705.04146 (2017).</p>
<p>Improving language understanding by generative pre-training. Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. 2018. Improving language understanding by generative pre-training. (2018).</p>
<p>Nils Reimers, Iryna Gurevych, arXiv:1908.10084Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprintNils Reimers and Iryna Gurevych. 2019. Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprint arXiv:1908.10084 (2019).</p>
<p>The Sigma cognitive architecture and system: Towards functionally elegant grand unification. Abram Paul S Rosenbloom, Volkan Demski, Ustun, Journal of Artificial General Intelligence. 71Paul S Rosenbloom, Abram Demski, and Volkan Ustun. 2016. The Sigma cognitive architecture and system: Towards functionally elegant grand unification. Journal of Artificial General Intelligence 7, 1 (2016), 1.</p>
<p>Toolformer: Language models can teach themselves to use tools. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom, arXiv:2302.04761arXiv preprintTimo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer: Lan- guage models can teach themselves to use tools. arXiv preprint arXiv:2302.04761 (2023).</p>
<p>Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data. Kashun Shum, Shizhe Diao, Tong Zhang, arXiv:2302.12822cs.CLKaShun Shum, Shizhe Diao, and Tong Zhang. 2023. Automatic Prompt Aug- mentation and Selection with Chain-of-Thought from Labeled Data. (2023). arXiv:2302.12822 [cs.CL]</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Naman Baptiste Rozière, Eric Goyal, Hambro, arXiv:2302.13971Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprintHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 (2023).</p>
<p>Self-Consistency Improves Chain of Thought Reasoning in Language Models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, arXiv:2203.11171cs.CLXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023. Self-Consistency Improves Chain of Thought Reasoning in Language Models. (2023). arXiv:2203.11171 [cs.CL]</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Denny Zhou, arXiv:2203.11171arXiv preprintXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. 2022. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171 (2022).</p>
<p>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, arXiv:2201.11903cs.CLJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. (2023). arXiv:2201.11903 [cs.CL]</p>
<p>SeqZero: Few-shot Compositional Semantic Parsing with Sequential Prompts and Zero-shot Models. Jingfeng Yang, Haoming Jiang, Qingyu Yin, Danqing Zhang, Bing Yin, Diyi Yang, arXiv:2205.07381cs.CLJingfeng Yang, Haoming Jiang, Qingyu Yin, Danqing Zhang, Bing Yin, and Diyi Yang. 2022. SeqZero: Few-shot Compositional Semantic Parsing with Sequential Prompts and Zero-shot Models. (2022). arXiv:2205.07381 [cs.CL]</p>
<p>Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, arXiv:2210.03629React: Synergizing reasoning and acting in language models. arXiv preprintShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2022. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629 (2022).</p>
<p>STaR: Bootstrapping Reasoning With Reasoning. Eric Zelikman, Yuhuai Wu, Jesse Mu, Noah D Goodman, arXiv:2203.14465cs.LGEric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D. Goodman. 2022. STaR: Boot- strapping Reasoning With Reasoning. (2022). arXiv:2203.14465 [cs.LG]</p>
<p>Automatic Chain of Thought Prompting in Large Language Models. Zhuosheng Zhang, Aston Zhang, Mu Li, Alex Smola, arXiv:2210.03493cs.CLZhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. 2022. Auto- matic Chain of Thought Prompting in Large Language Models. (2022). arXiv:2210.03493 [cs.CL]</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5355 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5355</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5355</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-111.html">extraction-schema-111</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <p><strong>Paper ID:</strong> paper-4181af9b5eb2135c39b6a3e9d4acd09977a2bfa6</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/4181af9b5eb2135c39b6a3e9d4acd09977a2bfa6" target="_blank">Graph Pre-training for AMR Parsing and Generation</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> This work investigates graph self-supervised training to improve the structure awareness of PLMs over AMR graphs and introduces two graph auto-encoding strategies for graph-to-graph pre-training and four tasks to integrate text and graph information during pre- training.</p>
                <p><strong>Paper Abstract:</strong> Abstract meaning representation (AMR) highlights the core semantic information of text in a graph structure.Recently, pre-trained language models (PLMs) have advanced tasks of AMR parsing and AMR-to-text generation, respectively.However, PLMs are typically pre-trained on textual data, thus are sub-optimal for modeling structural knowledge.To this end, we investigate graph self-supervised training to improve the structure awareness of PLMs over AMR graphs.In particular, we introduce two graph auto-encoding strategies for graph-to-graph pre-training and four tasks to integrate text and graph information during pre-training.We further design a unified framework to bridge the gap between pre-training and fine-tuning tasks.Experiments on both AMR parsing and AMR-to-text generation show the superiority of our model.To our knowledge, we are the first to consider pre-training on semantic graphs.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5355.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5355.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DFS linearization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Depth-First Search (DFS) linearization of AMR</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A parenthesized sequence serialization of AMR graphs produced by a depth-first traversal (DFS), with special co-reference tokens and an expanded vocabulary of relation and frame labels so that AMR graphs can be fed into sequence models (e.g., BART). Used as the primary graph->text representation for pretraining and finetuning in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Neural AMR: Sequence-to-sequence models for parsing and generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>DFS-based linearization (parenthesized sequence with co-reference tokens)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>AMR graphs are converted to a flat token sequence by performing a depth-first search over the graph and emitting parentheses, concept/frame tokens and role labels (e.g. '( <Z0> possible :domain ( <Z1> go :arg0 ( <Z2> boy ) ) :polarity ( <Z3> negative ) )'). Co-referent nodes are represented using special indexed tokens (<Z0>, <Z1>, ...). The model vocabulary is extended with AMR relations and frames, and graph sequences are delimited with special markers (<g> ... </g>) to distinguish them from text.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Abstract Meaning Representation (AMR)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Faithful to AMR structure when augmented with co-reference tokens and relation/frame vocabulary; compact linear sequence compatible with seq2seq pretrained LMs; requires vocabulary extension for frames/relations and special tokens for co-reference/reentrancy; expressivity matches AMR (with explicit markers) but relies on ordering heuristics (DFS) which imposes a traversal-dependent linear order; interpretability is high (it is a readable PENMAN-like bracketed form).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>AMR parsing (text->AMR) and AMR-to-text generation (AMR->text) on standard AMR benchmarks (AMR2.0, AMR3.0, New3, TLP, Bio).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Parsing: Smatch. Generation: BLEU, CHRF++ (CH.), METEOR (MET.). Key reported results (this paper, with DFS linearization + graph pretraining + unified framework): AMR2.0 parsing Smatch = 83.6 (base), 85.4 (large); AMR2.0 generation BLEU = 46.6 (base), 49.8 (large). Baseline (BART fine-tuned) comparisons: baseline Smatch 82.7, baseline BLEU 42.5-42.7. Ablations: removing node/edge masking -> parsing 83.4, gen BLEU 45.1; removing subgraph masking -> parsing 83.1, gen BLEU 44.7. Using 200k silver data for denoising pretraining vs fine-tuning: BART baseline +silver (fine-tune) -> Smatch 82.6 / BLEU 44.9; +silver (denoising pretraining) -> Smatch 83.6 / BLEU 45.6 (validation numbers).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Compared to PENMAN linearization (used by some prior works) and graph-to-sequence (graph encoder) models, the DFS linearization + pretrained seq2seq LM (BART) with added graph pretraining outperforms previous methods. Specifically, this paper's DFS-based approach with graph pretraining surpasses a vanilla BART baseline (Bevilacqua et al. 2021) and other graph-to-sequence and GPT-based linearization systems (e.g., Mager et al. 2020). Example comparisons: Mager (PENMAN + GPT) BLEU ≈ 33.0 (AMR2.0); Bevilacqua (BART fine-tune) BLEU ≈ 42.7 and Smatch ≈ 82.7; this paper achieves BLEU up to 49.8 and Smatch up to 85.4 with DFS linearization plus graph-aware pretraining. The paper also shows that joint text+graph denoising pretraining yields larger gains than simply fine-tuning on silver linearized data.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Requires vocabulary expansion and special tokens for co-reference / reentrancy handling; linearization imposes an arbitrary traversal order (DFS) which may affect model learning and can obscure graph symmetries; potential for information/structural nuance loss unless careful markers are used (e.g., disambiguating senses and reentrancies); initial gap between standard text pretraining and graph->text fine-tuning formats (addressed here via a unified tg input format); graphs with many reentrancies and very deep graphs remain challenging (performance gains shrink for extreme cases); sensitivity of simple fine-tuning on noisy (silver) linearized data is higher than denoising-style pretraining, so data quality still matters.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph Pre-training for AMR Parsing and Generation', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5355.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5355.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PENMAN linearization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PENMAN notation linearization of AMR</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An alternative parenthesized textual serialization/notation for AMR (PENMAN format) used by prior works to feed AMR graphs into language models (e.g., GPT-based systems). It lists concepts and relation labels in a readable bracketed format.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>GPT-too: A language-model-first approach for AMR-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>PENMAN linearization (AMR in PENMAN bracketed notation)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>AMR graphs are written in PENMAN bracketed notation (a parenthesized human-readable form that records node concepts and role labels). That serialized text is used as the input sequence to language models without an explicit graph encoder; models learn to map PENMAN sequences to text (or vice versa).</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Abstract Meaning Representation (AMR)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Readable, human-friendly serialization preserving explicit role labels and concept entries; compatible with LMs without graph encoders; does not require specialized graph encoders but may still require tokens for concept senses and role labels; order depends on chosen printing convention (not inherently canonical).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>AMR-to-text generation (used in prior work cited here).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported in cited prior work (Mager et al. 2020): BLEU ≈ 33.0 on AMR2.0 (from this paper's comparative table). Other PENMAN-based approaches exist; in this paper PENMAN is mentioned as an alternative to DFS-based linearization.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Mentioned as an alternative linearization used by prior LM-first systems (e.g., GPT-too). In the comparisons reported here, PENMAN/GPT-based systems underperform compared to pretrained BART-based systems with DFS linearization and graph-aware pretraining. The paper cites that different linearizations have been used (PENMAN vs DFS) and that their unified DFS+pretraining approach outperforms PENMAN-based baselines in reported metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Ordering conventions and representation choices can affect model learning; PENMAN-based fine-tuning without graph-aware denoising pretraining may be less effective than combining linearization with pretraining that explicitly models graph structure; like DFS linearization, PENMAN needs mechanisms to handle reentrancies/co-reference if the LM is to reconstruct full graph structures.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph Pre-training for AMR Parsing and Generation', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5355.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5355.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for converting graphs into text for language model training, including details of the representation, properties, evaluation tasks, performance, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Graph-to-sequence (graph encoder)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph-to-sequence models using explicit graph encoders (GNN/graph-aware Transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Approaches that do not linearize graphs but instead encode the graph structure with a graph encoder (e.g., gated GNNs, graph-enhanced Transformers) and decode to text with a sequence decoder; these preserve relational structure explicitly.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Graph-to-sequence learning using gated graph neural networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Graph-to-sequence with graph encoder</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>AMR graphs are kept in structured form and processed by a graph encoder (e.g., gated graph neural networks, graph-aware self-attention), which produces node/edge-aware representations that a sequence decoder uses to generate text. This avoids full serialization but requires a model component designed for graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Abstract Meaning Representation (AMR) (or other semantic/knowledge graphs)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_properties</strong></td>
                            <td>Preserves explicit relational structure and potentially handles reentrancies and cycles more naturally; may better capture multi-relational dependencies; less directly compatible with off-the-shelf pretrained seq2seq LMs unless adapters or fusion mechanisms are used; typically requires more architectural changes and cannot directly leverage text-only pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>AMR-to-text generation and AMR parsing in prior literature.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Earlier graph-to-sequence systems reported lower BLEU/Smatch than PLM-based seq2seq linearization baselines in the comparisons summarized by this paper (examples: BLEU ≈ 31.8–34.2 for some graph-to-sequence systems on AMR2.0 in Table 6).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_representations</strong></td>
                            <td>Paper positions graph-to-sequence as an alternative class: while graph encoders better model structure, current state-of-the-art favors seq2seq approaches that linearize graphs and use large pretrained LMs (especially when augmented with graph-aware pretraining as in this paper). The paper's results show that seq2seq linearization + graph-aware pretraining (BART-based) outperforms the graph-encoder methods listed in their comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Integration with large text-pretrained LMs is non-trivial and often requires architectural adapters; may need large supervised data to train graph encoders effectively; comparatively lower performance in some reported benchmarks vs PLM-based linearization methods (per the comparisons here).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph Pre-training for AMR Parsing and Generation', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Neural AMR: Sequence-to-sequence models for parsing and generation <em>(Rating: 2)</em></li>
                <li>GPT-too: A language-model-first approach for AMR-to-text generation <em>(Rating: 2)</em></li>
                <li>One spring to rule them both: Symmetric AMR semantic parsing and generation without a complex pipeline <em>(Rating: 2)</em></li>
                <li>Investigating pretrained language models for graph-to-text generation <em>(Rating: 2)</em></li>
                <li>Modeling graph structure in transformer for better AMR-to-text generation <em>(Rating: 1)</em></li>
                <li>Graph-to-sequence learning using gated graph neural networks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5355",
    "paper_id": "paper-4181af9b5eb2135c39b6a3e9d4acd09977a2bfa6",
    "extraction_schema_id": "extraction-schema-111",
    "extracted_data": [
        {
            "name_short": "DFS linearization",
            "name_full": "Depth-First Search (DFS) linearization of AMR",
            "brief_description": "A parenthesized sequence serialization of AMR graphs produced by a depth-first traversal (DFS), with special co-reference tokens and an expanded vocabulary of relation and frame labels so that AMR graphs can be fed into sequence models (e.g., BART). Used as the primary graph-&gt;text representation for pretraining and finetuning in this paper.",
            "citation_title": "Neural AMR: Sequence-to-sequence models for parsing and generation",
            "mention_or_use": "use",
            "representation_name": "DFS-based linearization (parenthesized sequence with co-reference tokens)",
            "representation_description": "AMR graphs are converted to a flat token sequence by performing a depth-first search over the graph and emitting parentheses, concept/frame tokens and role labels (e.g. '( &lt;Z0&gt; possible :domain ( &lt;Z1&gt; go :arg0 ( &lt;Z2&gt; boy ) ) :polarity ( &lt;Z3&gt; negative ) )'). Co-referent nodes are represented using special indexed tokens (&lt;Z0&gt;, &lt;Z1&gt;, ...). The model vocabulary is extended with AMR relations and frames, and graph sequences are delimited with special markers (&lt;g&gt; ... &lt;/g&gt;) to distinguish them from text.",
            "graph_type": "Abstract Meaning Representation (AMR)",
            "representation_properties": "Faithful to AMR structure when augmented with co-reference tokens and relation/frame vocabulary; compact linear sequence compatible with seq2seq pretrained LMs; requires vocabulary extension for frames/relations and special tokens for co-reference/reentrancy; expressivity matches AMR (with explicit markers) but relies on ordering heuristics (DFS) which imposes a traversal-dependent linear order; interpretability is high (it is a readable PENMAN-like bracketed form).",
            "evaluation_task": "AMR parsing (text-&gt;AMR) and AMR-to-text generation (AMR-&gt;text) on standard AMR benchmarks (AMR2.0, AMR3.0, New3, TLP, Bio).",
            "performance_metrics": "Parsing: Smatch. Generation: BLEU, CHRF++ (CH.), METEOR (MET.). Key reported results (this paper, with DFS linearization + graph pretraining + unified framework): AMR2.0 parsing Smatch = 83.6 (base), 85.4 (large); AMR2.0 generation BLEU = 46.6 (base), 49.8 (large). Baseline (BART fine-tuned) comparisons: baseline Smatch 82.7, baseline BLEU 42.5-42.7. Ablations: removing node/edge masking -&gt; parsing 83.4, gen BLEU 45.1; removing subgraph masking -&gt; parsing 83.1, gen BLEU 44.7. Using 200k silver data for denoising pretraining vs fine-tuning: BART baseline +silver (fine-tune) -&gt; Smatch 82.6 / BLEU 44.9; +silver (denoising pretraining) -&gt; Smatch 83.6 / BLEU 45.6 (validation numbers).",
            "comparison_to_other_representations": "Compared to PENMAN linearization (used by some prior works) and graph-to-sequence (graph encoder) models, the DFS linearization + pretrained seq2seq LM (BART) with added graph pretraining outperforms previous methods. Specifically, this paper's DFS-based approach with graph pretraining surpasses a vanilla BART baseline (Bevilacqua et al. 2021) and other graph-to-sequence and GPT-based linearization systems (e.g., Mager et al. 2020). Example comparisons: Mager (PENMAN + GPT) BLEU ≈ 33.0 (AMR2.0); Bevilacqua (BART fine-tune) BLEU ≈ 42.7 and Smatch ≈ 82.7; this paper achieves BLEU up to 49.8 and Smatch up to 85.4 with DFS linearization plus graph-aware pretraining. The paper also shows that joint text+graph denoising pretraining yields larger gains than simply fine-tuning on silver linearized data.",
            "limitations_or_challenges": "Requires vocabulary expansion and special tokens for co-reference / reentrancy handling; linearization imposes an arbitrary traversal order (DFS) which may affect model learning and can obscure graph symmetries; potential for information/structural nuance loss unless careful markers are used (e.g., disambiguating senses and reentrancies); initial gap between standard text pretraining and graph-&gt;text fine-tuning formats (addressed here via a unified tg input format); graphs with many reentrancies and very deep graphs remain challenging (performance gains shrink for extreme cases); sensitivity of simple fine-tuning on noisy (silver) linearized data is higher than denoising-style pretraining, so data quality still matters.",
            "uuid": "e5355.0",
            "source_info": {
                "paper_title": "Graph Pre-training for AMR Parsing and Generation",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "PENMAN linearization",
            "name_full": "PENMAN notation linearization of AMR",
            "brief_description": "An alternative parenthesized textual serialization/notation for AMR (PENMAN format) used by prior works to feed AMR graphs into language models (e.g., GPT-based systems). It lists concepts and relation labels in a readable bracketed format.",
            "citation_title": "GPT-too: A language-model-first approach for AMR-to-text generation",
            "mention_or_use": "mention",
            "representation_name": "PENMAN linearization (AMR in PENMAN bracketed notation)",
            "representation_description": "AMR graphs are written in PENMAN bracketed notation (a parenthesized human-readable form that records node concepts and role labels). That serialized text is used as the input sequence to language models without an explicit graph encoder; models learn to map PENMAN sequences to text (or vice versa).",
            "graph_type": "Abstract Meaning Representation (AMR)",
            "representation_properties": "Readable, human-friendly serialization preserving explicit role labels and concept entries; compatible with LMs without graph encoders; does not require specialized graph encoders but may still require tokens for concept senses and role labels; order depends on chosen printing convention (not inherently canonical).",
            "evaluation_task": "AMR-to-text generation (used in prior work cited here).",
            "performance_metrics": "Reported in cited prior work (Mager et al. 2020): BLEU ≈ 33.0 on AMR2.0 (from this paper's comparative table). Other PENMAN-based approaches exist; in this paper PENMAN is mentioned as an alternative to DFS-based linearization.",
            "comparison_to_other_representations": "Mentioned as an alternative linearization used by prior LM-first systems (e.g., GPT-too). In the comparisons reported here, PENMAN/GPT-based systems underperform compared to pretrained BART-based systems with DFS linearization and graph-aware pretraining. The paper cites that different linearizations have been used (PENMAN vs DFS) and that their unified DFS+pretraining approach outperforms PENMAN-based baselines in reported metrics.",
            "limitations_or_challenges": "Ordering conventions and representation choices can affect model learning; PENMAN-based fine-tuning without graph-aware denoising pretraining may be less effective than combining linearization with pretraining that explicitly models graph structure; like DFS linearization, PENMAN needs mechanisms to handle reentrancies/co-reference if the LM is to reconstruct full graph structures.",
            "uuid": "e5355.1",
            "source_info": {
                "paper_title": "Graph Pre-training for AMR Parsing and Generation",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "Graph-to-sequence (graph encoder)",
            "name_full": "Graph-to-sequence models using explicit graph encoders (GNN/graph-aware Transformer)",
            "brief_description": "Approaches that do not linearize graphs but instead encode the graph structure with a graph encoder (e.g., gated GNNs, graph-enhanced Transformers) and decode to text with a sequence decoder; these preserve relational structure explicitly.",
            "citation_title": "Graph-to-sequence learning using gated graph neural networks",
            "mention_or_use": "mention",
            "representation_name": "Graph-to-sequence with graph encoder",
            "representation_description": "AMR graphs are kept in structured form and processed by a graph encoder (e.g., gated graph neural networks, graph-aware self-attention), which produces node/edge-aware representations that a sequence decoder uses to generate text. This avoids full serialization but requires a model component designed for graphs.",
            "graph_type": "Abstract Meaning Representation (AMR) (or other semantic/knowledge graphs)",
            "representation_properties": "Preserves explicit relational structure and potentially handles reentrancies and cycles more naturally; may better capture multi-relational dependencies; less directly compatible with off-the-shelf pretrained seq2seq LMs unless adapters or fusion mechanisms are used; typically requires more architectural changes and cannot directly leverage text-only pretraining.",
            "evaluation_task": "AMR-to-text generation and AMR parsing in prior literature.",
            "performance_metrics": "Earlier graph-to-sequence systems reported lower BLEU/Smatch than PLM-based seq2seq linearization baselines in the comparisons summarized by this paper (examples: BLEU ≈ 31.8–34.2 for some graph-to-sequence systems on AMR2.0 in Table 6).",
            "comparison_to_other_representations": "Paper positions graph-to-sequence as an alternative class: while graph encoders better model structure, current state-of-the-art favors seq2seq approaches that linearize graphs and use large pretrained LMs (especially when augmented with graph-aware pretraining as in this paper). The paper's results show that seq2seq linearization + graph-aware pretraining (BART-based) outperforms the graph-encoder methods listed in their comparisons.",
            "limitations_or_challenges": "Integration with large text-pretrained LMs is non-trivial and often requires architectural adapters; may need large supervised data to train graph encoders effectively; comparatively lower performance in some reported benchmarks vs PLM-based linearization methods (per the comparisons here).",
            "uuid": "e5355.2",
            "source_info": {
                "paper_title": "Graph Pre-training for AMR Parsing and Generation",
                "publication_date_yy_mm": "2022-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Neural AMR: Sequence-to-sequence models for parsing and generation",
            "rating": 2
        },
        {
            "paper_title": "GPT-too: A language-model-first approach for AMR-to-text generation",
            "rating": 2
        },
        {
            "paper_title": "One spring to rule them both: Symmetric AMR semantic parsing and generation without a complex pipeline",
            "rating": 2
        },
        {
            "paper_title": "Investigating pretrained language models for graph-to-text generation",
            "rating": 2
        },
        {
            "paper_title": "Modeling graph structure in transformer for better AMR-to-text generation",
            "rating": 1
        },
        {
            "paper_title": "Graph-to-sequence learning using gated graph neural networks",
            "rating": 1
        }
    ],
    "cost": 0.014786,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Graph Pre-training for AMR Parsing and Generation</h1>
<p>Xuefeng Bai ${ }^{\text {® }}$, Yulong Chen ${ }^{\text {® }}$, Yue Zhang ${ }^{\text {® }}$<br>$\cdot$Zhejiang University, China<br>${ }^{\circ}$ School of Engineering, Westlake University, China<br>${ }^{\text {® }}$ Institute of Advanced Technology, Westlake Institute for Advanced Study, China</p>
<h4>Abstract</h4>
<p>Abstract meaning representation (AMR) highlights the core semantic information of text in a graph structure. Recently, pre-trained language models (PLMs) have advanced tasks of AMR parsing and AMR-to-text generation, respectively. However, PLMs are typically pretrained on textual data, thus are sub-optimal for modeling structural knowledge. To this end, we investigate graph self-supervised training to improve the structure awareness of PLMs over AMR graphs. In particular, we introduce two graph auto-encoding strategies for graph-to-graph pre-training and four tasks to integrate text and graph information during pre-training. We further design a unified framework to bridge the gap between pre-training and fine-tuning tasks. Experiments on both AMR parsing and AMR-to-text generation show the superiority of our model. To our knowledge, we are the first to consider pre-training on semantic graphs.</p>
<h2>1 Introduction</h2>
<p>Abstract meaning representation (AMR; Banarescu et al. (2013)) is a semantic structure formalism. It represents the meaning of a text in a rooted directed graph, where nodes represent basic semantic units such as entities and predicates, and edges represent their semantic relations, respectively. One example is shown in Figure 1(a), with the corresponding sentence in Figure 1(b). Serving as a structural representation, AMR has been shown useful for NLP tasks such as text summarization (Liu et al., 2015; Liao et al., 2018; Chen et al., 2021), machine translation (Song et al., 2019), information extraction (Huang et al., 2016; Zhang and Ji, 2021) and dialogue systems (Bai et al., 2021).</p>
<p>There are two fundamental NLP tasks concerning AMR, namely AMR parsing (Flanigan et al., 2014; Konstas et al., 2017; Lyu and Titov, 2018; Guo and Lu, 2018; Zhang et al., 2019a; Cai and Lam, 2020; Bevilacqua et al., 2021) and AMR-totext generation (Konstas et al., 2017; Song et al.,
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Illustration of AMR tasks: (a) an AMR graph; (b) a corresponding sentence.</p>
<p>2018; Zhu et al., 2019; Zhao et al., 2020; Bai et al., 2020; Ribeiro et al., 2021a). As shown in Figure 1, the former transforms a textual input (e.g., a sentence) into a corresponding AMR structure, and the latter transforms an AMR input into a fluent and grammatical sentence that conveys the same meaning. A common challenge to both tasks is that AMR exists in the form of a graph structure, which is difficult for neural models to learn with limited human-curated data.</p>
<p>Recently, large-scale pre-trained sequence-tosequence (seq2seq) language models (Lewis et al., 2020; Raffel et al., 2020) have been shown useful for both tasks above. The basic idea is to linearize AMR structures into a sequence form, so that both AMR parsing and AMR-to-text generation can be solved as standard seq2seq tasks, using a pre-trained language model fine-tuned on taskspecific data. In this way, semantic knowledge learned in self-supervised text-to-text ( $t 2 t$ ) pretraining can benefit both text-to-graph ( $t 2 g$ ) and graph-to-text ( $g 2 t$ ) transformation.</p>
<p>Intuitively, structural knowledge from AMR can be a useful complement to semantic knowledge from text. A natural question is whether similar self-supervision strategy can be useful for AMR graphs, so that graph-to-graph ( $g 2 g$ ) denoise autoencoder training can serve as effective addition to $t 2 t$ pre-training, before a model is fine-tuned on $t 2 g$ and $g 2 t$ tasks. We investigate this problem in this paper. In particular, there are three questions of interest. First, as mentioned before, is $g 2 g$</p>
<p>pre-training complementary to t2t pre-training? Second, what is the most effective way to combine t2t and g2g training? Third, is silver data useful for AMR self-supervision training, and what is the most effective way of making use of such data?</p>
<p>Taking BART (Lewis et al., 2020) as the seq-to-seq model, we introduce two strategies for g2g pre-training and propose four tasks to combine t 2 t and g 2 g training. To reduce the gap among different pre-training tasks and between pre-training and fine-tuing, we unify all pre-training tasks and fine-tuning tasks in a general framework. Experimental results on standard benchmarks show that: 1) graph pre-training achieves significant improvements over the state-of-the-art systems; 2) silver data are useful for our pre-training framework; 3) our pre-training framework is a better way than finetuning to make use of silver data and; 4) our model is more robust than existing systems in unseen domains. Our final models give the best reported results on both AMR parsing and AMR-to-text generation tasks, with a large margin of improvement over the previous best results. To our knowledge, we are the first to consider graph-to-graph selfsupervised training on semantic graphs. We release code at https://github.com/muyeby/AMRBART.</p>
<h2>2 Related Work</h2>
<p>AMR Parsing. Early AMR parsing systems use statistical methods (Flanigan et al., 2014, 2016; Wang et al., 2015a,b). With the advance in deep learning, various neural models are developed for AMR parsing. Those models can be categorized into: 1) neural transition-based parsers (Ballesteros and Al-Onaizan, 2017; Liu et al., 2018; Fernandez Astudillo et al., 2020; Zhou et al., 2021); 2) sequence-to-graph parsers (Zhang et al., 2019a; Lyu et al., 2020; Cai and Lam, 2020) and; 3) sequence-to-sequence parsers (Konstas et al., 2017; Peng et al., 2017, 2018; Zhang et al., 2019b; Xu et al., 2020; Bevilacqua et al., 2021). Recently, pretraining techniques have significantly boosted the performance of AMR parsing. For example, Lyu and Titov (2018), Zhang et al. (2019a,b) and Cai and Lam (2020) use BERT (Devlin et al., 2019) for sentence encoding; Bevilacqua et al. (2021) finetune BART for sequence-to-AMR generation. Xu et al. (2020) pre-train a model on relevant seq2seq learning tasks (e.g., machine translation (Bahdanau et al., 2015), syntactic parsing (Zhu et al., 2013)) before fine-tuning on AMR parsing. Similar to
those methods, we consider using pre-trained models to improve the model capacity. However, previous studies focus on fine-tuning language models trained on text data for AMR parsing task, in contract, we focus on integrating structural information into the pre-training. In addition, our method does not require information from auxiliary tasks.
AMR-to-Text Generation. On a coarse-grained level, we categorize existing AMR-to-text generation approaches into two main classes: Graph-to-sequence models that adopt a graph encoder to process an AMR graph and use a sequence decoder for generation (Beck et al., 2018; Damonte and Cohen, 2019; Zhu et al., 2019), and sequence-tosequence models that linearize an AMR graph into a sequence and solve it as a seq2seq problem using randomly initialized (Konstas et al., 2017) or pretrained models (Mager et al., 2020; Ribeiro et al., 2021a; Bevilacqua et al., 2021). This work follows a seq2seq manner, but we use an encoder that integrates AMR and text information. The closest to our work, Ribeiro et al. (2021b) integrate AMR structures into pre-trained T5 (Raffel et al., 2020) using adapters (Houlsby et al., 2019) for AMR-totext generation. However, they do not pre-train on AMR graphs, and their method cannot solve both AMR parsing and AMR-to-text generation tasks as they require the full AMR structure as the input.
Graph Self-supervised Learning. Kipf and Welling (2016) introduce a variational graph autoencoder to allow self-supervised learning on graph data. Hu et al. (2020a,b) propose local and global learning strategies to pre-train a graph neural network on large-scale protein ego-networks, academic graphs and recommendation data. Lu et al. (2021) enhance the graph learning strategies of Hu et al. (2020b) with dual adaptations. While existing work considers graph neural networks, we pre-train a seq2seq model on AMR graphs. In addition, we jointly pre-train on graphs and text for graph-text correlation modeling. In contrast, existing work pre-trains models on graphs and in isolation with text pre-training. To our knowledge, we are the first to consider AMR as a graph pre-training target.</p>
<h2>3 Method</h2>
<p>We take BART (Lewis et al., 2020) as the basic seq2seq model (Section 3.1), and introduce graph pre-training strategies (Section 3.2) and an unified pre-training framework (Section 3.3) for both AMR parsing and AMR-to-text generation.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Illustration of two graph pre-training strategies: 1) node/edge level denoising (a→ b); 2) sub-graph level denoising (c→ b). Two transformations can be composed.</p>
<h3>3.1 BART</h3>
<p>BART (Lewis et al., 2020) is a pre-trained denoising auto-encoder, which is implemented as a seq2seq model based on standard Transformer (Vaswani et al., 2017) architecture. Typically, BART is trained to reconstruct original text based on a corrupted text generated by 5 noising functions: 1) Token Masking. Tokens are randomly replaced by [mask] elements; 2) Token Deletion. Tokens are randomly deleted from the input; 3) Text Infilling. Text spans are randomly replaced by a single [mask] token; 4) Sentence Permutation. Text is divided into segments and then shuffled; 5) Document Rotation. A document is rotated to start with a random token. In the fine-tuning, BART takes a complete text as input and maps it into a task-specific output sequence.</p>
<p>We linearize an AMR graph into a sequence, so that both AMR parsing and AMR-to-text generation can be performed using a seq2seq model. In addition, it allows pre-training on AMR structures using BART. Following Konstas et al. (2017), we adopt the depth-first search (DFS) algorithm, which is closely related to the linearized natural language syntactic trees (Bevilacqua et al., 2021). For instance, the AMR graph in Figure 1 is linearized into: ( <Z0> possible :domain ( <Z1> go :arg0 ( <Z2> boy ) ) :polarity ( <Z3> negative ) ), where <Z0>, <Z1> and <Z2> are special tokens to handle co-referring nodes. To deal with such AMR symbols, we follow previous work (Bevilacqua et al., 2021) and expand the vocabulary by adding all relations and frames. In addition, to distinguish between texts and AMR graphs, we add two special tokens, <g> and </g>, to mark the beginning and end of AMR graphs, respectively.</p>
<h3>3.2 Pre-training on AMR graphs</h3>
<p>We introduce two self-supervised training strategies to further pre-train a BART model on AMR graphs. As shown in Figure 2(a), the node/edge level denoising strategy encourages the model to capture local knowledge about nodes and edges. The graph level denoising strategy (Figure 2(c)) enforces the model to predict a sub-graph, thus facilitating the graph-level learning.</p>
<p>1) Node/edge level denoising. We apply a noise function on AMR nodes/edges to construct a noisy input graph. In particular, the noise function is implemented by masking 15% nodes and 15% edges in each graph. As shown in Figure 2(a), the node [go-01] and edge [:arg0] are replaced with two [mask] tokens.</p>
<p>2) Sub-graph level denoising. This task aims to recover the complete graph when given part of the graph. We randomly remove a sub-graph<sup>1</sup> from the graph and replace it with a [mask] token (cf. Figure 2(c)). The masking probability is 0.35.</p>
<h3>3.3 Unified Pre-training Framework</h3>
<p>The above standard pre-training and fine-tuning strategies are shown in Table 1(a), by using <s> and <g> for differentiating text and graphic information, respectively. However, the model does not fully learn the interaction between textual and AMR information during pre-training. To further address this issue, we consider a unified pre-training framework, which combines text and AMR sequences as input to the denoise auto-encoder. In this way, dynamic masking can be carried out on the text, AMR or both ends, so that the model can learn to make use of one source of information for inferring the other. This can benefit both a parser and a generation model by enforcing the learning of correspondence between text and AMR structures.</p>
<p>In addition, as shown in Table 1, there is a gap between standard pre-training and fine-tuning for AMR from/to text transduction. Specifically, the input and output formats are same in the pre-training (i.e., ť2t and ģ2g) but different in the fine-tuning</p>
<p><sup>1</sup>We define a sub-graph has at least one edge and one node.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Phase</th>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Input</th>
<th style="text-align: center;">Output</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">(a)</td>
<td style="text-align: center;">Std. P.T.</td>
<td style="text-align: center;">$\begin{aligned} &amp; \tilde{t} 2 t \ &amp; \tilde{g} 2 g \ &amp; \text { g2t } \ &amp; t 2 g \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; <s>x_{1}, . .[\text { mask }] \ldots, x_{n}&lt;/ s&gt; \ &amp; <g>g_{1}, \ldots[\text { mask }] \ldots, g_{m}&lt;/ g&gt; \ &amp; <s>g_{1}, g_{2}, \ldots, g_{m}&lt;/ g&gt; \ &amp; <s>x_{1}, x_{2}, \ldots, x_{n}&lt;/ s&gt; \ &amp; <g>g_{1}, g_{2}, \ldots, g_{m}&lt;/ g&gt; \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;">(b)</td>
<td style="text-align: center;">Unified P.T.</td>
<td style="text-align: center;">$\begin{aligned} &amp; \tilde{t} \tilde{g} 2 t \ &amp; \tilde{t} \tilde{g} 2 g \ &amp; \tilde{t} g 2 t \ &amp; t g 2 t \ &amp; \tilde{t} g 2 t \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; <s>x_{1}, . .[\text { mask }] \ldots, x_{n}&lt;/ s&gt;<g>[\text { mask }]</g> \ &amp; <s>[\text { mask }]</s><g>g_{1}, . .[\text { mask }] \ldots, g_{m}&lt;/ g&gt; \ &amp; <s>x_{1}, . .[\text { mask }] \ldots, x_{n}&lt;/ s&gt;<g>g_{1}, g_{2}, \ldots, g_{m}&lt;/ g&gt; \ &amp; <s>x_{1}, x_{2}, \ldots, x_{n}&lt;/ s&gt;<g>g_{1}, . .[\text { mask }] \ldots, g_{m}&lt;/ g&gt; \ &amp; <s>x_{1}, . .[\text { mask }] \ldots, x_{n}&lt;/ s&gt;<g>g_{1}, . .[\text { mask }] \ldots, g_{m}&lt;/ g&gt; \ &amp; \tilde{t} \tilde{g} 2 t \ &amp; \tilde{t} \tilde{g} 2 g \ &amp; \tilde{t} g 2 t \ &amp; t \tilde{g} 2 g \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Unified F.T.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\begin{aligned} &amp; <s>[t g 2 t \ &amp; <s>x_{1}, x_{2}, \ldots, x_{n}&lt;/ s&gt;<g>[m a s k]</g> \end{aligned}$</td>
</tr>
</tbody>
</table>
<p>Table 1: Different pre-training and fine-tuning strategies. P.T. $=$ pre-training, F.T. $=$ fine-tuning. $t / g$ denotes the original text/graph. $\tilde{t} / \tilde{g}$ represents a noisy text/graph. $\tilde{t} / \bar{g}$ means an empty text/graph.
(i.e., t 2 g and g 2 t ). This gap restrains models to make the best use of pre-trained knowledge in the fine-tuning phase. The unified pre-training framework can also benefit task-specific fine-tuning by eliminating the difference of input and output formats between pre-training and fine-tuning.</p>
<p>Formally, denoting the text and linearized graph sequence as $t$ and $g$, where $t=\left{x_{1}, x_{2}, \ldots, x_{n}\right}$ and $g=\left{g_{1}, g_{2}, \ldots, g_{n}\right} . \tilde{t}$ and $\tilde{g}$ represent the noisy text and graph, respectively, and $\bar{t}$ and $\bar{g}$ refer to the empty text and graph, respectively. As shown in Table 1(b), we unify the input format for both pre-training and fine-tuning to tg. For consistency, all input sequences start with a text sequence and end with a graph sequence.
Joint Text and Graph Pre-training. We introduce 4 auxiliary pre-training tasks to encourage information exchanging between graphs and text. As shown in Table 1(b), the auxiliary tasks are:</p>
<p>1) Graph augmented text denoising ( $\tilde{t} g 2 t$ ), where an AMR graph is taken as additional input to help masked text reconstruction;
2) Text augmented graph denoising ( $\mathrm{t} \tilde{g} 2 \mathrm{~g}$ ), where text helps masked graph reconstruction;
3) Noisy graph augmented text denoising ( $\tilde{t} \tilde{g} 2 t$ ), where the target text is generated based on a pair of masked text and masked graph;
4) Noisy text augmented graph denoising ( $\tilde{t} \tilde{g} 2 g$ ), where a target graph is generated based on a pair of masked text and masked graph.
Dynamic masking rate. Different from standard masking (Devlin et al., 2019) that uses a static masking rate, we adopt a dynamic masking rate $p$ for task $\tilde{t} g 2 t$ and $\operatorname{tg} 2 g$. Formally, at step $t$, we calculate the masking probability $p$ as:</p>
<p>$$
p=0.1+0.75 * t / T
$$</p>
<p>where 0.1 is the initial masking rate, $T$ denotes the total training step. $p$ increases as $t$ grows, as $t$ approaches to $T$, the pre-training tasks $\tilde{t} g 2 t$ and $\mathrm{t} \tilde{g} 2 \mathrm{~g}$ are closer to fine-tuning tasks.
Unified Pre-training and Fine-tuning. In our unified framework, fine-tuning tasks can be viewed as having an empty text/graph in the original input, resulting in an input format of $\bar{t} g 2 t$ for AMR-totext generation and $\mathrm{t} \bar{g} 2 \mathrm{~g}$ for AMR parsing. In this way, pre-training and fine-tuning tasks share the same input format, thus facilitating knowledge transfer from pre-training to fine-tuning.</p>
<h3>3.4 Training</h3>
<p>To pre-train our model, we optimize the total loss $\left(\mathcal{L}_{\text {total }}\right)$ which is calculated as:</p>
<p>$$
\begin{aligned}
\mathcal{L}<em _tilde_g="\tilde{g">{\tilde{t} 2 t} &amp; =-\log P(t \mid \tilde{t}, \bar{g}) \
\mathcal{L}</em>) \
\mathcal{L}} 2 g} &amp; =-\log P(\mathrm{~g} \mid \tilde{t}, \tilde{g<em _mathrm_t="\mathrm{t">{\tilde{t} g 2 t} &amp; =-\log P(t \mid \tilde{t}, g) \
\mathcal{L}</em>) \
\mathcal{L}} \tilde{g} 2 g} &amp; =-\log P(\mathrm{~g} \mid \mathrm{t}, \tilde{g<em _tilde_t="\tilde{t">{\tilde{t} \tilde{g} 2 t} &amp; =-\log P(\mathrm{t} \mid \tilde{t}, \tilde{g}) \
\mathcal{L}</em>) \
\mathcal{L}} \tilde{g} 2 g} &amp; =-\log P(\mathrm{~g} \mid \tilde{t}, \tilde{g<em _tilde_t="\tilde{t">{\text {total }} &amp; =\mathcal{L}</em>} 2 t}+\mathcal{L<em _tilde_t="\tilde{t">{\tilde{g} 2 g}+\mathcal{L}</em> \
&amp; +\mathcal{L}} g 2 t<em _tilde_t="\tilde{t">{\tilde{t} \tilde{g} 2 g}+\mathcal{L}</em>
\end{aligned}
$$} \tilde{g} 2 t}+\mathcal{L}_{\tilde{t} \tilde{g} 2 g</p>
<p>where $\mathcal{L}<em _tilde_g="\tilde{g">{\tilde{t} 2 t}$ and $\mathcal{L}</em>} 2 g}$ are standard pre-training loss on text (Section 3.1) and graph (Section 3.2), respectively. $\mathcal{L<em _mathrm_t="\mathrm{t">{\tilde{t} g 2 t}, \mathcal{L}</em>} \tilde{g} 2 g}, \mathcal{L<em _tilde_t="\tilde{t">{\tilde{t} \tilde{g} 2 t}$ and $\mathcal{L}</em>$ denote joint pre-training losses (Section 3.3), respectively.} \tilde{g} 2 g</p>
<p>For fine-tuning, the training objectives are:</p>
<p>$$
\begin{aligned}
&amp; \mathcal{L}<em _text="\text" _text2amr="{text2amr">{\text {amr2text }}=-\log P(\mathrm{t} \mid \tilde{\mathrm{t}}, \mathrm{~g}) \
&amp; \mathcal{L}</em>)
\end{aligned}
$$}}=-\log P(\mathrm{~g} \mid \mathrm{t}, \overline{\mathrm{g}</p>
<p>where $\mathcal{L}<em _text="\text" _text2amr="{text2amr">{\text {amr2text }}$ and $\mathcal{L}</em>$ are training loss of AMR-to-text generation and AMR parsing, respectively.}</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Datasets</th>
<th style="text-align: center;">AMR2.0</th>
<th style="text-align: center;">AMR3.0</th>
<th style="text-align: center;">New3</th>
<th style="text-align: center;">TLP</th>
<th style="text-align: center;">Bio</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Train</td>
<td style="text-align: center;">36521</td>
<td style="text-align: center;">55635</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Valid</td>
<td style="text-align: center;">1368</td>
<td style="text-align: center;">1722</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Test</td>
<td style="text-align: center;">1371</td>
<td style="text-align: center;">1898</td>
<td style="text-align: center;">527</td>
<td style="text-align: center;">1562</td>
<td style="text-align: center;">500</td>
</tr>
</tbody>
</table>
<p>Table 2: Benchmark AMR datasets.</p>
<h2>4 Experiments</h2>
<p>We evaluate the effectiveness of our model on five benchmarks and compare the results with state-of-the-art models on AMR parsing and AMR-to-text generation, respectively. In addition to standard supervised training settings, we evaluate the robustness of our model in a zero-shot domain adaptation setting.</p>
<h3>4.1 Datasets</h3>
<p>Table 2 shows the statistics of datasets. Following Bevilacqua et al. (2021), we use the AMR2.0 (LDC2017T10) and AMR3.0 (LDC2020T02). We also evaluate the model performance on New3, The Little Prince (TLP) and Bio AMR (Bio) corpora. For pre-training, we additionally use 200k silver data parsed by SPRING (Bevilacqua et al., 2021). These data are randomly selected from Gigaword (LDC2011T07) corpus, which shares the same textual source with AMR data. ${ }^{2}$</p>
<h3>4.2 Settings</h3>
<p>We follow Bevilacqua et al. (2021) in preprocessing and post-processing AMR graphs, except for omitting the recategorization step which does not consistently improve model performance in our preliminary experiments. Our model is built based on a vanilla BART ${ }^{3}$. The best model and hyper-parameters are selected by performance on the validation set. The detailed hyper-parameters are given in Appendix A.
Metrics. Following Bevilacqua et al. (2021), we evaluate on the AMR parsing benchmarks by using Smatch (Cai and Knight, 2013) and other finegrained metrics. ${ }^{4}$ Regarding AMR-to-text, we use three common Natural Language Generation measures, including BLEU (Papineni et al., 2002), CHRF++ (Popović, 2017) and METEOR (Banerjee and Lavie, 2005), tokenizing with the script provided with JAMR (Flanigan et al., 2014).</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 3: AMP parsing (Smatch) and AMR-to-text generation (BLEU) performance on valid set of AMR2.0.</p>
<h3>4.3 Compared Models</h3>
<p>For AMR parsing, we consider following systems for comparison: 1) Lyu and Titov (2018; LyuT), a neural parser trained by jointly modeling alignments, concepts and relations; 2) Zhang et al. (2019b; Zhang+), a seq2seq approach that incrementally builds up an AMR via predicting semantic relations; 3) Zhou et al. (2020; Zhou+), an alignerfree parser enhanced by explicit dependency and latent structures; 4) Cai and Lam (2020a; CaiL), a graph-based parser that enhances incremental sequence-to-graph model with a graph-sequence iterative inference mechanism; 5) Bevilacqua et al. (2021; Bevilacqua+), a fine-tuned BART model that predicts a linearized AMR graph.</p>
<p>For AMR-to-text generation, the compared models are: 1) Zhu et al. (2019; Zhu+), a Transformer-based model that enhances selfattention with graph relations; 2) Zhang et al. (2020; Zhang+), a graph-to-sequence model which uses a dynamic graph convolutional networks for better graph modeling. 3) Bai et al. (2020; Bai+), a graph encoder (Zhu et al., 2019) with a structural decoder that jointly predicts the target text and the input structure; 4) Mager et al. (2020; Mager+), a fine-tuned GPT that predicts text based on a PENMAN linearized AMR graph; 5) Bevilacqua et al. (2021; Bevilacqua+), a fine-tuned BART that predicts text based on a DFS linearized AMR graph; 6) Ribeiro et al. (2021; Ribeiro+), a fine-tuned BART based on a PENMAN linearized AMR graph. For a fair comparison, we leave out models based on T5 (Ribeiro et al., 2021a,b), which has about two times more parameters than BART.</p>
<table>
<thead>
<tr>
<th>Setting</th>
<th>AMR parsing</th>
<th>AMR-to-text</th>
</tr>
</thead>
<tbody>
<tr>
<td>Full Model</td>
<td>83.6</td>
<td>45.6</td>
</tr>
<tr>
<td>- Node/edge masking</td>
<td>83.4</td>
<td>45.1</td>
</tr>
<tr>
<td>- Sub-graph masking</td>
<td>83.1</td>
<td>44.7</td>
</tr>
</tbody>
</table>
<p>Table 4: Impact of two masking strategies.</p>
<h3>4.4 Development Experiments</h3>
<p>Table 3 shows results on the validation set of AMR2.0 under different model settings, where we take a fine-tuned BART-based model (Bevilacqua et al., 2021) as our baseline.</p>
<p>We first study the effectiveness of pre-training only on text and graphs. As shown in Table 3, both pre-training on the text (t̄g2t) and graph (t̄g2g) leads to better results, and combining them can give better results on both tasks. Also, adding joint pre-training tasks improves the performance. In particular, tg2g gives a Smatch improvement of 0.7 for AMR paring, and tg2t reaches a BLEU of 45.3 for AMR-to-text generation, which is 2.8 points higher than baseline. Adding tg2g gives a Smatch of 83.2 for AMR parsing, and tg2t improves the baseline by 1.7 BLEU points for generation. By combining tg2g and tg2t, the performance increase by 0.6 and 2.5 points on AMR parsing and AMR-to-text generation, respectively. Similar trend can be observed by combining tg2g and tg2t. Finally, using all 6 pre-training tasks, our model reach a result of 83.6 Smatch and 45.6 BLEU, respectively.</p>
<p>We also study the impact of two graph self-supervised training strategies. In particular, we evaluate the performance after removing the node/edge or the sub-graph masking task independently. As shown in Table 4, the performance decreases on both AMR parsing and AMR-to-text generation tasks without the node/edge level masking strategy. The performance drop is larger when removing the sub-graph masking task, with a margin of 0.5 Smatch and 0.9 BLEU, respectively.</p>
<p>Figure 3(a) compares the performance of standard pre-training (t̄2t, ḡ2g) and fine-tuning (t2g, g2t) with our unified framework. The unified framework gives better results than standard versions on both tasks. This confirms our assumption that our unified framework is helpful for reducing the gap between pre-training and fine-tuning. Besides, we find that by unifying pre-training and finetuning formats, our model converges faster than the baseline during fine-tuning (cf. Appendix C.1).</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Development results: (a) comparison of standard pre-training and fine-tuning phase (std) and our unified frameworks; (b) impact of silver data.</p>
<p>Figure 3(b) shows the model performance regarding different scales of silver data. Even without silver data, the performance of our model is better than the baseline, indicating that graph pre-training is beneficial for downstream tasks when using various auxiliary tasks. When silver data are available, the performance of both AMR parsing and AMR-to-text generation tasks increases as the scale of silver data increases, with a margin of 2 BLEU score. We also fine-tune a BART model on silver data under our unified framework (i.e., tg2t and tg2g), and find that our dual graph and text denoising tasks are more useful (cf. Appendix C.2 for more analysis and discussion).</p>
<h3>4.5 Main Results</h3>
<p>AMR parsing. Table 5 lists the result of different models on AMR2.0 and AMR3.0. Among previous works, Bevilacqua+ (2021, large) achieves the best results, consistently outperforming other systems. Compared with the system of Bevilacqua et al. (2021), our model obtains significantly (p&lt;0.01) better Smatch scores in both base and large settings on both datasets. In particular, our base model outperforms the Bevilacqua+ (2021, base) by 0.9 Smatch point on AMR2.0, and our large model obtains a Smatch of 85.4 and 84.2 on AMR2.0 and AMR3.0, respectively. To our knowledge, these are the best-reported results, showing the effectiveness of our method.</p>
<p>Besides, Bevilacqua+ (2021, large)<sup>s</sup> uses silver data for fine-tuning, yet does not lead to consistent improvement over Bevilacqua+ (2021, large). In contrast, our large model gives 1.1 and 1.2 higher Smatch than Bevilacqua+ (2021, large)<sup>s</sup> on AMR2.0 and AMR3.0, respectively. This indicates that our pre-training framework is a better way than fine-tuning to make use of silver data. The main</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Smatch</th>
<th style="text-align: center;">Unlab.</th>
<th style="text-align: center;">NoWSD</th>
<th style="text-align: center;">Con.</th>
<th style="text-align: center;">Wiki.</th>
<th style="text-align: center;">NER</th>
<th style="text-align: center;">Reent.</th>
<th style="text-align: center;">Neg.</th>
<th style="text-align: center;">SRL</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">AMR2.0</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">LyuT (2018)</td>
<td style="text-align: center;">74.4</td>
<td style="text-align: center;">77.1</td>
<td style="text-align: center;">75.5</td>
<td style="text-align: center;">85.9</td>
<td style="text-align: center;">75.7</td>
<td style="text-align: center;">86.0</td>
<td style="text-align: center;">52.3</td>
<td style="text-align: center;">58.4</td>
<td style="text-align: center;">69.8</td>
</tr>
<tr>
<td style="text-align: left;">Zhang+ (2019b) ${ }^{\dagger}$</td>
<td style="text-align: center;">77.0</td>
<td style="text-align: center;">80.0</td>
<td style="text-align: center;">78.0</td>
<td style="text-align: center;">86.0</td>
<td style="text-align: center;">86.0</td>
<td style="text-align: center;">79.0</td>
<td style="text-align: center;">61.0</td>
<td style="text-align: center;">77.0</td>
<td style="text-align: center;">71.0</td>
</tr>
<tr>
<td style="text-align: left;">Zhou+ (2020) ${ }^{\dagger}$</td>
<td style="text-align: center;">77.5</td>
<td style="text-align: center;">80.4</td>
<td style="text-align: center;">78.2</td>
<td style="text-align: center;">85.9</td>
<td style="text-align: center;">86.5</td>
<td style="text-align: center;">78.8</td>
<td style="text-align: center;">61.1</td>
<td style="text-align: center;">76.1</td>
<td style="text-align: center;">71.0</td>
</tr>
<tr>
<td style="text-align: left;">CaiL (2020a) ${ }^{\dagger}$</td>
<td style="text-align: center;">80.2</td>
<td style="text-align: center;">82.8</td>
<td style="text-align: center;">80.0</td>
<td style="text-align: center;">88.1</td>
<td style="text-align: center;">86.3</td>
<td style="text-align: center;">81.1</td>
<td style="text-align: center;">64.6</td>
<td style="text-align: center;">78.9</td>
<td style="text-align: center;">74.2</td>
</tr>
<tr>
<td style="text-align: left;">Xu+ (2020) ${ }^{\dagger}$</td>
<td style="text-align: center;">80.2</td>
<td style="text-align: center;">83.7</td>
<td style="text-align: center;">80.8</td>
<td style="text-align: center;">87.4</td>
<td style="text-align: center;">75.1</td>
<td style="text-align: center;">85.4</td>
<td style="text-align: center;">66.5</td>
<td style="text-align: center;">71.5</td>
<td style="text-align: center;">78.9</td>
</tr>
<tr>
<td style="text-align: left;">Bevilacqua+ (2021, base) ${ }^{\dagger}$</td>
<td style="text-align: center;">82.7</td>
<td style="text-align: center;">85.1</td>
<td style="text-align: center;">83.3</td>
<td style="text-align: center;">89.7</td>
<td style="text-align: center;">82.2</td>
<td style="text-align: center;">90.0</td>
<td style="text-align: center;">70.8</td>
<td style="text-align: center;">72.0</td>
<td style="text-align: center;">79.1</td>
</tr>
<tr>
<td style="text-align: left;">Bevilacqua+ (2021, large) ${ }^{\dagger}$</td>
<td style="text-align: center;">84.5</td>
<td style="text-align: center;">86.7</td>
<td style="text-align: center;">84.9</td>
<td style="text-align: center;">89.6</td>
<td style="text-align: center;">$\mathbf{8 7 . 3}$</td>
<td style="text-align: center;">83.7</td>
<td style="text-align: center;">72.3</td>
<td style="text-align: center;">$\mathbf{7 9 . 9}$</td>
<td style="text-align: center;">79.7</td>
</tr>
<tr>
<td style="text-align: left;">Bevilacqua+ (2021, large) ${ }^{\dagger *}$</td>
<td style="text-align: center;">84.3</td>
<td style="text-align: center;">86.7</td>
<td style="text-align: center;">84.8</td>
<td style="text-align: center;">90.8</td>
<td style="text-align: center;">83.1</td>
<td style="text-align: center;">90.5</td>
<td style="text-align: center;">72.4</td>
<td style="text-align: center;">73.6</td>
<td style="text-align: center;">80.5</td>
</tr>
<tr>
<td style="text-align: left;">Ours (base) ${ }^{\dagger}$</td>
<td style="text-align: center;">83.6</td>
<td style="text-align: center;">86.7</td>
<td style="text-align: center;">84.0</td>
<td style="text-align: center;">90.2</td>
<td style="text-align: center;">78.6</td>
<td style="text-align: center;">90.0</td>
<td style="text-align: center;">71.3</td>
<td style="text-align: center;">73.7</td>
<td style="text-align: center;">79.5</td>
</tr>
<tr>
<td style="text-align: left;">Ours (large) ${ }^{\dagger}$</td>
<td style="text-align: center;">$\mathbf{8 5 . 4}$</td>
<td style="text-align: center;">$\mathbf{8 8 . 3}$</td>
<td style="text-align: center;">$\mathbf{8 5 . 8}$</td>
<td style="text-align: center;">$\mathbf{9 1 . 2}$</td>
<td style="text-align: center;">81.4</td>
<td style="text-align: center;">$\mathbf{9 1 . 5}$</td>
<td style="text-align: center;">$\mathbf{7 3 . 5}$</td>
<td style="text-align: center;">74.0</td>
<td style="text-align: center;">$\mathbf{8 1 . 5}$</td>
</tr>
<tr>
<td style="text-align: left;">AMR3.0</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Bevilacqua+ (2021, large) ${ }^{\dagger}$</td>
<td style="text-align: center;">83.0</td>
<td style="text-align: center;">85.4</td>
<td style="text-align: center;">83.5</td>
<td style="text-align: center;">89.8</td>
<td style="text-align: center;">$\mathbf{8 2 . 7}$</td>
<td style="text-align: center;">87.2</td>
<td style="text-align: center;">70.4</td>
<td style="text-align: center;">$\mathbf{7 3 . 0}$</td>
<td style="text-align: center;">78.9</td>
</tr>
<tr>
<td style="text-align: left;">Bevilacqua+ (2021, large) ${ }^{\dagger *}$</td>
<td style="text-align: center;">83.0</td>
<td style="text-align: center;">85.4</td>
<td style="text-align: center;">83.5</td>
<td style="text-align: center;">89.5</td>
<td style="text-align: center;">81.2</td>
<td style="text-align: center;">87.1</td>
<td style="text-align: center;">71.3</td>
<td style="text-align: center;">71.7</td>
<td style="text-align: center;">79.1</td>
</tr>
<tr>
<td style="text-align: left;">Ours (base) ${ }^{\dagger}$</td>
<td style="text-align: center;">82.5</td>
<td style="text-align: center;">85.7</td>
<td style="text-align: center;">82.9</td>
<td style="text-align: center;">89.4</td>
<td style="text-align: center;">76.1</td>
<td style="text-align: center;">86.8</td>
<td style="text-align: center;">69.9</td>
<td style="text-align: center;">70.3</td>
<td style="text-align: center;">78.2</td>
</tr>
<tr>
<td style="text-align: left;">Ours (large) ${ }^{\dagger}$</td>
<td style="text-align: center;">$\mathbf{8 4 . 2}$</td>
<td style="text-align: center;">$\mathbf{8 7 . 1}$</td>
<td style="text-align: center;">$\mathbf{8 4 . 6}$</td>
<td style="text-align: center;">$\mathbf{9 0 . 2}$</td>
<td style="text-align: center;">78.9</td>
<td style="text-align: center;">$\mathbf{8 8 . 5}$</td>
<td style="text-align: center;">$\mathbf{7 2 . 4}$</td>
<td style="text-align: center;">72.1</td>
<td style="text-align: center;">$\mathbf{8 0 . 3}$</td>
</tr>
</tbody>
</table>
<p>Table 5: AMR parsing results on AMR2.0 and AMR3.0. * means the model uses 200k silver data for fine-tuning. $\dagger$ means the model is based on pre-trained models. The best result within each row block is shown in bold.
reason is that our models are pre-trained using a denoising auto-encoding manner, which is less sensitive to silver (or noisy) data than fine-tuning. We also find that further fine-tuning our models on silver data (same with pre-training) cannot bring improvement (cf. Appendix C.3).
AMR-to-text generation. We report the results of different systems on AMR2.0 and AMR3.0 in Table 6, respectively. With the help of BART, Ribeiro+ (2021) and Bevilacqua+ (2021, large) obtain significantly better results than previous graph-to-sequence and GPT-based models. Compared with Bevilacqua+ (2021), our models (base and large) give significantly ( $p&lt;0.001$ ) better results in terms of all evaluation metrics. In particular, our base model achieves comparable or better performance than Bevilacqua+ (2021, large). Compared with Bevilacqua+ (2021, large) ${ }^{*}$, our large model improves the performance by 3.9 and 2.7 points on AMR2.0 and AMR3.0, respectively. Similar with AMR parsing, we observe that when fine-tuning our models on silver data cannot bring improvement for AMR-to-text generation task (Table 6 and Appendix C.3).
Zero-shot Domain Adaption. We use the model trained on AMR2.0 to get predictions on out-ofdomain test sets. Table 7 shows the results on AMR parsing and AMR-to-text generation tasks. Similar to in-domain experiments, our models achieve better results than existing methods. In particular, our base model can give comparable performance than Bevilacqua+ (2021, large), and our large model obtains the best-reported results. This indicates that</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">BLEU</th>
<th style="text-align: center;">CH.</th>
<th style="text-align: center;">MET.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">AMR2.0</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Zhu+ (2019)</td>
<td style="text-align: center;">31.8</td>
<td style="text-align: center;">64.1</td>
<td style="text-align: center;">36.4</td>
</tr>
<tr>
<td style="text-align: left;">Zhang+ (2020)</td>
<td style="text-align: center;">33.6</td>
<td style="text-align: center;">63.2</td>
<td style="text-align: center;">37.5</td>
</tr>
<tr>
<td style="text-align: left;">Bai+ (2020)</td>
<td style="text-align: center;">34.2</td>
<td style="text-align: center;">65.7</td>
<td style="text-align: center;">38.2</td>
</tr>
<tr>
<td style="text-align: left;">Mager+ (2020) ${ }^{\dagger}$</td>
<td style="text-align: center;">33.0</td>
<td style="text-align: center;">63.9</td>
<td style="text-align: center;">37.7</td>
</tr>
<tr>
<td style="text-align: left;">Ribeiro+ (2021) ${ }^{\dagger \dagger}$</td>
<td style="text-align: center;">45.9</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">41.2</td>
</tr>
<tr>
<td style="text-align: left;">Bevilacqua+ (2021, base) ${ }^{\dagger}$</td>
<td style="text-align: center;">42.7</td>
<td style="text-align: center;">72.2</td>
<td style="text-align: center;">40.7</td>
</tr>
<tr>
<td style="text-align: left;">Bevilacqua+ (2021, large) ${ }^{\dagger}$</td>
<td style="text-align: center;">45.3</td>
<td style="text-align: center;">73.5</td>
<td style="text-align: center;">41.0</td>
</tr>
<tr>
<td style="text-align: left;">Bevilacqua+ (2021, large) ${ }^{* \dagger}$</td>
<td style="text-align: center;">45.9</td>
<td style="text-align: center;">74.2</td>
<td style="text-align: center;">41.8</td>
</tr>
<tr>
<td style="text-align: left;">Ours (base) ${ }^{\dagger}$</td>
<td style="text-align: center;">46.6</td>
<td style="text-align: center;">74.6</td>
<td style="text-align: center;">41.4</td>
</tr>
<tr>
<td style="text-align: left;">Ours (large) ${ }^{\dagger}$</td>
<td style="text-align: center;">$\mathbf{4 9 . 8}$</td>
<td style="text-align: center;">$\mathbf{7 6 . 2}$</td>
<td style="text-align: center;">$\mathbf{4 2 . 6}$</td>
</tr>
<tr>
<td style="text-align: left;">AMR3.0</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Zhang+ (2020)</td>
<td style="text-align: center;">34.3</td>
<td style="text-align: center;">63.7</td>
<td style="text-align: center;">38.2</td>
</tr>
<tr>
<td style="text-align: left;">Bevilacqua+ (2021, large) ${ }^{\dagger}$</td>
<td style="text-align: center;">44.9</td>
<td style="text-align: center;">72.9</td>
<td style="text-align: center;">40.6</td>
</tr>
<tr>
<td style="text-align: left;">Bevilacqua+ (2021, large) ${ }^{* \dagger}$</td>
<td style="text-align: center;">46.5</td>
<td style="text-align: center;">73.9</td>
<td style="text-align: center;">41.7</td>
</tr>
<tr>
<td style="text-align: left;">Ours (base) ${ }^{\dagger}$</td>
<td style="text-align: center;">45.9</td>
<td style="text-align: center;">73.8</td>
<td style="text-align: center;">40.8</td>
</tr>
<tr>
<td style="text-align: left;">Ours (large) ${ }^{\dagger}$</td>
<td style="text-align: center;">$\mathbf{4 9 . 2}$</td>
<td style="text-align: center;">$\mathbf{7 6 . 1}$</td>
<td style="text-align: center;">$\mathbf{4 2 . 3}$</td>
</tr>
</tbody>
</table>
<p>Table 6: AMR-to-text results on AMR2.0 and AMR3.0. CH.=CHRF++. MET.=METEOR. * means the model uses 200k silver data for fine-tuning. Models marked with $\dagger$ are based on PLMs. The best result within each row block is shown in bold. ${ }^{\dagger}$ For fair comparison, we report results of tokenized output of Ribeiro+ (2021).
our model is more robust to new domains, thanks to joint graph and text pre-training. Regarding different domains, our method achieves bigger improvements on New3 than the other two domains. This is intuitive, as pre-training strengthens the model representation power on the domain of graph pretraining data, and New3 is closer to it than other two datasets.</p>
<p>In addition, Bevilacqua+ (2021, large)* gives lower results than Bevilacqua+ (2021, large) in</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">New3</th>
<th style="text-align: center;">TLP</th>
<th style="text-align: center;">Bio</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">AMR Parsing</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Bevilacqua+ (2021, large)</td>
<td style="text-align: center;">73.7</td>
<td style="text-align: center;">77.3</td>
<td style="text-align: center;">59.7</td>
</tr>
<tr>
<td style="text-align: left;">Bevilacqua+ (2021, large) ${ }^{a}$</td>
<td style="text-align: center;">71.8</td>
<td style="text-align: center;">77.5</td>
<td style="text-align: center;">59.5</td>
</tr>
<tr>
<td style="text-align: left;">Ours (base)</td>
<td style="text-align: center;">74.4</td>
<td style="text-align: center;">77.8</td>
<td style="text-align: center;">58.8</td>
</tr>
<tr>
<td style="text-align: left;">Ours (large)</td>
<td style="text-align: center;">$\mathbf{7 6 . 9}$</td>
<td style="text-align: center;">$\mathbf{7 9 . 8}$</td>
<td style="text-align: center;">$\mathbf{6 3 . 2}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">AMR-to-Text</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Bevilacqua+ (2021, large)</td>
<td style="text-align: center;">38.8</td>
<td style="text-align: center;">25.4</td>
<td style="text-align: center;">18.7</td>
</tr>
<tr>
<td style="text-align: left;">Bevilacqua+ (2021, large) ${ }^{a}$</td>
<td style="text-align: center;">38.2</td>
<td style="text-align: center;">25.1</td>
<td style="text-align: center;">19.4</td>
</tr>
<tr>
<td style="text-align: left;">Ours (base)</td>
<td style="text-align: center;">41.0</td>
<td style="text-align: center;">26.4</td>
<td style="text-align: center;">16.9</td>
</tr>
<tr>
<td style="text-align: left;">Ours (large)</td>
<td style="text-align: center;">$\mathbf{4 4 . 8}$</td>
<td style="text-align: center;">$\mathbf{2 9 . 1}$</td>
<td style="text-align: center;">$\mathbf{2 0 . 7}$</td>
</tr>
</tbody>
</table>
<p>Table 7: Out of distribution performance on AMR parsing (Smatch) and AMR-to-text (BLEU).</p>
<p>New3 (both tasks) and TLP (only AMR-to-text generation). In contrast, our model gives consistent improvements on all 3 domains. This can be because fine-tuning leads to catastrophic forgetting of distributional knowledge (Kirkpatrick et al., 2017).</p>
<h3>4.6 Impact of Graph</h3>
<p>Table 8 shows the effects of the graph size, graph diameter and reentrancies on the performance. We split the test set of AMR2.0 into different groups and report the performance improvement over the baseline model (Bevilacqua et al., 2021). All models are trained on AMR2.0. We first consider graph size, which records the number of nodes in an AMR graph. Our model consistently outperforms the baseline model on both tasks, with the performance gap growing on larger graphs. This indicates that our system is more powerful in dealing with larger graphs. The main reason is that our joint text and graph pre-training mechanism enhances the model with the ability to capture word or span level correlation between text and graph, which is helpful for dealing with long sequence and large graphs.</p>
<p>The graph depth is defined as the longest distance between the AMR node and root node. A graph with deeper depth has more long-range dependencies. For AMR parsing, our model gives a better Smatch than the baseline model on the first two groups of graphs, and a comparable score on graphs with a depth bigger than 6. For AMR-to-text generation, our model consistently improves over the baseline model on all graphs, and the improvements are bigger on deeper graphs. This shows that our model is better for learning more complex graphs. It can be that our graph masking strategies train the model to learn the relationships between a sub-graph and the remaining graph context, making it easier to understand deep graphs.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Graph Size</th>
<th style="text-align: center;">$1-10(522)$</th>
<th style="text-align: center;">$11-20(556)$</th>
<th style="text-align: center;">$&gt;20(293)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">AMR parsing</td>
<td style="text-align: center;">+0.3</td>
<td style="text-align: center;">+1.0</td>
<td style="text-align: center;">+0.8</td>
</tr>
<tr>
<td style="text-align: left;">AMR-to-text</td>
<td style="text-align: center;">+0.9</td>
<td style="text-align: center;">+3.2</td>
<td style="text-align: center;">+2.1</td>
</tr>
<tr>
<td style="text-align: left;">Graph Depth</td>
<td style="text-align: center;">$1-3(422)$</td>
<td style="text-align: center;">$4-6(667)$</td>
<td style="text-align: center;">$&gt;6(282)$</td>
</tr>
<tr>
<td style="text-align: left;">AMR parsing</td>
<td style="text-align: center;">+0.8</td>
<td style="text-align: center;">+0.9</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">AMR-to-text</td>
<td style="text-align: center;">+1.2</td>
<td style="text-align: center;">+2.3</td>
<td style="text-align: center;">+2.8</td>
</tr>
<tr>
<td style="text-align: left;">Reentrancies</td>
<td style="text-align: center;">$0(622)$</td>
<td style="text-align: center;">$1-3(712)$</td>
<td style="text-align: center;">$&gt;4(37)$</td>
</tr>
<tr>
<td style="text-align: left;">AMR parsing</td>
<td style="text-align: center;">+1.1</td>
<td style="text-align: center;">+0.6</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">AMR-to-text</td>
<td style="text-align: center;">+2.0</td>
<td style="text-align: center;">+2.7</td>
<td style="text-align: center;">+0.4</td>
</tr>
</tbody>
</table>
<p>Table 8: Performance improvements on AMR parsing (Smatch) and AMR-to-text (BLEU).</p>
<p>Reentrancy is the number of nodes that has multiple parents. Reentrancies pose difficulties to both AMR parsing and AMR-to-text tasks (Damonte and Cohen, 2019; Szubert et al., 2020). The more reentrancies, the harder the graph is to be understood. Our method gives significantly ( $p&lt;0.01$ ) better results on both tasks when the input graphs have less than 4 reentrancies. For graphs with more than 4 reentrancies, the proposed model is 0.4 better on AMR-to-text generation task and comparable than the baseline model on AMR parsing task. This means that our system has an overall better ability on learning reentrancies.</p>
<h3>4.7 Case study</h3>
<p>Table 9 presents two cases of AMR parsing, with the model outputs generated by our model and the baseline model, and the gold output given the same input sentence. As shown in the first case, the baseline model omits the semantic unit "hard", thus generates an incomplete AMR graph of a different meaning compared with the input sentence. In contrast, our system preserves the concept "hard" and transfers the semantic relations correctly, thanks to the modeling of correspondence between text and graph during pre-training. In the second case, the baseline output includes a cyclic sub-graph (i.e., ( z1 harm-01 :ARG1 z1 )), which is contrary to the grammar that AMRs should be acyclic. Our system gives a valid AMR graph which is semantically similar with gold graph.</p>
<p>Table 10 lists two AMR graphs and model outputs of our AMR-to-text model and the baseline model. In the first case, although the baseline generates a fluent sentence, it ignores the concept "have-purpose-91", resulting in that the generated sentence is of a different meaning compared with the input graph. In the second AMR graph, "before" modifies the phrase "won many championships". However, in the baseline output, "before" is used to</p>
<div class="codehilite"><pre><span></span><code>Text\#1: It&#39;s getting hard to keep strong and keep
    carrying on with life.
    Gold:
    (g / get-03
        :ARG1 (a / and
            :op1 (k / keep-02
                :ARG1 (s / strong-02))
            :op2 (k2 / keep-02
                :ARG1 (c / carry-on-02
                    :ARG1 (1/ live-01))))
            :ARG2 (h / hard-02))
</code></pre></div>

<h2>Baseline:</h2>
<p>(z0 / get-03
:ARG1 (z1 / and
:op1 (z2 / keep-02
:ARG1 (z3 / strong-02))
:op2 (z4 / carry-on-02
:ARG1 (z5 / life))))
Ours:
(z0 / get-03
:ARG1 (z1 / and
:op1 (z2 / keep-02
:ARG1 (z3 / strong-02))
:op2 (z4 / keep-02
:ARG1 (z5 / carry-on-02
:ARG1 (z6 / life))))
:ARG2 (z7 / hard-02
:ARG1 z1))
Text#2: Self harming is addictive, but you can overcome it.</p>
<h2>Gold:</h2>
<p>(c / contrast-01
:ARG1 (a / addictive-02
:ARG0 (h / harm-01
:ARG1 (s / self)))
:ARG2 (p / possible-01
:ARG1 (o / overcome-01
:ARG0 (y / you)
:ARG1 h)))
Baseline:
(z0 / addictive-02
:ARG0 (z1 / harm-01
:ARG1 z1)
:concession-of (z2 / possible-01
:ARG1 (z3 / overcome-01
:ARG0 (z4 / you)
:ARG1 z1)))
Ours:
(z0 / contrast-01
:ARG1 (z1 / addictive-02
:ARG0 (z2 / harm-01
:ARG1 (z3 / self)))
:ARG2 (z4 / possible-01
:ARG1 (z5 / overcome-01
:ARG0 (z6 / you)
:ARG1 z1)))
Table 9: Two AMR parsing cases. Given a text input, we present the gold AMR graph and two model outputs, parsed by the baseline and our model, respectively.
modify the phrase "participating in international competitions". Compared with the baseline, our</p>
<p>AMR#1: (h / have-purpose-91
:ARG1 (t / thing
:ARG1-of (e / expend-01
:ARG2 (t2 / transport-01)))
:ARG2 (a / amr-unknown))
Gold: What is the purpose of transportation-related expenditures?
Baseline: What are the transportation expenses?
Ours: What is the purpose of transportation expenses?</p>
<h2>AMR#2:</h2>
<p>(w / win-01
:ARG0 (p2 / person
:wiki -
:name (n / name
:op1 "Fengzhu"
:op2 "Xu"))
:ARG1 (c / championship-02
:ARG0 p2
:quant (m / many))
:time (b / before)
:part-of (c2 / compete-01
:mod (i / international)))
Gold: Fengzhu Xu has won many championships in international competitions before.</p>
<p>Baseline: Fengzhu Xu won many championships before participating in international competitions.</p>
<p>Ours: Fengzhu Xu has won many championships in international competitions before.</p>
<p>Table 10: Two AMR-to-text generation cases. Given an AMR graph, we present the gold text and two generated outputs, given by baseline and our model, respectively.
system recovers all concepts and maps the modification relationship from the AMR graph to text correctly. This indicates that our model generates more faithful sentences than the baseline.</p>
<h2>5 Conclusion</h2>
<p>We investigated graph pre-training as a complement to text pre-training for AMR parsing and AMR-totext generation tasks, using a novel unified framework with dual graph and text denoising. We find that graph pre-training is highly effective for both AMR parsing and AMR -to-text generation, and is a more effective way of making use of silver data compared with fine-tuning. Our methods give the best results on multiple benchmarks for both tasks.</p>
<h2>Acknowledgments</h2>
<p>Yue Zhang is the corresponding author. We would like to thank anonymous reviewers for their insightful comments. This work is supported by the National Natural Science Foundation of China under grant No. 61976180 and the Tencent AI Lab RhinoBird Focused Research Program.</p>
<h2>References</h2>
<p>Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to align and translate. In 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.</p>
<p>Xuefeng Bai, Yulong Chen, Linfeng Song, and Yue Zhang. 2021. Semantic representation for dialogue modeling. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4430-4445, Online. Association for Computational Linguistics.</p>
<p>Xuefeng Bai, Linfeng Song, and Yue Zhang. 2020. Online back-parsing for AMR-to-text generation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1206-1219, Online. Association for Computational Linguistics.</p>
<p>Miguel Ballesteros and Yaser Al-Onaizan. 2017. AMR parsing using stack-LSTMs. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1269-1275, Copenhagen, Denmark. Association for Computational Linguistics.</p>
<p>Laura Banarescu, Claire Bonial, Shu Cai, Madalina Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin Knight, Philipp Koehn, Martha Palmer, and Nathan Schneider. 2013. Abstract Meaning Representation for sembanking. In Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse, pages 178-186, Sofia, Bulgaria. Association for Computational Linguistics.</p>
<p>Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 65-72, Ann Arbor, Michigan. Association for Computational Linguistics.</p>
<p>Daniel Beck, Gholamreza Haffari, and Trevor Cohn. 2018. Graph-to-sequence learning using gated graph neural networks. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 273-283, Melbourne, Australia. Association for Computational Linguistics.</p>
<p>Michele Bevilacqua, Rexhina Blloshmi, and Roberto Navigli. 2021. One spring to rule them both: Symmetric AMR semantic parsing and generation without a complex pipeline. Proceedings of the AAAI Conference on Artificial Intelligence, 35(14):12564-12573.</p>
<p>Deng Cai and Wai Lam. 2020. AMR parsing via graphsequence iterative inference. In Proceedings of the</p>
<p>58th Annual Meeting of the Association for Computational Linguistics, pages 1290-1301, Online. Association for Computational Linguistics.</p>
<p>Shu Cai and Kevin Knight. 2013. Smatch: an evaluation metric for semantic feature structures. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 748-752, Sofia, Bulgaria. Association for Computational Linguistics.</p>
<p>Yulong Chen, Yang Liu, Liang Chen, and Yue Zhang. 2021. DialogSum: A real-life scenario dialogue summarization dataset. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 5062-5074, Online. Association for Computational Linguistics.</p>
<p>Marco Damonte and Shay B. Cohen. 2019. Structural neural encoders for AMR-to-text generation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 3649-3658, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Ramón Fernandez Astudillo, Miguel Ballesteros, Tahira Naseem, Austin Blodgett, and Radu Florian. 2020. Transition-based parsing with stack-transformers. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1001-1007, Online. Association for Computational Linguistics.</p>
<p>Jeffrey Flanigan, Chris Dyer, Noah A. Smith, and Jaime Carbonell. 2016. CMU at SemEval-2016 task 8: Graph-based AMR parsing with infinite ramp loss. In Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016), pages 12021206, San Diego, California. Association for Computational Linguistics.</p>
<p>Jeffrey Flanigan, Sam Thomson, Jaime Carbonell, Chris Dyer, and Noah A. Smith. 2014. A discriminative graph-based parser for the Abstract Meaning Representation. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1426-1436, Baltimore, Maryland. Association for Computational Linguistics.</p>
<p>Zhijiang Guo and Wei Lu. 2018. Better transition-based AMR parsing with a refined search space. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1712-1722,</p>
<p>Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019. Parameter-efficient transfer learning for NLP. In Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pages 2790-2799. PMLR.</p>
<p>Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay S. Pande, and Jure Leskovec. 2020a. Strategies for pre-training graph neural networks. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.</p>
<p>Ziniu Hu, Yuxiao Dong, Kuansan Wang, Kai-Wei Chang, and Yizhou Sun. 2020b. GPT-GNN: generative pre-training of graph neural networks. In KDD '20: The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event, CA, USA, August 23-27, 2020, pages 1857-1867. ACM.</p>
<p>Lifu Huang, Taylor Cassidy, Xiaocheng Feng, Heng Ji, Clare R. Voss, Jiawei Han, and Avirup Sil. 2016. Liberal event extraction and event schema induction. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 258-268, Berlin, Germany. Association for Computational Linguistics.</p>
<p>Thomas N. Kipf and Max Welling. 2016. Variational graph auto-encoders. CoRR, abs/1611.07308.</p>
<p>James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. 2017. Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences, 114(13):3521-3526.</p>
<p>Ioannis Konstas, Srinivasan Iyer, Mark Yatskar, Yejin Choi, and Luke Zettlemoyer. 2017. Neural AMR: Sequence-to-sequence models for parsing and generation. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 146-157, Vancouver, Canada. Association for Computational Linguistics.</p>
<p>Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871-7880, Online. Association for Computational Linguistics.</p>
<p>Kexin Liao, Logan Lebanoff, and Fei Liu. 2018. Abstract Meaning Representation for multi-document summarization. In Proceedings of the 27th International Conference on Computational Linguistics, pages 1178-1190, Santa Fe, New Mexico, USA. Association for Computational Linguistics.</p>
<p>Fei Liu, Jeffrey Flanigan, Sam Thomson, Norman Sadeh, and Noah A. Smith. 2015. Toward abstractive summarization using semantic representations. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1077-1086, Denver, Colorado. Association for Computational Linguistics.</p>
<p>Yijia Liu, Wanxiang Che, Bo Zheng, Bing Qin, and Ting Liu. 2018. An AMR aligner tuned by transitionbased parser. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2422-2430, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Yuanfu Lu, Xunqiang Jiang, Yuan Fang, and Chuan Shi. 2021. Learning to pre-train graph neural networks. In Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021, pages 4276-4284. AAAI Press.</p>
<p>Chunchuan Lyu, Shay B. Cohen, and Ivan Titov. 2020. A differentiable relaxation of graph segmentation and alignment for AMR parsing. CoRR, abs/2010.12676.</p>
<p>Chunchuan Lyu and Ivan Titov. 2018. AMR parsing as graph prediction with latent alignment. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 397-407, Melbourne, Australia. Association for Computational Linguistics.</p>
<p>Manuel Mager, Ramón Fernandez Astudillo, Tahira Naseem, Md Arafat Sultan, Young-Suk Lee, Radu Florian, and Salim Roukos. 2020. GPT-too: A language-model-first approach for AMR-to-text generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1846-1852, Online. Association for Computational Linguistics.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311-318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.</p>
<p>Xiaochang Peng, Linfeng Song, Daniel Gildea, and Giorgio Satta. 2018. Sequence-to-sequence models for cache transition systems. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages</p>
<p>1842-1852, Melbourne, Australia. Association for Computational Linguistics.</p>
<p>Xiaochang Peng, Chuan Wang, Daniel Gildea, and Nianwen Xue. 2017. Addressing the data sparsity issue in neural AMR parsing. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 366-375, Valencia, Spain. Association for Computational Linguistics.</p>
<p>Maja Popović. 2017. chrF++: words helping character n-grams. In Proceedings of the Second Conference on Machine Translation, pages 612-618, Copenhagen, Denmark. Association for Computational Linguistics.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1-67.</p>
<p>Leonardo F. R. Ribeiro, Martin Schmitt, Hinrich Schütze, and Iryna Gurevych. 2021a. Investigating pretrained language models for graph-to-text generation. In Proceedings of the 3rd Workshop on Natural Language Processing for Conversational AI, pages 211-227, Online. Association for Computational Linguistics.</p>
<p>Leonardo F. R. Ribeiro, Yue Zhang, and Iryna Gurevych. 2021b. Structural adapters in pretrained language models for AMR-to-Text generation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 4269-4282, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Linfeng Song, Daniel Gildea, Yue Zhang, Zhiguo Wang, and Jinsong Su. 2019. Semantic neural machine translation using AMR. Transactions of the Association for Computational Linguistics, 7:19-31.</p>
<p>Linfeng Song, Yue Zhang, Zhiguo Wang, and Daniel Gildea. 2018. A graph-to-sequence model for AMR-to-text generation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1616-1626, Melbourne, Australia. Association for Computational Linguistics.</p>
<p>Ida Szubert, Marco Damonte, Shay B. Cohen, and Mark Steedman. 2020. The role of reentrancies in Abstract Meaning Representation parsing. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2198-2207, Online. Association for Computational Linguistics.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural</p>
<p>Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 5998-6008.</p>
<p>Chuan Wang, Nianwen Xue, and Sameer Pradhan. 2015a. Boosting transition-based AMR parsing with refined actions and auxiliary analyzers. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pages 857-862, Beijing, China. Association for Computational Linguistics.</p>
<p>Chuan Wang, Nianwen Xue, and Sameer Pradhan. 2015b. A transition-based algorithm for AMR parsing. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 366-375, Denver, Colorado. Association for Computational Linguistics.</p>
<p>Dongqin Xu, Junhui Li, Muhua Zhu, Min Zhang, and Guodong Zhou. 2020. Improving AMR parsing with sequence-to-sequence pre-training. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 25012511, Online. Association for Computational Linguistics.</p>
<p>Sheng Zhang, Xutai Ma, Kevin Duh, and Benjamin Van Durme. 2019a. AMR parsing as sequence-tograph transduction. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 80-94, Florence, Italy. Association for Computational Linguistics.</p>
<p>Sheng Zhang, Xutai Ma, Kevin Duh, and Benjamin Van Durme. 2019b. Broad-coverage semantic parsing as transduction. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLPIJCNLP), pages 3786-3798, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Yan Zhang, Zhijiang Guo, Zhiyang Teng, Wei Lu, Shay B. Cohen, Zuozhu Liu, and Lidong Bing. 2020. Lightweight, dynamic graph convolutional networks for AMR-to-text generation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2162-2172, Online. Association for Computational Linguistics.</p>
<p>Zixuan Zhang and Heng Ji. 2021. Abstract Meaning Representation guided graph encoding and decoding for joint information extraction. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 39-49, Online. Association for Computational Linguistics.</p>
<p>Yanbin Zhao, Lu Chen, Zhi Chen, Ruisheng Cao, Su Zhu, and Kai Yu. 2020. Line graph enhanced AMR-to-text generation with mix-order graph attention networks. In Proceedings of the 58th Annual Meeting of the Association for Computational</p>
<p>Linguistics, pages 732-741, Online. Association for Computational Linguistics.</p>
<p>Jiawei Zhou, Tahira Naseem, Ramón Fernandez Astudillo, and Radu Florian. 2021. AMR parsing with action-pointer transformer. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5585-5598, Online. Association for Computational Linguistics.</p>
<p>Qiji Zhou, Yue Zhang, Donghong Ji, and Hao Tang. 2020. AMR parsing with latent structural information. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4306-4319, Online. Association for Computational Linguistics.</p>
<p>Jie Zhu, Junhui Li, Muhua Zhu, Longhua Qian, Min Zhang, and Guodong Zhou. 2019. Modeling graph structure in transformer for better AMR-to-text generation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 5459-5468, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. 2013. Fast and accurate shift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 434-443, Sofia, Bulgaria. Association for Computational Linguistics.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Param. Name</th>
<th style="text-align: center;">Value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Pre-training</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Batch Size</td>
<td style="text-align: center;">32</td>
</tr>
<tr>
<td style="text-align: left;">Optimizer</td>
<td style="text-align: center;">AdamW</td>
</tr>
<tr>
<td style="text-align: left;">Learning Rate (lr)</td>
<td style="text-align: center;">$5 \mathrm{e}-5$</td>
</tr>
<tr>
<td style="text-align: left;">Lr Scheduler</td>
<td style="text-align: center;">inverse_sqrt</td>
</tr>
<tr>
<td style="text-align: left;">Warmup Step</td>
<td style="text-align: center;">2,500</td>
</tr>
<tr>
<td style="text-align: left;">Total Step</td>
<td style="text-align: center;">100,000</td>
</tr>
<tr>
<td style="text-align: left;">Extended Vocabulary Size</td>
<td style="text-align: center;">53,843</td>
</tr>
<tr>
<td style="text-align: left;">Max Sequence Length</td>
<td style="text-align: center;">512</td>
</tr>
<tr>
<td style="text-align: left;">Mix Precision</td>
<td style="text-align: center;">fp16 (O1)</td>
</tr>
<tr>
<td style="text-align: left;">Number of Parameters</td>
<td style="text-align: center;">142M (base), 409M (large)</td>
</tr>
<tr>
<td style="text-align: left;">Training Time</td>
<td style="text-align: center;">13h (base), 70h (large)</td>
</tr>
<tr>
<td style="text-align: left;">Fine-tuning (Parsing)</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Batch Size</td>
<td style="text-align: center;">8</td>
</tr>
<tr>
<td style="text-align: left;">Optimizer</td>
<td style="text-align: center;">AdamW</td>
</tr>
<tr>
<td style="text-align: left;">Learning Rate (lr)</td>
<td style="text-align: center;">$3 \mathrm{e}-5$ (base), $8 \mathrm{e}-6$ (large)</td>
</tr>
<tr>
<td style="text-align: left;">Lr Scheduler</td>
<td style="text-align: center;">constant</td>
</tr>
<tr>
<td style="text-align: left;">Warmup Step</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: left;">Total Epoch</td>
<td style="text-align: center;">20</td>
</tr>
<tr>
<td style="text-align: left;">Early Stop</td>
<td style="text-align: center;">5</td>
</tr>
<tr>
<td style="text-align: left;">Max Sequence Length</td>
<td style="text-align: center;">512</td>
</tr>
<tr>
<td style="text-align: left;">Beam Size</td>
<td style="text-align: center;">5</td>
</tr>
<tr>
<td style="text-align: left;">Length Penalty</td>
<td style="text-align: center;">1.0</td>
</tr>
<tr>
<td style="text-align: left;">Label Smoothing</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: left;">Mix Precision</td>
<td style="text-align: center;">fp16 (O1)</td>
</tr>
<tr>
<td style="text-align: left;">Training Time</td>
<td style="text-align: center;">6h (base), 12h (large)</td>
</tr>
<tr>
<td style="text-align: left;">Fine-tuning (Generation)</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Batch Size</td>
<td style="text-align: center;">8</td>
</tr>
<tr>
<td style="text-align: left;">Optimizer</td>
<td style="text-align: center;">AdamW</td>
</tr>
<tr>
<td style="text-align: left;">Learning Rate (lr)</td>
<td style="text-align: center;">$1 \mathrm{e}-5$ (base), $2 \mathrm{e}-6$ (large)</td>
</tr>
<tr>
<td style="text-align: left;">Lr scheduler</td>
<td style="text-align: center;">constant</td>
</tr>
<tr>
<td style="text-align: left;">Warmup Step</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: left;">Total Epoch</td>
<td style="text-align: center;">20</td>
</tr>
<tr>
<td style="text-align: left;">Early Stop</td>
<td style="text-align: center;">5</td>
</tr>
<tr>
<td style="text-align: left;">Max Sequence Length</td>
<td style="text-align: center;">512</td>
</tr>
<tr>
<td style="text-align: left;">Beam Size</td>
<td style="text-align: center;">5</td>
</tr>
<tr>
<td style="text-align: left;">Length Penalty</td>
<td style="text-align: center;">1.0</td>
</tr>
<tr>
<td style="text-align: left;">Label Smoothing</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: left;">Mix Precision</td>
<td style="text-align: center;">fp16 (O1)</td>
</tr>
<tr>
<td style="text-align: left;">Training Time</td>
<td style="text-align: center;">3h (base), 6h (large)</td>
</tr>
</tbody>
</table>
<p>Table 11: Hyper-parameters of our models on Pretraining and Fine-tuning.</p>
<h2>A Model Hyper-Parameters</h2>
<p>Table 11 lists all model hyper-parameters used for our experiments. We implement our model based on Pytorch and Huggingface Transformers. The pre-processed data, source code and pretrained models are released at https://github. com/muyeby/AMRBART.</p>
<h2>B Fine-grained Evaluation Metric for AMR Parsing</h2>
<p>The Smatch score (Cai and Knight, 2013) measures the degree of overlap between the gold and the prediction AMR graphs. It can be further broken into different sub-metrics, including:</p>
<ul>
<li>Unlabeled (Unlab.): Smatch score after re-</li>
</ul>
<table>
<thead>
<tr>
<th style="text-align: left;">Setting</th>
<th style="text-align: center;">AMR parsing</th>
<th style="text-align: center;">AMR-to-text</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">BART</td>
<td style="text-align: center;">82.7</td>
<td style="text-align: center;">42.5</td>
</tr>
<tr>
<td style="text-align: left;">+ silver (fine-tuning)</td>
<td style="text-align: center;">82.6</td>
<td style="text-align: center;">44.9</td>
</tr>
<tr>
<td style="text-align: left;">+ silver (denoising)</td>
<td style="text-align: center;">83.6</td>
<td style="text-align: center;">45.6</td>
</tr>
</tbody>
</table>
<p>Table 12: Ablation study on silver data and denoising tasks.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: The learning curve of baseline and our system on AMR-to-text generation task.
moving edge-labels</p>
<ul>
<li>NoWSD: Smatch score after ignoring Propbank senses (e.g., go-01 vs go-02)</li>
<li>Concepts (Con.): $F$-score on the concept identification task</li>
<li>Wikification (Wiki.): $F$-score on the wikification (:wiki roles)</li>
<li>Named Entity Recognition (NER): $F$-score on the named entities (:name roles).</li>
<li>Reentrancy (Reen.): Smatch score on reentrant edges.</li>
<li>Negation (Neg.): $F$-score on the negation detection (:polarity roles).</li>
<li>Semantic Role Labeling (SRL): Smatch score computed on : ARG-i roles.</li>
</ul>
<h2>C More Experimental Results</h2>
<h2>C. 1 Effect of Unified Framework</h2>
<p>Figure 4 compares the learning curve between our system (fine-tuning from our pre-trained model) and baseline (fine-tuning from vanilla BART, i.e., Bevilacqua+) on AMR2.0 validation set ${ }^{5}$. It can be observed that our system has a initial BLEU score of 26.0 , which is significantly ( $p&lt;0.001$ ) better than the baseline. This confirm that our unified</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Setting</th>
<th style="text-align: center;">AMR parsing</th>
<th style="text-align: center;">AMR-to-text</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">AMR2.0</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Ours (large)</td>
<td style="text-align: center;">85.4</td>
<td style="text-align: center;">49.8</td>
</tr>
<tr>
<td style="text-align: left;">+ silver</td>
<td style="text-align: center;">85.1</td>
<td style="text-align: center;">49.6</td>
</tr>
<tr>
<td style="text-align: left;">AMR3.0</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Ours (large)</td>
<td style="text-align: center;">84.2</td>
<td style="text-align: center;">49.2</td>
</tr>
<tr>
<td style="text-align: left;">+ silver</td>
<td style="text-align: center;">83.8</td>
<td style="text-align: center;">48.9</td>
</tr>
</tbody>
</table>
<p>Table 13: Model performance on AMR2.0 and 3.0 datasets for AMR parsing and AMR-to-text. For AMR parsing, we report Smatch score here, and for AMR-totext, we report BLEU-4 score here. +silver denotes to that further fine-tuning the model on silver data.
framework can reduce the gap between pre-training and fine-tuning. In addition, the training curve of the proposed model converges faster while the BLEU score is better than the baseline. This indicates that our model has a larger capacity than baseline.</p>
<h2>C. 2 Impact of denoising Tasks</h2>
<p>To distinguish the contribution of de-nosing tasks and silver data, an ablation study is present where we 1) "fine-tune" a vanilla BART on silver data following our unified framework (i.e., Eg2t and tg2g); 2) continue pre-train a BART on silver data according to proposed de-nosing tasks (in Table 1). As shown in Table 12, we observe that using sliver data for fine-tuning leads to a 0.1 Smatch decrease in AMR parsing and 2.4 BLEU increase in AMR-to-text. This observation is consistent with previous works (Konstas et al., 2017; Song et al., 2018; Bevilacqua et al., 2021). In addition, using silver data for pre-training gives further improvements on both tasks, with 1.0 Smatch for AMR pasring and 0.7 BLEU for AMR-to-text generation. This indicates that our de-nosing tasks can help model to better understand silver data.</p>
<h2>C. 3 Are Silver Data Still Helpful for Finetuning after Being Used for Pre-training?</h2>
<p>As discussed in Section 4.5, we find that graph pre-training is a better way to make use of silver data compared with fine-tuning. We further investigate whether fine-tuning our model on silver data can still bring improvement. As shown in Table 13, our models achieve the best performance on all tasks and datasets, indicating that further fine-tuning our models on silver data decreases the performance. This can be that silver data are already presented in the pre-training phase and thus further fine-tuning can bring no improvement. In
addition, fine-tuning can be more sensitive to data quality than pre-training. When training data contain noise (silver data), fine-tuning on such data can in turn damage the model performance.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{5}$ We use the same learning rate and optimizer.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
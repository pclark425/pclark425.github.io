<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3983 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3983</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3983</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-92.html">extraction-schema-92</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <p><strong>Paper ID:</strong> paper-e64003447d9d186b96a1e557b97d514997a4b76e</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/e64003447d9d186b96a1e557b97d514997a4b76e" target="_blank">Evaluation of ChatGPT as a Question Answering System for Answering Complex Questions</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3983.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3983.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Extended Answer Matching</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Extended Exact Match / Extended Answer Matching (constituent-extraction + alias expansion + mBERT cosine similarity)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An evaluation method introduced in this paper that converts LLM free-text QA outputs into an EM-style correctness decision by (1) extracting candidate answer phrases via constituency parsing, (2) expanding gold answers with Wikidata aliases, and (3) performing fuzzy matching using m-BERT cosine similarity with an empirically chosen threshold.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Semantic equivalence between the LLM-produced answer phrase(s) and the reference KB answer(s) (i.e., whether an LLM's textual output corresponds to the canonical KB answer or its aliases).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>1) Parse LLM output with constituent trees to extract NP/VP root candidate phrases; 2) build reference-answer pool by expanding gold answers with multilingual alias lists from Wikidata; 3) compute maximum cosine similarity between candidate and reference using m-BERT embeddings; 4) apply fuzzy-thresholded matching (non-NUM/DATE/Boolean types) to decide correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Applied across the paper's combined KB-based Complex Question Answering benchmark set (eight datasets, ~194k collected samples including KQApro, LC-quad2.0, WQSP, CWQ, GrailQA, GraphQ, QALD-9, MKQA).</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>Exact Match (EM) computed with fuzzy matching; reported effects on false negative/positive rates (raw EM threshold=1 had 2.38%-4.17% false negatives; fuzzy-matching threshold 0.78 reduces average false rate from 3.89% to 2.71%). Accuracy sensitivity curves vs threshold shown; chosen threshold = 0.78.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Human annotators manually verified a stratified sample of 3,000 LLM outputs (binary correct/incorrect) to (a) compute gold-to-alias minimum similarity (lower bound 0.38) and (b) select the empirical threshold of 0.78 that minimized average false rate across models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Relies on correct constituency extraction (parser errors can harm candidate pools); fuzzy threshold is empirical and may introduce false positives if mis-set; excludes fuzzy matching for NUM/DATE/Boolean types; coverage depends on Wikidata aliases; LLMs' verbose context and paraphrasing still risk mismatch; does not fully measure explanatory quality or correctness of reasoning trace — only answer equivalence.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Using extended matching enabled large-scale, automated EM-style evaluation of LLM outputs; empirical threshold 0.78 stabilized model accuracy (>0.7) and decreased average false rate from 3.89% to 2.71%, allowing comparison of GPT-family and non-GPT LLMs on KBQA datasets.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3983.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3983.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CheckList testing (MFT/INV/DIR)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CheckList black-box behavioral testing paradigm (Minimum Functionality Test, Invariance test, Directional Expectation test) as applied to KBQA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper applies the CheckList testing strategy to LLM QA outputs to probe capability (MFT), robustness to non-answer-affecting perturbations (INV), and controllability/directional behavior (DIR) using targeted test designs including paraphrases, spelling errors, prompt-modifications, and CoT-guided multi-round interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Beyond accuracy: Behavioral testing of nlp models with checklist</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Capability (can the model perform required reasoning operations?), stability/robustness (does correctness survive paraphrase or minor perturbations?), and controllability/directional expectation (does model behaviour change in predictable ways when input is modified?).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>MFT: evaluate questions containing only single reasoning type(s) vs multiple reasoning operations; INV: generate two perturbations per input (introduce random spelling errors; paraphrase semantically equivalent question) and test consistency across original+2 variants (three runs); DIR: three modes — (1) replace reasoning-operation phrases and check whether outputs follow modifications (manual/keyword SPARQL checks), (2) add explicit answer-type prompts and measure EM changes, (3) CoT-inspired multi-round prompting to provide key context then ask question and measure EM deltas.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Applied to the collected KB-based CQA datasets (~194k samples) with focused subsets for MFT/INV/DIR tests; DIR tests often sampled or targeted question types (e.g., SetOperation, Filtering, Counting, Comparison).</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>MFT: EM per reasoning-type for single vs multiple reasoning (Table 6). INV: stability categories (CCC, CCW, CWC, ... WWW) and computed Stability Rate (percentage where all three variants consistent), e.g., GPT-4 Stability Rate = 91.70, ChatGPT = 79.06, GPT-3 = 76.76. DIR: percentage of expected outputs (Table 8 per reasoning type, e.g., ChatGPT overall = 73.75%), EM deltas when adding answer-type prompts (Table 9) and EM deltas from CoT prompting per answer/reasoning type (Table 10).</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Manual design of INV/DIR test cases; in DIR first mode manual observation of SPARQL keyword presence was used to assess whether model applied expected reasoning; human labelling of correctness consistency for INV runs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Black-box behavioral tests detect empirical behaviors but do not explain internal causes; manual test-case creation and manual observation (e.g., SPARQL keyword checks) are labor-intensive and can be subjective; DIR/CoT effects are model- and prompt-sensitive so results may not generalize; INV stability depends on randomness in LLM outputs; checks measure surface behavior not deeper scientific validity.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Key findings: (a) MFT — ChatGPT performs better on multiple reasoning in several categories but still poor on Counting; (b) INV — model stability improved across GPT generations to GPT-4 (91.70% stability); (c) DIR — models showed substantial randomness in applying explicit reasoning operations (~65-85% expected-output rates depending on operation/model), addition of answer-type prompts had mixed effects across models, and CoT prompting significantly improved numerical (NUM) questions (~+30% EM) and some reasoning types while sometimes harming others.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3983.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3983.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KBQA Benchmark Suite (8 datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Collected KB-based Complex Question Answering benchmark (KQApro, LC-quad2.0, WQSP, CWQ, GrailQA, GraphQ, QALD-9, MKQA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-dataset evaluation set assembled and used in this paper, comprising six English and two multilingual KB-based complex QA datasets totaling ~194,782 collected samples to stress-test LLMs on multi-hop, set operations, comparisons, and other compositional reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Accurate retrieval/production of exact KB answers (entity names, dates, numbers, booleans) and correct execution of compositional reasoning (multi-hop, filtering, set operations, counting, comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Zero-shot QA by LLMs (no external KB queries) evaluated against gold answers using the paper's extended answer matching and EM/F1 metrics; feature-driven unified labeling of questions by answer type, reasoning type, and language-type to enable fine-grained analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>The eight datasets: KQApro (complex compositional programs over KB), LC-quad2.0, WQSP (WebQuestionsSP), CWQ (ComplexWebQuestions), GrailQA, GraphQ, QALD-9 (multilingual), MKQA (multilingual). Collected sizes reported in Table 2.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>Dataset-specific metrics: Accuracy/Exact Match or F1 depending on dataset; overall and per-answer-type/per-reasoning-type EM/F1; multilingual per-language EM (Table 4); comparative results across GPT family and FLAN-T5 (Table 3 & 4).</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Datasets originate from previous human annotation; additional manual sampling and verification by authors used for threshold selection and some evaluations; some datasets were sampled due to API limits.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Datasets vary in recency and annotation rules requiring unified tagging; sampling applied to large datasets (MKQA, GrailQA) due to API constraints may bias results; some dataset metrics (EM vs F1) differ complicating cross-dataset comparison; KB alias coverage affects matching; open-domain vs domain-specific generalization uncertain.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Overall: newer GPT-family models improved across datasets (GPT-4 approaching or exceeding SOTA on some older datasets), but LLMs still lag SOTA on newer, more challenging KBQA datasets (e.g., KQApro, GrailQA). Per-language and per-feature analyses revealed strengths (Boolean, ORG, LOC, set ops) and weaknesses (NUM, DATE, counting).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3983.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3983.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Metrics & Quantitative Measures</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Evaluation metrics: Exact Match (EM), F1, m-BERT cosine similarity threshold, Stability Rate, DIR expected-output percentage, EM deltas from prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Set of quantitative metrics and measures the paper uses to quantify LLM correctness, robustness, controllability, and sensitivity to prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Correctness (EM/F1), semantic similarity (m-BERT cosine), robustness/consistency (stability rate across INV variants), controllability (DIR expected-output % and EM deltas when adding prompts or CoT).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Compute EM or F1 per dataset; when LLM output is free text, compute similarity with m-BERT and use threshold (0.78) for fuzzy EM; for INV compute pattern of correctness across original+perturbed inputs and classify as CCC/CCW/etc.; for DIR compute presence of expected SPARQL keywords and percent of outputs conforming to expected reasoning, plus EM changes when adding prompts or CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Applied across the paper's KBQA benchmark (eight datasets) and targeted INV/DIR subsets used in tables and figures.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>EM and F1 scores per model/dataset (Tables 3-5); chosen cosine threshold 0.78; average false rates 3.89% -> 2.71% after fuzzy matching; Stability Rate values (GPT-4 91.70%, ChatGPT 79.06%, etc.); DIR expected-output percentages (e.g., ChatGPT overall 73.75%), EM deltas from prompting and CoT (Table 9 and 10: NUM improvements often +30%+).</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Manual labeling to compute ground-truth and to validate threshold and sampled correctness for sensitivity analysis and stability categorizations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>EM and F1 do not capture quality of explanations or reasoning chains; cosine-similarity thresholding is empirical and language/model dependent; stability categories don't explain why inconsistency occurs; some metrics (presence of SPARQL keywords) are heuristic and depend on model's ability to emit SPARQL-like tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Metrics reveal that while EM/F1 aggregate performance improves across GPT generations, significant variability remains across question types and languages; CoT yields large EM gains on numerical questions; stability (INV) improves markedly in GPT-4.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3983.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3983.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human / Manual Evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Manual verification and human-in-the-loop evaluation steps used in thresholding, sampling, and qualitative checks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Human reviewers were used to label sampled LLM outputs as correct/incorrect for threshold selection, to inspect DIR SPARQL keyword presence, and to perform some manual evaluations where automated EM is insufficient.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Binary correctness judgments on sampled LLM outputs; manual inspection of SPARQL-like outputs for expected reasoning keywords; verification of alias matching and error analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Annotated 3,000 sampled LLM answers (binary correct/incorrect) to (a) derive empirical cosine threshold and (b) measure model accuracy sensitivity; manual observation/annotation used in DIR SPARQL keyword checks and to validate selected automated matches.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Applied to samples drawn from the KBQA benchmark; used particularly for fuzzy-matching threshold selection and validation of extended matching.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>Binary correctness labels used to compute false positive/negative rates and to select a cosine threshold minimizing average false rate; sampling-based accuracy measurements as threshold varies (Figure 3b).</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Essential: humans performed the binary correctness labeling and manual SPARQL observation; paper notes that many previous works relied on manual evaluation due to difficulties measuring LLM outputs with exact-match metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Manual evaluation is costly and scales poorly (hence sampling); subjective judgments possible; manual checks were limited to samples not the full dataset; reliance on human labels can introduce annotation biases.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Human labeling enabled calibration of fuzzy matching (setting threshold to 0.78) and quantified that automated extended matching reduced average false rate from 3.89% to 2.71% on sampled data.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3983.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3983.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought (CoT) prompting evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting (CoT) used as an evaluation/control technique</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper evaluates the impact of CoT-inspired multi-round and stepwise prompting on the LLMs' ability to produce correct answers and follow directional expectations, observing that CoT often improves numerical and certain reasoning operations but may harm other question types.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain of thought prompting elicits reasoning in large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Does guiding the model through intermediate reasoning steps (or providing key noun-related context before the question) increase EM correctness, controllability, or expected reasoning behavior?</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>DIR third mode: provide crucial information step-by-step (naive CoT-guided process) then ask original question; measure EM deltas per answer type and reasoning type (Table 10); compare before/after CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Applied to subsets of KBQA datasets and to DIR test cases; aggregated EM deltas reported per answer and reasoning type.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>EM deltas per answer type and reasoning type (Table 10), e.g., NUM-type improvements between +20% and +38% depending on model; set operation and counting gains also notable; CoT's largest consistent positive impact on NUM-type questions (~+30%).</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Prompt design and interpretation of CoT effects involved human researchers; evaluation of outcome correctness used automated extended matching supported by human-sampled validation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>CoT can improve some categories but negatively affect others; the effectiveness is model-dependent (newer models more sensitive); CoT increases prompt engineering complexity and may not generalize across tasks and languages.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>CoT improved performance substantially for numerical questions across GPT models (often >+30% EM), improved performance for set operations and counting, had modest or negative effects on some other answer types (MISC, PER), and its effect varies by model (GPT-4 saw meaningful gains, others varied).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3983.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e3983.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Limitations / Challenges (evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Observed limitations and open challenges in evaluating LLM QA outputs (as discussed in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A consolidated summary of practical and conceptual evaluation challenges the authors report when measuring LLM outputs against KB answers, including metric mismatches, alias and paraphrase handling, stability/randomness, prompt sensitivity, manual-evaluation costs, and dataset sampling constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>N/A (this entity records limitations rather than a method), but impacts evaluation validity, generalizability, and reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>N/A — this entry aggregates reported empirical and methodological challenges encountered while applying the paper's evaluation protocol.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Challenges affect the entire KBQA benchmark suite used in the paper (e.g., sampling limitations for MKQA/GrailQA due to API limits).</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>Empirical numbers illustrating some limitations: EM-only evaluation yields 2.38%-4.17% false negatives (mean 3.89%); after fuzzy matching average false rate reduced to 2.71%; stability rates show non-negligible randomness (GPT-3 ~76.76% stability), and DIR expected-output rates ~60-75% indicating reasoning randomness.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Paper emphasizes human cost: manual evaluation is expensive and many prior works use small sample sizes (<10k); authors used manual sampling (3,000) to calibrate fuzzy matching but cannot manually verify entire large-scale datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Key challenges: (1) Exact Match unsuitable for LLM free-text output; (2) need for fuzzy/semantic matching with empirical thresholds; (3) parser/alias coverage limits; (4) LLM output randomness reduces stability; (5) prompt sensitivity — same prompt can help/hurt across models; (6) multilingual and domain-generalization gaps; (7) manual evaluation scalability and subjectivity.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>These limitations motivated the extended-answer-matching approach, CheckList tests, human-sampled calibration, and careful interpretation: although LLMs improve across generations, substantial room remains for robust, generalizable evaluation and for improving LLM reasoning determinism.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Beyond accuracy: Behavioral testing of nlp models with checklist <em>(Rating: 2)</em></li>
                <li>Holistic evaluation of language models <em>(Rating: 2)</em></li>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Language models as knowledge bases? <em>(Rating: 2)</em></li>
                <li>Gpt-4 technical report <em>(Rating: 2)</em></li>
                <li>A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3983",
    "paper_id": "paper-e64003447d9d186b96a1e557b97d514997a4b76e",
    "extraction_schema_id": "extraction-schema-92",
    "extracted_data": [
        {
            "name_short": "Extended Answer Matching",
            "name_full": "Extended Exact Match / Extended Answer Matching (constituent-extraction + alias expansion + mBERT cosine similarity)",
            "brief_description": "An evaluation method introduced in this paper that converts LLM free-text QA outputs into an EM-style correctness decision by (1) extracting candidate answer phrases via constituency parsing, (2) expanding gold answers with Wikidata aliases, and (3) performing fuzzy matching using m-BERT cosine similarity with an empirically chosen threshold.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_criteria": "Semantic equivalence between the LLM-produced answer phrase(s) and the reference KB answer(s) (i.e., whether an LLM's textual output corresponds to the canonical KB answer or its aliases).",
            "evaluation_methods": "1) Parse LLM output with constituent trees to extract NP/VP root candidate phrases; 2) build reference-answer pool by expanding gold answers with multilingual alias lists from Wikidata; 3) compute maximum cosine similarity between candidate and reference using m-BERT embeddings; 4) apply fuzzy-thresholded matching (non-NUM/DATE/Boolean types) to decide correctness.",
            "benchmark_or_dataset": "Applied across the paper's combined KB-based Complex Question Answering benchmark set (eight datasets, ~194k collected samples including KQApro, LC-quad2.0, WQSP, CWQ, GrailQA, GraphQ, QALD-9, MKQA).",
            "metrics_reported": "Exact Match (EM) computed with fuzzy matching; reported effects on false negative/positive rates (raw EM threshold=1 had 2.38%-4.17% false negatives; fuzzy-matching threshold 0.78 reduces average false rate from 3.89% to 2.71%). Accuracy sensitivity curves vs threshold shown; chosen threshold = 0.78.",
            "human_involvement": "Human annotators manually verified a stratified sample of 3,000 LLM outputs (binary correct/incorrect) to (a) compute gold-to-alias minimum similarity (lower bound 0.38) and (b) select the empirical threshold of 0.78 that minimized average false rate across models.",
            "limitations_or_challenges": "Relies on correct constituency extraction (parser errors can harm candidate pools); fuzzy threshold is empirical and may introduce false positives if mis-set; excludes fuzzy matching for NUM/DATE/Boolean types; coverage depends on Wikidata aliases; LLMs' verbose context and paraphrasing still risk mismatch; does not fully measure explanatory quality or correctness of reasoning trace — only answer equivalence.",
            "llm_theory_example": null,
            "evaluation_results": "Using extended matching enabled large-scale, automated EM-style evaluation of LLM outputs; empirical threshold 0.78 stabilized model accuracy (&gt;0.7) and decreased average false rate from 3.89% to 2.71%, allowing comparison of GPT-family and non-GPT LLMs on KBQA datasets.",
            "uuid": "e3983.0"
        },
        {
            "name_short": "CheckList testing (MFT/INV/DIR)",
            "name_full": "CheckList black-box behavioral testing paradigm (Minimum Functionality Test, Invariance test, Directional Expectation test) as applied to KBQA",
            "brief_description": "The paper applies the CheckList testing strategy to LLM QA outputs to probe capability (MFT), robustness to non-answer-affecting perturbations (INV), and controllability/directional behavior (DIR) using targeted test designs including paraphrases, spelling errors, prompt-modifications, and CoT-guided multi-round interactions.",
            "citation_title": "Beyond accuracy: Behavioral testing of nlp models with checklist",
            "mention_or_use": "use",
            "evaluation_criteria": "Capability (can the model perform required reasoning operations?), stability/robustness (does correctness survive paraphrase or minor perturbations?), and controllability/directional expectation (does model behaviour change in predictable ways when input is modified?).",
            "evaluation_methods": "MFT: evaluate questions containing only single reasoning type(s) vs multiple reasoning operations; INV: generate two perturbations per input (introduce random spelling errors; paraphrase semantically equivalent question) and test consistency across original+2 variants (three runs); DIR: three modes — (1) replace reasoning-operation phrases and check whether outputs follow modifications (manual/keyword SPARQL checks), (2) add explicit answer-type prompts and measure EM changes, (3) CoT-inspired multi-round prompting to provide key context then ask question and measure EM deltas.",
            "benchmark_or_dataset": "Applied to the collected KB-based CQA datasets (~194k samples) with focused subsets for MFT/INV/DIR tests; DIR tests often sampled or targeted question types (e.g., SetOperation, Filtering, Counting, Comparison).",
            "metrics_reported": "MFT: EM per reasoning-type for single vs multiple reasoning (Table 6). INV: stability categories (CCC, CCW, CWC, ... WWW) and computed Stability Rate (percentage where all three variants consistent), e.g., GPT-4 Stability Rate = 91.70, ChatGPT = 79.06, GPT-3 = 76.76. DIR: percentage of expected outputs (Table 8 per reasoning type, e.g., ChatGPT overall = 73.75%), EM deltas when adding answer-type prompts (Table 9) and EM deltas from CoT prompting per answer/reasoning type (Table 10).",
            "human_involvement": "Manual design of INV/DIR test cases; in DIR first mode manual observation of SPARQL keyword presence was used to assess whether model applied expected reasoning; human labelling of correctness consistency for INV runs.",
            "limitations_or_challenges": "Black-box behavioral tests detect empirical behaviors but do not explain internal causes; manual test-case creation and manual observation (e.g., SPARQL keyword checks) are labor-intensive and can be subjective; DIR/CoT effects are model- and prompt-sensitive so results may not generalize; INV stability depends on randomness in LLM outputs; checks measure surface behavior not deeper scientific validity.",
            "llm_theory_example": null,
            "evaluation_results": "Key findings: (a) MFT — ChatGPT performs better on multiple reasoning in several categories but still poor on Counting; (b) INV — model stability improved across GPT generations to GPT-4 (91.70% stability); (c) DIR — models showed substantial randomness in applying explicit reasoning operations (~65-85% expected-output rates depending on operation/model), addition of answer-type prompts had mixed effects across models, and CoT prompting significantly improved numerical (NUM) questions (~+30% EM) and some reasoning types while sometimes harming others.",
            "uuid": "e3983.1"
        },
        {
            "name_short": "KBQA Benchmark Suite (8 datasets)",
            "name_full": "Collected KB-based Complex Question Answering benchmark (KQApro, LC-quad2.0, WQSP, CWQ, GrailQA, GraphQ, QALD-9, MKQA)",
            "brief_description": "A multi-dataset evaluation set assembled and used in this paper, comprising six English and two multilingual KB-based complex QA datasets totaling ~194,782 collected samples to stress-test LLMs on multi-hop, set operations, comparisons, and other compositional reasoning tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_criteria": "Accurate retrieval/production of exact KB answers (entity names, dates, numbers, booleans) and correct execution of compositional reasoning (multi-hop, filtering, set operations, counting, comparisons).",
            "evaluation_methods": "Zero-shot QA by LLMs (no external KB queries) evaluated against gold answers using the paper's extended answer matching and EM/F1 metrics; feature-driven unified labeling of questions by answer type, reasoning type, and language-type to enable fine-grained analysis.",
            "benchmark_or_dataset": "The eight datasets: KQApro (complex compositional programs over KB), LC-quad2.0, WQSP (WebQuestionsSP), CWQ (ComplexWebQuestions), GrailQA, GraphQ, QALD-9 (multilingual), MKQA (multilingual). Collected sizes reported in Table 2.",
            "metrics_reported": "Dataset-specific metrics: Accuracy/Exact Match or F1 depending on dataset; overall and per-answer-type/per-reasoning-type EM/F1; multilingual per-language EM (Table 4); comparative results across GPT family and FLAN-T5 (Table 3 & 4).",
            "human_involvement": "Datasets originate from previous human annotation; additional manual sampling and verification by authors used for threshold selection and some evaluations; some datasets were sampled due to API limits.",
            "limitations_or_challenges": "Datasets vary in recency and annotation rules requiring unified tagging; sampling applied to large datasets (MKQA, GrailQA) due to API constraints may bias results; some dataset metrics (EM vs F1) differ complicating cross-dataset comparison; KB alias coverage affects matching; open-domain vs domain-specific generalization uncertain.",
            "llm_theory_example": null,
            "evaluation_results": "Overall: newer GPT-family models improved across datasets (GPT-4 approaching or exceeding SOTA on some older datasets), but LLMs still lag SOTA on newer, more challenging KBQA datasets (e.g., KQApro, GrailQA). Per-language and per-feature analyses revealed strengths (Boolean, ORG, LOC, set ops) and weaknesses (NUM, DATE, counting).",
            "uuid": "e3983.2"
        },
        {
            "name_short": "Metrics & Quantitative Measures",
            "name_full": "Evaluation metrics: Exact Match (EM), F1, m-BERT cosine similarity threshold, Stability Rate, DIR expected-output percentage, EM deltas from prompting",
            "brief_description": "Set of quantitative metrics and measures the paper uses to quantify LLM correctness, robustness, controllability, and sensitivity to prompts.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_criteria": "Correctness (EM/F1), semantic similarity (m-BERT cosine), robustness/consistency (stability rate across INV variants), controllability (DIR expected-output % and EM deltas when adding prompts or CoT).",
            "evaluation_methods": "Compute EM or F1 per dataset; when LLM output is free text, compute similarity with m-BERT and use threshold (0.78) for fuzzy EM; for INV compute pattern of correctness across original+perturbed inputs and classify as CCC/CCW/etc.; for DIR compute presence of expected SPARQL keywords and percent of outputs conforming to expected reasoning, plus EM changes when adding prompts or CoT.",
            "benchmark_or_dataset": "Applied across the paper's KBQA benchmark (eight datasets) and targeted INV/DIR subsets used in tables and figures.",
            "metrics_reported": "EM and F1 scores per model/dataset (Tables 3-5); chosen cosine threshold 0.78; average false rates 3.89% -&gt; 2.71% after fuzzy matching; Stability Rate values (GPT-4 91.70%, ChatGPT 79.06%, etc.); DIR expected-output percentages (e.g., ChatGPT overall 73.75%), EM deltas from prompting and CoT (Table 9 and 10: NUM improvements often +30%+).",
            "human_involvement": "Manual labeling to compute ground-truth and to validate threshold and sampled correctness for sensitivity analysis and stability categorizations.",
            "limitations_or_challenges": "EM and F1 do not capture quality of explanations or reasoning chains; cosine-similarity thresholding is empirical and language/model dependent; stability categories don't explain why inconsistency occurs; some metrics (presence of SPARQL keywords) are heuristic and depend on model's ability to emit SPARQL-like tokens.",
            "llm_theory_example": null,
            "evaluation_results": "Metrics reveal that while EM/F1 aggregate performance improves across GPT generations, significant variability remains across question types and languages; CoT yields large EM gains on numerical questions; stability (INV) improves markedly in GPT-4.",
            "uuid": "e3983.3"
        },
        {
            "name_short": "Human / Manual Evaluation",
            "name_full": "Manual verification and human-in-the-loop evaluation steps used in thresholding, sampling, and qualitative checks",
            "brief_description": "Human reviewers were used to label sampled LLM outputs as correct/incorrect for threshold selection, to inspect DIR SPARQL keyword presence, and to perform some manual evaluations where automated EM is insufficient.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_criteria": "Binary correctness judgments on sampled LLM outputs; manual inspection of SPARQL-like outputs for expected reasoning keywords; verification of alias matching and error analysis.",
            "evaluation_methods": "Annotated 3,000 sampled LLM answers (binary correct/incorrect) to (a) derive empirical cosine threshold and (b) measure model accuracy sensitivity; manual observation/annotation used in DIR SPARQL keyword checks and to validate selected automated matches.",
            "benchmark_or_dataset": "Applied to samples drawn from the KBQA benchmark; used particularly for fuzzy-matching threshold selection and validation of extended matching.",
            "metrics_reported": "Binary correctness labels used to compute false positive/negative rates and to select a cosine threshold minimizing average false rate; sampling-based accuracy measurements as threshold varies (Figure 3b).",
            "human_involvement": "Essential: humans performed the binary correctness labeling and manual SPARQL observation; paper notes that many previous works relied on manual evaluation due to difficulties measuring LLM outputs with exact-match metrics.",
            "limitations_or_challenges": "Manual evaluation is costly and scales poorly (hence sampling); subjective judgments possible; manual checks were limited to samples not the full dataset; reliance on human labels can introduce annotation biases.",
            "llm_theory_example": null,
            "evaluation_results": "Human labeling enabled calibration of fuzzy matching (setting threshold to 0.78) and quantified that automated extended matching reduced average false rate from 3.89% to 2.71% on sampled data.",
            "uuid": "e3983.4"
        },
        {
            "name_short": "Chain-of-Thought (CoT) prompting evaluation",
            "name_full": "Chain-of-Thought prompting (CoT) used as an evaluation/control technique",
            "brief_description": "The paper evaluates the impact of CoT-inspired multi-round and stepwise prompting on the LLMs' ability to produce correct answers and follow directional expectations, observing that CoT often improves numerical and certain reasoning operations but may harm other question types.",
            "citation_title": "Chain of thought prompting elicits reasoning in large language models",
            "mention_or_use": "use",
            "evaluation_criteria": "Does guiding the model through intermediate reasoning steps (or providing key noun-related context before the question) increase EM correctness, controllability, or expected reasoning behavior?",
            "evaluation_methods": "DIR third mode: provide crucial information step-by-step (naive CoT-guided process) then ask original question; measure EM deltas per answer type and reasoning type (Table 10); compare before/after CoT.",
            "benchmark_or_dataset": "Applied to subsets of KBQA datasets and to DIR test cases; aggregated EM deltas reported per answer and reasoning type.",
            "metrics_reported": "EM deltas per answer type and reasoning type (Table 10), e.g., NUM-type improvements between +20% and +38% depending on model; set operation and counting gains also notable; CoT's largest consistent positive impact on NUM-type questions (~+30%).",
            "human_involvement": "Prompt design and interpretation of CoT effects involved human researchers; evaluation of outcome correctness used automated extended matching supported by human-sampled validation.",
            "limitations_or_challenges": "CoT can improve some categories but negatively affect others; the effectiveness is model-dependent (newer models more sensitive); CoT increases prompt engineering complexity and may not generalize across tasks and languages.",
            "llm_theory_example": null,
            "evaluation_results": "CoT improved performance substantially for numerical questions across GPT models (often &gt;+30% EM), improved performance for set operations and counting, had modest or negative effects on some other answer types (MISC, PER), and its effect varies by model (GPT-4 saw meaningful gains, others varied).",
            "uuid": "e3983.5"
        },
        {
            "name_short": "Limitations / Challenges (evaluation)",
            "name_full": "Observed limitations and open challenges in evaluating LLM QA outputs (as discussed in paper)",
            "brief_description": "A consolidated summary of practical and conceptual evaluation challenges the authors report when measuring LLM outputs against KB answers, including metric mismatches, alias and paraphrase handling, stability/randomness, prompt sensitivity, manual-evaluation costs, and dataset sampling constraints.",
            "citation_title": "",
            "mention_or_use": "use",
            "evaluation_criteria": "N/A (this entity records limitations rather than a method), but impacts evaluation validity, generalizability, and reliability.",
            "evaluation_methods": "N/A — this entry aggregates reported empirical and methodological challenges encountered while applying the paper's evaluation protocol.",
            "benchmark_or_dataset": "Challenges affect the entire KBQA benchmark suite used in the paper (e.g., sampling limitations for MKQA/GrailQA due to API limits).",
            "metrics_reported": "Empirical numbers illustrating some limitations: EM-only evaluation yields 2.38%-4.17% false negatives (mean 3.89%); after fuzzy matching average false rate reduced to 2.71%; stability rates show non-negligible randomness (GPT-3 ~76.76% stability), and DIR expected-output rates ~60-75% indicating reasoning randomness.",
            "human_involvement": "Paper emphasizes human cost: manual evaluation is expensive and many prior works use small sample sizes (&lt;10k); authors used manual sampling (3,000) to calibrate fuzzy matching but cannot manually verify entire large-scale datasets.",
            "limitations_or_challenges": "Key challenges: (1) Exact Match unsuitable for LLM free-text output; (2) need for fuzzy/semantic matching with empirical thresholds; (3) parser/alias coverage limits; (4) LLM output randomness reduces stability; (5) prompt sensitivity — same prompt can help/hurt across models; (6) multilingual and domain-generalization gaps; (7) manual evaluation scalability and subjectivity.",
            "llm_theory_example": null,
            "evaluation_results": "These limitations motivated the extended-answer-matching approach, CheckList tests, human-sampled calibration, and careful interpretation: although LLMs improve across generations, substantial room remains for robust, generalizable evaluation and for improving LLM reasoning determinism.",
            "uuid": "e3983.6"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Beyond accuracy: Behavioral testing of nlp models with checklist",
            "rating": 2
        },
        {
            "paper_title": "Holistic evaluation of language models",
            "rating": 2
        },
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2
        },
        {
            "paper_title": "Language models as knowledge bases?",
            "rating": 2
        },
        {
            "paper_title": "Gpt-4 technical report",
            "rating": 2
        },
        {
            "paper_title": "A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity",
            "rating": 1
        }
    ],
    "cost": 0.017106749999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Can ChatGPT Replace Traditional KBQA Models? An In-depth Analysis of the Question Answering Performance of the GPT LLM Family</h1>
<p>Yiming Tan ${ }^{1,4 \star}$, Dehai Min ${ }^{2,4 \star}$, Yu $\mathrm{Li}^{2,4}$, Wenbo $\mathrm{Li}^{3}$, Nan $\mathrm{Hu}^{2,4}$, Yongrui Chen $^{2,4}$, and Guilin $\mathrm{Qi}^{2,4 \star \star}$<br>${ }^{1}$ School of Cyber Science and Engineering, Southeast University, Nanjing, China<br>${ }^{2}$ School of Computer Science and Engineering, Southeast University, Nanjing, China<br>${ }^{3}$ School of Computer Science and Technology, Anhui Unviersity, Hefei, China<br>${ }^{4}$ Key Laboratory of New Generation Artificial Intelligence Technology and Its Interdisciplinary Applications (Southeast University), Ministry of Education, China<br>{tt_yymm,zhishanq, yuli_11, nanhu, yrchen, gqi}@seu.edu.cn, wenboli@stu.ahu.edu.cn</p>
<h4>Abstract</h4>
<p>ChatGPT is a powerful large language model (LLM) that covers knowledge resources such as Wikipedia and supports natural language question answering using its own knowledge. Therefore, there is growing interest in exploring whether ChatGPT can replace traditional knowledge-based question answering (KBQA) models. Although there have been some works analyzing the question answering performance of ChatGPT, there is still a lack of large-scale, comprehensive testing of various types of complex questions to analyze the limitations of the model. In this paper, we present a framework that follows the blackbox testing specifications of CheckList proposed by [38]. We evaluate ChatGPT and its family of LLMs on eight real-world KB-based complex question answering datasets, which include six English datasets and two multilingual datasets. The total number of test cases is approximately 190,000. In addition to the GPT family of LLMs, we also evaluate the well-known FLAN-T5 to identify commonalities between the GPT family and other LLMs. The dataset and code are available at https://github.com/tan92hl/Complex-Question-Answering-Evaluation-o f-GPT-family.git</p>
<p>Keywords: Large language model $\cdot$ Complex question answering $\cdot$ Knowledge base $\cdot$ ChatGPT $\cdot$ Evaluation $\cdot$ Black-box testing.</p>
<h2>1 Introduction</h2>
<p>Given its extensive coverage of knowledge from Wikipedia as training data and its impressive natural language understanding ability, ChatGPT has demonstrated powerful question-answering abilities by leveraging its own knowledge.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Additionally, a study conducted by [30] suggests that language models can be considered as knowledge bases (KBs) to support downstream natural language processing (NLP) tasks. This has led to growing interest in exploring whether ChatGPT and related large language models (LLMs) can replace traditional Knowledge-Based Question Answering (KBQA) models.</p>
<p>There have been many evaluations of ChatGPT 52|19|7|54|16|26|47|46|33|2], some of which include the testing of question answering tasks and have yielded interesting conclusions: for example, 26] showed that ChatGPT has lower stability than traditional KBQA models on a test set of 200 questions, and 2] found that ChatGPT is a "lazy reasoner" that suffers more with induction after analyzing 30 samples. However, due to the limited number of test cases, it is difficult to perform a comprehensive evaluation of ChatGPT's performance on the KBQA task based on these findings. Moreover, the reliability of these findings still requires further testing for validation. We find that the difficulty in answer evaluation is the main reason why existing works have not conducted large-scale KBQA tests on ChatGPT, which outputs sentences or paragraphs that contain answers rather than an exact answer. Furthermore, due to the influence of the generated textual context, the answer sequence of ChatGPT may not necessarily correspond strictly to entity names in the knowledge base. Therefore, the traditional Exact Match (EM) metric cannot directly evaluate the output of ChatGPT for question-answering. Consequently, most of the works mentioned above rely on manual evaluation.</p>
<p>In this paper, we select the KB-based Complex Question Answering (KBbased CQA) task to comprehensively evaluate the ability of LLMs to answer complex questions based on their own knowledge. This task requires the model to use compositional reasoning to obtain the answer to the question, which includes multi-hop reasoning, attribute comparison, set operations, and other complex reasoning. We believe that evaluating ChatGPT's performance in complex knowledge question answering using its own knowledge can help us understand whether existing LLMs have the potential to surpass traditional KBQA models or whether ChatGPT is already capable of replacing the current best KBQA models. Therefore, we collect test data from existing KB-based CQA datasets and establish an evaluation framework.</p>
<p>Our evaluation framework consists of two parts: 1) the feature-driven unified labeling method is established for the KBQA datasets involved in the testing; and 2) the evaluation of answers generated by LLMs. Inspired by the approach of using multiple scenario tags to evaluate language models in the HELM framework [21], we label each test question with unified answer-type, inference-type, and language-type tags. In the answer evaluation part, we first improve the Exact Match (EM) method so that it can be used to evaluate the accuracy of LLMs' output. The main process of improved EM is to extract potential answer phrases from the LLM output through constituent trees as the candidate answer pool, and then match them with the reference answer pool formed by annotated answers and aliases provided by wikidata. Next, we follow the CheckList testing specification 38] and set up three tests: the minimal functionality test (MFT),</p>
<p>invariance test (INV) [40], and directional expectation test (DIR). Along with an overall evaluation, these tests assess the LLMs' capability, stability, and control when answering questions and performing specific reasoning operations.</p>
<p>Finally, we collect six English real-world KB-based CQA datasets and two multilingual real-world KB-based CQA datasets for our evaluation experiment, with a scale of approximately 190,000 questions, including approximately 12,000 multilingual questions covering 13 languages. In the experiment, we mainly compare the QA performance differences between the traditional the current state-of-the-art (SOTA) models and the GPT family models [4,28,27]. In addition, we also introduce the open-source LLM FLAN-T5 [9] model as a representative of the non-GPT family for comparison. Like ChatGPT, all the LLMs involved in the comparison in this paper use their own knowledge to answer questions and are considered unsupervised models.</p>
<p>Our key findings and insights are summarized as follows:
ChatGPT and the LLMs of GPT family outperform the best traditional models on some old datasets like WQSP and LC-quad2.0, but they still lag behind the current state-of-the-art on the latest released KBQA datase such as KQApro and GrailQA.</p>
<p>GPT family LLMs and the FLAN-T5 model tend to have similar tendencies in terms of strengths and weaknesses when answering different types of questions.</p>
<p>Using chain-of-thought prompts in CheckList testing enhances GPT LLMs' ability to answer specific questions but may negatively impact other question types, suggesting their potential and sensitivities for future task-specific applications.</p>
<h1>2 Related Work</h1>
<h3>2.1 Large language models and prompting</h3>
<p>In recent years, LLMs and prompt learning have attracted considerable attention. Groundbreaking studies such as [30,17,4] revealed that LLMs, when given appropriate textual prompts, can perform a wide range of NLP tasks with zeroshot or few-shot learning without gradient updates. On the one hand, improved prompting can enable the information contained in the LLM to be more accurately applied to the target task, and early representative works include [37,34] The chain-of-thought (CoT) [48] method is a distinguished approach in effective prompt research. CoT enables LLMs to have a better understanding and think more when answering questions. On the other hand, much work has been done to improve the natural language understanding ability of LLMs, including Gopher [35] and PaLM [8], which aim to extend LLMs. Undoubtedly, ChatGPT has garnered significant attention as a prominent LLM due to its remarkable natural language understanding abilities. It is trained on the GPT-3.5 series of models [11] using RLHF.</p>
<h1>2.2 Evaluation of the large language model</h1>
<p>While LLMs have demonstrated outstanding natural language understanding and generation capabilities, it is still necessary to further research their strengths, limitations, and potential risks to fully understand their advantages. Recently, many works aimed at evaluating LLMs have been proposed [6], including general benchmarks like HELM [21], Bigbench [41], Promptbench [53], and MME [10]. These aim to categorize and summarize multiple existing tasks, providing a macro-level assessment of LLM performance and potential biases. Other studies focus on specific NLP tasks, such as summarization [2], question-answering [21|26], and machine translation [23]. In these existing works, the advantages of the general benchmark approaches lie in their fine-grained sample classification and high testing efficiency. However, these benchmarks are limited by the use of automated metrics, which restrict the diversity of testing objectives. On the other hand, evaluating task-specialized LLMs introduces more manually defined testing objectives, such as interpretability, determinism, robustness, and question understanding. Nevertheless, due to manual testing costs, these evaluations often rely on small samples (less than 10k) and coarsely categorized datasets.</p>
<p>In this paper, we combine the strengths of both benchmark studies and taskspecific manual evaluations to test the GPT family LLMs. To achieve this, we adopt a strategy inspired by HELM [21], which uses multiple feature labels to describe and categorize task types, especially complex problem types. Additionally, we incorporate the manually predefined testing objectives from [26] and combine them with the CheckList natural language model's black-box testing strategy. This comprehensive and diverse testing approach allows us to draw more comprehensive and valuable conclusions.</p>
<h3>2.3 Black-box testing of the NLP model</h3>
<p>The prohibitive expense associated with training LLMs renders white-box testing an impractical approach. Consequently, the majority of assessment efforts presently concentrate on black-box evaluation approaches for LLMs. For example, the methods used by $[3,39]$ for evaluating robustness, the methods used by [49] for adversarial changes, and attention and interpretability within LLMs research conducted by [45]. The most comprehensive approach currently available is the CheckList approach proposed by [38], which categorizes evaluation targets into three parts: the minimum functionality test (MFT), invariance test (INV), and directional expectation test (DIR). The MFT examines a model's basic functionality, INV examines whether the model can maintain functional correctness when non-answer-affecting information is added to the input, and DIR examines whether the model can output the expected result when the input is modified. In this work, we follow the idea of CheckList and use CoT prompting to generate test cases for DIR.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1. Overview of proposed Evaluation Framework.</p>
<h1>3 Evaluation Framework</h1>
<p>As mentioned in Section 1, our KBQA evaluation framework consists of two parts. The first part aims to assign uniform feature labels to the questions in the datasets. The second part includes an improved Exact Match answer evaluation strategy and an extended CheckList test. Figure 1 illustrates the overall process of the framework. The detailed process is described in the following section.</p>
<h3>3.1 Feature-driven unified question labeling</h3>
<p>We collect multiple existing KB-based CQA datasets for the evaluation. However, due to the different annotation rules used for features such as answer and reasoning type in each dataset, we need to establish a standardized and unified set of question feature tags for evaluating and analyzing question types.</p>
<p>Referring to the question tags provided by existing KBQA datasets 24|22|5|51, we categorize the tags that describe the features of complex questions into three types, including answer type, reasoning type and language type. Table 1 lists the eight answer type tags and seven reasoning type tags we defined. Generally,</p>
<p>Table 1. The feature-driven question tags defined in this paper.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Answer type</th>
<th style="text-align: left;">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">MISC</td>
<td style="text-align: left;">The answer to the question is the miscellaneous fact de- <br> fined by the named entity recognition task.</td>
</tr>
<tr>
<td style="text-align: left;">PER</td>
<td style="text-align: left;">The answer to the question is the name of a person.</td>
</tr>
<tr>
<td style="text-align: left;">LOC</td>
<td style="text-align: left;">The answer to the question is a location. <br> The answer explains the reasons for the facts mentioned <br> in the question.</td>
</tr>
<tr>
<td style="text-align: left;">DATE</td>
<td style="text-align: left;">The answer to the question is a date or time. <br> The answer to the question is a number.</td>
</tr>
<tr>
<td style="text-align: left;">NU</td>
<td style="text-align: left;">The answer to the question is yes or no. <br> The answer to the question is the name of a organization.</td>
</tr>
<tr>
<td style="text-align: left;">Boolean</td>
<td style="text-align: left;">The input question is unable to answer.</td>
</tr>
<tr>
<td style="text-align: left;">ORG</td>
<td style="text-align: left;">Description</td>
</tr>
<tr>
<td style="text-align: left;">UNA</td>
<td style="text-align: left;">The process of obtaining answers involves set operations. <br> The answer is obtained through condition filtering.</td>
</tr>
<tr>
<td style="text-align: left;">Reasoning type</td>
<td style="text-align: left;">The process of obtaining an answer involves counting op- <br> erations.</td>
</tr>
<tr>
<td style="text-align: left;">SetOperation</td>
<td style="text-align: left;">The answer needs to be obtained by comparing or sorting <br> numerical values.</td>
</tr>
<tr>
<td style="text-align: left;">Filter</td>
<td style="text-align: left;">Answering questions requires a single-hop Reasoning. <br> Answering questions requires multi-hop Reasoning.</td>
</tr>
<tr>
<td style="text-align: left;">Clustering</td>
<td style="text-align: left;">The reasoning graph corresponding to inputting question <br> is star-shape.</td>
</tr>
</tbody>
</table>
<p>a question contains one answer type tag, one language type tag and several reasoning type tags. Figure 2 presents the label distribution of the data collected in this paper. For an input question, our labeling process is as follows: when the dataset provides question type tags, we simply match them to our feature tag list. When no tag is provided, we use an existing bert-base-NER model 4418 to identify the type of answer, and use keywords in SPARQL to identify the type of inference.</p>
<h1>3.2 Answer evaluation</h1>
<p>The output of traditional KBQA models typically takes two forms: either a SPARQL query or a precise answer. The evaluation strategy for traditional KBQA models is based on exact match (EM), which involves comparing the model's output with a reference answer or to assess its accuracy. However, without adding additional prompts, LLMs generate text paragraphs containing answers, rather than precise answers. Furthermore, this answer may be a restatement of the reference answer.</p>
<p>Extended Answer Matching To obtain evaluation results on KBQA outputs of LLMs resembling exact match, we propose an extended answer matching approach. This approach consists of three main parts: 1) Parsing LLMs' output using constituent trees [14] to extract NP or VP root node phrases as the can-</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 2. The distribution of feature labels in the collect KB-based CQA datasets
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 3. (a) The GPT family and T5 models show changing error rates on sampled questions as the threshold varies. (b) LLMs' QA accuracy (evaluated manually) on sampled questions varies with the threshold.
didate answer pool. 2) Expanding each reference answer using multilingual alias lists from Wikidata, including various names and aliases. 3) Using m-bert 18 to calculate the maximum Cosine similarity between reference and candidate answers for precise match evaluation, with a fuzzy matching strategy applied only to non-"NUM, DATE, Boolean" answer types.</p>
<p>Threshold Selection and Sensitivity Analysis As shown in Figure 3 (a), the analysis of various models reveals that using only EM evaluation for answers (threshold=1) may result in $2.38 \%-4.17 \%$ (average $3.89 \%$ ) false negative cases. To address this issue, we opt for establishing a fuzzy matching process based on cosine similarity to alleviate the problem. However, selecting an inadequate threshold may introduce additional false positive issues. Therefore, we followed the steps below to find an empirical threshold that minimizes the overall false rate (false pos + false neg) across all models: (1) We randomly sampled 3000 question samples from the test data of answer types involved in fuzzy matching and manually verified the correctness of the six LLM output answers shown in Figure</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig. 4. Test cases design for INV and DIR.</p>
<p>3(a) (binary labels, correct/incorrect). (2) We calculate the minimum cosine similarity (the value is 0.38 ) between the gold answer and its aliases, and used it as the lower bound for finding the threshold. (3) We observed the changes in false rates for each model as the threshold increased from 0.38 to 1 and selected the threshold of 0.78 that minimized the average false rate across models. From the Figure 3(a), it can be observed that the false rates of each model stabilize around this value. To evaluate the sensitivity of model performance to the threshold, as shown in Figure 3(b), we compared the accuracy of each model on the test data as the threshold varied. The accuracy of each model tended to stabilize when the threshold was $&gt;0.7$. Finally, we use 0.78 as the empirical threshold for further experiments. Sampling tests show that this threshold decreases the average false rate from $3.89 \%$ to $2.71 \%$.</p>
<h1>3.3 CheckList testing</h1>
<p>Following the idea of CheckList, we also evaluate ChatGPT and other LLMs with three distinct objectives: (1) to evaluate the ability of LLMs to handle each feature in KB-based CQA through the MFT; (2) to evaluate the robustness of LLMs' ability to handle various features in KB-based CQA scenarios through the INV; and (3) to evaluate whether the outputs of LLMs meet human expectations</p>
<p>for modified inputs through the DIR, the controllability. The specific INV and DIR procedures are presented as follows, and Figure 4 presents the instances:</p>
<p>Minimum Functionality Test In this work, we choose to examine the performance of LLMs in performing basic reasoning tasks by only including quesitons that involve a single type of reasoning operation. We then compare and analyze the performance differences of the models in answering questions that require performing a single reasoning operation versus those that require performing multiple reasoning operations.</p>
<p>Invariance Test We designe two methods to generate test cases for INV: the first method is to randomly introduce spelling errors into the original sentence, and the second method is to generate a question that is semantically equivalent (paraphrased) to the original sentence. Subsequently, we evaluate the invariance of the LLMs by checking the consistency of their correctness in the outputs generated from three inputs, i.e. the original test sentence, the version of the question with added spelling errors, and the paraphrased question.</p>
<p>Directional Expectation Test In this study, we designed three modes for DIR test cases: (1) Replacing phrases related to reasoning operations in questions, to observe if LLMs' outputs correspond to our modifications. (2) Adding prompts with answer types after the original question text to check LLMs' ability to control the output answer type. (3) Using multi-round questioning inspired by CoT , where LLMs consider information related to key nouns before asking the original question, to observe the effectiveness and sensitivity of CoT prompts for different question types.</p>
<h1>4 Experiments</h1>
<h3>4.1 Datasets</h3>
<p>To highlight the complexity of the testing questions and the breadth of the testing dataset, after careful consideration, we selected six representative English monolingual KBQA datasets and two multilingual KBQA datasets for evaluation. These datasets include classic datasets such as WebQuestionSP [51], ComplexWebQuestions [43], GraphQ [42] and QALD-9 [24], as well as newly proposed datasets such as KQApro [5], GrailQA [12] and MKQA [22]. Due to the limitations of the OpenAI API, we sampled some datasets, such as MKQA (sampled by answer type) and GrailQA (only using the test set). The collection size for each dataset and the scale we collected are summarized in Table 2.</p>
<h3>4.2 Comparative models</h3>
<p>State-of-the-art models for each dataset We introduce current SOTA models' report scores from the KBQA leaderboard [29] for each dataset as traditional KBQA models in this paper for comparison. This primarily reflects the comparison between LLMs and traditional KBQA models in terms of the overall results.</p>
<p>Large-language models of the GPT family ChatGPT is a landmark model in the GPT family, and we believe that comparing it to its predecessors</p>
<p>Table 2. The Statistical of collected KB-based CQA datasets, "Col. Size" represents the size of the dataset we collected in our experiments. "Size" denotes the original size of the dataset.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Datasets</th>
<th style="text-align: left;">Size</th>
<th style="text-align: left;">Col. Size</th>
<th style="text-align: left;">Lang</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">KQApro</td>
<td style="text-align: left;">117,970</td>
<td style="text-align: left;">106,173</td>
<td style="text-align: left;">EN</td>
</tr>
<tr>
<td style="text-align: left;">LC-quad2.0</td>
<td style="text-align: left;">26,975</td>
<td style="text-align: left;">26,975</td>
<td style="text-align: left;">EN</td>
</tr>
<tr>
<td style="text-align: left;">WQSP</td>
<td style="text-align: left;">4737</td>
<td style="text-align: left;">4,700</td>
<td style="text-align: left;">EN</td>
</tr>
<tr>
<td style="text-align: left;">CWQ</td>
<td style="text-align: left;">31,158</td>
<td style="text-align: left;">31,158</td>
<td style="text-align: left;">EN</td>
</tr>
<tr>
<td style="text-align: left;">GrailQA</td>
<td style="text-align: left;">64,331</td>
<td style="text-align: left;">6,763</td>
<td style="text-align: left;">EN</td>
</tr>
<tr>
<td style="text-align: left;">GraphQ</td>
<td style="text-align: left;">4,776</td>
<td style="text-align: left;">4,776</td>
<td style="text-align: left;">EN</td>
</tr>
<tr>
<td style="text-align: left;">QALD-9</td>
<td style="text-align: left;">6,045</td>
<td style="text-align: left;">6,045</td>
<td style="text-align: left;">Mul</td>
</tr>
<tr>
<td style="text-align: left;">MKQA</td>
<td style="text-align: left;">260,000</td>
<td style="text-align: left;">6,144</td>
<td style="text-align: left;">Mul</td>
</tr>
<tr>
<td style="text-align: left;">Total Collected</td>
<td style="text-align: left;"></td>
<td style="text-align: left;">194,782</td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<p>and subsequent versions is very valuable. By doing so, we can observe and analyze the technical increments of the GPT family at each stage and the benefits they bring. In this paper, we compare the GPT family models, which include GPT-3, GPT-3.5 v2, GPT-3.5 v3, ChatGPT (Their names on OpenAI's Model Index document are: text-davinci-001, text-davinci-002, text-davinci-003, gpt-3.5-turbo-0301) and the newest addition, GPT-4 [27].</p>
<p>Large-language model not belongs to GPT family The LLM we have chosen is the famous FLAN-T5 (Text-to-Text Transfer Transformer 11B, [7]), which does not belong to the GPT family. Considering its multilingual questionanswering ability and open-source nature, we have chosen it to participate in the comparison in this paper. FLAN-T5 is an encoder-decoder transformer language model that is trained on a filtered variant of CommonCrawl (C4) 36]. The release date and model size for this model are also based on 36.</p>
<h1>4.3 Overall results</h1>
<p>The overall results are presented in Table 3. First, ChatGPT outperforms the current SOTA traditional models on three of the eight test sets, and the subsequently released GPT-4 surpasses on four test sets. By comparing the performance of GPT-4 and SOTA models, we can see that as LLMs represented by the GPT family, their zero-shot ability is constantly approaching and even surpassing traditional deep learning and knowledge representation models.</p>
<p>Second, comparing models in the GPT family, the newer models perform better than the previous ones, as expected. Interestingly, the performance improvement of the new GPT models is relatively consistent across all datasets, as shown in Figure 5(a), where the line shapes of all GPT models are almost identical. This means that each generation of GPT models retains some commonalities. Based on the known cases, these commonalities may come from the transformerbased encoding. We will discuss in detail the impact they have in section 4.5 In addition, we can observe that the newer versions of the GPT model show increasingly significant improvements compared to the previous generations.</p>
<p>Table 3. Overall results of the evaluation. We compare the exact match of ChatGPT with current SOTA traditional KBQA models (fine-tuned (FT) and zero-shot (ZS)), GPT family LLMs, and Non-GPT LLM. In GraphQ, QALD-9 and LC-quad2, the evaluation metric used is F1, while other datasets use Accuracy (Exact match).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Datasets</th>
<th style="text-align: center;">KQApro</th>
<th style="text-align: center;">LC-quad2</th>
<th style="text-align: center;">WQSP</th>
<th style="text-align: center;">CWQ</th>
<th style="text-align: center;">GrailQA</th>
<th style="text-align: center;">GraphQ</th>
<th style="text-align: center;">QALD-9</th>
<th style="text-align: center;">MKQA</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">Acc</td>
</tr>
<tr>
<td style="text-align: center;">SOTA(FT)</td>
<td style="text-align: center;">93.85 [29]</td>
<td style="text-align: center;">33.10 [31]</td>
<td style="text-align: center;">73.10 [15]</td>
<td style="text-align: center;">72.20 [15]</td>
<td style="text-align: center;">76.31 ${ }^{\dagger}$</td>
<td style="text-align: center;">31.8 [13]</td>
<td style="text-align: center;">67.82 [32]</td>
<td style="text-align: center;">46.00 [22]</td>
</tr>
<tr>
<td style="text-align: center;">SOTA(ZS)</td>
<td style="text-align: center;">94.20 [25]</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">62.98 [50]</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">FLAN-T5</td>
<td style="text-align: center;">37.27</td>
<td style="text-align: center;">30.14</td>
<td style="text-align: center;">59.87</td>
<td style="text-align: center;">46.69</td>
<td style="text-align: center;">29.02</td>
<td style="text-align: center;">32.27</td>
<td style="text-align: center;">30.17</td>
<td style="text-align: center;">20.17</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3</td>
<td style="text-align: center;">38.28</td>
<td style="text-align: center;">33.04</td>
<td style="text-align: center;">67.68</td>
<td style="text-align: center;">51.77</td>
<td style="text-align: center;">27.58</td>
<td style="text-align: center;">38.32</td>
<td style="text-align: center;">38.54</td>
<td style="text-align: center;">26.97</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3.5v2</td>
<td style="text-align: center;">38.01</td>
<td style="text-align: center;">33.77</td>
<td style="text-align: center;">72.34</td>
<td style="text-align: center;">53.96</td>
<td style="text-align: center;">30.50</td>
<td style="text-align: center;">40.85</td>
<td style="text-align: center;">44.96</td>
<td style="text-align: center;">30.14</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3.5v3</td>
<td style="text-align: center;">40.35</td>
<td style="text-align: center;">39.04</td>
<td style="text-align: center;">79.60</td>
<td style="text-align: center;">57.54</td>
<td style="text-align: center;">35.43</td>
<td style="text-align: center;">47.95</td>
<td style="text-align: center;">46.19</td>
<td style="text-align: center;">39.05</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT</td>
<td style="text-align: center;">47.93</td>
<td style="text-align: center;">42.76</td>
<td style="text-align: center;">83.70</td>
<td style="text-align: center;">64.02</td>
<td style="text-align: center;">46.77</td>
<td style="text-align: center;">53.10</td>
<td style="text-align: center;">45.71</td>
<td style="text-align: center;">44.30</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">57.20</td>
<td style="text-align: center;">54.95</td>
<td style="text-align: center;">90.45</td>
<td style="text-align: center;">71.00</td>
<td style="text-align: center;">51.40</td>
<td style="text-align: center;">63.20</td>
<td style="text-align: center;">57.20</td>
<td style="text-align: center;">59.20</td>
</tr>
</tbody>
</table>
<p>Table 4. Comparison of LLMs on multilingual test sets.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Languages</th>
<th style="text-align: center;">FLAN-T5</th>
<th style="text-align: center;">GPT-3</th>
<th style="text-align: center;">GPT-3.5v2</th>
<th style="text-align: center;">GPT-3.5v3</th>
<th style="text-align: center;">ChatGPT</th>
<th style="text-align: center;">GPT-4</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">en</td>
<td style="text-align: center;">30.29</td>
<td style="text-align: center;">57.53</td>
<td style="text-align: center;">56.99</td>
<td style="text-align: center;">64.16</td>
<td style="text-align: center;">66.49</td>
<td style="text-align: center;">66.09</td>
</tr>
<tr>
<td style="text-align: center;">nl</td>
<td style="text-align: center;">20.75</td>
<td style="text-align: center;">50.47</td>
<td style="text-align: center;">54.58</td>
<td style="text-align: center;">60.56</td>
<td style="text-align: center;">65.05</td>
<td style="text-align: center;">69.72</td>
</tr>
<tr>
<td style="text-align: center;">de</td>
<td style="text-align: center;">22.40</td>
<td style="text-align: center;">50.54</td>
<td style="text-align: center;">54.48</td>
<td style="text-align: center;">57.17</td>
<td style="text-align: center;">62.54</td>
<td style="text-align: center;">73.91</td>
</tr>
<tr>
<td style="text-align: center;">es</td>
<td style="text-align: center;">21.68</td>
<td style="text-align: center;">48.22</td>
<td style="text-align: center;">55.70</td>
<td style="text-align: center;">58.50</td>
<td style="text-align: center;">61.87</td>
<td style="text-align: center;">57.69</td>
</tr>
<tr>
<td style="text-align: center;">fr</td>
<td style="text-align: center;">26.16</td>
<td style="text-align: center;">49.46</td>
<td style="text-align: center;">55.02</td>
<td style="text-align: center;">57.89</td>
<td style="text-align: center;">62.19</td>
<td style="text-align: center;">62.00</td>
</tr>
<tr>
<td style="text-align: center;">it</td>
<td style="text-align: center;">24.19</td>
<td style="text-align: center;">47.67</td>
<td style="text-align: center;">52.33</td>
<td style="text-align: center;">58.06</td>
<td style="text-align: center;">58.96</td>
<td style="text-align: center;">73.91</td>
</tr>
<tr>
<td style="text-align: center;">ro</td>
<td style="text-align: center;">22.28</td>
<td style="text-align: center;">44.38</td>
<td style="text-align: center;">50.94</td>
<td style="text-align: center;">54.12</td>
<td style="text-align: center;">59.55</td>
<td style="text-align: center;">63.41</td>
</tr>
<tr>
<td style="text-align: center;">pt_br</td>
<td style="text-align: center;">15.38</td>
<td style="text-align: center;">38.46</td>
<td style="text-align: center;">38.46</td>
<td style="text-align: center;">42.31</td>
<td style="text-align: center;">50.00</td>
<td style="text-align: center;">66.67</td>
</tr>
<tr>
<td style="text-align: center;">pt</td>
<td style="text-align: center;">20.58</td>
<td style="text-align: center;">37.70</td>
<td style="text-align: center;">44.26</td>
<td style="text-align: center;">50.27</td>
<td style="text-align: center;">52.64</td>
<td style="text-align: center;">52.25</td>
</tr>
<tr>
<td style="text-align: center;">ru</td>
<td style="text-align: center;">7.29</td>
<td style="text-align: center;">20.58</td>
<td style="text-align: center;">29.69</td>
<td style="text-align: center;">21.68</td>
<td style="text-align: center;">32.24</td>
<td style="text-align: center;">49.58</td>
</tr>
<tr>
<td style="text-align: center;">hi_in</td>
<td style="text-align: center;">3.61</td>
<td style="text-align: center;">9.93</td>
<td style="text-align: center;">19.13</td>
<td style="text-align: center;">13.54</td>
<td style="text-align: center;">21.48</td>
<td style="text-align: center;">25.00</td>
</tr>
<tr>
<td style="text-align: center;">fa</td>
<td style="text-align: center;">2.45</td>
<td style="text-align: center;">6.59</td>
<td style="text-align: center;">21.09</td>
<td style="text-align: center;">11.49</td>
<td style="text-align: center;">22.03</td>
<td style="text-align: center;">31.71</td>
</tr>
<tr>
<td style="text-align: center;">zh_cn</td>
<td style="text-align: center;">3.65</td>
<td style="text-align: center;">17.45</td>
<td style="text-align: center;">22.40</td>
<td style="text-align: center;">24.87</td>
<td style="text-align: center;">33.46</td>
<td style="text-align: center;">44.62</td>
</tr>
</tbody>
</table>
<p>Third, as shown in Figure 5(a), although FLAN-T5's overall performance is weaker than that of the GPT family, its line shape is quite similar to that of the GPT family. This further supports our inference that the transformer-based architecture leads to commonalities in the abilities of current LLMs.</p>
<h1>4.4 Multilingual KBQA results</h1>
<p>Based on the results from MKQA and QALD-9, we further present the performance of LLMs on multilingual QA in Table 4 Despite the overall trend showing improvement in the model's ability to answer questions in different languages as the GPT family continues to iterate, we observe that GPT-4 has not surpassed ChatGPT in the four languages. This suggests that the evolution of GPT's multilingual capabilities may be starting to slow down. Figure 5(b) shows the line</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Fig. 5. (a) is the line chart based on Table 3, showing the EM scores of each model on different datasets. (b) corresponds to Table 5, with lines representing the EM scores of each model in different languages. (c) and (d) correspond to Table 4, reflecting the trend of EM scores of each model on different types of questions.
chart of the EM scores of the models in each language. We can find that the shapes of the lines for GPT-3 and ChatGPT are very similar, while there is a significant change in the shape of the line for GPT-4. We believe that the main reason for this change is due to the introduction of multimodal data in GPT-4, which plays a positive role in mapping between some languages.</p>
<h1>4.5 Feature tags based results</h1>
<p>The results in Table 5 show the performance of ChatGPT and other LLMs when answering different types of questions. As such, traditional models are not compared in this section. Overall, models from the GPT family are better at answering questions with boolean (yes/no) answers, questions about organizations and locations, as well as those involving set operations and numerical comparisons. However, they do not perform well when answering questions that require precise dates or involve numerical calculations. From the performance of models in the GPT family and Flan-T5, it can be found that Flan-T5 performs worse in all cases except for questions with boolean answer types. This is consistent with the conclusion of [21]: The performance of knowledge-intensive tasks is closely related to the size of the model. For comparisons within the GPT family of models, following the iteration process of the GPT model summarized</p>
<p>Table 5. Exact Match comparison based on Answer Types (AnsType) and Reasoning Types (RsgType)</p>
<table>
<thead>
<tr>
<th style="text-align: center;">MF</th>
<th style="text-align: center;">FLAN-T5</th>
<th style="text-align: center;">GPT-3</th>
<th style="text-align: center;">GPT-3.5v2</th>
<th style="text-align: center;">GPT-3.5v3</th>
<th style="text-align: center;">ChatGPT</th>
<th style="text-align: center;">GPT-4</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">AnsType</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">MISC</td>
<td style="text-align: center;">35.67</td>
<td style="text-align: center;">40.79</td>
<td style="text-align: center;">42.35</td>
<td style="text-align: center;">46.42</td>
<td style="text-align: center;">51.02</td>
<td style="text-align: center;">60.73</td>
</tr>
<tr>
<td style="text-align: center;">PER</td>
<td style="text-align: center;">30.84</td>
<td style="text-align: center;">37.53</td>
<td style="text-align: center;">41.36</td>
<td style="text-align: center;">45.10</td>
<td style="text-align: center;">48.65</td>
<td style="text-align: center;">65.71</td>
</tr>
<tr>
<td style="text-align: center;">LOC</td>
<td style="text-align: center;">52.91</td>
<td style="text-align: center;">56.92</td>
<td style="text-align: center;">58.93</td>
<td style="text-align: center;">62.71</td>
<td style="text-align: center;">63.55</td>
<td style="text-align: center;">73.98</td>
</tr>
<tr>
<td style="text-align: center;">ORG</td>
<td style="text-align: center;">41.62</td>
<td style="text-align: center;">50.01</td>
<td style="text-align: center;">50.58</td>
<td style="text-align: center;">54.62</td>
<td style="text-align: center;">61.18</td>
<td style="text-align: center;">69.20</td>
</tr>
<tr>
<td style="text-align: center;">DATE</td>
<td style="text-align: center;">24.81</td>
<td style="text-align: center;">37.07</td>
<td style="text-align: center;">36.15</td>
<td style="text-align: center;">42.54</td>
<td style="text-align: center;">36.92</td>
<td style="text-align: center;">41.57</td>
</tr>
<tr>
<td style="text-align: center;">Boolean</td>
<td style="text-align: center;">62.43</td>
<td style="text-align: center;">39.96</td>
<td style="text-align: center;">42.56</td>
<td style="text-align: center;">53.23</td>
<td style="text-align: center;">62.92</td>
<td style="text-align: center;">72.28</td>
</tr>
<tr>
<td style="text-align: center;">NUM</td>
<td style="text-align: center;">16.08</td>
<td style="text-align: center;">19.66</td>
<td style="text-align: center;">21.01</td>
<td style="text-align: center;">20.31</td>
<td style="text-align: center;">30.70</td>
<td style="text-align: center;">44.59</td>
</tr>
<tr>
<td style="text-align: center;">WHY</td>
<td style="text-align: center;">27.69</td>
<td style="text-align: center;">32.31</td>
<td style="text-align: center;">27.69</td>
<td style="text-align: center;">49.23</td>
<td style="text-align: center;">40.00</td>
<td style="text-align: center;">47.83</td>
</tr>
<tr>
<td style="text-align: center;">UNA</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">RsgType</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">SetOperation</td>
<td style="text-align: center;">60.11</td>
<td style="text-align: center;">60.12</td>
<td style="text-align: center;">62.03</td>
<td style="text-align: center;">66.86</td>
<td style="text-align: center;">70.00</td>
<td style="text-align: center;">79.70</td>
</tr>
<tr>
<td style="text-align: center;">Filtering</td>
<td style="text-align: center;">45.01</td>
<td style="text-align: center;">49.06</td>
<td style="text-align: center;">51.24</td>
<td style="text-align: center;">55.43</td>
<td style="text-align: center;">63.40</td>
<td style="text-align: center;">68.40</td>
</tr>
<tr>
<td style="text-align: center;">Counting</td>
<td style="text-align: center;">10.68</td>
<td style="text-align: center;">17.56</td>
<td style="text-align: center;">20.83</td>
<td style="text-align: center;">20.83</td>
<td style="text-align: center;">28.41</td>
<td style="text-align: center;">42.50</td>
</tr>
<tr>
<td style="text-align: center;">Comparison</td>
<td style="text-align: center;">72.13</td>
<td style="text-align: center;">72.44</td>
<td style="text-align: center;">74.00</td>
<td style="text-align: center;">80.00</td>
<td style="text-align: center;">74.74</td>
<td style="text-align: center;">82.79</td>
</tr>
<tr>
<td style="text-align: center;">Single-hop</td>
<td style="text-align: center;">41.00</td>
<td style="text-align: center;">38.72</td>
<td style="text-align: center;">42.54</td>
<td style="text-align: center;">49.22</td>
<td style="text-align: center;">54.00</td>
<td style="text-align: center;">74.14</td>
</tr>
<tr>
<td style="text-align: center;">Multi-hop</td>
<td style="text-align: center;">35.68</td>
<td style="text-align: center;">41.09</td>
<td style="text-align: center;">42.98</td>
<td style="text-align: center;">47.06</td>
<td style="text-align: center;">44.88</td>
<td style="text-align: center;">57.20</td>
</tr>
<tr>
<td style="text-align: center;">Star-shape</td>
<td style="text-align: center;">37.23</td>
<td style="text-align: center;">42.28</td>
<td style="text-align: center;">43.96</td>
<td style="text-align: center;">48.17</td>
<td style="text-align: center;">47.43</td>
<td style="text-align: center;">60.91</td>
</tr>
</tbody>
</table>
<p>in [11, we also observe some positive effects of certain technical introductions on the model, including: (1) 11] point out that GPT-3.5 v3 has better in-context learning abilities, while ChatGPT sacrifices these abilities in order to model dialogue history. This may explain why GPT-3.5 v3 performs better in answering multi-hop and star-shaped questions that require distinguishing entity mentions through context. (2) ChatGPT's dialogue learning helps it better answer short questions (Single-hop). (3) The GPT-3.5 v2, obtained through language model and code training followed by supervised instruction tuning, but its overall capabilities do not appear to have significantly improved compared to GPT-3. The possible reason could be that alignment harms performance, and the alignment tax offsets the increase in zero-shot ability obtained through training 2812. (4) One possible reason why the successor models outperform GPT3.5 V2 in most aspects is that the complex reasoning ability acquired through training on code, which did not manifest prominently in GPT3.5 V2, but were unlocked after the introduction of instruction tuning with RLHF 928.</p>
<p>Figures 5(c) and (d) respectively show line chart of the EM scores formed by each model in answering questions of different answer and reasoning types. The two Figures are consistent with what we observed in the Overall results, that is, various models of the GPT family and FLAN-T5 have similar line shapes. In addition, we also find that the performance of the new GPT models has improved significantly in some specific types of questions, such as Boolean-type (ChatGPT, GPT-4) and WHY-type (GPT-3.5 v3). However, in some other types of questions, there is no significant improvement for the multi-generation models</p>
<p>of GPT, such as Num-type and Counting-type. This indicates that there is still a significant room for improvement for LLMs, and the iteration is far from over. Another interesting finding is that FLAN-T5 performs similarly to ChatGPT in answering boolean questions, but performs worse than the GPT family in other types of answers. Due to the difference in their training data, we cannot accurately determine in the current evaluation whether the reason for this situation is the difference in training data or whether certain training strategies used by the GPT family have a negative impact on specific types of questions.</p>
<p>Table 6. MFT results of ChatGPT</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">SetOperation</th>
<th style="text-align: left;">Filtering</th>
<th style="text-align: left;">Counting</th>
<th style="text-align: left;">Comparison</th>
<th style="text-align: left;">Single-hop</th>
<th style="text-align: left;">Multi-hop</th>
<th style="text-align: left;">Star-shape</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Single Reasoning</td>
<td style="text-align: left;">60.22</td>
<td style="text-align: left;">51.39</td>
<td style="text-align: left;">24.16</td>
<td style="text-align: left;">31.48</td>
<td style="text-align: left;">44.07</td>
<td style="text-align: left;">$\mathbf{4 8 . 2 7}$</td>
<td style="text-align: left;">$\mathbf{5 0 . 7 5}$</td>
</tr>
<tr>
<td style="text-align: left;">Multiple Reasoning</td>
<td style="text-align: left;">$\mathbf{7 0 . 0 0}$</td>
<td style="text-align: left;">$\mathbf{6 3 . 4 0}$</td>
<td style="text-align: left;">$\mathbf{2 8 . 4 1}$</td>
<td style="text-align: left;">$\mathbf{7 4 . 7 4}$</td>
<td style="text-align: left;">$\mathbf{5 4 . 0 0}$</td>
<td style="text-align: left;">44.88</td>
<td style="text-align: left;">47.43</td>
</tr>
</tbody>
</table>
<h1>4.6 CheckList results</h1>
<p>MFT results In the MFT tests, we only evaluate questions that contain a single type of reasoning or multiple reasoning labels of the same type (such as SetOperation+Comparison and SetOperation+Filtering). Based on the results of MFT, we compared the performance of ChatGPT in answering single and multiple reasoning questions. Table 6 shows the following findings. (1) Except for multi-hop and star type questions, ChatGPT performs better in executing multiple reasoning than in performing single reasoning in answering questions involving other types of reasoning operations. (2) ChatGPT is not good at answering counting questions despite the improvements generated by multiple reasoning.</p>
<p>INV results Table 7 presents the stability of LLMs from the GPT family across three runs on three different test cases. As a reference, [26] noted that the stability of traditional KBQA models is 100 . The results in Table 7 are reported using the following symbols: 'CCC' indicates that all answers to the three inquiries are correct, while 'WWW' indicates that none of the three inquiries received correct answers or the model did not return any useful answers. Only when the correctness of the three queries is consistent, the model's performance on the problem is considered stable. As shown in Tables 7, the overall stability of the GPT models has improved from GPT-3 to GPT-4, and GPT-4 has reached a stability rate of 91.70 , which is very close to that of traditional KBQA models. The stability rate of ChatGPT is slightly lower than that of GPT-3.5, and we infer that this is due to the fact that the ChatGPT model focuses more on conversation training, resulting in higher instability (randomness) in the output.</p>
<p>DIR results As mentioned in Figure 4, we designed three DIR modes to examine the controllability of LLMs from the GPT family. In the first mode, we manually observe whether the SPARQL statements output by the model contain the expected keywords, and calculate the failure rate of the model's expected reasoning operations. Since GPT-3.5 v2 and its earlier versions did</p>
<p>Table 7. INV results of GPT family</p>
<table>
<thead>
<tr>
<th style="text-align: left;">LLM</th>
<th style="text-align: center;">CCC</th>
<th style="text-align: center;">CCW</th>
<th style="text-align: center;">CWC</th>
<th style="text-align: center;">CWW</th>
<th style="text-align: center;">WCC</th>
<th style="text-align: center;">WCW</th>
<th style="text-align: center;">WWC</th>
<th style="text-align: center;">WWW</th>
<th style="text-align: center;">Stability Rate</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT-3</td>
<td style="text-align: center;">434</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">59</td>
<td style="text-align: center;">52</td>
<td style="text-align: center;">42</td>
<td style="text-align: center;">43</td>
<td style="text-align: center;">73</td>
<td style="text-align: center;">666</td>
<td style="text-align: center;">76.76</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3.5 v2</td>
<td style="text-align: center;">495</td>
<td style="text-align: center;">44</td>
<td style="text-align: center;">65</td>
<td style="text-align: center;">42</td>
<td style="text-align: center;">43</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">58</td>
<td style="text-align: center;">656</td>
<td style="text-align: center;">80.30</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3.5 v3</td>
<td style="text-align: center;">604</td>
<td style="text-align: center;">46</td>
<td style="text-align: center;">43</td>
<td style="text-align: center;">49</td>
<td style="text-align: center;">34</td>
<td style="text-align: center;">35</td>
<td style="text-align: center;">49</td>
<td style="text-align: center;">583</td>
<td style="text-align: center;">82.83</td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT</td>
<td style="text-align: center;">588</td>
<td style="text-align: center;">49</td>
<td style="text-align: center;">72</td>
<td style="text-align: center;">68</td>
<td style="text-align: center;">52</td>
<td style="text-align: center;">27</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">545</td>
<td style="text-align: center;">79.06</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4</td>
<td style="text-align: center;">798</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">65</td>
<td style="text-align: center;">54</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">516</td>
<td style="text-align: center;">$\mathbf{9 1 . 7 0}$</td>
</tr>
</tbody>
</table>
<p>Table 8. DIR results for RsgType, the score represents the percentage of expected output produced by the LLMs.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">SetOperation</th>
<th style="text-align: center;">Filtering</th>
<th style="text-align: center;">Counting</th>
<th style="text-align: center;">Comparison</th>
<th style="text-align: center;">Overall</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT-3.5 v3</td>
<td style="text-align: center;">$45 \%$</td>
<td style="text-align: center;">$75 \%$</td>
<td style="text-align: center;">$65 \%$</td>
<td style="text-align: center;">$\mathbf{6 5 \%}$</td>
<td style="text-align: center;">$62.5 \%$</td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT</td>
<td style="text-align: center;">$\mathbf{7 5 \%}$</td>
<td style="text-align: center;">$85 \%$</td>
<td style="text-align: center;">$\mathbf{7 0 \%}$</td>
<td style="text-align: center;">$\mathbf{6 5 \%}$</td>
<td style="text-align: center;">$\mathbf{7 3 . 7 5 \%}$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4</td>
<td style="text-align: center;">$65 \%$</td>
<td style="text-align: center;">$\mathbf{9 0 \%}$</td>
<td style="text-align: center;">$\mathbf{7 0 \%}$</td>
<td style="text-align: center;">$60 \%$</td>
<td style="text-align: center;">$71.25 \%$</td>
</tr>
</tbody>
</table>
<p>not undergo code learning, it is difficult for them to generate correct SPARQL queries. Therefore, in this test, we compare the GPT-3.5 v3, ChatGPT, and GPT-4. As shown in Table 8, the scores of around $73 \%$ indicate that even the latest GPT model still has a high degree of randomness in performing reasoning operations, which will affect its applicable scenarios.</p>
<p>In the second mode, we provide prompts to the model's input indicating the answer type and observe the change in the EM score. In Table 9, red values indicate that adding prompts increases the EM score, while blue values indicate negative effects. For most models, prompts have a relatively stable positive effect on Boolean and NUM type questions, while the answers to MISC type questions are mostly negatively affected. In addition, in new models such as ChatGPT and GPT-4, the effect of answer type prompts is much worse than in GPT-3.5 and earlier models.This suggests that different models have different internal knowledge and understanding of the same input text, and the effectiveness and helpfulness of the same prompt vary among different models. More powerful models are more sensitive to the content of prompts because of their powerful natural language understanding ability. It may be difficult to design simple and universally effective prompts that work well across all models.</p>
<p>In the third mode, we guide the model step by step through a naive CoTguided process to first provide the crucial information required to answer the question, and then answer the original question. Table 10 shows the difference in EM scores of the GPT model's answers before and after using CoT-guided process for each type of questions. We can observe that positive impact brought by CoT to GPT-4 is greater than that of other models, and the improvement of CoT on the model's ability to answer NUM-type questions is significant and stable. In terms of reasoning types, CoT improves the ability of all models in set operations, conditional filtering, and counting, but it doesn't help much with multi-hop and star-shape questions. Specifically, the most significant improvement introduced by CoT for the GPT family of models is a score increase of over</p>
<p>Table 9. DIR results for AnsType prompting</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">MISC</th>
<th style="text-align: center;">PER</th>
<th style="text-align: center;">LOC</th>
<th style="text-align: center;">ORG</th>
<th style="text-align: center;">DATE</th>
<th style="text-align: center;">Boolean</th>
<th style="text-align: center;">NUM</th>
<th style="text-align: center;">WHY</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT-3</td>
<td style="text-align: center;">+1.43</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">+5.71</td>
<td style="text-align: center;">+4.29</td>
<td style="text-align: center;">+4.29</td>
<td style="text-align: center;">+15.71</td>
<td style="text-align: center;">+17.14</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3.5 v2</td>
<td style="text-align: center;">-4.28</td>
<td style="text-align: center;">+2.85</td>
<td style="text-align: center;">+7.14</td>
<td style="text-align: center;">+14.28</td>
<td style="text-align: center;">+2.86</td>
<td style="text-align: center;">-8.57</td>
<td style="text-align: center;">+14.28</td>
<td style="text-align: center;">+12.13</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3.5 v3</td>
<td style="text-align: center;">-12.86</td>
<td style="text-align: center;">+10.00</td>
<td style="text-align: center;">+18.57</td>
<td style="text-align: center;">-7.14</td>
<td style="text-align: center;">+4.71</td>
<td style="text-align: center;">+17.14</td>
<td style="text-align: center;">+22.85</td>
<td style="text-align: center;">+9.09</td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT</td>
<td style="text-align: center;">+6.78</td>
<td style="text-align: center;">-3.64</td>
<td style="text-align: center;">-1.72</td>
<td style="text-align: center;">-5.35</td>
<td style="text-align: center;">-8.58</td>
<td style="text-align: center;">+4.28</td>
<td style="text-align: center;">+7.15</td>
<td style="text-align: center;">-3.03</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4</td>
<td style="text-align: center;">-4.29</td>
<td style="text-align: center;">-2.86</td>
<td style="text-align: center;">+11.43</td>
<td style="text-align: center;">+5.71</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">+7.14</td>
<td style="text-align: center;">+4.29</td>
<td style="text-align: center;">-6.06</td>
</tr>
</tbody>
</table>
<p>Table 10. DIR results for CoT prompting</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">MISC</th>
<th style="text-align: center;">PER</th>
<th style="text-align: center;">LOC</th>
<th style="text-align: center;">ORG</th>
<th style="text-align: center;">DATE</th>
<th style="text-align: center;">Boolean</th>
<th style="text-align: center;">NUM</th>
<th style="text-align: center;">WHY</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT-3</td>
<td style="text-align: center;">-1.40</td>
<td style="text-align: center;">-2.00</td>
<td style="text-align: center;">-2.67</td>
<td style="text-align: center;">+2.73</td>
<td style="text-align: center;">-3.77</td>
<td style="text-align: center;">+3.36</td>
<td style="text-align: center;">+35.66</td>
<td style="text-align: center;">+6.06</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3.5 v2</td>
<td style="text-align: center;">-0.35</td>
<td style="text-align: center;">-5.33</td>
<td style="text-align: center;">+1.78</td>
<td style="text-align: center;">-3.64</td>
<td style="text-align: center;">+0.76</td>
<td style="text-align: center;">-5.04</td>
<td style="text-align: center;">+32.95</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3.5 v3</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">-2.00</td>
<td style="text-align: center;">-1.33</td>
<td style="text-align: center;">-1.82</td>
<td style="text-align: center;">-1.51</td>
<td style="text-align: center;">-2.10</td>
<td style="text-align: center;">+34.12</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT</td>
<td style="text-align: center;">-1.75</td>
<td style="text-align: center;">-4.66</td>
<td style="text-align: center;">+0.89</td>
<td style="text-align: center;">-3.63</td>
<td style="text-align: center;">-1.50</td>
<td style="text-align: center;">+3.36</td>
<td style="text-align: center;">+30.62</td>
<td style="text-align: center;">+6.06</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4</td>
<td style="text-align: center;">-3.00</td>
<td style="text-align: center;">+11.11</td>
<td style="text-align: center;">+2.22</td>
<td style="text-align: center;">+3.3</td>
<td style="text-align: center;">-2.71</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">+20.00</td>
<td style="text-align: center;">+2.62</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">SetOperation</td>
<td style="text-align: center;">Filtering</td>
<td style="text-align: center;">Counting</td>
<td style="text-align: center;">Comparison</td>
<td style="text-align: center;">Multi-hop</td>
<td style="text-align: center;">Star-shape</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">GPT-3</td>
<td style="text-align: center;">+10.79</td>
<td style="text-align: center;">+10.43</td>
<td style="text-align: center;">+35.66</td>
<td style="text-align: center;">+1.35</td>
<td style="text-align: center;">-1.60</td>
<td style="text-align: center;">-1.69</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">GPT-3.5 v2</td>
<td style="text-align: center;">+4.86</td>
<td style="text-align: center;">+5.46</td>
<td style="text-align: center;">+38.54</td>
<td style="text-align: center;">-2.26</td>
<td style="text-align: center;">-1.18</td>
<td style="text-align: center;">-0.85</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">GPT-3.5 v3</td>
<td style="text-align: center;">+6.34</td>
<td style="text-align: center;">+8.18</td>
<td style="text-align: center;">+38.99</td>
<td style="text-align: center;">-1.13</td>
<td style="text-align: center;">-1.61</td>
<td style="text-align: center;">-1.26</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT</td>
<td style="text-align: center;">+7.82</td>
<td style="text-align: center;">+9.47</td>
<td style="text-align: center;">+35.78</td>
<td style="text-align: center;">+0.45</td>
<td style="text-align: center;">-1.47</td>
<td style="text-align: center;">-1.41</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">GPT-4</td>
<td style="text-align: center;">+2.05</td>
<td style="text-align: center;">+0.93</td>
<td style="text-align: center;">+11.11</td>
<td style="text-align: center;">-1.88</td>
<td style="text-align: center;">+2.82</td>
<td style="text-align: center;">+2.68</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>30.00 for answer types that are numerical(NUM). This result strongly supports the importance of CoT for using LLMs to solve numerical-related questions 20 .</p>
<h1>5 Conclusion</h1>
<p>In this paper, we extensively tested the ability of ChatGPT and other LLMs to answer questions on KB-based CQA datasets using their own knowledge. The experimental results showed that the question-answering performance and reliability of the GPT model have been continuously improving with version iterations, approaching that of traditional models. CheckList testing showed that current LLMs still have a lot of room for improvement in some reasoning abilities, and CoT-inspired prompts can improve the performance of the original model on certain types of questions. Consequently, this evaluation serves as a valuable reference for future research in the relevant community. In future work, we need to further expand on the following two points: Firstly, conduct tests in various domains to validate which conclusions obtained from open-domain KBQA are universal and which are domain-specific. Secondly, perform tests on various types of models. With ongoing LLM research, besides the GPT family, many new largescale open-source models have been proposed. It requires further exploration and summarization to determine if they possess better mechanisms for self-knowledge organization or stronger capabilities to accept human prompts and find answers.</p>
<h1>6 Acknowledgments</h1>
<p>This work is supported by the Natural Science Foundation of China (Grant No.U21A20488). We thank the Big Data Computing Center of Southeast University for providing the facility support on the numerical calculations in this paper.</p>
<h2>References</h2>
<ol>
<li>Bai, Y., Ying, J., Cao, Y., Lv, X., He, Y., Wang, X., Yu, J., Zeng, K., Xiao, Y., Lyu, H., et al.: Benchmarking foundation models with language-model-as-an-examiner. arXiv preprint arXiv:2306.04181 (2023)</li>
<li>Bang, Y., Cahyawijaya, S., Lee, N., Dai, W., Su, D., Wilie, B., Lovenia, H., Ji, Z., Yu, T., Chung, W., et al.: A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. arXiv e-prints pp. arXiv2302 (2023)</li>
<li>Belinkov, Y., Glass, J.: Analysis methods in neural language processing: A survey. Transactions of the Association for Computational Linguistics 7, 49-72 (2019)</li>
<li>Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al.: Language models are few-shot learners. Advances in neural information processing systems 33, 1877-1901 (2020)</li>
<li>Cao, S., Shi, J., Pan, L., Nie, L., Xiang, Y., Hou, L., Li, J., He, B., Zhang, H.: Kqa pro: A dataset with explicit compositional programs for complex question answering over knowledge base. In: In Proc. ACL Conf. pp. 6101-6119 (2022)</li>
<li>Chang, Y., Wang, X., Wang, J., Wu, Y., Zhu, K., Chen, H., Yang, L., Yi, X., Wang, C., Wang, Y., et al.: A survey on evaluation of large language models. arXiv preprint arXiv:2307.03109 (2023)</li>
<li>Chen, X., Ye, J., Zu, C., Xu, N., Zheng, R., Peng, M., Zhou, J., Gui, T., Zhang, Q., Huang, X.: How robust is gpt-3.5 to predecessors? a comprehensive study on language understanding tasks. arXiv e-prints pp. arXiv-2303 (2023)</li>
<li>Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H.W., Sutton, C., Gehrmann, S., et al.: Palm: Scaling language modeling with pathways. arXiv e-prints pp. arXiv-2204 (2022)</li>
<li>Chung, H.W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, E., Wang, X., Dehghani, M., Brahma, S., et al.: Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416 (2022)</li>
<li>Fu, C., Chen, P., Shen, Y., Qin, Y., Zhang, M., Lin, X., Qiu, Z., Lin, W., Yang, J., Zheng, X., et al.: Mme: A comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394 (2023)</li>
<li>Fu, Y., Peng, H., Khot, T.: How does gpt obtain its ability? tracing emergent abilities of language models to their sources. Yao Fu's Notion (2022)</li>
<li>Gu, Y., Kase, S., Vanni, M., Sadler, B., Liang, P., Yan, X., Su, Y.: Beyond iid: three levels of generalization for question answering on knowledge bases. In: In Proc. WWW Conf. pp. 3477-3488 (2021)</li>
<li>Gu, Y., Su, Y.: Arcaneqa: Dynamic program induction and contextualized encoding for knowledge base question answering. In: In Proc. COLING Conf. pp. 1718-1731 (2022)</li>
<li>
<p>He, H., Choi, J.D.: The stem cell hypothesis: Dilemma behind multi-task learning with transformer encoders. In: In Proc. EMNLP Conf. pp. 5555-5577 (2021)</p>
</li>
<li>
<p>Hu, X., Wu, X., Shu, Y., Qu, Y.: Logical form generation via multi-task learning for complex question answering over knowledge bases. In: In Proc. ICCL Conf. pp. $1687-1696$ (2022)</p>
</li>
<li>Huang, F., Kwak, H., An, J.: Is chatgpt better than human annotators? potential and limitations of chatgpt in explaining implicit hate speech. arXiv e-prints pp. arXiv-2302 (2023)</li>
<li>Jiang, Z., Xu, F.F., Araki, J., Neubig, G.: How can we know what language models know? Transactions of the Association for Computational Linguistics 8, 423-438 (2020)</li>
<li>Kenton, J.D.M.W.C., Toutanova, L.K.: Bert: Pre-training of deep bidirectional transformers for language understanding. In: In Proc. NAACL-HLT Conf. pp. $4171-4186$ (2019)</li>
<li>Kocoń, J., Cichecki, I., Kaszyca, O., Kochanek, M., Szydło, D., Baran, J., Bielaniewicz, J., Gruza, M., Janz, A., Kanclerz, K., et al.: Chatgpt: Jack of all trades, master of none. arXiv e-prints pp. arXiv-2302 (2023)</li>
<li>Kojima, T., Gu, S.S., Reid, M., Matsuo, Y., Iwasawa, Y.: Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916 (2022)</li>
<li>Liang, P., Bommasani, R., Lee, T., Tsipras, D., Soylu, D., Yasunaga, M., Zhang, Y., Narayanan, D., Wu, Y., Kumar, A., et al.: Holistic evaluation of language models. arXiv e-prints pp. arXiv-2211 (2022)</li>
<li>Longpre, S., Lu, Y., Daiber, J.: Mkqa: A linguistically diverse benchmark for multilingual open domain question answering. Transactions of the Association for Computational Linguistics 9, 1389-1406 (2021)</li>
<li>Lyu, C., Xu, J., Wang, L.: New trends in machine translation using large language models: Case examples with chatgpt. arXiv preprint arXiv:2305.01181 (2023)</li>
<li>Ngomo, N.: 9th challenge on question answering over linked data (qald-9). language 7(1), 58-64 (2018)</li>
<li>Nie, L., Cao, S., Shi, J., Sun, J., Tian, Q., Hou, L., Li, J., Zhai, J.: Graphq ir: Unifying the semantic parsing of graph query languages with one intermediate representation. In: In Proc. EMNLP Conf. pp. 5848-5865 (2022)</li>
<li>Omar, R., Mangukiya, O., Kalnis, P., Mansour, E.: Chatgpt versus traditional question answering for knowledge graphs: Current status and future directions towards knowledge graph chatbots. arXiv e-prints pp. arXiv-2302 (2023)</li>
<li>OpenAI: Gpt-4 technical report (2023)</li>
<li>Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al.: Training language models to follow instructions with human feedback. arXiv e-prints pp. arXiv-2203 (2022)</li>
<li>Perevalov, A., Yan, X., Kovriguina, L., Jiang, L., Both, A., Usbeck, R.: Knowledge graph question answering leaderboard: A community resource to prevent a replication crisis. In: In Proc. LREC Conf. pp. 2998-3007 (2022)</li>
<li>Petroni, F., Rocktäschel, T., Riedel, S., Lewis, P., Bakhtin, A., Wu, Y., Miller, A.: Language models as knowledge bases? In: In Proc. IJCAI Conf. pp. 2463-2473 (2019)</li>
<li>Pramanik, S., Alabi, J., Saha Roy, R., Weikum, G.: Uniqorn: Unified question answering over rdf knowledge graphs and natural language text. arXiv e-prints pp. arXiv-2108 (2021)</li>
<li>
<p>Purkayastha, S., Dana, S., Garg, D., Khandelwal, D., Bhargav, G.S.: A deep neural approach to kgqa via sparql silhouette generation. In: In Proc. IJCNN Conf. pp. 18. IEEE (2022)</p>
</li>
<li>
<p>Qin, C., Zhang, A., Zhang, Z., Chen, J., Yasunaga, M., Yang, D.: Is chatgpt a general-purpose natural language processing task solver? arXiv e-prints pp. arXiv2302 (2023)</p>
</li>
<li>Qin, G., Eisner, J.: Learning how to ask: Querying lms with mixtures of soft prompts. In: In Proc. NAACL-HLT Conf. (2021)</li>
<li>Rae, J.W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J., Song, F., Aslanides, J., Henderson, S., Ring, R., Young, S., et al.: Scaling language models: Methods, analysis \&amp; insights from training gopher. arXiv e-prints pp. arXiv-2112 (2021)</li>
<li>Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., Liu, P.J.: Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research 21(1), 5485-5551 (2020)</li>
<li>Reynolds, L., McDonell, K.: Prompt programming for large language models: Beyond the few-shot paradigm. In: In Proc. CHI EA Conf. pp. 1-7 (2021)</li>
<li>Ribeiro, M.T., Wu, T., Guestrin, C., Singh, S.: Beyond accuracy: Behavioral testing of nlp models with checklist. In: In Proc. ACL Conf. pp. 4902-4912 (2020)</li>
<li>Rychalska, B., Basaj, D., Gosiewska, A., Biecek, P.: Models in the wild: On corruption robustness of neural nlp systems. In: In Proc. ICONIP Conf. pp. 235-247 (2019)</li>
<li>Segura, S., Fraser, G., Sanchez, A.B., Ruiz-Cortés, A.: A survey on metamorphic testing. IEEE Transactions on software engineering 42(9), 805-824 (2016)</li>
<li>Srivastava, A., Rastogi, A., Rao, A., Shoeb, A.A.M., Abid, A., Fisch, A., Brown, A.R., Santoro, A., Gupta, A., Garriga-Alonso, A., et al.: Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615 (2022)</li>
<li>Su, Y., Sun, H., Sadler, B., Srivatsa, M., Gür, I., Yan, Z., Yan, X.: On generating characteristic-rich question sets for qa evaluation. In: In Proc. EMNLP Conf. pp. $562-572(2016)$</li>
<li>Talmor, A., Berant, J.: The web as a knowledge-base for answering complex questions. In: In Proc. ACL Conf. pp. 641-651 (2018)</li>
<li>Tjong Kim Sang, E.F., De Meulder, F.: Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition. In: In Proc. NAACL-HLT Conf. pp. 142-147 (2003)</li>
<li>Wang, A., Pruksachatkun, Y., Nangia, N., Singh, A., Michael, J., Hill, F., Levy, O., Bowman, S.R.: Superglue: a stickier benchmark for general-purpose language understanding systems. In: In Proc. NeurIPS Conf. pp. 3266-3280 (2019)</li>
<li>Wang, J., Liang, Y., Meng, F., Li, Z., Qu, J., Zhou, J.: Cross-lingual summarization via chatgpt. arXiv e-prints pp. arXiv-2302 (2023)</li>
<li>Wang, S., Scells, H., Koopman, B., Zuccon, G.: Can chatgpt write a good boolean query for systematic review literature search? arXiv e-prints pp. arXiv-2302 (2023)</li>
<li>Wei, J., Wang, X., Schuurmans, D., Bosma, M., Chi, E., Le, Q., Zhou, D.: Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903 (2022)</li>
<li>Wu, T., Ribeiro, M.T., Heer, J., Weld, D.S.: Errudite: Scalable, reproducible, and testable error analysis. In: In Proc. ACL Conf. pp. 747-763 (2019)</li>
<li>Ye, X., Yavuz, S., Hashimoto, K., Zhou, Y., Xiong, C.: Rng-kbqa: Generation augmented iterative ranking for knowledge base question answering. In: In Proc. ACL Conf. pp. 6032-6043 (2022)</li>
<li>
<p>Yih, W.t., Richardson, M., Meek, C., Chang, M.W., Suh, J.: The value of semantic parse labeling for knowledge base question answering. In: In Proc. ACL Conf. pp. $201-206(2016)$</p>
</li>
<li>
<p>Zhong, Q., Ding, L., Liu, J., Du, B., Tao, D.: Can chatgpt understand too? a comparative study on chatgpt and fine-tuned bert. arXiv e-prints pp. arXiv-2302 (2023)</p>
</li>
<li>Zhu, K., Wang, J., Zhou, J., Wang, Z., Chen, H., Wang, Y., Yang, L., Ye, W., Gong, N.Z., Zhang, Y., et al.: Promptbench: Towards evaluating the robustness of large language models on adversarial prompts. arXiv preprint arXiv:2306.04528 (2023)</li>
<li>Zhuo, T.Y., Huang, Y., Chen, C., Xing, Z.: Exploring ai ethics of chatgpt: A diagnostic analysis. arXiv e-prints pp. arXiv-2301 (2023)</li>
</ol>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{\dagger}$ https://dki-lab.github.io/GrailQA/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
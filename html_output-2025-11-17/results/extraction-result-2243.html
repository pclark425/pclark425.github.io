<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2243 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2243</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2243</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-61.html">extraction-schema-61</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <p><strong>Paper ID:</strong> paper-269009961</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2404.05950v1.pdf" target="_blank">Efficient Multitask Reinforcement Learning via Task-Specific Action Correction</a></p>
                <p><strong>Paper Abstract:</strong> Multitask reinforcement learning (MTRL) holds potential for building general-purpose agents, enabling them to generalize across a variety of tasks. However, MTRL may still be susceptible to conflicts between tasks. A primary reason for this problem is that a universal policy struggles to balance short-term and dense learning signals across various tasks, e.g., distinct reward functions in reinforcement learning. In social cognitive theory, internalized future goals, as a form of cognitive representations, can effectively mitigate potential short-term conflicts in multitask settings. Considering the benefits of future goals, we propose a novel and general framework called task-specific action correction (TSAC) from the goal perspective as an orthogonal research to previous MTRL methods. Specifically, to avoid myopia, TSAC introduces goal-oriented sparse rewards and decomposes policy learning into two separate policies: a shared policy (SP) and an action correction policy (ACP). The SP outputs a short-term perspective action based on guiding dense rewards. To alleviate conflicts resulting from excessive focus on specific tasks’ details in SP, the ACP incorporates goal-oriented sparse rewards, enabling an agent to adopt a long-term perspective to output a correction action and achieve generalization across tasks. Finally, the actions output by SP and ACP are combined based on the action correction function to form a final action that interact with the environment. Extensive experiments conducted on meta-world and multitask StarCraft II multiagent scenarios demonstrate that TSAC outperforms existing state-of-the-art methods, achieving significant improvements in sample efficiency, generalization and effective action execution across tasks.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2243.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2243.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TSAC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Task-Specific Action Correction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multitask RL framework that decomposes policy learning into a shared short-horizon policy (SP) and a goal-oriented, long-horizon action correction policy (ACP), combining their outputs via an additive action-correction function and trained with a CARE backbone and Lagrangian weighting for sparse goal rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>TSAC</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Two-policy architecture: (1) Shared Policy (SP) trained on dense guiding rewards to provide a fast suboptimal policy; (2) Action Correction Policy (ACP) conditioned on SP's action trained on goal-oriented sparse rewards to output a correction Δa; final action a = h(â,Δa) (additive clipping version used). Uses CARE mixture-of-encoders backbone for representations and a Lagrangian multiplier to balance sparse reward budget.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>Behavioral decomposition (two separate policies) combined with task-conditioned shared representation from CARE (mixture of encoders with attention weighting using metadata/task id).</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Multitask reinforcement learning: robotic manipulation (Meta-World MT10/MT50) and multitask StarCraft II multiagent scenarios (SMAC).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td>MT10: peak success rate 0.827 (long horizon). MT50: average success rates 0.450 and 0.445 at 0.8M and 1M steps respectively. In SMAC multitask settings TSAC attains highest win rates among compared baselines (qualitative; see Table VII and Fig.7).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td>Compared baselines (MT-SAC / CARE / Soft Modularization / PCGrad) perform worse: e.g., TSAC reaches accuracies achieved by multitask SAC at ~1M steps in roughly ~300k steps (sample-efficiency claim). Exact baseline numeric values not fully enumerated in text.</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td>TSAC is substantially more sample-efficient: paper states that a multitask SAC agent requires ~1M steps to reach the accuracy TSAC achieves at ~300k steps (≈3× faster in wall-step sample efficiency for comparable accuracy in reported curves).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td>Zero-shot generalization: when trained on 8 MT10 environments and evaluated on 2 held-out, TSAC 'outperforms all baselines by a large margin within 1M steps' (quantitative table referenced but aggregate numbers not reproduced); in SMAC zero-shot settings TSAC outperforms DT2GS, UPDeT, ASN on several challenging source→target transfers (e.g., 2s3z→3s5z, 8m_vs_9m→5m_vs_6m) as shown in Fig.8.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td>Visualization of SP vs corrected actions (Fig.15) shows SP actions are more conservative (smaller fluctuations) while ACP-corrected actions are more aggressive; authors interpret this as SP encoding task-conflict-averse behavior and ACP providing long-horizon goal corrections.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Evaluated across MT10 and MT50 and multitask SMAC; TSAC outperforms CARE, MT-SAC, Soft Modularization and PCGrad in MT10 (short and long horizons) and retains better performance in MT50 despite stronger task conflicts; in SMAC multitask settings TSAC converges faster and reaches higher win rates than DT2GS, UPDeT, ASN.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>Decomposing behavior into a fast shared (dense-reward) policy and a goal-oriented correction policy, combined with task-adaptive representations (CARE), yields large gains in sample efficiency, multi-task performance and zero-shot generalization compared to uniform single-policy baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>TSAC's improvements (sample efficiency and zero-shot generalization) support the idea that task-aligned/adaptive representations and task-specific behavioral modules improve multitask RL over uniform representations.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2243.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2243.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CARE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CARE backbone (Representation-sharing via mixture of encoders)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A representation-sharing backbone that uses a mixture of k encoders and an attention-weighted combination (conditioned on metadata/task identifiers) to produce task-conditioned contextual representations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CARE (mixture-of-encoders backbone)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Uses multiple encoder 'experts' to produce k state representations which are combined by attention weights computed from a contextual representation (metadata or task id) to yield task-specific combined representations; concatenated with context and fed to downstream policies.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>Mixture of encoders with attention-weighted combination conditioned on task metadata (task-aligned representations).</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Multitask RL representation backbone used with TSAC and as a baseline on Meta-World MT10/MT50 and SMAC.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td>As a baseline, CARE is competitive but inferior to TSAC on MT10/MT50: CARE stops learning and declines around 600k steps in MT50 while TSAC maintains better performance (qualitative from Fig.6 and text).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td>Compared to uniform shared-backbone MT-SAC, CARE yields better representation sharing in many settings but still underperforms TSAC when sparse-goal correction is needed.</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td>CARE as backbone helps learning but combining CARE with TSAC's two-policy decomposition produces larger sample-efficiency gains; CARE alone (without ACP) is the ablation that drops performance significantly.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td>CARE backbone improves transfer relative to fully uniform representations in prior work (cited) and was used here as the representation module enabling TSAC's generalization, but CARE alone is outperformed by TSAC in zero-shot settings reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>CARE provides strong representation-sharing across tasks but suffers under intense task conflicts (MT50) compared to TSAC.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>Task-conditioned mixture-of-encoders (CARE) provides adaptive representations that are useful across tasks, but coupling CARE with TSAC's behavioral decomposition yields substantially better multitask performance and generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>CARE demonstrates the benefit of adaptive, task-conditioned representations over uniform representations, though the paper shows further gains when paired with task-specific behavioral modules.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2243.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2243.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MT-SAC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multitask Soft Actor-Critic (MT-SAC)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline that directly applies SAC to the multitask setting using a shared backbone with disentangled alphas (single-policy, shared representation).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MT-SAC (shared single-policy SAC)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Single shared policy trained with Soft Actor-Critic across multiple tasks using a shared backbone and disentangled temperature (alpha) parameters per task; represents a uniform/shared-representation baseline in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>Uniform shared policy/backbone across tasks (no explicit task-adaptive representation allocation described).</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Multitask RL: Meta-World MT10/MT50 experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td>Per paper, MT-SAC learns smoothly and approaches CARE and TSAC at later points in training on some metrics, but its best-as- trained performance is inferior to TSAC (TSAC outperforms MT-SAC in best performance and sample efficiency).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td>MT-SAC requires ~1M steps to reach accuracy that TSAC achieves at ~300k steps (paper claim), indicating lower sample efficiency compared to the task-aligned TSAC approach.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Performs worse than TSAC across MT10 and MT50 benchmarks in best performance and sample efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>A uniform single-policy SAC baseline exhibits lower sample efficiency and weaker multitask performance than the task-aligned TSAC decomposition.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>MT-SAC as a uniform baseline is outperformed by TSAC, supporting that task-specific/adaptive mechanisms can improve multi-task RL.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2243.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2243.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SoftMod</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Soft Modularization (soft modularization method)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A parameter-sharing multitask method that composes policies via soft combinations of modules/routes to share parameters across tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Soft Modularization</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Shares parameters by generating soft combinations of different network modules (routing network) to compose task-specific subnetworks; an adaptive parameter-sharing approach.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>Soft combinations of modules (soft modular routing) to form task-specific subnetworks (adaptive parameter sharing).</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Multitask RL: used as a baseline on Meta-World MT10/MT50.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td>Per experiments, Soft Modularization is outperformed by TSAC on MT10/MT50 (TSAC has better sample efficiency and final performance); exact numeric baseline values not enumerated in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Adaptive module composition helps multitask performance vs naive sharing, but still underperforms TSAC which uses goal-oriented sparse correction plus CARE.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>Adaptive parameter-sharing via soft modularization improves multitask learning relative to naive sharing but is outperformed by TSAC's combination of task-adaptive representations plus behavior decomposition.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>Soft modularization is an example of task-adaptive representation allocation and the paper's results align with the broader claim that such adaptations help multitask learning, though TSAC's additional components provide larger improvements.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2243.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2243.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AdaShare</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AdaShare (learning what to share)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An adaptive method that learns what features or layers to share across tasks, determining feature-sharing modes automatically.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>AdaShare</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Automatically determines feature-sharing patterns across tasks by learning binary/soft gating to choose which layers/features are shared per task (adaptive sharing mechanism).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>Adaptive feature-sharing gating (learned routing/pruning of shared parameters per task).</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Mentioned in related work on multitask learning; not used in experiments in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>Referenced as an example of adaptive feature-sharing approaches that allocate representation capacity per task; cited as prior work motivation for adaptive representations.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>AdaShare exemplifies adaptive allocation of representational resources across tasks which the paper cites as beneficial.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2243.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2243.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>UPDeT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>UPDeT (Universal multi-agent RL via policy decoupling with transformers)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer-based multiagent RL approach designed to adapt to tasks with varying observation/state/action spaces by decoupling policy components.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>UPDeT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based policy-decoupling architecture to handle varying observation/state/action spaces in multiagent RL; used as a baseline on SMAC multitask experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>Transformer-based decoupling to adapt policy components across varying task/agent spaces (implicit adaptive representation).</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Multiagent reinforcement learning (SMAC multitask evaluations and zero-shot generalization).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td>In SMAC multitask experiments UPDeT achieves reasonable asymptotic performance but is outperformed by TSAC in convergence speed and win rate in the presented multitask settings (Fig.7 and Fig.8).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td>UPDeT provides competitive generalization but is outperformed by TSAC in several zero-shot SMAC transfers (Fig.8).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Competent in multiagent settings but TSAC achieves faster convergence and better win rates in the paper's multitask SMAC experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>Transformer-based decoupling (UPDeT) is an adaptive representation approach for multiagent tasks, but TSAC's combination of CARE + ACP improves zero-shot and multitask performance over UPDeT in reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>UPDeT supports the utility of adaptive architectures for generalization, but the paper's results show that additional task-specific behavioral modules (ACP) can further improve outcomes.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2243.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2243.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DT2GS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DT2GS</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A state-of-the-art method for zero-shot generalization in multiagent RL that maintains consistent yet scalable semantics across tasks (used as a baseline in SMAC experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DT2GS</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Method focused on maintaining consistent and scalable semantics to enable zero-shot generalization across tasks in multiagent RL; used as a baseline in SMAC multitask comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>Semantic consistency / scalable semantics mapping across tasks (architecture-level semantic alignment).</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Multiagent reinforcement learning (SMAC zero-shot experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td>DT2GS attains competitive asymptotic performance in SMAC but is beaten by TSAC in win rate and convergence in the multitask settings evaluated (Fig.7, Fig.8).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td>Designed for zero-shot generalization; in the paper DT2GS is outperformed by TSAC on several source→target transfers (Fig.8).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Strong baseline for generalization but lower performance than TSAC in the evaluated multitask SMAC settings.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>Semantic-consistency methods like DT2GS improve cross-task generalization, but TSAC's use of goal-oriented sparse rewards plus behavior decomposition yields superior zero-shot performance in the reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>DT2GS supports adaptive, task-aligned semantics for generalization; TSAC's better empirical results indicate that additional behavioral specialization helps further.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2243.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2243.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ASN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Action Semantics Network (ASN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that models the semantic effects of actions as part of multiagent RL to improve generalization; used (ASN_G variant) as a baseline in SMAC experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ASN (Action Semantics Network)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Models action semantics (effects) to capture differences across actions and form foundations for generalizable policies in multiagent RL.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>Action-effect semantic modeling used to produce action representations aligned to task semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Multiagent reinforcement learning (SMAC multitask/zero-shot evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td>ASN_G fails to win in the hard multitask SMAC setting per paper's reported experiments; TSAC outperforms ASN_G in these settings (Fig.7).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td>ASN provides a semantics-aware representation but in the paper's SMAC multitask tests it is outperformed by TSAC, particularly on harder multitask settings.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Underperforms TSAC in the paper's hard multiagent multitask scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>Action-semantic representations can aid generalization, but in the presented multitask SMAC experiments TSAC's combination of adaptive representations and behavioral decomposition yields superior performance.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>ASN supports the value of action-aligned representations but paper shows benefit from also using task-specific behavioral modules and sparse goal signals.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2243.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e2243.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PCGrad</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PCGrad (Projecting Conflicting Gradients)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A gradient-manipulation method that projects gradients of one task to remove conflicts with others (multiobjective gradient surgery), used as a baseline in MT10 (but noted to be expensive for MT50).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PCGrad</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Per-update gradient projection method that modifies per-task gradients to avoid conflicts by projecting onto non-conflicting directions (gradient-level conflict mitigation), not a representation adaptation mechanism but a baseline for handling task conflicts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>Gradient projection manipulation (not representation allocation).</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Multitask RL (MT10 baseline, computationally costly for MT50).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td>Included as a baseline; generally outperformed by TSAC in MT10 and not suitable for MT50 long-horizon due to high time complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td>Noted to have high time complexity and impractical for MT50 long-horizon in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>PCGrad helps with gradient conflicts in some multitask setups but is computationally expensive and does not match TSAC's performance on the reported benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td>High time complexity noted; impractical for larger MT50 experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>Gradient-manipulation methods alleviate task conflicts but do not substitute for task-aligned representations and task-specific behavioral modules, and can be computationally costly.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>neutral</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>PCGrad addresses conflicts via gradients rather than by allocating representational resources; it complements but does not directly validate task-aligned representation benefits shown by TSAC.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Multi-task reinforcement learning with soft modularization <em>(Rating: 2)</em></li>
                <li>Adashare: Learning what to share for efficient deep multi-task learning <em>(Rating: 2)</em></li>
                <li>Gradient surgery for multi-task learning <em>(Rating: 2)</em></li>
                <li>Soft actor-critic: Offpolicy maximum entropy deep reinforcement learning with a stochastic actor <em>(Rating: 2)</em></li>
                <li>Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning <em>(Rating: 2)</em></li>
                <li>Updet: Universal multi-agent reinforcement learning via policy decoupling with transformers <em>(Rating: 2)</em></li>
                <li>Action semantics network: Considering the effects of actions in multiagent systems <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2243",
    "paper_id": "paper-269009961",
    "extraction_schema_id": "extraction-schema-61",
    "extracted_data": [
        {
            "name_short": "TSAC",
            "name_full": "Task-Specific Action Correction",
            "brief_description": "A multitask RL framework that decomposes policy learning into a shared short-horizon policy (SP) and a goal-oriented, long-horizon action correction policy (ACP), combining their outputs via an additive action-correction function and trained with a CARE backbone and Lagrangian weighting for sparse goal rewards.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "TSAC",
            "model_description": "Two-policy architecture: (1) Shared Policy (SP) trained on dense guiding rewards to provide a fast suboptimal policy; (2) Action Correction Policy (ACP) conditioned on SP's action trained on goal-oriented sparse rewards to output a correction Δa; final action a = h(â,Δa) (additive clipping version used). Uses CARE mixture-of-encoders backbone for representations and a Lagrangian multiplier to balance sparse reward budget.",
            "model_size": null,
            "uses_task_aligned_abstraction": true,
            "abstraction_mechanism": "Behavioral decomposition (two separate policies) combined with task-conditioned shared representation from CARE (mixture of encoders with attention weighting using metadata/task id).",
            "is_dynamic_or_adaptive": true,
            "task_domain": "Multitask reinforcement learning: robotic manipulation (Meta-World MT10/MT50) and multitask StarCraft II multiagent scenarios (SMAC).",
            "performance_task_aligned": "MT10: peak success rate 0.827 (long horizon). MT50: average success rates 0.450 and 0.445 at 0.8M and 1M steps respectively. In SMAC multitask settings TSAC attains highest win rates among compared baselines (qualitative; see Table VII and Fig.7).",
            "performance_uniform_baseline": "Compared baselines (MT-SAC / CARE / Soft Modularization / PCGrad) perform worse: e.g., TSAC reaches accuracies achieved by multitask SAC at ~1M steps in roughly ~300k steps (sample-efficiency claim). Exact baseline numeric values not fully enumerated in text.",
            "has_direct_comparison": true,
            "computational_efficiency_task_aligned": null,
            "computational_efficiency_baseline": null,
            "sample_efficiency_results": "TSAC is substantially more sample-efficient: paper states that a multitask SAC agent requires ~1M steps to reach the accuracy TSAC achieves at ~300k steps (≈3× faster in wall-step sample efficiency for comparable accuracy in reported curves).",
            "transfer_generalization_results": "Zero-shot generalization: when trained on 8 MT10 environments and evaluated on 2 held-out, TSAC 'outperforms all baselines by a large margin within 1M steps' (quantitative table referenced but aggregate numbers not reproduced); in SMAC zero-shot settings TSAC outperforms DT2GS, UPDeT, ASN on several challenging source→target transfers (e.g., 2s3z→3s5z, 8m_vs_9m→5m_vs_6m) as shown in Fig.8.",
            "interpretability_results": "Visualization of SP vs corrected actions (Fig.15) shows SP actions are more conservative (smaller fluctuations) while ACP-corrected actions are more aggressive; authors interpret this as SP encoding task-conflict-averse behavior and ACP providing long-horizon goal corrections.",
            "multi_task_performance": "Evaluated across MT10 and MT50 and multitask SMAC; TSAC outperforms CARE, MT-SAC, Soft Modularization and PCGrad in MT10 (short and long horizons) and retains better performance in MT50 despite stronger task conflicts; in SMAC multitask settings TSAC converges faster and reaches higher win rates than DT2GS, UPDeT, ASN.",
            "resource_constrained_results": null,
            "key_finding_summary": "Decomposing behavior into a fast shared (dense-reward) policy and a goal-oriented correction policy, combined with task-adaptive representations (CARE), yields large gains in sample efficiency, multi-task performance and zero-shot generalization compared to uniform single-policy baselines.",
            "supports_or_challenges_theory": "supports",
            "supports_or_challenges_theory_explanation": "TSAC's improvements (sample efficiency and zero-shot generalization) support the idea that task-aligned/adaptive representations and task-specific behavioral modules improve multitask RL over uniform representations.",
            "uuid": "e2243.0"
        },
        {
            "name_short": "CARE",
            "name_full": "CARE backbone (Representation-sharing via mixture of encoders)",
            "brief_description": "A representation-sharing backbone that uses a mixture of k encoders and an attention-weighted combination (conditioned on metadata/task identifiers) to produce task-conditioned contextual representations.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "CARE (mixture-of-encoders backbone)",
            "model_description": "Uses multiple encoder 'experts' to produce k state representations which are combined by attention weights computed from a contextual representation (metadata or task id) to yield task-specific combined representations; concatenated with context and fed to downstream policies.",
            "model_size": null,
            "uses_task_aligned_abstraction": true,
            "abstraction_mechanism": "Mixture of encoders with attention-weighted combination conditioned on task metadata (task-aligned representations).",
            "is_dynamic_or_adaptive": true,
            "task_domain": "Multitask RL representation backbone used with TSAC and as a baseline on Meta-World MT10/MT50 and SMAC.",
            "performance_task_aligned": "As a baseline, CARE is competitive but inferior to TSAC on MT10/MT50: CARE stops learning and declines around 600k steps in MT50 while TSAC maintains better performance (qualitative from Fig.6 and text).",
            "performance_uniform_baseline": "Compared to uniform shared-backbone MT-SAC, CARE yields better representation sharing in many settings but still underperforms TSAC when sparse-goal correction is needed.",
            "has_direct_comparison": true,
            "computational_efficiency_task_aligned": null,
            "computational_efficiency_baseline": null,
            "sample_efficiency_results": "CARE as backbone helps learning but combining CARE with TSAC's two-policy decomposition produces larger sample-efficiency gains; CARE alone (without ACP) is the ablation that drops performance significantly.",
            "transfer_generalization_results": "CARE backbone improves transfer relative to fully uniform representations in prior work (cited) and was used here as the representation module enabling TSAC's generalization, but CARE alone is outperformed by TSAC in zero-shot settings reported in the paper.",
            "interpretability_results": null,
            "multi_task_performance": "CARE provides strong representation-sharing across tasks but suffers under intense task conflicts (MT50) compared to TSAC.",
            "resource_constrained_results": null,
            "key_finding_summary": "Task-conditioned mixture-of-encoders (CARE) provides adaptive representations that are useful across tasks, but coupling CARE with TSAC's behavioral decomposition yields substantially better multitask performance and generalization.",
            "supports_or_challenges_theory": "supports",
            "supports_or_challenges_theory_explanation": "CARE demonstrates the benefit of adaptive, task-conditioned representations over uniform representations, though the paper shows further gains when paired with task-specific behavioral modules.",
            "uuid": "e2243.1"
        },
        {
            "name_short": "MT-SAC",
            "name_full": "Multitask Soft Actor-Critic (MT-SAC)",
            "brief_description": "A baseline that directly applies SAC to the multitask setting using a shared backbone with disentangled alphas (single-policy, shared representation).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "MT-SAC (shared single-policy SAC)",
            "model_description": "Single shared policy trained with Soft Actor-Critic across multiple tasks using a shared backbone and disentangled temperature (alpha) parameters per task; represents a uniform/shared-representation baseline in experiments.",
            "model_size": null,
            "uses_task_aligned_abstraction": false,
            "abstraction_mechanism": "Uniform shared policy/backbone across tasks (no explicit task-adaptive representation allocation described).",
            "is_dynamic_or_adaptive": false,
            "task_domain": "Multitask RL: Meta-World MT10/MT50 experiments.",
            "performance_task_aligned": "Per paper, MT-SAC learns smoothly and approaches CARE and TSAC at later points in training on some metrics, but its best-as- trained performance is inferior to TSAC (TSAC outperforms MT-SAC in best performance and sample efficiency).",
            "performance_uniform_baseline": null,
            "has_direct_comparison": true,
            "computational_efficiency_task_aligned": null,
            "computational_efficiency_baseline": null,
            "sample_efficiency_results": "MT-SAC requires ~1M steps to reach accuracy that TSAC achieves at ~300k steps (paper claim), indicating lower sample efficiency compared to the task-aligned TSAC approach.",
            "transfer_generalization_results": null,
            "interpretability_results": null,
            "multi_task_performance": "Performs worse than TSAC across MT10 and MT50 benchmarks in best performance and sample efficiency.",
            "resource_constrained_results": null,
            "key_finding_summary": "A uniform single-policy SAC baseline exhibits lower sample efficiency and weaker multitask performance than the task-aligned TSAC decomposition.",
            "supports_or_challenges_theory": "supports",
            "supports_or_challenges_theory_explanation": "MT-SAC as a uniform baseline is outperformed by TSAC, supporting that task-specific/adaptive mechanisms can improve multi-task RL.",
            "uuid": "e2243.2"
        },
        {
            "name_short": "SoftMod",
            "name_full": "Soft Modularization (soft modularization method)",
            "brief_description": "A parameter-sharing multitask method that composes policies via soft combinations of modules/routes to share parameters across tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Soft Modularization",
            "model_description": "Shares parameters by generating soft combinations of different network modules (routing network) to compose task-specific subnetworks; an adaptive parameter-sharing approach.",
            "model_size": null,
            "uses_task_aligned_abstraction": true,
            "abstraction_mechanism": "Soft combinations of modules (soft modular routing) to form task-specific subnetworks (adaptive parameter sharing).",
            "is_dynamic_or_adaptive": true,
            "task_domain": "Multitask RL: used as a baseline on Meta-World MT10/MT50.",
            "performance_task_aligned": "Per experiments, Soft Modularization is outperformed by TSAC on MT10/MT50 (TSAC has better sample efficiency and final performance); exact numeric baseline values not enumerated in main text.",
            "performance_uniform_baseline": null,
            "has_direct_comparison": true,
            "computational_efficiency_task_aligned": null,
            "computational_efficiency_baseline": null,
            "sample_efficiency_results": null,
            "transfer_generalization_results": null,
            "interpretability_results": null,
            "multi_task_performance": "Adaptive module composition helps multitask performance vs naive sharing, but still underperforms TSAC which uses goal-oriented sparse correction plus CARE.",
            "resource_constrained_results": null,
            "key_finding_summary": "Adaptive parameter-sharing via soft modularization improves multitask learning relative to naive sharing but is outperformed by TSAC's combination of task-adaptive representations plus behavior decomposition.",
            "supports_or_challenges_theory": "supports",
            "supports_or_challenges_theory_explanation": "Soft modularization is an example of task-adaptive representation allocation and the paper's results align with the broader claim that such adaptations help multitask learning, though TSAC's additional components provide larger improvements.",
            "uuid": "e2243.3"
        },
        {
            "name_short": "AdaShare",
            "name_full": "AdaShare (learning what to share)",
            "brief_description": "An adaptive method that learns what features or layers to share across tasks, determining feature-sharing modes automatically.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "AdaShare",
            "model_description": "Automatically determines feature-sharing patterns across tasks by learning binary/soft gating to choose which layers/features are shared per task (adaptive sharing mechanism).",
            "model_size": null,
            "uses_task_aligned_abstraction": true,
            "abstraction_mechanism": "Adaptive feature-sharing gating (learned routing/pruning of shared parameters per task).",
            "is_dynamic_or_adaptive": true,
            "task_domain": "Mentioned in related work on multitask learning; not used in experiments in this paper.",
            "performance_task_aligned": null,
            "performance_uniform_baseline": null,
            "has_direct_comparison": false,
            "computational_efficiency_task_aligned": null,
            "computational_efficiency_baseline": null,
            "sample_efficiency_results": null,
            "transfer_generalization_results": null,
            "interpretability_results": null,
            "multi_task_performance": null,
            "resource_constrained_results": null,
            "key_finding_summary": "Referenced as an example of adaptive feature-sharing approaches that allocate representation capacity per task; cited as prior work motivation for adaptive representations.",
            "supports_or_challenges_theory": "supports",
            "supports_or_challenges_theory_explanation": "AdaShare exemplifies adaptive allocation of representational resources across tasks which the paper cites as beneficial.",
            "uuid": "e2243.4"
        },
        {
            "name_short": "UPDeT",
            "name_full": "UPDeT (Universal multi-agent RL via policy decoupling with transformers)",
            "brief_description": "A transformer-based multiagent RL approach designed to adapt to tasks with varying observation/state/action spaces by decoupling policy components.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "UPDeT",
            "model_description": "Transformer-based policy-decoupling architecture to handle varying observation/state/action spaces in multiagent RL; used as a baseline on SMAC multitask experiments.",
            "model_size": null,
            "uses_task_aligned_abstraction": true,
            "abstraction_mechanism": "Transformer-based decoupling to adapt policy components across varying task/agent spaces (implicit adaptive representation).",
            "is_dynamic_or_adaptive": true,
            "task_domain": "Multiagent reinforcement learning (SMAC multitask evaluations and zero-shot generalization).",
            "performance_task_aligned": "In SMAC multitask experiments UPDeT achieves reasonable asymptotic performance but is outperformed by TSAC in convergence speed and win rate in the presented multitask settings (Fig.7 and Fig.8).",
            "performance_uniform_baseline": null,
            "has_direct_comparison": true,
            "computational_efficiency_task_aligned": null,
            "computational_efficiency_baseline": null,
            "sample_efficiency_results": null,
            "transfer_generalization_results": "UPDeT provides competitive generalization but is outperformed by TSAC in several zero-shot SMAC transfers (Fig.8).",
            "interpretability_results": null,
            "multi_task_performance": "Competent in multiagent settings but TSAC achieves faster convergence and better win rates in the paper's multitask SMAC experiments.",
            "resource_constrained_results": null,
            "key_finding_summary": "Transformer-based decoupling (UPDeT) is an adaptive representation approach for multiagent tasks, but TSAC's combination of CARE + ACP improves zero-shot and multitask performance over UPDeT in reported experiments.",
            "supports_or_challenges_theory": "mixed",
            "supports_or_challenges_theory_explanation": "UPDeT supports the utility of adaptive architectures for generalization, but the paper's results show that additional task-specific behavioral modules (ACP) can further improve outcomes.",
            "uuid": "e2243.5"
        },
        {
            "name_short": "DT2GS",
            "name_full": "DT2GS",
            "brief_description": "A state-of-the-art method for zero-shot generalization in multiagent RL that maintains consistent yet scalable semantics across tasks (used as a baseline in SMAC experiments).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "DT2GS",
            "model_description": "Method focused on maintaining consistent and scalable semantics to enable zero-shot generalization across tasks in multiagent RL; used as a baseline in SMAC multitask comparisons.",
            "model_size": null,
            "uses_task_aligned_abstraction": true,
            "abstraction_mechanism": "Semantic consistency / scalable semantics mapping across tasks (architecture-level semantic alignment).",
            "is_dynamic_or_adaptive": true,
            "task_domain": "Multiagent reinforcement learning (SMAC zero-shot experiments).",
            "performance_task_aligned": "DT2GS attains competitive asymptotic performance in SMAC but is beaten by TSAC in win rate and convergence in the multitask settings evaluated (Fig.7, Fig.8).",
            "performance_uniform_baseline": null,
            "has_direct_comparison": true,
            "computational_efficiency_task_aligned": null,
            "computational_efficiency_baseline": null,
            "sample_efficiency_results": null,
            "transfer_generalization_results": "Designed for zero-shot generalization; in the paper DT2GS is outperformed by TSAC on several source→target transfers (Fig.8).",
            "interpretability_results": null,
            "multi_task_performance": "Strong baseline for generalization but lower performance than TSAC in the evaluated multitask SMAC settings.",
            "resource_constrained_results": null,
            "key_finding_summary": "Semantic-consistency methods like DT2GS improve cross-task generalization, but TSAC's use of goal-oriented sparse rewards plus behavior decomposition yields superior zero-shot performance in the reported experiments.",
            "supports_or_challenges_theory": "mixed",
            "supports_or_challenges_theory_explanation": "DT2GS supports adaptive, task-aligned semantics for generalization; TSAC's better empirical results indicate that additional behavioral specialization helps further.",
            "uuid": "e2243.6"
        },
        {
            "name_short": "ASN",
            "name_full": "Action Semantics Network (ASN)",
            "brief_description": "A method that models the semantic effects of actions as part of multiagent RL to improve generalization; used (ASN_G variant) as a baseline in SMAC experiments.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "ASN (Action Semantics Network)",
            "model_description": "Models action semantics (effects) to capture differences across actions and form foundations for generalizable policies in multiagent RL.",
            "model_size": null,
            "uses_task_aligned_abstraction": true,
            "abstraction_mechanism": "Action-effect semantic modeling used to produce action representations aligned to task semantics.",
            "is_dynamic_or_adaptive": true,
            "task_domain": "Multiagent reinforcement learning (SMAC multitask/zero-shot evaluation).",
            "performance_task_aligned": "ASN_G fails to win in the hard multitask SMAC setting per paper's reported experiments; TSAC outperforms ASN_G in these settings (Fig.7).",
            "performance_uniform_baseline": null,
            "has_direct_comparison": true,
            "computational_efficiency_task_aligned": null,
            "computational_efficiency_baseline": null,
            "sample_efficiency_results": null,
            "transfer_generalization_results": "ASN provides a semantics-aware representation but in the paper's SMAC multitask tests it is outperformed by TSAC, particularly on harder multitask settings.",
            "interpretability_results": null,
            "multi_task_performance": "Underperforms TSAC in the paper's hard multiagent multitask scenarios.",
            "resource_constrained_results": null,
            "key_finding_summary": "Action-semantic representations can aid generalization, but in the presented multitask SMAC experiments TSAC's combination of adaptive representations and behavioral decomposition yields superior performance.",
            "supports_or_challenges_theory": "mixed",
            "supports_or_challenges_theory_explanation": "ASN supports the value of action-aligned representations but paper shows benefit from also using task-specific behavioral modules and sparse goal signals.",
            "uuid": "e2243.7"
        },
        {
            "name_short": "PCGrad",
            "name_full": "PCGrad (Projecting Conflicting Gradients)",
            "brief_description": "A gradient-manipulation method that projects gradients of one task to remove conflicts with others (multiobjective gradient surgery), used as a baseline in MT10 (but noted to be expensive for MT50).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "PCGrad",
            "model_description": "Per-update gradient projection method that modifies per-task gradients to avoid conflicts by projecting onto non-conflicting directions (gradient-level conflict mitigation), not a representation adaptation mechanism but a baseline for handling task conflicts.",
            "model_size": null,
            "uses_task_aligned_abstraction": false,
            "abstraction_mechanism": "Gradient projection manipulation (not representation allocation).",
            "is_dynamic_or_adaptive": true,
            "task_domain": "Multitask RL (MT10 baseline, computationally costly for MT50).",
            "performance_task_aligned": "Included as a baseline; generally outperformed by TSAC in MT10 and not suitable for MT50 long-horizon due to high time complexity.",
            "performance_uniform_baseline": null,
            "has_direct_comparison": true,
            "computational_efficiency_task_aligned": "Noted to have high time complexity and impractical for MT50 long-horizon in experiments.",
            "computational_efficiency_baseline": null,
            "sample_efficiency_results": null,
            "transfer_generalization_results": null,
            "interpretability_results": null,
            "multi_task_performance": "PCGrad helps with gradient conflicts in some multitask setups but is computationally expensive and does not match TSAC's performance on the reported benchmarks.",
            "resource_constrained_results": "High time complexity noted; impractical for larger MT50 experiments.",
            "key_finding_summary": "Gradient-manipulation methods alleviate task conflicts but do not substitute for task-aligned representations and task-specific behavioral modules, and can be computationally costly.",
            "supports_or_challenges_theory": "neutral",
            "supports_or_challenges_theory_explanation": "PCGrad addresses conflicts via gradients rather than by allocating representational resources; it complements but does not directly validate task-aligned representation benefits shown by TSAC.",
            "uuid": "e2243.8"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Multi-task reinforcement learning with soft modularization",
            "rating": 2
        },
        {
            "paper_title": "Adashare: Learning what to share for efficient deep multi-task learning",
            "rating": 2
        },
        {
            "paper_title": "Gradient surgery for multi-task learning",
            "rating": 2
        },
        {
            "paper_title": "Soft actor-critic: Offpolicy maximum entropy deep reinforcement learning with a stochastic actor",
            "rating": 2
        },
        {
            "paper_title": "Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Updet: Universal multi-agent reinforcement learning via policy decoupling with transformers",
            "rating": 2
        },
        {
            "paper_title": "Action semantics network: Considering the effects of actions in multiagent systems",
            "rating": 2
        }
    ],
    "cost": 0.020400249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>15 October 2025</p>
<p>Min Chen 
Tenghai Qiu 
Jie Zhang </p>
<p>Institute of Automation
Chinese Academy of Sciences
100190BeijingChina</p>
<p>School of Artificial Intelligence
University of Chinese Academy of Sciences
100049BeijingChina</p>
<p>Institute of Automation
Chi-nese Academy of Sciences
100190BeijingChina</p>
<p>Department of Computer Science
University of Bath
BA2 7AYBathU.K
15 October 2025A2099C0545E904EF93618FDCAC0F3A0D10.1109/TCDS.2025.3543694</p>
<p>Efficient Multitask Reinforcement Learning via</p>
<p>Task-Specific Action Correction Jinyuan Feng , Min Chen , Zhiqiang Pu , Member, IEEE, Tenghai Qiu , Jianqiang Yi , Senior Member, IEEE, and Jie Zhang</p>
<p>Abstract-Multitask reinforcement learning (MTRL) holds potential for building general-purpose agents, enabling them to generalize across a variety of tasks.However, MTRL may still be susceptible to conflicts between tasks.A primary reason for this problem is that a universal policy struggles to balance short-term and dense learning signals across various tasks, e.g., distinct reward functions in reinforcement learning.In social cognitive theory, internalized future goals, as a form of cognitive representations, can effectively mitigate potential short-term conflicts in multitask settings.Considering the benefits of future goals, we propose a novel and general framework called taskspecific action correction (TSAC) from the goal perspective as an orthogonal research to previous MTRL methods.Specifically, to avoid myopia, TSAC introduces goal-oriented sparse rewards and decomposes policy learning into two separate policies: a shared policy (SP) and an action correction policy (ACP).The SP outputs a short-term perspective action based on guiding dense rewards.To alleviate conflicts resulting from excessive focus on specific tasks' details in SP, the ACP incorporates goal-oriented sparse rewards, enabling an agent to adopt a long-term perspective to output a correction action and achieve generalization across tasks.Finally, the actions output by SP and ACP are combined based on the action correction function to form a final action that interact with the environment.Extensive experiments conducted on meta-world and multitask StarCraft II multiagent scenarios demonstrate that TSAC outperforms existing state-of-the-art methods, achieving significant improvements in sample efficiency, generalization and effective action execution across tasks.</p>
<p>Index Terms-Future goals, generalization, Lagrangian method, multitask reinforcement learning.</p>
<p>I. INTRODUCTION</p>
<p>R EINFORCEMENT learning (RL) has witnessed remark- able advancements in a wide array of decision-making problems with the assistance of neural networks [1], [2], [3] and has become a crucial methodology in various domains, such as gaming [4], [5], [6], large language models [7] and real-world applications including robotics [8].However, the majority of research in RL predominantly focuses on specific problem scenarios, prioritizing mastery of individual tasks through the learning of single policies, often at the expense of generalization.To empower general-purpose agents, multitask reinforcement learning (MTRL) is introduced to scale up the RL framework, holding the promise of learning a universal policy capable of accomplishing a diverse range of tasks.</p>
<p>Compared to RL, a fundamental challenge of MTRL is that different tasks may conflict with each other when learning simultaneously.Although MTRL naturally incorporates a curriculum-like teaching approach, wherein easier tasks are learned to facilitate the learning of more challenging tasks [9], [10], [11], [12], MTRL is prone to negative transfer [13].Negative transfer refers to the phenomenon where the taskspecific knowledge acquired from one task may impede the overall learning process of other tasks.This phenomenon is also referred to as task conflict, which becomes more acute as the number of tasks increases.From the optimization standpoint, task conflict arises from conflicting gradients [14] between tasks, where the gradients move in opposite directions.Negative transfer between tasks can result in an excessive focus on the immediate rewards of individual tasks during the multitask learning process, resulting in myopia and hindering the ability to learn a universal policy that can effectively accomplish multiple tasks.</p>
<p>The key to resolving short-term conflicts between tasks lies in enabling agents to learn to consider long-term goals.In recent researches, incorporating long-term goals or future representations has emerged as one of the primary solutions for addressing myopic decision-making.QFuture [15] adjusts agents' actions to avoid myopia by predicting potential future outcomes.LS-OII [16] infers the opponent's long-term tactical intentions from a macroperspective to learn superior adversarial policies.SAC-X [17] is capable of learning complex behaviors from scratch in the presence of multiple goal-oriented sparse reward signals.The main efforts on this direction have been paid on single-task settings, making MTRL lags thus far.In current MTRL, guiding dense rewards are designed for each task, providing feedback to the agent at each state.However, from a multitask perspective, avoiding conflicts between tasks and designing a globally optimal reward function for each task is challenging.Additionally, the use of guiding dense rewards enables the agent to focus solely on task-specific information and details within each task, which may result in myopia and hinder generalization across tasks.In contrast to guiding dense rewards, goal-oriented sparse rewards use binary rewards to indicate goal achievement [18], [19].Using goaloriented sparse rewards to guide training involves relinquishing the design of potentially unreliable rewards, removing assumptions regarding the intermediate processes of tasks as much as possible, and fully restoring the autonomy of learning to the reinforcement learning algorithm.If there are sufficient high-quality samples available, this can enable the agent to coordinate multiple tasks from a global perspective [20].The advantages and disadvantages of these two reward functions are complementary.Guiding dense rewards are efficient in early exploration but may lead to potential conflicts between tasks.Goal-oriented sparse rewards are less efficient in early exploration, but after sufficient exploration, the agent can alleviate conflicts between tasks from a global perspective.Building upon the existing dense rewards in MTRL, our work introduces goal-oriented sparse learning signals to achieve efficient multitask learning and cross-task generalization.</p>
<p>Drawing inspiration from goal orientation theory in cognitive science [21].Humans adjust their behavior in handling different tasks based on set future goals to avoid short-term conflicts between tasks.Imagine that humans are simultaneously learning a variety of related manipulation tasks, for instance, as depicted in Fig. 1.Within these tasks, several actions exhibit similarity, such as approaching objects like doors or drawers and interacting with them.Humans have the ability to leverage previously learned behaviors when encountering a specific task, making slight adjustments based on the task's goal.Furthermore, when humans are confronted with numerous tasks simultaneously, having detailed instructions and requirements for each task can lead to confusion and a sense of being overwhelmed, even though they can be helpful in completing individual tasks.In contrast, when each task has a clearly defined goal, humans naturally adopt a long-term perspective regarding conflicts and priorities among tasks.The above human behaviors imply their recognition that current conflicts may not be necessary for achieving the overall objectives of the tasks.Our idea is inspired by this recognition.</p>
<p>We introduce goal-oriented sparse rewards to investigate MTRL to address short-term conflicts between tasks and to learn a universal policy from a goal-oriented perspective.In this article, we propose a novel framework called task-specific action correction (TSAC) as a general and complementary framework for MTRL.TSAC decomposes policy learning into two policies: a shared policy (SP) and an action correction policy (ACP).SP maximizes well-shaped dense rewards, which focus on task-specific information and accelerate the learning process.Its output actions, referred to as shared actions, are potentially short-sighted.On the other hand, ACP utilizes goaloriented sparse rewards to generate far-sighted edited actions.The goal-oriented sparse rewards which are sparse and strongly correlated with the completion of the objective.SP and ACP collaborate with each other, where SP provides a suboptimal policy that facilitates the training of ACP in the sparse rewards setting.ACP, in return, improves the overall performance.To balance the training of these two policies and avoid potential conflicts, we assign a virtual expected budget to the sparse rewards and use the Lagrangian method to adjust the weights of the loss in ACP dynamically.Our two-policy paradigm draws inspiration from works in safe reinforcement learning [22], [23], [24].However, our framework differs significantly in terms of motivation and interpretation.</p>
<p>To implement our framework, we employ the soft actorcritic algorithm [25] as the underlying reinforcement learning policy.It is worth noting that our approach is algorithmagnostic and can be integrated with existing MTRL methods, it can even be extended to multiagent settings.Our experimental results demonstrate the efficiency and significance of the cooperation between the two policies, and simply combining the two rewards do not yield comparable results.Moreover, our experiments conducted on the meta-world [26] and multitask StarCraft II multiagent scenarios showcase significant improvements in both sample efficiency and final performance compared to previous state-of-the-art multitask policies.In summary, our contributions are as follows.</p>
<p>1) We propose task-specific action correction (TSAC), a general and complementary framework for MTRL, which decomposes policy learning into two policies, facilitating efficient MTRL.TSAC can be combined with any existing MTRL methods and extend to multiagent settings.2) We introduce goal-oriented sparse rewards to provide agents with a long-term perspective for handling task conflicts that arise from excessive focus on specific tasks' details.Furthermore, the action correction function we designed integrates the actions of the two policies to facilitate efficient interaction with the environment.</p>
<p>3) We assign a virtual expected budget to the sparse rewards and utilize the Lagrangian method to simplify the complex optimization process and avoid conflicts between SP and ACP.The Lagrangian multiplier dynamically adjusts the loss weights.4) We conduct comprehensive experiments on Meta-world and multitask StarCraft II multiagent scenarios.The superior performance of TSAC demonstrate the efficiency and significance of the cooperation between the two policies and show that TSAC achieves significant improvements in both sample efficiency and effective action execution.</p>
<p>II. RELATED WORK</p>
<p>A. Multitask Reinforcement Learning</p>
<p>A multitude of methods have been developed to alleviate negative transfer and achieve efficient MTRL.Classical approaches include policy distillation [27], [28], [29], explicit measurement of task relatedness [30], [31], and information sharing [32], [33], [34], among others.Policy distillation involves training a smaller network structure to bring previous tasks to an expert level, thereby integrating multiple policies into a single policy.However, these methods increase the number of network parameters as they require separate networks for different tasks and an additional distillation step.Some researchers utilize validation loss on tasks [30] or causal influence [31] to determine better task groupings.One drawback of the aforementioned methods is the need for substantial computational resources.Calculating task correlations and adjusting learning methods can often only be done through trial and error, leading to high costs.</p>
<p>In MTRL, information sharing can be achieved through data sharing, parameters sharing, representations sharing, or behavior sharing.For instance, CDS [33] routes data based on task-specific data to improve information sharing.Similarly, a simple method proposed in [32] applies a zero reward to unlabeled data, facilitating data sharing in theory and practice.Parameter sharing through learning shared representations can effectively transfer knowledge between policies.For instance, a soft modularization method presented in [35], [36] shares parameters by generating soft combinations of different modules.Similarly, AdaShare [37] and an automated multitask learning algorithm in [38] adaptively determine the feature-sharing mode across multiple tasks.However, these methods suffer from high computational complexity as they require dynamic exploration of network connectivity.Attention mechanisms can be leveraged to share representations, as proposed in [39], [40], [41], which group task knowledge into subnetworks without requiring prior assumptions.CARE [42] leverages metadata to capture the shared structure among tasks.There is another significant MTRL work that focuses on the challenge of multiobjective optimization from a gradient perspective, aiming to reduce the impact of conflicting gradients by manipulating the gradients based on various criteria [14], [43].However, these methods impose an additional computational burden.The aforementioned methods reduce or coordinate conflicts between tasks from the perspectives of representation, gradient, and task grouping, effectively achieving MTRL.While they neglect the potential causes of task conflict.</p>
<p>Due to the exceptional multitask parallelism and generalization capabilities of MTRL, researchers have applied it to unlock more general-purpose robotics, achieving significant advancements in autonomous cognition and behavioral development [44], [45], [46].BIMTRL [47], which mimics the learning processes in the human brain, enhances parallelism and scalability in multitask reinforcement learning (MTRL), thereby contributing to improved cognitive capabilities and adaptability in robots.Ding et al. [12] proposed a meta-based task offloading scheduling strategy to solve the adaptive problems in dynamic environments.NuEMT [48], based on the mechanism of multitask learning, transfers information from a set of auxiliary tasks to the current task.This allows the robot to master complex behaviors more quickly.SURRL [49] and TINet [50] leverage unsupervised learning techniques to assist in multitask learning, enabling robots to perform subsequent policy learning based on learned structures in the absence of labeled data.</p>
<p>III. PRELIMINARIES</p>
<p>A. Multitask Reinforcement Learning</p>
<p>We extend the single-task Markov decision process (MDP) problem to multitask MDP for an agent under the framework of contextual MDP (CMDP) [51].CMDP is defined by a tuple C, S, A, γ, M .Here, C can be viewd as a task set
C = {T i } N i=1
, where T i denotes the task i and N is the number of tasks.S represents the shared state space, A denotes the shared action space, and γ is the discount factor.State s ∈ S and action a ∈ A. M is a fuction that maps a task T i ∈ C to MDP parameters, such that M(T i ) = {P i , R i , ρ i }.The transition probability P i , reward function R i and initial state distribution ρ i vary by each task.During training, tasks are sampled from a uniform distribution p(T ).The agent's policy takes state s as input and outputs action a.The objective of the agent's policy π is to maximize the expected return
E Ti∼p(T ) [E π [ t γ t R i (s t , a t )]],
where s t and a t represent the state and the action at timestep t.</p>
<p>B. Soft Actor-Critic</p>
<p>In this article, we adopt soft actor-critic (SAC) [25] with disentangled alphas as the fundamental policy.As observed in [26], SAC, being an off-policy actor-critic algorithm with a strong exploration ability based on maximum entropy, exhibits superior performance.</p>
<p>The concept of SAC goes beyond merely maximizing cumulative rewards.It also introduces additional stochasticity to the policy.In practice, the critic learns a soft Q-function Q θ parameterized by θ, and the actor learns a policy π φ parameterized by φ.A regularization term that incorporates entropy is included in the reinforcement learning objective.The correction term added with entropy can be defined as follows:
π * = arg max π φ E π φ t r (s t , a t ) + αH (π φ (• | s t )) . (1)
Authorized licensed use limited to the terms of the applicable license agreement with IEEE.Restrictions apply.Fig. 2. Structure of TSAC has two policies: a shared policy (SP) and an action correction policy (ACP).Given the states of tasks, the CARE backbone network leverages a mixture of encoders to primarily extract information.Then, the SP and ACP, respectively output actions based on guided dense rewards and goal-oriented sparse rewards.According to the action correction function, the final action is obtained and then interacts with the environment.</p>
<p>In this context, α represents a learnable regularization coefficient that controls the significance of the entropy term.H denotes the entropy.The training objective of the critic is to minimize the soft Bellman residual
J Q (θ) = E (st,at)∼D 1 2 (Q θ (s t , a t ) − (r (s t , a t ) + γE st+1∼p [Vθ (s t+1 )] 2(2)
where
Vθ(s t+1 ) = E at+1∼π <a href="3">Qθ(s t+1 , a t+1 ) − α log π(a t+1 |s t+1 )</a>
is the soft state value function.D is a replay buffer that stores past (s t , a t ).θ represents the target network parameters, which are obtained as an exponentially moving average of θ.The policy π φ is learned by minimizing
J π (φ) = E st∼D [E at∼π φ [αlog(π φ (a t |s t )) − Q θ (s t , a t )]].(4)
The learnable regularization coefficient α is updated by the gradient
J(α) = E at∼πt <a href="5">−αlogπ t (a t |s t ) − α H</a>
where H denotes the target entropy.</p>
<p>IV. METHOD</p>
<p>In this section, we present a general and complementary framework named task-specific action correction (TSAC) to decompose policy learning into two separate policies: a shared policy (SP) and an action correction policy (ACP).</p>
<p>A. Overall Structure of TSAC</p>
<p>As illustrated in Fig. 2, TSAC consists of a CARE backbone and a pair of cooperative policies.The CARE backbone [42] is utilized to extract information across all tasks by incorporating a mixture of encoders.Then, the representation is passed to the pair of cooperative policies.The first policy, called the shared policy (SP) and denoted as π φ , optimizes the guiding dense rewards by proposing a preliminary action â ∼ π φ (•|s).This preliminary action may be shortsighted.The second policy, referred to as the action correction policy (ACP) and denoted as π ψ , corrects the preliminary action by providing an action correction Δa ∼ π ψ (•|s, â) to execute an effective action.The final action denoted as a = h(â, Δa), is then output to the environment, where h represents an editing function.ACP is conditioned on SP's output â.Together, these two policies cooperate to improve sample efficiency and performance.We denote the overall composed policy as π ψ•φ (a|s) for simplicity.</p>
<p>Motivation: TSAC decomposes policy learning into two subtasks that focus on guiding dense rewards or goal-oriented sparse rewards.This decomposition is motivated by the following considerations:</p>
<p>1) Different Conflict Horizons: To achieve effective MTRL, most applications incorporate guiding dense rewards into each task.Therefore, to balance the dense reward signals between various tasks, SP's actual decision horizon is myopic.SP focuses solely on the details of specific tasks to coordinate conflicts between multiple tasks, while neglecting the overall completion of multiple tasks.In contrast, ACP aims to maximize goal-oriented sparse rewards, adopting a longer decision horizon that focuses on the overall completion of multiple tasks.However, the learning of this strategy necessitates high-quality trajectory data.Due to its ease of optimization, SP can quickly train a suboptimal policy that provides relatively high-quality trajectory data, thereby facilitating the policy learning of ACP under sparse reward settings.</p>
<p>2) Efficient Exploration: From the perspective of SP, its action is altered by ACP.Instead of discouraging SP from taking suboptimal actions, ACP offers suggestions to improve the preliminary action, enabling SP to continue exploration in an effective and far-sighted manner.This guarded exploration leads to a better overall exploration policy because ACP is less likely to hinder SP's exploration.From the perspective of ACP, it determines the action based on SP, enhancing its exploration ability.From an entropy perspective, the decomposition of policy learning into two policy networks introduces additional entropy.</p>
<p>B. Feature Extraction and CARE Backbone</p>
<p>We utilize CARE [42] backbone for extracting shared representations across all tasks.CARE backbone leverages a mixture of encoders to encode an input state into representations, which enables the incorporation of metadata and priori knowledge.Specifically, CARE uses a mixture of k encoders to learn k staterepresentations z k enc .By utilizing prior information, such as metadata or task identifiers, we can apply it to the combination of representations from k encoders.Contextual representation Z context is obtained by passing the metadata or task identifiers through a MLP.Then, the representations of k encoders are combined into a composite representation z enc by performing a weight sum using attention mechanism
z enc = k i α i × z i enc (6)
where
α j = softmax(z j enc • z i context ) ∀j ∈ {1, ..., k}.(7)
We concatenate the composite representation z enc with the contextual representation Z context as the shared representations across all tasks, which is used as an input to the SP and ACP.Note that the CARE backbone is updated using both the SP policy loss and ACP policy loss.</p>
<p>C. Goal-Oriented Sparse Rewards</p>
<p>Manually designed dense rewards incorporate prior knowledge and effectively guide policy learning.However, the magnitude of reward values does not directly indicate the ability of a policy to accomplish tasks.Therefore, we introduce goaloriented sparse rewards, which are solely related to the completion of the final objective and are not influenced by biases from prior knowledge.With sufficient exploration and high-quality data, the agent can learn a globally optimal policy.The goaloriented sparse rewards of a task T i is characterized by an " -region" in state space, represented by
R s i (s, a) = δ sg (s) if f (s, s g ) ≤ 0 else . (8)
In this equation, R s i denotes the goal-oriented sparse rewards of task T i , s is the current state, s g denotes the goal state, f (s, s g ) is a function that maps the goal state and current state to a latent space, computing the distance between them, i.e. we could set f (s, s g ) = s − s g 2 .δ sg defines the reward surface within the epsilon region and set δ sg = 1.represents a small distance threshold.</p>
<p>D. Problem Formulation</p>
<p>Following the description of CMDP (Section II-A), the initial state distribution ρ i determines the probability density of an episode starting at state s 0 .For each transition (s t , a t , s t+1 ) from task T i at timestep t, the environment produces a scalar R i (s t , a t ).It is worth mentioning that the reward function R i is manually designed and intensive, which we refer to as guiding dense rewards.Similarly, the environment produces a scalar R s i (s t , a t ) as a goal-oriented sparse reward.Higher values for both the guiding dense rewards and the goal-oriented sparse rewards indicate better performance.</p>
<p>For each state s t , the guiding dense rewards state value of policy π is denoted as
V π (s t ) = E Ti∼p(T ) E π ∞ t =t γ t −t R i (s t , a t ) . (9)
The guiding dense rewards state-action value is denoted as
Q π (s t , a t ) = E Ti∼p(T ) [R i (s t , a t ) + γE st+1∼Pi V π (s t+1 )].(10)
Similarly, We can define V π s and Q π s for the goal-oriented sparse rewards.</p>
<p>After introducing goal-oriented sparse rewards, we reconsider the learning objectives of MTRL
max π E Ti∼p(T ) E s0∼ρi V π (s 0 ), max π E Ti∼p(T ) E s0∼ρi V π s (s 0 ) . (11)
MTRL itself can be viewed as a multiobjective optimization problem.In our settings, the introduction of sparse rewards adds an additional objective to the MTRL framework, which increases the problem's complexity.Furthermore, the coupling relationship between the outputs of SP and ACP further complicates the optimization process.The dynamic nature of the two training objectives makes training difficult and hinders the collaboration of the two policies.To address this dilemma, we consider transforming multiobjective optimization into singleobjective optimization.To convert multiobjective MTRL into single-objective MTRL, we assign a virtual expected budget C to the sparse rewards.This allows us to transform the MTRL objective as follows:
max π E Ti∼p(T ) E s0∼ρi V π (s 0 ) s.t. E Ti∼p(T ) E s0∼ρi V π s (s 0 ) + C ≥ 0(12)
In this equation, C represents a virtual expected budget.Specifically for each time step, the constraint in ( 12) can be rewritten as
E Ti∼p(T ) E π t γ t (R s i (s t , a t ) + c) ≥ 0. (13)
Here, c denotes the expected budget specific to each step, and it relates to C through the equation
∞ t=0 γ t c = (c/1 − γ) = C.
Notably, the expected budget represents an average target to be achieved rather than a strict enforcement.</p>
<p>To simplify the problem, we transform the constrained single-objective MTRL into an unconstrained single-objective Authorized licensed use limited to the terms of the applicable license agreement with IEEE.Restrictions apply.</p>
<p>MTRL using the Lagrangian method.This method converts the constrained optimization problem ( 12) into an unconstrained one by introducing a multiplier λ
min λ≥0 max π E Ti∼p(T ) E s0∼ρi V π (s 0 ) + λ E Ti∼p(T ) E s0∼ρi V π s (s 0 ) + C . (14)
In this formulation, the weight of the goal-oriented sparse rewards combined with the guiding dense rewards is represented by λ.We solve this objective by using model-free MTRL algorithms.</p>
<p>E. Objective Function</p>
<p>We utilize an off-policy actor-critic approach to train the two policies.Given the overall policy π ψ•φ (a|s), we use the typical temporal difference (TD) [52] backup to learn Q π ψ•φ (s, a; θ) and Q π ψ•φ s (s, a; θ s ), which are parameterized as Q(s, a; θ) and Q s (s, a; θ s ) respectively.Here, θ and θ s represents the network parameters separately for the two state-action values.When provided with s t+1 and a t+1 , the Bellman backup operator for the guiding dense rewards state-action value is expressed as
T π ψ•φ Q(s t , a t ; θ) = E Ti∼p(T ) R i (s t , a t ) + γQ(s t+1 , a t+1 ; θ) (15)
where R i represents the guiding dense rewards obtained from task T i .The backup operator for the goal-oriented sparse rewards state-action value Q s is defined similarly with R s .Both Q(s, a; θ) and Q s (s, a; θ s ) can be learned from transitions (s t , a t , s t+1 ) sampled from a replay buffer.</p>
<p>To achieve off-policy training of ψ and φ, we convert ( 14) into a bi-level optimization surrogate.The formulation is presented as follows:
(a) max φ,ψ E s∼D [Q(s, a; θ) + λQ s (s, a; θ s ))] (b) min λ≥0 λΛ π ψ•φ . Λ π ψ•φ E s0∼ρi V π ψ•φ s (s 0 ) + C. (16)
Here, D represents a replay buffer containing a historical marginal state distribution used to train the policies.The initial state distribution ρ is employed to train λ.This distinction arises from the idea that when fine-tuning λ, we should primarily consider how well the policy satisfies the virtual expected budget starting with ρ, rather than with some historical state distribution.Subsequently, We further transform the off-policy objective [16(a)] into two parts:
(a) max φ E s∼D,â∼π φ (•|s), Δa∼π ψ (•|s,â) a=h(â,Δa) [Q(s, a); θ] (b) max ψ E s∼D,â∼π φ (•|s), Δa∼π ψ (•|s,â) a=h(â,Δa) [−d(a, â) + λQ s (s, a; θ s )].(17)
In this modified formulation, the distance function d(a, â) quantifies the change from â to a. h represents the action correction function, which is used to combine the actions output by the two policy networks to obtain the final action.ACP π ψ aims to maximize the goal-oriented sparse rewards while minimizing the distance between the actions before and after the correction.On the other hand, SP π φ solely focuses on maximizing the guiding dense rewards.Notably, ACP modifies SP's action, which aligns with the discussed motivation for efficient exploration.The training objective [17(b)] for ACP relies on a critic Q s , which learns the expected future goal-oriented sparse rewards.Consequently, guided by Q s , ACP explores actions with more significant potential in long-term sequences.</p>
<p>F. Action Correction Function and Distance Function</p>
<p>In this section, we present the design for the action correction function h(â, Δa) and the distance function d(a, â).</p>
<p>1) Action Correction Function: We opt for the correction function h to be primarily additive and nonparametric, which helps to reduce training difficulty.Without loss of generality, we assume a bounded action space [−A, A], and that both â and Δa are already within this space.Consequently, we define
a = h(â, Δa) = min(max(2â + Δa, −A), A) (18)
where min and max are element-wise.The multiplication by 2 and the clipping ensure that a ∈ [−A, A], which means that SP retains full control over the final action and can overwrite ACP's action if necessary.This is crucial because ACP faces challenges in learning an effective correction policy in the short horizon when SP still dominates the learning process.Although the additive operation is simple, the overall editing process is sufficiently general to encompass any modification.This additive action correction function is driven by the objective of achieving sparsity.Policies are generally assessed using metrics such as success, which are only triggered for specific states.To explicitly incorporate this inductive bias, we adopt the additive action correction function, which guarantees that ACP learns a policy that optimizes metrics like success in relation to SP.This simplifies the optimization landscape of ACP.</p>
<p>2) Distance Function: we utilize the hinge loss to compare the guiding dense rewards state-action values of â and a
d(a, â) max(0, Q(s, â; θ) − Q(s, a; θ)). (19)
Here, Q represents the critic and θ denotes the parameters.This loss yields zero if the edited action a already attains a higher state-action value than the preliminary action â.In such cases, only Q s is optimized by π ψ .Otherwise, the inner part of [(17(b)] is recovered as Q(s, a) + λQ s (s, a).Our distance function in the critic Q is more appropriate than L 2 distance in the action space because we ultimately care about how the Q changes after the action is edited.</p>
<p>G. Training Process</p>
<p>To practically train the objectives, we employ stochastic gradient descent (SGD) simultaneously to [16(b)] and (17).The reparameterization trick is utilized for both π ψ and π φ to enable the application of SGD.To assess Λ π ψ•φ , a batch of
Λ π ψ•φ ≈ 1 N N n=1 R s (s n , a n ) + c. (20)
Here, c represents the virtual expected budget defined in (13).Subsequent to each rollout, a batch of goal-oriented sparse rewards is collected, each reward is compared to −c, and the mean of the differences is used to adjust λ.This approximation enables the updating of λ using mini-batches of data, rather than waiting for complete episodes to conclude or relying on the often inaccurate estimated V πψ•φ s .Furthermore, the policy of SP and ACP are learned by minimizing
J π (φ) = E st∼D [E â∼π φ [αlog(π φ (â|s t )) − Q(s t , â; θ)]] (21) J π (ψ) = λE st∼D [E Δa∼π ψ [αlog(π ψ (Δa|s t )) − Q s (s t , Δa; θ s )]] − d(a, â)(22)
where α represents a learnable regularization coefficient that controls the significance of the entropy term.Multiple parallel environments are employed to mitigate temporal correlation within the rollout batch data, which constitutes the data to be placed into the replay buffer.The computational graph presenting (17) is illustrated in Fig. 3. Our approach is applicable to various goal-oriented sparse rewards and action distance functions.To encourage exploration, we integrate SAC [25], which incorporates the entropy terms of π ψ and π φ in (17), dynamically adjusting their weights based on two entropy targets as described in [25].In our experiments, both SP and ACP are trained from scratch.The pseudocode for the overall TSAC is shown in the Algorithm 1.</p>
<p>V. EXPERIMENTS</p>
<p>A. Experimental Setup</p>
<p>In this section, we evaluate the performance of TSAC on the meta-world multitask RL environment [26] and use metaworld's MT10 and MT50 benchmarks, which is shown in Fig. 4. To further validate that TSAC is algorithm-agnostic, highly generalizable, and can be extended to multiagent scenarios, we apply it to more challenging multitask StarCraft II multiagent scenarios (SMAC) [53], which is shown in Fig. 5. SMAC contains several tasks that similar but different, such as the marine-series tasks (3m, 8m, 8m_vs_9m...) and the stalker_zealotseries tasks (2s3z, 3s5z, 3s5z_vs_3s6z...), which naturally form multitask settings.</p>
<p>We compare TSAC against several baseline methods and conduct ablation studies to verify the effectiveness of our approach.</p>
<p>The first goal of our experimental evaluation is to assess.Whether TSAC improves the performance of a multitask agent.In meta-world, e compare the performance of TSAC in two different settings: a short horizon to evaluate its sample efficiency and a long horizon to measure its overall performance.For comparison, We select CARE [42], MT-SAC, soft modularization [35] and PCGrad [14] as our baselines.</p>
<p>Authorized licensed use limited to the terms of the applicable license agreement with IEEE.Restrictions apply.Furthermore, we evaluate different action correction functions to identify which yields the best performance.The action correction functions considered are SP-dominated, ACPdominated, equal, and softclip (see Section IV-C for the definitions).</p>
<p>B. Baselines</p>
<p>In this article, we will compare our method against the following baselines:</p>
<p>1) CARE: As a representations sharing method, representations are shared to learn how to compose them by leveraging additional information about each task.2) MT-SAC: This approach directly applies the SAC algorithm to the multitask setting.It utilizes a shared backbone with disentangled alphas.3) Soft modularization: As a parameters sharing method, Soft Modularization shares parameters and uses a routing network to combine all possible routes for each task softly.4) PCGrad: It is a gradient manipulation method that projects a task's gradient onto the normal plane of any other conflicting task.However, it has high time complexity and is not suitable for MT50 in the long horizon.5) TSAC (ours): Our proposed method builds upon the CARE backbone, leveraging its representation-sharing module.In contrast, our approach is based on behavior sharing and utilizes goal-oriented sparse rewards.In multitask starcraft II multiagent scenarios (SMAC), We compare TSAC with DT2GS [54], UPDeT [55], and ASN [56], which are mainly dedicated to generalization in multiagent settings.DT2GS is chosen because it is the SOTA method and possesses sound zero-shot generalization capability across tasks by maintaining consistent yet scalable semantics.It is also used in the attentive action decoder in MA2RL.UPDeT develops a multiagent  framework based on the transformer block to adapt to tasks with varying observation/state/action spaces.ASN considers the semantic difference of actions and forms a foundation of generalizable models in MARL.Similar to the settings in DT2GS, our experiments use "ASN_G" to denote the generalizable ASN.The criteria for choosing baselines depend on whether they can be applied across tasks in MARL or state-of-the-art methods.In TSAC, we introduce an important hyperparameter:c as in (13), correlated to the virtual expected budget.For MT10, we use c = 0.6 and c = 0.4 for MT50.c are used to adjust the learning weights between SP and ACP.The hyperparameters of TSAC are shown in Tables I and II.</p>
<p>C. Comparative Evaluation</p>
<p>Fig. 6(a) shows the average success rate on the 10 tasks of the MT10 benchmark form Meta-world for TSAC, CARE, MT-SAC, Soft Modularization, and PCGrad.Since the success rate is a binary variable, it is noisy; therefore, the results were averaged across multiple seeds, and the curves were smoothed.Mean and standard error are reported for each value.</p>
<p>We consider 1 million steps as a long horizon and 150 000 steps as a short horizon.The short horizon is utilized to observe the exploration ability of different methods, while the long horizon is used to visualize the performance of the method at various time points.It is worth noting that all methods are trained using SAC with disentangled alphas.</p>
<p>Table III and Fig. 6(a) demonstrate that our method outperforms all the baselines on the short horizon in MT10.Additionally, Fig. 6(c) illustrates that even in MT50 our method still outperforms all baselines.For comparison, it takes around 1 million steps for the multitask SAC agent to reach the accuracy that our TSAC agent achieves around 300 000 steps, suggesting   that our method is highly sample-efficient and explorationefficient.</p>
<p>Tables IV and V as well as Fig. 6(b) and 6(d) depict that TSAC is able to learn a good policy on the long horizon.For MT10, TSAC performs best and reaches a top success rate of 0.827 during the training.Furthermore, sampling the success rate around 0.8 million and 1 million steps reveals that TSAC has the best performance.For MT50, conflicts between tasks become more acute.CARE stops learning around 600 000 steps, and performance begins to decline in the following training steps.Despite suffering from the conflicts between tasks, TSAC's performance declines, but it still performs better than CARE.MT-SAC achieves smooth learning and performs similarly to CARE and TSAC at 0.8 million and 1 million steps.However, in terms of the best performance throughout training, MT-SAC is far inferior to TSAC, which outperforms all methods.Since the success metric is a noisy binary variable, the best performance is obtained by smoothing over the curve.Importantly, TSAC achieves average success rates of 0.450 and 0.445 on MT50 at 0.8 million and 1 million steps, surpassing the reported results from CARE, MT-SAC and Soft Modularization.</p>
<p>To further evaluate the proposed method in multitask multiagent scenarios, We conduct two multitask settings with different distributions of difficulty: stalker_zealot-series tasks (easy, easy, superhard) and marine-series tasks (hard, hard, hard).In each multitask setting, the policy interacts synchronously with multiple tasks and updates the policy using a mixed experience replay buffer.As shown in Fig. 7, TSAC exhibits the fastest convergence and highest win rate among the compared methods (DT2GS, UPDeT, ASN_G).DT2GS and UPDeT closely follow MA2RL in asymptotic performance.ASN_G fails to win in the hard multitask setting.</p>
<p>D. Zero-Shot Generalization</p>
<p>We want to evaluate whether the two policies of TSAC can cooperate and contribute to zero-shot generalization in unseen environments.In the experiment, we trained the agents on eight environments from MT10 and evaluate on two held-out environments.As illustrated in Table VI, TSAC surprisingly exhibits promising performance, outperforming all the baselines by a large margin within 1 M steps.</p>
<p>To further evaluate TSAC, we evaluate the generalization capability in six different settings in SMAC, each of which includes a target task that is either more difficult or equally difficult compared to the source task.Fig. 8 shows that TSAC outperforms all baselines in terms of zero-shot capability, especially in challenging settings (e.g.2s3z→3s5z, 8m_vs_9m→5m_vs_6m).We aim to highlight the potential Authorized licensed use limited to the terms of the applicable license agreement with IEEE.Restrictions apply.benefits of leveraging goal-oriented sparse rewards for zeroshot generalization.Furthermore, compared to baselines, the structure of two policies in TSAC is more advantageous for generalization.</p>
<p>E. Investigation on Action Correction Functions</p>
<p>The result in the previous section suggests that TSAC is both sample efficient and exploration efficient.Furthermore, TSAC yields a good policy on long horizons.In this section, we further investigate the impact of the various action correction functions employed by TSAC and discuss the potential for improved performance.</p>
<p>In this experiment, We will consider four different functions h 1) SP dominated (ours): h = min(max(2â + Δa, −A), A), SP's action dominates the final action.2) ACP dominated: h = min(max(â + 2Δa, −A), A), ACP's action dominates the final action.3) Equal contribution: h = min(max(â + Δa, −A), A), SP and ACP contribute equally to the final action.4) Softclip: h = Sof tclip(2â + Δa), h use softclip to smooth out the output action and bring in nonlinearity.Fig. 9 illustrates that SP dominated action correction function outperforms other functions in producing the final action.However, when SP and ACP contribute equally to the final action, the agent encounters difficulties learning a policy.This conflict relationship between SP and ACP presents challenges in optimizing each respective goal.Conversely, when either SP or ACP dominates the action correction function, the agent can successfully learn a well-performing policy.Comparatively, SP outperforms ACP due to ACP's focus on optimizing the goaloriented sparse rewards, which is challenging to train due to its sparsity.In Fig. 9(b) and 9(d), We found that the SP dominated significantly outperforms the ACP dominated in terms of performance, which can be attributed to the guiding dense rewards.Although ACP is trained under sparse rewards, ACP dominated exhibits similar performance to SP dominated, indicating that excessive focus on specific task details can lead to conflicts between tasks.The Softclip function exhibits a slight decrease in performance compared to our action correction function.This decline can be attributed to the introduction of nonlinearity, which further complicates and hampers the learning process.Consequently, we opt for a simple and linear operation instead of training a learnable network.</p>
<p>F. Investigation on Virtual Expected Budget c and λ</p>
<p>To investigate the effect of virtual expected budget c and λ on the model's performance, we conduct experiments on different values of c and λ as constants (λ = 1).When λ = 1, both SP and ACP optimize their objectives during the training process with equal weights.</p>
<p>To further analyze the influence of the virtual expected budget c on the performance of TSAC, experiments were conducted on the MT10 and MT50.The performance is shown in Fig. 10.In MT10, Fig. 10(a) indicates that both too small and too large values of c will affect the performance of TSAC.This is because, during the training process, ACP serves as an auxiliary learner when the performance of SP reaches a bottleneck.When c is too large, the entire training process degenerates into a multiobjective optimization, and the coupling relationship between SP and ACP greatly increases the difficulty of training.</p>
<p>Its impact on performance is equivalent to lambda being a</p>
<p>As claimed in Section IV-C, the coupling relationship between the outputs of SP and ACP further complicates the optimization process.Note: Results are reported at the end of the 6M steps on marine-series tasks and stalker_zealot-series tasks.cooperation.Thus, One key step to ensure the extraordinary performance of TSAC is to transform multiobjective optimization into single-objective optimization.</p>
<p>G. Ablations</p>
<p>We carry out ablation studies to investigate the contributions of SP and ACP.We compare TSAC against two ablations: 1) TSAC_w/o_ACP(CARE), which excludes ACP; and 2)TSAC_w/o_SP, which excludes SP.In these ablations, all other structural models are kept consistent strictly with the full TSAC.As shown in Fig. 12, the removal of ACP significantly drops the performance of TSAC on MT10 and MT50.These results indicate that ACP is crucial in enabling the agent to adopt a long-term perspective regarding conflicts.Furthermore, the comparison of TSAC and TSAC_w/o_SP means that ACP struggles with sparse rewards, while SP can provide highquality samples for ACP to assist in learning under sparse rewards.</p>
<p>To further elucidate the general applicability of TSAC, we conducted supplementary experiments across environments characterized by varying degrees of reward sparsity.In the context of multitask SMAC, the dense rewards are comprised of two components: the damage reward, which is awarded for inflicting damage on enemy units, and the kill reward, which is granted for successfully eliminating enemy units.As presented in Table VII, we assess the performance of TSAC under different levels of reward sparsity and compare it with UPDeT.The experimental findings indicate that TSAC demonstrates robust performance across a spectrum of sparse reward conditions and exhibits adaptability to diverse task reward configurations.</p>
<p>H. Analysis of Two Strategies and Sparse Rewards</p>
<p>In Section V-G, we discussed the individual contributions of SP and ACP.To further validate the necessity of decomposing policy learning into two policies in TSAC and the performance enhancement in multitask scenarios brought about by the cooperation of these two strategies, this subsection designs two additional experiments to address the following two questions: Q1: Why does TSAC decompose the policy into two policy networks rather than directly doubling the parameters of a single policy network?Q2: Why not directly combine goal-oriented sparse rewards and guiding dense rewards?As shown in Fig. 13, doubling the parameters of a single policy network enhances the learning capacity of the network, resulting in a slight improvement in overall policy learning.However, it does not effectively address the short-term conflicts between tasks in multitask settings.TSAC, on the other hand, addresses this challenge through policy decomposition and the introduction of goal-oriented sparse rewards, providing a longer-term perspective for handling short-term conflicts.This framework better leverages the advantages brought by the increase in parameter count, leading to improved performance in multitask reinforcement learning.</p>
<p>To answer Q2, Fig. 14 demonstrates that merely combining sparse rewards with goal-oriented sparse rewards does not lead to performance improvements.This is because, during the learning process of the Q-network, most updates are guided by guiding dense rewards.Dense rewards can mislead the agent into deceptive local optima, rendering goal-oriented sparse rewards nearly ineffective during the policy learning process.In contrast, TSAC employs two policy networks that update based on guiding dense rewards and goal-oriented sparse rewards, respectively.This paradigm effectively leverages the long-term perspective offered by sparse rewards, thereby addressing short-term task conflicts in multitask settings.</p>
<p>I. Visualization of the Interaction Between the SP and ACP</p>
<p>To conduct a more detailed analysis of the interaction between two actions, we have plotted the curves of the original actions (a_hat) output by the SP and the final actions (a) corrected by the ACP.In the Metaworld environment, the action space consists of four dimensions, and we have created separate plots for each dimension.As shown in Fig. 15, it is observed that the action output by the SP, denoted as a hat , is more conservative when handling multiple tasks.This conservativeness is reflected in the relatively small fluctuations of the curve in the figure.The underlying reason for this behavior is the tradeoff between the dense guiding rewards associated with the multiple tasks, which necessitates that the SP considers all tasks to the greatest extent possible in order to generate its action.Conversely, the action a that has been corrected by the ACP appears to be more aggressive.This shift in behavior can be attributed to the introduction of goal-oriented sparse rewards, which allows the policy to adopt a long-term perspective and disregard shortterm conflicts between tasks.The primary objective here is to facilitate the successful completion of the final task.In the figure, this is manifested as greater fluctuations in the action output.</p>
<p>VI. CONCLUSION AND FUTURE WORK</p>
<p>In this article, we present the task-specific action correction (TSAC), a general and complementary MTRL framework inspired by safe reinforcement learning, that surpasses the several well-performed baselines on meta-world and multitask starcraft II multiagent scenarios.We show that TSAC is able to learn a high-performing policy and achieve significant improvements in both sample efficiency and final performance compared to previous state-of-the-art multitask policies.Furthermore, TSAC is general and complementary, allowing it to be integrated with existing methods such as CARE and MT-SAC, and it can even be combined with multiagent reinforcement learning approaches.Finally, we show the benefits of decomposing policy learning into two policies and the validity of introducing goaloriented sparse rewards.</p>
<p>Although the article presents a novel framework for MTRL, there are still some limitations.TSAC decomposes policy learning into two strategies, which introduces an additional burden in terms of parameter count.Furthermore, the hyperparameter c requires fine-tuning to achieve optimal performance, although its value range can be approximately determined based on empirical observations.</p>
<p>TSAC can be well extended to other domains or types of MTRL problems, particularly in cases where prior knowledge is lacking and guiding dense rewards are not well-designed.We believe that research in fields such as real-world robotics can fully utilize goal-oriented reward signals to enhance learning performance.In future work, We will introduce a pretrained paradigm which policy is pretrained to maximize guiding dense rewards, and this policy is used as the initialization for SP.In this case, SP cannot be frozen because ACP continuously modifies its MDP.Instead, SP needs to be fine-tuned to adapt to the evolving actions of ACP.This pretrained SP has the potential to accelerate the convergence of TSAC.Authorized licensed use limited to the terms of the applicable license agreement with IEEE.Restrictions apply.</p>
<p>Fig. 1 .
1
Fig. 1.Variety of related manipulation tasks.Several actions are similar across these tasks: getting closer to the objects and interacting with them.(a) Door open.(b) Window open.(c) Drawer open.(d) Drawer close.</p>
<p>Fig. 3 .
3
Fig. 3. Computation graph of (17).Nodes denote variables or networks, and edges denote operations.The orange blocks are negative losses, the blue paths are the gradient paths of φ, and the red paths are the gradient paths of ψ.</p>
<p>Algorithm 1 : 4 for each rollout step do 5 for789
145
TSAC: Task-Specific Action Correction.Input: N tasks; virtual expected budget C 1 Initialize θ,φ and ψ; reset the replay buffer D ← ∅ 2 for each training iteration do 3 Reset the rollout batch B ← ∅; task i • • • N do 6 Action proposal by SP: â ∼ π φ (•|s); Action correcting by ACP: Δa ∼ π ψ (•|s, â); Output action h(â, Δa); Task i's transition s ∼ P i (s |s, a); 10 Add the transition to the rollout batch B ← B {(s, a, s , R i (s, a), R s i (s, a))}; batch in the buffer D ← D B;</p>
<p>Fig. 4 .
4
Fig. 4. MT10 benchmark from meta-world contains 10 tasks: reach; push; pick; and open window.</p>
<p>Fig. 5 .
5
Fig. 5. Snapshoots of SMAC.(a) SMAC 3s_vs_5z.(b) SMAC 2c_vs_64zg.(c) SMAC corridor.</p>
<p>Fig. 6 .
6
Fig. 6.Performance of TSAC and baselines, including CARE, MT-SAC, soft modularization, PCGrad, are compared in MT10 and MT50.The evaluation is conducted on two horizons: short horizon (150 K) and long horizon (1 M).The bolded lines represents the mean over four runs for both the short horizon and long horizon.The shaded area represents the standard error.(a) MT10 on short horizon.(b) MT10 on long horizon.(c) MT50 on short horizon.(d) MT50 on long horizon.</p>
<p>Fig. 7 .
7
Fig.7.Performance of TSAC and baselines, including DT2GS, UPDeT, ASN_G, are compared in multitask settings.The evaluation is conducted on two multitask problems with different distributions of difficulty: (2s3z, 3s5z, 3s5z_vs_3s6z), (5m_vs_6m, 8m_vs_9m, 10m_vs_11m).</p>
<p>Fig. 8 .
8
Fig. 8. Zero-shot generalization capability of TSAC and its baselines, including DT2GS, UPDeT, ASN_G, was compared across various source and target tasks.The evaluation was conducted on 6 zero-shot settings and the horizontal axis represent the source task → the target task, where: (a) 2s3z→3s5z; (b) 3s_vs_4z→3s_vs_5z; (c) 3s5z→3s5z_vs_3s6z; (d) 8m_vs_9m→5m_vs_6m; (e) 10m_vs_11m→8m_vs_9m; and (f) 5m_vs_6m→10m_vs_11m.</p>
<p>Fig. 9 .
9
Fig. 9. Performance of TSAC with different action correction functions, including SP dominated, ACP dominated, equal contribution, Softclip, are compared in MT10 and MT50.The evaluation is conducted on two horizons: short horizon (150 K) and long horizon (1 M).The bolded lines represents the mean over four runs for both the short horizon and long horizon.The shaded area represents the standard error.(a) MT10 on short horizon.(b) MT10 on long horizon.(c) MT50 on short horizon.(d) MT50 on long horizon.</p>
<p>Fig.</p>
<p>Fig. Performance of TSAC with different c in and MT50.(a) MT10 on long horizon.(b) MT50 on long horizon.</p>
<p>Fig. 11 .
11
Fig. 11. of TSAC with λ as constants (λ = 1), including TSAC, CARE (without ACP), TSAC_lambda = 1, are compared in MT10 and MT50.(a) MT10 on long horizon.(b) MT50 on long horizon.</p>
<p>Fig. 11 provides strong support for this argument.shown in the figure, TSAC_lambda = 1 exhibits a performance degradation compared to TSAC and CARE, indicating that SP and ACP interfere with each other, hindering their Authorized licensed use limited the terms of the applicable license agreement with IEEE.Restrictions apply.</p>
<p>Fig. 12 .
12
Fig. 12. Ablation studies.TSAC_w/o_ACP(CARE), it removes the usage of ACP and TSAC_w/o_SP, it removes the usage of SP.(a) MT10 on long horizon.(b) MT50 on long horizon.</p>
<p>Fig. 13 .
13
Fig. 13.Experiments on doubling the parameters.UPDeT-2x, it simply doubles the parameters of the policy network.</p>
<p>Fig. 14 .
14
Fig.14.Experiments on combine goal-oriented sparse rewards.UPDeTaddreward, it directly combines goal-oriented sparse rewards and guiding dense rewards in an additive manner.</p>
<p>Fig. 15 .
15
Fig. 15.Interactive of SP and ACP.we have plotted the curves of the original actions (a_hat) output by the SP and the final actions (a) corrected by the ACP.</p>
<p>Jinyuan</p>
<p>Feng received the B.Eng. degree in computer science and technology from China University of Geosciences, Beijing, China, in 2022.He is currently working toward the Ph.D. degree in control theory and control engineering with the Institute of Automation, Chinese Academy of Sciences, Beijing.His research interests include multiagent deep reinforcement learning, multitask learning, and generalization in reinforcement learning.Min Chen received the B.Sc. degree in physics from the University of Chinese Academy of Sciences, Beijing, China, in 2020, and the M.Eng.degree in control theory and control engineering from the Institute of Automation, Chinese Academy of Sciences, Beijing, in 2023.Currently, he is an Assistant Engineer with the Institute of Automation, Chinese Academy of Sciences.</p>
<p>TABLE I HYERPARAMETER
I
VALUES THAT ARE COMMON ACROSS ALL THE METHODS
HyperparameterValueBatch size1280Actor/critic sizeThree fully connected layers with 400 unitsActivation functionReLUPolicy initializationStandard GaussianPolicy learning rate3e-4Q funtion learning rate3e-4OptimizerAdamBeta for Adam optimizer(0.9, 0.999)Discount0.99Episode length150Reward scale1.0</p>
<p>TABLE II HYPERPARAMETER
II
VALUES FOR TSAC AND CARE
HyperparameterValueTask encoder sizeTwo layer network. Hidden units = 50Number of encoders/experts4 for MT10 and MT50TemperatureLearned and disentangled with tasksExpected budget0.6 for MT10 and 0.4 for MT50</p>
<p>TABLE VII WIN
VII
RATE OF TSAC IN MULTITASK SMAC WITH VARYING DEGREES OF REWARD SPARSITY AND COMPARE WITH UPDET UPDeT in stalker_zealot 99.29 ± 0.42; 98.89 ± 0.95; 70.96 ± 13.42 96.33 ± 3.43; 98.06 ± 2.40; 62.71 ± 31.89 98.13 ± 1.36; 99.02 ± 0.80; 86.54 ± 6.90 TSAC in stalker_zealot 98.34 ± 1.64; 99.
Algorithm in SMAC TasksComplete RewardOnly Damage RewardOnly Death Reward</p>
<p>29 ± 0.42; 88.96 ± 2.90 97.75</p>
<p>± 1.67; 97.93 ± 1.11; 66.92 ± 10.</p>
<p>05 98.28 ± 1.24; 99.03 ± 0.73; 85</p>
<p>.48 ± 2.23 UPDeT in marine 35.88 ± 13.54; 84.42 ± 5.80; 94.65 ± 3.24 24.77 ± 15.16; 65.32 ± 21.49; 69.63 ± 18.23 61.63 ± 26.73; 63.44 ± 28.14; 58.10 ± 26.13 TSAC in marine 54.</p>
<p>71 ± 4.43; 91.88</p>
<p>± 6.68; 86.28 ± 16.87 45.26 ± 10.21; 92.48 ± 3.69; 94.89 ± 3.46 54.08 ± 36.46; 69.25 ± 19.85; 78.35 ± 13.90</p>
<p>Authorized licensed use limited to the terms of the applicable license agreement with IEEE. Restrictions apply.
. This work was supported in part by the National Natural Science Foundation of China under Grant 62322316, and in part by Beijing Nova Program under Grant 20220484077 and Grant 20230484435.Recommended for acceptance by Associate Editor Fei Chao.
Decision making in multiagent systems: A survey. Y Rizk, M Awad, E W Tunstel, IEEE Trans. Cogn. Develop. Syst. 1032018</p>
<p>Agent-based intelligent decision support systems: A systematic review. F Khemakhem, H Ellouzi, H Ltifi, M B Ayed, IEEE Trans. Cogn. Develop. Syst. 1412022</p>
<p>Mastering the game of go with deep neural networks and tree search. D Silver, Nature. 52975872016</p>
<p>Mastering the game of go without human knowledge. D Silver, Nature. 55076762017</p>
<p>Dota 2 with large scale deep reinforcement learning. C Berner, arXiv:1912.066802019</p>
<p>Grandmaster level in starcraft II using multi-agent reinforcement learning. O Vinyals, Nature. 57577822019</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, Adv. Neural Inf. Process. Syst. 352022</p>
<p>Continuous control with deep reinforcement learning. T P Lillicrap, arXiv:1509.029712015</p>
<p>Learning to push by grasping: Using multiple tasks for effective learning. L Pinto, A Gupta, Proc. IEEE Int. Conf. Robot. Automat. (ICRA). IEEE Int. Conf. Robot. Automat. (ICRA)Piscataway, NJ, USAIEEE Press2017</p>
<p>Robust subtask learning for compositional generalization. K Jothimurugan, S Hsu, O Bastani, R Alur, Int. Conf. Mach. Learn. PMLR, 2023. </p>
<p>Lancon-learn: Learning with language to enable generalization in multi-task manipulation. A Silva, N Moorman, W Silva, Z Zaidi, N Gopalan, M Gombolay, IEEE Robot. Automat. Lett. 722021</p>
<p>A multiagent meta-based task offloading strategy for mobile-edge computing. W Ding, F Luo, C Gu, Z Dai, H Lu, IEEE Trans. Cogn. Develop. Syst. 1612024</p>
<p>An overview of multi-task learning in deep neural networks. S Ruder, arXiv:1706.050982017</p>
<p>Authorized licensed use limited to the terms of the applicable license agreement with IEEE. </p>
<p>Gradient surgery for multi-task learning. T Yu, S Kumar, A Gupta, S Levine, K Hausman, C Finn, Adv. Neur. Inf. Process. Syst. 332020</p>
<p>Qfuture: Learning future expectation cognition in multiagent reinforcement learning. B Liu, Z Pu, Y Pan, J Yi, M Chen, S Wang, IEEE Trans. Cogn. Develop. Syst. 1642024</p>
<p>Long-term and short-term opponent intention inference for football multi-player policy learning. S Wang, Z Pu, Y Pan, B Liu, H Ma, J Yi, IEEE Trans. Cogn. Develop. Syst. 166Dec. 2024</p>
<p>Learning by playing solving sparse reward tasks from scratch. M Riedmiller, Int. Conf. Mach. Learn. PMLR. 2018</p>
<p>Curriculum learning with hindsight experience replay for sequential object manipulation tasks. B Manela, A Biess, Neural Networks. 1452022</p>
<p>D2sr: Transferring dense reward function to sparse by network resetting. Y Luo, Proc. IEEE Int. Conf. Real-time Comput. Robot. (RCAR). IEEE Int. Conf. Real-time Comput. Robot. (RCAR)Piscataway, NJ, USAIEEE Press2023</p>
<p>Leveraging demonstrations for deep reinforcement learning on robotics problems with sparse rewards. M Vecerik, arXiv:1707.088172017</p>
<p>Motivation and social cognitive theory. D H Schunk, M K Dibenedetto, Contemp. Educ. Psychol. 602020. 101832</p>
<p>Towards safe reinforcement learning with a safety editor policy. H Yu, W Xu, H Zhang, Adv. Neur. Inf. Process. Syst. 352022</p>
<p>First order constrained optimization in policy space. Y Zhang, Q Vuong, K Ross, Adv. Neur. Inf. Process. Syst. 332020</p>
<p>Density constrained reinforcement learning. Z Qin, Y Chen, C Fan, Int. Conf. Mach. Learn.. PMLR, 2021. </p>
<p>Soft actor-critic: Offpolicy maximum entropy deep reinforcement learning with a stochastic actor. T Haarnoja, A Zhou, P Abbeel, S Levine, Proc. 35th Int. Conf. Mach. Learn. PMLR. 35th Int. Conf. Mach. Learn. PMLR2018</p>
<p>Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning. T Yu, Proc. Conf. Robot Learn. PMLR. Conf. Robot Learn. PMLR2020</p>
<p>Policy distillation. A A Rusu, arXiv:1511.062952015</p>
<p>Distral: Robust multitask reinforcement learning. Y Teh, Adv. Inf. Process. Syst. 302017</p>
<p>Curriculum-based asymmetric multi-task reinforcement learning. H Huang, D Ye, L Shen, W Liu, IEEE Trans. Pattern Anal. Mach. Intell. 456Jun. 2023</p>
<p>Auto-lambda: Disentangling dynamic task relationships. S Liu, S James, A J Davison, E Johns, arXiv:2202.030912022</p>
<p>Efficiently identifying task groupings for multi-task learning. C Fifty, E Amid, Z Zhao, T Yu, R Anil, C Finn, Advances in Neural Inf. Process. Syst. 342021</p>
<p>Conservative data sharing for multi-task offline reinforcement learning. T Yu, A Kumar, Y Chebotar, K Hausman, S Levine, C Finn, Adv. Neur. Inf. Process. Syst. 342021</p>
<p>How to leverage unlabeled data in offline reinforcement learning. T Yu, A Kumar, Y Chebotar, K Hausman, C Finn, S Levine, Int. Conf. Mach. Learn. PMLR. 2022</p>
<p>Paco: Parametercompositional multi-task reinforcement learning. L Sun, H Zhang, W Xu, M Tomizuka, Adv. Neur. Inf. Process. Syst. 352022</p>
<p>Multi-task reinforcement learning with soft modularization. R Yang, H Xu, Y Wu, X Wang, Adv. Neur. Inf. Process. Syst. 332020</p>
<p>Not all tasks are equally difficult: Multi-task deep reinforcement learning with dynamic depth routing. J He, Proc. AAAI Conf. AAAI Conf202438</p>
<p>Adashare: Learning what to share for efficient deep multi-task learning. X Sun, R Panda, R Feris, K Saenko, Adv. Neur. Inf. Process. Syst. 332020</p>
<p>Learning to branch for multi-task learning. P Guo, C.-Y Lee, D Ulbricht, Proc. 37th Int. Conf. Mach. Learn. PMLR. 37th Int. Conf. Mach. Learn. PMLR2020</p>
<p>Attentive multitask deep reinforcement learning. T Bram, G Brunner, O Richter, R Wattenhofer, Proc. Mach. Learn. Knowl. Discov. Databases: Eur. Conf. ECML PKDD 2019, Part III. Mach. Learn. Knowl. Discov. Databases: Eur. Conf. ECML PKDD 2019, Part IIIWürzburg, Germany. New YorkSpringer2020</p>
<p>Multi-task reinforcement learning with attention-based mixture of experts. G Cheng, L Dong, W Cai, C Sun, IEEE Robot. Automat. Lett. 86Jun. 2023</p>
<p>Multi-task reinforcement learning with mixture of orthogonal experts. A Hendawy, J Peters, C D'eramo, arXiv:2311.113852023</p>
<p>Multi-task reinforcement learning with context-based representations. S Sodhani, A Zhang, J Pineau, Int. Conf. Mach. Learn. PMLR, 2021. </p>
<p>Conflict-averse gradient descent for multi-task learning. B Liu, X Liu, X Jin, P Stone, Q Liu, Adv. Neur. Inf. Process. Syst. 342021</p>
<p>Mt-opt: Continuous multi-task robotic reinforcement learning at scale. D Kalashnikov, arXiv:2104.082122021</p>
<p>Scaling up multi-task robotic reinforcement learning. D Kalashnikov, Conf. Robot Learn. PMLR, 2022. </p>
<p>Hierarchical diffusion policy for kinematics-aware multi-task robotic manipulation. X Ma, S Patidar, I Haughton, S James, Proc. IEEE/CVF Conf. Comput. Vision Pattern Recognit. IEEE/CVF Conf. Comput. Vision Pattern Recognit2024</p>
<p>A brain-inspired incremental multitask reinforcement learning approach. C Jin, X Feng, H Yu, IEEE Trans. Cogn. Develop. Syst. 1632024</p>
<p>Multitask neuroevolution for reinforcement learning with long and short episodes. N Zhang, A Gupta, Z Chen, Y.-S Ong, IEEE Trans. Cogn. Develop. Syst. 1532023</p>
<p>Surrl: Structural unsupervised representations for robot learning. F Zhang, Y Chen, H Qiao, Z Liu, IEEE Trans. Cogn. Developmental Syst. 1522023</p>
<p>Continual robot learning using selfsupervised task inference. M B Hafez, S Wermter, IEEE Trans. Cogn. Develop. Syst. 1632024</p>
<p>Contextual markov decision processes. A Hallak, D Di Castro, S Mannor, arXiv:1502.022592015</p>
<p>Temporal difference learning and td-gammon. G Tesauro, Commun. ACM. 3831995</p>
<p>The starcraft multi-agent challenge. M Samvelyan, arXiv:1902.040432019</p>
<p>Decompose a task into generalizable subtasks in multi-agent reinforcement learning. Z Tian, Advances in Neural Information Processing Systems. A Oh, T Neumann, A Globerson, K Saenko, M Hardt, S Levine, Curran Associates, Inc202336</p>
<p>Updet: Universal multi-agent reinforcement learning via policy decoupling with transformers. S Hu, F Zhu, X Chang, X Liang, arXiv:2101.080012021</p>
<p>Action semantics network: Considering the effects of actions in multiagent systems. W Wang, Proc. 8th Int. Conf. Learn. Represent. ICLR 2020. 8th Int. Conf. Learn. Represent. ICLR 2020Addis Ababa2020</p>            </div>
        </div>

    </div>
</body>
</html>
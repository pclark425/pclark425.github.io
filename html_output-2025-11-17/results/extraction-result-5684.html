<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5684 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5684</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5684</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-116.html">extraction-schema-116</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <p><strong>Paper ID:</strong> paper-265235814</p>
                <p><strong>Paper Title:</strong> Prompt Engineering: a methodology for optimizing interactions with AI-Language Models in the field of engineering â€¢</p>
                <p><strong>Paper Abstract:</strong> ChatGPT is a versatile conversational Artificial Intelligence model that responds to user input prompts, with applications in academia and various sectors. However, crafting effective prompts can be challenging, leading to potentially inaccurate or contextually inappropriate responses, emphasizing the importance of prompt engineering in achieving accurate outcomes across different domains. This study aims to address this void by introducing a methodology for optimizing interactions with Artificial Intelligence language models, like ChatGPT, through prompts in the field of engineering. The approach is called GPEI and relies on the latest advancements in this area; and consists of four steps: define the objective, design the prompt, evaluate the response, and iterate. Our proposal involves two key aspects: data inclusion in prompt design for engineering applications and the integration of Explainable Artificial Intelligence principles to assess responses, enhancing transparency. It combines insights from various methodologies to address issues like hallucinations, emphasizing iterative prompt refinement techniques like posing opposing questions and using specific patterns for improvement. This methodology could improve prompt precision and utility in engineering.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5684.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5684.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPEI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Goal Prompt Evaluation Iteration</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative four-step methodology (define goal, design prompt, evaluate answer, iterate) proposed to optimize prompt-based interactions with LLMs for engineering tasks, emphasizing inclusion of task data and XAI-style evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>General engineering problem solving / prompt optimization</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Guiding the design and refinement of prompts to obtain accurate, justifiable responses from LLMs in engineering contexts; demonstrated with a 0-1 knapsack example.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Iterative prompt refinement: initial plain instruction -> request multiple candidate sets -> request formal problem formulation (objective + constraints) -> request code generation for solver execution; prompts include explicit data (item list) and constraints, and request stepwise/bullet breakdowns as needed.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>The iterative process and inclusion of explicit problem data plus XAI-style checks (ask for reasoning, break down steps, request sources) reduce hallucinations and improve result quality; converting a natural-language task into formal formulation or executable code leverages external exact solvers when the LLM's direct combinatorial search is unreliable.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5684.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5684.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Open-vs-Closed prompts</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Open-ended versus Closed-ended prompts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Categorization of prompts by answer openness: open-ended prompts elicit extended, detailed responses; closed-ended prompts request specific/limited answers (yes/no, multiple choice, short factual answer).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Unspecified LLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>General response elicitation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks where either expansive discussion or precise factual outputs are needed; choice of open vs closed affects verbosity and specificity of LLM outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Open-ended questions vs closed-ended questions (explicitly restricting response length/format).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Open-ended vs closed-ended prompts</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>implied: format affects response detail and suitability (no quantitative data)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Open-ended prompts allow fuller reasoning and richer content (useful for critical reading), while closed-ended prompts constrain output and can produce more precise, narrow answers; format choice should match the desired output type.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5684.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5684.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt Levels 1-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt Information Levels (1 to 4)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A level-based categorization where Level 1 is a simple question, Level 2 adds context about writer/model, Level 3 supplies examples for the LLM to reference, and Level 4 asks the model to break tasks into subcomponents (step-by-step).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Unspecified LLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>General prompting sophistication</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Improving prompt context and structure progressively to obtain more accurate or structured outputs from LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Level 1 (simple) -> Level 2 (context) -> Level 3 (examples/few-shot) -> Level 4 (task decomposition/stepwise instructions).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Levels compared implicitly (simple vs context vs examples vs decomposition)</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>implied: higher levels (more context/examples/decomposition) improve accuracy/structure</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Providing context and examples, or instructing the model to decompose the task, gives the model more constraints and guidance, which tends to improve precision and reduce ambiguity/hallucination.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5684.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5684.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Instruction/System/QA/Mixed</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Instructive, System, Question-Answer, and Mixed prompt types</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Classification of prompts by structural intent: instructive (verb-first commands), system (context-setting), question-answer (wh-type queries), and mixed (combinations).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Unspecified LLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Prompt style selection</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Different prompt styles used across tasks to direct LLM behavior and output form.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Instructive (commands), system (context + role), QA (direct questions), mixed combinations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>implied: choice of style affects response framing and completeness</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Selecting a prompt style aligned with the task improves relevance and format of output (e.g., system prompts can prime a persona or starting context; instructive prompts specify exact actions).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5684.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5684.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zero-shot/Few-shot</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-shot and Few-shot prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Classification by number of examples included: zero-shot uses none (relies on model internal knowledge), few-shot includes example(s) to guide desired output format or reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Unspecified LLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>General NLP tasks (translation, summarization, classification)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Zero-shot works when model can answer from pretraining; few-shot provides demonstrations to steer responses for tasks where explicit examples help.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Zero-shot (no examples) vs few-shot (one or several examples included in prompt).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Zero-shot vs few-shot</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>implied: few-shot can improve task-specific performance over zero-shot in some tasks</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Providing examples reduces ambiguity and demonstrates desired output structure, improving consistency and task adherence.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5684.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5684.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought prompts</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought / stepwise reasoning prompts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompt templates that encourage the model to produce intermediate reasoning steps (e.g., 'Let's think step by step...') to improve multi-step reasoning and reduce errors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Unspecified LLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multi-step reasoning tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Problems requiring intermediate logical or arithmetic steps (e.g., math word problems, multi-step planning).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Explicit instruction to reason step-by-step or decompose the task; can be implemented by phrases such as 'Let's think step by step' or requesting bullets/steps.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Standard concise answer vs chain-of-thought stepwise answer</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>implied: improved</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>For complex reasoning tasks, instructing stepwise decomposition provides intermediate structure that helps the LLM avoid skipping steps and producing incorrect final answers.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5684.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5684.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Tree of Thoughts</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tree of Thoughts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting/decoding strategy that structures reasoning as a tree of intermediate states and deliberation steps to improve problem-solving on complex tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Tree of thoughts: deliberate problem solving with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Unspecified LLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Complex problem solving / deliberative reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks where exploring alternative reasoning branches and backtracking improves solution quality (e.g., puzzles, planning).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Structured prompts or decoding that request generation and evaluation of multiple intermediate 'thoughts' (branches) before committing to a solution.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Standard single-chain reasoning vs tree-structured deliberation</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>claimed: improved for hard reasoning tasks</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Exploring multiple candidate reasoning paths and selecting among them reduces the chance of committing early to incorrect chains of thought and yields better outcomes for hard combinatorial tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5684.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e5684.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt Pattern Catalog</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt Pattern Catalog (12 patterns)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A catalog of prompt design patterns (including output automator, persona, visualization generator, recipe, template, error identification, improvement patterns) used to customize outputs and improve input/output quality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A prompt pattern catalog to enhance prompt engineering with chatgpt</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Unspecified LLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Prompt engineering across tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>A taxonomy of reusable prompt templates/patterns to achieve common intents (customize output format, debug errors, improve input quality, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Pattern-based prompt structures including templates, persona specification, output format constraints (paragraph, bullets, table, JSON), and improvement/error-identification prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>implied: using patterns improves output control and quality</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Using standardized patterns helps users convey intent, expected format, and verification steps to the LLM, reducing ambiguity and facilitating iteration.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5684.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e5684.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Instruction-Context Separation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Separating instructions from context / use of quoting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Recommendation to separate the user's instructions from contextual text (e.g., via breaks or quotation marks) and to include examples to articulate desired output.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Unspecified LLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Context-heavy tasks / text transformation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Prompts that mix long source texts with instructions for transformation, summarization, or analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Explicit separation of the instruction block from the context block; quoting contexts and giving a short directive after.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Mixed instruction+context vs separated instruction/context</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>implied: improved clarity/accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Separating instruction from context disambiguates which text is the source and which text are commands, helping the model to correctly apply instructions to the intended context.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5684.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e5684.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Evaluation Prompts & XAI checks</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Response-evaluation prompts and XAI-style checks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using additional prompts to evaluate LLM outputs, e.g., rephrasing the question, requesting opposing viewpoints, listing facts to fact-check, asking for reasoning and data sources, and having the model rate source credibility.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Unspecified LLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Result verification and hallucination reduction</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Procedures to assess completeness, accuracy, relevance, constraints satisfaction, and potential hallucinations in LLM outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Secondary evaluation prompts (error identification, counterfactual/opposing prompts, source listing, credibility ratings) and XAI requests (explain reasoning, break steps, justify answers).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Single-pass generation vs generate+evaluate cycles</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>implied: improved reliability after evaluation/iteration</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Asking the model to self-evaluate, list dependent facts, or provide sources helps detect hallucinations and incomplete reasoning, supporting iterative prompt refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5684.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e5684.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Knapsack case</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>0-1 Knapsack example (application of GPEI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Applied example where ChatGPT was iteratively prompted to solve a knapsack optimization using progressively more structured prompts; initial direct prompts produced suboptimal solutions but asking for formalization and code generation led to an optimal solution when executed externally.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>0-1 Knapsack optimization (engineering example)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given 10 items (value and weight) and a 10 kg capacity, find the item subset with maximum total value not exceeding capacity.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Iteration 1: simple instruction 'choose set with highest total value' including item list; Iteration 2: 'give three sets with highest total value, only sets not exceeding 10 kg'; Iteration 3: 'formulate objective function and constraints' (explicit mathematical formulation); Iteration 4: 'generate code for Google Colab to solve the problem' and run solver.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Direct natural-language solution requests vs providing formal optimization formulation vs generating executable solver code</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Iteration 1/2 (direct natural-language): best solution reported had total value 24 (items 1,2,3) within 10 kg; Optimization tool / final Colab-run solution: total value 30 (items 1,2,4,6,8) at 10 kg.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Natural-language direct prompts produced total value 24 versus code+solver produced total value 30.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+6 total value (30 vs 24) when converting to formal problem formulation and executable code solved by external optimizer</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>ChatGPT's direct combinatorial reasoning was unable to reliably find the global optimum for this discrete optimization; converting the problem into a formal mathematical program and executable code allowed use of exact solvers (external computation), yielding the true optimum. Iterative prompt reformulation (including formalization and code generation) improved final task performance.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>Counterexample: Even when asked for multiple sets (iteration 2) or direct solution (iteration 1), ChatGPT failed to find the true optimum â€” demonstrating formats that ask for direct enumeration/solution can fail on combinatorial problems.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>A prompt pattern catalog to enhance prompt engineering with chatgpt <em>(Rating: 2)</em></li>
                <li>Tree of thoughts: deliberate problem solving with large language models <em>(Rating: 2)</em></li>
                <li>Dehallucinating large language models using formal methods guided iterative prompting <em>(Rating: 2)</em></li>
                <li>Pretrain, prompt, and predict: a systematic survey of prompting methods in natural language processing <em>(Rating: 2)</em></li>
                <li>Prompting large language models with the socratic method <em>(Rating: 1)</em></li>
                <li>The art and science of prompt engineering: a new literacy in the information age <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5684",
    "paper_id": "paper-265235814",
    "extraction_schema_id": "extraction-schema-116",
    "extracted_data": [
        {
            "name_short": "GPEI",
            "name_full": "Goal Prompt Evaluation Iteration",
            "brief_description": "An iterative four-step methodology (define goal, design prompt, evaluate answer, iterate) proposed to optimize prompt-based interactions with LLMs for engineering tasks, emphasizing inclusion of task data and XAI-style evaluation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ChatGPT",
            "model_size": null,
            "task_name": "General engineering problem solving / prompt optimization",
            "task_description": "Guiding the design and refinement of prompts to obtain accurate, justifiable responses from LLMs in engineering contexts; demonstrated with a 0-1 knapsack example.",
            "problem_format": "Iterative prompt refinement: initial plain instruction -&gt; request multiple candidate sets -&gt; request formal problem formulation (objective + constraints) -&gt; request code generation for solver execution; prompts include explicit data (item list) and constraints, and request stepwise/bullet breakdowns as needed.",
            "comparison_format": null,
            "performance": null,
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "improved",
            "explanation_or_hypothesis": "The iterative process and inclusion of explicit problem data plus XAI-style checks (ask for reasoning, break down steps, request sources) reduce hallucinations and improve result quality; converting a natural-language task into formal formulation or executable code leverages external exact solvers when the LLM's direct combinatorial search is unreliable.",
            "counterexample_or_null_result": null,
            "uuid": "e5684.0"
        },
        {
            "name_short": "Open-vs-Closed prompts",
            "name_full": "Open-ended versus Closed-ended prompts",
            "brief_description": "Categorization of prompts by answer openness: open-ended prompts elicit extended, detailed responses; closed-ended prompts request specific/limited answers (yes/no, multiple choice, short factual answer).",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Unspecified LLM",
            "model_size": null,
            "task_name": "General response elicitation",
            "task_description": "Tasks where either expansive discussion or precise factual outputs are needed; choice of open vs closed affects verbosity and specificity of LLM outputs.",
            "problem_format": "Open-ended questions vs closed-ended questions (explicitly restricting response length/format).",
            "comparison_format": "Open-ended vs closed-ended prompts",
            "performance": null,
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "implied: format affects response detail and suitability (no quantitative data)",
            "explanation_or_hypothesis": "Open-ended prompts allow fuller reasoning and richer content (useful for critical reading), while closed-ended prompts constrain output and can produce more precise, narrow answers; format choice should match the desired output type.",
            "counterexample_or_null_result": null,
            "uuid": "e5684.1"
        },
        {
            "name_short": "Prompt Levels 1-4",
            "name_full": "Prompt Information Levels (1 to 4)",
            "brief_description": "A level-based categorization where Level 1 is a simple question, Level 2 adds context about writer/model, Level 3 supplies examples for the LLM to reference, and Level 4 asks the model to break tasks into subcomponents (step-by-step).",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Unspecified LLM",
            "model_size": null,
            "task_name": "General prompting sophistication",
            "task_description": "Improving prompt context and structure progressively to obtain more accurate or structured outputs from LLMs.",
            "problem_format": "Level 1 (simple) -&gt; Level 2 (context) -&gt; Level 3 (examples/few-shot) -&gt; Level 4 (task decomposition/stepwise instructions).",
            "comparison_format": "Levels compared implicitly (simple vs context vs examples vs decomposition)",
            "performance": null,
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "implied: higher levels (more context/examples/decomposition) improve accuracy/structure",
            "explanation_or_hypothesis": "Providing context and examples, or instructing the model to decompose the task, gives the model more constraints and guidance, which tends to improve precision and reduce ambiguity/hallucination.",
            "counterexample_or_null_result": null,
            "uuid": "e5684.2"
        },
        {
            "name_short": "Instruction/System/QA/Mixed",
            "name_full": "Instructive, System, Question-Answer, and Mixed prompt types",
            "brief_description": "Classification of prompts by structural intent: instructive (verb-first commands), system (context-setting), question-answer (wh-type queries), and mixed (combinations).",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Unspecified LLM",
            "model_size": null,
            "task_name": "Prompt style selection",
            "task_description": "Different prompt styles used across tasks to direct LLM behavior and output form.",
            "problem_format": "Instructive (commands), system (context + role), QA (direct questions), mixed combinations.",
            "comparison_format": null,
            "performance": null,
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "implied: choice of style affects response framing and completeness",
            "explanation_or_hypothesis": "Selecting a prompt style aligned with the task improves relevance and format of output (e.g., system prompts can prime a persona or starting context; instructive prompts specify exact actions).",
            "counterexample_or_null_result": null,
            "uuid": "e5684.3"
        },
        {
            "name_short": "Zero-shot/Few-shot",
            "name_full": "Zero-shot and Few-shot prompting",
            "brief_description": "Classification by number of examples included: zero-shot uses none (relies on model internal knowledge), few-shot includes example(s) to guide desired output format or reasoning.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Unspecified LLM",
            "model_size": null,
            "task_name": "General NLP tasks (translation, summarization, classification)",
            "task_description": "Zero-shot works when model can answer from pretraining; few-shot provides demonstrations to steer responses for tasks where explicit examples help.",
            "problem_format": "Zero-shot (no examples) vs few-shot (one or several examples included in prompt).",
            "comparison_format": "Zero-shot vs few-shot",
            "performance": null,
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "implied: few-shot can improve task-specific performance over zero-shot in some tasks",
            "explanation_or_hypothesis": "Providing examples reduces ambiguity and demonstrates desired output structure, improving consistency and task adherence.",
            "counterexample_or_null_result": null,
            "uuid": "e5684.4"
        },
        {
            "name_short": "Chain-of-Thought prompts",
            "name_full": "Chain-of-Thought / stepwise reasoning prompts",
            "brief_description": "Prompt templates that encourage the model to produce intermediate reasoning steps (e.g., 'Let's think step by step...') to improve multi-step reasoning and reduce errors.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Unspecified LLM",
            "model_size": null,
            "task_name": "Multi-step reasoning tasks",
            "task_description": "Problems requiring intermediate logical or arithmetic steps (e.g., math word problems, multi-step planning).",
            "problem_format": "Explicit instruction to reason step-by-step or decompose the task; can be implemented by phrases such as 'Let's think step by step' or requesting bullets/steps.",
            "comparison_format": "Standard concise answer vs chain-of-thought stepwise answer",
            "performance": null,
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "implied: improved",
            "explanation_or_hypothesis": "For complex reasoning tasks, instructing stepwise decomposition provides intermediate structure that helps the LLM avoid skipping steps and producing incorrect final answers.",
            "counterexample_or_null_result": null,
            "uuid": "e5684.5"
        },
        {
            "name_short": "Tree of Thoughts",
            "name_full": "Tree of Thoughts",
            "brief_description": "A prompting/decoding strategy that structures reasoning as a tree of intermediate states and deliberation steps to improve problem-solving on complex tasks.",
            "citation_title": "Tree of thoughts: deliberate problem solving with large language models",
            "mention_or_use": "mention",
            "model_name": "Unspecified LLM",
            "model_size": null,
            "task_name": "Complex problem solving / deliberative reasoning",
            "task_description": "Tasks where exploring alternative reasoning branches and backtracking improves solution quality (e.g., puzzles, planning).",
            "problem_format": "Structured prompts or decoding that request generation and evaluation of multiple intermediate 'thoughts' (branches) before committing to a solution.",
            "comparison_format": "Standard single-chain reasoning vs tree-structured deliberation",
            "performance": null,
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "claimed: improved for hard reasoning tasks",
            "explanation_or_hypothesis": "Exploring multiple candidate reasoning paths and selecting among them reduces the chance of committing early to incorrect chains of thought and yields better outcomes for hard combinatorial tasks.",
            "counterexample_or_null_result": null,
            "uuid": "e5684.6"
        },
        {
            "name_short": "Prompt Pattern Catalog",
            "name_full": "Prompt Pattern Catalog (12 patterns)",
            "brief_description": "A catalog of prompt design patterns (including output automator, persona, visualization generator, recipe, template, error identification, improvement patterns) used to customize outputs and improve input/output quality.",
            "citation_title": "A prompt pattern catalog to enhance prompt engineering with chatgpt",
            "mention_or_use": "mention",
            "model_name": "Unspecified LLM",
            "model_size": null,
            "task_name": "Prompt engineering across tasks",
            "task_description": "A taxonomy of reusable prompt templates/patterns to achieve common intents (customize output format, debug errors, improve input quality, etc.).",
            "problem_format": "Pattern-based prompt structures including templates, persona specification, output format constraints (paragraph, bullets, table, JSON), and improvement/error-identification prompts.",
            "comparison_format": null,
            "performance": null,
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "implied: using patterns improves output control and quality",
            "explanation_or_hypothesis": "Using standardized patterns helps users convey intent, expected format, and verification steps to the LLM, reducing ambiguity and facilitating iteration.",
            "counterexample_or_null_result": null,
            "uuid": "e5684.7"
        },
        {
            "name_short": "Instruction-Context Separation",
            "name_full": "Separating instructions from context / use of quoting",
            "brief_description": "Recommendation to separate the user's instructions from contextual text (e.g., via breaks or quotation marks) and to include examples to articulate desired output.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Unspecified LLM",
            "model_size": null,
            "task_name": "Context-heavy tasks / text transformation",
            "task_description": "Prompts that mix long source texts with instructions for transformation, summarization, or analysis.",
            "problem_format": "Explicit separation of the instruction block from the context block; quoting contexts and giving a short directive after.",
            "comparison_format": "Mixed instruction+context vs separated instruction/context",
            "performance": null,
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "implied: improved clarity/accuracy",
            "explanation_or_hypothesis": "Separating instruction from context disambiguates which text is the source and which text are commands, helping the model to correctly apply instructions to the intended context.",
            "counterexample_or_null_result": null,
            "uuid": "e5684.8"
        },
        {
            "name_short": "Evaluation Prompts & XAI checks",
            "name_full": "Response-evaluation prompts and XAI-style checks",
            "brief_description": "Using additional prompts to evaluate LLM outputs, e.g., rephrasing the question, requesting opposing viewpoints, listing facts to fact-check, asking for reasoning and data sources, and having the model rate source credibility.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Unspecified LLM",
            "model_size": null,
            "task_name": "Result verification and hallucination reduction",
            "task_description": "Procedures to assess completeness, accuracy, relevance, constraints satisfaction, and potential hallucinations in LLM outputs.",
            "problem_format": "Secondary evaluation prompts (error identification, counterfactual/opposing prompts, source listing, credibility ratings) and XAI requests (explain reasoning, break steps, justify answers).",
            "comparison_format": "Single-pass generation vs generate+evaluate cycles",
            "performance": null,
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "implied: improved reliability after evaluation/iteration",
            "explanation_or_hypothesis": "Asking the model to self-evaluate, list dependent facts, or provide sources helps detect hallucinations and incomplete reasoning, supporting iterative prompt refinement.",
            "counterexample_or_null_result": null,
            "uuid": "e5684.9"
        },
        {
            "name_short": "Knapsack case",
            "name_full": "0-1 Knapsack example (application of GPEI)",
            "brief_description": "Applied example where ChatGPT was iteratively prompted to solve a knapsack optimization using progressively more structured prompts; initial direct prompts produced suboptimal solutions but asking for formalization and code generation led to an optimal solution when executed externally.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ChatGPT",
            "model_size": null,
            "task_name": "0-1 Knapsack optimization (engineering example)",
            "task_description": "Given 10 items (value and weight) and a 10 kg capacity, find the item subset with maximum total value not exceeding capacity.",
            "problem_format": "Iteration 1: simple instruction 'choose set with highest total value' including item list; Iteration 2: 'give three sets with highest total value, only sets not exceeding 10 kg'; Iteration 3: 'formulate objective function and constraints' (explicit mathematical formulation); Iteration 4: 'generate code for Google Colab to solve the problem' and run solver.",
            "comparison_format": "Direct natural-language solution requests vs providing formal optimization formulation vs generating executable solver code",
            "performance": "Iteration 1/2 (direct natural-language): best solution reported had total value 24 (items 1,2,3) within 10 kg; Optimization tool / final Colab-run solution: total value 30 (items 1,2,4,6,8) at 10 kg.",
            "performance_comparison": "Natural-language direct prompts produced total value 24 versus code+solver produced total value 30.",
            "format_effect_size": "+6 total value (30 vs 24) when converting to formal problem formulation and executable code solved by external optimizer",
            "format_effect_direction": "improved",
            "explanation_or_hypothesis": "ChatGPT's direct combinatorial reasoning was unable to reliably find the global optimum for this discrete optimization; converting the problem into a formal mathematical program and executable code allowed use of exact solvers (external computation), yielding the true optimum. Iterative prompt reformulation (including formalization and code generation) improved final task performance.",
            "counterexample_or_null_result": "Counterexample: Even when asked for multiple sets (iteration 2) or direct solution (iteration 1), ChatGPT failed to find the true optimum â€” demonstrating formats that ask for direct enumeration/solution can fail on combinatorial problems.",
            "uuid": "e5684.10"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "A prompt pattern catalog to enhance prompt engineering with chatgpt",
            "rating": 2,
            "sanitized_title": "a_prompt_pattern_catalog_to_enhance_prompt_engineering_with_chatgpt"
        },
        {
            "paper_title": "Tree of thoughts: deliberate problem solving with large language models",
            "rating": 2,
            "sanitized_title": "tree_of_thoughts_deliberate_problem_solving_with_large_language_models"
        },
        {
            "paper_title": "Dehallucinating large language models using formal methods guided iterative prompting",
            "rating": 2,
            "sanitized_title": "dehallucinating_large_language_models_using_formal_methods_guided_iterative_prompting"
        },
        {
            "paper_title": "Pretrain, prompt, and predict: a systematic survey of prompting methods in natural language processing",
            "rating": 2,
            "sanitized_title": "pretrain_prompt_and_predict_a_systematic_survey_of_prompting_methods_in_natural_language_processing"
        },
        {
            "paper_title": "Prompting large language models with the socratic method",
            "rating": 1,
            "sanitized_title": "prompting_large_language_models_with_the_socratic_method"
        },
        {
            "paper_title": "The art and science of prompt engineering: a new literacy in the information age",
            "rating": 1,
            "sanitized_title": "the_art_and_science_of_prompt_engineering_a_new_literacy_in_the_information_age"
        }
    ],
    "cost": 0.013786999999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Prompt Engineering: a methodology for optimizing interactions with AI-Language Models in the field of engineering â€¢</p>
<p>Juan David VelÃ¡squez-Henao 
Carlos Jaime Franco-Cardona 
Lorena Cadavid-Higuita </p>
<p>Facultad de Minas
Universidad Nacional de Colombia, sede MedellÃ­n
MedellÃ­nColombia</p>
<p>Universidad Nacional de Colombia
MedellÃ­nColombia</p>
<p>Universidad Nacional de Colombia
MedellÃ­n</p>
<p>Prompt Engineering: a methodology for optimizing interactions with AI-Language Models in the field of engineering â€¢
5F701F8DC24582AC0E87ACF484EA3AC710.15446/dyna.v90n230.111700ChatGPTprompt engineeringlarge language modelsprompt design
ChatGPT is a versatile conversational Artificial Intelligence model that responds to user input prompts, with applications in academia and various sectors.However, crafting effective prompts can be challenging, leading to potentially inaccurate or contextually inappropriate responses, emphasizing the importance of prompt engineering in achieving accurate outcomes across different domains.This study aims to address this void by introducing a methodology for optimizing interactions with Artificial Intelligence language models, like ChatGPT, through prompts in the field of engineering.The approach is called GPEI and relies on the latest advancements in this area; and consists of four steps: define the objective, design the prompt, evaluate the response, and iterate.Our proposal involves two key aspects: data inclusion in prompt design for engineering applications and the integration of Explainable Artificial Intelligence principles to assess responses, enhancing transparency.It combines insights from various methodologies to address issues like hallucinations, emphasizing iterative prompt refinement techniques like posing opposing questions and using specific patterns for improvement.This methodology could improve prompt precision and utility in engineering.</p>
<p>Introduction</p>
<p>ChatGPT is the acronym for Chat Generative Pre-Trained Transformer, which can respond based on user requests using input text prompts [1].As a conversational large language based on the knowledge accumulated during the training phase [4].Typical applications include text generation, summarization, and translation [5].</p>
<p>A prompt is a text describing a set of instructions that customize, refine, or enhance the capabilities of a GPT model [6].Effective prompts are characterized by the following fundamental principles [7]: clarity and precision, contextual information, desired format, and verbosity control.However, writing effective prompts seems complicated for nontechnical users, requiring creativity, intuition, and iterative refinement [7].The problem becomes more significant when it is necessary to incorporate precise information to solve tasks in specific contexts.</p>
<p>Consequently, the answers can be vague, imprecise, factually incorrect, or contextually inappropriate [5] when an inadequate prompt is used.In this context, prompt engineering emerges as a discipline to design prompts by which users can program a Large Language Model, such as ChatGPT, to produce accurate answers [8,9].</p>
<p>During the last year, much gray literature has been generated that mainly presents prompt templates for specific workflows and tasks in marketing, advertising, SEO, and text translation.There are also many prompt templates for using these technologies in everyday tasks such as travel preparation.A quick analysis of this material allows us to conclude that the considerations made tend to be repetitive and need more depth to use these technologies as assistants in professional practice and engineering education.On the other hand, in the most relevant literature, efforts have been made to formalize the construction of prompts, as will be discussed later.However, research has concentrated mainly on practice, research, teaching, and scientific publication in health.Consequently, there is a gap when considering the opportunities of using GPT and conversational assistants in engineering; a similar conclusion is achieved in [10] by analyzing the potential uses of LLM in business process management.</p>
<p>This work seeks to fill this gap.We propose a methodology for developing prompts in engineering based on an iterative process based on the main developments that have been presented to date on this topic.</p>
<p>The rest of this work is organized as follows: Section 2 overviews the current discourse on GPT and prompt engineering.Section 3 presents the methodology employed to propose a framework for engineering prompts, elaborating and illustrating it further in Section 4. Finally, the paper is concluded in Section 5.</p>
<p>2</p>
<p>Literature review</p>
<p>Generative language models are part of the broader category of pre-trained Generative Transformers (GPT) and are part of deep learning models [11].Their competence in interpreting and producing human language is based on the principles and techniques of natural language processing (NLP) [12].The GPT approach is based on the principles of deep learning and NLP.Prompt engineering is essential to use generative language models effectively [13].</p>
<p>To evaluate generative language models, sentiment analysis and opinion mining are usually used, where users rate their performance and express their opinions.This evaluation allows for continuous improvement of these systems.However, there are also profound ethical implications to its use, given its potential for disseminating biased or erroneous data and its implications for plagiarism and copyright [14,15].In essence, generative language models offer substantial opportunities in various fields but require careful implementation, ethical oversight, and responsive adaptation to ensure their positive impact and reliability [16][17][18][19].</p>
<p>Discussion about ChatGPT</p>
<p>The current academic discussion about ChatGPT and LLM revolves around three axes: Generative Artificial Intelligence, Education, and Ethics.</p>
<p>Generative artificial intelligence (Generative AI), based on transformer architectures, is rapidly advancing in the domain of artificial intelligence [20,21].These models can generate content in various formats, such as text, images, and more, that closely resemble what humans produce [20,21].These elements profoundly impact professional practice and education [3,22].Many concerns relate to academic and professional integrity and student evaluation [22,23].The dual nature of technology (enabling academic dishonesty while potentially enriching pedagogical approaches) forces institutions to critically evaluate their assessment methodologies to ensure content accuracy and authenticity [22,23].The foray of generative AI into the educational space has motivated academics to rethink traditional educational frameworks, generating opportunities and challenges [24,25].In summary, the integration of generative artificial intelligence at the professional and academic levels requires rigorous evaluation, continuous research, and adaptation strategies to take advantage of its advantages and address the associated challenges [22][23][24][25].</p>
<p>Education is one of the areas most impacted by the popularization of GPTs, mainly by conversational agents powered by AI [26].There are already many publications on this aspect in the most relevant literature, particularly for medical education.Since conversational models can generate human-like text, they can be used in curriculum design, as part of teaching methodologies, and to create personalized learning resources [26].These capabilities facilitate the teaching of complex concepts and help educators monitor and refine their pedagogical approaches [26].Beyond the educational field, conversational agents can offer relevant and accurate information to individuals and communities, thus demonstrating their usefulness as a complementary information resource that improves access to information and decision-making [14].As in other cases discussed, there are essential concerns about possible bias, security, and ethical implications associated with using these tools [17,15].For this reason, it is imperative to guarantee the accurate, transparent, and ethically sound deployment of these tools, especially for public consultation in general [16].</p>
<p>Models, such as ChatGPT, have generated immense interest due to their transformative potential in different sectors, such as administration, finance, health, and education [27].However, their integration has raised complex questions, particularly around authorship, plagiarism, and the distinction between human-and AI-generated content [27][28][29].One of the fundamental issues is whether AI systems should be credited as authors in academic writings [27,28,30].The distinction between human-written content and AI-generated content becomes more blurred, emphasizing concerns about plagiarism [27,28,31].AI models can generate seemingly genuine but potentially misleading scientific texts [32].In response, there is an emphasis on greater scrutiny, transparency, and ethical standards in using AI in research [31,32].In this way, it is necessary to achieve a balance between the advantages of AI and ethical considerations becomes paramount [27,29,30,33], which requires an emphasis on transparency, fairness, and initiatives of open source [32,33].</p>
<p>Discussion about prompt engineering</p>
<p>Prompt engineering is a set of techniques and methods to design, write, and optimize instructions for LLM, called prompts, such that the model answers will be precise, concrete, accurate, replicable, and factually correct [8,9,18].Prompts are understood as a form of programming because they can customize the outputs and interactions with an LLM [9].They involve adapting instructions in natural language, obtaining the desired responses, guaranteeing contextually accurate results, and increasing the usefulness of generative language models in various applications [13].Its applications include fields such as medical education, radiology, and science education [11,12,34].These systems can be used, for example, as virtual assistants for student care or report writing, transforming complex information into a coherent narrative [11,26,34].</p>
<p>Efforts are underway to standardize the terminology and concepts within prompt engineering, with various classifications of prompts emerging based on different criteria.</p>
<p>According to the structure, prompts can be formulated using open-ended or closed-ended questions.Open-ended questions do not have a specific or limited answer and allow for a more extensive and detailed response from the model.They are helpful, for example, for critical reading tasks [35].</p>
<p>In contrast, closed-ended questions typically have specific and limited answers, often yes or no, multiple-choice, or a short and defined response.For example, instead of asking "What is the capital of Italy?" (close-ended question), an open-ended question might be, "Tell me about the history and culture of Rome."</p>
<p>According to the information provided, prompts can be categorized into levels 1 to 4. The first level consists of straightforward questions, while the second level introduces additional context about the writer and the language model.The third level includes provided examples for the language model to reference, and the fourth level allows the language model to break down the request into individual components (much like requesting a step-by-step solution to a mathematical problem, offering the language model a more structured way to handle the prompt for improved accuracy) [36].</p>
<p>Comparably, prompts have also been classified as instructive, system, question-answer, and mixed.Instructive prompts start with a verb that specifies the action to be performed by the system.System prompts provide the system with a starting point or context to develop content.Questionanswer prompts formulate a wh* type question.Mixed prompts blend two or more techniques mentioned above [8].</p>
<p>According to the number of examples provided, instructions are classified as zero-shot and few-shot prompts, where "shot" is equivalent to an example [36].Zero-shot prompts are used in situations where it is not necessary to train the LLM or present sample outputs [37].Examples of zero-shot prompts include prompts used to translate or summarize texts; other examples of zero-shot prompts are simple questions that are answered with the internal knowledge of the LLM, such as, for example, "define prompt engineering."Few-shot prompts cover prompts with more detailed information.</p>
<p>Reproducibility is a desired characteristic, but LLM produces an inherent random response due to its intrinsic design [7].</p>
<p>Many sources recognize that the development of prompts is an iterative process.Also, it is desired that the prompt text must be clear, concise, and to the point, avoiding unnecessary complexity [38].</p>
<p>Following the discussion, poorly designed prompts generate vague, biased, misleading, or ambiguous responses.Another major problem is hallucinations [5,9].Many researchers highlight the necessity of verifying facts presented in the response of conversational LLMs, such as academic citations.</p>
<p>Methodology</p>
<p>We conducted a comprehensive literature search using the Scopus database to identify scientific papers on prompt engineering.Scopus is renowned as one of the largest repositories of peer-reviewed scientific literature, and it encompasses a broad spectrum of disciplines, including science, technology, medicine, and social sciences [39].We designed and used the following search equation, which retrieved 184 documents.</p>
<p>TITLE ( ( prompt AND ChatGPT ) OR ( prompt AND engineering ) ) OR KEY ( ( prompt AND ChatGPT ) OR ( prompt AND engineering ) )</p>
<p>The analysis of the documents and the valuable findings for a prompt design methodology are presented below.</p>
<p>Results</p>
<p>Analysis</p>
<p>The majority of the literature found can be categorized into two groups: specific applications (particularly in the field of medicine) and guidelines and recommendations for prompt design [36] [38].Only seven papers go beyond prompt design to propose a methodology for interacting with Chang (2023) [35] Critical Reading X X X Eager and Brunton, 2023 [18] Education X X X X Giray, 2023 [8] Academic Writing X X Jha et al., 2023 [5] General X X Lo, 2023 [19] General X X X Shieh, 2023 [40] General X Source: The authors chatGPT using prompts.This situation can be attributed to the large number of documents and gray literature offering compilations of prompt examples and templates for specific tasks such as marketing, advertising, or text translation.These guides are designed for the non-technical user, and they play an essential role in popularizing ChatGPT and LLM, although they may not be part of the scientific literature.Table 1 presents the components of these methodologies and other pertinent sources within the gray literature.</p>
<p>Several methodologies have been developed for general applications (though they are typically published in medical field journals), while some have been proposed for specific domains.We did not find any methodologies proposed for the field of engineering.Most of the methodologies provide guidelines for prompt design and incorporate interactiveness.However, given the nature of the application field, some methodologies are based on specific prompt designs, such as open-ended questions for critical reading [35] or persona design for academic writers [8].Only two methodologies include providing data within the prompt for the system's response retrieval.</p>
<p>While some methodologies involve evaluating the response before iterating on the prompt [5,18,19,35], not all of them provide components for conducting this assessment [5,18].</p>
<p>Even though some methodologies hold promise, they are not currently directly applicable to prompt design.For instance, in [5,41], a methodology is proposed that may be useful for the internal programming of LLMs but not for human-user interaction with such systems.Furthermore, in [42], a hermeneutic exercise is conducted without a proposed methodology that can be applied to other domains.</p>
<p>As of the publication date of this paper, we have not found official documentation from Google on recommendations for interacting with Bard (the Artificial Intelligence system developed by the company [43]).Similarly, we have not come across official documentation from Microsoft regarding recommendations for interacting with 12hatGPT through their Bing browser [44].</p>
<p>Proposal</p>
<p>We collected the guidelines, recommendations, and common elements from the various methodologies for prompt design that were analyzed earlier.Furthermore, we also considered issues related to hallucinations and lowquality responses and integrated those elements into a methodology for interacting with ChatGPT through prompts for the engineering field.</p>
<p>The methodology is called GPEI, which stands for Goal Prompt Evaluation Iteration.GPEI consists of 4 steps: (1) define a goal, (1) design the prompt, ( 3) evaluate the answer, and (4) iterate, as Figure 1 shows.The methodology is explained below.</p>
<p>Step 1: Define a goal</p>
<p>The process begins by defining the goal to be achieved by the AI model.The goal will determine the structure of the prompt to be designed in the following step and assist in evaluating the quality of the system's response before further iterations.Despite its significance, this activity is explicitly outlined only in one of the analyzed methodologies [18]; in the remaining methodologies, the objective is disaggregated within the prompt design.</p>
<p>Step 2: Prompt designing</p>
<p>The first step consists of the design of the prompt.In [9], a catalog of prompt patterns is presented and discussed.The authors describe 12 patterns for prompt designing; also, they identify for each pattern the intent, motivation, key ideas, and consequences of the approach.Five of these patterns are oriented toward customizing the output obtained from the system: output Automater, persona, visualization generator, recipe, and template.paragraph, a bulleted list, a table, JSON data, etc).â€¢ A description of limits for the expected results.</p>
<p>In an engineering context, we advise that the prompt includes the necessary data for the system to generate responses.An example is presented in Fig. 2.</p>
<p>The literature provides some recommendations for prompt design.</p>
<p>â€¢ Extending prompts with phrases such as "within the scope"</p>
<p>and "Let's think step by step â€¦ to reach conclusions" could improve the response of the system.â€¢ A strategy for complex responses involves asking LLM to break the result into small chunks [45].â€¢ Think that prompts are instructions in the context of computer programming, such that it is unnecessary to be polite; avoid phrases such as "Please, give me â€¦" [45].â€¢ Strategies, such as the Tree of Thoughts [41], can be used to structure prompts for complex problems.â€¢ Frameworks, such as CLEAR, propose a Concise, Logical, Explicit, Adaptive, and Reflective process to optimize interactions with AI language models like ChatGPT through prompts.â€¢ OpenAI suggests separating instructions from the context through a break in the prompt or using quotation marks to reference textual elements from the context [40].</p>
<p>â€¢ Adding examples to articulate the desired output could also be useful [40].</p>
<p>Moreover, the prompt can be used as a template for solving similar problems or generating new prompts for similar problems (template pattern [9]).Disambiguation can be avoided by providing a detailed description or scope of the problem or the data, as exemplified in Fig. 3.</p>
<p>A more comprehensive guide for developing template prompts can be found in [35].A very important recommendation is to use the designed prompt in the chat and then collect and save the system output.At this point, it is imperative to preserve the history of the process design to realize ex-post evaluations of the performance of the designed prompts.</p>
<p>Step 3: Evaluate the answer.</p>
<p>Realize a formal verification of the answer provided by ChatGPT in terms of the design criteria specified in Step 1. Evaluating the obtained response is not trivial since it can potentially reduce system hallucination.</p>
<p>The following questions could be helpful in this purpose [19]: Is the answer as complete as expected?Is the answer as accurate as expected?Is the answer as relevant as expected?</p>
<p>Were the specified limits met?Does the answer have elements that may be factually incorrect (hallucinations)?</p>
<p>Does the answer have elements that may be contextually inappropriate?</p>
<p>The available literature offers various methods for assessing ChatGPT's responses.For instance, one approach involves rephrasing a question to elicit different responses, which can help identify inconsistencies among multiple answers.Additionally, requesting additional evidence, such as querying top-k information sources and having the language model rate the credibility of each source, is another strategy.Also, one can seek opposing viewpoints from the language model, including their sources and credibility, to evaluate the strength of a different perspective [35].It is also possible.</p>
<p>In [5], formal methods are integrated into the design of prompts for critical and autonomous systems with the aim of self-monitoring and automatic detection of errors and hallucinations.Among the recommendations, the authors suggest that one could consider providing counterexamples in the prompt to prevent hallucinations [5].</p>
<p>Furthermore, it is possible to design other prompts to evaluate a response.For instance, prompts falling under the error identification category in [9] involve generating a list of facts the output depends on that should be fact-checked and then introspecting on its output to identify any errors.</p>
<p>A potentially useful strategy to evaluate the answer of an LLM is to incorporate elements commonly used to design Explainable AI systems (XAI) [46].We propose the following guidelines to incorporate these principles to evaluate the answer's quality: â€¢ Ask for the reasoning behind a particular answer.</p>
<p>â€¢ Verify that the prompt asking the LLM provides a simple, direct, and unambiguous response.â€¢ Verify the prompt requires the LLM to justify the answer.</p>
<p>â€¢ Ask the LLM to break down the answer in bullets, steps, or stages for complex answers.â€¢ Inquire about the data sources or training data.</p>
<p>Step 4: Iterate</p>
<p>If the answer fails to meet the evaluation criteria, prompt modification is required, which entails adjusting the design obtaining and assessing a new answer.This iterative refinement process continues until the system's response is deemed adequate.</p>
<p>As suggested in [35], the iterative process could be as simple as posing the same question from an opposing perspective.However, other strategies that use specific prompt patterns can also be helpful, like the patterns within the improvement group focus on enhancing the quality of both input and output.Patterns in this category involve suggesting alternative ways for the Language Model (LLM) to accomplish a user-specified task, instructing the LLM to automatically propose a series of subquestions for users to answer before combining these subquestion answers to produce a response to the overarching question (similar to [41]) and mandating the LLM to automatically rephrase user queries when it declines to provide an answer.</p>
<p>Moreover, the utilization of particular prompt patterns can prove advantageous.For example, patterns found within the Improvement category in [9] focus on enhancing input and output quality.These patterns suggest alternative methods for the LLM to execute a task specified by the user, directing the LLM to autonomously present a sequence of subquestions for users to address before amalgamating these subquestion responses to formulate a comprehensive answer to the primary inquiry.Additionally, they instruct the LLM to automatically rephrase user queries in cases where it refuses to produce a solution.</p>
<p>Application</p>
<p>For this example, we will seek the solution to a wellknown engineering problem: the knapsack problem [47].In this problem, one must decide which items to take on a trip based on the utility value of each item, the weight of each item, and a limit on the total weight that can be carried.</p>
<p>4.3.1.</p>
<p>Step 1: Define a goal.</p>
<p>From a given list of items, one should find the combination that has the highest total benefit without exceeding a total weight of 10 kg.</p>
<p>4.3.2.</p>
<p>Step 2: Design the prompt.</p>
<p>The designed prompt and the corresponding response are presented in Table 2.</p>
<p>Table 2. First iteration for the application case.</p>
<p>Role Dialogue</p>
<p>Prompt</p>
<p>Given the items below, choose the set of items that has the highest total value, as long as the sum of the weights of those items does not exceed 10 kilograms:</p>
<p>GPT</p>
<p>Given the items provided and the constraint that the total weight cannot exceed 10 kilograms, here are three sets with the highest total value that do not exceed the weight limit: These sets provide the highest total value without exceeding the 10 kilograms weight limit.Source: The authors.4.3.3.Step 3: Evaluate the answer.</p>
<p>The items meet the imposed restriction; however, when independently verified using an optimization tool, the following is the optimal solution:</p>
<p>â€¢ Item 1: Value -8, Weight -3 kg.</p>
<p>â€¢ Item 2: Value -6, Weight -2 kg.</p>
<p>â€¢ Item 4: Value -3, Weight -1 kg.</p>
<p>â€¢ Item 6: Value -9, Weight -3 kg.</p>
<p>â€¢ Item 8: Value -4, Weight -1 kg.</p>
<p>â€¢ Total value: 30.</p>
<p>â€¢ Total weight: 10 kg Let's see if we can find a better solution than the one found in iteration 1 using ChatGPT.</p>
<p>Step 4: Iterate</p>
<p>A new designed prompt, presented int Table 3, is used for the second iteration.Although a better answer was achieved in this iteration than the previous one, ChatGPT still does not find the system's optimum.Other strategies will be employed in iterations 3 and 4 to aid in solving this problem.</p>
<p>For the iteration 3, the strategy is changed from asking for the solution to requesting the formulation of the problem (See Table 4).In the iteration 4, we designed a prompt to request the generation of the code necessary for Google Collaboratory [48] to solve the previous problem.The dialogue is presented in Table 5.Table 4. Iteration 3 for the application case.</p>
<p>Role Dialogue</p>
<p>Prompt</p>
<p>Formulate the objective function and the correspondent constraints for the following problem:</p>
<p>Given a number of items, each characterized by a value and a weight in kilograms, find the set of items where the sum of their values is maximum, and the sum of their weights is less than or equal to 10 kilograms.The sets can have any number of elements, as long as they are not repeated.</p>
<p>GPT This is a classic Knapsack problem.</p>
<p>Objective function:</p>
<p>Maximize  where:
ð‘ð‘ = ï¿½ ð‘¤ð‘¤ ð‘–ð‘– ð‘¥ð‘¥ ð‘–ð‘– ð‘›ð‘› ð‘–ð‘–=1 â‰¤10
This objective function represents the total value of the selected items.</p>
<p>Constraints:</p>
<p>1.The total weight of the selected items should not exceed 10 kg:
ï¿½ ð‘¤ð‘¤ ð‘–ð‘– ð‘¥ð‘¥ ð‘–ð‘– ð‘›ð‘› ð‘–ð‘–=1 â‰¤10
2. Decision variable constraints:
0 â‰¤ ð‘¥ð‘¥ ð‘–ð‘– â‰¤ 1
and   must be an integer (binary)</p>
<p>The objective function aims to maximize the total value of selected items, subject to the constraints that the total weight does not exceed 10 kilograms and each item is either selected or not and not repeated.</p>
<p>Source: The authors.Which is the correct solution to the problem.So here, we halted the process.</p>
<p>Conclusions</p>
<p>Prompt engineering plays a pivotal role in optimizing the performance of LLM by crafting instructions or prompts that elicit precise, accurate, and contextually appropriate responses.Designing effective prompts is iterative and requires clear and concise language to avoid generating vague or biased responses.</p>
<p>A literature analysis found that multiple methodologies for prompt engineering have been developed.Notably, no methodologies were found specifically designed for engineering.Most of these methodologies offer guidance for prompt design and emphasize iterative processes.Only two methodologies include data within the prompt to facilitate system response retrieval.While some methodologies involve response evaluation before iterating on the prompt, not all of them provide components for this assessment.</p>
<p>We propose an iterative methodology for optimizing interactions with AI language models in engineering through prompts named GPEI.It is a four-step process, including defining a goal, designing the prompt, evaluating the answer, and iterating to achieve an adequate response.GPEI has two key elements: the inclusion of data in prompt design, making it suitable for applications in the field of engineering, and the inclusion of principles from Explainable AI (XAI) systems to evaluate answers is proposed, promoting transparency and justifiability in the responses generated by LLM.</p>
<p>Our methodology integrates guidelines, recommendations, and common elements from various methodologies to address issues like hallucinations and low-quality responses.The iterative nature of prompt refinement is emphasized, with suggestions such as asking opposing questions and using specific prompt patterns for improvement.This methodology is a valuable tool for designing prompts in engineering.</p>
<p>The application example showcased the capabilities of chatGPT in addressing engineering problems when integrated with other calculation tools.Future work stemming from this research is related to applying the methodology in various engineering applications to incorporate the necessary enhancements for improving its utility.</p>
<p>Figure 1 .
1
Figure 1.GPEI Methodology Source: The authors.</p>
<p>Figure 2 .
2
Figure 2. Example of a prompt template for including specific information.Source: The authors.</p>
<p>Figure 3 .
3
Figure 3. Prompt example for including specific information.Source: The authors.</p>
<p>Item 1: Value -8, Weight -3 kg; Item 2: Value -6, Weight -2 kg; Item 3: Value -10, Weight -5 kg; Item 4: Value -3, Weight -1 kg; Item 5: Value -7, Weight -4 kg; Item 6: Value -9, Weight -3 kg; Item 7: Value -5, Weight -2 kg; Item 8: Value -4, Weight -1 kg; Item 9: Value -2, Weight -6 kg; Item 10: Value -10, weight -10 kg GPT Given the items provided, the optimal selection to achieve the highest total value without exceeding 10 kilograms would be: * Item 1: Value -8, Weight -3 kg * Item 2: Value -6, Weight -2 kg * Item 3: Value -10, Weight -5 kg * Total value: 24; Total weight: 10 kg Source: The authors.</p>
<p>Set 1 :
1
* Item 3: Value -10, Weight -5 kg * Item 6: Value -9, Weight -3 kg.* Item 1: Value -8, Weight -3 kg.Total Value: 27 Total Weight: 11 kg (exceeds weight limit) Set 2: * Item 3: Value -10, Weight -5 kg * Item 2: Value -6, Weight -2 kg * Item 1: Value -8, Weight -3 kg Total Value: 24 Total Weight: 10 kg (within weight limit); Set 3: * Item 3: Value -10, Weight -5 kg * Item 6: Value -9, Weight -3 kg * Item 2: Value -6, Weight -2 kg Total Value: 25 Total Weight: 10 kg (within weight limit).</p>
<p>Source:</p>
<p>The authors.When we ran the code in Google Colab, we obtained this response: Requirement already satisfied: pulp in /usr/local/lib/python3.10/dist-packages (2.7.0)Selected items: Item 1: Value -8, Weight -3 kg; Item 2: Value -6, Weight -2 kg; Item 4: Value -3, Weight -1 kg; Item 6: Value -9, Weight -3 kg; Item 8: Value -4, weight -1 kg; Total Value: 30; Total Weight: 10 kg</p>
<p>Table 1 .
1
Components of methodologies.
AuthorsApplicationGuidelinesDataEvaluation CriteriaIteration</p>
<p>Table 3 .
3
Second iteration for the application case.
RoleDialogueGiven the items below, give me three sets with the highest totalvalue. The sets can have any number of elements as long as theyare not repeated, and the sum of the weights of those items doesnot exceed 10 kilograms. Show only sets that do not exceed 10kilograms: Item 1: Value -8, Weight -3 kg; Item 2: Value -6,PromptWeight -2 kg; Item 3: Value -10, Weight -5 kg; Item 4: Value -3, Weight -1 kg; Item 5: Value -7, Weight -4 kg; Item 6: Value -9, Weight -3 kg; Item 7: Value -5, Weight -2 kg; Item 8: Value -4, Weight -1 kg; Item 9: Value -2, Weight -6 kg; Item 10: Value-10, weight -10 kg; Your best answer has been: Item 1: Value -8, Weight -3 kg; Item 2: Value -6, Weight -2 kg; Item 3: Value -10, weight -5 kg</p>
<p>Table 5 .
5
Iteration 4for the application case.
RoleDialogueGenerate a code to solve this problem for Google Colab, Usethe following items for the code: Item 1: Value -8, Weight -3 kg; Item 2: Value -6, Weight -2 kg; Item 3: Value -10,PromptWeight -5 kg; Item 4: Value -3, Weight -1 kg; Item 5: Value -7, Weight -4 kg; Item 6: Value -9, Weight -3 kg;Item 7: Value -5, Weight -2 kg; Item 8: Value -4, Weight -1 kg; Item 9: Value -2, Weight -6 kg; Item 10: Value -10,weight -10 kg</p>
<p>ChatGPT and a new academic reality: Artificial Intelligence-written research papers and the ethics of the large language models in scholarly publishing. B D Lund, T Wang, N R Mannuru, B Nie, S Shimray, Z Wang, 10.1002/asi.24750J. Assoc. Inf. Sci. Technol. 7452023</p>
<p>Can ChatGPT draft a research article? An example of population-level vaccine effectiveness analysis. C Macdonald, D Adeloye, A Sheikh, I Rudan, 10.7189/JOGH.13.01003J. Glob. Health. 132023</p>
<p>ChatGPT: bullshit spewer or the end of traditional assessments in higher education?. J Rudolph, S Tan, S Tan, 10.37074/jalt.2023.6.1.9J. Appl. Learn. Teach. 612023</p>
<p>Collaborating with ChatGPT: considering the implications of generative artificial intelligence for journalism and media education. J V Pavlik, 10.1177/10776958221149577Journal. Mass Commun. Educ. 7812023</p>
<p>Dehallucinating large language models using formal methods guided iterative prompting. S Jha, S K Jha, P Lincoln, N D Bastian, A Velasquez, S Neema, 2023 IEEE International Conference on Assured Autonomy (ICAA). IEEE2023</p>
<p>Pretrain, prompt, and predict: a systematic survey of prompting methods in natural language processing. P Liu, W Yuan, J Fu, Z Jiang, H Hayashi, G Neubig, ACM Comput. Surv. 5592023</p>
<p>The art and science of prompt engineering: a new literacy in the information age. L S Lo, 10.1080/10875301.2023.2227621Internet Ref. Serv. Q. 2023</p>
<p>Prompt engineering with ChatGPT: a guide for academic writers. L Giray, 10.1007/s10439-023-03272-4Ann. Biomed. Eng. 2023</p>
<p>A prompt pattern catalog to enhance prompt engineering with chatgpt, ArXiv Prepr. J White, ArXiv230211382, 2023</p>
<p>Just tell me: prompt engineering in business process management. K Busch, A Rochlitzer, D Sola, Leopold , H , International Conference on Business Process Modeling, Development and Support. Springer2023</p>
<p>Revolutionizing radiologand with GPT-based models: current applications, future possibilities and limitations of ChatGPT. A Lecler, L Duron, P Soander, 10.1016/j.diii.2023.02.003Diagn. Interv. Imaging. 10462023</p>
<p>The role of ChatGPT, generative language models, and artificial intelligence in medical education: a conversation with ChatGPT and a call for papers. G Eandsenbach, 10.2196/46885JMIR Med. Educ. 92023</p>
<p>AI chains: transparent and controllable Human-AI interaction band chaining large language model prompts. T Wu, M Terrand, C J Cai, 10.1145/3491102.3517582Conference on Human Factors in Computing Sandstems -Proceedings. 2022</p>
<p>Assessing the Accuracand of responses band the language model ChatGPT to questions regarding bariatric surgerand: a critical appraisal. P P Raand, P Majumder, 10.1007/s11695-023-06664-6Obes. Surg. 3382023</p>
<p>Utilization of ChatGPT for plastic surgerand research: friend or foe?. R Gupta, I Herzog, J Weisberger, J Chao, K Chaiandasate, E S Lee, 10.1016/j.bjps.2023.03.004J. Plast. Reconstr. Aesthet. Surg. 802023</p>
<p>Artificial intelligence and public health: evaluating ChatGPT responses to vaccination mandths and misconceptions. G Deiana, M Dettori, A Arghittu, A Azara, G Gabutti, P Castiglia, 10.3390/vaccines11071217Vaccines. 117art. 11071217, 2023. DOI</p>
<p>Maand artificial intelligence influence future pediatric research?-The case of ChatGPT. A Corsello, A Santangelo, 10.3390/children10040757children. 1042023</p>
<p>Prompting higher education towards AI-Augmented teaching and learning practice. B Eager, R Brunton, 10.53761/1.20.5.02J. Univ. Teach. Learn. Pract. 2052023</p>
<p>The CLEAR path: a framework for enhancing information literacand through prompt engineering. L S Lo, 10.1016/j.acalib.2023.102720J. Acad. Librariansh. 4942023</p>
<p>So what if ChatGPT wrote it? Multidisciplinarand perspectives on opportunities, challenges and implications of generative conversational AI for research, practice and policand. Dk, Dwivedi, 10.1016/j.ijinfomgt.2023.102642Int. J. Inf. Manag. 712023</p>
<p>Attention is not all andou need: the complicated case of ethicalland using large language models in healthcare and medicine, eBioMedicine, 90. S Harrer, 10.1016/j.ebiom.2023.1045122023</p>
<p>Leadership is needed for ethical ChatGPT: character, assessment, and learning using artificial intelligence (AI). J Crawford, M Cowling, K.-A Allen, 10.53761/1.20.3.02J. Univ. Teach. Learn. Pract. 2032023</p>
<p>Who wrote this essaand? Detecting ai-generated writing in second language education in higher education. K Alexander, C Savvidou, C Alexander, 10.56297/BUKA4060/XHLD5365Teach. Engl. Technol. 2322023</p>
<p>Generative AI and the future of education: RagnarÃ¶k or reformation? A paradoxical perspective from management educators. W M Lim, A Gunasekara, J L Pallant, J I Pallant, E Pechenkina, 10.1016/j.ijme.2023.100790Int. J. Manag. Educ. 2122023</p>
<p>Enhancing awareness and Self-diagnosis of obstructive sleep apnea using AI-Powered chatbots: the role of ChatGPT in revolutionizing healthcare. M Bilal, Y Jamil, D Rana, H H Shah, 10.1007/s10439-023-03298-8Ann. Biomed. Eng. 2023</p>
<p>Variabilitand in large language Models' responses to medical licensing and certification examinations. comment on "How Does ChatGPT Perform on the United States Medical Licensing Examination? The Implications of Large Language Models for Medical Education and Knowledge Assessment. R H Epstein, F Dexter, 10.2196/48305JMIR Med. Educ. 92023</p>
<p>ChatGPT: the next frontier in academic writing for cardiologists or a pandora's box of ethical dilemmas. B Marchandot, K Matsushita, A Carmona, A Trimaille, O Morel, 10.1093/ehjopen/oead007Eur. Heart J. Open. 322023</p>
<p>A Graf, R E Bernardi, 10.1016/j.neuroscience.2023.02.008ChatGPT in research: balancing ethics, transparencand and advancement. 2023515</p>
<p>Impact of ChatGPT on learners in a L2 writing practicum: an exploratorand investigation. D Yan, 10.1007/s10639-023-11742-4Educ. Inf. Technol. 2023</p>
<p>Using ChatGPT in Medical research: current status and future directions. S Ruksakulpiwat, A Kumar, A Ajibade, 10.2147/JMDH.S413470J. Multidiscip. Healthc. 162023</p>
<p>Artificial intelligence can generate fraudulent but authentic-looking scientific medical articles: pandora's box has been opened. M MÃ¡jovskÃ½, M ÄŒernÃ½, M Kasal, M Komarc, D Netuka, 10.2196/46924J. Med. Internet Res. 252023</p>
<p>Whand open-source generative AI models are an ethical waand forward for science. A Spirling, 10.1038/d41586-023-01295-4nature. 6167957art. 413, 2023. DOI:</p>
<p>OpenAI -explain whand some countries are excluded from ChatGPT. S H Wang, 10.1038/d41586-023-00553-9nature. 7950615art. 34, 2023. DOI:</p>
<p>Examining science education in ChatGPT: An exploratorand studand of generative artificial intelligence. G Cooper, 10.1007/s10956-023-10039-andJ. Sci. Educ. Technol. 3232023</p>
<p>Prompting large language models with the socratic method. E Y Chang, 10.1109/CCWC57344.2023.100991792023 IEEE 13 th Annual Computing and Communication Workshop and Conference, CCWC 2023. 2023</p>
<p>Prompt engineering in medical education. T F Heston, C Khun, Int. Med. Educ. 232023</p>
<p>Prompt engineering for zeroshot and few-shot defect detection and classification using a visuallanguage pretrained model. G Yong, K Jeon, D Gil, G Lee, 10.1111/mice.12954Comput.-Aided Civ. Infrastruct. Eng. 38112023</p>
<p>Generative AI and prompt engineering: the art of whispering to let the genie out of the algorithmic world. A Bozkurt, R C Sharma, Asian J. Distance Educ. 2023</p>
<p>Best practices for prompt engineering with OpenAI API, OpenIA. J Shieh, </p>
<p>Tree of thoughts: deliberate problem solving with large language models. S Yao, ArXiv Prepr. ArXiv230510601, 2023</p>
<p>Prompting meaning: a hermeneutic approach to optimising prompt engineering with ChatGPT. L Henrickson, A MeroÃ±o-PeÃ±uela, 10.1007/s00146-023-01752-82023AI Soc</p>
<p>Bard -Chat based AI tool from Google, powered band PaLM 2. Google, October 4 th of 2023</p>
<p>Your AI-Powered Copilot for the Web | Microsoft Bing. October 4 th of 2023Microsoft</p>
<p>Using ChatGPT standard prompt engineering techniques in lesson preparation: role, instructions and seed-word prompts. A J Spasic, D S Jankovic, 10.1109/ICEST58410.2023.101872692023 58 th International Scientific Conference on Information, Communication and Energand Sandstems and Technologies, ICEST 2023 -Proceedings. 2023</p>
<p>Whand are we using black box models in AI when we don't need to? A lesson from an explainable AI competition. C Rudin, J Radin, 10.1162/99608f92.5a8a3a3dHarv. Data Sci. Rev. 1292019</p>
<p>The knapsack problem: a surveand. H M Salkin, C A De Kluandver, 10.1002/nav.3800220110Nav. Res. Logist. Q. 2211975</p>
<p>E Bisong, Google Colaboratorand, 10.1007/978-1-4842-4470-8_7en Building Machine Learning and Deep Learning Models on Google Cloud Platform: A Comprehensive Guide for Beginners, E. Bisong. Berkeleand, CAApress2019</p>
<p>His research and publications span simulation, modeling, optimization, and forecasting in energy markets. He specializes in nonlinear time-series analysis and forecasting using statistical and computational intelligence techniques, numerical optimization with metaheuristics, and analytics and data science. He currently instructs postgraduate courses in data science, machine learning. J , ORCID: 0000-0003-3043-3037he led the Computing and Decision Science Department at the Facultad de Minas. Colombia; Medellin Campus1994 to 1999. 2000. 2004 and 2006. 2009. 2018all from the Universidad Nacional de Colombia in Medellin ; Universidad Nacional de Colombiahe joined the Universidad Nacional de Colombia in Medellin and was appointed as a Full Professor of Computer Science by 2012. and big data in the Analytics program, emphasizing Python programming</p>
<p>Franco is a Full Professor at the Universidad Nacional de Colombia. C J , ORCID: 0000-0002-7750-857XHe earned his MSc. in 1996 and his PhD in 2002, both from the Universidad Nacional de Colombia. Currently, his research interests include energy markets, system dynamics, and complexity. MedellÃ­n Campus, MedellÃ­n, Colombia</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2584 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2584</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2584</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-67.html">extraction-schema-67</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <p><strong>Paper ID:</strong> paper-0c8413ab8de0c1b8f2e86402b8d737d94371610f</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/0c8413ab8de0c1b8f2e86402b8d737d94371610f" target="_blank">Information-Theoretic Regret Bounds for Gaussian Process Optimization in the Bandit Setting</a></p>
                <p><strong>Paper Venue:</strong> IEEE Transactions on Information Theory</p>
                <p><strong>Paper TL;DR:</strong> This work analyzes an intuitive Gaussian process upper confidence bound algorithm, and bound its cumulative regret in terms of maximal in- formation gain, establishing a novel connection between GP optimization and experimental design and obtaining explicit sublinear regret bounds for many commonly used covariance functions.</p>
                <p><strong>Paper Abstract:</strong> Many applications require optimizing an unknown, noisy function that is expensive to evaluate. We formalize this task as a multiarmed bandit problem, where the payoff function is either sampled from a Gaussian process (GP) or has low norm in a reproducing kernel Hilbert space. We resolve the important open problem of deriving regret bounds for this setting, which imply novel convergence rates for GP optimization. We analyze an intuitive Gaussian process upper confidence bound (GP-UCB) algorithm, and bound its cumulative regret in terms of maximal in- formation gain, establishing a novel connection between GP optimization and experimental design. Moreover, by bounding the latter in terms of operator spectra, we obtain explicit sublinear regret bounds for many commonly used covariance functions. In some important cases, our bounds have surprisingly weak dependence on the dimensionality. In our experiments on real sensor data, GP-UCB compares favorably with other heuristical GP optimization approaches.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2584.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2584.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GP-UCB</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gaussian Process Upper Confidence Bound</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sequential Bayesian optimization algorithm that selects queries by maximizing a posterior upper confidence bound μ_{t-1}(x) + sqrt(β_t) σ_{t-1}(x), trading off exploitation (high posterior mean) and exploration (high posterior variance). The paper provides the first sublinear cumulative-regret bounds for GP-UCB in nonparametric GP and RKHS settings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GP-UCB (Gaussian Process Upper Confidence Bound)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>At each round t the algorithm computes the GP posterior mean μ_{t-1}(x) and standard deviation σ_{t-1}(x) given previous noisy evaluations, and selects x_t = argmax_x [ μ_{t-1}(x) + sqrt(β_t) σ_{t-1}(x) ]. β_t is a time-varying confidence parameter chosen to ensure high-probability coverage of the posterior. After sampling y_t = f(x_t) + ε_t the GP posterior is updated analytically. Theoretical analysis reduces cumulative regret bounds to quantities involving the maximum information gain γ_T (mutual information between f and T observations) and the chosen β_T.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Black-box expensive function optimization / experimental design; demonstrated on sensor-network spatial temperature and traffic-speed data; applicable generally to experimental science problems where evaluations are costly.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocates the fixed evaluation budget sequentially by selecting points maximizing an upper-confidence bound combining expected reward (posterior mean) and uncertainty (posterior standard deviation). The allocation implicitly balances sampling promising hypotheses (exploitation) and informative/diverse hypotheses (exploration) via the σ term.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Number of expensive function evaluations (T) is the primary cost unit; additional computational cost includes GP posterior updates and solving argmax over D (often dominated by evaluation cost). The theoretical costs are expressed asymptotically in T (number of queries).</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Mutual information I(y_A; f) between observations and the function; the cumulative-regret bounds are expressed in terms of γ_T = max_{|A|=T} I(y_A; f).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Explicit UCB rule μ + sqrt(β) σ: the posterior mean term drives exploitation toward high expected reward, the posterior standard deviation term drives exploration to reduce uncertainty. β_t controls the strength of exploration and is set to guarantee probabilistic confidence bounds.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Not an explicit diversity-regularizer, but the σ-term encourages sampling in regions of high posterior uncertainty which, together with the information-gain analysis and submodularity arguments, promotes coverage/diverse sampling across the domain.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed number of experiments / function evaluations (T).</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Algorithm operates under a pre-specified horizon T; theoretical analysis gives regret bounds as functions of T and γ_T. For experimental-design style allocation (information-maximization) the greedy rule approximates the best T-point selection up to constant factor 1-1/e.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>No separate 'breakthrough' score; high-impact discoveries correspond to finding near-optimal/high-reward inputs. Performance measured via cumulative regret R_T and the best-observed function value max_{t ≤ T} f(x_t) relative to f(x^*).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Theoretical cumulative-regret bounds: R_T = O^*( sqrt(T β_T γ_T) ). Kernel-specific asymptotics (up to polylog factors): linear kernel R_T = O(d sqrt(T)); squared-exponential (RBF) R_T = O( sqrt{T (log T)^{d+1}} ); Matérn kernels R_T = O( T^{(ν + d(d+1) / 2) / (2ν + d(d+1))} ) as given in the paper (Figure 1). Empirical comparisons: GP-UCB 'compares favorably' with EI and MPI on sensor and synthetic tasks (no single numeric improvement universally reported).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Expected Improvement (EI), Most Probable Improvement (MPI), greedy mean-only selection, variance-only selection, and other heuristic Bayesian optimization methods including EGO variants.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Empirically GP-UCB performs at least on par with EI and MPI on the tested synthetic and sensor datasets; on real temperature data GP-UCB and EI outperform naive alternatives, while on some datasets MPI performs similarly to GP-UCB. No consistent percentage gains are given; main claims are favorable qualitative comparisons backed by plotted average-regret curves.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Theoretical sample-efficiency: sublinear regret implies average per-step regret → 0 as T grows, so fewer evaluations are asymptotically needed to approach optimum compared to naive strategies; kernel-dependent rates show substantial efficiency gains for smooth kernels (e.g., RBF kernel yields only polylog dependence on dimension in the bound). No explicit finite-sample percent reductions reported.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Provides formal tradeoff analysis: regret bounds depend on exploration parameter β_T, horizon T, and information gain γ_T. γ_T captures how quickly sampling reduces uncertainty (depends on kernel spectrum); smoother kernels (faster eigenvalue decay) yield smaller γ_T and hence lower regret, formalizing the tradeoff between model smoothness, information gathered, and sampling cost. The paper also contrasts pure experimental-design (global info maximization) versus optimization-focused allocation (GP-UCB) and shows that ED can be wasteful for optimization despite maximizing information.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Key theoretical findings: (1) Greedy maximization of mutual information (experimental design rule) yields a (1-1/e)-approximation to the optimal T-point information gain due to submodularity. (2) GP-UCB achieves cumulative-regret bounds scaling as O^*(sqrt(T β_T γ_T)), tying optimization performance to information-theoretic experimental design quantities. (3) Information gain γ_T can be bounded by kernel operator spectra; fast spectral decay (e.g., squared-exponential) implies small γ_T and hence sample-efficient allocation. Practical recommendation: use GP-UCB with kernel choice reflecting smoothness to optimize resources; for pure information collection use greedy info-maximization but not if goal is optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Information-Theoretic Regret Bounds for Gaussian Process Optimization in the Bandit Setting', 'publication_date_yy_mm': '2009-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2584.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2584.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Greedy-ED (info-max)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Greedy Experimental Design via Information Gain</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sequential sampling rule that greedily selects points to maximize the marginal information gain (mutual information) about the unknown function; equivalent to selecting points with maximal posterior standard deviation σ_{t-1}(x) in the GP setting. Submodularity gives (1-1/e) approximation guarantees to the optimal T-point design.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Near-optimal nonmyopic value of information in graphical models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Greedy information-maximizing experimental design</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>At each of T rounds, select x_t = argmax_x I(y_{A_{t-1} ∪ {x}}; f) which in the GP/noiseless marginal case is equivalent to selecting the point with largest posterior predictive variance σ_{t-1}(x). The procedure requires only kernel evaluations and does not depend on observed y_t values (it is a nonadaptive greedy design when used for planning), and by submodularity guarantees achieves at least (1-1/e) of the optimal information gain for |A|=T.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Bayesian experimental design / active learning for function estimation; applicable to sensor placement, spatial sampling, model learning where global information coverage is desired.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocate a fixed budget of T samples by greedy sequential maximization of marginal mutual information; the allocation focuses on covering directions (eigenvectors) of the kernel covariance with largest remaining unexplained variance, effectively allocating samples to leading eigenfunctions until diminishing returns.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Number of planned evaluations (T) and cost of computing posterior variances/ information gains; in practice dominated by cost of computing kernel variances and evaluating the greedy argmax each round (matrix operations, O(n^3) for naïve GP updates but can use approximations).</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Mutual information I(y_A; f) = (1/2) log |I + σ^{-2} K_A|; greedy marginal criterion is σ_{t-1}(x) or the incremental mutual information.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Pure exploration focused on reducing uncertainty globally; does not directly exploit high objective values because selection depends only on information gain (or σ), not on observed rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Submodularity of mutual information inherently encourages diversity: marginal gains diminish when sampling near previously sampled directions, so the greedy rule spreads samples to cover diverse informative regions/eigenfunctions.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed number of experiments (cardinality constraint |A| ≤ T).</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Directly optimizes mutual information under the cardinality budget using the greedy algorithm which attains a (1-1/e) approximation to the optimum.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Not designed to directly detect breakthroughs; focuses on global uncertainty reduction. Breakthroughs must be inferred downstream from improved models (e.g., discovering high-value x via subsequent optimization).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Theoretical guarantee: greedy design yields F(A_T) ≥ (1-1/e) max_{|A|≤T} F(A) where F is mutual information; information gain bounds γ_T used in regret analysis. Empirical performance not separately quantified in this paper beyond its role in bounding γ_T.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Optimal combinatorial design (intractable), random sampling, and GP-UCB (different objective).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Greedy-ED achieves provable constant-factor approximation to the optimal information-maximizing selection; however, it can be wasteful for optimization objectives because it ignores reward values and focuses on global uncertainty reduction.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Provides near-optimal information capture per sample (within factor 1-1/e of best possible for the same budget), which can significantly reduce the number of samples required for a given mutual-information target compared to naive or random sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper contrasts information-maximization (Greedy-ED) with optimization goals: ED is provably near-optimal for global learning but may allocate resources away from high-value regions needed for optimization. The authors incorporate γ_T (max information gain) into regret bounds for optimization algorithms to quantify this tradeoff.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Greedy information-maximization is near-optimal for the information objective under a cardinality budget due to submodularity; for optimization tasks, information gain (γ_T) is a key quantity that determines attainable regret, and kernel spectral properties determine how information should be allocated.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Information-Theoretic Regret Bounds for Gaussian Process Optimization in the Bandit Setting', 'publication_date_yy_mm': '2009-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2584.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2584.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EI / MPI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Expected Improvement / Most Probable Improvement</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Heuristic Bayesian optimization acquisition functions that trade off exploration and exploitation by estimating the expected or most probable improvement over the current best observation under a GP posterior; widely used in practice but lacked the formal regret guarantees provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Bayesian Approach to Global Optimization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Expected Improvement (EI) / Most Probable Improvement (MPI)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>EI selects the point maximizing the expected positive difference between f(x) and the current best observed value under the GP posterior; MPI selects the point with highest probability of improvement. Both combine mean and variance information but in different utility forms compared to the UCB rule.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Bayesian optimization for expensive black-box functions, hyperparameter tuning, engineering design.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Sequential allocation via acquisition function optimization: at each round, select x_t maximizing EI(x) or MPI(x). These criteria balance sampling where the posterior predicts high potential improvement or where there is significant chance of improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Number of function evaluations (T) and cost of acquisition-function optimization per round; no explicit computational cost/information tradeoff analysis in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Not explicitly mutual information; based on expected utility (improvement) and posterior predictive distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Mechanisms embedded in the acquisition functions: EI and MPI implicitly trade exploration and exploitation through the distributional tail mass relative to current best, but without explicit information-theoretic guarantees.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>No explicit diversity-promotion mechanism beyond the implicit effect of posterior variance in the acquisition function.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed evaluation budget T (implicit in sequential procedure).</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Greedy sequential acquisition until budget exhausted; no formal guarantee tied to a budget provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Improvement over current best observation (absolute value difference); emphasis on immediate potential gains rather than information content.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Empirically compared against GP-UCB in experiments: EI performed comparably to GP-UCB on several datasets (temperature data), while MPI performed similarly on some synthetic/traffic datasets. No universal numeric superiority reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against GP-UCB, mean-only, variance-only selection in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>On temperature data EI and GP-UCB outperformed other simple baselines; on other datasets MPI performed similarly to GP-UCB. The paper does not provide statistically summarized numerical improvement percentages.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Heuristically efficient in many applications; lacks the formal sample-complexity/regret bounds derived for GP-UCB in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper discusses that these heuristics have been successful empirically but lack formal regret guarantees; contrasts them with GP-UCB which has provable sublinear regret tied to information gain.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Not the focus of this paper; EI/MPI are presented as practical baselines without formal optimality guarantees in the bandit/regret sense discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Information-Theoretic Regret Bounds for Gaussian Process Optimization in the Bandit Setting', 'publication_date_yy_mm': '2009-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2584.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2584.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EGO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Efficient Global Optimization (EGO)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A global optimization framework for expensive black-box functions using a surrogate model (often kriging/Gaussian processes) and acquisition functions (like expected improvement) to guide sampling; historically influential in engineering design.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Efficient global optimization of expensive black-box functions</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Efficient Global Optimization (EGO)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Builds a GP (kriging) surrogate of the objective, then sequentially optimizes an acquisition function (commonly Expected Improvement) to choose new sample points; surrogate updated after each evaluation. Designed for global optimization under tight evaluation budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Global optimization of expensive simulators and black-box functions in engineering and design.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Sequentially select points that maximize an acquisition function (e.g., EI) to efficiently search the domain under a limited evaluation budget.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Main cost metric is number of expensive function evaluations; also surrogate fitting and acquisition optimization per round.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Implicit (via acquisition functions) rather than explicit mutual information maximization.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Acquisition functions like EI trade exploration and exploitation by quantifying expected benefit of sampling at candidate points relative to the current best.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>No explicit diversity enforcement; diversity arises implicitly through acquisition functions and surrogate uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed evaluation budget or user-determined stopping criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Sequential greedy acquisition until budget exhausted; no provable global approximation guarantees provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Improvement over current best observed value, as estimated by the acquisition function.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Known practical success across many engineering applications; in this paper EGO-style methods (EI) are used as baselines and found empirically comparable to GP-UCB in several tests.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared empirically to GP-UCB and other heuristics in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>EI (EGO-style) performs comparably to GP-UCB in the paper's experiments on temperature data; specifics depend on dataset and tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Heuristic gains in practice; formal regret/approximation bounds are not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper contrasts EGO/heuristics with GP-UCB and emphasizes that GP-UCB provides provable regret bounds connected to information gain while EGO heuristics lack such theoretical guarantees.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>EGO is effective in practice for low-budget black-box optimization but the paper recommends GP-UCB when theoretical guarantees and information-theoretic analysis of allocation are desired.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Information-Theoretic Regret Bounds for Gaussian Process Optimization in the Bandit Setting', 'publication_date_yy_mm': '2009-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Near-optimal nonmyopic value of information in graphical models <em>(Rating: 2)</em></li>
                <li>Bayesian Approach to Global Optimization <em>(Rating: 2)</em></li>
                <li>Efficient global optimization of expensive black-box functions <em>(Rating: 2)</em></li>
                <li>Bayesian experimental design: A review <em>(Rating: 2)</em></li>
                <li>A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning <em>(Rating: 2)</em></li>
                <li>Regret bounds for gaussian process bandit problems <em>(Rating: 2)</em></li>
                <li>Automatic gait optimization with Gaussian process regression <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2584",
    "paper_id": "paper-0c8413ab8de0c1b8f2e86402b8d737d94371610f",
    "extraction_schema_id": "extraction-schema-67",
    "extracted_data": [
        {
            "name_short": "GP-UCB",
            "name_full": "Gaussian Process Upper Confidence Bound",
            "brief_description": "A sequential Bayesian optimization algorithm that selects queries by maximizing a posterior upper confidence bound μ_{t-1}(x) + sqrt(β_t) σ_{t-1}(x), trading off exploitation (high posterior mean) and exploration (high posterior variance). The paper provides the first sublinear cumulative-regret bounds for GP-UCB in nonparametric GP and RKHS settings.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "GP-UCB (Gaussian Process Upper Confidence Bound)",
            "system_description": "At each round t the algorithm computes the GP posterior mean μ_{t-1}(x) and standard deviation σ_{t-1}(x) given previous noisy evaluations, and selects x_t = argmax_x [ μ_{t-1}(x) + sqrt(β_t) σ_{t-1}(x) ]. β_t is a time-varying confidence parameter chosen to ensure high-probability coverage of the posterior. After sampling y_t = f(x_t) + ε_t the GP posterior is updated analytically. Theoretical analysis reduces cumulative regret bounds to quantities involving the maximum information gain γ_T (mutual information between f and T observations) and the chosen β_T.",
            "application_domain": "Black-box expensive function optimization / experimental design; demonstrated on sensor-network spatial temperature and traffic-speed data; applicable generally to experimental science problems where evaluations are costly.",
            "resource_allocation_strategy": "Allocates the fixed evaluation budget sequentially by selecting points maximizing an upper-confidence bound combining expected reward (posterior mean) and uncertainty (posterior standard deviation). The allocation implicitly balances sampling promising hypotheses (exploitation) and informative/diverse hypotheses (exploration) via the σ term.",
            "computational_cost_metric": "Number of expensive function evaluations (T) is the primary cost unit; additional computational cost includes GP posterior updates and solving argmax over D (often dominated by evaluation cost). The theoretical costs are expressed asymptotically in T (number of queries).",
            "information_gain_metric": "Mutual information I(y_A; f) between observations and the function; the cumulative-regret bounds are expressed in terms of γ_T = max_{|A|=T} I(y_A; f).",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Explicit UCB rule μ + sqrt(β) σ: the posterior mean term drives exploitation toward high expected reward, the posterior standard deviation term drives exploration to reduce uncertainty. β_t controls the strength of exploration and is set to guarantee probabilistic confidence bounds.",
            "diversity_mechanism": "Not an explicit diversity-regularizer, but the σ-term encourages sampling in regions of high posterior uncertainty which, together with the information-gain analysis and submodularity arguments, promotes coverage/diverse sampling across the domain.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Fixed number of experiments / function evaluations (T).",
            "budget_constraint_handling": "Algorithm operates under a pre-specified horizon T; theoretical analysis gives regret bounds as functions of T and γ_T. For experimental-design style allocation (information-maximization) the greedy rule approximates the best T-point selection up to constant factor 1-1/e.",
            "breakthrough_discovery_metric": "No separate 'breakthrough' score; high-impact discoveries correspond to finding near-optimal/high-reward inputs. Performance measured via cumulative regret R_T and the best-observed function value max_{t ≤ T} f(x_t) relative to f(x^*).",
            "performance_metrics": "Theoretical cumulative-regret bounds: R_T = O^*( sqrt(T β_T γ_T) ). Kernel-specific asymptotics (up to polylog factors): linear kernel R_T = O(d sqrt(T)); squared-exponential (RBF) R_T = O( sqrt{T (log T)^{d+1}} ); Matérn kernels R_T = O( T^{(ν + d(d+1) / 2) / (2ν + d(d+1))} ) as given in the paper (Figure 1). Empirical comparisons: GP-UCB 'compares favorably' with EI and MPI on sensor and synthetic tasks (no single numeric improvement universally reported).",
            "comparison_baseline": "Expected Improvement (EI), Most Probable Improvement (MPI), greedy mean-only selection, variance-only selection, and other heuristic Bayesian optimization methods including EGO variants.",
            "performance_vs_baseline": "Empirically GP-UCB performs at least on par with EI and MPI on the tested synthetic and sensor datasets; on real temperature data GP-UCB and EI outperform naive alternatives, while on some datasets MPI performs similarly to GP-UCB. No consistent percentage gains are given; main claims are favorable qualitative comparisons backed by plotted average-regret curves.",
            "efficiency_gain": "Theoretical sample-efficiency: sublinear regret implies average per-step regret → 0 as T grows, so fewer evaluations are asymptotically needed to approach optimum compared to naive strategies; kernel-dependent rates show substantial efficiency gains for smooth kernels (e.g., RBF kernel yields only polylog dependence on dimension in the bound). No explicit finite-sample percent reductions reported.",
            "tradeoff_analysis": "Provides formal tradeoff analysis: regret bounds depend on exploration parameter β_T, horizon T, and information gain γ_T. γ_T captures how quickly sampling reduces uncertainty (depends on kernel spectrum); smoother kernels (faster eigenvalue decay) yield smaller γ_T and hence lower regret, formalizing the tradeoff between model smoothness, information gathered, and sampling cost. The paper also contrasts pure experimental-design (global info maximization) versus optimization-focused allocation (GP-UCB) and shows that ED can be wasteful for optimization despite maximizing information.",
            "optimal_allocation_findings": "Key theoretical findings: (1) Greedy maximization of mutual information (experimental design rule) yields a (1-1/e)-approximation to the optimal T-point information gain due to submodularity. (2) GP-UCB achieves cumulative-regret bounds scaling as O^*(sqrt(T β_T γ_T)), tying optimization performance to information-theoretic experimental design quantities. (3) Information gain γ_T can be bounded by kernel operator spectra; fast spectral decay (e.g., squared-exponential) implies small γ_T and hence sample-efficient allocation. Practical recommendation: use GP-UCB with kernel choice reflecting smoothness to optimize resources; for pure information collection use greedy info-maximization but not if goal is optimization.",
            "uuid": "e2584.0",
            "source_info": {
                "paper_title": "Information-Theoretic Regret Bounds for Gaussian Process Optimization in the Bandit Setting",
                "publication_date_yy_mm": "2009-12"
            }
        },
        {
            "name_short": "Greedy-ED (info-max)",
            "name_full": "Greedy Experimental Design via Information Gain",
            "brief_description": "A sequential sampling rule that greedily selects points to maximize the marginal information gain (mutual information) about the unknown function; equivalent to selecting points with maximal posterior standard deviation σ_{t-1}(x) in the GP setting. Submodularity gives (1-1/e) approximation guarantees to the optimal T-point design.",
            "citation_title": "Near-optimal nonmyopic value of information in graphical models",
            "mention_or_use": "use",
            "system_name": "Greedy information-maximizing experimental design",
            "system_description": "At each of T rounds, select x_t = argmax_x I(y_{A_{t-1} ∪ {x}}; f) which in the GP/noiseless marginal case is equivalent to selecting the point with largest posterior predictive variance σ_{t-1}(x). The procedure requires only kernel evaluations and does not depend on observed y_t values (it is a nonadaptive greedy design when used for planning), and by submodularity guarantees achieves at least (1-1/e) of the optimal information gain for |A|=T.",
            "application_domain": "Bayesian experimental design / active learning for function estimation; applicable to sensor placement, spatial sampling, model learning where global information coverage is desired.",
            "resource_allocation_strategy": "Allocate a fixed budget of T samples by greedy sequential maximization of marginal mutual information; the allocation focuses on covering directions (eigenvectors) of the kernel covariance with largest remaining unexplained variance, effectively allocating samples to leading eigenfunctions until diminishing returns.",
            "computational_cost_metric": "Number of planned evaluations (T) and cost of computing posterior variances/ information gains; in practice dominated by cost of computing kernel variances and evaluating the greedy argmax each round (matrix operations, O(n^3) for naïve GP updates but can use approximations).",
            "information_gain_metric": "Mutual information I(y_A; f) = (1/2) log |I + σ^{-2} K_A|; greedy marginal criterion is σ_{t-1}(x) or the incremental mutual information.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Pure exploration focused on reducing uncertainty globally; does not directly exploit high objective values because selection depends only on information gain (or σ), not on observed rewards.",
            "diversity_mechanism": "Submodularity of mutual information inherently encourages diversity: marginal gains diminish when sampling near previously sampled directions, so the greedy rule spreads samples to cover diverse informative regions/eigenfunctions.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Fixed number of experiments (cardinality constraint |A| ≤ T).",
            "budget_constraint_handling": "Directly optimizes mutual information under the cardinality budget using the greedy algorithm which attains a (1-1/e) approximation to the optimum.",
            "breakthrough_discovery_metric": "Not designed to directly detect breakthroughs; focuses on global uncertainty reduction. Breakthroughs must be inferred downstream from improved models (e.g., discovering high-value x via subsequent optimization).",
            "performance_metrics": "Theoretical guarantee: greedy design yields F(A_T) ≥ (1-1/e) max_{|A|≤T} F(A) where F is mutual information; information gain bounds γ_T used in regret analysis. Empirical performance not separately quantified in this paper beyond its role in bounding γ_T.",
            "comparison_baseline": "Optimal combinatorial design (intractable), random sampling, and GP-UCB (different objective).",
            "performance_vs_baseline": "Greedy-ED achieves provable constant-factor approximation to the optimal information-maximizing selection; however, it can be wasteful for optimization objectives because it ignores reward values and focuses on global uncertainty reduction.",
            "efficiency_gain": "Provides near-optimal information capture per sample (within factor 1-1/e of best possible for the same budget), which can significantly reduce the number of samples required for a given mutual-information target compared to naive or random sampling.",
            "tradeoff_analysis": "Paper contrasts information-maximization (Greedy-ED) with optimization goals: ED is provably near-optimal for global learning but may allocate resources away from high-value regions needed for optimization. The authors incorporate γ_T (max information gain) into regret bounds for optimization algorithms to quantify this tradeoff.",
            "optimal_allocation_findings": "Greedy information-maximization is near-optimal for the information objective under a cardinality budget due to submodularity; for optimization tasks, information gain (γ_T) is a key quantity that determines attainable regret, and kernel spectral properties determine how information should be allocated.",
            "uuid": "e2584.1",
            "source_info": {
                "paper_title": "Information-Theoretic Regret Bounds for Gaussian Process Optimization in the Bandit Setting",
                "publication_date_yy_mm": "2009-12"
            }
        },
        {
            "name_short": "EI / MPI",
            "name_full": "Expected Improvement / Most Probable Improvement",
            "brief_description": "Heuristic Bayesian optimization acquisition functions that trade off exploration and exploitation by estimating the expected or most probable improvement over the current best observation under a GP posterior; widely used in practice but lacked the formal regret guarantees provided here.",
            "citation_title": "Bayesian Approach to Global Optimization",
            "mention_or_use": "mention",
            "system_name": "Expected Improvement (EI) / Most Probable Improvement (MPI)",
            "system_description": "EI selects the point maximizing the expected positive difference between f(x) and the current best observed value under the GP posterior; MPI selects the point with highest probability of improvement. Both combine mean and variance information but in different utility forms compared to the UCB rule.",
            "application_domain": "Bayesian optimization for expensive black-box functions, hyperparameter tuning, engineering design.",
            "resource_allocation_strategy": "Sequential allocation via acquisition function optimization: at each round, select x_t maximizing EI(x) or MPI(x). These criteria balance sampling where the posterior predicts high potential improvement or where there is significant chance of improvement.",
            "computational_cost_metric": "Number of function evaluations (T) and cost of acquisition-function optimization per round; no explicit computational cost/information tradeoff analysis in this paper.",
            "information_gain_metric": "Not explicitly mutual information; based on expected utility (improvement) and posterior predictive distribution.",
            "uses_information_gain": false,
            "exploration_exploitation_mechanism": "Mechanisms embedded in the acquisition functions: EI and MPI implicitly trade exploration and exploitation through the distributional tail mass relative to current best, but without explicit information-theoretic guarantees.",
            "diversity_mechanism": "No explicit diversity-promotion mechanism beyond the implicit effect of posterior variance in the acquisition function.",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Fixed evaluation budget T (implicit in sequential procedure).",
            "budget_constraint_handling": "Greedy sequential acquisition until budget exhausted; no formal guarantee tied to a budget provided in this paper.",
            "breakthrough_discovery_metric": "Improvement over current best observation (absolute value difference); emphasis on immediate potential gains rather than information content.",
            "performance_metrics": "Empirically compared against GP-UCB in experiments: EI performed comparably to GP-UCB on several datasets (temperature data), while MPI performed similarly on some synthetic/traffic datasets. No universal numeric superiority reported.",
            "comparison_baseline": "Compared against GP-UCB, mean-only, variance-only selection in experiments.",
            "performance_vs_baseline": "On temperature data EI and GP-UCB outperformed other simple baselines; on other datasets MPI performed similarly to GP-UCB. The paper does not provide statistically summarized numerical improvement percentages.",
            "efficiency_gain": "Heuristically efficient in many applications; lacks the formal sample-complexity/regret bounds derived for GP-UCB in this paper.",
            "tradeoff_analysis": "Paper discusses that these heuristics have been successful empirically but lack formal regret guarantees; contrasts them with GP-UCB which has provable sublinear regret tied to information gain.",
            "optimal_allocation_findings": "Not the focus of this paper; EI/MPI are presented as practical baselines without formal optimality guarantees in the bandit/regret sense discussed here.",
            "uuid": "e2584.2",
            "source_info": {
                "paper_title": "Information-Theoretic Regret Bounds for Gaussian Process Optimization in the Bandit Setting",
                "publication_date_yy_mm": "2009-12"
            }
        },
        {
            "name_short": "EGO",
            "name_full": "Efficient Global Optimization (EGO)",
            "brief_description": "A global optimization framework for expensive black-box functions using a surrogate model (often kriging/Gaussian processes) and acquisition functions (like expected improvement) to guide sampling; historically influential in engineering design.",
            "citation_title": "Efficient global optimization of expensive black-box functions",
            "mention_or_use": "mention",
            "system_name": "Efficient Global Optimization (EGO)",
            "system_description": "Builds a GP (kriging) surrogate of the objective, then sequentially optimizes an acquisition function (commonly Expected Improvement) to choose new sample points; surrogate updated after each evaluation. Designed for global optimization under tight evaluation budgets.",
            "application_domain": "Global optimization of expensive simulators and black-box functions in engineering and design.",
            "resource_allocation_strategy": "Sequentially select points that maximize an acquisition function (e.g., EI) to efficiently search the domain under a limited evaluation budget.",
            "computational_cost_metric": "Main cost metric is number of expensive function evaluations; also surrogate fitting and acquisition optimization per round.",
            "information_gain_metric": "Implicit (via acquisition functions) rather than explicit mutual information maximization.",
            "uses_information_gain": false,
            "exploration_exploitation_mechanism": "Acquisition functions like EI trade exploration and exploitation by quantifying expected benefit of sampling at candidate points relative to the current best.",
            "diversity_mechanism": "No explicit diversity enforcement; diversity arises implicitly through acquisition functions and surrogate uncertainty.",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Fixed evaluation budget or user-determined stopping criteria.",
            "budget_constraint_handling": "Sequential greedy acquisition until budget exhausted; no provable global approximation guarantees provided in this paper.",
            "breakthrough_discovery_metric": "Improvement over current best observed value, as estimated by the acquisition function.",
            "performance_metrics": "Known practical success across many engineering applications; in this paper EGO-style methods (EI) are used as baselines and found empirically comparable to GP-UCB in several tests.",
            "comparison_baseline": "Compared empirically to GP-UCB and other heuristics in experiments.",
            "performance_vs_baseline": "EI (EGO-style) performs comparably to GP-UCB in the paper's experiments on temperature data; specifics depend on dataset and tuning.",
            "efficiency_gain": "Heuristic gains in practice; formal regret/approximation bounds are not provided here.",
            "tradeoff_analysis": "Paper contrasts EGO/heuristics with GP-UCB and emphasizes that GP-UCB provides provable regret bounds connected to information gain while EGO heuristics lack such theoretical guarantees.",
            "optimal_allocation_findings": "EGO is effective in practice for low-budget black-box optimization but the paper recommends GP-UCB when theoretical guarantees and information-theoretic analysis of allocation are desired.",
            "uuid": "e2584.3",
            "source_info": {
                "paper_title": "Information-Theoretic Regret Bounds for Gaussian Process Optimization in the Bandit Setting",
                "publication_date_yy_mm": "2009-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Near-optimal nonmyopic value of information in graphical models",
            "rating": 2
        },
        {
            "paper_title": "Bayesian Approach to Global Optimization",
            "rating": 2
        },
        {
            "paper_title": "Efficient global optimization of expensive black-box functions",
            "rating": 2
        },
        {
            "paper_title": "Bayesian experimental design: A review",
            "rating": 2
        },
        {
            "paper_title": "A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Regret bounds for gaussian process bandit problems",
            "rating": 2
        },
        {
            "paper_title": "Automatic gait optimization with Gaussian process regression",
            "rating": 1
        }
    ],
    "cost": 0.0205535,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design</h1>
<p>Niranjan Srinivas<br>California Institute of Technology<br>niranjan@caltech.edu<br>Sham M. Kakade<br>University of Pennsylvania<br>skakade@wharton.upenn.edu</p>
<p>Andreas Krause<br>California Institute of Technology<br>krausea@caltech.edu<br>Matthias Seeger<br>Saarland University<br>mseeger@mmci.uni-saarland.de</p>
<h4>Abstract</h4>
<p>Many applications require optimizing an unknown, noisy function that is expensive to evaluate. We formalize this task as a multiarmed bandit problem, where the payoff function is either sampled from a Gaussian process (GP) or has low RKHS norm. We resolve the important open problem of deriving regret bounds for this setting, which imply novel convergence rates for GP optimization. We analyze GP-UCB, an intuitive upper-confidence based algorithm, and bound its cumulative regret in terms of maximal information gain, establishing a novel connection between GP optimization and experimental design. Moreover, by bounding the latter in terms of operator spectra, we obtain explicit sublinear regret bounds for many commonly used covariance functions. In some important cases, our bounds have surprisingly weak dependence on the dimensionality. In our experiments on real sensor data, GP-UCB compares favorably with other heuristical GP optimization approaches.</p>
<h2>1. Introduction</h2>
<p>In most stochastic optimization settings, evaluating the unknown function is expensive, and sampling is to be minimized. Examples include choosing advertisements in sponsored search to maximize profit in a click-through model (Pandey \&amp; Olston, 2007) or learning optimal control strategies for robots (Lizotte et al., 2007). Predominant approaches to this problem include the multi-armed bandit paradigm (Robbins, 1952), where the goal is to maximize cumulative reward by optimally balancing exploration and exploitation, and experimental design (Chaloner \&amp; Verdinelli, 1995), where the function is to be explored globally with as few evaluations as possible, for example by maximizing information</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>gain. The challenge in both approaches is twofold: we have to estimate an unknown function $f$ from noisy samples, and we must optimize our estimate over some high-dimensional input space. For the former, much progress has been made in machine learning through kernel methods and Gaussian process (GP) models (Rasmussen \&amp; Williams, 2006), where smoothness assumptions about $f$ are encoded through the choice of kernel in a flexible nonparametric fashion. Beyond Euclidean spaces, kernels can be defined on diverse domains such as spaces of graphs, sets, or lists.</p>
<p>We are concerned with GP optimization in the multiarmed bandit setting, where $f$ is sampled from a GP distribution or has low "complexity" measured in terms of its RKHS norm under some kernel. We provide the first sublinear regret bounds in this nonparametric setting, which imply convergence rates for GP optimization. In particular, we analyze the Gaussian Process Upper Confidence Bound (GP-UCB) algorithm, a simple and intuitive Bayesian method (Auer et al., 2002; Auer, 2002; Dani et al., 2008). While objectives are different in the multi-armed bandit and experimental design paradigm, our results draw a close technical connection between them: our regret bounds come in terms of an information gain quantity, measuring how fast $f$ can be learned in an information theoretic sense. The submodularity of this function allows us to prove sharp regret bounds for particular covariance functions, which we demonstrate for commonly used Squared Exponential and Matérn kernels.</p>
<p>Related Work. Our work generalizes stochastic linear optimization in a bandit setting, where the unknown function comes from a finite-dimensional linear space. GPs are nonlinear random functions, which can be represented in an infinite-dimensional linear space. For the standard linear setting, Dani et al. (2008) provide a near-complete characterization</p>
<p>(also see Auer 2002; Dani et al. 2007; Abernethy et al. 2008; Rusmevichientong \&amp; Tsitsiklis 2008), explicitly dependent on the dimensionality. In the GP setting, the challenge is to characterize complexity in a different manner, through properties of the kernel function. Our technical contributions are twofold: first, we show how to analyze the nonlinear setting by focusing on the concept of information gain, and second, we explicitly bound this information gain measure using the concept of submodularity (Nemhauser et al., 1978) and knowledge about kernel operator spectra.</p>
<p>Kleinberg et al. (2008) provide regret bounds under weaker and less configurable assumptions (only Lipschitz-continuity w.r.t. a metric is assumed; Bubeck et al. 2008 consider arbitrary topological spaces), which however degrade rapidly with the dimensionality of the problem $\left(\Omega\left(T^{\frac{d+1}{d+2}}\right)\right)$. In practice, linearity w.r.t. a fixed basis is often too stringent an assumption, while Lipschitz-continuity can be too coarse-grained, leading to poor rate bounds. Adopting GP assumptions, we can model levels of smoothness in a fine-grained way. For example, our rates for the frequently used Squared Exponential kernel, enforcing a high degree of smoothness, have weak dependence on the dimensionality: $\mathcal{O}\left(\sqrt{T(\log T)^{d+1}}\right)$ (see Fig. 1).</p>
<p>There is a large literature on GP (response surface) optimization. Several heuristics for trading off exploration and exploitation in GP optimization have been proposed (such as Expected Improvement, Mockus et al. 1978, and Most Probable Improvement, Mockus 1989) and successfully applied in practice (c.f., Lizotte et al. 2007). Brochu et al. (2009) provide a comprehensive review of and motivation for Bayesian optimization using GPs. The Efficient Global Optimization (EGO) algorithm for optimizing expensive black-box functions is proposed by Jones et al. (1998) and extended to GPs by Huang et al. (2006). Little is known about theoretical performance of GP optimization. While convergence of EGO is established by Vazquez \&amp; Bect (2007), convergence rates have remained elusive. Grünewälder et al. (2010) consider the pure exploration problem for GPs, where the goal is to find the optimal decision over $T$ rounds, rather than maximize cumulative reward (with no exploration/exploitation dilemma). They provide sharp bounds for this exploration problem. Note that this methodology would not lead to bounds for minimizing the cumulative regret. Our cumulative regret bounds translate to the first performance guarantees (rates) for GP optimization.</p>
<p>Summary. Our main contributions are:</p>
<ul>
<li>We analyze GP-UCB, an intuitive algorithm for GP optimization, when the function is either sam-</li>
</ul>
<table>
<thead>
<tr>
<th style="text-align: center;">Kernel</th>
<th style="text-align: center;">Linear</th>
<th style="text-align: center;">RBF</th>
<th style="text-align: center;">Matérn</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Regret $R_{T}$</td>
<td style="text-align: center;">$d \sqrt{T}$</td>
<td style="text-align: center;">$\sqrt{T(\log T)^{d+1}}$</td>
<td style="text-align: center;">$T^{\frac{\nu+d(d+1)}{2 \nu+d(d+1)}}$</td>
</tr>
</tbody>
</table>
<p>Figure 1. Our regret bounds (up to polylog factors) for linear, radial basis, and Matérn kernels $-d$ is the dimension, $T$ is the time horizon, and $\nu$ is a Matérn parameter.
pled from a known GP, or has low RKHS norm.</p>
<ul>
<li>We bound the cumulative regret for GP-UCB in terms of the information gain due to sampling, establishing a novel connection between experimental design and GP optimization.</li>
<li>By bounding the information gain for popular classes of kernels, we establish sublinear regret bounds for GP optimization for the first time. Our bounds depend on kernel choice and parameters in a fine-grained fashion.</li>
<li>We evaluate GP-UCB on sensor network data, demonstrating that it compares favorably to existing algorithms for GP optimization.</li>
</ul>
<h2>2. Problem Statement and Background</h2>
<p>Consider the problem of sequentially optimizing an unknown reward function $f: D \rightarrow \mathbb{R}$ : in each round $t$, we choose a point $\boldsymbol{x}<em t="t">{t} \in D$ and get to see the function value there, perturbed by noise: $y</em>}=f\left(\boldsymbol{x<em t="t">{t}\right)+\epsilon</em>}$. Our goal is to maximize the sum of rewards $\sum_{t=1}^{T} f\left(\boldsymbol{x<em _boldsymbol_x="\boldsymbol{x">{t}\right)$, thus to perform essentially as well as $\boldsymbol{x}^{*}=\operatorname{argmax}</em>$, and sensor accuracy is quantified by the noise variance. Each activation draws battery power, so we want to sample from as few sensors as possible.} \in D} f(\boldsymbol{x})$ (as rapidly as possible). For example, we might want to find locations of highest temperature in a building by sequentially activating sensors in a spatial network and regressing on their measurements. $D$ consists of all sensor locations, $f(\boldsymbol{x})$ is the temperature at $\boldsymbol{x</p>
<p>Regret. A natural performance metric in this context is cumulative regret, the loss in reward due to not knowing $f$ 's maximum points beforehand. Suppose the unknown function is $f$, its maximum point ${ }^{1}$ $\boldsymbol{x}^{<em>}=\operatorname{argmax}<em t="t">{\boldsymbol{x} \in D} f(\boldsymbol{x})$. For our choice $\boldsymbol{x}</em>^{}$ in round $t$, we incur instantaneous regret $r_{t}=f\left(\boldsymbol{x</em>}\right)-f\left(\boldsymbol{x}<em T="T">{t}\right)$. The cumulative regret $R</em>$. A desirable asymptotic property of an algorithm is to be no-regret: $\lim }$ after $T$ rounds is the sum of instantaneous regrets: $R_{T}=\sum_{t=1}^{T} r_{t<em T="T">{T \rightarrow \infty} R</em> / T$ translate to convergence rates for GP optimization: the maximum $\max } / T=0$. Note that neither $r_{t}$ nor $R_{T}$ are ever revealed to the algorithm. Bounds on the average regret $R_{T<em t="t">{t \leq T} f\left(\boldsymbol{x}</em>\right)$ than the average.}\right)$ in the first $T$ rounds is no further from $f\left(\boldsymbol{x}^{*</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h3>2.1. Gaussian Processes and RKHS's</h3>
<p>Gaussian Processes. Some assumptions on $f$ are required to guarantee no-regret. While rigid parametric assumptions such as linearity may not hold in practice, a certain degree of smoothness is often warranted. In our sensor network, temperature readings at closeby locations are highly correlated (see Figure 2(a)). We can enforce implicit properties like smoothness without relying on any parametric assumptions, modeling $f$ as a sample from a Gaussian process (GP): a collection of dependent random variables, one for each $\boldsymbol{x} \in D$, every finite subset of which is multivariate Gaussian distributed in an overall consistent way (Rasmussen \&amp; Williams, 2006). A $G P\left(\mu(\boldsymbol{x}), k\left(\boldsymbol{x}, \boldsymbol{x}^{\prime}\right)\right)$ is specified by its mean function $\mu(\boldsymbol{x})=\mathbb{E}[f(\boldsymbol{x})]$ and covariance (or kernel) function $k\left(\boldsymbol{x}, \boldsymbol{x}^{\prime}\right)=\mathbb{E}[(f(x)-$ $\left.\mu(\boldsymbol{x}))\left(f\left(x^{\prime}\right)-\mu\left(\boldsymbol{x}^{\prime}\right)\right)\right]$. For GPs not conditioned on data, we assume ${ }^{2}$ that $\mu \equiv 0$. Moreover, we restrict $k(\boldsymbol{x}, \boldsymbol{x}) \leq 1, \boldsymbol{x} \in D$, i.e., we assume bounded variance. By fixing the correlation behavior, the covariance function $k$ encodes smoothness properties of sample functions $f$ drawn from the GP. A range of commonly used kernel functions is given in Section 5.2.</p>
<p>In this work, GPs play multiple roles. First, some of our results hold when the unknown target function is a sample from a known GP distribution GP $\left(0, k\left(\boldsymbol{x}, \boldsymbol{x}^{\prime}\right)\right)$. Second, the Bayesian algorithm we analyze generally uses $\operatorname{GP}\left(0, k\left(\boldsymbol{x}, \boldsymbol{x}^{\prime}\right)\right)$ as prior distribution over $f$. A major advantage of working with GPs is the existence of simple analytic formulae for mean and covariance of the posterior distribution, which allows easy implementation of algorithms. For a noisy sample $\boldsymbol{y}<em 1="1">{T}=\left[y</em>} \ldots y_{T}\right]^{T}$ at points $A_{T}=\left{\boldsymbol{x<em T="T">{1}, \ldots, \boldsymbol{x}</em>}\right}$, $y_{t}=f\left(\boldsymbol{x<em t="t">{t}\right)+\epsilon</em>)$ :}$ with $\epsilon_{t} \sim N\left(0, \sigma^{2}\right)$ i.i.d. Gaussian noise, the posterior over $f$ is a GP distribution again, with mean $\mu_{T}(\boldsymbol{x})$, covariance $k_{T}\left(\boldsymbol{x}, \boldsymbol{x}^{\prime}\right)$ and variance $\sigma_{T}^{2}(\boldsymbol{x</p>
<p>$$
\begin{aligned}
\mu_{T}(\boldsymbol{x}) &amp; =\boldsymbol{k}<em T="T">{T}(\boldsymbol{x})^{T}\left(\boldsymbol{K}</em>}+\sigma^{2} \boldsymbol{I}\right)^{-1} \boldsymbol{y<em T="T">{T} \
k</em>}\left(\boldsymbol{x}, \boldsymbol{x}^{\prime}\right) &amp; =k\left(\boldsymbol{x}, \boldsymbol{x}^{\prime}\right)-\boldsymbol{k<em T="T">{T}(\boldsymbol{x})^{T}\left(\boldsymbol{K}</em>}+\sigma^{2} \boldsymbol{I}\right)^{-1} \boldsymbol{k<em T="T">{T}\left(\boldsymbol{x}^{\prime}\right) \
\sigma</em>)
\end{aligned}
$$}^{2}(\boldsymbol{x}) &amp; =k_{T}(\boldsymbol{x}, \boldsymbol{x</p>
<p>where $\boldsymbol{k}<em 1="1">{T}(\boldsymbol{x})=\left[k\left(\boldsymbol{x}</em>}, \boldsymbol{x}\right) \ldots k\left(\boldsymbol{x<em T="T">{T}, \boldsymbol{x}\right)\right]^{T}$ and $\boldsymbol{K}</em>\right)\right]}$ is the positive definite kernel matrix $\left[k\left(\boldsymbol{x}, \boldsymbol{x}^{\prime<em T="T">{\boldsymbol{x}, \boldsymbol{x}^{\prime} \in A</em>$.
RKHS. Instead of the Bayes case, where $f$ is sampled from a GP prior, we also consider the more agnostic case where $f$ has low "complexity" as measured under an RKHS norm (and distribution free assumptions on the noise process). The notion of reproducing kernel Hilbert spaces (RKHS, Wahba 1990) is intimately related to GPs and their covariance functions $k\left(\boldsymbol{x}, \boldsymbol{x}^{\prime}\right)$. The RKHS $\mathcal{H}}<em 2="2">{k}(D)$ is a complete subspace of $L</em>(D)$ of nicely behaved functions, with an</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>inner product $\langle\cdot, \cdot\rangle_{k}$ obeying the reproducing property: $\langle f, k(\boldsymbol{x}, \cdot)\rangle_{k}=f(\boldsymbol{x})$ for all $f \in \mathcal{H}<em T="T">{k}(D)$. It is literally constructed by completing the set of mean functions $\mu</em>}$ for all possible $T,\left{\boldsymbol{x<em 1="1">{t}\right}</em>}$ and $\boldsymbol{y<em k="k">{T}$. The induced RKHS norm $|f|</em>$ as GP covariance functions, $|\cdot|}=\sqrt{\langle f, f\rangle_{k}}$ measures smoothness of $f$ w.r.t. $k$ : in much the same way as $k_{1}$ would generate smoother samples than $k_{2<em 1="1">{k</em>$ assigns larger penalties than $|\cdot|}<em 2="2">{k</em>(D)$, in which case $|f|}} .\langle\cdot, \cdot\rangle_{k}$ can be extended to all of $L_{2<em k="k">{k}&lt;\infty$ iff $f \in \mathcal{H}</em>(D)$ can uniformly approximate any continuous function on any compact subset of $D$.}(D)$. For most kernels discussed in Section 5.2, members of $\mathcal{H}_{k</p>
<h3>2.2. Information Gain \&amp; Experimental Design</h3>
<p>One approach to maximizing $f$ is to first choose points $\boldsymbol{x}<em A="A">{t}$ so as to estimate the function globally well, then play the maximum point of our estimate. How can we learn about $f$ as rapidly as possible? This question comes down to Bayesian Experimental Design (henceforth "ED"; see Chaloner \&amp; Verdinelli 1995), where the informativeness of a set of sampling points $A \subset D$ about $f$ is measured by the information gain (c.f., Cover \&amp; Thomas 1991), which is the mutual information between $f$ and observations $\boldsymbol{y}</em>}=\boldsymbol{f<em A="A">{A}+\epsilon</em>$ at these points:</p>
<p>$$
\mathrm{I}\left(\boldsymbol{y}<em A="A">{A} ; f\right)=\mathrm{H}\left(\boldsymbol{y}</em> \mid f\right)
$$}\right)-\mathrm{H}\left(\boldsymbol{y}_{A</p>
<p>quantifying the reduction in uncertainty about $f$ from revealing $\boldsymbol{y}<em A="A">{A}$. Here, $\boldsymbol{f}</em>)]}=[f(\boldsymbol{x<em A="A">{\boldsymbol{x} \in A}$ and $\boldsymbol{\varepsilon}</em>} \sim N\left(\mathbf{0}, \sigma^{2} \boldsymbol{I}\right)$. For a Gaussian, $\mathrm{H}(N(\boldsymbol{\mu}, \boldsymbol{\Sigma}))=$ $\frac{1}{2} \log |2 \pi c \boldsymbol{\Sigma}|$, so that in our setting $\mathrm{I}\left(\boldsymbol{y<em A="A">{A} ; f\right)=$ $\mathrm{I}\left(\boldsymbol{y}</em>} ; \boldsymbol{f<em A="A">{A}\right)=\frac{1}{2} \log \left|\boldsymbol{I}+\sigma^{-2} \boldsymbol{K}</em>}\right|$, where $\boldsymbol{K<em _boldsymbol_x="\boldsymbol{x">{A}=$ $\left[k\left(\boldsymbol{x}, \boldsymbol{x}^{\prime}\right)\right]</em>}, \boldsymbol{x}^{\prime} \in A}$. While finding the information gain maximizer among $A \subset D,|A| \leq T$ is NP-hard (Ko et al., 1995), it can be approximated by an efficient greedy algorithm. If $F(A)=\mathrm{I}\left(\boldsymbol{y<em t="t">{A} ; f\right)$, this algorithm picks $\boldsymbol{x}</em>}=\operatorname{argmax<em t-1="t-1">{\boldsymbol{x} \in D} F\left(A</em>}\right)$ in round $t$, which can be shown to be equivalent to} \cup{\boldsymbol{x</p>
<p>$$
\boldsymbol{x}<em t-1="t-1">{t}=\underset{\boldsymbol{x} \in D}{\operatorname{argmax}} \sigma</em>)
$$}(\boldsymbol{x</p>
<p>where $A_{t-1}=\left{\boldsymbol{x}<em t-1="t-1">{1}, \ldots, \boldsymbol{x}</em>$ obtained after $T$ rounds, we have that}\right}$. Importantly, this simple algorithm is guaranteed to find a near-optimal solution: for the set $A_{T</p>
<p>$$
F\left(A_{T}\right) \geq(1-1 / e) \max _{|A| \leq T} F(A)
$$</p>
<p>at least a constant fraction of the optimal information gain value. This is because $F(A)$ satisfies a diminishing returns property called submodularity (Krause \&amp; Guestrin, 2005), and the greedy approximation guarantee (5) holds for any submodular function (Nemhauser et al., 1978).</p>
<p>While sequentially optimizing Eq. 4 is a provably good way to explore $f$ globally, it is not well suited for func-</p>
<p>tion optimization. For the latter, we only need to identify points $\boldsymbol{x}$ where $f(\boldsymbol{x})$ is large, in order to concentrate sampling there as rapidly as possible, thus exploit our knowledge about maxima. In fact, the ED rule (4) does not even depend on observations $y_{t}$ obtained along the way. Nevertheless, the maximum information gain after $T$ rounds will play a prominent role in our regret bounds, forging an important connection between GP optimization and experimental design.</p>
<h2>3. GP-UCB Algorithm</h2>
<p>For sequential optimization, the ED rule (4) can be wasteful: it aims at decreasing uncertainty globally, not just where maxima might be. Another idea is to pick points as $\boldsymbol{x}<em _boldsymbol_x="\boldsymbol{x">{t}=\operatorname{argmax}</em>)$, maximizing the expected reward based on the posterior so far. However, this rule is too greedy too soon and tends to get stuck in shallow local optima. A combined strategy is to choose} \in D} \mu_{t-1}(\boldsymbol{x</p>
<p>$$
\boldsymbol{x}<em t-1="t-1">{t}=\underset{\boldsymbol{x} \in D}{\operatorname{argmax}} \mu</em>)
$$}(\boldsymbol{x})+\beta_{t}^{1 / 2} \sigma_{t-1}(\boldsymbol{x</p>
<p>where $\beta_{t}$ are appropriate constants. This latter objective prefers both points $\boldsymbol{x}$ where $f$ is uncertain (large $\sigma_{t-1}(\cdot)$ ) and such where we expect to achieve high rewards (large $\mu_{t-1}(\cdot)$ ): it implicitly negotiates the exploration-exploitation tradeoff. A natural interpretation of this sampling rule is that it greedily selects points $\boldsymbol{x}$ such that $f(\boldsymbol{x})$ should be a reasonable upper bound on $f\left(\boldsymbol{x}^{*}\right)$, since the argument in (6) is an upper quantile of the marginal posterior $P\left(f(\boldsymbol{x}) \mid \boldsymbol{y}<em t="t">{t-1}\right)$. We call this choice the Gaussian process upper confidence bound rule (GP-UCB), where $\beta</em>)$.
The GP-UCB selection rule Eq. 6 is motivated by the UCB algorithm for the classical multi-armed bandit problem (Auer et al., 2002; Kocsis \&amp; Szepesvári, 2006). Among competing criteria for GP optimization (see Section 1), a variant of the GP-UCB rule has been demonstrated to be effective for this application (Dorard et al., 2009). To our knowledge, strong theoretical results of the kind provided for GP-UCB in this paper have not been given for any of these search heuristics. In Section 6, we show that in practice GP-UCB compares favorably with these alternatives.
If $D$ is infinite, finding $\boldsymbol{x}_{t}$ in (6) may be hard: the upper confidence index is multimodal in general. However, global search heuristics are very effective in practice (Brochu et al., 2009). It is generally assumed}$ is specified depending on the context (see Section 4). Pseudocode for the GP-UCB algorithm is provided in Algorithm 1. Figure 2 illustrates two subsequent iterations, where GP-UCB both explores (Figure 2(b)) by sampling an input $\boldsymbol{x}$ with large $\sigma_{t-1}^{2}(\boldsymbol{x})$ and exploits (Figure 2(c)) by sampling $\boldsymbol{x}$ with large $\mu_{t-1}(\boldsymbol{x</p>
<div class="codehilite"><pre><span></span><code><span class="nt">Algorithm</span><span class="w"> </span><span class="nt">1</span><span class="w"> </span><span class="nt">The</span><span class="w"> </span><span class="nt">GP-UCB</span><span class="w"> </span><span class="nt">algorithm</span><span class="o">.</span>
<span class="w">    </span><span class="nt">Input</span><span class="o">:</span><span class="w"> </span><span class="nt">Input</span><span class="w"> </span><span class="nt">space</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">D</span><span class="w"> </span><span class="o">;</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">GP</span><span class="w"> </span><span class="nt">Prior</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">mu_</span><span class="p">{</span><span class="err">0</span><span class="p">}</span><span class="o">=</span><span class="nt">0</span><span class="o">,</span><span class="w"> </span><span class="err">\</span><span class="nt">sigma_</span><span class="p">{</span><span class="err">0</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="nt">k</span><span class="err">\</span><span class="o">)</span>
<span class="w">    </span><span class="nt">for</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">t</span><span class="o">=</span><span class="nt">1</span><span class="o">,</span><span class="nt">2</span><span class="o">,</span><span class="w"> </span><span class="err">\</span><span class="nt">ldots</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">do</span>
<span class="w">        </span><span class="nt">Choose</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">boldsymbol</span><span class="p">{</span><span class="err">x</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">t</span><span class="p">}</span><span class="o">=</span><span class="err">\</span><span class="nt">underset</span><span class="p">{</span><span class="err">\boldsymbol{x</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">in</span><span class="w"> </span><span class="nt">D</span><span class="err">}</span><span class="p">{</span><span class="err">\operatorname{argmax</span><span class="p">}</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="nt">mu_</span><span class="p">{</span><span class="err">t-1</span><span class="p">}</span><span class="o">(</span><span class="err">\</span><span class="nt">boldsymbol</span><span class="p">{</span><span class="err">x</span><span class="p">}</span><span class="o">)+</span><span class="err">\</span><span class="nt">sqrt</span><span class="p">{</span><span class="err">\beta_{t</span><span class="p">}</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="nt">sigma_</span><span class="p">{</span><span class="err">t-1</span><span class="p">}</span><span class="o">(</span><span class="err">\</span><span class="nt">boldsymbol</span><span class="p">{</span><span class="err">x</span><span class="p">}</span><span class="o">)</span><span class="err">\</span><span class="o">)</span>
<span class="w">        </span><span class="nt">Sample</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">y_</span><span class="p">{</span><span class="err">t</span><span class="p">}</span><span class="o">=</span><span class="nt">f</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="err">\</span><span class="nt">boldsymbol</span><span class="p">{</span><span class="err">x</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">t</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="o">)+</span><span class="err">\</span><span class="nt">epsilon_</span><span class="p">{</span><span class="err">t</span><span class="p">}</span><span class="err">\</span><span class="o">)</span>
<span class="w">        </span><span class="nt">Perform</span><span class="w"> </span><span class="nt">Bayesian</span><span class="w"> </span><span class="nt">update</span><span class="w"> </span><span class="nt">to</span><span class="w"> </span><span class="nt">obtain</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">mu_</span><span class="p">{</span><span class="err">t</span><span class="p">}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">and</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">sigma_</span><span class="p">{</span><span class="err">t</span><span class="p">}</span><span class="err">\</span><span class="o">)</span>
<span class="w">    </span><span class="nt">end</span><span class="w"> </span><span class="nt">for</span>
</code></pre></div>

<p>that evaluating $f$ is more costly than maximizing the UCB index.</p>
<p>UCB algorithms (and GP optimization techniques in general) have been applied to a large number of problems in practice (Kocsis \&amp; Szepesvári, 2006; Pandey \&amp; Olston, 2007; Lizotte et al., 2007). Their performance is well characterized in both the finite arm setting and the linear optimization setting, but no convergence rates for GP optimization are known.</p>
<h2>4. Regret Bounds</h2>
<p>We now establish cumulative regret bounds for GP optimization, treating a number of different settings: $f \sim \operatorname{GP}\left(0, k\left(\boldsymbol{x}, \boldsymbol{x}^{\prime}\right)\right)$ for finite $D, f \sim \operatorname{GP}\left(0, k\left(\boldsymbol{x}, \boldsymbol{x}^{\prime}\right)\right)$ for general compact $D$, and the agnostic case of arbitrary $f$ with bounded RKHS norm.</p>
<p>GP optimization generalizes stochastic linear optimization, where a function $f$ from a finite-dimensional linear space is optimized over. For the linear case, Dani et al. (2008) provide regret bounds that explicitly depend on the dimensionality ${ }^{3} d$. GPs can be seen as random functions in some infinite-dimensional linear space, so their results do not apply in this case. This problem is circumvented in our regret bounds. The quantity governing them is the maximum information gain $\gamma_{T}$ after $T$ rounds, defined as:</p>
<p>$$
\gamma_{T}:=\max <em A="A">{A \subset D:|A|=T} \mathrm{I}\left(\boldsymbol{y}</em>\right)
$$} ; \boldsymbol{f}_{A</p>
<p>where $\mathrm{I}\left(\boldsymbol{y}<em A="A">{A} ; \boldsymbol{f}</em>}\right)=\mathrm{I}\left(\boldsymbol{y<em A="A">{A} ; f\right)$ is defined in (3). Recall that $\mathrm{I}\left(\boldsymbol{y}</em>} ; \boldsymbol{f<em A="A">{A}\right)=\frac{1}{2} \log \left|\boldsymbol{I}+\sigma^{-2} \boldsymbol{K}</em>}\right|$, where $\boldsymbol{K<em _boldsymbol_x="\boldsymbol{x">{A}=$ $\left[k\left(\boldsymbol{x}, \boldsymbol{x}^{\prime}\right)\right]</em>}, \boldsymbol{x}^{\prime} \in A}$ is the covariance matrix of $\boldsymbol{f<em _boldsymbol_x="\boldsymbol{x">{A}=$ $[f(\boldsymbol{x})]</em>^{} \in A}$ associated with the samples $A$. Our regret bounds are of the form $\mathcal{O<em>}\left(\sqrt{T \beta_{T} \gamma_{T}}\right)$, where $\beta_{T}$ is the confidence parameter in Algorithm 1, while the bounds of Dani et al. (2008) are of the form $\mathcal{O}^{</em>}\left(\sqrt{T \beta_{T} d}\right)$ ( $d$ the dimensionality of the linear function space). Here and below, the $\mathcal{O}^{*}$ notation is a variant of $\mathcal{O}$, where log factors are suppressed. While our proofs - all provided in the Appendix - use techniques similar to those of Dani et al. (2008), we face a number of additional</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 2. (a) Example of temperature data collected by a network of 46 sensors at Intel Research Berkeley. (b,c) Two iterations of the GP-UCB algorithm. It samples points that are either uncertain (b) or have high posterior mean (c).
significant technical challenges. Besides avoiding the finite-dimensional analysis, we must handle confidence issues, which are more delicate for nonlinear random functions.</p>
<p>Importantly, note that the information gain is a problem dependent quantity - properties of both the kernel and the input space will determine the growth of regret. In Section 5, we provide general methods for bounding $\gamma_{T}$, either by efficient auxiliary computations or by direct expressions for specific kernels of interest. Our results match known lower bounds (up to $\log$ factors) in both the $K$-armed bandit and the $d$-dimensional linear optimization case.</p>
<p>Bounds for a GP Prior. For finite $D$, we obtain the following bound.</p>
<p>Theorem 1 Let $\delta \in(0,1)$ and $\beta_{t}=$ $2 \log \left(|D| t^{2} \pi^{2} / 6 \delta\right)$. Running GP-UCB with $\beta_{t}$ for a sample $f$ of a GP with mean function zero and covariance function $k\left(\boldsymbol{x}, \boldsymbol{x}^{\prime}\right)$, we obtain a regret bound of $\mathcal{O}^{*}\left(\sqrt{T \gamma_{T} \log |D|}\right)$ with high probability. Precisely,</p>
<p>$$
\operatorname{Pr}\left{R_{T} \leq \sqrt{C_{1} T \beta_{T} \gamma_{T}} \quad \forall T \geq 1\right} \geq 1-\delta
$$</p>
<p>where $C_{1}=8 / \log \left(1+\sigma^{-2}\right)$.
The proof methodology follows Dani et al. (2007) in that we relate the regret to the growth of the log volume of the confidence ellipsoid - a novelty in our proof is showing how this growth is characterized by the information gain.</p>
<p>This theorem shows that, with high probability over samples from the GP, the cumulative regret is bounded in terms of the maximum information gain, forging a novel connection between GP optimization and experimental design. This link is of fundamental technical importance, allowing us to generalize Theorem 1 to infinite decision spaces. Moreover, the submodularity of $\mathrm{I}\left(\boldsymbol{y}<em A="A">{A} ; \boldsymbol{f}</em>\right)$ allows us to derive sharp a priori bounds,
depending on choice and parameterization of $k$ (see Section 5). In the following theorem, we generalize our result to any compact and convex $D \subset \mathbb{R}^{d}$ under mild assumptions on the kernel function $k$.</p>
<p>Theorem 2 Let $D \subset[0, r]^{d}$ be compact and convex, $d \in \mathbb{N}, r&gt;0$. Suppose that the kernel $k\left(\boldsymbol{x}, \boldsymbol{x}^{\prime}\right)$ satisfies the following high probability bound on the derivatives of GP sample paths $f$ : for some constants $a, b&gt;0$,
$\operatorname{Pr}\left{\sup <em j="j">{\boldsymbol{x} \in D}\left|\partial f / \partial x</em>, \quad j=1, \ldots, d$.
Pick $\delta \in(0,1)$, and define
$\beta_{t}=2 \log \left(t^{2} 2 \pi^{2} /(3 \delta)\right)+2 d \log \left(t^{2} d b r \sqrt{\log (4 d a / \delta)}\right)$.
Running the GP-UCB with $\beta_{t}$ for a sample $f$ of a GP with mean function zero and covariance function $k\left(\boldsymbol{x}, \boldsymbol{x}^{\prime}\right)$, we obtain a regret bound of $\mathcal{O}^{*}\left(\sqrt{d T \gamma_{T}}\right)$ with high probability. Precisely, with $C_{1}=8 / \log \left(1+\sigma^{-2}\right)$ we have}\right|&gt;L\right} \leq a e^{-(L / b)^{2}</p>
<p>$$
\operatorname{Pr}\left{R_{T} \leq \sqrt{C_{1} T \beta_{T} \gamma_{T}}+2 \quad \forall T \geq 1\right} \geq 1-\delta
$$</p>
<p>The main challenge in our proof (provided in the Appendix) is to lift the regret bound in terms of the confidence ellipsoid to general $D$. The smoothness assumption on $k\left(\boldsymbol{x}, \boldsymbol{x}^{\prime}\right)$ disqualifies GPs with highly erratic sample paths. It holds for stationary kernels $k\left(\boldsymbol{x}, \boldsymbol{x}^{\prime}\right)=k\left(\boldsymbol{x}-\boldsymbol{x}^{\prime}\right)$ which are four times differentiable (Theorem 5 of Ghosal \&amp; Roy (2006)), such as the Squared Exponential and Matérn kernels with $\nu&gt;2$ (see Section 5.2), while it is violated for the OrnsteinUhlenbeck kernel (Matérn with $\nu=1 / 2$; a stationary variant of the Wiener process). For the latter, sample paths $f$ are nondifferentiable almost everywhere with probability one and come with independent increments. We conjecture that a result of the form of Theorem 2 does not hold in this case.</p>
<p>Bounds for Arbitrary $f$ in the RKHS. Thus far, we have assumed that the target function $f$ is sampled</p>
<p>from a GP prior and that the noise is $N\left(0, \sigma^{2}\right)$ with known variance $\sigma^{2}$. We now analyze GP-UCB in an agnostic setting, where $f$ is an arbitrary function from the RKHS corresponding to kernel $k\left(\boldsymbol{x}, \boldsymbol{x}^{\prime}\right)$. Moreover, we allow the noise variables $\varepsilon_{t}$ to be an arbitrary martingale difference sequence (meaning that $\mathbb{E}\left[\varepsilon_{t} \mid \boldsymbol{\varepsilon}_{&lt;t}\right]=0$ for all $t \in \mathbb{N}$ ), uniformly bounded by $\sigma$. Note that we still run the same GP-UCB algorithm, whose prior and noise model are misspecified in this case. Our following result shows that GP-UCB attains sublinear regret even in the agnostic setting.</p>
<p>Theorem 3 Let $\delta \in(0,1)$. Assume that the true underlying $f$ lies in the RKHS $\mathcal{H}<em t="t">{k}(D)$ corresponding to the kernel $k\left(\boldsymbol{x}, \boldsymbol{x}^{\prime}\right)$, and that the noise $\varepsilon</em>$ has zero mean conditioned on the history and is bounded by $\sigma$ almost surely. In particular, assume $|f|<em t="t">{k}^{2} \leq B$ and let $\beta</em>\right)\right)$ with high probability (over the noise). Precisely,}=2 B+300 \gamma_{t} \log ^{3}(t / \delta)$. Running GP-UCB with $\beta_{t}$, prior $G P\left(0, k\left(\boldsymbol{x}, \boldsymbol{x}^{\prime}\right)\right)$ and noise model $N\left(0, \sigma^{2}\right)$, we obtain a regret bound of $\mathcal{O}^{*}\left(\sqrt{T}\left(B \sqrt{\gamma_{T}}+\gamma_{T</p>
<p>$$
\operatorname{Pr}\left{R_{T} \leq \sqrt{C_{1} T \beta_{T} \gamma_{T}} \quad \forall T \geq 1\right} \geq 1-\delta
$$</p>
<p>where $C_{1}=8 / \log \left(1+\sigma^{-2}\right)$.
Note that while our theorem implicitly assumes that GP-UCB has knowledge of an upper bound on $|f|<em k="k">{k}$, standard guess-and-doubling approaches suffice if no such bound is known a priori. Comparing Theorem 2 and Theorem 3, the latter holds uniformly over all functions $f$ with $|f|</em>=\infty$ almost surely (Wahba, 1990): sample paths are rougher than RKHS functions. Neither Theorem 2 nor 3 encompasses the other.}&lt;\infty$, while the former is a probabilistic statement requiring knowledge of the GP that $f$ is sampled from. In contrast, if $f \sim \operatorname{GP}\left(0, k\left(\boldsymbol{x}, \boldsymbol{x}^{\prime}\right)\right)$, then $|f|_{k</p>
<h2>5. Bounding the Information Gain</h2>
<p>Since the bounds developed in Section 4 depend on the information gain, the key remaining question is how to bound the quantity $\gamma_{T}$ for practical classes of kernels.</p>
<h3>5.1. Submodularity and Greedy Maximization</h3>
<p>In order to bound $\gamma_{T}$, we have to maximize the information gain $F(A)=\mathrm{I}\left(\boldsymbol{y}<em T="T">{A} ; f\right)$ over all subsets $A \subset D$ of size $T$ : a combinatorial problem in general. However, as noted in Section $2, F(A)$ is a submodular function, which implies the performance guarantee (5) for maximizing $F$ sequentially by the greedy ED rule (4). Dividing both sides of (5) by $1-1 / e$, we can upper-bound $\gamma</em>}$ by $(1-1 / e)^{-1} \mathrm{I}\left(\boldsymbol{y<em T="T">{A</em>\right)$ is near-optimal, we use it in order to show that
$\gamma_{T}$ is "near-greedy". As noted in Section 2, the ED rule does not depend on observations $y_{t}$ and can be run without evaluating $f$.}} ; f\right)$, where $A_{T}$ is constructed by the greedy procedure. Thus, somewhat counterintuitively, instead of using submodularity to prove that $F\left(A_{T</p>
<p>The importance of this greedy bound is twofold. First, it allows us to numerically compute highly problem-specific bounds on $\gamma_{T}$, which can be plugged into our results in Section 4 to obtain high-probability bounds on $R_{T}$. This being a laborious procedure, one would prefer a priori bounds for $\gamma_{T}$ in practice which are simple analytical expressions of $T$ and parameters of $k$. In this section, we sketch a general procedure for obtaining such expressions, instantiating them for a number of commonly used covariance functions, once more relying crucially on the greedy ED rule upper bound. Suppose that $D$ is finite for now, and let $\boldsymbol{f}=[f(\boldsymbol{x})]<em D="D">{\boldsymbol{x} \in D}, \boldsymbol{K}</em>\right)\right]}=\left[k\left(\boldsymbol{x}, \boldsymbol{x}^{\prime<em t="t">{\boldsymbol{x}, \boldsymbol{x}^{\prime} \in D}$. Sampling $f$ at $\boldsymbol{x}</em>}$, we obtain $y_{t} \sim N\left(\boldsymbol{v<em t="t">{t}^{T} \boldsymbol{f}, \sigma^{2}\right)$, where $\boldsymbol{v}</em>} \in \mathbb{R}^{|D|}$ is the indicator vector associated with $\boldsymbol{x<em t="t">{t}$. We can upper-bound the greedy maximum once more, by relaxing this constraint to $\left|\boldsymbol{v}</em>}\right|=1$ in round $t$ of the sequential method. For this relaxed greedy procedure, all $\boldsymbol{v<em D="D">{t}$ are leading eigenvectors of $\boldsymbol{K}</em>}$, since successive covariance matrices of $P\left(\boldsymbol{f} \mid \boldsymbol{y<em D="D">{t-1}\right)$ share their eigenbasis with $\boldsymbol{K}</em>$ :}$, while eigenvalues are damped according to how many times the corresponding eigenvector is selected. We can upper-bound the information gain by considering the worst-case allocation of $T$ samples to the $\min {T,|D|}$ leading eigenvectors of $\boldsymbol{K}_{D</p>
<p>$$
\gamma_{T} \leq \frac{1 / 2}{1-e^{-1}} \max <em t="t">{\left(m</em>\right)
$$}\right)} \sum_{t=1}^{|D|} \log \left(1+\sigma^{-2} m_{t} \hat{\lambda}_{t</p>
<p>subject to $\sum_{t} m_{t}=T$, and $\operatorname{spec}\left(\boldsymbol{K}<em 1="1">{D}\right)=\left{\hat{\lambda}</em> \geq\right.$ $\ldots}$. We can split the sum into two parts in order to obtain a bound to leading order. The following Theorem captures this intuition:} \geq \hat{\lambda}_{2</p>
<p>Theorem 4 For any $T \in \mathbb{N}$ and any $T_{*}=1, \ldots, T$ :</p>
<p>$$
\gamma_{T} \leq \mathcal{O}\left(\sigma^{-2}\left[B\left(T_{<em>}\right) T+T_{</em>}\left(\log n_{T} T\right)\right]\right)
$$</p>
<p>where $n_{T}=\sum_{t=1}^{|D|} \hat{\lambda}<em>{t}$ and $B\left(T</em>{<em>}\right)=\sum_{t=T_{</em>}+1}^{|D|} \hat{\lambda}<em>{t}$.
Therefore, if for some $T</em>{<em>}=o(T)$ the first $T_{</em>}$ eigenvalues carry most of the total mass $n_{T}$, the information gain will be small. The more rapidly the spectrum of $\boldsymbol{K}<em T="T">{D}$ decays, the slower the growth of $\gamma</em>$. Figure 3 illustrates this intuition.</p>
<h3>5.2. Bounds for Common Kernels</h3>
<p>In this section we bound $\gamma_{T}$ for a range of commonly used covariance functions: finite dimensional linear, Squared Exponential and Matérn kernels. Together with our results in Section 4, these imply sublinear regret bounds for GP-UCB in all cases.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3. Spectral decay (left) and information gain bound (right) for independent (diagonal), linear, squared exponential and Matérn kernels ( $\nu=2.5$.) with equal trace.</p>
<p>Finite dimensional linear kernels have the form $k\left(\boldsymbol{x}, \boldsymbol{x}^{\prime}\right)=\boldsymbol{x}^{T} \boldsymbol{x}^{\prime}$. GPs with this kernel correspond to random linear functions $f(\boldsymbol{x})=\boldsymbol{w}^{T} \boldsymbol{x}, \boldsymbol{w} \sim N(\mathbf{0}, \boldsymbol{I})$.</p>
<p>The Squared Exponential kernel is $k\left(\boldsymbol{x}, \boldsymbol{x}^{\prime}\right)=$ $\exp \left(-\left(2 l^{2}\right)^{-1}\left|\boldsymbol{x}-\boldsymbol{x}^{\prime}\right|^{2}\right), l$ a lengthscale parameter. Sample functions are differentiable to any order almost surely (Rasmussen \&amp; Williams, 2006).</p>
<p>The Matérn kernel is given by $k\left(\boldsymbol{x}, \boldsymbol{x}^{\prime}\right)=$ $\left(2^{1-\nu} / \Gamma(\nu)\right) r^{\nu} B_{\nu}(r), r=(\sqrt{2 \nu} / l)\left|\boldsymbol{x}-\boldsymbol{x}^{\prime}\right|$, where $\nu$ controls the smoothness of sample paths (the smaller, the rougher) and $B_{\nu}$ is a modified Bessel function. Note that as $\nu \rightarrow \infty$, appropriately rescaled Matérn kernels converge to the Squared Exponential kernel.</p>
<p>Figure 4 shows random functions drawn from GP distributions with the above kernels.</p>
<p>Theorem 5 Let $D \subset \mathbb{R}^{d}$ be compact and convex, $d \in$ $\mathbb{N}$. Assume the kernel function satisfies $k\left(\boldsymbol{x}, \boldsymbol{x}^{\prime}\right) \leq 1$.</p>
<ol>
<li>Finite spectrum. For the d-dimensional Bayesian linear regression case: $\gamma_{T}=\mathcal{O}(d \log T)$.</li>
<li>Exponential spectral decay. For the Squared Exponential kernel: $\gamma_{T}=\mathcal{O}\left((\log T)^{d+1}\right)$.</li>
<li>Power law spectral decay. For Matérn kernels with $\nu&gt;1: \gamma_{T}=\mathcal{O}\left(T^{d(d+1) /(2 \nu+d(d+1))}(\log T)\right)$.</li>
</ol>
<p>A proof of Theorem 5 is given in the Appendix, , we only sketch the idea here. $\gamma_{T}$ is bounded by Theorem 4 in terms the eigendecay of the kernel matrix $\boldsymbol{K}<em T="T">{D}$. If $D$ is infinite or very large, we can use the operator spectrum of $k\left(\boldsymbol{x}, \boldsymbol{x}^{\prime}\right)$, which likewise decays rapidly. For the kernels of interest here, asymptotic expressions for the operator eigenvalues are given in Seeger et al. (2008), who derived bounds on the information gain for fixed and random designs (in contrast to the worst-case information gain considered here, which is substantially more challenging to bound). The main challenge in the proof is to ensure
the existence of discretizations $D</em>$ in Theorem 4 are close to corresponding operator spectra tail sums.} \subset D$, dense in the limit, for which tail sums $B\left(T_{*}\right) / n_{T</p>
<p>Together with Theorems 2 and 3, this result guarantees sublinear regret of GP-UCB for any dimension (see Figure 1). For the Squared Exponential kernel, the dimension $d$ appears as exponent of $\log T$ only, so that the regret grows at most as $\mathcal{O}^{*}\left(\sqrt{T}(\log T)^{\frac{d+1}{2}}\right)$ - the high degree of smoothness of the sample paths effectively combats the curse of dimensionality.</p>
<h2>6. Experiments</h2>
<p>We compare GP-UCB with heuristics such as the Expected Improvement (EI) and Most Probable Improvement (MPI), and with naive methods which choose points of maximum mean or variance only, both on synthetic and real sensor network data.</p>
<p>For synthetic data, we sample random functions from a squared exponential kernel with lengthscale parameter 0.2 . The sampling noise variance $\sigma^{2}$ was set to 0.025 or $5 \%$ of the signal variance. Our decision set $D=[0,1]$ is uniformly discretized into 1000 points. We run each algorithm for $T=1000$ iterations with $\delta=0.1$, averaging over 30 trials (samples from the kernel). While the choice of $\beta_{t}$ as recommended by Theorem 1 leads to competitive performance of GP-UCB, we find (using cross-validation) that the algorithm is improved by scaling $\beta_{t}$ down by a factor 5 . Note that we did not optimize constants in our regret bounds.</p>
<p>Next, we use temperature data collected from 46 sensors deployed at Intel Research Berkeley over 5 days at 1 minute intervals, pertaining to the example in Section 2. We take the first two-thirds of the data set to compute the empirical covariance of the sensor readings, and use it as the kernel matrix. The functions $f$ for optimization consist of one set of observations from all the sensors taken from the remaining third of the</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4. Sample functions drawn from a GP with linear, squared exponential and Matérn kernels (ν = 2.5.)</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5. Comparison of performance: GP-UCB and various heuristics on synthetic (a), and sensor network data (b, c).</p>
<p>Data set, and the results (for T = 46, σ² = 0.5 or 5% noise, δ = 0.1) were averaged over 2000 possible choices of the objective function.</p>
<p>Lastly, we take data from traffic sensors deployed along the highway I-880 South in California. The goal was to find the point of minimum speed in order to identify the most congested portion of the highway; we used traffic speed data for all working days from 6 AM to 11 AM for one month, from 357 sensors. We again use the covariance matrix from two-thirds of the data set as kernel matrix, and test on the other third. The results (for T = 357, σ² = 4.78 or 5% noise, δ = 0.1) were averaged over 900 runs.</p>
<p>Figure 5 compares the mean average regret incurred by the different heuristics and the GP-UCB algorithm on synthetic and real data. For temperature data, the GP-UCB algorithm and EI heuristic clearly outperform the others, and do not exhibit significant difference between each other. On synthetic and traffic data MPI does equally well. In summary, GP-UCB performs at least on par with the existing approaches which are not equipped with regret bounds.</p>
<h2>7. Conclusions</h2>
<p>We prove the first sublinear regret bounds for GP optimization with commonly used kernels (see Figure 1), both for f sampled from a known GP and f of low RKHS norm. We analyze GP-UCB, an intuitive, Bayesian upper confidence bound based sampling rule. Our regret bounds crucially depend on the information gain due to sampling, establishing a novel connection between bandit optimization and experimental design. We bound the information gain in terms of the kernel spectrum, providing a general methodology for obtaining regret bounds with kernels of interest. Our experiments on real sensor network data indicate that GP-UCB performs at least on par with competing criteria for GP optimization, for which no regret bounds are known at present. Our results provide an interesting step towards understanding exploration–exploitation tradeoffs with complex utility functions.</p>
<h2>Acknowledgements</h2>
<p>We thank Marcus Hutter for insightful comments on an earlier version of this paper. This research was partially supported by ONR grant N00014-09-1-1044, NSF grant CNS-0932392, a gift from Microsoft Corporation and the Excellence Initiative of the German research foundation (DFG).</p>
<h2>References</h2>
<ul>
<li>Abernethy et al. (2008) Abernethy, J., Hazan, E., and Rakhlin, A. An efficient algorithm for linear bandit optimization, 2008. COLT.</li>
<li>Auer (1991) Auer, P. Using confidence bounds for exploitation-exploration trade-offs. JMLR, 3:397–422, 2002.</li>
<li>Auer et al. (1991) Auer, P., Cesa-Bianchi, N., and Fischer, P. Finite-time</li>
</ul>
<p>analysis of the multiarmed bandit problem. Mach. Learn., 47(2-3):235-256, 2002.</p>
<p>Brochu, E., Cora, M., and de Freitas, N. A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning. In TR-2009-23, UBC, 2009.</p>
<p>Bubeck, S., Munos, R., Stoltz, G., and Szepesvári, C. Online optimization in X-armed bandits. In NIPS, 2008.</p>
<p>Chaloner, K. and Verdinelli, I. Bayesian experimental design: A review. Stat. Sci., 10(3):273-304, 1995.</p>
<p>Cover, T. M. and Thomas, J. A. Elements of Information Theory. Wiley Interscience, 1991.</p>
<p>Dani, V., Hayes, T. P., and Kakade, S. The price of bandit information for online optimization. In NIPS, 2007.</p>
<p>Dani, V., Hayes, T. P., and Kakade, S. M. Stochastic linear optimization under bandit feedback. In COLT, 2008.</p>
<p>Dorard, L., Glowacka, D., and Shawe-Taylor, J. Gaussian process modelling of dependencies in multi-armed bandit problems. In Int. Symp. Op. Res., 2009.</p>
<p>Freedman, D. A. On tail probabilities for martingales. Ann. Prob., 3(1):100-118, 1975.</p>
<p>Ghosal, S. and Roy, A. Posterior consistency of Gaussian process prior for nonparametric binary regression. Ann. Stat., 34(5):2413-2429, 2006.</p>
<p>Grünewälder, S., Audibert, J-Y., Opper, M., and ShaweTaylor, J. Regret bounds for gaussian process bandit problems. In AISTATS, 2010.</p>
<p>Huang, D., Allen, T. T., Notz, W. I., and Zeng, N. Global optimization of stochastic black-box systems via sequential kriging meta-models. J Glob. Opt., 34:441-466, 2006.</p>
<p>Jones, D. R., Schonlau, M., and Welch, W. J. Efficient global optimization of expensive black-box functions. $J$ Glob. Opti., 13:455-492, 1998.</p>
<p>Kleinberg, R., Slivkins, A., and Upfal, E. Multi-armed bandits in metric spaces. In STOC, pp. 681-690, 2008.</p>
<p>Ko, C., Lee, J., and Queyranne, M. An exact algorithm for maximum entropy sampling. Ops Res, 43(4):684-691, 1995.</p>
<p>Kocsis, L. and Szepesvári, C. Bandit based monte-carlo planning. In ECML, 2006.</p>
<p>Krause, A. and Guestrin, C. Near-optimal nonmyopic value of information in graphical models. In UAI, 2005.</p>
<p>Lizotte, D., Wang, T., Bowling, M., and Schuurmans, D. Automatic gait optimization with Gaussian process regression. In IJCAI, pp. 944-949, 2007.</p>
<p>McDiarmid, C. Concentration. In Probabilistiic Methods for Algorithmic Discrete Mathematics. Springer, 1998.</p>
<p>Mockus, J. Bayesian Approach to Global Optimization. Kluwer Academic Publishers, 1989.</p>
<p>Mockus, J., Tiesis, V., and Zilinskas, A. Toward Global Optimization, volume 2, chapter Bayesian Methods for Seeking the Extremum, pp. 117-128. 1978.</p>
<p>Nemhauser, G., Wolsey, L., and Fisher, M. An analysis of the approximations for maximizing submodular set functions. Math. Prog., 14:265-294, 1978.</p>
<p>Pandey, S. and Olston, C. Handling advertisements of unknown quality in search advertising. In NIPS. 2007.</p>
<p>Rasmussen, C. E. and Williams, C. K. I. Gaussian Processes for Machine Learning. MIT Press, 2006.</p>
<p>Robbins, H. Some aspects of the sequential design of experiments. Bul. Am. Math. Soc., 58:527-535, 1952.</p>
<p>Rusmevichientong, P. and Tsitsiklis, J. N. Linearly parameterized bandits. abs/0812.3465, 2008.</p>
<p>Seeger, M. W., Kakade, S. M., and Foster, D. P. Information consistency of nonparametric Gaussian process methods. IEEE Tr. Inf. Theo., 54(5):2376-2382, 2008.</p>
<p>Shawe-Taylor, J., Williams, C., Cristianini, N., and Kandola, J. On the eigenspectrum of the Gram matrix and the generalization error of kernel-PCA. IEEE Trans. Inf. Theo., 51(7):2510-2522, 2005.</p>
<p>Srinivas, N., Krause, A., Kakade, S., and Seeger, M. Gaussian process optimization in the bandit setting: No regret and experimental design. In ICML, 2010.</p>
<p>Stein, M. Interpolation of Spatial Data: Some Theory for Kriging. Springer, 1999.</p>
<p>Vazquez, E. and Bect, J. Convergence properties of the expected improvement algorithm, 2007.</p>
<p>Wahba, G. Spline Models for Observational Data. SIAM, 1990.</p>
<h2>A. Regret Bounds for Target Function Sampled from GP</h2>
<p>In this section, we provide details for the proofs of Theorem 1 and Theorem 2. In both cases, the strategy is to show that $\left|f(\boldsymbol{x})-\mu_{t-1}(\boldsymbol{x})\right| \leq \beta_{t}^{1 / 2} \sigma_{t-1}(\boldsymbol{x})$ for all $t \in \mathbb{N}$ and all $\boldsymbol{x} \in D$, or in the infinite case, all $\boldsymbol{x}$ in a discretization of $D$ which becomes dense as $t$ gets large.</p>
<h2>A.1. Finite Decision Set</h2>
<p>We begin with the finite case, $|D|&lt;\infty$.
Lemma 5.1 Pick $\delta \in(0,1)$ and set $\beta_{t}=$ $2 \log \left(|D| \pi_{t} / \delta\right)$, where $\sum_{t \geq 1} \pi_{t}^{-1}=1, \pi_{t}&gt;0$. Then,</p>
<p>$$
\left|f(\boldsymbol{x})-\mu_{t-1}(\boldsymbol{x})\right| \leq \beta_{t}^{1 / 2} \sigma_{t-1}(\boldsymbol{x}) \quad \forall \boldsymbol{x} \in D \forall t \geq 1
$$</p>
<p>holds with probability $\geq 1-\delta$.</p>
<p>Proof Fix $t \geq 1$ and $\boldsymbol{x} \in D$. Conditioned on $\boldsymbol{y}<em 1="1">{t-1}=$ $\left(y</em>}, \ldots, y_{t-1}\right),\left{\boldsymbol{x<em t-1="t-1">{1}, \ldots, \boldsymbol{x}</em>)\right)$. Now, if $r \sim N(0,1)$, then}\right}$ are deterministic, and $f(\boldsymbol{x}) \sim N\left(\mu_{t-1}(\boldsymbol{x}), \sigma_{t-1}^{2}(\boldsymbol{x</p>
<p>$$
\begin{aligned}
\operatorname{Pr}{r&gt;c} &amp; =e^{-c^{2} / 2}(2 \pi)^{-1 / 2} \int e^{-(r-c)^{2} / 2-c(r-c)} d r \
&amp; \leq e^{-c^{2} / 2} \operatorname{Pr}{r&gt;0}=(1 / 2) e^{-c^{2} / 2}
\end{aligned}
$$</p>
<p>for $c&gt;0$, since $e^{-c(r-c)} \leq 1$ for $r \geq c$. Therefore, $\operatorname{Pr}\left{\left|f(\boldsymbol{x})-\mu_{t-1}(\boldsymbol{x})\right|&gt;\beta_{t}^{1 / 2} \sigma_{t-1}(\boldsymbol{x})\right} \leq e^{-\beta_{t} / 2}$, using $r=\left(f(\boldsymbol{x})-\mu_{t-1}(\boldsymbol{x})\right) / \sigma_{t-1}(\boldsymbol{x})$ and $c=\beta_{t}^{1 / 2}$. Applying the union bound,</p>
<p>$$
\left|f(\boldsymbol{x})-\mu_{t-1}(\boldsymbol{x})\right| \leq \beta_{t}^{1 / 2} \sigma_{t-1}(\boldsymbol{x}) \quad \forall \boldsymbol{x} \in D
$$</p>
<p>holds with probability $\geq 1-|D| e^{-\beta_{t} / 2}$. Choosing $|D| e^{-\beta_{t} / 2}=\delta / \pi_{t}$ and using the union bound for $t \in \mathbb{N}$, the statement holds. For example, we can use $\pi_{t}=\pi^{2} t^{2} / 6$.</p>
<p>Lemma 5.2 Fix $t \geq 1$. If $\left|f(\boldsymbol{x})-\mu_{t-1}(\boldsymbol{x})\right| \leq$ $\beta_{t}^{1 / 2} \sigma_{t-1}(\boldsymbol{x})$ for all $\boldsymbol{x} \in D$, then the regret $r_{t}$ is bounded by $2 \beta_{t}^{1 / 2} \sigma_{t-1}\left(\boldsymbol{x}_{t}\right)$.</p>
<p>Proof By definition of $\boldsymbol{x}<em t-1="t-1">{t}: \mu</em>}\left(\boldsymbol{x<em t="t">{t}\right)+\beta</em>}^{1 / 2} \sigma_{t-1}\left(\boldsymbol{x<em t-1="t-1">{t}\right) \geq$ $\mu</em>^{}\left(\boldsymbol{x<em>}\right)+\beta_{t}^{1 / 2} \sigma_{t-1}\left(\boldsymbol{x}^{</em>}\right) \geq f\left(\boldsymbol{x}^{*}\right)$. Therefore,</p>
<p>$$
\begin{aligned}
r_{t} &amp; =f\left(\boldsymbol{x}^{*}\right)-f\left(\boldsymbol{x}<em t="t">{t}\right) \leq \beta</em>}^{1 / 2} \sigma_{t-1}\left(\boldsymbol{x<em t-1="t-1">{t}\right)+\mu</em>}\left(\boldsymbol{x<em t="t">{t}\right)-f\left(\boldsymbol{x}</em>\right) \
&amp; \leq 2 \beta_{t}^{1 / 2} \sigma_{t-1}\left(\boldsymbol{x}_{t}\right)
\end{aligned}
$$</p>
<p>Lemma 5.3 The information gain for the points selected can be expressed in terms of the predictive variances. If $\boldsymbol{f}<em t="t">{T}=\left(f\left(\boldsymbol{x}</em>$ :}\right)\right) \in \mathbb{R}^{T</p>
<p>$$
\mathrm{I}\left(\boldsymbol{y}<em T="T">{T} ; \boldsymbol{f}</em>\right)\right)
$$}\right)=\frac{1}{2} \sum_{t=1}^{T} \log \left(1+\sigma^{-2} \sigma_{t-1}^{2}\left(\boldsymbol{x}_{t</p>
<p>Proof Recall that $\mathrm{I}\left(\boldsymbol{y}<em T="T">{T} ; \boldsymbol{f}</em>}\right)=\mathrm{H}\left(\boldsymbol{y<em T="T">{T}\right)-$ $(1 / 2) \log \left|2 \pi e \sigma^{2} \boldsymbol{I}\right|$. Now, $\mathrm{H}\left(\boldsymbol{y}</em>}\right)=\mathrm{H}\left(\boldsymbol{y<em T="T">{T-1}\right)+$ $\mathrm{H}\left(y</em>} \mid \boldsymbol{y<em T-1="T-1">{T-1}\right)=\mathrm{H}\left(\boldsymbol{y}</em>}\right)+\log \left(2 \pi e\left(\sigma^{2}+\sigma_{t-1}^{2}\left(\boldsymbol{x<em 1="1">{T}\right)\right)\right) / 2$. Here, we use that $\boldsymbol{x}</em>}, \ldots, \boldsymbol{x<em T-1="T-1">{T}$ are deterministic conditioned on $\boldsymbol{y}</em>}$, and that the conditional variance $\sigma_{T-1}^{2}\left(\boldsymbol{x<em T-1="T-1">{T}\right)$ does not depend on $\boldsymbol{y}</em>$. The result follows by induction.</p>
<p>Lemma 5.4 Pick $\delta \in(0,1)$ and let $\beta_{t}$ be defined as in Lemma 5.1. Then, the following holds with probability $\geq 1-\delta$ :</p>
<p>$$
\sum_{t=1}^{T} r_{t}^{2} \leq \beta_{T} C_{1} \mathrm{I}\left(\boldsymbol{y}<em T="T">{T} ; \boldsymbol{f}</em> \quad \forall T \geq 1
$$}\right) \leq C_{1} \beta_{T} \gamma_{T</p>
<p>where $C_{1}:=8 / \log \left(1+\sigma^{-2}\right) \geq 8 \sigma^{2}$.
Proof By Lemma 5.1 and Lemma 5.2, we have that $\left{r_{t}^{2} \leq 4 \beta_{t} \sigma_{t-1}^{2}\left(\boldsymbol{x}<em t="t">{t}\right) \forall t \geq 1\right}$ with probability $\geq 1-\delta$. Now, $\beta</em>$ is nondecreasing, so that</p>
<p>$$
\begin{aligned}
4 \beta_{t} \sigma_{t-1}^{2}\left(\boldsymbol{x}<em T="T">{t}\right) &amp; \leq 4 \beta</em>} \sigma^{2}\left(\sigma^{-2} \sigma_{t-1}^{2}\left(\boldsymbol{x<em T="T">{t}\right)\right) \
&amp; \leq 4 \beta</em>\right)\right)
\end{aligned}
$$} \sigma^{2} C_{2} \log \left(1+\sigma^{-2} \sigma_{t-1}^{2}\left(\boldsymbol{x}_{t</p>
<p>with $C_{2}=\sigma^{-2} / \log \left(1+\sigma^{-2}\right) \geq 1$, since $s^{2} \leq C_{2} \log \left(1+s^{2}\right)$ for $s \in\left[0, \sigma^{-2}\right]$, and $\sigma^{-2} \sigma_{t-1}^{2}\left(\boldsymbol{x}<em t="t">{t}\right) \leq \sigma^{-2} k\left(\boldsymbol{x}</em>}, \boldsymbol{x<em 1="1">{t}\right) \leq \sigma^{-2}$. Noting that $C</em>$, the result follows by plugging in the representation of Lemma 5.3.}=8 \sigma^{2} C_{2</p>
<p>Finally, Theorem 1 is a simple consequence of Lemma 5.4, since $R_{T}^{2} \leq T \sum_{t=1}^{T} r_{t}^{2}$ by the CauchySchwarz inequality.</p>
<h2>A.2. General Decision Set</h2>
<p>Theorem 2 extends the statement of Theorem 1 to the general case of $D \subset \mathbb{R}^{d}$ compact. We cannot expect this generalization to work without any assumptions on the kernel $k\left(\boldsymbol{x}, \boldsymbol{x}^{\prime}\right)$. For example, if $k\left(\boldsymbol{x}, \boldsymbol{x}^{\prime}\right)=e^{-\left|\boldsymbol{x}-\boldsymbol{x}^{\prime}\right|}$ (Ornstein-Uhlenbeck), while sample paths $f$ are a.s. continuous, they are still very erratic: $f$ is a.s. nondifferentiable almost everywhere, and the process comes with independent increments, a stationary variant of Brownian motion. The additional assumption on $k$ in Theorem 2 is rather mild and is satisfied by several common kernels, as discussed in Section 4.</p>
<p>Recall that the finite case proof is based on Lemma 5.1 paving the way for Lemma 5.2. However, Lemma 5.1 does not hold for infinite $D$. First, let us observe that we have confidence on all decisions actually chosen.</p>
<p>Lemma 5.5 Pick $\delta \in(0,1)$ and set $\beta_{t}=2 \log \left(\pi_{t} / \delta\right)$, where $\sum_{t \geq 1} \pi_{t}^{-1}=1, \pi_{t}&gt;0$. Then,</p>
<p>$$
\left|f\left(\boldsymbol{x}<em t-1="t-1">{t}\right)-\mu</em>}\left(\boldsymbol{x<em t="t">{t}\right)\right| \leq \beta</em>\right) \quad \forall t \geq 1
$$}^{1 / 2} \sigma_{t-1}\left(\boldsymbol{x}_{t</p>
<p>holds with probability $\geq 1-\delta$.
Proof Fix $t \geq 1$ and $\boldsymbol{x} \in D$. Conditioned on $\boldsymbol{y}<em 1="1">{t-1}=\left(y</em>}, \ldots, y_{t-1}\right),\left{\boldsymbol{x<em t-1="t-1">{1}, \ldots, \boldsymbol{x}</em>}\right}$ are deterministic, and $f(\boldsymbol{x}) \sim N\left(\mu_{t-1}(\boldsymbol{x}), \sigma_{t-1}^{2}(\boldsymbol{x})\right)$. As before, $\operatorname{Pr}\left{\left|f\left(\boldsymbol{x<em t-1="t-1">{t}\right)-\mu</em>}\left(\boldsymbol{x<em t="t">{t}\right)\right|&gt;\beta</em>}^{1 / 2} \sigma_{t-1}\left(\boldsymbol{x<em t="t">{t}\right)\right} \leq e^{-\beta</em>$, the statement holds.} / 2}$. Since $e^{-\beta_{t} / 2}=\delta / \pi_{t}$ and using the union bound for $t \in \mathbb{N</p>
<p>Purely for the sake of analysis, we use a set of discretizations $D_{t} \subset D$, where $D_{t}$ will be used at time</p>
<p>$t$ in the analysis. Essentially, we use this to obtain a valid confidence interval on $\boldsymbol{x}^{*}$. The following lemma provides a confidence bound for these subsets.</p>
<p>Lemma 5.6 Pick $\delta \in(0,1)$ and set $\beta_{t}=$ $2 \log \left(\left|D_{t}\right| \pi_{t} / \delta\right)$, where $\sum_{t \geq 1} \pi_{t}^{-1}=1, \pi_{t}&gt;0$. Then,</p>
<p>$$
\left|f(\boldsymbol{x})-\mu_{t-1}(\boldsymbol{x})\right| \leq \beta_{t}^{1 / 2} \sigma_{t-1}(\boldsymbol{x}) \quad \forall \boldsymbol{x} \in D_{t}, \forall t \geq 1
$$</p>
<p>holds with probability $\geq 1-\delta$.
Proof The proof is identical to that in Lemma 5.1, except now we use $D_{t}$ at each timestep.</p>
<p>Now by assumption and the union bound, we have that</p>
<p>$$
\operatorname{Pr}\left{\forall j, \forall \boldsymbol{x} \in D,\left|\partial f /\left(\partial x_{j}\right)\right|&lt;L\right} \geq 1-d a e^{-L^{2} / b^{2}}
$$</p>
<p>which implies that, with probability greater than $1-$ $d a e^{-L^{2} / b^{2}}$, we have that</p>
<p>$$
\forall \boldsymbol{x} \in D,\left|f(x)-f\left(x^{\prime}\right)\right| \leq L\left|x-x^{\prime}\right|_{1}
$$</p>
<p>This allows us to obtain confidence on $\boldsymbol{x}^{*}$ as follows.
Now let us choose a discretization $D_{t}$ of size $\left(\tau_{t}\right)^{d}$ so that for all $\boldsymbol{x} \in D_{t}$</p>
<p>$$
\left|\boldsymbol{x}-[\boldsymbol{x}]<em 1="1">{t}\right|</em>
$$} \leq r d / \tau_{t</p>
<p>where $[\boldsymbol{x}]<em t="t">{t}$ denotes the closest point in $D</em>$ uniformly spaced points.}$ to $\boldsymbol{x}$. A sufficient discretization has each coordinate with $\tau_{t</p>
<p>Lemma 5.7 Pick $\delta \in(0,1)$ and set $\beta_{t}=$ $2 \log \left(2 \pi_{t} / \delta\right)+4 d \log \left(d t b r \sqrt{\log (2 d a / \delta)}\right)$, where $\sum_{t \geq 1} \pi_{t}^{-1}=1, \pi_{t}&gt;0$. Let $\tau_{t}=d t^{2} b r \sqrt{\log (2 d a / \delta)}$ Let $\left[\boldsymbol{x}^{<em>}\right]<em t="t">{t}$ denotes the closest point in $D</em>^{}$ to $\boldsymbol{x</em>}$. Hence, Then,
$\left|f\left(\boldsymbol{x}^{<em>}\right)-\mu_{t-1}\left(\left[\boldsymbol{x}^{</em>}\right]<em t="t">{t}\right)\right| \leq \beta</em> \quad \forall t \geq 1$
holds with probability $\geq 1-\delta$.
Proof Using (9), we have that with probability greater than $1-\delta / 2$,}^{1 / 2} \sigma_{t-1}\left(\left[\boldsymbol{x}^{*}\right]_{t}\right)+\frac{1}{t^{2}</p>
<p>$$
\forall \boldsymbol{x} \in D,\left|f(x)-f\left(x^{\prime}\right)\right| \leq b \sqrt{\log (2 d a / \delta)}\left|x-x^{\prime}\right|_{1}
$$</p>
<p>Hence,</p>
<p>$$
\forall \boldsymbol{x} \in D_{t},\left|f(x)-f\left([x]<em t="t">{t}\right)\right| \leq r d b \sqrt{\log (2 d a / \delta)} / \tau</em>
$$</p>
<p>Now by choosing $\tau_{t}=d t^{2} b r \sqrt{\log (2 d a / \delta)}$, we have that</p>
<p>$$
\forall \boldsymbol{x} \in D_{t},\left|f(x)-f\left([x]_{t}\right)\right| \leq \frac{1}{t^{2}}
$$</p>
<p>This implies that $\left|D_{t}\right|=\left(d t^{2} b r \sqrt{\log (2 d a / \delta)}\right)^{d}$. Using $\delta / 2$ in Lemma 5.6, we can apply the confidence bound to $\left[\boldsymbol{x}^{*}\right]<em t="t">{t}$ (as this lives in $D</em>$ ) to obtain the result.</p>
<p>Now we are able to bound the regret.
Lemma 5.8 Pick $\delta \in(0,1)$ and set $\beta_{t}=$ $2 \log \left(4 \pi_{t} / \delta\right)+4 d \log \left(d t b r \sqrt{\log (4 d a / \delta)}\right)$, where $\sum_{t \geq 1} \pi_{t}^{-1}=1, \pi_{t}&gt;0$. Then, with probability greater than $1-\delta$, for all $t \in \mathbb{N}$, the regret is bounded as follows:</p>
<p>$$
r_{t} \leq 2 \beta_{t}^{1 / 2} \sigma_{t-1}\left(\boldsymbol{x}_{t}\right)+\frac{1}{t^{2}}
$$</p>
<p>Proof We use $\delta / 2$ in both Lemma 5.5 and Lemma 5.7, so that these events hold with probability greater than $1-\delta$. Note that the specification of $\beta_{t}$ in the above lemma is greater than the specification used in Lemma 5.5 (with $\delta / 2$ ), so this choice is valid.</p>
<p>By definition of $\boldsymbol{x}<em t-1="t-1">{t}: \quad \mu</em>}\left(\boldsymbol{x<em t="t">{t}\right)+\beta</em>}^{1 / 2} \sigma_{t-1}\left(\boldsymbol{x<em t-1="t-1">{t}\right) \geq$ $\mu</em>^{}\left(\left[\boldsymbol{x<em>}\right]<em t="t">{t}\right)+\beta</em>^{}^{1 / 2} \sigma_{t-1}\left(\left[\boldsymbol{x</em>}\right]<em t-1="t-1">{t}\right)$. Also, by Lemma 5.7, we have that $\mu</em>^{}\left(\left[\boldsymbol{x<em>}\right]<em t="t">{t}\right)+\beta</em>^{}^{1 / 2} \sigma_{t-1}\left(\left[\boldsymbol{x</em>}\right]_{t}\right)+1 / t^{2} \geq f\left(\boldsymbol{x}^{<em>}\right)$, which implies $\mu_{t-1}\left(\boldsymbol{x}<em t="t">{t}\right)+\beta</em>^{}^{1 / 2} \sigma_{t-1}\left(\boldsymbol{x}_{t}\right) \geq f\left(\boldsymbol{x</em>}\right)-1 / t^{2}$. Therefore,</p>
<p>$$
\begin{aligned}
r_{t} &amp; =f\left(\boldsymbol{x}^{*}\right)-f\left(\boldsymbol{x}<em t="t">{t}\right) \
&amp; \leq \beta</em>}^{1 / 2} \sigma_{t-1}\left(\boldsymbol{x<em t-1="t-1">{t}\right)+1 / t^{2}+\mu</em>}\left(\boldsymbol{x<em t="t">{t}\right)-f\left(\boldsymbol{x}</em>\right) \
&amp; \leq 2 \beta_{t}^{1 / 2} \sigma_{t-1}\left(\boldsymbol{x}_{t}\right)+1 / t^{2}
\end{aligned}
$$</p>
<p>which completes the proof.</p>
<p>Now we are ready to complete the proof of Theorem 2. As shown in the proof of Lemma 5.4, we have that with probability greater than $1-\delta$,</p>
<p>$$
\sum_{t=1}^{T} 4 \beta_{t} \sigma_{t-1}^{2}\left(\boldsymbol{x}<em 1="1">{t}\right) \leq C</em> \quad \forall T \geq 1
$$} \beta_{T} \gamma_{T</p>
<p>so that by Cauchy-Schwarz:</p>
<p>$$
\sum_{t=1}^{T} 2 \beta_{t}^{1 / 2} \sigma_{t-1}\left(\boldsymbol{x}<em 1="1">{t}\right) \leq \sqrt{C</em> \quad \forall T \geq 1
$$} T \beta_{T} \gamma_{T}</p>
<p>Hence,</p>
<p>$$
\sum_{t=1}^{T} r_{t} \leq \sqrt{C_{1} T \beta_{T} \gamma_{T}}+\pi^{2} / 6 \quad \forall T \geq 1
$$</p>
<p>(since $\sum 1 / t^{2}=\pi^{2} / 6$ ). Theorem 2 now follows.
Finally, we now discuss the additional assumption on $k$ in Theorem 2. For samples $f$ of the GP, consider partial derivatives $\partial f /\left(\partial x_{j}\right)$ of this sample path for $j=1, \ldots, d$. Theorem 5 of Ghosal \&amp; Roy (2006)</p>
<p>states that if derivatives up to fourth order exists for $\left(\boldsymbol{x}, \boldsymbol{x}^{\prime}\right) \mapsto k\left(\boldsymbol{x}, \boldsymbol{x}^{\prime}\right)$, then $f$ is almost surely continuously differentiable, with $\partial f /\left(\partial x_{j}\right)$ distributed as Gaussian processes again. Moreover, there are constants $a, b_{j}&gt;0$ such that</p>
<p>$$
\operatorname{Pr}\left{\sup <em j="j">{\boldsymbol{x} \in D}\left|\partial f /\left(\partial x</em>
$$}\right)\right|&gt;L\right} \leq a e^{-b_{j} L^{2}</p>
<p>Picking $L=\left[\log (d a 2 / \delta) / \min <em j="j">{j} b</em>\right)$.
This statement is about the joint distribution of $f(\cdot)$ and its partial derivatives w.r.t. each component. For a certain event in this sample space, all $\partial f /\left(\partial x_{j}\right)$ exist, are continuous, and the complement of (10) holds for all $j$. Theorem 5 of Ghosal \&amp; Roy (2006), together with the union bound, implies that this event has probability $\geq 1-\delta / 2$. Derivatives up to fourth order exist for the Gaussian covariance function, and for Matérn kernels with $\nu&gt;2$ (Stein, 1999).}\right]^{1 / 2}$, we have that $a e^{-b_{j} L^{2}} \leq \delta /(2 d)$ for all $j=1, \ldots, d$, so that for $K_{1}=d^{1 / 2} L$, by the mean value theorem, we have $\operatorname{Pr}\left{\left|f(\boldsymbol{x})-f\left(\boldsymbol{x}^{\prime}\right)\right| \leq K_{1}\left|\boldsymbol{x}-\boldsymbol{x}^{\prime}\right| \forall \boldsymbol{x}, \boldsymbol{x}^{\prime} \in D\right} \geq 1-\delta / 2$. Also, note that $K_{1}=\mathcal{O}\left(\left(\log \delta^{-1}\right)^{1 / 2</p>
<h2>B. Regret Bound for Target Function in RKHS</h2>
<p>In this section, we detail a proof of Theorem 3. Recall that in this setting, we do not know the generator of the target function $f$, but only a bound on its RKHS norm $|f|<em T="T">{k}$.
Recall the posterior mean function $\mu</em>}(\cdot)$ and posterior covariance function $k_{T}(\cdot, \cdot)$ from Section 2, conditioned on data $\left(\boldsymbol{x<em t="t">{t}, y</em>$ is given by}\right), t=1, \ldots, T$. It is easy to see that the RKHS norm corresponding to $k_{T</p>
<p>$$
|f|<em T="T">{k</em>=|f|}}^{2<em t="1">{k}^{2}+\sigma^{-2} \sum</em>
$$}^{T} f\left(\boldsymbol{x}_{t}\right)^{2</p>
<p>This implies that $\mathcal{H}<em k__T="k_{T">{k}(D)=\mathcal{H}</em>(D)$ for any $T$, while the RKHS inner products are different: $|f|}<em T="T">{k</em> \geq|f|}<em T="T">{k}$. Since $\left\langle f(\cdot), k</em>}(\cdot, \boldsymbol{x})\right\rangle_{k_{T}}=f(\boldsymbol{x})$ for any $f \in \mathcal{H<em T="T">{k</em>(D)$ by the reproducing property, then}</p>
<p>$$
\begin{aligned}
\left|\mu_{t}(\boldsymbol{x})-f(\boldsymbol{x})\right| &amp; \leq k_{T}\left(\boldsymbol{x}, \boldsymbol{x}\right)^{1 / 2}\left|\mu_{t}-f\right|<em T="T">{k</em> \
&amp; =\sigma_{T}(\boldsymbol{x})\left|\mu_{t}-f\right|}<em T="T">{k</em>
\end{aligned}
$$}</p>
<p>by the Cauchy-Schwarz inequality.
Compared to our other results, Theorem 3 is an agnostic statement, in that the assumptions the Bayesian UCB algorithm bases its predictions on differ from how $f$ and data $y_{t}$ are generated. First, $f$ is not drawn from a GP, but can be an arbitrary function
from $\mathcal{H}<em t="t">{k}(D)$. Second, while the UCB method assumes that the noise $\varepsilon</em>}=y_{t}-f\left(\boldsymbol{x<em t="t">{t}\right)$ is drawn independently from $N\left(0, \sigma^{2}\right)$, the true sequence of noise variables $\varepsilon</em>$. All we have to do in order to lift the proof of Theorem 1 to the agnostic setting is to establish an analogue to Lemma 5.1, by way of the following concentration result.}$ can be a uniformly bounded martingale difference sequence: $\varepsilon_{t} \leq \sigma$ for all $t \in \mathbb{N</p>
<p>Theorem 6 Let $\delta \in(0,1)$. Assume the noise variables $\varepsilon_{t}$ are uniformly bounded by $\sigma$. Define:</p>
<p>$$
\beta_{t}=2|f|<em t="t">{k}^{2}+300 \gamma</em>(t / \delta)
$$} \ln ^{3</p>
<p>Then
$\operatorname{Pr}{\forall T, \forall x \in D,\left|\mu_{T}(\boldsymbol{x})-f(\boldsymbol{x})\right| \leq \beta_{T+1}^{1 / 2} \sigma_{T}(\boldsymbol{x})} \geq 1-\delta$.</p>
<h2>B.1. Concentration of Martingales</h2>
<p>In our analysis, we use the following Bernstein-type concentration inequality for martingale differences, due to Freedman (1975) (see also Theorem 3.15 of McDiarmid 1998).</p>
<p>Theorem 7 (Freedman) Suppose $X_{1}, \ldots, X_{T}$ is a martingale difference sequence, and $b$ is an uniform upper bound on the steps $X_{i}$. Let $V$ denote the sum of conditional variances,</p>
<p>$$
V=\sum_{i=1}^{n} \operatorname{Var}\left(X_{i} \mid X_{1}, \ldots, X_{i-1}\right)
$$</p>
<p>Then, for every $a, v&gt;0$,</p>
<p>$$
\operatorname{Pr}\left{\sum X_{i} \geq a \text { and } V \leq v\right} \leq \exp \left(\frac{-a^{2}}{2 v+2 a b / 3}\right)
$$</p>
<h2>B.2. Proof of Theorem 6</h2>
<p>We will show that:</p>
<p>$$
\operatorname{Pr}\left{\forall T,\left|\mu_{T}-f\right|<em T="T">{k</em>\right} \geq 1-\delta
$$}}^{2} \leq \beta_{T+1</p>
<p>Theorem 6 then follows from (11). Recall that $\varepsilon_{t}=$ $y_{t}-f\left(\boldsymbol{x}<em T="T">{t}\right)$. We will analyze the quantity $Z</em>-f\right|}=$ $\left|\mu_{T<em T="T">{k</em>}}^{2}$, measuring the error of $\mu_{T}$ as approximation to $f$ under the RKHS norm of $\mathcal{H<em T="T">{k</em>(D)$. The following lemma provides the connection with the information gain. This lemma is important since our concentration argument is an inductive argument roughly speaking, we condition on getting concentration in the past, in order to achieve good concentration in the future.}</p>
<p>Lemma 7.1 We have that
$\sum_{t=1}^{T} \min \left{\sigma^{-2} \sigma_{t-1}^{2}\left(\boldsymbol{x}<em T="T">{t}\right), \alpha\right} \leq \frac{2 \alpha}{\log (1+\alpha)} \gamma</em>, \quad \alpha&gt;0$.</p>
<p>Proof We have that $\min {r, \alpha} \leq(\alpha / \log (1+$ $\alpha)) \log (1+r)$. The statement follows from Lemma 5.3.</p>
<p>The next lemma bounds the growth of $Z_{T}$. It is formulated in terms of normalized quantities: $\widetilde{\varepsilon}<em t="t">{t}=\varepsilon</em>} / \sigma$, $\widetilde{f}=f / \sigma, \widetilde{\mu<em t="t">{t}=\mu</em>} / \sigma, \widetilde{\sigma<em t="t">{t}=\sigma</em>} / \sigma$. Also, to ease notation, we will use $\mu_{t-1}, \sigma_{t-1}$ as shorthand for $\mu_{t-1}\left(\boldsymbol{x<em t-1="t-1">{t}\right)$, $\sigma</em>\right)$.}\left(\boldsymbol{x}_{t</p>
<p>Lemma 7.2 For all $T \in \mathbb{N}$,</p>
<p>$$
\begin{aligned}
Z_{T} \leq|f|<em t="1">{k}^{2} &amp; +2 \sum</em>}^{T} \widetilde{\varepsilon<em t-1="t-1">{t} \frac{\widetilde{\mu}</em>}-\widetilde{f}\left(\boldsymbol{x<em t-1="t-1">{t}\right)}{1+\widetilde{\sigma}</em> \
&amp; +\sum_{t=1}^{T} \widetilde{\varepsilon}}^{2}<em t-1="t-1">{t}^{2} \frac{\widetilde{\sigma}</em>
\end{aligned}
$$}^{2}}{1+\widetilde{\sigma}_{t-1}^{2}</p>
<p>Proof If $\boldsymbol{\alpha}<em t="t">{t}=\left(\boldsymbol{K}</em>}+\sigma^{2} \boldsymbol{I}\right)^{-1} \boldsymbol{y<em t="t">{t}$, then $\mu</em>}(\boldsymbol{x})=$ $\boldsymbol{\alpha<em t="t">{t}^{T} \boldsymbol{k}</em>}(\boldsymbol{x})$. Then, $\left\langle\mu_{T}, f\right\rangle_{k}=\boldsymbol{f<em T="T">{T}^{T} \boldsymbol{\alpha}</em>\right|},\left|\mu_{T<em T="T">{k}^{2}=$ $\boldsymbol{y}</em>}^{T} \boldsymbol{\alpha<em T="T">{T}-\sigma^{2}\left|\boldsymbol{\alpha}</em>}\right|^{2}$. Moreover, for $t \leq T, \mu_{T}\left(x_{t}\right)=$ $\boldsymbol{\delta<em T="T">{t}^{T} \boldsymbol{K}</em>}\left(\boldsymbol{K<em T="T">{T}+\sigma^{2} \boldsymbol{I}\right)^{-1} \boldsymbol{y}</em>-f\right|}=y_{t}-\sigma^{2} \alpha_{t}$. Since $Z_{T}=$ $\left|\mu_{T<em T="T" _leq="\leq" t="t">{k}+\sigma^{-2} \sum</em>}\left(\mu_{T}\left(\boldsymbol{x<em t="t">{t}\right)-f\left(\boldsymbol{x}</em>$, we have that}\right)\right)^{2</p>
<p>$$
\begin{aligned}
Z_{T} &amp; =|f|<em T="T">{k}^{2}-2 \boldsymbol{f}</em>}^{T} \boldsymbol{\alpha<em T="T">{T}+\boldsymbol{y}</em>}^{T} \boldsymbol{\alpha<em T="T">{T}-\sigma^{2}\left|\boldsymbol{\alpha}</em> \
&amp; +\sigma^{-2} \sum_{t=1}^{T}\left(\varepsilon_{t}-\sigma^{2} \alpha_{t}\right)^{2}=|f|}\right|^{2<em T="T">{k}^{2} \
&amp; -\boldsymbol{y}</em>}^{T}\left(\boldsymbol{K<em T="T">{T}+\sigma^{2} \boldsymbol{I}\right)^{-1} \boldsymbol{y}</em>
\end{aligned}
$$}+\sigma^{-2}\left|\boldsymbol{\varepsilon}_{T}\right|^{2</p>
<p>Now, $-\boldsymbol{y}<em T="T">{T}^{T}\left(\boldsymbol{K}</em>}+\sigma^{2} \boldsymbol{I}\right)^{-1} \boldsymbol{y<em T="T">{T} \doteq 2 \log P\left(\boldsymbol{y}</em>}\right)$, where " $\doteq$ " means that we drop determinant terms, thus concentrate on quadratic functions. Since $\log P\left(\boldsymbol{y<em t="t">{T}\right)=$ $\sum</em>} \log P\left(y_{t} \mid \boldsymbol{y<em t="t">{\leq t}\right)=\sum</em>} \log N\left(y_{t} \mid \mu_{t-1}\left(\boldsymbol{x<em t-1="t-1">{t}\right), \sigma</em>\right)$, we have that}^{2}\left(\boldsymbol{x}_{t}\right)+\right.$ $\left.\sigma^{2</p>
<p>$$
\begin{aligned}
&amp; -\boldsymbol{y}<em T="T">{T}^{T}\left(\boldsymbol{K}</em>}+\sigma^{2} \boldsymbol{I}\right)^{-1} \boldsymbol{y<em t="t">{T}=-\sum</em> \
&amp; =2 \sum_{t} \varepsilon_{t} \frac{\mu_{t-1}-f\left(\boldsymbol{x}} \frac{\left(y_{t}-\mu_{t-1}\right)^{2}}{\sigma^{2}+\sigma_{t-1}^{2}<em t-1="t-1">{t}\right)}{\sigma^{2}+\sigma</em>}^{2}}-\sum_{t} \frac{\varepsilon_{t}^{2} \widetilde{\sigma<em t-1="t-1">{t-1}^{2}}{\sigma^{2}+\sigma</em>-R
\end{aligned}
$$}^{2}</p>
<p>with $R=\sum_{t}\left(\mu_{t-1}-f\left(\boldsymbol{x}<em t-1="t-1">{t}\right)\right)^{2} /\left(\sigma^{2}+\sigma</em>\right) \geq 0$. Dropping $-R$ and changing to normalized quantities concludes the proof.}^{2</p>
<p>We now define a useful martingale difference sequence. First, it is convenient to define an "escape event" $E_{T}$ as:</p>
<p>$$
E_{T}=\mathrm{I}\left{Z_{t} \leq \beta_{t+1} \text { for all } t \leq T\right}
$$</p>
<p>where $\mathrm{I}{\cdot}$ is the indicator function. Define the random variables $M_{t}$ by</p>
<p>$$
M_{t}=2 \widetilde{\varepsilon}<em t-1="t-1">{t} E</em>} \frac{\widetilde{\mu<em t="t">{t-1}-\widetilde{f}\left(\boldsymbol{x}</em>
$$}\right)}{1+\widetilde{\sigma}_{t-1}^{2}</p>
<p>Now, since $\widetilde{\varepsilon}<em _leq="\leq" t="t">{t}$ is a martingale difference sequence with respect to the histories $\mathcal{H}</em>}$ and $M_{t} / \widetilde{\varepsilon<em _leq="\leq" t="t">{t}$ is deterministic given $\mathcal{H}</em>$ does not grow too large.}, M_{t}$ is a martingale difference sequence as well. Next, we show that with high probability, the associated martingale $\sum_{t=1}^{T} M_{t</p>
<p>Lemma 7.3 Given $\delta \in(0,1)$ and $\beta_{t}$ as defined in in Theorem 6, we have that</p>
<p>$$
\operatorname{Pr}\left{\forall T, \sum_{t=1}^{T} M_{t} \leq \beta_{T+1} / 2\right} \geq 1-\delta
$$</p>
<p>The proof is given below in Section B.3. Equipped with this lemma, we can prove Theorem 6.</p>
<p>Proof [of Theorem 6] It suffices to show that the highprobability event described in Lemma 7.3 is contained in the support of $E_{T}$ for every $T$. We prove the latter by induction on $T$.</p>
<p>By Lemma 7.2 and the definition of $\beta_{1}$, we know that $Z_{0} \leq|f|<em 1="1">{k} \leq \beta</em>=1$. Using this and Lemma 7.2:}$. Hence $E_{0}=1$ always. Now suppose the high-probability event of Lemma 7.3 holds, in particular $\sum_{t=1}^{T} M_{t} \leq \beta_{T+1} / 2$. For the inductive hypothesis, assume $E_{T-1</p>
<p>$$
\begin{aligned}
Z_{T} &amp; \leq|f|<em t="1">{k}^{2}+2 \sum</em>}^{T} \frac{\widetilde{\varepsilon<em t-1="t-1">{t}\left(\widetilde{\mu}</em>}-\widetilde{f}\left(\boldsymbol{x<em t-1="t-1">{t}\right)\right)}{1+\widetilde{\sigma}</em>}^{2}}+\sum_{t=1}^{T} \frac{\widetilde{\varepsilon<em t-1="t-1">{t}^{2} \widetilde{\sigma}</em>}^{2}}{1+\widetilde{\sigma<em k="k">{t-1}^{2}} \
&amp; =|f|</em>}^{2}+\sum_{t=1}^{T} M_{t}+\sum_{t=1}^{T} \widetilde{\varepsilon<em t-1="t-1">{t}^{2} \frac{\widetilde{\sigma}</em>}^{2}}{1+\widetilde{\sigma<em k="k">{t-1}^{2}} \
&amp; \leq|f|</em>}^{2}+\beta_{T+1} / 2+\sum_{t=1}^{T} \min \left{\widetilde{\sigma<em k="k">{t-1}^{2}, 1\right} \
&amp; \leq|f|</em>
\end{aligned}
$$}^{2}+\beta_{T+1} / 2+(2 / \log 2) \gamma_{T} \leq \beta_{T+1</p>
<p>The equality in the second step uses the inductive hypothesis. Thus we have shown $E_{T}=1$, completing the induction.</p>
<h2>B.3. Concentration</h2>
<p>What remains to be shown is Lemma 7.3. While the step sizes $\left|M_{t}\right|$ are uniformly bounded, a standard application of the Hoeffding-Azuma inequality leads to a bound of $T^{3 / 4}$, too large for our purpose. We use the more specific Theorem 7 instead, which requires to control the conditional variances rather than the marginal variances which can be much larger.
Proof [of Lemma 7.3] Let us first obtain upper bounds</p>
<p>on the step sizes of our martingale.</p>
<p>$$
\begin{aligned}
\left|M_{t}\right| &amp; =2\left|\widetilde{\varepsilon}<em t-1="t-1">{t}\right| E</em>} \frac{\left|\widetilde{\mu<em t="t">{t-1}-\widetilde{f}\left(\boldsymbol{x}</em>}\right)\right|}{1+\widetilde{\sigma<em t="t">{t-1}^{2}} \
&amp; \leq 2\left|\widetilde{\varepsilon}</em>}\right| E_{t-1} \frac{\beta_{t}^{1 / 2} \widetilde{\sigma<em t-1="t-1">{t-1}}{1+\widetilde{\sigma}</em> \
&amp; \leq 2\left|\widetilde{\varepsilon}}^{2}<em t-1="t-1">{t}\right| E</em>, 1 / 2\right}
\end{aligned}
$$} \beta_{t}^{1 / 2} \min \left{\widetilde{\sigma}_{t-1</p>
<p>where the first inequality follows from the definition of $E_{t}$. Moreover, $r /\left(1+r^{2}\right) \leq \min {r, 1 / 2}$ for $r \geq 0$. Therefore, $\left|M_{t}\right| \leq \beta_{T}^{1 / 2}$, since $\left|\widetilde{\varepsilon}<em t="t">{t}\right| \leq 1$ and $\beta</em>$ in nondecreasing. Next, we bound the sum of the conditional variances of the martingale:</p>
<p>$$
\begin{aligned}
V_{T} &amp; :=\sum_{t=1}^{T} \operatorname{Var}\left(M_{t} \mid M_{1} \ldots M_{t-1}\right) \
&amp; \leq \sum_{t=1}^{T} 4\left|\widetilde{\varepsilon}<em t-1="t-1">{t}\right|^{2} E</em>} \beta_{t} \min \left{\widetilde{\sigma<em T="T">{t-1}^{2}, 1 / 4\right} \
&amp; \leq 4 \beta</em>} \sum_{t=1}^{T} E_{t-1} \min \left{\widetilde{\sigma<em T="T">{t-1}^{2}, 1 / 4\right} \
&amp; \leq 9 \beta</em>
\end{aligned}
$$} \gamma_{T</p>
<p>In the last line, we used Lemma 7.1 with $\alpha=1 / 4$, noting that $8 \alpha / \log (1+\alpha) \leq 9$. Since we have established that the sum of conditional variances, $V_{T}$, is always bounded by $9 \beta_{T} \gamma_{T}$, we can apply Theorem 7 with parameters $a=\beta_{T+1} / 2, b=\beta_{T+1}^{1 / 2}$ and $v=9 \beta_{T} \gamma_{T}$ to get</p>
<p>$$
\begin{aligned}
&amp; \operatorname{Pr}\left{\sum_{t=1}^{T} M_{t} \geq \beta_{T+1} / 2\right} \
&amp; =\operatorname{Pr}\left{\sum_{t=1}^{T} M_{t} \geq \beta_{T+1} / 2 \text { and } V_{T} \leq 9 \beta_{T} \gamma_{T}\right} \
&amp; \leq \exp \left(\frac{-\left(\beta_{T+1} / 2\right)^{2}}{2\left(9 \beta_{T} \gamma_{T}\right)+\frac{2}{3}\left(\beta_{T+1} / 2\right) \beta_{T+1}^{1 / 2}}\right) \
&amp; =\exp \left(\frac{-\beta_{T+1}}{72 \gamma_{T}+\frac{4}{3} \beta_{T+1}^{1 / 2}}\right) \
&amp; \leq \max \left{\exp \left(\frac{-\beta_{T+1}}{144 \gamma_{T}}\right), \exp \left(\frac{-3 \beta_{T+1}^{1 / 2}}{8}\right)\right}
\end{aligned}
$$</p>
<p>Note that our choice of $\beta_{T+1}$ satisfies:</p>
<p>$$
\max \left{144 \gamma_{T} \log \left(T^{2} / \delta\right),\left((8 / 3) \log \left(T^{2} / \delta\right)\right)^{2}\right} \leq \beta_{T+1}
$$</p>
<p>Therefore, the previous probability is bounded by $\delta / T^{2}$, whereas the last inequality follows from the definition of $\beta_{T+1}$. With a final application of the union
bound:</p>
<p>$$
\begin{aligned}
&amp; \operatorname{Pr}\left{\sum_{t=1}^{T} M_{t} \geq \beta_{T+1} / 2 \text { for some } T\right} \
&amp; \leq \sum_{T \geq 1} \operatorname{Pr}\left{\sum_{t=1}^{T} M_{t} \geq \beta_{T+1} / 2\right} \
&amp; \leq \sum_{T \geq 2} \delta / T^{2} \leq \delta\left(\pi^{2} / 6-1\right) \leq \delta
\end{aligned}
$$</p>
<p>completing the proof of Lemma 7.3.</p>
<h2>C. Bounds on Information Gain</h2>
<p>In this section, we show how to bound $\gamma_{T}$, the maximum information gain after $T$ rounds, for compact $D \subset \mathbb{R}^{d}$ (assumptions of Theorem 2) and several commonly used covariance functions. In this section, we assume ${ }^{4}$ that $k(\boldsymbol{x}, \boldsymbol{x})=1$ for all $\boldsymbol{x} \in D$.</p>
<p>The plan of attack is as follows. First, we note that the argument of $\gamma_{T}, \mathrm{I}\left(\boldsymbol{y}<em A="A">{A} ; \boldsymbol{f}</em>}\right)$ is a submodular function, so $\gamma_{T}$ can be bounded by the value obtained by greedy maximization. Next, we use a discretization $D_{T} \subset D$ with $n_{T}=\left|D_{T}\right|=T^{r}$ with nearest neighbour distance $o(1)$, consider the kernel matrix $\boldsymbol{K<em T="T">{D</em>$, for which tails of the empirical spectrum can be bounded by tails of the process spectrum. We will invoke the probabilistic method for that.}} \in \mathbb{R}^{n_{T} \times n_{T}}$, and bound $\gamma_{T}$ by an expression involving the eigenvalues $\left{\lambda_{t}\right}$ of this matrix, which is done by a further relaxation of the greedy procedure. Finally, we bound this empirical expression in terms of the kernel operator eigenvalues of $k$ w.r.t. the uniform distribution on $D$. Asymptotic expressions for the latter are reviewed in Seeger et al. (2008), which we plug in to obtain our results. A key step in this argument is to ensure the existence of a discretization $D_{T</p>
<h2>C.1. Greedy Maximization and Discretization</h2>
<p>In this section, we fix $T \in \mathbb{N}$ and assume the existence of a discretization $D_{T} \subset D, n_{T}=\left|D_{T}\right|$ on the order of $T^{r}$, such that:</p>
<p>$$
\forall \boldsymbol{x} \in D \exists[\boldsymbol{x}]<em T="T">{T} \in D</em>\right)
$$}:\left|\boldsymbol{x}-[\boldsymbol{x}]_{T}\right|=\mathcal{O}\left(T^{-\tau / d</p>
<p>We come back to the choice of $D_{T}$ below. We restrict the information gain to subsets $A \subset D_{T}$ :</p>
<p>$$
\tilde{\gamma}<em A="A" D__T="D_{T" _subset="\subset">{T}=\max </em>},|A|=T} \mathrm{I}\left(\boldsymbol{y<em A="A">{A} ; \boldsymbol{f}</em>\right)
$$</p>
<p>Of course, $\tilde{\gamma}<em T="T">{T} \leq \gamma</em>$, but we can bound the slack.</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Lemma 7.4 Under the assumptions of Theorem 2, the information gain $F_{T}\left(\left{\boldsymbol{x}<em _left_123_boldsymbol_x="\left{\boldsymbol{x">{t}\right}\right)=(1 / 2) \log |\boldsymbol{I}+$ $\sigma^{-2} \boldsymbol{K}</em><em t="t">{t}\right}}|$ is uniformly Lipschitz-continuous in each component $\boldsymbol{x}</em> \in D$.</p>
<p>Proof The assumptions of Theorem 2 imply that the kernel $K\left(\boldsymbol{x}, \boldsymbol{x}^{\prime}\right)$ is continuously differentiable. The result follows from the fact that $F_{T}\left(\left{\boldsymbol{x}<em _left_123_boldsymbol_x="\left{\boldsymbol{x">{t}\right}\right)$ is continuously differentiable in the kernel matrix $\boldsymbol{K}</em>$.}_{t}\right}</p>
<p>Lemma 7.5 Let $D_{T}$ be a discretization of $D$ such that (13) holds. Under the assumptions of Theorem 2, we have that</p>
<p>$$
0 \leq \gamma_{T}-\tilde{\gamma}_{T}=\mathcal{O}\left(T^{1-\tau / d}\right)
$$</p>
<p>Proof Fix $T \in \mathbb{N}$, and let $A=\left{\boldsymbol{x}<em T="T">{1}, \ldots, \boldsymbol{x}</em>}\right}$ be a maximizer for $\gamma_{T}$. Consider neighbours $\left[\boldsymbol{x<em T="T">{t}\right]</em>$ according to (13), $[A]} \in D_{T<em t="t">{T}=\left{\left[\boldsymbol{x}</em>\right]<em T="T">{T}\right}$. Then,
$0 \leq \gamma</em>}-\tilde{\gamma<em T="T">{T} \leq \gamma</em>}-\mathrm{I}\left(\boldsymbol{y<em T="T">{[A]</em>}} ; \boldsymbol{f<em T="T">{[A]</em>\left([A]}}\right)=F_{T}(A)-F_{T<em T="T">{T}\right)$, where $F</em>}\left(\left{\boldsymbol{x<em _left_123_boldsymbol_x="\left{\boldsymbol{x">{t}\right}\right)=(1 / 2) \log \left|\boldsymbol{I}+\sigma^{-2} \boldsymbol{K}</em><em T="T">{t}\right}}\right|$. By Lemma 7.4, $F</em>}$ is uniformly Lipschitz-continuous in each component, so that $\left|\gamma_{T}-\mathrm{I}\left(\boldsymbol{y<em T="T">{[A]</em>}} ; \boldsymbol{f<em T="T">{[A]</em>\left(T \max }}\right)\right|=$ $\mathcal{O<em t="t">{t}\left|\boldsymbol{x}</em>}-\left[\boldsymbol{x<em T="T">{t}\right]</em>\right)$ by (13) and the mean value theorem.}\right|\right)=\mathcal{O}\left(T^{1-\tau / d</p>
<p>We concentrate on $\tilde{\gamma}<em D__T="D_{T">{T}$ in the sequel. Let $\boldsymbol{K}</em>\right)\right]}}=$ $\left[k\left(\boldsymbol{x}, \boldsymbol{x}^{\prime<em T="T">{\boldsymbol{x}, \boldsymbol{x}^{\prime} \in D</em>}}$ be the kernel matrix over the entire $D_{T}$, and $\boldsymbol{K<em T="T">{D</em>}}=\boldsymbol{U} \hat{\boldsymbol{\Lambda}} \boldsymbol{U}^{T}$ its eigendecomposition, with $\hat{\lambda<em 2="2">{1} \geq \hat{\lambda}</em>} \geq \cdots \geq 0$ and $\boldsymbol{U}=\left[\boldsymbol{u<em 2="2">{1} \boldsymbol{u}</em>} \ldots\right]$ orthonormal. Here, if $T&gt;n_{T}$, define $\hat{\lambda<em T="T">{t}=0$ for $t=n</em>}+1, \ldots, T$. Information gain maximization over a finite $D_{T}$ can be described in terms of a simple linear-Gaussian model over the unknown $\boldsymbol{f} \in \mathbb{R}^{n_{T}}$, with prior $P(\boldsymbol{f})=N\left(\mathbf{0}, \boldsymbol{K<em T="T">{D</em>}}\right)$ and likelihood potentials $P\left(y_{t} \mid \boldsymbol{f}\right)=N\left(\boldsymbol{v<em t="t">{t}^{T} \boldsymbol{f}, \sigma^{2}\right)$ with unit-norm features, $\left|\boldsymbol{v}</em>$ by way of two relaxations.}\right|=1$. With the following lemma, we upper-bound $\tilde{\gamma}_{T</p>
<p>Lemma 7.6 For any $T \geq 1$, we have that</p>
<p>$$
\tilde{\gamma}<em m__1="m_{1">{T} \leq \frac{1 / 2}{1-e^{-1}} \max </em>\right)
$$}, \ldots, m_{T}} \sum_{t=1}^{T} \log \left(1+\sigma^{-2} m_{t} \hat{\lambda}_{t</p>
<p>subject to $m_{t} \in \mathbb{N}, \sum_{t} m_{T}=T$, where $\hat{\lambda}<em 2="2">{1} \geq \hat{\lambda}</em>} \geq \ldots$ is the spectrum of the kernel matrix $\boldsymbol{K<em T="T">{D</em>$.}}$. Here, if $T&gt;n_{T}$, then $m_{t}=0$ for $t&gt;n_{T</p>
<p>Proof As shown by Krause \&amp; Guestrin (2005), the function $F(A)=\mathrm{I}\left(\boldsymbol{y}<em A="A">{A} ; \boldsymbol{f}\right)$ is submodular. In
the particular case considered here, this can be seen as follows: $F(A)=\mathrm{H}\left(\boldsymbol{y}</em>}\right)-\mathrm{H}\left(\boldsymbol{y<em A="A">{A} \mid \boldsymbol{f}\right)$, where the entropy $\mathrm{H}\left(\boldsymbol{y}</em>}\right)$ is a (not-necessarily monotonic) submodular function in $A$, and since the noise is conditionally independent given $\boldsymbol{f}, \mathrm{H}\left(\boldsymbol{y<em T="T">{A} \mid \boldsymbol{f}\right)$ is an additive (modular) function in $A$. Subtracting a modular function preserves submodularity, thus $F(A)$ is submodular. Furthermore, the information gain is monotonic in $A$ (i.e., $F(A) \leq F(B)$ whenever $A \subseteq B$ ) (Cover \&amp; Thomas, 1991). Thus, we can apply the result of Nemhauser et al. (1978) ${ }^{5}$ which guarantees that $\tilde{\gamma}</em>}$ is upper-bounded by $1 /(1-1 / e)$ times the value the greedy maximization algorithm attains. The latter chooses features of the form $\boldsymbol{v<em _boldsymbol_x="\boldsymbol{x">{t}=\boldsymbol{\delta}</em><em _left_123_boldsymbol_x="\left{\boldsymbol{x">{t}}=\left[\mathrm{I}</em>}=\boldsymbol{x<em t="t">{t}\right}}\right]$ in each round, $\boldsymbol{x}</em>} \in D_{T}$. We upper-bound the greedy maximum once more by relaxing these constraints to $\left|\boldsymbol{v<em 1="1">{t}\right|=1$ only. In the remainder of the proof, we concentrate on this relaxed greedy procedure. Suppose that up to round $t$, it chose $\boldsymbol{v}</em>}, \ldots, \boldsymbol{v<em t-1="t-1">{t-1}$. The posterior $P\left(\boldsymbol{f} \mid \boldsymbol{y}</em>}\right)$ has inverse covariance matrix $\boldsymbol{\Sigma<em D__T="D_{T">{t-1}^{-1}=\boldsymbol{K}</em>}}^{-1}+\sigma^{-2} \boldsymbol{V<em t-1="t-1">{t-1} \boldsymbol{V}</em>}^{T}$, $\boldsymbol{V<em 1="1">{t-1}=\left[\boldsymbol{v}</em>} \ldots \boldsymbol{v<em t-1="t-1">{t-1}\right]$, and the greedy procedure selects $\boldsymbol{v}$ so to maximize the variance $\boldsymbol{v}^{T} \boldsymbol{\Sigma}</em>} \boldsymbol{v}$ : the eigenvector corresponding to $\boldsymbol{\Sigma<em 0="0">{t-1}$ 's largest eigenvalue (by the Rayleigh-Ritz theorem). Since $\boldsymbol{\Sigma}</em>}=\boldsymbol{K<em T="T">{D</em>}}$, then $\boldsymbol{v<em 1="1">{1}=\boldsymbol{u}</em>}$. Moreover, if all $\boldsymbol{v<em D__T="D_{T">{t^{\prime}}, t^{\prime}&lt;t$, have been chosen among $\boldsymbol{U}$ 's columns, then by the inverse covariance expression just given, $\boldsymbol{K}</em>}}$ and $\boldsymbol{\Sigma<em t="t">{t-1}$ have the same eigenvectors, so that $\boldsymbol{v}</em>}$ is a column of $\boldsymbol{U}$ as well. For example, if $\boldsymbol{v<em j="j">{t}=\boldsymbol{u}</em>}$, then comparing $\boldsymbol{\Sigma<em t="t">{t-1}$ and $\boldsymbol{\Sigma}</em>}$, all eigenvalues other than the $j$-th remain the same, while the latter is shrunk. Therefore, after $T$ rounds of the relaxed greedy procedure: $\boldsymbol{v<em 1="1">{t} \in\left{\boldsymbol{u}</em>}, \ldots, \boldsymbol{u<em T="T">{\min \left{T, n</em>}\right}}\right}, t=1, \ldots, T$ : at most the leading $T$ eigenvectors of $\boldsymbol{K<em T="T">{D</em>$ has been selected, we obtain the theorem statement by a final bounding step.}}$ can have been selected (possibly multiple times). If $m_{t}$ denotes the number that the $t$-th column of $\boldsymbol{U</p>
<h2>C.2. From Empirical to Process Eigenvalues</h2>
<p>The final step will be to relate the empirical spectrum $\left{\hat{\lambda}<em t="t">{t}\right}$ to the kernel operator spectrum. Since $\log \left(1+\sigma^{-2} m</em>} \hat{\lambda<em t="t">{t}\right) \leq \sigma^{-2} m</em>} \hat{\lambda<em _123_boldsymbol_x="{\boldsymbol{x">{t}$ in Theorem 7.6, we will mainly be interested in relating the tail sums of the spectra. Let $\mu(\boldsymbol{x})=\mathcal{V}(D)^{-1} \mathbf{I}</em>)=1$, so that $k$ is Hilbert-} \in D}}$ be the uniform distribution on $D, \mathcal{V}(D)=\int_{\boldsymbol{x} \in D} d \boldsymbol{x}$, and assume that $k$ is continuous. Note that $\int k(\boldsymbol{x}, \boldsymbol{x}) \mu(\boldsymbol{x}) d \boldsymbol{x}=1$ by our assumption $k(\boldsymbol{x}, \boldsymbol{x</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Schmidt on $L_{2}(\mu)$. Then, Mercer's theorem (Wahba, 1990) states that the corresponding kernel operator has a discrete eigenspectrum $\left{\left(\lambda_{s}, \phi_{s}(\cdot)\right)\right}$, and</p>
<p>$$
k\left(\boldsymbol{x}, \boldsymbol{x}^{\prime}\right)=\sum_{s \geq 1} \lambda_{s} \phi_{s}(\boldsymbol{x}) \phi_{s}\left(\boldsymbol{x}^{\prime}\right)
$$</p>
<p>where $\lambda_{1} \geq \lambda_{2} \geq \cdots \geq 0$, and $\mathbb{E}<em s="s">{\mu}\left[\phi</em>}(\boldsymbol{x}) \phi_{t}(\boldsymbol{x})\right]=$ $\delta_{s, t}$. Moreover, $\sum_{s \geq 1} \lambda_{s}^{2}&lt;\infty$, and the expansion of $k$ converges absolutely and uniformly on $D \times$ $D$. Note that $\sum_{s \geq 1} \lambda_{s}=\sum_{s \geq 1} \lambda_{s} \mathbb{E<em s="s">{\mu}\left[\phi</em>$ for which (13) holds, and for which $\sum_{t&gt;T_{}(\boldsymbol{x})^{2}\right]=$ $\int K(\boldsymbol{x}, \boldsymbol{x}) \mu(\boldsymbol{x}) d \boldsymbol{x}=1$. In order to proceed from Theorem 7.6, we have to pick a discretization $D_{T<em>}} \hat{\lambda}<em>{t}$ is not much larger than $\sum</em>{t&gt;T_{</em>}} \lambda_{t}$. With the following lemma, we determine sizes $n_{T}$ for which such discretizations exist.</p>
<p>Lemma 7.7 Fix $T \in \mathbb{N}, \delta&gt;0$ and $\varepsilon&gt;0$. There exists a discretization $D_{T} \subset D$ of size
$n_{T}=\mathcal{V}(D)(\varepsilon / \sqrt{d})^{-d}[\log (1 / \delta)+d \log (\sqrt{d} / \varepsilon)+\log \mathcal{V}(D)]$
which fulfils the following requirements:</p>
<ul>
<li>$\varepsilon$-denseness: For any $\boldsymbol{x} \in D$, there exists $[\boldsymbol{x}]<em T="T">{T} \in$ $D</em>\right| \leq \varepsilon$.}$ such that $\left|\boldsymbol{x}-[\boldsymbol{x}]_{T</li>
<li>If $\operatorname{spec}\left(\boldsymbol{K}<em T="T">{D</em>}}\right)=\left{\hat{\lambda<em 2="2">{1} \geq \hat{\lambda}</em>$ :} \geq \ldots\right}$, then for any $T_{*}=1, \ldots, n_{T</li>
</ul>
<p>$$
n_{T}^{-1} \sum_{t=1}^{T_{<em>}} \hat{\lambda}<em t="1">{t} \geq \sum</em>^{T_{</em>}} \lambda_{t}-\delta
$$</p>
<p>Proof First, if we draw $n_{T}$ samples $\tilde{\boldsymbol{x}}<em T="T">{j} \sim \mu(\boldsymbol{x})$ independently at random, then $D</em>}=\left{\tilde{\boldsymbol{x}<em T="T">{j}\right}$ is $\varepsilon$-dense with probability $\geq 1-\delta$. Namely, cover $D$ with $N=\mathcal{V}(D)(\varepsilon / \sqrt{d})^{-d}$ hypercubes of sidelength $\varepsilon / \sqrt{d}$, within which the maximum Euclidean distance is $\varepsilon$. The probability of not hitting at least one cell is upperbounded by $N(1-1 / N)^{n</em> \geq N \log (N / \delta)$.
Now, let $S=n_{T}^{-1} \sum_{t=1}^{T_{}}$. Since $\log (1-1 / N) \leq$ $-1 / N$, this is upper-bounded by $\delta$ if $n_{T<em>}} \hat{\lambda}<em t="1">{t}$. Shawe-Taylor et al. (2005) show that $\mathbb{E}[S] \geq \sum</em>^{T_{</em>}} \lambda_{t}$. If $\mathcal{C}$ is the event $\left{D_{T} \text { is } \varepsilon\right.$-dense $}$, then $\operatorname{Pr}(\mathcal{C}) \geq 1-\delta$. Since $S \leq n_{T}^{-1} \operatorname{tr} \boldsymbol{K}<em T="T">{D</em>$ and the latter inequality holds.}}=1$ in any case, we have that $\mathbb{E}[S \mid \mathcal{C}] \geq \mathbb{E}[S]-\operatorname{Pr}\left(\mathcal{C}^{c}\right) \geq \sum_{t=1}^{T_{*}} \lambda_{t}-\delta$. By the probabilistic method, there must exist some $D_{T}$ for which $\mathcal{C</p>
<p>The following lemma, the equivalent of Theorem 4 in the context here, is a direct consequence of Lemma 7.6.</p>
<p>Lemma 7.8 Let $D_{T}$ be some discretization of $D$,
$n_{T}=\left|D_{T}\right|$. Then, for any $T_{*}=1, \ldots, \min \left{T, n_{T}\right}$ :</p>
<p>$$
\begin{aligned}
&amp; \tilde{\gamma}<em T="T" _ldots_="\ldots," r="1,">{T} \leq \frac{1 / 2}{1-e^{-1}} \max </em>\left(T_{<em>} \log \left(r n_{T} / \sigma^{2}\right)\right. \
&amp; \left.+(T-r) \sigma^{-2} \sum_{t=T_{</em>}+1}^{n_{T}} \hat{\lambda}_{t}\right)
\end{aligned}
$$</p>
<p>Proof We split the right hand side in Lemma 7.6 at $t=T_{<em>}$. Let $r=\sum_{t \leq T_{</em>}} m_{t}$. For $t \leq T_{<em>}$ : $\log \left(1+m_{t} \hat{\lambda}<em T="T">{t} / \sigma^{2}\right) \leq \log \left(r n</em>} / \sigma^{2}\right)$, since $\hat{\lambda<em T="T">{t} \leq n</em>$. For $t&gt;T_{</em>}: \log \left(1+m_{t} \hat{\lambda}<em t="t">{t} / \sigma^{2}\right) \leq m</em>} \hat{\lambda<em t="t">{t} / \sigma^{2} \leq(T-r) \hat{\lambda}</em>$.} / \sigma^{2</p>
<p>The following theorem describes our "recipe" for obtaining bounds on $\gamma_{T}$ for a particular kernel $k$, given that tail bounds on $B_{k}\left(T_{<em>}\right)=\sum_{s&gt;T_{</em>}} \lambda_{s}$ are known.</p>
<p>Theorem 8 Suppose that $D \subset \mathbb{R}^{d}$ is compact, and $k\left(\boldsymbol{x}, \boldsymbol{x}^{\prime}\right)$ is a covariance function for which the additional assumption of Theorem 2 holds. Moreover, let $B_{k}\left(T_{<em>}\right)=\sum_{s&gt;T_{</em>}} \lambda_{s}$, where $\left{\lambda_{s}\right}$ is the operator spectrum of $k$ with respect to the uniform distribution over $D$. Pick $\tau&gt;0$, and let $n_{T}=C_{4} T^{\tau}(\log T)$ with $C_{4}=2 \mathcal{V}(D)(2 \tau+1)$. Then, the following bound holds true:</p>
<p>$$
\begin{aligned}
\gamma_{T} \leq &amp; \frac{1 / 2}{1-e^{-1}} \max <em>{r=1, \ldots, T}\left(T</em>{<em>} \log \left(r n_{T} / \sigma^{2}\right)\right. \
&amp; \left.+C_{4} \sigma^{-2}(1-r / T)(\log T)\left(T^{\tau+1} B_{k}\left(T_{</em>}\right)+1\right)\right) \
&amp; +\mathcal{O}\left(T^{1-\tau / d}\right)
\end{aligned}
$$</p>
<p>for any $T_{<em>} \in\left{1, \ldots, n_{T}\right}$.
Proof Let $\varepsilon=d^{1 / 2} T^{-\tau / d}$ and $\delta=T^{-(\tau+1)}$. Lemma 7.7 provides the existence of a discretization $D_{T}$ of size $n_{T}$ which is $\varepsilon$-dense, and for which $n_{T}^{-1} \sum_{t=1}^{T_{</em>}} \hat{\lambda}<em t="1">{t} \geq \sum</em>^{T_{<em>}} \lambda_{t}-\delta$. Since $n_{T}^{-1} \sum_{t=1}^{n_{T}} \hat{\lambda}<em 1="1" _geq="\geq" t="t">{t}=1=\sum</em>$, then $\sum_{t&gt;T_{} \lambda_{t</em>}} \hat{\lambda}<em k="k">{t} \leq B</em>\right)+\delta$. The statement follows by using Lemma 7.8 with these bounds, and finally employing Lemma 7.5.}\left(T_{*</p>
<h2>C.3. Proof of Theorem 5</h2>
<p>In this section, we instantiate Theorem 8 in order to obtain bounds on $\gamma_{T}$ for Squared Exponential and Matérn kernels, results which are summarized in Theorem 5 .</p>
<h2>SQUARED EXPONENTIAL KERNEL</h2>
<p>For the Squared Exponential kernel $k, B_{k}\left(T_{*}\right)$ is given by Seeger et al. (2008). While $\mu(\boldsymbol{x})$ was Gaussian</p>
<p>there, the same decay rate holds for $\lambda_{s}$ w.r.t. uniform $\mu(\boldsymbol{x})$, while constants might change. In hindsight, it turns out that $\tau=d$ is the optimal choice for the discretization size, rendering the second term in Theorem 5 to be $\mathcal{O}(1)$, which is subdominant and will be neglected in the sequel. We have that $\lambda_{s} \leq c B^{s^{1 / d}}$ with $B&lt;1$. Following their analysis,</p>
<p>$$
B_{k}\left(T_{*}\right) \leq c(d!) \alpha^{-d} e^{-\beta} \sum_{j=0}^{d-1}(j!)^{-1} \beta^{j}
$$</p>
<p>where $\alpha=-\log B, \beta=\alpha T_{<em>}^{1 / d}$. Therefore, $B_{k}\left(T_{</em>}\right)=$ $\mathcal{O}\left(e^{-\beta} \beta^{d-1}\right), \beta=\alpha T_{<em>}^{1 / d}$.
We have to pick $T_{</em>}$ such that $e^{-\beta}$ is not much larger than $\left(T n_{T}\right)^{-1}$. Suppose that $T_{*}=\left[\log \left(T n_{T}\right) / \alpha\right]^{d}$, so that $e^{-\beta}=\left(T n_{T}\right)^{-1}, \beta=\log \left(T n_{T}\right)$. The bound becomes</p>
<p>$$
\begin{aligned}
\max <em _="*">{r=1, \ldots, T} &amp; \left(T</em>\right)\right. \
&amp; \left.\quad+\sigma^{-2}(1-r / T)\left(C_{5} \beta^{d-1}+C_{4}(\log T)\right)\right)
\end{aligned}
$$} \log \left(r n_{T} / \sigma^{2</p>
<p>with $n_{T}=C_{4} T^{d}(\log T)$. The first part dominates, so that $r=T$ and $\gamma_{T}=\mathcal{O}\left(\left[\log \left(T^{d+1}(\log T)\right)\right]^{d+1}\right)=$ $\mathcal{O}\left((\log T)^{d+1}\right)$. This should be compared with $\mathbb{E}\left[\mathrm{I}\left(\boldsymbol{y}<em T="T">{T} ; \boldsymbol{f}</em>}\right)\right]=\mathcal{O}\left((\log T)^{d+1}\right)$ given by Seeger et al. (2008), where the $\boldsymbol{x<em _left_123_boldsymbol_x="\left{\boldsymbol{x">{t}$ are drawn independently from a Gaussian base distribution. At least restricted to a compact set $D$, we obtain the same expression to leading order for $\max </em><em T="T">{t}\right}} \mathrm{I}\left(\boldsymbol{y}</em>\right)$.} ; \boldsymbol{f}_{T</p>
<h2>Matérn Kernels</h2>
<p>For Matérn kernels $k$ with roughness parameter $\nu$, $B_{k}\left(T_{<em>}\right)$ is given by Seeger et al. (2008) for the uniform base distribution $\mu(\boldsymbol{x})$ on $D$. Namely, $\lambda_{s} \leq$ $c s^{-(2 \nu+d) / d}$ for almost all $s \in \mathbb{N}$, and $B_{k}\left(T_{</em>}\right)=$ $\mathcal{O}\left(T_{<em>}^{1-(2 \nu+d) / d}\right)$. To match terms in the $\tilde{\gamma}<em>{T}$ bound, we choose $T</em>{</em>}=\left(T n_{T}\right)^{d /(2 \nu+d)}\left(\log \left(T n_{T}\right)\right)^{\kappa}$ ( $\kappa$ chosen below), so that the bound becomes</p>
<p>$$
\begin{aligned}
\max <em>{r=1, \ldots, T} &amp; \left(T</em>{<em>} \log \left(r n_{T} / \sigma^{2}\right)+\sigma^{-2}(1-r / T)\right. \
&amp; \left.\times\left(C_{5} T_{</em>}\left(\log \left(T n_{T}\right)\right)^{-\kappa(2 \nu+d) / d}+C_{4}(\log T)\right)\right) \
&amp; +\mathcal{O}\left(T^{1-\tau / d}\right)
\end{aligned}
$$</p>
<p>with $n_{T}=C_{4} T^{\tau}(\log T)$. For $\kappa=-d /(2 \nu+d)$, we obtain that the maximum over $r$ is $\mathcal{O}\left(T_{<em>} \log \left(T n_{T}\right)\right)=$ $\mathcal{O}\left(T^{(\tau+1) d /(2 \nu+d)}(\log T)\right)$. Finally, we choose $\tau=$ $2 \nu d /(2 \nu+d(d+1))$ to match this term with $\mathcal{O}\left(T^{1-\tau / d}\right)$. Plugging this in, we have $\gamma_{T}=\mathcal{O}\left(T^{1-2 \eta}(\log T)\right)$, $\eta=\frac{\nu}{2 \nu+d(d+1)}$. Together with Theorem 2 (for $\nu&gt;2$ ), we have that $R_{T}=\mathcal{O}^{</em>}\left(T^{1-\eta}\right)$ (suppressing log factors): for any $\nu&gt;2$ and any dimension $d$, the GP-</p>
<p>UCB algorithm is guaranteed to be no-regret in this case with arbitrarily high probability.</p>
<p>How does this bound compare to the bound on $\mathbb{E}\left[\mathrm{I}\left(\boldsymbol{y}<em T="T">{T} ; \boldsymbol{f}</em>}\right)\right]$ given by Seeger et al. (2008)? Here, $\gamma_{T}=$ $\mathcal{O}\left(T^{d(d+1) /(2 \nu+d(d+1))}(\log T)\right)$, while $\mathbb{E}\left[\mathrm{I}\left(\boldsymbol{y<em T="T">{T} ; \boldsymbol{f}</em>\right)$.}\right)\right]=$ $\mathcal{O}\left(T^{d /(2 \nu+d)}(\log T)^{2 \nu /(2 \nu+d)</p>
<h2>Linear Kernel</h2>
<p>For linear kernels $k\left(\boldsymbol{x}, \boldsymbol{x}^{\prime}\right)=\boldsymbol{x}^{T} \boldsymbol{x}^{\prime}, \boldsymbol{x} \in \mathbb{R}^{d}$ with $|\boldsymbol{x}| \leq$ 1 , we can bound $\gamma_{T}$ directly. Let $\boldsymbol{X}<em 1="1">{T}=\left[\boldsymbol{x}</em>} \ldots, \boldsymbol{x<em t="t">{T}\right] \in$ $\mathbb{R}^{d \times T}$ with all $\left|\boldsymbol{x}</em>\right| \leq 1$. Now,</p>
<p>$$
\begin{aligned}
\log \left|\boldsymbol{I}+\sigma^{-2} \boldsymbol{X}<em T="T">{T}^{T} \boldsymbol{X}</em>}\right| &amp; =\log \left|\boldsymbol{I}+\sigma^{-2} \boldsymbol{X<em T="T">{T} \boldsymbol{X}</em>\right| \
&amp; \leq \log \left|\boldsymbol{I}+\sigma^{-2} \boldsymbol{D}\right|
\end{aligned}
$$}^{T</p>
<p>with $\boldsymbol{D}=\operatorname{diag} \operatorname{diag}^{-1}\left(\boldsymbol{X}<em T="T">{T} \boldsymbol{X}</em>}^{T}\right)$, by Hadamard's inequality. The largest eigenvalue $\tilde{\lambda<em T="T">{1}$ of $\boldsymbol{X}</em>(T)$, so that} \boldsymbol{X}_{T}^{T}$ is $\mathcal{O</p>
<p>$$
\log \left|\boldsymbol{I}+\sigma^{-2} \boldsymbol{X}<em T="T">{T}^{T} \boldsymbol{X}</em>\right)
$$}\right| \leq d \log \left(1+\sigma^{-2} \tilde{\lambda}_{1</p>
<p>and $\gamma_{T}=\mathcal{O}(d \log T)$.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{5}$ While the result of Nemhauser et al. (1978) is stated in terms of finite sets, it extends to infinite sets as long as the greedy selection can be implemented efficiently.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5671 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5671</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5671</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-116.html">extraction-schema-116</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <p><strong>Paper ID:</strong> paper-17a6116e5bbd8b87082cbb2e795885567300c483</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/17a6116e5bbd8b87082cbb2e795885567300c483" target="_blank">Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> This work focuses on LLM sensitivity to a quintessential class of meaning-preserving design choices: prompt formatting, and presents a suite of analyses that characterize the nature of this sensitivity, including exploring the influence of particular atomic perturbations and the internal representation of particular formats.</p>
                <p><strong>Paper Abstract:</strong> As large language models (LLMs) are adopted as a fundamental component of language technologies, it is crucial to accurately characterize their performance. Because choices in prompt design can strongly influence model behavior, this design process is critical in effectively using any modern pre-trained generative language model. In this work, we focus on LLM sensitivity to a quintessential class of meaning-preserving design choices: prompt formatting. We find that several widely used open-source LLMs are extremely sensitive to subtle changes in prompt formatting in few-shot settings, with performance differences of up to 76 accuracy points when evaluated using LLaMA-2-13B. Sensitivity remains even when increasing model size, the number of few-shot examples, or performing instruction tuning. Our analysis suggests that work evaluating LLMs with prompting-based methods would benefit from reporting a range of performance across plausible prompt formats, instead of the currently-standard practice of reporting performance on a single format. We also show that format performance only weakly correlates between models, which puts into question the methodological validity of comparing models with an arbitrarily chosen, fixed prompt format. To facilitate systematic analysis we propose FormatSpread, an algorithm that rapidly evaluates a sampled set of plausible prompt formats for a given task, and reports the interval of expected performance without accessing model weights. Furthermore, we present a suite of analyses that characterize the nature of this sensitivity, including exploring the influence of particular atomic perturbations and the internal representation of particular formats.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5671.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5671.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Formatting Sensitivity (Global)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM Sensitivity to Prompt Formatting Across Tasks and Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Quantifies how semantically-equivalent but differently formatted prompts produce large variability in LLM task performance across many tasks and models, measuring spreads in accuracy across sampled prompt formats.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Various (LLaMA-2, Falcon, GPT-3.5)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>53 classification tasks from Super-NaturalInstructions</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Diverse human-written classification and multiple-choice tasks (2--4 basic fields) sampled from the Super-NaturalInstructions benchmark (53 tasks used).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Few-shot prompting (1- and 5-shot) where the formatting of the instruction and each few-shot example is varied according to a hand-crafted grammar of plausible, semantically-equivalent prompt formats (changes to separators, casing, enumeration style, spaces, newlines, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Multiple alternative formats sampled from the grammar (10+ randomly sampled formats per task in many experiments; up to 320 formats explored for some experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported distributional performance across formats; median spread 7.5 percentage points (accuracy) across models and shot choices; many tasks show much higher variation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Across experiments, observed spreads range from near 0 up to 0.76 (76 percentage points) on individual tasks (claimed maximum for LLaMA-2-13B); median spread ~0.075 (7.5 pts) across sampled formats (10 formats).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>format-dependent (could improve or reduce performance depending on chosen format)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Formatting choices act as spurious, meaning-preserving features that systematically alter model internal representations and output distributions; effects are unpredictable and can introduce biases, degeneration, or make formats appear 'good' for one model but not another.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting", 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5671.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5671.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FORMATSPREAD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>FORMATSPREAD (Bayesian optimization for prompt-format exploration)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sampling/optimization procedure (grammar + bandit search using Thompson sampling) to efficiently estimate the expected performance interval (min/max) across a user-specified set of plausible, semantically-equivalent prompt formats without access to model weights.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Model-agnostic (demonstrated with LLaMA-2 variants, Falcon, GPT-3.5)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Same Super-NaturalInstructions tasks and other selected tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Estimation of performance interval (spread) across many semantically-equivalent prompt formats under a fixed evaluation budget.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Exploration over a grammar-defined space of prompt formats; uses mini-batch evaluations as bandit arm pulls; first finds high-performing formats then low-performing formats under fixed budget E; uses Thompson sampling priors informed by original-format accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared against naive (uniform) sampling and UCB bandit sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>With budget 51,200 evaluations and 320 formats, Thompson sampling estimated spread within ~1 accuracy point of true spread (naive within ~4 points, UCB within ~11).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Thompson sampling outperformed naive sampling and UCB for efficiently approximating true spread under fixed budget (see numeric differences above).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Enables discovery of spreads up to 0.876 on some tasks (LLaMA-2-70B 1-shot reported max spread 0.876 across 53 tasks with 320 formats); Thompson sampling returned spreads within 1 percentage point of ground truth using 51,200 evals.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>N/A (method for measuring effect)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Bayesian bandit search (Thompson sampling) efficiently prioritizes promising formats, leveraging informative priors from original-format accuracy to locate extremes of the format-performance distribution with much lower cost than exhaustive evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting", 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5671.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5671.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-2-13B Extreme Spread</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large performance spread observed in LLaMA-2-13B across semantically-equivalent formats</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The authors report extremely large format sensitivity for LLaMA-2-13B, with task-level accuracy differences as high as ~76 percentage points between equivalent prompt formats in few-shot settings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Subset of Super-NaturalInstructions tasks (classification/multiple-choice)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Classification/multiple-choice few-shot tasks taken from Super-NaturalInstructions (varied tasks such as stereotype classification, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Few-shot (1- and 5-shot) prompts with alternative semantically-equivalent formatting (separator and casing changes, different enumeration styles, different spaces/newlines) sampled from grammar.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Other equivalent prompt formats sampled from grammar; the 76-pt difference is between two equivalent formats for same task.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Example extreme: up to ~76 percentage points difference in accuracy between two semantically-equivalent formats (reported as a maximum observed spread for LLaMA-2-13B).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Up to +76 percentage points (max observed spread) between two equivalent formats on a single task for LLaMA-2-13B.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>format-dependent (could either increase or decrease accuracy dramatically)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Formatting changes induce different prompt embeddings and thus alter the model's conditional distribution over outputs; sensitivity persists despite increasing model size, more few-shot examples, or instruction tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting", 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5671.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5671.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Atomic Format Changes (examples)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Single-constant atomic prompt formatting changes producing large accuracy deltas (examples from Table 2/4)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Concrete examples where changing a single formatting constant (separator, casing, enumeration punctuation) produced very large accuracy differences in LLaMA-2-7B 1-shot experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Specific Super-NaturalInstructions tasks (e.g., task280, task317, task190, task904, task320, task322, task279)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Classification/multiple-choice tasks drawn from Super-NaturalInstructions; each example shows two semantically-equivalent formats differing by a single small formatting atomic change.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>1-shot few-shot prompts where only one formatting constant is changed (e.g., removal/addition of colon, newline vs space, casing of descriptors).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>The alternative format differs by a single atomic change (examples: 'passage: { } \n answer: { }' vs 'passage { } \n answer { }', or 'COMMENT: { } ANSWER: { }' vs 'comment: { } answer: { }').</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Example (LLaMA-2-7B, 1-shot, probability ranking): task280: acc 0.043 -> 0.826 (diff 0.783); task317: 0.076 -> 0.638 (diff 0.562); task190: 0.360 -> 0.614 (diff 0.254); other examples show diffs 0.1--0.3.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Single atomic changes produced differences up to ~78.3 percentage points in specific cases (table examples); broadly, 24% of atomic changes produced >=5 point change under exact prefix matching (11% under probability ranking).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>format-dependent (single small formatting edits sometimes massively improve or degrade accuracy)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Even tiny, meaning-preserving surface variations act as spurious cues that strongly alter model decoding behavior; effects are non-monotonic and unpredictable, indicating format choices interact complexly with learned conditional distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting", 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5671.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5671.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Feature-level Impact (S1, S2, F_item, casing)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Impact of individual formatting constant choices on accuracy (Table 1 analysis)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Systematic conditioning of sampled formats on specific formatting constant choices shows some constant sets (e.g., primary separator S1 and enumeration number format F_item2) have larger individual predictive impact on accuracy than others (e.g., S2, F_item1, casing).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>31 selected Super-NaturalInstructions tasks (analysis used 1-shot LLaMA-2-7B)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Classification tasks for which 500 random formats were generated and accuracy measured per format.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>1-shot few-shot with random formats; analysis conditions on choice of a single constant (e.g., separator between descriptor and text S1, enumeration item format F_item2, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Other values for the same constant (different separators, different enumeration numbering styles, different casing options).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Median spread per-constant reported (example: median spread for C (joiner) 0.144; S1 0.132; S2 0.238; F_item1 0.176; F_item2 0.173; F_casing 0.188).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Weak differences: e.g., S1 produced weakly different distributions in 43% of tasks and strongly different in 22%; F_item2 weak 45% / strong 10%; C weak 29% / strong 1%; S2, F_item1, F_casing produced 0--3% weak/strong differences across tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>some constants can strongly alter performance while others do not independently predict accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Certain atomic constants (especially the main separator S1 and how enumeration numbers are formatted) more frequently induce systematic changes in model behavior; however, many constants do not independently predict outcomes and interact with other format choices.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>S2, F_item1, and F_casing were found not to independently predict performance differences (0% or near-0% strong/weak differences in the reported analysis).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting", 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5671.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5671.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Format Identifiability (embeddings)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt-format identifiability in model continuous prompt embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompt formats are encoded in the model's last hidden-layer prompt embeddings: a classifier trained on top principal components can identify the generating format with very high accuracy, and separability correlates with performance spread.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>31 tasks; 10 formats per task; embeddings collected for 1000 examples per format (1-shot and 5-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Classification tasks; analysis maps prompt embeddings to the format label.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Collect last pre-generation hidden states for entire prompt for different formats and reduce dimensionality with PCA, then classify format with XGBoost on top-n PCs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Using top 100 principal components, XGBoost classifier achieves >=0.98 accuracy at identifying format for all 31 tasks analyzed.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Classifier accuracy >=0.98 with top 100 PCs; correlation between classifier accuracy using top 2 PCs and spread: 0.424 (p=8.04e-6) for 1-shot, and 0.555 for 5-shot (exact prefix matching).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>N/A (diagnostic linking embedding separability to format spread)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Formats deterministically transform prompt embeddings; higher separability of format embeddings correlates with larger performance spread, suggesting formatting induces distinct internal representations that affect output distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting", 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5671.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5671.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Model Comparison Reversals</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Format-driven reversals in model performance ordering</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper shows that choosing different semantically-equivalent prompt formats can reverse which model appears better on a task, undermining fixed-format model comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Pairs including LLaMA-2-7B, LLaMA-2-13B, LLaMA-2-70B, Falcon-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>53 Super-NaturalInstructions tasks (1- and 5-shot analyses)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Model pairwise comparisons of accuracy when using different prompt formats.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Two different formats p and p' sampled from grammar; compare model ordering under p and under p'.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Different sampled semantically-equivalent formats p and p'.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Example statistic: probability that a model pair's ordering reverses (d >= 0.02) is ~0.141 for LLaMA-2-13B vs -70B and ~0.140 for LLaMA-2-7B vs Falcon-7B (i.e., ~14% chance of reversal for d=2%).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Reversal probabilities reported around 0.14 for selected model pairs; many reversals were statistically significant (paired tests) in both directions in a substantial fraction of trials (e.g., 76% and 47% for two comparisons mentioned).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>format-dependent (formats can reverse which model appears better)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Formats interact with each model's learned conditional distributions differently, so formats that are high-performing for one model may be low-performing for another; therefore single-format model comparisons can be confounded.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting", 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5671.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e5671.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5 Observations</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Formatting sensitivity observed on GPT-3.5-Turbo (API)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>FORMATSPREAD applied to GPT-3.5-Turbo (API) across 320 formats and 53 tasks shows non-trivial median spread and large maximum spreads, demonstrating sensitivity even for API models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>53 Super-NaturalInstructions tasks (320 formats sampled)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Classification tasks from Super-NaturalInstructions with formatting variations; evaluated using exact prefix matching when full logits unavailable.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>1-shot few-shot prompts varied by grammar; 320 formats per task evaluated under a budget.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Median spread = 0.064 (6.4 percentage points) across 53 tasks (exact prefix matching); maximum observed spread up to 0.562 for some tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Median spread 6.4 pts; max observed 56.2 pts on at least one task across 320 formats.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>format-dependent (both increases and decreases observed)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Even API-restricted, instruction-tuned models show sensitivity to formatting; FORMATSPREAD can be applied without model weights to reveal spread.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting", 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5671.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e5671.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Non-elimination by scale/shots/instruct-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Formatting sensitivity persists despite increases in model size, shots, and instruction tuning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The authors find that increasing model size, the number of few-shot examples, or applying instruction tuning does not eliminate formatting-induced performance variance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-2 family, Falcon family (examples LLaMA-2-7B/13B/70B, Falcon-7B/-7B-Instruct)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>53 Super-NaturalInstructions tasks (and additional experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Classification/multiple-choice tasks; experiments compare 1- and 5-shot, multiple model sizes, and instruction-tuned variants.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Varied formats drawn from grammar under fixed number of shots and model variant comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Spread remains significant after increasing model size (LLaMA-2-70B still shows median spread 0.171 in one experiment), after instruction tuning (Falcon-7B vs Falcon-7B-Instruct differences persist), and when increasing few-shot examples (1->5 shot does not eliminate spread).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Example: LLaMA-2-70B 1-shot median spread 0.171 (mean 0.221, std 0.200) across 53 tasks with 320 formats; 5-shot LLaMA-2-70B had 25% of tasks with spread >=0.310 and max 0.841.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>format-dependent (persistence of spread regardless of scaling/tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Scaling and instruction tuning change the model but do not remove reliance on spurious surface cues encoded by formats; format-induced differences are not simply a small-model artifact.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting", 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5671.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e5671.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Degeneration / Centered Mass</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of formatting on degeneration (model failing to output any valid option)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompt formatting affects not just accuracy but the frequency a model outputs a valid option at all; strong correlation reported between accuracy and 'centered mass' (ratio of outputs matching any valid option).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-2 and Falcon families</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>53 Super-NaturalInstructions tasks (format variations)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Classification tasks where generation can 'degenerate' (model fails to produce any valid option under prefix matching).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Varied prompt formats under exact prefix matching metric; centered mass measures whether any valid option was produced (regardless of correctness).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>High correlations between exact-match accuracy and centered mass: e.g., LLaMA-2-7B 1-shot r=0.702 (p=5.1e-77); Falcon-7B 1-shot r=0.936 (p~7.1e-233); Falcon-7B-Instruct 1-shot r=0.962.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Very strong correlations (up to r~0.96) show that formatting can dramatically change whether the model produces any valid answer at all; associated spread for exact prefix matching is substantially larger than for ranking accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>format-dependent (formatting can increase degeneration, reducing probability of producing any valid option)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Formatting interacts with the model's decoding behavior so that some formats induce degeneration (no valid output), explaining larger spreads when using exact prefix matching vs ranking metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting", 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Super-NaturalInstructions: Generalization via declarative instructions on 1600+ NLP tasks <em>(Rating: 2)</em></li>
                <li>Large language models are human-level prompt engineers <em>(Rating: 2)</em></li>
                <li>Automatic prompt optimization with "gradient descent" and beam search <em>(Rating: 2)</em></li>
                <li>Making pre-trained language models better few-shot learners <em>(Rating: 1)</em></li>
                <li>Jailbroken: How does llm safety training fail? <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5671",
    "paper_id": "paper-17a6116e5bbd8b87082cbb2e795885567300c483",
    "extraction_schema_id": "extraction-schema-116",
    "extracted_data": [
        {
            "name_short": "Formatting Sensitivity (Global)",
            "name_full": "LLM Sensitivity to Prompt Formatting Across Tasks and Models",
            "brief_description": "Quantifies how semantically-equivalent but differently formatted prompts produce large variability in LLM task performance across many tasks and models, measuring spreads in accuracy across sampled prompt formats.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Various (LLaMA-2, Falcon, GPT-3.5)",
            "model_size": null,
            "task_name": "53 classification tasks from Super-NaturalInstructions",
            "task_description": "Diverse human-written classification and multiple-choice tasks (2--4 basic fields) sampled from the Super-NaturalInstructions benchmark (53 tasks used).",
            "problem_format": "Few-shot prompting (1- and 5-shot) where the formatting of the instruction and each few-shot example is varied according to a hand-crafted grammar of plausible, semantically-equivalent prompt formats (changes to separators, casing, enumeration style, spaces, newlines, etc.).",
            "comparison_format": "Multiple alternative formats sampled from the grammar (10+ randomly sampled formats per task in many experiments; up to 320 formats explored for some experiments).",
            "performance": "Reported distributional performance across formats; median spread 7.5 percentage points (accuracy) across models and shot choices; many tasks show much higher variation.",
            "performance_comparison": null,
            "format_effect_size": "Across experiments, observed spreads range from near 0 up to 0.76 (76 percentage points) on individual tasks (claimed maximum for LLaMA-2-13B); median spread ~0.075 (7.5 pts) across sampled formats (10 formats).",
            "format_effect_direction": "format-dependent (could improve or reduce performance depending on chosen format)",
            "explanation_or_hypothesis": "Formatting choices act as spurious, meaning-preserving features that systematically alter model internal representations and output distributions; effects are unpredictable and can introduce biases, degeneration, or make formats appear 'good' for one model but not another.",
            "counterexample_or_null_result": null,
            "uuid": "e5671.0",
            "source_info": {
                "paper_title": "Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "FORMATSPREAD",
            "name_full": "FORMATSPREAD (Bayesian optimization for prompt-format exploration)",
            "brief_description": "A sampling/optimization procedure (grammar + bandit search using Thompson sampling) to efficiently estimate the expected performance interval (min/max) across a user-specified set of plausible, semantically-equivalent prompt formats without access to model weights.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Model-agnostic (demonstrated with LLaMA-2 variants, Falcon, GPT-3.5)",
            "model_size": null,
            "task_name": "Same Super-NaturalInstructions tasks and other selected tasks",
            "task_description": "Estimation of performance interval (spread) across many semantically-equivalent prompt formats under a fixed evaluation budget.",
            "problem_format": "Exploration over a grammar-defined space of prompt formats; uses mini-batch evaluations as bandit arm pulls; first finds high-performing formats then low-performing formats under fixed budget E; uses Thompson sampling priors informed by original-format accuracy.",
            "comparison_format": "Compared against naive (uniform) sampling and UCB bandit sampling.",
            "performance": "With budget 51,200 evaluations and 320 formats, Thompson sampling estimated spread within ~1 accuracy point of true spread (naive within ~4 points, UCB within ~11).",
            "performance_comparison": "Thompson sampling outperformed naive sampling and UCB for efficiently approximating true spread under fixed budget (see numeric differences above).",
            "format_effect_size": "Enables discovery of spreads up to 0.876 on some tasks (LLaMA-2-70B 1-shot reported max spread 0.876 across 53 tasks with 320 formats); Thompson sampling returned spreads within 1 percentage point of ground truth using 51,200 evals.",
            "format_effect_direction": "N/A (method for measuring effect)",
            "explanation_or_hypothesis": "Bayesian bandit search (Thompson sampling) efficiently prioritizes promising formats, leveraging informative priors from original-format accuracy to locate extremes of the format-performance distribution with much lower cost than exhaustive evaluation.",
            "counterexample_or_null_result": null,
            "uuid": "e5671.1",
            "source_info": {
                "paper_title": "Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "LLaMA-2-13B Extreme Spread",
            "name_full": "Large performance spread observed in LLaMA-2-13B across semantically-equivalent formats",
            "brief_description": "The authors report extremely large format sensitivity for LLaMA-2-13B, with task-level accuracy differences as high as ~76 percentage points between equivalent prompt formats in few-shot settings.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA-2",
            "model_size": "13B",
            "task_name": "Subset of Super-NaturalInstructions tasks (classification/multiple-choice)",
            "task_description": "Classification/multiple-choice few-shot tasks taken from Super-NaturalInstructions (varied tasks such as stereotype classification, etc.).",
            "problem_format": "Few-shot (1- and 5-shot) prompts with alternative semantically-equivalent formatting (separator and casing changes, different enumeration styles, different spaces/newlines) sampled from grammar.",
            "comparison_format": "Other equivalent prompt formats sampled from grammar; the 76-pt difference is between two equivalent formats for same task.",
            "performance": "Example extreme: up to ~76 percentage points difference in accuracy between two semantically-equivalent formats (reported as a maximum observed spread for LLaMA-2-13B).",
            "performance_comparison": null,
            "format_effect_size": "Up to +76 percentage points (max observed spread) between two equivalent formats on a single task for LLaMA-2-13B.",
            "format_effect_direction": "format-dependent (could either increase or decrease accuracy dramatically)",
            "explanation_or_hypothesis": "Formatting changes induce different prompt embeddings and thus alter the model's conditional distribution over outputs; sensitivity persists despite increasing model size, more few-shot examples, or instruction tuning.",
            "counterexample_or_null_result": null,
            "uuid": "e5671.2",
            "source_info": {
                "paper_title": "Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Atomic Format Changes (examples)",
            "name_full": "Single-constant atomic prompt formatting changes producing large accuracy deltas (examples from Table 2/4)",
            "brief_description": "Concrete examples where changing a single formatting constant (separator, casing, enumeration punctuation) produced very large accuracy differences in LLaMA-2-7B 1-shot experiments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA-2",
            "model_size": "7B",
            "task_name": "Specific Super-NaturalInstructions tasks (e.g., task280, task317, task190, task904, task320, task322, task279)",
            "task_description": "Classification/multiple-choice tasks drawn from Super-NaturalInstructions; each example shows two semantically-equivalent formats differing by a single small formatting atomic change.",
            "problem_format": "1-shot few-shot prompts where only one formatting constant is changed (e.g., removal/addition of colon, newline vs space, casing of descriptors).",
            "comparison_format": "The alternative format differs by a single atomic change (examples: 'passage: { } \\n answer: { }' vs 'passage { } \\n answer { }', or 'COMMENT: { } ANSWER: { }' vs 'comment: { } answer: { }').",
            "performance": "Example (LLaMA-2-7B, 1-shot, probability ranking): task280: acc 0.043 -&gt; 0.826 (diff 0.783); task317: 0.076 -&gt; 0.638 (diff 0.562); task190: 0.360 -&gt; 0.614 (diff 0.254); other examples show diffs 0.1--0.3.",
            "performance_comparison": null,
            "format_effect_size": "Single atomic changes produced differences up to ~78.3 percentage points in specific cases (table examples); broadly, 24% of atomic changes produced &gt;=5 point change under exact prefix matching (11% under probability ranking).",
            "format_effect_direction": "format-dependent (single small formatting edits sometimes massively improve or degrade accuracy)",
            "explanation_or_hypothesis": "Even tiny, meaning-preserving surface variations act as spurious cues that strongly alter model decoding behavior; effects are non-monotonic and unpredictable, indicating format choices interact complexly with learned conditional distributions.",
            "counterexample_or_null_result": null,
            "uuid": "e5671.3",
            "source_info": {
                "paper_title": "Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Feature-level Impact (S1, S2, F_item, casing)",
            "name_full": "Impact of individual formatting constant choices on accuracy (Table 1 analysis)",
            "brief_description": "Systematic conditioning of sampled formats on specific formatting constant choices shows some constant sets (e.g., primary separator S1 and enumeration number format F_item2) have larger individual predictive impact on accuracy than others (e.g., S2, F_item1, casing).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA-2",
            "model_size": "7B",
            "task_name": "31 selected Super-NaturalInstructions tasks (analysis used 1-shot LLaMA-2-7B)",
            "task_description": "Classification tasks for which 500 random formats were generated and accuracy measured per format.",
            "problem_format": "1-shot few-shot with random formats; analysis conditions on choice of a single constant (e.g., separator between descriptor and text S1, enumeration item format F_item2, etc.).",
            "comparison_format": "Other values for the same constant (different separators, different enumeration numbering styles, different casing options).",
            "performance": "Median spread per-constant reported (example: median spread for C (joiner) 0.144; S1 0.132; S2 0.238; F_item1 0.176; F_item2 0.173; F_casing 0.188).",
            "performance_comparison": null,
            "format_effect_size": "Weak differences: e.g., S1 produced weakly different distributions in 43% of tasks and strongly different in 22%; F_item2 weak 45% / strong 10%; C weak 29% / strong 1%; S2, F_item1, F_casing produced 0--3% weak/strong differences across tasks.",
            "format_effect_direction": "some constants can strongly alter performance while others do not independently predict accuracy",
            "explanation_or_hypothesis": "Certain atomic constants (especially the main separator S1 and how enumeration numbers are formatted) more frequently induce systematic changes in model behavior; however, many constants do not independently predict outcomes and interact with other format choices.",
            "counterexample_or_null_result": "S2, F_item1, and F_casing were found not to independently predict performance differences (0% or near-0% strong/weak differences in the reported analysis).",
            "uuid": "e5671.4",
            "source_info": {
                "paper_title": "Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Format Identifiability (embeddings)",
            "name_full": "Prompt-format identifiability in model continuous prompt embeddings",
            "brief_description": "Prompt formats are encoded in the model's last hidden-layer prompt embeddings: a classifier trained on top principal components can identify the generating format with very high accuracy, and separability correlates with performance spread.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA-2",
            "model_size": "7B",
            "task_name": "31 tasks; 10 formats per task; embeddings collected for 1000 examples per format (1-shot and 5-shot)",
            "task_description": "Classification tasks; analysis maps prompt embeddings to the format label.",
            "problem_format": "Collect last pre-generation hidden states for entire prompt for different formats and reduce dimensionality with PCA, then classify format with XGBoost on top-n PCs.",
            "comparison_format": null,
            "performance": "Using top 100 principal components, XGBoost classifier achieves &gt;=0.98 accuracy at identifying format for all 31 tasks analyzed.",
            "performance_comparison": null,
            "format_effect_size": "Classifier accuracy &gt;=0.98 with top 100 PCs; correlation between classifier accuracy using top 2 PCs and spread: 0.424 (p=8.04e-6) for 1-shot, and 0.555 for 5-shot (exact prefix matching).",
            "format_effect_direction": "N/A (diagnostic linking embedding separability to format spread)",
            "explanation_or_hypothesis": "Formats deterministically transform prompt embeddings; higher separability of format embeddings correlates with larger performance spread, suggesting formatting induces distinct internal representations that affect output distributions.",
            "counterexample_or_null_result": null,
            "uuid": "e5671.5",
            "source_info": {
                "paper_title": "Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Model Comparison Reversals",
            "name_full": "Format-driven reversals in model performance ordering",
            "brief_description": "The paper shows that choosing different semantically-equivalent prompt formats can reverse which model appears better on a task, undermining fixed-format model comparisons.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Pairs including LLaMA-2-7B, LLaMA-2-13B, LLaMA-2-70B, Falcon-7B",
            "model_size": null,
            "task_name": "53 Super-NaturalInstructions tasks (1- and 5-shot analyses)",
            "task_description": "Model pairwise comparisons of accuracy when using different prompt formats.",
            "problem_format": "Two different formats p and p' sampled from grammar; compare model ordering under p and under p'.",
            "comparison_format": "Different sampled semantically-equivalent formats p and p'.",
            "performance": "Example statistic: probability that a model pair's ordering reverses (d &gt;= 0.02) is ~0.141 for LLaMA-2-13B vs -70B and ~0.140 for LLaMA-2-7B vs Falcon-7B (i.e., ~14% chance of reversal for d=2%).",
            "performance_comparison": null,
            "format_effect_size": "Reversal probabilities reported around 0.14 for selected model pairs; many reversals were statistically significant (paired tests) in both directions in a substantial fraction of trials (e.g., 76% and 47% for two comparisons mentioned).",
            "format_effect_direction": "format-dependent (formats can reverse which model appears better)",
            "explanation_or_hypothesis": "Formats interact with each model's learned conditional distributions differently, so formats that are high-performing for one model may be low-performing for another; therefore single-format model comparisons can be confounded.",
            "counterexample_or_null_result": null,
            "uuid": "e5671.6",
            "source_info": {
                "paper_title": "Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GPT-3.5 Observations",
            "name_full": "Formatting sensitivity observed on GPT-3.5-Turbo (API)",
            "brief_description": "FORMATSPREAD applied to GPT-3.5-Turbo (API) across 320 formats and 53 tasks shows non-trivial median spread and large maximum spreads, demonstrating sensitivity even for API models.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-Turbo",
            "model_size": null,
            "task_name": "53 Super-NaturalInstructions tasks (320 formats sampled)",
            "task_description": "Classification tasks from Super-NaturalInstructions with formatting variations; evaluated using exact prefix matching when full logits unavailable.",
            "problem_format": "1-shot few-shot prompts varied by grammar; 320 formats per task evaluated under a budget.",
            "comparison_format": null,
            "performance": "Median spread = 0.064 (6.4 percentage points) across 53 tasks (exact prefix matching); maximum observed spread up to 0.562 for some tasks.",
            "performance_comparison": null,
            "format_effect_size": "Median spread 6.4 pts; max observed 56.2 pts on at least one task across 320 formats.",
            "format_effect_direction": "format-dependent (both increases and decreases observed)",
            "explanation_or_hypothesis": "Even API-restricted, instruction-tuned models show sensitivity to formatting; FORMATSPREAD can be applied without model weights to reveal spread.",
            "counterexample_or_null_result": null,
            "uuid": "e5671.7",
            "source_info": {
                "paper_title": "Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Non-elimination by scale/shots/instruct-tuning",
            "name_full": "Formatting sensitivity persists despite increases in model size, shots, and instruction tuning",
            "brief_description": "The authors find that increasing model size, the number of few-shot examples, or applying instruction tuning does not eliminate formatting-induced performance variance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA-2 family, Falcon family (examples LLaMA-2-7B/13B/70B, Falcon-7B/-7B-Instruct)",
            "model_size": null,
            "task_name": "53 Super-NaturalInstructions tasks (and additional experiments)",
            "task_description": "Classification/multiple-choice tasks; experiments compare 1- and 5-shot, multiple model sizes, and instruction-tuned variants.",
            "problem_format": "Varied formats drawn from grammar under fixed number of shots and model variant comparisons.",
            "comparison_format": null,
            "performance": "Spread remains significant after increasing model size (LLaMA-2-70B still shows median spread 0.171 in one experiment), after instruction tuning (Falcon-7B vs Falcon-7B-Instruct differences persist), and when increasing few-shot examples (1-&gt;5 shot does not eliminate spread).",
            "performance_comparison": null,
            "format_effect_size": "Example: LLaMA-2-70B 1-shot median spread 0.171 (mean 0.221, std 0.200) across 53 tasks with 320 formats; 5-shot LLaMA-2-70B had 25% of tasks with spread &gt;=0.310 and max 0.841.",
            "format_effect_direction": "format-dependent (persistence of spread regardless of scaling/tuning)",
            "explanation_or_hypothesis": "Scaling and instruction tuning change the model but do not remove reliance on spurious surface cues encoded by formats; format-induced differences are not simply a small-model artifact.",
            "counterexample_or_null_result": null,
            "uuid": "e5671.8",
            "source_info": {
                "paper_title": "Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Degeneration / Centered Mass",
            "name_full": "Effect of formatting on degeneration (model failing to output any valid option)",
            "brief_description": "Prompt formatting affects not just accuracy but the frequency a model outputs a valid option at all; strong correlation reported between accuracy and 'centered mass' (ratio of outputs matching any valid option).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA-2 and Falcon families",
            "model_size": null,
            "task_name": "53 Super-NaturalInstructions tasks (format variations)",
            "task_description": "Classification tasks where generation can 'degenerate' (model fails to produce any valid option under prefix matching).",
            "problem_format": "Varied prompt formats under exact prefix matching metric; centered mass measures whether any valid option was produced (regardless of correctness).",
            "comparison_format": null,
            "performance": "High correlations between exact-match accuracy and centered mass: e.g., LLaMA-2-7B 1-shot r=0.702 (p=5.1e-77); Falcon-7B 1-shot r=0.936 (p~7.1e-233); Falcon-7B-Instruct 1-shot r=0.962.",
            "performance_comparison": null,
            "format_effect_size": "Very strong correlations (up to r~0.96) show that formatting can dramatically change whether the model produces any valid answer at all; associated spread for exact prefix matching is substantially larger than for ranking accuracy.",
            "format_effect_direction": "format-dependent (formatting can increase degeneration, reducing probability of producing any valid option)",
            "explanation_or_hypothesis": "Formatting interacts with the model's decoding behavior so that some formats induce degeneration (no valid output), explaining larger spreads when using exact prefix matching vs ranking metrics.",
            "counterexample_or_null_result": null,
            "uuid": "e5671.9",
            "source_info": {
                "paper_title": "Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Super-NaturalInstructions: Generalization via declarative instructions on 1600+ NLP tasks",
            "rating": 2,
            "sanitized_title": "supernaturalinstructions_generalization_via_declarative_instructions_on_1600_nlp_tasks"
        },
        {
            "paper_title": "Large language models are human-level prompt engineers",
            "rating": 2,
            "sanitized_title": "large_language_models_are_humanlevel_prompt_engineers"
        },
        {
            "paper_title": "Automatic prompt optimization with \"gradient descent\" and beam search",
            "rating": 2,
            "sanitized_title": "automatic_prompt_optimization_with_gradient_descent_and_beam_search"
        },
        {
            "paper_title": "Making pre-trained language models better few-shot learners",
            "rating": 1,
            "sanitized_title": "making_pretrained_language_models_better_fewshot_learners"
        },
        {
            "paper_title": "Jailbroken: How does llm safety training fail?",
            "rating": 1,
            "sanitized_title": "jailbroken_how_does_llm_safety_training_fail"
        }
    ],
    "cost": 0.018495499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>QUANTIFYING LANGUAGE MODELS' SENSITIVITY TO Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting</h1>
<p>Melanie Sclar ${ }^{1} \quad$ Yejin Choi ${ }^{1,2} \quad$ Yulia Tsvetkov ${ }^{1} \quad$ Alane Suhr ${ }^{3}$<br>${ }^{1}$ Paul G. Allen School of Computer Science \&amp; Engineering, University of Washington<br>${ }^{2}$ Allen Institute for Artificial Intelligence ${ }^{3}$ University of California, Berkeley<br>msclar@cs.washington.edu</p>
<h4>Abstract</h4>
<p>As large language models (LLMs) are adopted as a fundamental component of language technologies, it is crucial to accurately characterize their performance. Because choices in prompt design can strongly influence model behavior, this design process is critical in effectively using any modern pre-trained generative language model. In this work, we focus on LLM sensitivity to a quintessential class of meaning-preserving design choices: prompt formatting. We find that several widely used open-source LLMs are extremely sensitive to subtle changes in prompt formatting in few-shot settings, with performance differences of up to 76 accuracy points when evaluated using LLaMA-2-13B. Sensitivity remains even when increasing model size, the number of few-shot examples, or performing instruction tuning. Our analysis suggests that work evaluating LLMs with prompting-based methods would benefit from reporting a range of performance across plausible prompt formats, instead of the currently-standard practice of reporting performance on a single format. We also show that format performance only weakly correlates between models, which puts into question the methodological validity of comparing models with an arbitrarily chosen, fixed prompt format. To facilitate systematic analysis we propose FORMATSPREAD, an algorithm that rapidly evaluates a sampled set of plausible prompt formats for a given task, and reports the interval of expected performance without accessing model weights ${ }^{1}$. Furthermore, we present a suite of analyses that characterize the nature of this sensitivity, including exploring the influence of particular atomic perturbations and the internal representation of particular formats.</p>
<h2>1 INTRODUCTION</h2>
<p>As the capabilities of LLMs have rapidly improved, their sensitivity to input prompt features has been used to optimize performance via prompt engineering (White et al., 2023). However, there has been little work in characterizing this sensitivity, especially to seemingly innocuous feature choices that preserve prompt meaning and intent. In this work, we analyze the sensitivity of widely used, open-source LLMs to a class of features that should not influence a prompt's interpretation: formatting choices. We find that pre-trained LLMs are sensitive to these choices in unpredictable ways, with accuracy varying in up to 76 points for LLaMA-2-13B between equivalent formats, and $\sim 10$ accuracy points on average across $50+$ tasks and several models. We also show that this variance is not eliminated by adding few-shot examples, increasing model size, or instruction tuning.</p>
<p>Designing prompt templates is a critical part of effectively using a pre-trained language model. This design process includes making choices about wording, choosing few-shot examples for in-context learning, and making decisions about seemingly trivial features like formatting. This process, and often even the resulting templates, is rarely reported or discussed in research papers, under the assumption that performance variance across these choices is insignificant compared to variance across data points or models. However, some anecdotal evidence points to formatting choices actually having a significant influence on model behavior (Aghajanyan, 2023). In some cases, researchers report a limited number of manually generated formats to show that scaling trends hold despite perfor-</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Slight modifications in prompt format templating may lead to significantly different model performance for a given task. Each <text> represents a different variable-length placeholder to be replaced with actual data samples. Example shown corresponds to 1-shot LLaMA-2-7B performances for task280 from SuperNaturalInstructions (Wang et al., 2022). This StereoSet-inspired task (Nadeem et al., 2021) requires the model to, given a short passage, classify it into one of four types of stereotype or anti-stereotype (gender, profession, race, and religion).
mance being significantly different (Schick et al., 2021). The assumption that formatting does not influence overall model performance may become problematic when improvements over existing approaches are attributed to the amount and source of training data, number of parameters, or model architecture, without also accounting for changes in prompt format. Ignoring variance across formats may also negatively affect user experience, e.g. if users inadvertently choose formats the LLM does not perform well on.</p>
<p>Our proposed tool, FORMATSPREAD, enables a systematic analysis of these variances across a wide set of semantically equivalent prompt formats within a user-specified computational budget. We find that choices in formatting few-shot examples during in-context learning introduce spurious biases that may lead to significantly different conclusions in model performance. The sensitivity to formatting choices that we discover across widely-used, open-source models suggests that future research would benefit from reporting a performance spread over a sufficient sample of plausible formats, instead of simply reporting the formatting used and its performance, as is currently standard. Moreover, we argue that this reporting is crucial when comparing the performance of different models, as we show the influence of formatting choices only weakly correlates between models, thus making and fixing a formatting choice could introduce a significant confounding factor.</p>
<p>Fully exploring the space of prompt formats is intractable, as computation costs scale linearly with the number of formats considered. FORMATSPREAD efficiently explores the space of prompt formats under a user-specified computational budget using Bayesian optimization. FORMATSPREAD does not require access to the model weights, allowing its use on API-gated models: we find a spread up to 56 accuracy points with a median spread of 6.4 accuracy points with GPT3.5 across 320 formats and 53 tasks at a cost of under 10USD on average per task. Beyond facilitating evaluation, we also propose a suite of analyses to further characterize model sensitivity to formatting. Among other results, we show that the separability of continuous prompt embeddings correlates with the spread observed in task performance.</p>
<h1>2 OVERVIEW</h1>
<p>We evaluate LLM performance over the space of prompt formats that may plausibly be chosen by a non-adversarial user when designing a prompt for a target task, where the space of formats is defined by a grammar (3.1). Our grammar's definition naturally induces a definition of semantic equivalence among formats. We quantify model sensitivity in terms of performance range in a target task across the space of equivalent prompt formats to the original choice (4.2). We cast the problem of searching across this space as a bandit problem, and propose FORMATSPREAD (3), which consists of a grammar (3.1) and a procedure to estimate the minimum and maximum performance across a set of semantically equivalent formats given a pre-defined metric (3.2). FORMATSPREAD uses Bayesian optimization to identify the expected performance range with low additional computational cost (4.5) all without requiring access to model weights, which enables use on API-gated</p>
<p>LLMs. Furthermore, we perform in-depth analysis of this observed sensitivity, including by quantifying the contribution of individual feature choices to the final performance (4.3) and measuring the identifiability of a format based solely on a model's internal, continuous representation of any prompt via correlation with model performance (4.4).</p>
<h1>3 Measuring Sensitivity with FormatSpread</h1>
<h3>3.1 Grammar of Plausible Prompt Formats</h3>
<p>We construct a grammar that defines both the space of plausible prompt formats and semantic equivalence between formats. The grammar is manually constructed, as opposed to automatically induced from data, to guarantee a higher level of precision when defining the set of equivalent formats. Our grammar is directly tested by verifying that it can generate the formatting associated with 100+ Super-NaturalInstructions tasks (Wang et al., 2022).</p>
<p>Our grammar consists of fields that are composed to create a prompt format. For example, the format 'Passage: <text> || Answer: <text>', has basic fields 'Passage: <text>', and 'Answer: <text>', denoted $a_{1}$, and $a_{2}$. Each basic field consists of a descriptor (e.g. 'Passage'), a separator (e.g. ' : ' ), and a text placeholder to replace with each data point. We define basic fields as $B_{1}(d, s, f):=f(d) s&lt;$ text&gt; using Backus-Naur notation, where $d$ is a descriptor string, $s \in \mathcal{S}<em _casing="{casing" _text="\text">{1}$ a separator, and $f \in \mathcal{F}</em>,\right.$ ' || ' $)$.}}$ a function that alters $d$ while preserving meaning. Thus, in our example, $a_{1}=B_{1}($ Passage, ': ', id $)$ and $a_{2}=B_{1}($ Answer, ': ', id $)$, with id the identity function. We define joining several fields as $B_{2}^{(n)}\left(X_{1}, \ldots, X_{n}, c\right):=X_{1} c X_{2} c \ldots c X_{n}$, with $c \in \mathcal{C}$ being a space. Our example's prompt format may be written as $B_{2}^{(2)}\left(a_{1}, a_{2</p>
<p>The grammar also supports enumeration, which is defined as joining several basic fields, each representing a different list item. For example, the enumeration 'Option (A) : <text>, Option (B) : <text>, Option (C) : <text>' may be written as $B_{2}^{(3)}\left(a_{1}, a_{2}, a_{3},\right.$ ' $\left.\left.\left.\right|^{1}\right)^{\prime}\right)$, where $a_{i}=$ $B_{1}\left(e_{i}, \prime: \prime\right.$ ', id $)$. In our example, $e_{1}$ represents 'Option (A)', and may in turn be written as the concatenation $e_{i}:=d s_{2} f_{\text {item }}(i)$ with $d=$ 'option', $s_{2}=$ ' (single space), and $f_{\text {item }}(1)=$ '(A)'. Each $f_{\text {item }}$ transforms an item $i$ using a number format (e.g. letters or Roman numerals, denoted as $\mathcal{F}<em _item1="{item1" _text="\text">{\text {item } 2}$ ) and an item wrapper (e.g. (A) or [A], denoted as $\mathcal{F}</em>$ ).}</p>
<p>In summary, we define valid prompt formats as those accepted by the following grammar:</p>
<p>$$
\begin{aligned}
B_{0}(): &amp; =&lt;\text { text }&gt; \
B_{0}^{\prime}(d, s) &amp; :=f(d) s \quad \text { with } s \in \mathcal{S}<em _casing="{casing" _text="\text">{1}, f \in \mathcal{F}</em> \
B_{1}(d, s, f) &amp; :=f(d) s&lt;\text { text }&gt;\quad \text { with } s \in \mathcal{S}}<em _casing="{casing" _text="\text">{1}, f \in \mathcal{F}</em> \
B_{2}^{(n)}\left(X_{1}, \ldots, X_{n}, c\right) &amp; :=X_{1} c \ldots c X_{n} \quad \text { with } c \in \mathcal{C}, X_{i} \in\left{B_{0}, B_{0}^{\prime}, B_{1}, B_{2}, B_{3}\right} \forall i \
B_{3}^{(n)}\left(d, j_{1}, \ldots, j_{n}, s_{1}, s_{2}, c, f_{1}, f_{2}\right):= &amp; B_{2}^{(n)}\left(B_{1}\left(e_{1}, s_{1}, f_{2}\right)\right), \ldots, B_{1}\left(e_{n}, s_{1}, f_{2}\right), c\right) \
&amp; \text { where } e_{i}:=f_{2}(d) s_{2} f_{1}\left(j_{i}\right), j_{i} \in \mathbb{N}}<em 1="1">{0} \forall i \
&amp; s</em>} \in \mathcal{S<em 2="2">{1}, s</em>} \in \mathcal{S<em 1="1">{2}, f</em>} \in \mathcal{F<em 2="2">{\text {item }}, f</em>
\end{aligned}
$$} \in \mathcal{F}_{\text {casing }</p>
<p>Our grammar defines valid formats as finite compositions of $B_{0}, B_{0}^{\prime}, B_{1}, B_{2}, B_{3}$. The sets $\mathcal{S}<em 2="2">{1}, \mathcal{S}</em>}, \mathcal{C}$, $\mathcal{F<em _item="{item" _text="\text">{\text {casing }}, \mathcal{F}</em>$ ) to guarantee semantic equivalence; one may also define a set of functions that paraphrases the descriptor, e.g., via synonym replacement. Appendix A. 2 contains the full list of values we use for the constant sets, as well as a visualization of a prompt template generated from the grammar.}}$ (two sets of separators, spaces, casing functions, and itemizing functions respectively) are pre-defined by the user. Throughout this work, we instantiate all sets with values typically observed in human-written prompt formats. We intentionally only modify the casing of descriptors (via $\mathcal{F}_{\text {casing }</p>
<p>Prompt Format Equivalence. Two prompt formats $p_{1}, p_{2}$ are equivalent if they represent the same rule application $B_{i}$, the descriptors (if any) are the same, and the sub-elements (if any) are equivalent. Appendix A. 1 contains the formal definition of equivalence. The grammar's strict definition allows us to assume that sets of equivalent formats share equivalent meanings. When measuring sensitivity (3.2), we explore only the space of formats equivalent to a task's original format.</p>
<p>Contextual Restrictions. We define restrictions to the combinations of spaces and separators to further ensure naturalness. For example, if $B_{2}\left(X_{1}, \ldots, X_{n}, c\right)$ where $c$ does not contain a newline, then each $X_{i}$ 's separators and any subcomponents' separators should not contain a newline. This</p>
<p>avoids unnatural formats like Input: $\backslash$ n <text> Output: $\backslash$ n <text>. We also allow for adding conditions that force constants (separators, spaces, etc.) in different applications of $B_{i}$ to be equal. When measuring sensitivity to format perturbations, if two separators or spaces are equal in an original format, they are forced to jointly change to be considered equivalent. Appendix A. 3 contains all contextual restrictions.</p>
<p>Final Prompt Construction. Given a valid format $p$ accepted by the grammar, the final prompt is constructed by concatenating with space $c$ an instruction string inst, $n$ fewshot data points $D_{1}, \ldots, D_{n}$ exemplifying the task, and a data point $D_{n+1}$ to be solved. All few-shot examples $D_{i}$ are formatted using $p$. Thus, the final prompt template is: inst $c p\left(D_{1}\right) c p\left(D_{2}\right) c \ldots c p\left(D_{n}\right) c p\left(D_{n+1}\right)$. Since $D_{n+1}$ 's output will be generated by the model, an empty string is added in place of the answer in the last field in the template. Prompt construction will modify inst to match specific choices encoded in $p$ : concretely, if $p$ enumerates valid multiple-choice options as characters $x_{1} \ldots x_{n}$, we ensure inst refers to these choices as $x_{1} \ldots x_{n}$.</p>
<h1>3.2 Measuring Sensitivity</h1>
<p>We measure how plausible choices in prompt formatting influence quantifiable metrics of generated outputs. Given a set of plausible formats $\left{p_{1}, \ldots, p_{n}\right}$, a dataset $\mathcal{D}$, and a scalar metric $m$, let the performance interval be $\left[\min <em i="i">{i} m\left(p</em>\right), \max }, \mathcal{D<em i="i">{i} m\left(p</em>\right)\right]$. We define the performance spread or simply spread as $\max }, \mathcal{D<em i="i">{i} m\left(p</em>\right)-\min }, \mathcal{D<em i="i">{i} m\left(p</em>\right)$. Higher spread indicates more sensitivity to variance within the space of plausible, semantically-equivalent formats. While our method is agnostic to the scalar metric $m$ used, and one could consider a number of metrics including text length, formality, or toxicity, throughout this work we focus our analysis on estimated task accuracy $a c c$. Due to ease in automatic evaluation, here we evaluate on classification tasks.}, \mathcal{D</p>
<p>Our goal is to compute spread for a given model and task. A comprehensive approach would be to fully evaluate each plausible format $p_{i}$ on the entire evaluation dataset $\mathcal{D}$. This increases the cost of reporting a model's performance linearly with $n$, which becomes computationally infeasible for large values of $n$. Following prior gradient-free prompt engineering work (Zhou et al., 2023; Pryzant et al., 2023), we model our problem as a multi-arm bandit. Given a random sample of $n$ formats (arms) $p_{1}, \ldots, p_{n}$ for a task, an arm $p_{i}$ 's hidden value is the actual performance $m\left(p_{i}, \mathcal{D}\right)$ when evaluated on the full dataset $\mathcal{D}$, and the reward for pulling the arm is an estimate $m\left(p_{i}, \hat{\mathcal{D}}\right)$ where $\hat{\mathcal{D}} \subset \mathcal{D},|\hat{\mathcal{D}}|=B$ (mini-batch size) and no element of $\hat{\mathcal{D}}$ has yet been evaluated with $p_{i}$.</p>
<p>We assume a budget of $E$ total data point evaluations. We first search for the highest performing format with budget $E / 2$, and then for the lowest performing format with budget $E / 2$. Evaluations done for the first exploration are readily available for the second exploration, which yields a more informative prior for many formats. We consider two well-known regret minimization bandit algorithms: Thompson sampling (used in FORMATSPREAD) and Upper Confidence Bound (UCB).</p>
<p>Thompson Sampling. This simple, high-performing Bayesian inference heuristic randomly draws each arm according to its probability of being optimal (Chapelle \&amp; Li, 2011). Each $m\left(p_{i}, \mathcal{D}\right)$ is modeled as a random variable, and since with our target metric each data point evaluation is a Bernoulli trial, it is natural to model $m\left(p_{i}, \hat{\mathcal{D}}\right)$ as a Beta distribution. In each round, Thompson sampling draws from each $m\left(p_{i}, \hat{\mathcal{D}}\right)$ and chooses the best arm $\hat{i}$ (Algorithm 1). It then updates $\hat{i}$ according to the number of observed successes $r$, and the corresponding $B-r$ failures, within $\hat{\mathcal{D}}$.</p>
<div class="codehilite"><pre><span></span><code><span class="nx">Algorithm</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="nx">Thompson</span><span class="w"> </span><span class="nx">Sampling</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nx">Bernoulli</span><span class="w"> </span><span class="nx">Bandits</span>
<span class="w">    </span><span class="err">\</span><span class="p">(</span><span class="nx">S_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="o">^</span><span class="p">{(</span><span class="mi">1</span><span class="p">)}</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="nx">N_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="o">^</span><span class="p">{(</span><span class="mi">1</span><span class="p">)}</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="w"> </span><span class="mi">0</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="nx">success</span><span class="w"> </span><span class="nx">counters</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">total</span><span class="w"> </span><span class="nx">times</span><span class="w"> </span><span class="nx">armed</span><span class="w"> </span><span class="nx">was</span><span class="w"> </span><span class="nx">drawn</span><span class="w"> </span><span class="nx">counter</span><span class="p">)</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">t</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="nx">ldots</span><span class="w"> </span><span class="nx">E</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="nx">B</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">do</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">i</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="nx">ldots</span><span class="p">,</span><span class="w"> </span><span class="nx">K</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">do</span>
<span class="w">            </span><span class="nx">Take</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">theta_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="o">^</span><span class="p">{(</span><span class="nx">t</span><span class="p">)}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">from</span><span class="w"> </span><span class="nx">Beta</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="err">\</span><span class="nx">alpha_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="o">+</span><span class="nx">S_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="o">^</span><span class="p">{(</span><span class="nx">t</span><span class="p">)},</span><span class="w"> </span><span class="err">\</span><span class="nx">beta_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="o">+</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">N_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="o">^</span><span class="p">{(</span><span class="nx">t</span><span class="p">)}</span><span class="o">-</span><span class="nx">S_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="o">^</span><span class="p">{(</span><span class="nx">t</span><span class="p">)}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="nx">Draw</span><span class="w"> </span><span class="nx">arm</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">hat</span><span class="p">{</span><span class="nx">i</span><span class="p">}=</span><span class="err">\</span><span class="nx">arg</span><span class="w"> </span><span class="err">\</span><span class="nx">max</span><span class="w"> </span><span class="nx">_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">theta_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="o">^</span><span class="p">{(</span><span class="nx">t</span><span class="p">)}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="p">(</span><span class="k">or</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">arg</span><span class="w"> </span><span class="err">\</span><span class="nx">min</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">minimization</span><span class="w"> </span><span class="nx">problems</span><span class="p">)</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="k">observe</span><span class="w"> </span><span class="nx">reward</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">r</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="err">\</span><span class="p">(</span><span class="nx">S_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="o">^</span><span class="p">{(</span><span class="nx">t</span><span class="o">+</span><span class="mi">1</span><span class="p">)}</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="w"> </span><span class="nx">S_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="o">^</span><span class="p">{(</span><span class="nx">t</span><span class="p">)}</span><span class="o">+</span><span class="nx">r</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="nx">quad</span><span class="w"> </span><span class="nx">N_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="o">^</span><span class="p">{(</span><span class="nx">t</span><span class="o">+</span><span class="mi">1</span><span class="p">)}</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="w"> </span><span class="nx">N_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="o">^</span><span class="p">{(</span><span class="nx">t</span><span class="p">)}</span><span class="o">+</span><span class="nx">B</span><span class="err">\</span><span class="p">)</span>
</code></pre></div>

<p>Thompson sampling allows for setting informative priors $\left(\alpha_{i}, \beta_{i}\right)$ based on domain knowledge to accelerate runtime. Appendix A. 4 details the exact priors we use. To our knowledge, we are the first to consider a Bayesian sampling method for prompt optimization.</p>
<p>Upper Confidence Bound (UCB) Sampling. UCB (Lai et al., 1985) computes an upper confidence bound to each arm's performance, derived from Chernoff's bound. The key difference with Thompson sampling is in how $\theta_{i}^{(t)}$ is defined. In UCB's frequentist approach, $\theta_{i}^{(t)}$ is assigned the estimated accuracy plus the upper confidence bound: $\theta_{i}^{(t)} \leftarrow S_{i} / N_{i}+c \sqrt{\log (t) / N_{i}}$. We use $c=2$ following Pryzant et al. (2023), who find UCB with $c=2$ to be most effective for prompt optimization.</p>
<p>Naive Sampling. Each prompt format is evaluated on $E / n$ points (with appropriate rounding).</p>
<h1>4 Characterizing Prompt Format Variance with FormatSpread</h1>
<h3>4.1 EXPERIMENTAL SETUP</h3>
<p>Data. We use a subset of 53 tasks from Super-NaturalInstructions (Wang et al., 2022) with diverse human-written formats and instructions, comprising 19 multiple-choice tasks and 34 classification tasks with ${2,3,4}$ basic fields. Appendix B. 1 details the exact task selection procedure. To construct the final prompt template, we concatenate each task's instruction and $n$ formatted few-shot examples using $\backslash \mathrm{n} \backslash \mathrm{n}$ as spacing. While selection and ordering of few-shot examples is a component of prompt design influencing features of model output (Lu et al., 2022), our work focuses on prompt formatting. To remove this confounder, we fix the exact choice and ordering of examples for each task and for a given number of shots $n$. Few-shot examples for each task are chosen randomly within each dataset and are not used for evaluation. We evaluate task data samples on an arbitrary order fixed across settings. Datasets are assumed to be of size 1,000 for fair evaluation across tasks.</p>
<p>Models. We evaluate LLaMA-2-{7B,13B,70B} (Touvron et al., 2023), Falcon-7B and Falcon-7BInstruct (Almazrouei et al., 2023), GPT-3.5-Turbo (Schulman et al., 2022), all autoregressive LMs.</p>
<p>Task Evaluation Metrics. We use two popular measures for computing accuracy: exact prefix matching and probability ranking. In exact prefix matching, we check if the output's prefix matches the expected answer after normalization (casing, spacing, newlines). Ranking accuracy computes the rate that the expected answer is the highest-ranked valid option (in multiple choice and classification tasks) according to the model's output distribution. Results are reported using ranking accuracy unless specified otherwise. Appendix B. 2 shows additional analysis of exact prefix matching, with spreads even higher than those shown in Section 4.2, and including how formatting choice affects task degeneration (i.e., not answering any valid option).</p>
<h3>4.2 PROMPT FORMATS HAVE A LARGE PERFORMANCE SPREAD, NOT ELIMINATED BY INCREASING FEW-SHOT EXAMPLES OR MODEL SIZE, NOR WITH INSTRUCTION TUNING</h3>
<p>For each evaluation task we randomly sample 10 plausible prompt formats and use FORMATSPREAD to compute performance spread for each modeling and $n$-shot choice (Figure 3). We find significant performance spread across tasks, with a median spread of 7.5 accuracy points across choices in the model and the number of few-shot examples. $20 \%$ of tasks consistently result in a spread of at least 15 accuracy points for all LLaMA-2 settings, and at least 9 points for all Falcon settings. We observe several tasks with performance spread over 70 accuracy points. Because this analysis uses only 10 randomly sampled formats, it represents a lower bound of the true spreads for each task. Furthermore, there exists significant performance spread regardless of increased model size (Figure 2a and Figure 11 for Llama-2-70B), instruction tuning (Figure 2b), or number of few-shot examples (Figure 2c; Figure 2a and 2b plot 1- and 5-shot jointly). Appendix B. 2 demonstrates similar results on a selection of non-classification tasks, and expands the spread discussion to plotting the entire accuracy distribution, along with a dispersion metric.</p>
<p>Comparison trends between models are often reversed just by choosing different formats. Assuming model $M$ is better than $M^{\prime}$ by at least $d$ accuracy using prompt $p$, we compute how often $M^{\prime}$ achieves at least $d$ higher accuracy than $M$ under a different format $p^{\prime}$. Figure 4 shows these trends are often reversed: LLaMA-2-13B and -70B reverse trend by at least $d=0.02$ with probability 0.141; LLaMA-2-7B and Falcon-7B reverse trend by at least $d=0.02$ with probability 0.140. Strikingly, often both experiments (first using $p$, and then $p^{\prime}$ ) were statistically significant (p-value $&lt;0.05$ ) on 1000 samples $^{2}: 76 \%$ and $47 \%$ respectively for the two model comparisons</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Spread comparison between evaluating the same task under different models or $n$-shots.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Spread across models and $n$-shots.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Probability that model $M$ performs worse than $M^{\prime}$ by at least $d$ when using format $p^{\prime}$, given that $M$ performed better than $M^{\prime}$ by at least $d$ using format $p$. 53 tasks, 1- and 5-shot.</p>
<p>mentioned above. We find that formats yielding high performance for model $M$ may not yield high performance for $M^{\prime}$, implying that formats may not be inherently good or bad (Appendix B.2).</p>
<h3>4.3 How do individual features contribute to performance?</h3>
<p>We analyze how choices in particular constants (i.e. $\mathcal{S}<em 2="2">{1}, \mathcal{S}</em>}, \mathcal{C}, \mathcal{F<em _item="{item" _text="\text">{\text {casing }}, \mathcal{F}</em>}}$ ) independently influence task performance across different formats. Figure 5 shows the distribution of accuracy for 500 sampled prompts conditioned on the choice of $\mathcal{S<em 1="1">{1}$ (the separator between a descriptor and the text placeholder) for one task in Super-NaturalInstructions. When comparing the individual influence of two feature choices, we measure both weak and strong notions of dissimilarity between distributions of accuracy across prompts conditioned on a chosen feature. We say two constant choices yield weakly different accuracy distributions if the values between the first quartile ( $Q</em>$ ' and ' : ' (fourth and sixth) are only weakly different.}$ ) and third quartile ( $Q_{3}$ ) do not intersect. This is equivalent to the boxes in a boxplot not overlapping. We say two constant choices yield strongly different accuracy distributions if the ranges $\left[2.5 Q_{1}-1.5 Q_{3}, 2.5 Q_{3}+1.5 Q_{1}\right]$ do not overlap (adjusted to end in a data point). This is equivalent to two boxplots with their whiskers not overlapping. In Figure 5, ' $\backslash \mathrm{n} \backslash \mathrm{c</p>
<p>We compute accuracy for 500 random formats with 250 samples each on 31 tasks for 1-shot Llama-2-7B. Table 1 shows that choices in $\mathcal{S}<em _item1="{item1" _text="\text">{2}, \mathcal{F}</em>}}, \mathcal{F<em 1="1">{\text {casing }}$ do not independently predict performance differences (weakly or strongly): although these features can have a large performance variance and thus should be explored with FORMATSPREAD, they cannot be used to independently predict accuracy changes. Other constant sets have varying degrees of differences, with $\mathcal{S}</em>$ (number format changes in enumerations) having the most individual impact. All tasks with strong dissimilarities are shown in Appendix B.4.}$ (separators) and $\mathcal{F}_{\text {item2 }</p>
<p>Small prompt variations often yield large performance differences. Table 2 shows a selection of tasks where changing a single constant on a format (e.g., casing in task322) results in large accuracy differences. Figure 6 shows that regardless of the scoring criterion used, a significant ratio of these atomic changes are associated with large accuracy changes. For example, 24\% of atomic changes have an associated accuracy change of at least 5 points when using exact prefix matching as scoring criteria ( $11 \%$ when using probability ranking).</p>
<p>The space of prompt format accuracy is highly non-monotonic, which makes local search algorithms over the space less effective. Let $\left(p_{1}, p_{2}, p_{3}\right)$ be a prompt format triple such that $p_{i+1}$ is obtained by making an atomic change to $p_{i}$. We argue that if the prompt format space is smooth, we should often see a triples' accuracy to be strictly monotonic over $i$. We choose 24 tasks ( 13 multiple choice,</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Example of accuracy variance for different choices of constants in $\mathcal{S}_{1}$ for task1283.</p>
<p>Table 1: Tasks where at least two constants yield different performance (weakly different if their boxes in a boxplot do not overlap, strongly if boxes including whiskers do not overlap).</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Median Spread <br> (range $[0,1]$ )</th>
<th style="text-align: center;">Weak <br> Diff.</th>
<th style="text-align: center;">Strong <br> Diff.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$\mathcal{C}$</td>
<td style="text-align: center;">0.144</td>
<td style="text-align: center;">$29 \%$</td>
<td style="text-align: center;">$1 \%$</td>
</tr>
<tr>
<td style="text-align: center;">$\mathcal{S}_{1}$</td>
<td style="text-align: center;">0.132</td>
<td style="text-align: center;">$43 \%$</td>
<td style="text-align: center;">$22 \%$</td>
</tr>
<tr>
<td style="text-align: center;">$\mathcal{S}_{2}$</td>
<td style="text-align: center;">0.238</td>
<td style="text-align: center;">$0 \%$</td>
<td style="text-align: center;">$0 \%$</td>
</tr>
<tr>
<td style="text-align: center;">$\mathcal{F}_{\text {item1 }}$</td>
<td style="text-align: center;">0.176</td>
<td style="text-align: center;">$0 \%$</td>
<td style="text-align: center;">$0 \%$</td>
</tr>
<tr>
<td style="text-align: center;">$\mathcal{F}_{\text {item2 }}$</td>
<td style="text-align: center;">0.173</td>
<td style="text-align: center;">$45 \%$</td>
<td style="text-align: center;">$10 \%$</td>
</tr>
<tr>
<td style="text-align: center;">$\mathcal{F}_{\text {casing }}$</td>
<td style="text-align: center;">0.188</td>
<td style="text-align: center;">$3 \%$</td>
<td style="text-align: center;">$0 \%$</td>
</tr>
</tbody>
</table>
<p>Table 2: Examples of atomic changes' impact on accuracy using probability ranking (prefix matching shown in Table 4). $}$ represents a text field; $p_{2}$ yields higher accuracy than $p_{1}$ for all tasks.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Task Id</th>
<th style="text-align: left;">Prompt Format 1 $\left(p_{1}\right)$</th>
<th style="text-align: left;">Prompt Format 2 $\left(p_{2}\right)$</th>
<th style="text-align: left;">Acc $p_{1}$</th>
<th style="text-align: left;">Acc $p_{2}$</th>
<th style="text-align: left;">Diff.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">task280</td>
<td style="text-align: left;">passage: ${}\backslash$ answer: $}$</td>
<td style="text-align: left;">passage ${}\backslash$ answer $}$</td>
<td style="text-align: left;">0.043</td>
<td style="text-align: left;">0.826</td>
<td style="text-align: left;">0.783</td>
</tr>
<tr>
<td style="text-align: left;">task317</td>
<td style="text-align: left;">Passage: : $}$ Answer: : $}$</td>
<td style="text-align: left;">Passage: : $}$ Answer: : $}$</td>
<td style="text-align: left;">0.076</td>
<td style="text-align: left;">0.638</td>
<td style="text-align: left;">0.562</td>
</tr>
<tr>
<td style="text-align: left;">task190</td>
<td style="text-align: left;">Sentence[II] - {}</td>
<td style="text-align: left;">Sentence[A] - {}Sentence[B] - {}</td>
<td style="text-align: left;">0.360</td>
<td style="text-align: left;">0.614</td>
<td style="text-align: left;">0.254</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">$\cdots$ Answer $\backslash t {}$</td>
<td style="text-align: left;">$\cdots$ Answer $\backslash t {}$</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">task904</td>
<td style="text-align: left;">input: : $}$ \n output: : $}$</td>
<td style="text-align: left;">input: : $}$ \n output: : $}$</td>
<td style="text-align: left;">0.418</td>
<td style="text-align: left;">0.616</td>
<td style="text-align: left;">0.198</td>
</tr>
<tr>
<td style="text-align: left;">task320</td>
<td style="text-align: left;">target - {} \n{} \nanswer - {}</td>
<td style="text-align: left;">target - {}; \n{}; \nanswer - {}</td>
<td style="text-align: left;">0.361</td>
<td style="text-align: left;">0.476</td>
<td style="text-align: left;">0.115</td>
</tr>
<tr>
<td style="text-align: left;">task322</td>
<td style="text-align: left;">COMMENT: {} ANSWER: {}</td>
<td style="text-align: left;">comment: {} answer: {}</td>
<td style="text-align: left;">0.614</td>
<td style="text-align: left;">0.714</td>
<td style="text-align: left;">0.100</td>
</tr>
<tr>
<td style="text-align: left;">task279</td>
<td style="text-align: left;">Passage : {}. Answer : {}</td>
<td style="text-align: left;">PASSAGE : {}. ANSWER : {}</td>
<td style="text-align: left;">0.372</td>
<td style="text-align: left;">0.441</td>
<td style="text-align: left;">0.069</td>
</tr>
</tbody>
</table>
<p>11 non-multiple choice), sample 300 $\left(p_{1}, p_{2}, p_{3}\right)$ triples for each, and the compute accuracy (using exact prefix matching) of each $p_{i}$ on 250 samples. 32.4 and $33.6 \%$ of triples were monotonic for multiple-choice and non-multiple-choice tasks respectively. Given that random shuffling within a triple will result in monotonicity $33.3 \%$ of the time, this suggests that local search mechanisms like simulated annealing may not be effective as they require a locally smooth search space.</p>
<h1>4.4 PROMPT FORMATS ARE IDENTIFIABLE TRANSFORMATIONS OF PROMPT EMBEDDINGS</h1>
<p>Prompt format choices represent a deterministic transformation of the input, even if its impact on the resulting performance is hard to predict. We represent prompt embeddings as the last hidden layer obtained when processing the whole input prompt (immediately before generating the first token). We demonstrate that format choice yields a highly identifiable transformation over this embedding, which suggests that formats can be seen as transformations of the output probability distribution.</p>
<p>For each task, and for both ${1,5}$-shot settings, we collect prompt embeddings from LLaMA-2-7B corresponding to 10 randomly sampled valid formats for 1000 evaluation examples. We train an XGBoost (Chen \&amp; Guestrin, 2016) classifier that maps from the top $n$ principal components of a prompt embedding to the prompt format. ${ }^{3}$ We find that although the original prompt embeddings are of size $4,096^{4}$, using just the top 100 principal components can result in a classifier with $\geq 0.98$ accuracy in format identification for all 31 tasks analyzed. Figure 7 shows the accuracy of format classification given a fixed number of principal components. ${ }^{5}$ We find that classifier accuracy given just the top two components correlates moderately with the spread of performance in the prompts they represent $\left(0.424, p=8.04 \cdot 10^{-6} ; 0.555\right.$ for the 5 -shot setting; using exact prefix matching).</p>
<h3>4.5 FAST EXPLORATION OF THE PROMPT FORMATTING SPACE: FORMATSPREAD</h3>
<p>In Section 4.2, we demonstrate that even when sampling just 10 formats from the space of plausible formats, we still observe significant performance spread on many tasks. However, this is only a lower</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Probability that an atomic change (e.g. changing a space, separator) has a given impact in accuracy for two scoring criteria. 53 tasks, 30 sampled atomic changes each.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 8: Probability of observing a spread increase of at least $d$ when increasing sample size from $k_{1}$ to $k_{2}$ formats. 31 tasks, 100 trials each.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 7: Cumulative ratio of tasks that can be classified with at most $a$ accuracy using the top principal components of the last decoding layer of the prompt.
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: Difference between the true sample spread and each algorithm-found spread with respect to $E$ (evaluation budget). 320 formats, $B=20$, average of 5 trials over 31 tasks shown.
bound of the spread a task may exhibit when increasing the number of formats: for example, about $17 \%$ of tasks are expected to increase their spread by at least 5 accuracy points when increasing from 10 to 20 sampled formats. Figure 8 quantifies the expected increase in spread when increasing the number of formats by evaluating 500 formats on 250 samples each and computing expected gains.</p>
<p>Figure 9 compares the efficiency of Thompson sampling, UCB, and naive sampling for estimating spread with respect to a budget $E$ (Section 3.2). To ensure accurate reports, we compute and show the true spread of the highest- and lowest-performing formats chosen by each method using all data. With a budget of 51,200 evaluations, Thompson sampling results in a spread within 1 accuracy point of the true spread, while naive sampling finds a spread within 4 points, and UCB within 11.</p>
<p>Finally, we use FormatSpread to measure sensitivity of several models where inference is expensive. With a budget of 40,000 evaluations and 320 prompt formats, we find that 1-shot LLaMA-2-70B-ran using 4-bit quantization (Dettmers et al., 2022)-yields a median spread of 0.171 (mean $=0.221$, std $=0.200$, using probability ranking across 53 tasks; $25 \%$ of tasks had a spread of 0.292 or higher, with a maximum spread of 0.876 ), and GPT-3.5 yields a median spread of 0.064 (mean $=0.110$, std $=0.115$, across 53 tasks using exact prefix matching given that we do not have access to the full logits; $25 \%$ of tasks had a spread of 0.148 or higher, with a maximum spread of 0.562 ), showing sensitivity to formatting is still present even on larger models. 5-shot LLaMA-270B still shows high spreads, with $25 \%$ of tasks having a spread of 0.310 and a maximum of 0.841 . See spread visualization in Figure 25, and a list of best and worst formats found in Table 6.</p>
<h1>5 Related Work</h1>
<p>The task of automatically finding the best-performing prompt for a given task without changing model parameters has recently gained attention, given the constantly improving yet somewhat unpredictable performance of LLMs. Prior work has often focused on discovering optimal prompts with gradient-based methods, which are effective, but often lead to disfluent or unnatural prompts (Shin</p>
<p>et al., 2020), which can be mitigated with a Langevin dynamics-based method (Shi et al., 2022). Another approach is to learn, optimize, and insert continuous representations of prompts and tasks as input to models (Qin \&amp; Eisner, 2021; Lester et al., 2021; Ding et al., 2022; Itharco et al., 2023). These methods also require access to the LLM's parameters, thus cannot be applied to models behind an API. In contrast, FORMATSPREAD does not assume access to any model internals. Prior gradientfree work has focused on edit-based enumeration over human-written prompts (Prasad et al., 2023), reinforcement learning (Deng et al., 2022), and by using LLMs themselves (Zhou et al., 2023; Gao et al., 2021). These works aim to achieve competitive task performance, even if the meaning of the prompt or instruction is modified. To our knowledge, we are the first to focus specifically on prompt formatting variance, a quintessential example of semantic equivalence.</p>
<p>Jailbreaking refers to the behavior of intentionally manipulating prompts to elicit inappropriate or sensitive responses, or otherwise reveal parts of the prompt that were intentionally not revealed. While the objective differs from our work, jailbreaking works (Wei et al., 2023; Zou et al., 2023) share the underlying technical question of finding the lowest-performing prompt. Our methods differ, since Wei et al. (2023) evaluate human-generated attacks to guide adversarial prompt design, and Zou et al. (2023) uses gradient-based search methods simultaneously across multiple models.</p>
<p>Some existing work has explored the influence of certain prompt design choices on model performance, for example the prompt's language (Gonen et al., 2022), the ordering of few-shot examples (Lu et al., 2022), and their patterns (Madaan et al., 2023). Other work has focused on providing textual interpretations of continuous prompt representations (Khashabi et al., 2022). Beyond autoregressive LLMs, existing work has focused on performance variance in masked language models (Elazar et al., 2021; Jiang et al., 2020). Our work follows efforts in other domains that explore the influence of spurious features on research evaluations, e.g., in deep reinforcement learning (Islam et al., 2017; Henderson et al., 2018) and statistical machine translation (Clark et al., 2011).</p>
<h1>6 DISCUSSION</h1>
<p>We introduce FORMATSPREAD, an algorithm that estimates the performance spread across prompt formatting choices. ${ }^{6}$ We use FORMATSPREAD to evaluate the spread of several widely-used opensource LLMs for classification tasks in few-shot learning settings. We find that spread is large regardless of model choice, even when increasing model size, number of few-shots, or when using instruction tuning. FORMATSPREAD is designed to efficiently search the space of plausible prompt formats under a user-specified computational budget. For example, with a computational budget of exploring only $5 \%$ of the entire search space for task with 2,500 test examples and 320 plausible formats, we are able to estimate spread within 2 accuracy points of the true spread.</p>
<p>We also characterize the space of prompt formats, finding that it is largely non-monotonic and that few atomic features can be predictors of performance alone, although the separability of format embeddings is highly correlated with observed performance spread. These findings informed the design of our search procedure, where local search methods are not advantageous.</p>
<p>Our findings suggest that performance spread caused by arbitrary prompt formatting choices may influence conclusions made about model performance, especially when comparing models on benchmark tasks. Thus, we recommend that work evaluating LLMs with prompting-based methods would benefit from reporting a range of performance across plausible formats. However, we want to emphasize that single-format evaluation may still be sufficient for many use cases. For example, for researchers or practitioners who build systems on top of LLMs, choosing a single prompt format that works sufficiently well for use in this larger system is a valid methodological choice. However, we encourage future research to compute FORMATSPREAD when comparing their systems to out-of-the-box models, to ensure fair baseline representation. Furthermore, FORMATSPREAD can be used to identify lower-bound performance of a model or system. For example, when using a model for socially impactful tasks, such as stereotype classification in Figure 1, it is important to report the range of accuracy a non-adversarial user might encounter. Likewise, it is crucial to consider robustness to spurious features when claiming that models possess general abilities, such as theory of mind; and beneficial to report when e.g. exploring model biases. We leave it to future research to develop regularization procedures either during training or with an already-trained model to make models robust to diverse formatting choices.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>7 ACKNOWLEDGEMENTS</h1>
<p>We thank Jillian Fisher, Sachin Kumar, Angela Zhou, and the Berkeley NLP group for valuable discussions. This work was conducted while A.S. was a Young Investigator at AI2. This material is based upon work partly funded by the DARPA CMO under Contract No. HR001120C0124, by DARPA MCS program through NIWC Pacific (N66001-19-2-4031), by NSF DMS-2134012, IIS2125201, IIS-2203097, by NSF CAREER Grant No. IIS2142739, and an Alfred P. Sloan Foundation Fellowship. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily state or reflect those of the United States Government or any agency thereof.</p>
<h2>REFERENCES</h2>
<p>Armen Aghajanyan. Tweet: Susan \&amp; I found MMLU performance jump 6-10 points in the 40 s by formatting multiple choice as (A) not A in MMLU (for internal model). All evaluation of LLM's are broken. Evaluating a task requires marginalizing across all prompts that describe the task, not point estimate of one. June 2023. URL https://twitter.com/ArmenAgha/status/1669084129261162497.</p>
<p>Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Merouane Debbah, Etienne Goffinet, Daniel Heslow, Julien Launay, Quentin Malartic, Badreddine Noune, Baptiste Pannier, and Guilherme Penedo. Falcon-40B: an open large language model with state-of-the-art performance. 2023.</p>
<p>Olivier Chapelle and Lihong Li. An empirical evaluation of thompson sampling. Advances in neural information processing systems, 24, 2011.</p>
<p>Tianqi Chen and Carlos Guestrin. XGBoost: A scalable tree boosting system. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '16, pp. 785-794, New York, NY, USA, 2016. ACM. ISBN 978-1-4503-4232-2. doi: 10.1145/ 2939672.2939785. URL http://doi.acm.org/10.1145/2939672.2939785.</p>
<p>Jonathan H Clark, Chris Dyer, Alon Lavie, and Noah A Smith. Better hypothesis testing for statistical machine translation: Controlling for optimizer instability. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pp. $176-181,2011$.</p>
<p>Mingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan Wang, Han Guo, Tianmin Shu, Meng Song, Eric Xing, and Zhiting Hu. RLPrompt: Optimizing discrete text prompts with reinforcement learning. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 3369-3391, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.222. URL https://aclanthology.org/2022.emnlp-main. 222.</p>
<p>Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. GPT3.int8(): 8-bit matrix multiplication for transformers at scale. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=dXiGWqBoxaD.</p>
<p>Ning Ding, Shengding Hu, Weilin Zhao, Yulin Chen, Zhiyuan Liu, Haitao Zheng, and Maosong Sun. Openprompt: An open-source framework for prompt-learning. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pp. $105-113,2022$.</p>
<p>Yanai Elazar, Nora Kassner, Shauli Ravfogel, Abhilasha Ravichander, Eduard Hovy, Hinrich Schtze, and Yoav Goldberg. Measuring and improving consistency in pretrained language models. Transactions of the Association for Computational Linguistics, 9:1012-1031, 2021.</p>
<p>Tianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained language models better few-shot learners. pp. 3816-3830, August 2021. doi: 10.18653/v1/2021.acl-long.295. URL https: //aclanthology.org/2021.acl-long. 295.</p>
<p>Hila Gonen, Srini Iyer, Terra Blevins, Noah A Smith, and Luke Zettlemoyer. Demystifying prompts in language models via perplexity estimation. arXiv preprint arXiv:2212.04037, 2022.</p>
<p>Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger. Deep reinforcement learning that matters. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018.</p>
<p>Or Honovich, Uri Shaham, Samuel R. Bowman, and Omer Levy. Instruction induction: From few examples to natural language task descriptions. pp. 1935-1952, July 2023. doi: 10.18653/v1/ 2023.acl-long.108. URL https://aclanthology.org/2023.acl-long. 108.</p>
<p>Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Suchin Gururangan, Ludwig Schmidt, Hannaneh Hajishirzi, and Ali Farhadi. Editing models with task arithmetic. In International Conference on Learning Representations, 2023.</p>
<p>Riashat Islam, Peter Henderson, Maziar Gomrokchi, and Doina Precup. Reproducibility of benchmarked deep reinforcement learning tasks for continuous control. arXiv preprint arXiv:1708.04133, 2017.</p>
<p>Zhengbao Jiang, Frank F Xu, Jun Araki, and Graham Neubig. How can we know what language models know? Transactions of the Association for Computational Linguistics, 8:423-438, 2020.</p>
<p>Daniel Khashabi, Xinxi Lyu, Sewon Min, Lianhui Qin, Kyle Richardson, Sean Welleck, Hannaneh Hajishirzi, Tushar Khot, Ashish Sabharwal, Sameer Singh, and Yejin Choi. Prompt waywardness: The curious case of discretized interpretation of continuous prompts. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 3631-3643, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.266. URL https://aclanthology.org/2022.naacl-main. 266.</p>
<p>Tze Leung Lai, Herbert Robbins, et al. Asymptotically efficient adaptive allocation rules. Advances in applied mathematics, 6(1):4-22, 1985.</p>
<p>Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 3045-3059, 2021.</p>
<p>Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pp. 74-81, 2004.</p>
<p>Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. pp. 80868098, May 2022. doi: 10.18653/v1/2022.acl-long.556. URL https://aclanthology. org/2022.acl-long.556.</p>
<p>Aman Madaan, Katherine Hermann, and Amir Yazdanbakhsh. What makes chain-of-thought prompting effective? a counterfactual study. In Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 1448-1535, 2023.</p>
<p>Moin Nadeem, Anna Bethke, and Siva Reddy. Stereoset: Measuring stereotypical bias in pretrained language models. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 5356-5371, 2021.</p>
<p>Archiki Prasad, Peter Hase, Xiang Zhou, and Mohit Bansal. GrIPS: Gradient-free, edit-based instruction search for prompting large language models. pp. 3845-3864, May 2023. doi: 10.18653/ v1/2023.eacl-main.277. URL https://aclanthology.org/2023.eacl-main. 277.</p>
<p>Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang Zhu, and Michael Zeng. Automatic prompt optimization with" gradient descent" and beam search. arXiv preprint arXiv:2305.03495, 2023.</p>
<p>Guanghui Qin and Jason Eisner. Learning how to ask: Querying lms with mixtures of soft prompts. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), 2021.</p>
<p>Timo Schick, Sahana Udupa, and Hinrich Schtze. Self-diagnosis and self-debiasing: A proposal for reducing corpus-based bias in nlp. Transactions of the Association for Computational Linguistics, 9:1408-1424, 2021.</p>
<p>John Schulman, Barret Zoph, Christina Kim, Jacob Hilton, Jacob Menick, Jiayi Weng, Juan Felipe Ceron Uribe, Liam Fedus, Luke Metz, Michael Pokorny, et al. Chatgpt: Optimizing language models for dialogue. OpenAI blog, 2022.</p>
<p>Weijia Shi, Xiaochuang Han, Hila Gonen, Ari Holtzman, Yulia Tsvetkov, and Luke Zettlemoyer. Toward human readable prompt tuning: Kubrick's the shining is a good movie, and a good prompt too? arXiv preprint arXiv:2212.10539, 2022.</p>
<p>Taylor Shin, Yasaman Razeghi, Robert L. Logan IV, Eric Wallace, and Sameer Singh. AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts. pp. 4222-4235, November 2020. doi: 10.18653/v1/2020.emnlp-main.346. URL https: //aclanthology.org/2020.emnlp-main. 346.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.</p>
<p>Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, Eshaan Pathak, Giannis Karamanolakis, Haizhi Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi, Kuntal Kumar Pal, Maitreya Patel, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, Rushang Karia, Savan Doshi, Shailaja Keyur Sampat, Siddhartha Mishra, Sujan Reddy A, Sumanta Patro, Tanay Dixit, and Xudong Shen. Super-NaturalInstructions: Generalization via declarative instructions on 1600+ NLP tasks. pp. 5085-5109, December 2022. doi: 10.18653/v1/2022.emnlp-main. 340. URL https://aclanthology.org/2022.emnlp-main. 340.</p>
<p>Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. Jailbroken: How does llm safety training fail? arXiv preprint arXiv:2307.02483, 2023.</p>
<p>Jules White, Quchen Fu, Sam Hays, Michael Sandborn, Carlos Olea, Henry Gilbert, Ashraf Elnashar, Jesse Spencer-Smith, and Douglas C Schmidt. A prompt pattern catalog to enhance prompt engineering with chatgpt. arXiv preprint arXiv:2302.11382, 2023.</p>
<p>Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation with bert. In International Conference on Learning Representations, 2019.</p>
<p>Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. Large language models are human-level prompt engineers. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/ forum?id=92gvk82DE-.</p>
<p>Andy Zou, Zifan Wang, J Zico Kolter, and Matt Fredrikson. Universal and transferable adversarial attacks on aligned language models. arXiv preprint arXiv:2307.15043, 2023.</p>
<h1>A Grammar Definition and InStantIation Details</h1>
<h2>A. 1 Equivalence Relation Definition</h2>
<p>Precisely, $p_{1} \sim p_{2}$ if and only if at least one of the following hold: $p_{1}=p_{2}=B_{0}$; or $p_{i}=$ $B_{0}^{\prime}\left(d_{i}, s_{i}\right)$ with $d_{1}=d_{2}$; or $p_{i}=B_{1}\left(d_{i}, s_{i}, f_{i}\right)$ with $d_{1}=d_{2}$; or $p_{i}=B_{2}^{(n)}\left(X_{1, i}, \ldots, X_{n, i}, c_{i}\right)$ with $X_{j, 1} \sim X_{j, 2} \forall 1 \leq j \leq n$; or $p_{i}=B_{3}^{(n)}\left(d_{i}, j_{1, i}, \ldots, j_{n, i}, s_{1}, s_{2}, c, f\right)$ where $d_{1}=d_{2}$ and $j_{k, 1}=j_{k, 2} \forall 1 \leq k \leq n$. It is possible that generated formats equivalent in their string representation are not equivalent according to this equivalence relation.</p>
<h2>A.1.1 Visualization of Prompt Format's Parsing and Full Format Generation</h2>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10: Visualization of a complex prompt format showing its parsing and which constants or functions affect each part of the format.</p>
<p>Figure 10 shows a visualization of how a complex format is parsed using our defined grammar. A full prompt consists of an instruction, n few-shots and a data point to solve. For example, if the instruction was Given a sentence and two words that appear in it, answer which one of the two (A or B) appeared first in the sentence., a full prompt may look as follow. Note that we always use $\backslash \mathrm{n} \backslash \mathrm{n}$ as space character between instruction and few-shots. The example below shows a 1-shot prompt. It is simply illustrative and does not correspond to any of the tasks considered.</p>
<div class="codehilite"><pre><span></span><code><span class="nt">Given</span><span class="w"> </span><span class="nt">a</span><span class="w"> </span><span class="nt">sentence</span><span class="w"> </span><span class="nt">and</span><span class="w"> </span><span class="nt">two</span><span class="w"> </span><span class="nt">words</span><span class="w"> </span><span class="nt">that</span><span class="w"> </span><span class="nt">appear</span><span class="w"> </span><span class="nt">in</span><span class="w"> </span><span class="nt">it</span><span class="o">,</span><span class="w"> </span><span class="nt">answer</span><span class="w"> </span><span class="nt">which</span><span class="w"> </span><span class="nt">one</span><span class="w"> </span><span class="nt">of</span>
<span class="nt">the</span><span class="w"> </span><span class="nt">two</span><span class="w"> </span><span class="o">(</span><span class="nt">A</span><span class="w"> </span><span class="nt">or</span><span class="w"> </span><span class="nt">B</span><span class="o">)</span><span class="w"> </span><span class="nt">appeared</span><span class="w"> </span><span class="nt">first</span><span class="w"> </span><span class="nt">in</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">sentence</span><span class="o">.</span>
<span class="nt">The</span><span class="w"> </span><span class="nt">quick</span><span class="w"> </span><span class="nt">brown</span><span class="w"> </span><span class="nt">fox</span><span class="w"> </span><span class="nt">jumps</span>
<span class="nt">OPTIONS</span><span class="o">:</span>
<span class="nt">CHOICE</span><span class="w"> </span><span class="o">(</span><span class="nt">A</span><span class="o">):</span><span class="w"> </span><span class="nt">fox</span><span class="w"> </span><span class="o">;</span><span class="w"> </span><span class="nt">CHOICE</span><span class="w"> </span><span class="o">(</span><span class="nt">B</span><span class="o">):</span><span class="w"> </span><span class="nt">brown</span>
<span class="nt">ANSWER</span><span class="o">:</span><span class="w"> </span><span class="nt">B</span>
<span class="nt">Over</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">lazy</span><span class="w"> </span><span class="nt">dog</span>
<span class="nt">OPTIONS</span><span class="o">:</span>
<span class="nt">CHOICE</span><span class="w"> </span><span class="o">(</span><span class="nt">A</span><span class="o">):</span><span class="w"> </span><span class="nt">lazy</span><span class="w"> </span><span class="o">;</span><span class="w"> </span><span class="nt">CHOICE</span><span class="w"> </span><span class="o">(</span><span class="nt">B</span><span class="o">):</span><span class="w"> </span><span class="nt">dog</span>
<span class="nt">ANSWER</span><span class="o">:</span>
</code></pre></div>

<p>FormatSpreAd forces all instantiations of a multiple choice variable to change jointly to maintain coherence, and this includes text in the instruction. Therefore, when changing the option items from A and B to I and II, the prompt will be generated as follows.</p>
<div class="codehilite"><pre><span></span><code><span class="nt">Given</span><span class="w"> </span><span class="nt">a</span><span class="w"> </span><span class="nt">sentence</span><span class="w"> </span><span class="nt">and</span><span class="w"> </span><span class="nt">two</span><span class="w"> </span><span class="nt">words</span><span class="w"> </span><span class="nt">that</span><span class="w"> </span><span class="nt">appear</span><span class="w"> </span><span class="nt">in</span><span class="w"> </span><span class="nt">it</span><span class="o">,</span><span class="w"> </span><span class="nt">answer</span><span class="w"> </span><span class="nt">which</span><span class="w"> </span><span class="nt">one</span><span class="w"> </span><span class="nt">of</span>
<span class="nt">the</span><span class="w"> </span><span class="nt">two</span><span class="w"> </span><span class="o">(</span><span class="nt">I</span><span class="w"> </span><span class="nt">or</span><span class="w"> </span><span class="nt">II</span><span class="o">)</span><span class="w"> </span><span class="nt">appeared</span><span class="w"> </span><span class="nt">first</span><span class="w"> </span><span class="nt">in</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">sentence</span><span class="o">.</span>
<span class="nt">The</span><span class="w"> </span><span class="nt">quick</span><span class="w"> </span><span class="nt">brown</span><span class="w"> </span><span class="nt">fox</span><span class="w"> </span><span class="nt">jumps</span>
<span class="nt">OPTIONS</span><span class="o">:</span>
<span class="nt">CHOICE</span><span class="w"> </span><span class="o">(</span><span class="nt">I</span><span class="o">):</span><span class="w"> </span><span class="nt">fox</span><span class="w"> </span><span class="o">;</span><span class="w"> </span><span class="nt">CHOICE</span><span class="w"> </span><span class="o">(</span><span class="nt">II</span><span class="o">):</span><span class="w"> </span><span class="nt">brown</span>
<span class="nt">ANSWER</span><span class="o">:</span><span class="w"> </span><span class="nt">II</span>
<span class="nt">Over</span><span class="w"> </span><span class="nt">the</span><span class="w"> </span><span class="nt">lazy</span><span class="w"> </span><span class="nt">dog</span>
<span class="nt">OPTIONS</span><span class="o">:</span>
<span class="nt">CHOICE</span><span class="w"> </span><span class="o">(</span><span class="nt">I</span><span class="o">):</span><span class="w"> </span><span class="nt">lazy</span><span class="w"> </span><span class="o">;</span><span class="w"> </span><span class="nt">CHOICE</span><span class="w"> </span><span class="o">(</span><span class="nt">II</span><span class="o">):</span><span class="w"> </span><span class="nt">dog</span>
<span class="nt">ANSWER</span><span class="o">:</span>
</code></pre></div>

<h1>A. 2 ALLOWED VALUES FOR EACH SET $\mathcal{S}<em 2="2">{1}, \mathcal{S}</em>}, \mathcal{C}, \mathcal{F<em _ITEM="{ITEM" _text="\text">{\text {CASING }}, \mathcal{F}</em>$}</h1>
<p>$$
\begin{aligned}
&amp; \mathcal{S}<em 2="2">{1}=\left{{ }^{\prime \prime}, \prime^{\prime}, \prime^{\prime}, \backslash \mathrm{n}^{\prime}, \backslash \mathrm{n}^{\prime}, \mathrm{n}^{\prime},--^{\prime}, \prime^{\prime}, \prime^{\prime} ; \backslash \mathrm{n}^{\prime}, \prime\left|^{\prime}, \prime&lt;\operatorname{sep}&gt;\right.^{\prime}, \prime--^{\prime}, \prime, \prime^{\prime}, \backslash \mathrm{n}^{\prime}, \prime, \prime^{\prime}, \backslash \mathrm{n}^{\prime}, \prime, \prime^{\prime}, \prime,^{\prime}\right} \
&amp; \mathcal{S}</em> \
&amp; \mathcal{C}=\left{{ }^{\prime \prime}, \prime::^{\prime}, \prime^{\prime}::^{\prime}, \prime^{\prime}, \prime^{\prime}, \backslash \mathrm{n} \backslash \mathrm{t}^{\prime}, \backslash \mathrm{n}^{\prime}, \prime^{\prime}:^{\prime}, \prime,-^{\prime}, \prime^{\prime}, \prime^{\prime} \backslash \mathrm{n}^{\prime}, \backslash \mathrm{n} \backslash \mathrm{t}^{\prime}, \prime^{\prime}, \prime^{\prime}::^{\prime}, \prime-^{\prime}, \backslash \mathrm{t}^{\prime}\right} \
&amp; \mathcal{F}}=\left{{ }^{\prime \prime}, \prime^{\prime}, \prime^{\prime}, \prime^{\prime}, \backslash \mathrm{t}^{\prime}\right} \text { (no space, single space, double space, tab) <em _item="{item" _text="\text">{\text {casing }}=\left{\mathbf{f}(\mathrm{x})=\mathrm{x}, \mathbf{f}(\mathrm{x})=\mathrm{x} . \text { title }(), \mathbf{f}(\mathrm{x})=\mathrm{x} . \text { upper }(), \mathbf{f}(\mathrm{x})=\mathrm{x} . \text { lower }()\right} \
&amp; \mathcal{F}</em>}}=\left{x \mapsto f(g(x)) \mid \text { such that } f \in \mathcal{F<em _item2="{item2" _text="\text">{\text {item1 }} \wedge g \in \mathcal{F}</em>\right} \
&amp; \mathcal{F}}<em _item2="{item2" _text="\text">{\text {item1 }}=\left{\mathrm{x} \mapsto(\mathrm{x}), \mathrm{x} \mapsto \mathrm{x} ., \mathrm{x} \mapsto \mathrm{x}), \mathrm{x} \mapsto \mathrm{x} ., \mathrm{x} \mapsto[\mathrm{x}], \mathrm{x} \mapsto&lt;\mathrm{x}&gt;\right} \
&amp; \mathcal{F}</em>\right. \
&amp; \left.\mathrm{x} \rightarrow 0 \mathrm{x} 215 \mathrm{~F}+\mathrm{x}+1, \mathrm{x} \rightarrow \operatorname{ROMAN}[\mathrm{x}] . \text { lower }(), \mathrm{x} \rightarrow \operatorname{ROMAN}[\mathrm{x}] . \text { upper }()\right}
\end{aligned}
$$}}=\left{\mathrm{x} \rightarrow \mathrm{x}+1, \mathrm{x} \rightarrow{ }^{\prime} \mathrm{A}^{\prime}+\mathrm{x}, \mathrm{x} \rightarrow{ }^{\prime} \mathrm{a}^{\prime}+\mathrm{x</p>
<p>Enumerations are indexed from (i.e., "1, 2, 3" rather than "0, 1, 2"). ROMAN[x] represents the Roman numerals written in regular ASCII characters. ' $0 \times 215 \mathrm{~F}^{\prime}+\mathrm{x}$ represent the series of Unicode characters for Roman numerals. ... denotes a spacing character for clarity.</p>
<h2>A. 3 RESTRICTIONS TO PROMPT FORMATS SPACES AND SEPARATORS' COMBINATIONS</h2>
<p>We define several restrictions to ensure format naturalness. Users can additionally customize FORMATSPREAD by defining their own rules and restrictions between values. Our rules are as follows:</p>
<ul>
<li>If $B_{2}\left(X_{1}, \ldots, X_{n}, c\right)$ where $c$ does not contain a newline, then each $X_{i}$ 's separators and any subcomponents' separators should not contain a newline.</li>
<li>Similar to the rule above, if $B_{3}^{(n)}\left(d, j_{1}, \ldots, j_{n}, s_{1}, s_{2}, c, f_{1}, f_{2}\right)$ such that some separator contains a newline (i.e. $s_{1}$ contains a newline and/or $s_{2}$ contains a newline) then the space $c$ must also contain a newline.</li>
<li>For $B_{1}(d, s, f):=f(d) s&lt;$ text $&gt;, s$ must not be the empty string (i.e., there has to be some separation between descriptor and text).</li>
<li>Having $c$ be an empty string space in $B_{2}^{(n)}$ is only allowed if the first $n-1$ components are $B_{1}$ fields with an empty <text>. Similarly, the newline restrictions mentioned above only apply if the <text> is not empty. This rarely happens in prompt formats, but there are formats such as Question: <text> Options: A. <text> B. <text> where the Options: do not have a corresponding field.</li>
</ul>
<h2>A. 4 THOMPSON SAMPLING PriORS</h2>
<p>For the first exploration (i.e., finding the best-performing prompt format), we set an informative prior $\operatorname{Beta}(\alpha, \beta):=\operatorname{Beta}\left(\max \left(\frac{\beta \cdot x}{1-x}, 1.1\right), 5\right)$ for all arms $p_{i}$, where $x$ is the original format's accuracy. Our goal is to set an informative prior where the expected value of the prior distribution is the original format accuracy $x$, since a priori it is the only information we have about performance.</p>
<p>This restricts the parameters as follows:</p>
<p>$$
\begin{aligned}
\mathbb{E}[\operatorname{Beta}(\alpha, \beta)]=\frac{\alpha}{\alpha+\beta} &amp; =x \
\alpha &amp; =\alpha \cdot x+\beta \cdot x \
\alpha &amp; =\frac{\beta \cdot x}{1-x}
\end{aligned}
$$</p>
<p>Since $\beta$ will modulate how confident is the prior, and we want to avoid the model being overconfident, we fix $\beta=5$. Because we want to have an informative prior $\operatorname{Beta}(\alpha, \beta)$ with a Gaussian-like PDF, we force $\alpha&gt;1$ and $\beta&gt;1$. In extreme cases, forcing $\alpha&gt;1$ might alter the expected value. The first exploration's priors are thus exactly $\operatorname{Beta}(\alpha, \beta)$ with $\alpha=\max \left(\frac{\beta \cdot x}{1-x}, 1.1\right)$ and $\beta=5$ for all arms $p_{i}$.</p>
<p>For the second exploration (i.e., finding the worst-performing prompt format), the model has access to the first explorations' counters $S_{i}^{(E / B)}$ and $N_{i}^{(E / B)}$. Therefore, we set the second exploration's priors to be $\operatorname{Beta}\left(\alpha+S_{i}^{(E / B)}, \beta+\left(N_{i}^{(E / B)}-S_{i}^{(E / B)}\right)\right)$.</p>
<h1>B Additional Experiments' Information and Plots</h1>
<h2>B. 1 TASK SELECTION</h2>
<p>We use a number of heuristics to filter Super-NaturalInstructions tasks to our set of 53 evaluation tasks. Datasets should have at least 1000 samples to be considered. We also remove tasks whose instructions are too long (over 3,000 characters) and datasets with inputs longer than 2,000 characters, given that this makes performing inference at scale intractable. We also filter datasets whose valid outputs include more than 20 different strings, given that we focus on classification tasks.</p>
<p>We also removed tasks where we found a priori performance on the task was $0 \%$ accuracy using LLaMA-2-7B 1-shot. Some Super-NaturalInstructions tasks are derived from the same original dataset, but ask different questions. We did not include more than 4 tasks from the same original dataset.</p>
<p>Finally, we also searched for having socially impactful tasks. Those tasks were the only SuperNaturalInstructions tasks where we included a format if one was not provided by the dataset.</p>
<p>The selected tasks were the following 53: task050, task065, task069, task070, task114, task133, task155, task158, task161, task162, task163, task190, task213, task214, task220, task279, task280, task286, task296, task297, task316, task317, task319, task320, task322, task323, task325, task326, task327, task328, task335, task337, task385, task580, task607, task608, task609, task904, task905, task1186, task1283, task1284, task1297, task1347, task1387, task1419, task1420, task1421, task1423, task1502, task1612, task1678, task1724.
<img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 11: Comparison between Llama-2-7B and Llama-2-70B spreads. Llama-2-70B was computed using 4bit quantization (Dettmers et al., 2022).</p>
<p>B. 2 Additional Results for Section 4.2</p>
<p>Table 3: Ratio of prompt format pairs $\left(p_{1}, p_{2}\right)$ such that if $p_{1}$ is worse than $p_{2}$ using model $M_{1}$, then the same trend holds for $M_{2}$.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model 1 <br> $\left(M_{1}\right)$</th>
<th style="text-align: center;">Model 2 <br> $\left(M_{2}\right)$</th>
<th style="text-align: center;">Performance Relative <br> Ordering Preservation</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Llama-2-7b</td>
<td style="text-align: center;">Llama-2-13b</td>
<td style="text-align: center;">$57.46 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Llama-2-7b</td>
<td style="text-align: center;">Falcon-2-7b</td>
<td style="text-align: center;">$55.91 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Falcon-7b</td>
<td style="text-align: center;">Falcon-7b-Inst</td>
<td style="text-align: center;">$61.11 \%$</td>
</tr>
</tbody>
</table>
<p><img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 12: Probability of a prompt $p$ being worse than $p^{\prime}$ by at least $d$ points using model $M^{\prime}$, given that prompt $p$ was better than prompt $p^{\prime}$ when using model $M$.</p>
<p>Formats are not inherently good or bad. Table 3 shows that if format $p_{1}$ has lower performance than format $p_{2}$ under model $M$, there is $&lt;0.62$ probability that this trend would hold under another model $M^{\prime}$ (random chance is 0.5 ). This weak relative order preservation suggests that prompt format performance in a model may not be extrapolated to a different model, or in other words, that there are no inherently good or bad formats. This finding is further supported by Figure 12, which shows that findings of a format being better or worse than another are often inconsistent across models.</p>
<p>Experiments with exact prefix matching accuracy. Here we show results with using exact prefix matching to compute accuracy. Often, failures in prefix matching are associated with degeneration, i.e., cases where the model does not answer any of the valid options, motivating the use of ranking accuracy. Degeneration makes models (specially smaller models) more unlikely to have high accuracy out of the box. As seen in Figure 6, prefix matching is linked to having higher changes when performing atomic changes. Moreover, exact prefix matching can lead to lower performance as generation is less constrained (see Figure 16). Table 4 shows examples of atomic changes yielding large accuracy changes with exact prefix matching metric.</p>
<p>Figure 13c shows spread remains regardless of model size increase, architecture change, or number of few-shot examples also when using exact prefix matching as accuracy metric. In line with the results shown for probability ranking in Section 4.2, Figure 15 shows that the probability of reversing performance trends between two models just by changing prompt remains high when using exact prefix matching as metric. Strikingly, spread is significantly higher than in the probability ranking setting (see Figure 14), with median spread ranging from 12 to 28 accuracy points depending on the model used. This further motivates the need for running FORMATSPREAD when benchmarking models with this accuracy metric. This increased spread may be partly due to degeneration, as we will detail next.</p>
<p>Degeneration. Sometimes when a model does not generate the correct answer with exact prefix matching, it also does not generate a valid response, i.e. it degenerates. We will now quantify this phenomenon using 53 SuperNaturalInstructions classification and multiple choice tasks.</p>
<p>Given a model, a task, and a format, let the centered mass be the ratio of examples where the model's output matched with any valid option (regardless of correctness). Table 5 shows that the correlation between accuracy and centered mass is moderate or high depending on the model. This suggests that very often when a model does not return a valid answer, it does not return any valid answer</p>
<p><img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Figure 13: Spread comparison between evaluating the same task under different models or n-shots using exact prefix matching as accuracy metric.
<img alt="img-13.jpeg" src="img-13.jpeg" /></p>
<p>Figure 14: Spread across models and $n$-shots. Exact prefix matching metric.
<img alt="img-14.jpeg" src="img-14.jpeg" />
(a) Accuracy boxplot for the selected 53 Super Natural-Instructions tasks, option ranking metric.
<img alt="img-15.jpeg" src="img-15.jpeg" /></p>
<p>Figure 15: Probability that model $M$ performs worse than $M^{\prime}$ by at least $d$ when using format $p^{\prime}$, given that $M$ performed better than $M^{\prime}$ by at least $d$ using format $p .53$ tasks, 1- and 5-shot. Exact prefix matching metric.
<img alt="img-16.jpeg" src="img-16.jpeg" />
(b) Accuracy boxplot selected 53 Super NaturalInstructions tasks, exact prefix matching metric.</p>
<p>Figure 16: Accuracy metric used can strongly impact final performance. 53 Super NaturalInstructions tasks shown. Ranking accuracy yields higher accuracies overall.
at all. This is especially true for Falcon models, where we observe an almost perfect correlation between accuracy and centered mass. In conclusion, prompt format chosen often do not solely affect accuracy, but they also affect the frequency in which a model is actually able to perform a task. This will especially affect tasks for which there are no alternative metrics. Further research may focus specifically on targeting features that cause degeneration.</p>
<p>Experiments with Instruction Induction tasks. All experiments thus far focused solely on classification tasks. We will now focus on tasks that require generating (short) text, and cannot be framed as classification tasks. We selected 10 tasks from Instruction Induction (Honovich et al., 2023) that require generating a unique, valid string to be considered a correct response. Examples include identifying the second letter of a word, adding numbers, or answering a synonym to a given word. Instruction Induction tasks also show a wide range of difficulty, resulting in varied settings to be</p>
<p>Table 4: Examples of atomic changes' impact on accuracy using prefix matching (probability ranking shown in Table 2). $}$ represents a text field; $p_{2}$ yields higher accuracy than $p_{1}$ for all tasks.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task Id</th>
<th style="text-align: center;">Prompt Format $1\left(p_{1}\right)$</th>
<th style="text-align: center;">Prompt Format $2\left(p_{2}\right)$</th>
<th style="text-align: center;">Acc $p_{1}$</th>
<th style="text-align: center;">Acc $p_{2}$</th>
<th style="text-align: center;">Diff.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">task213</td>
<td style="text-align: center;">Title: {} Sentence $&lt;$ X $&gt;$ : {}</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Sentence $&lt;$ X $&gt;$ : {}</td>
<td style="text-align: center;">Title:({} Sentence $&lt;$ X $&gt;$ : : {}</td>
<td style="text-align: center;">0.113</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">| Sentence $&lt;$ X $&gt;$ : {}</td>
<td style="text-align: center;">| Sentence $&lt;$ X $&gt;$ : : {}</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Sentence $&lt;$ C $&gt;$ : {} } }</td>
<td style="text-align: center;">Sentence $&lt;$ C $&gt;$ : : {}</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$&lt;$ i $&gt;$ : : : {} } }</td>
<td style="text-align: center;">Sentence $&lt;$ C $&gt;$ : : {}</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Answer: {}</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">task296</td>
<td style="text-align: center;">Sentence I : : {} \nSentence II</td>
<td style="text-align: center;">Sentence I ) : {} \nSentence II</td>
<td style="text-align: center;">0.201</td>
<td style="text-align: center;">0.522</td>
<td style="text-align: center;">0.321</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">) : {} \nSentence III ) : {}</td>
<td style="text-align: center;">) : {} \nSentence III ) : {}</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">\nSentence IV ) : {} \nSentence</td>
<td style="text-align: center;">\nSentence IV ) : {} \nSentence</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">V ) : {} \nSentence VI ) : {}</td>
<td style="text-align: center;">V ) : {} \nSentence VI ) : {}</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">\nSentence VII ) : {} \nSentence</td>
<td style="text-align: center;">\nSentence VII ) : {} \nSentence</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">VIII ) : {} \nSentence IX ) : {}</td>
<td style="text-align: center;">VIII ) : {} \nSentence IX ) : {}</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">\nSentence X ) : {}, I : {},</td>
<td style="text-align: center;">\nSentence X ) : {}, I, : {}</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">II. : {}, Answer: {}</td>
<td style="text-align: center;">, 2. : {}, Answer: {}</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">task905</td>
<td style="text-align: center;">Tweet::: {}, \nLabel::: {},</td>
<td style="text-align: center;">Tweet::{}, \nLabel::{},</td>
<td style="text-align: center;">0.252</td>
<td style="text-align: center;">0.559</td>
<td style="text-align: center;">0.307</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">\nAnswer::: {}</td>
<td style="text-align: center;">\nAnswer:: {}</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">task317</td>
<td style="text-align: center;">Passage:: {} \nAnswer:: {}</td>
<td style="text-align: center;">Passage::{} \nAnswer::{}</td>
<td style="text-align: center;">0.245</td>
<td style="text-align: center;">0.546</td>
<td style="text-align: center;">0.301</td>
</tr>
<tr>
<td style="text-align: center;">task280</td>
<td style="text-align: center;">passage {}\n answer {}</td>
<td style="text-align: center;">passage: {}\n answer: {}</td>
<td style="text-align: center;">0.332</td>
<td style="text-align: center;">0.612</td>
<td style="text-align: center;">0.28</td>
</tr>
<tr>
<td style="text-align: center;">task050</td>
<td style="text-align: center;">SENTENCE - {} \nQUESTION - {}</td>
<td style="text-align: center;">SENTENCE\n\t{} \nQUESTION\n\t{}</td>
<td style="text-align: center;">0.244</td>
<td style="text-align: center;">0.504</td>
<td style="text-align: center;">0.26</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">\nANSWER - {}</td>
<td style="text-align: center;">\nANSWER\n\t{}</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">task070</td>
<td style="text-align: center;">Beginning - {}\nMiddle [I]{}</td>
<td style="text-align: center;">Beginning - {}\nMiddle I]{}</td>
<td style="text-align: center;">0.143</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">0.157</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">, Middle [II]{}\nEnding -</td>
<td style="text-align: center;">, Middle II]{}\nEnding -</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">{} \nAnswer - {}</td>
<td style="text-align: center;">{} \nAnswer - {}</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 5: Correlation between accuracy using exact prefix matching and the centered mass (the opposite of degeneration). 53 tasks, 10 formats each, evaluated on 1000 samples.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">n-shot</th>
<th style="text-align: center;">correlation between accuracy <br> $\&amp;$ and centered mass</th>
<th style="text-align: center;">p-value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Llama-2-7b</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0.702</td>
<td style="text-align: center;">$5.1 \mathrm{E}-77$</td>
</tr>
<tr>
<td style="text-align: left;">Llama-2-7b</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">0.762</td>
<td style="text-align: center;">$4.9 \mathrm{E}-98$</td>
</tr>
<tr>
<td style="text-align: left;">Llama-2-13b</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0.639</td>
<td style="text-align: center;">$5.8 \mathrm{E}-61$</td>
</tr>
<tr>
<td style="text-align: left;">Llama-2-13b</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">0.662</td>
<td style="text-align: center;">$9.2 \mathrm{E}-67$</td>
</tr>
<tr>
<td style="text-align: left;">falcon-7b</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0.936</td>
<td style="text-align: center;">$7.1 \mathrm{E}-233$</td>
</tr>
<tr>
<td style="text-align: left;">falcon-7b</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">0.933</td>
<td style="text-align: center;">$8.4 \mathrm{E}-228$</td>
</tr>
<tr>
<td style="text-align: left;">falcon-7b-instruct</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0.962</td>
<td style="text-align: center;">$3.6 \mathrm{E}-289$</td>
</tr>
<tr>
<td style="text-align: left;">falcon-7b-instruct</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">0.958</td>
<td style="text-align: center;">$5.5 \mathrm{E}-277$</td>
</tr>
</tbody>
</table>
<p>analyzed (see Figure 18b). Given that the collection does not contain human-generated formats, we applied a simple 'Input: {}\n Output: {}' format. Results for 1-shot and 5-shot settings show spread is still high across models and n-shot choices (see Figure 17).</p>
<p>Tasks are: antonyms, diff, first_word_letter, larger_animal, letters_list, num_to_verbal, second_word_letter, singular_to_plural, sum, synonyms.
<img alt="img-17.jpeg" src="img-17.jpeg" /></p>
<p>Figure 17: Spread comparison between evaluating the same task under different models or n-shots for Instruction Induction tasks. Exact prefix matching used as accuracy metric.</p>
<p><img alt="img-18.jpeg" src="img-18.jpeg" /></p>
<p>Figure 18: Instruction Induction tasks' spreads and accuracy across models. Exact prefix matching is used as accuracy metric.</p>
<p>Experiments with continuous metrics in open-ended text generation tasks. Throughout the paper we focus on tasks with a single valid output, whether in classification tasks or in short-text generation tasks. This decision is intentional, since it guarantees that a variation in the metric truly represents a variation in model performance. We have shown that spread remains high when considering option ranking or exact prefix matching as accuracy metric.</p>
<p>Since LLMs are often used in more open-ended generation contexts, we will now explore the performance variance across prompt formats when considering sentence-length generation tasks (e.g. generate the next sentence of a story, given the four initial sentences of a story, generate a question whose answer is the sentence given). To analyze the automatic generations, we use two widely used metrics: ROUGE-L (Lin, 2004), and BERTScore (Zhang et al., 2019). The first is an n-gram-based metric, and the latter is a model-based metric, and both are $[0,1]$ metrics where higher is better. Figure 19 shows that variance remains high for LLaMA-2-7B regardless of the metric and the number of n-shots considered, with LLaMA-2-7B 5-shot having $25 \%$ of tasks with a ROUGE-L spread of 0.098 or higher, and a BERTScore spread of 0.09 or higher.</p>
<p>We observe that the median spread is sometimes smaller than in the accuracy tasks. This may be because although ROUGE, BERTScore, and accuracy are all $[0,1]$ metrics, typical metric values may be different, which may in turn affect the final spread (an absolute difference). We leave it to future work to quantify the differences in style or content that each format may be inducing.</p>
<p>Finally, it is worth noting that text generation metrics are known to be noisier, and thus not all metric decreases necessarily correspond to a true performance loss, as is the case for accuracy in single-valid-output tasks. We used 17 SuperNatural Instructions tasks: task037, task038,
<img alt="img-19.jpeg" src="img-19.jpeg" /></p>
<p>Figure 19: Spread across n-shots for LLaMA-2-7B, considering ROUGE-L and BERTScore metrics. 17 sentence-level open-generation tasks are considered, all extracted from SuperNatural Instructions. 10 prompt formats are considered for each task.
task040, task067, task071, task072, task105, task216, task223, task240, task348, task389, task443, task845, task1326, task1401, task1613. We selected the 17 open-ended text generation tasks among those with at least 1000 samples, with some formatting present in the original task (e.g. 'Passage: ' <text>). We only considered tasks whose instructions were under 1,000 characters and that contained inputs no longer than 5,000 characters.</p>
<p>We limit generations to 50 tokens. To parse model outputs more faithfully, and given that none of our expected generations include a newline, we only consider a models generation up to the first newline (excluding leading spaces and newlines in a given generation). This consideration is important given that often models start to generate a new data sample from scratch, immediately after generating the requested answer.
Characterizing a model's accuracy distribution beyond spread. Spread gives a quantitative jump in information with respect to informing a single point in the performance distribution since it measures the distribution range (maximum minus minimum). However, distributions that may share the same range, may yield a widely different probability of obtaining each value in the distribution. Figure 20 plots the accuracy distribution of 30 tasks, sorted in decreasing order by standard deviation. Tasks with high standard deviation reflect a higher likelihood of obtaining dissimilar values when making a formatting selection; Figure 20 shows that the median standard distribution is $\sigma \approx 0.04$, which can be considered high in our context.
<img alt="img-20.jpeg" src="img-20.jpeg" /></p>
<p>Figure 20: Accuracy distribution across 500 formats for 30 tasks evaluated on 250 samples each, sorted by standard deviation in decreasing order. LLaMA-2-7B 1-shot, option ranking metric.</p>
<p>On factors influencing spread besides prompt formatting. We believe many factors beyond formatting may be influencing performance variance, but were unable to find a feature that reliably predicts spread. We found that the average prompt length in a task has a negligible correlation with its performance spread: $r=0.228\left(p=1.4 \times 10^{-7}\right)$ for exact prefix matching metric, and $r=-0.022(p=0.615)$ for option ranking metric, when jointly considering all models and nshots. Similarly, the standard deviation of the prompt length had negligible correlation with spread: $r=0.125(p=0.004)$ for exact prefix matching, and $r=-0.099(p=0.024)$ for option ranking metric. When considering each model individually, only LLaMA-2-7B with exact prefix matching showed a correlation $|r|&gt;0.5$, with the average prompt length having a correlation $r=0.559$ $p=6.86 \times 10^{-10}$. All other settings had $|r|&lt;0.36$.</p>
<h1>B. 3 PCA EXAMPLES</h1>
<p>Section 4.4 systematically analyzes whether we can predict the prompt format that generated a given pre-softmax activation layer (i.e., prompt embeddings) by using solely its top- $n$ principal components. Figure 21 shows the top two principal components for two different tasks where all 10 formats considered are easily identifiable solely with a prompt embedding's top two principal compoenents.
<img alt="img-21.jpeg" src="img-21.jpeg" /></p>
<p>Figure 21: Plot of the top two principal components of the last decoder layer of the prompt, as a representation of the output probability distribution. Two different tasks shown, with each prompt format shown in a different color.</p>
<h2>B. 4 Notable Features</h2>
<p>As discussed in Section 4.3, sometimes the choice of a constant may lead to significantly different accuracy ranges. Figures 22,23, and 24 show all strongly dissimilar choices of constants found on any given task, across 53 Super Natural-Instructions tasks, and on both accuracy metrics considered throughout the work. As can be appreciated, choices of constants do not consistently predict performance in isolation.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{6}$ We thoroughly describe the limitations of our method in Appendix C.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
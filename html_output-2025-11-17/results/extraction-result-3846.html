<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3846 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3846</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3846</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-91.html">extraction-schema-91</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-8df67942e29cba92bb5913b62d1d2df7371842d9</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/8df67942e29cba92bb5913b62d1d2df7371842d9" target="_blank">AugGPT: Leveraging ChatGPT for Text Data Augmentation</a></p>
                <p><strong>Paper Venue:</strong> IEEE Transactions on Big Data</p>
                <p><strong>Paper TL;DR:</strong> Experimental results on multiple few-shot learning text classification tasks show the superior performance of the proposed AugGPT approach over state-of-the-art text data augmentation methods in terms of testing accuracy and distribution of the augmented samples.</p>
                <p><strong>Paper Abstract:</strong> Text data augmentation is an effective strategy for overcoming the challenge of limited sample sizes in many natural language processing (NLP) tasks. This challenge is especially prominent in the few-shot learning (FSL) scenario, where the data in the target domain is generally much scarcer and of lowered quality. A natural and widely used strategy to mitigate such challenges is to perform data augmentation to better capture data invariance and increase the sample size. However, current text data augmentation methods either can’t ensure the correct labeling of the generated data (lacking faithfulness), or can’t ensure sufficient diversity in the generated data (lacking compactness), or both. Inspired by the recent success of large language models (LLM), especially the development of ChatGPT, we propose a text data augmentation approach based on ChatGPT (named ”AugGPT”). AugGPT rephrases each sentence in the training samples into multiple conceptually similar but semantically different samples. The augmented samples can then be used in downstream model training. Experiment results on multiple few-shot learning text classification tasks show the superior performance of the proposed AugGPT approach over state-of-the-art text data augmentation methods in terms of testing accuracy and distribution of the augmented samples.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3846.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3846.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT-science-summarization (potential)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (for domain-specific scientific paper summarization; proposed usage in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The authors propose using ChatGPT to generate diverse, domain-specific summarization training samples and to perform few-shot in-context summarization of scientific papers, as a means to alleviate scarcity of annotated summarization datasets in specialized domains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>ChatGPT for domain-specific science paper summarization (proposed)</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>A proposed application of the ChatGPT LLM to produce summaries or multiple summarization-style training examples for domain-specific scientific literature via prompting and in‑context few-shot examples; suggested as a way to augment small, expert-labeled summarization datasets and to generate different representation styles.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Domain-specific scientific papers / clinical reports (the paper notes such datasets are rare, small, and often privacy-constrained; no specific corpus size or dataset was used in this work for summarization).</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Natural-language prompts and few-shot in-context examples (prompt engineering); authors explicitly describe designing prompts and including few-shot examples to instruct ChatGPT for downstream tasks (see section 4.7 and the Conclusion discussion).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Prompt-based generation / in-context few-shot prompting (the paper suggests using tailored prompts and few-shot examples to elicit summarization outputs; no retrieval-augmentation, chain-of-thought, or multi-document synthesis pipeline is implemented or evaluated here).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Concise summarization outputs or multiple augmented summarization samples in different representation styles (text summaries); intended to be used as augmented training data or direct few-shot system output.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Not performed in this paper for the summarization application; the paper suggests this as future work and notes scarcity of domain-specific datasets as a barrier to standard evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>No empirical results are reported for ChatGPT applied to scientific-paper summarization in this paper; the authors only propose the idea and argue it is promising given ChatGPT's summarization and abstraction capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Not empirically evaluated here; authors explicitly note potential limitations including domain knowledge gaps of general-domain LLMs, dataset scarcity for domain-specific evaluation, and implied risks such as incorrect domain-specific augmentations (faithfulness/hallucination). They recommend domain adaptation strategies (fine-tuning, in-context learning, knowledge distillation, style transfer) as future work.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>No direct comparison or quantitative evaluation versus baseline summarization systems or human summaries is provided in this paper for this proposed use-case.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AugGPT: Leveraging ChatGPT for Text Data Augmentation', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3846.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3846.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Covidsum</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Covidsum: A linguistically enriched SciBERT-based summarization model for COVID-19 scientific papers</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced model/dataset for summarization of COVID-19 scientific papers (SciBERT-based) cited by the authors as an example of domain-specific scientific paper summarization work that could be complemented by LLM-generated augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Covidsum: A linguistically enriched scibert-based summarization model for covid-19 scientific papers</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>Covidsum (SciBERT-based summarization)</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>A SciBERT-based summarization approach developed for COVID-19 scientific papers (cited as prior work / dataset for domain-specific scientific summarization). The current paper references it as an example domain where ChatGPT could be applied to generate additional summarization samples.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>COVID-19 scientific papers (the current paper does not specify corpus size or details; referenced only by name in the discussion).</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Summarization of COVID-19 scientific literature (task is to produce concise summaries from scientific paper text).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>SciBERT-based supervised summarization (as implied by the referenced title); this paper does not describe the original Covidsum algorithm beyond citing the work.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Extractive/abstractive summaries of scientific papers (text summaries); specifics are not described in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Not specified in this paper; Covidsum is only cited as an example — no evaluation details or metrics from the Covidsum paper are reproduced here.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>No results from the Covidsum system are reported in this paper; it is mentioned to motivate the potential value of ChatGPT for domain-specific summarization.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>The current paper notes broader challenges for domain-specific summarization: scarcity of public datasets, small scales, and privacy/expert-annotation constraints; these are cited as reasons ChatGPT-generated augmentation could be helpful.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>No comparisons are reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AugGPT: Leveraging ChatGPT for Text Data Augmentation', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3846.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3846.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChestXrayBERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChestXrayBERT: A pretrained language model for chest radiology report summarization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced pretrained language-model approach for summarizing chest radiology reports; cited as an example of domain-specific summarization where LLM-generated augmentation may be beneficial.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chestxraybert: A pretrained language model for chest radiology report summarization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>ChestXrayBERT (pretrained LM for radiology report summarization)</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>A domain-adapted pretrained language model tailored for summarizing chest radiology reports; cited here as an example of clinical summarization work that faces data scarcity and could benefit from LLM-based augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Chest radiology reports (no corpus size or dataset details provided in this paper; referenced only by title).</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Clinical radiology report summarization (task framed as mapping reports to concise summaries).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Domain-specific pretraining / fine-tuning of a transformer-based language model (details are in the referenced work; not described within this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Summaries of radiology reports (text); this paper does not provide the exact formats or examples from ChestXrayBERT beyond citing it.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Not provided in this paper (ChestXrayBERT is only cited as prior work).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>No numerical or qualitative results from ChestXrayBERT are reported in this paper; it is mentioned to illustrate potential application areas for ChatGPT-based augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Authors note general limitations for clinical summarization: dataset scarcity, need for expert annotation, privacy concerns — motivating the suggestion to use LLMs for augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>Not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AugGPT: Leveraging ChatGPT for Text Data Augmentation', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Covidsum: A linguistically enriched scibert-based summarization model for covid-19 scientific papers <em>(Rating: 2)</em></li>
                <li>Chestxraybert: A pretrained language model for chest radiology report summarization <em>(Rating: 2)</em></li>
                <li>Is chatgpt a general-purpose natural language processing task solver? <em>(Rating: 1)</em></li>
                <li>A comprehensive survey on pretrained foundation models: A history from bert to chatgpt <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3846",
    "paper_id": "paper-8df67942e29cba92bb5913b62d1d2df7371842d9",
    "extraction_schema_id": "extraction-schema-91",
    "extracted_data": [
        {
            "name_short": "ChatGPT-science-summarization (potential)",
            "name_full": "ChatGPT (for domain-specific scientific paper summarization; proposed usage in this paper)",
            "brief_description": "The authors propose using ChatGPT to generate diverse, domain-specific summarization training samples and to perform few-shot in-context summarization of scientific papers, as a means to alleviate scarcity of annotated summarization datasets in specialized domains.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "system_or_method_name": "ChatGPT for domain-specific science paper summarization (proposed)",
            "system_or_method_description": "A proposed application of the ChatGPT LLM to produce summaries or multiple summarization-style training examples for domain-specific scientific literature via prompting and in‑context few-shot examples; suggested as a way to augment small, expert-labeled summarization datasets and to generate different representation styles.",
            "input_corpus_description": "Domain-specific scientific papers / clinical reports (the paper notes such datasets are rare, small, and often privacy-constrained; no specific corpus size or dataset was used in this work for summarization).",
            "topic_or_query_specification": "Natural-language prompts and few-shot in-context examples (prompt engineering); authors explicitly describe designing prompts and including few-shot examples to instruct ChatGPT for downstream tasks (see section 4.7 and the Conclusion discussion).",
            "distillation_method": "Prompt-based generation / in-context few-shot prompting (the paper suggests using tailored prompts and few-shot examples to elicit summarization outputs; no retrieval-augmentation, chain-of-thought, or multi-document synthesis pipeline is implemented or evaluated here).",
            "output_type_and_format": "Concise summarization outputs or multiple augmented summarization samples in different representation styles (text summaries); intended to be used as augmented training data or direct few-shot system output.",
            "evaluation_or_validation_method": "Not performed in this paper for the summarization application; the paper suggests this as future work and notes scarcity of domain-specific datasets as a barrier to standard evaluation.",
            "results_summary": "No empirical results are reported for ChatGPT applied to scientific-paper summarization in this paper; the authors only propose the idea and argue it is promising given ChatGPT's summarization and abstraction capabilities.",
            "limitations_or_challenges": "Not empirically evaluated here; authors explicitly note potential limitations including domain knowledge gaps of general-domain LLMs, dataset scarcity for domain-specific evaluation, and implied risks such as incorrect domain-specific augmentations (faithfulness/hallucination). They recommend domain adaptation strategies (fine-tuning, in-context learning, knowledge distillation, style transfer) as future work.",
            "comparison_to_baselines_or_humans": "No direct comparison or quantitative evaluation versus baseline summarization systems or human summaries is provided in this paper for this proposed use-case.",
            "uuid": "e3846.0",
            "source_info": {
                "paper_title": "AugGPT: Leveraging ChatGPT for Text Data Augmentation",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "Covidsum",
            "name_full": "Covidsum: A linguistically enriched SciBERT-based summarization model for COVID-19 scientific papers",
            "brief_description": "Referenced model/dataset for summarization of COVID-19 scientific papers (SciBERT-based) cited by the authors as an example of domain-specific scientific paper summarization work that could be complemented by LLM-generated augmentation.",
            "citation_title": "Covidsum: A linguistically enriched scibert-based summarization model for covid-19 scientific papers",
            "mention_or_use": "mention",
            "system_or_method_name": "Covidsum (SciBERT-based summarization)",
            "system_or_method_description": "A SciBERT-based summarization approach developed for COVID-19 scientific papers (cited as prior work / dataset for domain-specific scientific summarization). The current paper references it as an example domain where ChatGPT could be applied to generate additional summarization samples.",
            "input_corpus_description": "COVID-19 scientific papers (the current paper does not specify corpus size or details; referenced only by name in the discussion).",
            "topic_or_query_specification": "Summarization of COVID-19 scientific literature (task is to produce concise summaries from scientific paper text).",
            "distillation_method": "SciBERT-based supervised summarization (as implied by the referenced title); this paper does not describe the original Covidsum algorithm beyond citing the work.",
            "output_type_and_format": "Extractive/abstractive summaries of scientific papers (text summaries); specifics are not described in this paper.",
            "evaluation_or_validation_method": "Not specified in this paper; Covidsum is only cited as an example — no evaluation details or metrics from the Covidsum paper are reproduced here.",
            "results_summary": "No results from the Covidsum system are reported in this paper; it is mentioned to motivate the potential value of ChatGPT for domain-specific summarization.",
            "limitations_or_challenges": "The current paper notes broader challenges for domain-specific summarization: scarcity of public datasets, small scales, and privacy/expert-annotation constraints; these are cited as reasons ChatGPT-generated augmentation could be helpful.",
            "comparison_to_baselines_or_humans": "No comparisons are reported in this paper.",
            "uuid": "e3846.1",
            "source_info": {
                "paper_title": "AugGPT: Leveraging ChatGPT for Text Data Augmentation",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "ChestXrayBERT",
            "name_full": "ChestXrayBERT: A pretrained language model for chest radiology report summarization",
            "brief_description": "Referenced pretrained language-model approach for summarizing chest radiology reports; cited as an example of domain-specific summarization where LLM-generated augmentation may be beneficial.",
            "citation_title": "Chestxraybert: A pretrained language model for chest radiology report summarization",
            "mention_or_use": "mention",
            "system_or_method_name": "ChestXrayBERT (pretrained LM for radiology report summarization)",
            "system_or_method_description": "A domain-adapted pretrained language model tailored for summarizing chest radiology reports; cited here as an example of clinical summarization work that faces data scarcity and could benefit from LLM-based augmentation.",
            "input_corpus_description": "Chest radiology reports (no corpus size or dataset details provided in this paper; referenced only by title).",
            "topic_or_query_specification": "Clinical radiology report summarization (task framed as mapping reports to concise summaries).",
            "distillation_method": "Domain-specific pretraining / fine-tuning of a transformer-based language model (details are in the referenced work; not described within this paper).",
            "output_type_and_format": "Summaries of radiology reports (text); this paper does not provide the exact formats or examples from ChestXrayBERT beyond citing it.",
            "evaluation_or_validation_method": "Not provided in this paper (ChestXrayBERT is only cited as prior work).",
            "results_summary": "No numerical or qualitative results from ChestXrayBERT are reported in this paper; it is mentioned to illustrate potential application areas for ChatGPT-based augmentation.",
            "limitations_or_challenges": "Authors note general limitations for clinical summarization: dataset scarcity, need for expert annotation, privacy concerns — motivating the suggestion to use LLMs for augmentation.",
            "comparison_to_baselines_or_humans": "Not provided in this paper.",
            "uuid": "e3846.2",
            "source_info": {
                "paper_title": "AugGPT: Leveraging ChatGPT for Text Data Augmentation",
                "publication_date_yy_mm": "2023-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Covidsum: A linguistically enriched scibert-based summarization model for covid-19 scientific papers",
            "rating": 2
        },
        {
            "paper_title": "Chestxraybert: A pretrained language model for chest radiology report summarization",
            "rating": 2
        },
        {
            "paper_title": "Is chatgpt a general-purpose natural language processing task solver?",
            "rating": 1
        },
        {
            "paper_title": "A comprehensive survey on pretrained foundation models: A history from bert to chatgpt",
            "rating": 1
        }
    ],
    "cost": 0.0129535,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>AugGPT: Leveraging ChatGPT for Text Data Augmentation</h1>
<p>Haixing Dai», Zhengliang Liu», Wenxiong Liao», Xiaoke Huang, Yihan Cao, Zihao Wu, Lin Zhao, Shaochen Xu, Wei Liu, Ninghao Liu, Sheng Li, Dajiang Zhu, Hongmin Cai, Lichao Sun, Quanzheng Li, Dinggang Shen, Tianming Liu, and Xiang Li</p>
<h4>Abstract</h4>
<p>Text data augmentation is an effective strategy for overcoming the challenge of limited sample sizes in many natural language processing (NLP) tasks. This challenge is especially prominent in the few-shot learning scenario, where the data in the target domain is generally much scarcer and of lowered quality. A natural and widely-used strategy to mitigate such challenges is to perform data augmentation to better capture the data invariance and increase the sample size. However, current text data augmentation methods either can't ensure the correct labeling of the generated data (lacking faithfulness) or can't ensure sufficient diversity in the generated data (lacking compactness), or both. Inspired by the recent success of large language models, especially the development of ChatGPT, which demonstrated improved language comprehension abilities, in this work, we propose a text data augmentation approach based on ChatGPT (named AugGPT). AugGPT rephrases each sentence in the training samples into multiple conceptually similar but semantically different samples. The augmented samples can then be used in downstream model training. Experiment results on few-shot learning text classification tasks show the superior performance of the proposed AugGPT approach over state-of-the-art text data augmentation methods in terms of testing accuracy and distribution of the augmented samples.</p>
<p>Index Terms-Large language model, few-shot learning, natural language processing, data augmentation.</p>
<h2>1 INTRODUCTION</h2>
<p>THE effectiveness of natural language processing (NLP) heavily relies on the quality and quantity of the training data. With limited training data available, which is a common issue in practice due to privacy concerns or the cost of annotations, it can be challenging to train an accurate NLP model that generalizes well to unseen samples. The challenge of training data insufficiency is especially</p>
<ul>
<li>These authors contributed equally to this paper.</li>
<li>Haixing Dai, Zhengliang Liu, Zihao Wu, Lin Zhao, Shaochen Xu, Ninghao Liu, and Tianming Liu are with the School of Computing, University of Georgia, Athens, GA, USA. (e-mail: {hd54134, zf18864 ,zw63397, lin.zhao, shaochen.xu25, ninghao.liu, tliu}@uga.edu).</li>
<li>Wenxiong Liao, Xiaoke Huang, Hongmin Cai are with the School of Computer Science and Engineering, South China University of Technology, China. (e-mail: {cssexliao@mail.scut.edu.cn, csxkhuang@mail.scut.edu.cn, hmca@scut.edu.cn).</li>
<li>Yihan Cao and Lichao Sun are with the Department of Computer Science and Engineering, Lehigh University, Bethlehem, PA, USA. (e-mail: {yihanc@andrew.cmu.edu, lis221@lehigh.edu).</li>
<li>Yihan Cao is also with the Heinz College of Information Systems and Public Policy, Carnegie Mellon University, Pittsburgh, PA, USA (e-mail: {email yihanc@andrew.cmu.edu).</li>
<li>Wei Liu is with the Department of Radiation Oncology, Mayo Clinic, Phoenix, AZ, USA. (e-mail: liu.wei@mayo.edu)</li>
<li>Sheng Li is with the School of Data Science, University of Virginia, Charlottesville, VA, USA. (email: shengli@virginia.edu)</li>
<li>Dajiang Zhu is with the Department of Computer Science and Engineering, The University of Texas at Arlington, Arlington, TX, USA. (e-mail: dajiang.zhu@uta.edu)</li>
<li>Quanzheng Li and Xiang Li are with the Department of Radiology, Massachusetts General Hospital and Harvard Medical School, Boston, MA, USA. (e-mail: li.quanzheng@mgh.harvard.edu, xiangli.shaun@gmail.com)</li>
<li>Dinggang Shen is with the School of Biomedical Engineering, ShanghaiTech University, Shanghai 201210, China. He is also with Shanghai United Imaging Intelligence Co., Ltd., Shanghai 200230, China, and Shanghai Clinical Research and Trial Center, Shanghai, 201210, China. (e-mail: Dinggang.Shen@gmail.com)
prominent in few-shot learning (FSL) scenarios, where the model trained on the original (source) domain data is expected to generalize from only a few examples in the new (target) domain [1]. Many FSL methods have shown promising results in overcoming this challenge in various tasks. Existing FSL methods mainly focus on improving the learning and generalization capability of the model via better architectural design [2], [3], leveraging pre-trained language models as the basis and then fine-tuning it using limited samples [4] with meta-learning [2], [5] or promptbased methods [6], [7], [8], [9]. However, the performance of these methods is still intrinsically limited by the data quality and quantity in both the source and target domains.</li>
</ul>
<p>Besides model development, text data augmentation can also overcome the sample size limit and work together with other FSL methods in NLP [10], [11]. Data augmentation is usually model-agnostic and involves no change to the underlying model architecture, which makes this approach particularly practical and applicable to a wide range of tasks. In NLP, there are several types of data augmentation methods. Traditional text-level data augmentation methods rely on direct operations on the existing sample base. Some frequently used techniques include synonym replacement, random deletion, and random insertion [12]. More recent methods utilize language models to generate reliable samples for more effective data augmentation, including backtranslation [13] and word vector interpolation in the latent space [14]. However, existing data augmentation methods are limited in the accuracy and diversity of the generated text data, and human annotation is still mandatory in many application scenarios [12], [15], [16].</p>
<p>The advent of (very) large language models (LLMs) such as the GPT family [6], [17] brings new opportunities for gen-</p>
<p>erating text samples that resemble human-labeled data [18], which significantly alleviates the burden of human annotators [19]. LLMs are trained in self-supervised manners, which scale up with the amount of text corpus available in the open domains. The large parameter space of LLMs also allows them to store a large amount of knowledge, while large-scale pre-training (e.g., the autoregressive objective in training GPTs) enables LLMs to encode rich factual knowledge for language generation even in very specific domains [20]. Furthermore, the training of ChatGPT follows that of Instruct-GPT [21], which utilizes reinforcement learning with human feedback (RLHF), thus enabling it to produce more informative and impartial responses to input.</p>
<p>Inspired by the success of language models in text generation, we propose a new data augmentation method named AugGPT, which leverages ChatGPT to generate auxiliary samples for few-shot text classification. We test the performance of AugGPT via experiments on both general domain and medical domain datasets. Performance comparison of the proposed AugGPT approach with existing data augmentation methods shows double-digit improvements in sentence classification accuracy. Further investigation into the faithfulness and compactness of the generated text samples reveals that AugGPT can generate more diversified augmented samples while simultaneously maintaining their accuracy (i.e., semantic similarity to the original labels). We envision that the development of LLMs will lead to humanlevel annotation performance, thus revolutionizing the field of few-shot learning and other tasks in NLP.</p>
<h2>2 Related work</h2>
<h3>2.1 Data Augmentation</h3>
<p>Data augmentation, the artificial generation of new text through transformations, is widely used to improve model training in text classification. In NLP, existing data augmentation methods work at different granularity levels: characters, words, sentences, and documents.</p>
<p>Data augmentation at the character level refers to the randomly inserting, exchanging, replacing, or deleting of characters in the text [22], which improves the robustness of the NLP model against noises. Another method called optical character recognition (OCR) data augmentation generates new text by simulating the errors that occur when using OCR tools to recognize text from pictures. Spelling augmentation [23] deliberately misspells some frequently misspelled words. Keyboard augmentation [22] simulates random typo errors by replacing a selected key with another key close to it on the QWERTY layout keyboard.</p>
<p>Data augmentation also works at the word level. Random swap augmentation randomly exchanges two words in the text, and random deletion augmentation randomly deletes some words [24]. Synonym augmentation uses synonym databases such as PPDB [25] to replace randomly selected words [26]. WordNet [27] is also widely used as a reference for synonym augmentation. These methods maintain semantic consistency and are suitable for text classification tasks. Wang et al. [28] proposed a data augmentation method based on word embeddings, replacing words with their top- $n$ similar words to create a new sentence. Different
pre-trained word embeddings are considered (e.g., GoogleNews Lexical Embeddings [29]). This method is based on the principle that words close to each other in the embedding space often appear in similar contexts, which might help with maintaining grammatical consistency.</p>
<p>However, a serious limitation of word embedding-based methods is that close words in the embedding space are not necessarily semantically similar, yet semantic changes can affect the classification results. For example, "hot" and "cold" usually appear in similar contexts, so their word embeddings are close, but they have exactly opposite semantic meanings. The counter-fitting embedding augmentation [30], [31] solves this problem by using a synonym dictionary and an antonym dictionary to adjust the initial word embeddings. Specifically, the distance between embeddings of synonyms will be shortened, and the distance between embeddings of antonyms will become enlarged.</p>
<p>Contextual augmentation [32], [33] is another wordlevel data augmentation method, which uses masked language models (MLMs) such as BERT [34], [35], DistilBERT [36] and RoBERTA [37] to generate new text based on the context. Specifically, they insert $&lt;\operatorname{mask}&gt;$ tokens in some positions of the text, or replace some words in the text with $&lt;\operatorname{mask}&gt;$ tokens, and then let the MLM predict what words should be put in these masked positions. Since MLMs are pre-trained on a large number of texts, contextual augmentation can usually generate meaningful new texts.</p>
<p>Some text data augmentation methods work at the sentence and document level. For example, back translation [38] uses translation models for data augmentation. Specifically, the language model first translates the text into another language and then translates it back to the original language. Due to the randomness of the translation process, the augmented text is different from the original text, but semantic consistency is maintained. At the document level, Gangal et al. [39] proposed a method to paraphrase the entire document to preserve document-level consistency.</p>
<p>In general, regardless of the granularity level or the text generation backbone (i.e., rule-based or language models), the goal of data augmentation is to produce sensible and diverse new samples that maintain semantic consistency.</p>
<h3>2.2 Few-shot Learning</h3>
<p>Deep learning has achieved remarkable success in various data-intensive applications. However, the performance of deep models could be affected if the dataset size is small in the downstream tasks. Few-shot Learning is a branch of science that focuses on developing solutions to address the challenge of small sample sizes [1], [40]. FSL research aims to leverage prior knowledge to rapidly generalize to new tasks that contain only a few labeled samples. A classic application scenario for few-shot learning is when obtaining supervised examples is difficult or not possible due to privacy, safety, or ethical considerations. The development of few-shot learning enables practitioners to improve the efficiency and accuracy of text classification in various scenarios and deploy practical applications.</p>
<p>Recent advances in few-shot learning have shown promising results in overcoming the challenges of limited training data for text classification. For example, a common</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1. The framework of AugGPT. a (top panel): First, we apply ChatGPT for data augmentation. We input samples of all classes into ChatGPT and prompt ChatGPT to generate samples that preserves semantic consistency with existing labelled instance. b (bottom panel): In the next step, we train a BERT-based sentence classifier on the few-shot samples and the generated data samples and evaluate the model's classification performance.</p>
<p>approach in NLP is to use a pre-trained language model such as BERT [4] as a starting point and then fine-tune it with limited samples. Some of the most recent methodological developments [2], [41] approaches that have gained traction include prompt-tuning [6], [7], [8], [9] and meta-learning [2], [5]. In general, existing FSL methods target either architectural design [2], [3], data augmentation [10], [11] or the training process [42].</p>
<p>Despite the recent development of prompt-tuning and meta-learning methods, they suffer from some major limitations. For example, prompt engineering is a cumbersome art that requires extensive experience and manual trial-and-errors [43]. Meta-learning, on the other hand, suffers from problems such as training instability [44], [45], [46] and sensitivity to hyper-parameters [44], [45]. In addition, all these FSL pipelines demand deep machine learning expertise and acquaintance with complex model architectures and training strategies, which are not attainable by common practitioners and general developers. As discussed in section 2.1, data augmentation is an effective solution for FSL and can be combined with other FSL models. Thus, the AugGPT method proposed in this paper, which has demonstrated the capability to generate accurate and comprehensive training samples, can overcome the issues of current FSL methods and potentially change the landscape of few-shot learning in NLP.</p>
<h3>2.3 Very Large Language Models</h3>
<p>Pre-trained language models (PLMs) based on the transformer architecture, such as the BERT [4] and GPT [47] model families, have revolutionized natural language processing. Compared to previous methods, they deliver state-of-the-art performance on a wide range of downstream tasks and contribute to the rising popularity and democratization of language models. In general, there are three classes of pre-trained language models: autoregressive language models (e.g., the decoder-based GPT), masked language models (e.g., the encoder-based BERT), and encoder-decoder models (e.g., BART [48] and T5 [49]). These models typically contain between 100M and 1B parameters [17].</p>
<p>In recent years, NLP communities have witnessed the rise of very large language models such as GPT-3 (175B parameters) [6], PaLM (540B parameters) [50], Bloom (176B parameters) [51], OPT (up to 175B parameters) [52], and the FLAN series (FLAN has 137B parameters) [53]. At their core, these large language models are transformer models inspired by BERT and GPT, albeit at a much larger scale.</p>
<p>Large language models aim to learn accurate latent feature representations of input text. These representations are often context-dependent and domain-dependent. For example, the vector representation of the word "treat" might be vastly different between medical domains and the general domain. For smaller pre-trained language models, it is often necessary to continuously pre-train and fine-tune such models to attain acceptable performance [54]. However, very large language models can potentially eliminate the need for fine-tuning while maintaining competitive performance [6], [18], [55].</p>
<p>Existing studies indicate that pre-trained language models can help augment a dataset with new samples with</p>
<p>TABLE 1 Generated Samples From ChatGPT on PubMed20K</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Original Text</th>
<th style="text-align: center;">Altered Text</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">The follow-up rate after 5 years was $85 \%$.</td>
<td style="text-align: center;">1. The study showed a follow-up rate of $85 \%$ after 5 years. <br> 2. The 5 -year follow-up rate was found to be significantly high at $85 \%$. <br> 3. The participants had an impressive follow-up rate of $85 \%$ after 5 years. <br> 4. The data indicated a follow-up rate of $85 \% 5$ years post-study. <br> 5. The 5 -year follow-up rate was recorded as $85 \%$, according to the findings. <br> 6. The results of the study showed a follow-up rate of $85 \%$ after a 5 -year period.</td>
</tr>
<tr>
<td style="text-align: center;">This means that the maturation of calluses was $27 \%$ faster in the LIPUS group.</td>
<td style="text-align: center;">1. The LIPUS group showed a $27 \%$ acceleration in callus maturation. <br> 2. The results revealed that the callus maturation process was $27 \%$ faster in the LIPUS group. <br> 3. The study found that the callus maturation rate in the LIPUS group was $27 \%$ quicker. <br> 4. Callus maturation was $27 \%$ more rapid in the LIPUS group compared to others. <br> 5. The callus maturation process in the LIPUS group was $27 \%$ more efficient, according to the results.</td>
</tr>
<tr>
<td style="text-align: center;">Test Registration: IRCT.ir IRCT2012071010230N1.</td>
<td style="text-align: center;">1. The trial was registered with the IRCT under the identifier IRCT2012071010230N1. <br> 2. The study was officially registered with IRCT.ir, with the registration number IRCT2012071010230N1. <br> 3. The trial was documented with IRCT.ir with the identification code IRCT2012071010230N1. <br> 4. IRCT.ir served as the official registrar for the trial, with the registration number IRCT2012071010230N1. <br> 5. The study was recorded with IRCT under the registration number IRCT2012071010230N1. <br> 6. IRCT.ir recorded the trial with the identifier IRCT2012071010230N1.</td>
</tr>
<tr>
<td style="text-align: center;">While behavioral and technological interventions can lead to some modest improvements in glycemic control, these interventions have not performed much better than conventional prevention in achieving glycemic control.</td>
<td style="text-align: center;">1. The study found that although behavioral and technological interventions led to some slight improvements in glycemic control, they were not significantly more effective than typical care. <br> 2. Despite the modest improvement in glycemic control through behavioral and technological interventions, they did not perform better than the standard care. <br> 3. The results showed that while behavioral and technological interventions resulted in some minimal gains in glycemic control, they did not surpass the usual care in achieving glycemic control. <br> 4. Although behavioral and technological interventions showed some improvement in glycemic control, they were not found to be significantly superior to the usual care. <br> 5. The study showed that the usual care was not outperformed by behavioral and technological interventions in terms of achieving glycemic control, despite some small improvements.</td>
</tr>
</tbody>
</table>
<p>similar semantic meaning [12], [16], which is of significant practical value to real-world applications. In this study, we aim to use ChatGPT, a popular LLM to conduct data augmentation. ChatGPT is based on GPT-3 [6], which was trained on massive web data with diverse and rich information. Furthermore, ChatGPT was trained through Reinforcement learning from Human Feedback (RLHF). During RLHF, human feedback is incorporated into the process of generating and selecting the best results. More specifically, a reward model is trained based on human annotators' ranking or generated results. In turn, this reward model rewards model outputs that are most aligned with human preference and human values. We believe these innovations make ChatGPT the best candidate for generating humanlevel quality data samples.</p>
<h3>2.4 ChatGPT: Present and Future</h3>
<p>ChatGPT is a game changer in natural language processing. For the first time in human history, the power of large language models is accessible to the general public through a user-friendly chatbot interface. In turn, this common accessibility contributes to ChatGPT's unprecedented popularity. ChatGPT has emerged as a general-purpose problem solver for many NLP applications [56]. Qin et al. [56] evaluated ChatGPT on a comprehensive set of NLP tasks, including common benchmarks in natural language inference, arithmetic reasoning, named entity recognition, sentiment analysis, question answering, dialogue and summarization.</p>
<p>They conclude that ChatGPT excels in most tasks, except for tasks that focus on specific details (e.g., sequence tagging).</p>
<p>ChatGPT is also a valuable solution for multilingual tasks. A recent empirical study [57] reports that ChatGPT excels at tasks involving high-resource languages (various European languages and Chinese) and is comparable with Google Translate, DeepL Translate and Tencent TranSmart. Nonetheless, ChatGPT performs poorly on low-resource languages and faces extra challenges handling distant language translation (i.e., English-German translation is considered to be less "distant", compared to English-Hindi translation). A later study [58] confirms that ChatGPT struggles with low-resource languages, although the authors observe that ChatGPT does better in understanding nonLatin scripts than generating them.</p>
<p>In addition, it is also possible to use the purely textbased ChatGPT to interact with multi-modal data. A group of researchers [58] use HTML Canvas and Python Turtle graphics as media for text-to-image generation. ChatGPT can faithfully generate HTML and Python code, which can be then used to generate desired images. The authors designed a flag drawing task that required ChatGPT to generate code that can generate country flags. It was found that ChatGPT could generate better flags when the prompt for code was preceded by a prompt that queries ChatGPT for the flag's description. In other words, descriptive text prompts could improve multimodal task performance.</p>
<p>Beyond computer science, ChatGPT can be readily applied to medical report generation and comprehension [59],</p>
<p>[60], education [61], [62], [63], rigorous math research [64] and finance [65]. Overall, ChatGPT is a versatile tool that promotes general AI usage.</p>
<p>However, researchers are also cautious about the possible negative impact of ChatGPT. Some of the more prominent concerns are related to bias [66], [67], ethics [68], [69], plagiarism [70], [71] and job replacement en masse [72], [73]. In response, a commentary published in Nature advocates for urgent attention to accountability, open-source large language models and societal embrace of AI [66].</p>
<h2>3 DATASET</h2>
<p>We first use an open domain dataset Amazon to verify the effectiveness of our method. Then, we use clinical natural language processing (clinical NLP) as the task and carry out our experiments on two popular public benchmarks. Data augmentation is particularly in demand in clinical NLP, because the significant burden of expert annotation and stringent privacy regulations make large-scale data labeling infeasible. We will describe these datasets in detail in the following sections.</p>
<h3>3.1 Amazon dataset</h3>
<p>Amazon [74], [75], [76] contains customer reviews from 24 product categories. The task is to classify reviews into their respective product categories. Since the original Amazon product dataset is proverbially large, we sample a subset of 300 samples from each category.</p>
<h3>3.2 Symptoms Dataset</h3>
<p>This dataset is published on Kaggle ${ }^{1}$. It contains the audio data of common medical symptom descriptions over 8 hours. We use the text transcripts corresponding to the audio data and perform sample de-duplication, and use them as model input. The dataset after preprocessing includes 231 samples of 7 symptom categories. Every example represents a sentence describing the provided symptoms, and the task is to classify the sentence into the corresponding symptoms.</p>
<h3>3.3 PubMed20k Dataset</h3>
<p>The PubMed20K dataset is an extensively utilized resource in NLP and text mining research, comprising around 20,000 annotated scientific abstracts from the biomedical field. These annotations encompass named entities, relationships between entities, and various semantic roles, making the dataset valuable for diverse NLP tasks such as named entity recognition, relation extraction, and text classification. The dataset originates from the PubMed database, which spans a wide array of biomedical subjects. Owing to its substantial size, variety, and high-quality annotations, PubMed20K has emerged as a popular benchmark dataset for assessing the performance of machine learning models in the realm of biomedical NLP. The abstracts in the PubMed 20K dataset undergo preprocessing and segmentation into individual sentences. Each sentence is labeled with one of the following five categories: background, objective, method, result, or conclusion. The task is to map the input sentences to their corresponding categories.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Algorithm 1 The framework of AugGPT for few-shot text classification.
Input: base dataset $D_{b}$ and novel dataset $D_{n}$
Initialize: Initialized pre-trained BERT model
Definition: $D^{*}$ is the dataset with the base dataset $D_{b}$ and augmented dataset $D_{a}^{a u g}$, and chatGPT_aug is the data augmentation method based on ChatGPT
Parameters: Fine-tuning epochs of base dataset $e p o c h_{b}$, finetuning epochs of FSL $e p o c h_{f}$
for epoch in $e p o c h_{b}$ do
$\operatorname{train}\left(\right.$ model, $\left.D_{b}\right)$
end for
$D_{n}^{a u g}=$ chatGPT_aug $\left(D_{n}\right)$
for epoch in $e p o c h_{f}$ do
$\operatorname{train}\left(\right.$ model, $\left.D_{n}^{a u g}\right)$
end for</p>
<h2>4 Method</h2>
<h3>4.1 Overall Framework</h3>
<p>Given a base dataset $D_{b}=\left{\left(x_{i}, y_{i}\right)\right}<em b="b">{i=1}^{N</em>\right)\right}}}$ with a label space $y_{i} \in Y_{b}$, a novel dataset $D_{n}=\left{\left(x_{j}, y_{j<em n="n">{j=1}^{N</em>$ has only a few labeled samples. The performance of fewshot learning is evaluated on the novel dataset. Our goal is to train a model with both base and limited novel datasets, while achieving satisfying generalizability on the novel dataset.}}$ with a label space $y_{j} \in Y_{n}$, and $Y_{b} \cap Y_{n}=\emptyset$. In the few-shot classification scenario, the base dataset $D_{b}$ has a relatively larger set of labeled samples, while the novel dataset $D_{n</p>
<p>The overall framework of AugGPT is shown in Fig 1, and the training steps are shown in Algorithm 1. First of all, we fine-tune BERT on $D_{b}$. Then, the $D_{a}^{a u g}$ is generated by data augmentation with ChatGPT. Finally, we fine-tune BERT with $D_{n}^{a u g}$.</p>
<h3>4.2 Data Augmentation with ChatGPT</h3>
<p>Similar to GPT [47], GPT-2 [77], and GPT-3 [6], ChatGPT belongs to the family of autoregressive language models and uses transformer decoder blocks [78] as the model backbone.</p>
<p>During pre-training, ChatGPT is regarded as an unsupervised distribution estimation from a set of samples $X=\left{x_{1}, x_{2}, \ldots, x_{n}\right}$, and sample $x_{i}$ composed of $m$ tokens is defined as $x_{i}=\left(s_{1}, s_{2}, \ldots, s_{m}\right)$. The objective of pretraining is to maximize the following likelihood:</p>
<p>$$
L\left(x_{i}\right)=\sum_{i=1}^{m} \log P\left(s_{i} \mid s_{1}, \ldots, s_{i-1} ; \theta\right)
$$</p>
<p>where $\theta$ represents the trainable parameters of ChatGPT. The tokens are represented by token embedding and position embedding:</p>
<p>$$
h_{0}=x_{i} W_{e}+W_{p}
$$</p>
<p>where $W_{e}$ is the token embedding matrix and $W_{p}$ is the position embedding matrix. Then $N$ transformer blocks are used to extract the features of the sample:</p>
<p>$$
h_{n}=\text { transformer_blocks }\left(h_{n-1}\right)
$$</p>
<p>where $n \in[1, N]$.</p>
<p>Finally, the target token is predicted:</p>
<p>$$
s_{i}=\operatorname{softmax}\left(h_{N} W_{c}^{T}\right)
$$</p>
<p>where $h_{N}$ is the output of top transformer blocks.
After pre-training, the developers of ChatGPT apply Reinforcement Learning from Human Feedback (RLHF) [21] to fine-tune the pre-trained language model. The RLHF aligns language models with user intent on a wide range of tasks by fine-tuning them according to human feedback. The RLHF of ChatGPT contains three steps:</p>
<p>Supervised Fine-tuning (SFT): Unlike GPT, GPT-2, and GPT-3, ChatGPT uses labeled data for further training. The AI trainers play as users and AI assistants to build the answers based on prompts. The answers with prompts are used as supervised data for further training of the pretrained model. After further pre-training, a SFT model can be obtained.</p>
<p>Reward Modeling (RM): Based on the SFT method, a reward model is trained to take in a pair of prompt and response, and output a scalar reward. Human labelers rank the outputs from best to worst to build a ranking dataset. The loss function between two outputs is defined as follows:</p>
<p>$$
\operatorname{loss}\left(\theta_{r}\right)=E_{\left(x, y_{w}, y_{l}\right) \sim D_{c}}\left[\log \left(\sigma\left(r_{\theta_{r}}\left(x, y_{w}\right)-r_{\theta_{r}}\left(x, y_{l}\right)\right)\right)\right]
$$</p>
<p>where $\theta_{r}$ is the parameters of reward model; $x$ is the prompt, $y_{w}$ is the preferred completion out of the pair of $y_{w}$ and $y_{l}$; $D_{c}$ is the dataset of human comparisons.</p>
<p>Reinforcement Learning (RL): By using reward models, ChatGPT can be fine-tuned using Proximal Policy Optimization (PPO) [79]. In order to fix the performance degradation on public NLP datasets, the RLHF mixes the pretraining gradients into the PPO gradients, which is also known as PPO-ptx:</p>
<p>$$
\begin{aligned}
&amp; \operatorname{objective}(\phi)=\gamma E_{x \sim D_{\text {pretrain }}}\left[\log \left(\pi_{\phi}^{\mathrm{RL}}(x)\right)\right]+ \
&amp; E_{(x, y) \sim D_{\pi_{\phi}^{\mathrm{RL}}}}\left[r_{\theta_{r}}(x, y)-\beta \log \left(\pi_{\phi}^{\mathrm{RL}}(y \mid x) / \theta_{\mathrm{SFT}}(y \mid x)\right)\right]
\end{aligned}
$$</p>
<p>where $\pi_{\phi}^{\mathrm{RL}}$ is the learned RL policy, $\theta_{\mathrm{SFT}}$ is the supervised trained model, and $D_{\text {pretrain }}$ is the pretraining distribution. The $\gamma$ is the pre-training loss coefficient that controls the strength of pre-training gradients, and the $\beta$ is the KL (Kullback-Leibler) reward coefficient that controls the strength of the KL penalty.</p>
<p>Compared to previous data augmentation methods, ChatGPT is more suitable for data augmentation for the following reasons:</p>
<ul>
<li>ChatGPT is pre-trained on large-scale corpora, so it has a broader semantic expression space, and is helpful to enhance the diversity of data augmentation.</li>
<li>Since the fine-tuning stage of ChatGPT introduces a large number of manual annotation samples, the language generated by ChatGPT is more in line with human expression habits.</li>
<li>Through reinforcement learning, ChatGPT can compare the advantages and disadvantages of different expressions and ensure that the generated data are of high quality.
Under the BERT framework, we introduce ChatGPT as the data augmentation tool for few-shot text classification.</li>
</ul>
<p>Specifically, ChatGPT is applied to rephrase each input sentence into six additional sentences, thereby augmenting the few-shot samples.</p>
<h3>4.3 Few-shot Text Classification</h3>
<p>We apply BERT [80] to train a few-shot text classification model. The output features $h$ of the top layer of BERT can be written as:</p>
<p>$$
z=\left[z_{c}, z_{1}, z_{2}, \ldots, z_{n}\right]
$$</p>
<p>where $z_{c}$ is the representation of the class-specific token CLS. For text classification, $z_{c}$ is usually fed into a taskspecific classifier header for final prediction. However, in the FSL scenario, it is difficult to achieve satisfactory performance through BERT fine-tuning because the small scale of few-shot samples will easily lead to over-fitting and lack of generalization ability.</p>
<p>To effectively address the challenge of few-shot text classification, many approaches have been proposed. Generally, there are four categories of methods for few-shot text classification based on large language models: metalearning, prompt-tuning, model design, and data augmentation. meta-learning refers to the process of learning to learn with tasks that update meta-parameters [2], [5]. Promptbased methods guide large language models to predict correct results by designing templates [6], [7], [8], [9]. Model design methods guide the model to learn from few-shot samples by changing the structure of the model [81]. Data augmentation uses similar characters [22], similar word semantics [30], [31], or knowledge base [55], [82] to expand samples. Our method directly data augmentation through the language capabilities of large language models, which is a simple and efficient data augmentation method.</p>
<p>Objective Function: Our objective function of few-shot learning consists of two parts: cross entropy and contrastive learning loss. We feed $z_{c}$ into a fully connected layer, the classifier for the final prediction:</p>
<p>$$
\hat{y}=W_{c}^{T} z_{c}+b_{c}
$$</p>
<p>where $W_{c}$ and $b_{c}$ are trainable parameters, and take crossentropy as one of the objective functions:</p>
<p>$$
L_{C E}=-\sum_{d \in D^{\prime}} \sum_{c=1}^{C} y_{d c} \ln \hat{y}_{d c}
$$</p>
<p>where $C$ is the output dimension, which is equal to the union of label spaces of the base dataset and novel dataset, and $y_{d}$ is the ground truth.</p>
<p>Then, to make full use of the prior knowledge in the base dataset to guide the learning of the novel dataset, we introduce the contrastive loss function to make the sample representation of the same category more compact and the sample representation of different categories more separate. The contrastive loss between pairs of samples in the same batch is defined as follows:</p>
<p>$$
L_{C L}=-\log \frac{\sum e^{\cos \left(v_{i}, v_{i^{\prime}}\right)}}{\sum e^{\cos \left(v_{i}, v_{i^{\prime}}\right)}+\sum e^{\cos \left(v_{i}, v_{j}\right)}}
$$</p>
<p>where $v_{i}$ and $v_{i}^{\prime}$ are the $z_{c}$ of samples that belong to the same category; $v_{i}$ and $v_{j}$ are the $z_{c}$ of samples belong to different categories; $\cos (\cdot ; \cdot)$ is the cosine similarity.</p>
<p>In the BERT fine-tuning stage on the base dataset, we only use cross entropy as the objective function. In the few-shot learning stage, we combine cross entropy and contrastive learning loss as the objective function:</p>
<p>$$
L=L_{C E}+\lambda L_{C L}
$$</p>
<h3>4.4 Baseline Methods</h3>
<p>In the experiment section, we compare our method with other popular data augmentation methods. For these methods, we use the implementation in open-source libraries including, nlpaug [83] and textattack [84].</p>
<ul>
<li>InsertCharAugmentation. This method inserts random characters at random locations in text, which improves the generalization ability of the model by injecting noise into the data.</li>
<li>SubstituteCharAugmentation. This method randomly replaces selected characters with other ones.</li>
<li>SwapCharAugmentation [22]. This method randomly exchanges two characters.</li>
<li>DeleteCharAugmentation. This method randomly deletes characters.</li>
<li>OCRAugmentation. OCRAugmentation simulates possible errors during OCR recognition. For example, OCR tool may wrongly identify " 0 " as " o ", and wrongly identify " I " as " l ".</li>
<li>SpellingAugmentation [23]. It creates new text by deliberately misspelling some words. The method uses a list of English words that are most likely to be misspelled provided by Oxford Dictionary, for example, misspelling "because" as "because".</li>
<li>KeyboardAugmentation [22]. It simulates typo error by replacing randomly selected characters with the adjacent characters in the QWERTY layout keyboard. For example, replacing ' $g$ ' with ' $r$ ', ' $t$ ', ' $y$ ', ' $f$ ', ' $h$ ', ' $v$ ', 'b' or ' $n$ '.</li>
<li>SwapWordAug [24]. It randomly exchanges words in text. This method is a submethod of Easy Data Augmentation (EDA) proposed by Wei et al.</li>
<li>DeleteWordAug. DeleteWordAug randomly deletes words in the text, which is also a submethod of EDA.</li>
<li>PPDBSynonymAug [26]. It replaces words with their synonym in PPDB thesaurus. Synonym replacement can ensure semantic consistency and is suitable for classification tasks.</li>
<li>WordNetSynonymAug. It replaces words with their synonym in WordNet thesaurus.</li>
<li>SubstituteWordByGoogleNewsEmbeddings [28]. It replaces words with their top- $n$ similar words in the embedding space. The word embeddings used are pre-trained with GoogleNews corpus.</li>
<li>InsertWordByGoogleNewsEmbeddings [83]. It randomly selects word from vocabulary of GoogleNews corpus and inserts it the random position of the text.</li>
<li>CounterFittedEmbeddingAug [30], [31]. It replaces words with their neighbors in counter-fitting embedding space. Compared with GoogleNews word vectors used by SubstituteWordByGoogleNewsEmbeddings, counter-fitting embedding introduces the constraint of synonyms and antonyms, that is, the
embedding between synonyms will be pulled closer, and vice versa.</li>
<li>ContextualWordAugUsingBert(Insert) [32], [33]. This method uses BERT to insert words based on context, that is, add $&lt;$ mask $&gt;$ token at random position of the input text, and then let BERT predict the token at that position.</li>
<li>ContextualWordAugUsingDistilBERT(Insert). This method uses DistilBERT to replace BERT for prediction, and the rest is the same as ContextualWordAugUsingBert(Insert).</li>
<li>ContextualWordAugUsingRoBERTA(Insert). This method uses RoBERTA to replace BERT for prediction, and the rest is the same as ContextualWordAugUsingBert(Insert).</li>
<li>ContextualWordAugUsingBert(Substitute). This method [32], [33] uses BERT to replace words based on context, that is, replace randomly selected words in text with $&lt;$ mask $&gt;$ token, and then let BERT predict the token at that position.</li>
<li>ContextualWordAugUsingDistilBERT(Substitute). This method uses DistilBERT to replace BERT for prediction, and the rest is the same as ContextualWordAugUsingBert(Substitute).</li>
<li>ContextualWordAugUsingRoBERTA(Substitute). This method uses RoBERTA to replace BERT for prediction, and the rest is the same as ContextualWordAugUsingBert(Substitute).</li>
<li>BackTranslationAug. The method [38] translates the text into German and then into English, resulting in a new text that is different from the original but has the same semantics. We use wmt19-en-de and facebook/wmt19-de-en language translation models [85] developed by Facebook for translation.</li>
</ul>
<h3>4.5 Prompt Design</h3>
<p>We have designed prompts for single-turn dialogue and multi-turn dialogues. The prompts are shown in Fig 2. The Amazon dataset use the multi-turn dialogues prompt for data augmentation. The Symptoms and PubMed20K use the single-turn dialogue prompt for data augmentation.</p>
<h3>4.6 Evaluation Metrics</h3>
<p>We employed cosine similarity and TransRate [86] as metrics to assess the faithfulness (i.e., whether the generated data samples are close to the original samples) and compactness (i.e., whether samples of each class are compact enough for good discrimination) of the augmented data.</p>
<h3>4.6.1 Embedding Similarity</h3>
<p>To evaluate the semantic similarity between the samples generated by data augmentation methods and actual samples, we adopt embedding similarity between the generated samples and the actual samples of the test dataset. Some of the most common similarity metrics include Euclidean distance, cosine similarity and dot product similarity. In this study, we select cosine similarity to capture the distance relationship in the latent space. The cosine similarity measures the cosine value of the angle between two vectors. This value increases when two vectors are more similar, and is</p>
<p>TABLE 2
Data Augmentation and Ablation Study. The BERT + C indicates BERT with contrastive loss.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Data Augmentation</th>
<th style="text-align: center;">Amazon</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Symptoms</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">PubMed20K</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BERT</td>
<td style="text-align: center;">BERT + C</td>
<td style="text-align: center;">BERT</td>
<td style="text-align: center;">BERT + C</td>
<td style="text-align: center;">BERT</td>
<td style="text-align: center;">BERT + C</td>
</tr>
<tr>
<td style="text-align: center;">Raw</td>
<td style="text-align: center;">0.734</td>
<td style="text-align: center;">0.745</td>
<td style="text-align: center;">0.636</td>
<td style="text-align: center;">0.606</td>
<td style="text-align: center;">0.792</td>
<td style="text-align: center;">0.798</td>
</tr>
<tr>
<td style="text-align: center;">BackTranslationAug</td>
<td style="text-align: center;">0.757</td>
<td style="text-align: center;">0.748</td>
<td style="text-align: center;">0.778</td>
<td style="text-align: center;">0.747</td>
<td style="text-align: center;">0.812</td>
<td style="text-align: center;">0.83</td>
</tr>
<tr>
<td style="text-align: center;">ContextualWordAugUsingBert(Insert)</td>
<td style="text-align: center;">0.761</td>
<td style="text-align: center;">0.750</td>
<td style="text-align: center;">0.697</td>
<td style="text-align: center;">0.677</td>
<td style="text-align: center;">0.802</td>
<td style="text-align: center;">0.811</td>
</tr>
<tr>
<td style="text-align: center;">ContextualWordAugUsingBert(Substitute)</td>
<td style="text-align: center;">0.770</td>
<td style="text-align: center;">0.757</td>
<td style="text-align: center;">0.626</td>
<td style="text-align: center;">0.667</td>
<td style="text-align: center;">0.815</td>
<td style="text-align: center;">0.830</td>
</tr>
<tr>
<td style="text-align: center;">ContextualWordAugUsingDistilBERT(Insert)</td>
<td style="text-align: center;">0.759</td>
<td style="text-align: center;">0.762</td>
<td style="text-align: center;">0.707</td>
<td style="text-align: center;">0.747</td>
<td style="text-align: center;">0.796</td>
<td style="text-align: center;">0.796</td>
</tr>
<tr>
<td style="text-align: center;">ContextualWordAugUsingDistilBERT(Substitute)</td>
<td style="text-align: center;">0.787</td>
<td style="text-align: center;">0.766</td>
<td style="text-align: center;">0.667</td>
<td style="text-align: center;">0.646</td>
<td style="text-align: center;">0.797</td>
<td style="text-align: center;">0.800</td>
</tr>
<tr>
<td style="text-align: center;">ContextualWordAugUsingRoBERTA(Insert)</td>
<td style="text-align: center;">0.775</td>
<td style="text-align: center;">0.768</td>
<td style="text-align: center;">0.758</td>
<td style="text-align: center;">0.707</td>
<td style="text-align: center;">0.815</td>
<td style="text-align: center;">0.814</td>
</tr>
<tr>
<td style="text-align: center;">ContextualWordAugUsingRoBERTA(Substitute)</td>
<td style="text-align: center;">0.745</td>
<td style="text-align: center;">0.730</td>
<td style="text-align: center;">0.727</td>
<td style="text-align: center;">0.667</td>
<td style="text-align: center;">0.782</td>
<td style="text-align: center;">0.782</td>
</tr>
<tr>
<td style="text-align: center;">CounterFittedEmbeddingAug</td>
<td style="text-align: center;">0.754</td>
<td style="text-align: center;">0.741</td>
<td style="text-align: center;">0.667</td>
<td style="text-align: center;">0.626</td>
<td style="text-align: center;">0.805</td>
<td style="text-align: center;">0.805</td>
</tr>
<tr>
<td style="text-align: center;">InsertCharAugmentation</td>
<td style="text-align: center;">0.771</td>
<td style="text-align: center;">0.775</td>
<td style="text-align: center;">0.404</td>
<td style="text-align: center;">0.475</td>
<td style="text-align: center;">0.826</td>
<td style="text-align: center;">0.831</td>
</tr>
<tr>
<td style="text-align: center;">InsertWordByGoogleNewsEmbeddings</td>
<td style="text-align: center;">0.816</td>
<td style="text-align: center;">0.794</td>
<td style="text-align: center;">0.636</td>
<td style="text-align: center;">0.677</td>
<td style="text-align: center;">0.786</td>
<td style="text-align: center;">0.784</td>
</tr>
<tr>
<td style="text-align: center;">KeyboardAugmentation</td>
<td style="text-align: center;">0.764</td>
<td style="text-align: center;">0.766</td>
<td style="text-align: center;">0.545</td>
<td style="text-align: center;">0.505</td>
<td style="text-align: center;">0.809</td>
<td style="text-align: center;">0.815</td>
</tr>
<tr>
<td style="text-align: center;">OCRAugmentation</td>
<td style="text-align: center;">0.775</td>
<td style="text-align: center;">0.782</td>
<td style="text-align: center;">0.768</td>
<td style="text-align: center;">0.778</td>
<td style="text-align: center;">0.789</td>
<td style="text-align: center;">0.789</td>
</tr>
<tr>
<td style="text-align: center;">PPDBSynonymAug</td>
<td style="text-align: center;">0.691</td>
<td style="text-align: center;">0.690</td>
<td style="text-align: center;">0.697</td>
<td style="text-align: center;">0.758</td>
<td style="text-align: center;">0.795</td>
<td style="text-align: center;">0.829</td>
</tr>
<tr>
<td style="text-align: center;">SpellingAugmentation</td>
<td style="text-align: center;">0.727</td>
<td style="text-align: center;">0.736</td>
<td style="text-align: center;">0.697</td>
<td style="text-align: center;">0.707</td>
<td style="text-align: center;">0.808</td>
<td style="text-align: center;">0.811</td>
</tr>
<tr>
<td style="text-align: center;">SubstituteCharAugmentation</td>
<td style="text-align: center;">0.762</td>
<td style="text-align: center;">0.768</td>
<td style="text-align: center;">0.535</td>
<td style="text-align: center;">0.586</td>
<td style="text-align: center;">0.816</td>
<td style="text-align: center;">0.821</td>
</tr>
<tr>
<td style="text-align: center;">SubstituteWordByGoogleNewsEmbeddings</td>
<td style="text-align: center;">0.729</td>
<td style="text-align: center;">0.741</td>
<td style="text-align: center;">0.727</td>
<td style="text-align: center;">0.727</td>
<td style="text-align: center;">0.807</td>
<td style="text-align: center;">0.822</td>
</tr>
<tr>
<td style="text-align: center;">SwapCharAugmentation</td>
<td style="text-align: center;">0.762</td>
<td style="text-align: center;">0.766</td>
<td style="text-align: center;">0.475</td>
<td style="text-align: center;">0.485</td>
<td style="text-align: center;">0.797</td>
<td style="text-align: center;">0.801</td>
</tr>
<tr>
<td style="text-align: center;">SwapWordAug</td>
<td style="text-align: center;">0.771</td>
<td style="text-align: center;">0.766</td>
<td style="text-align: center;">0.687</td>
<td style="text-align: center;">0.727</td>
<td style="text-align: center;">0.798</td>
<td style="text-align: center;">0.794</td>
</tr>
<tr>
<td style="text-align: center;">WordNetSynonymAug</td>
<td style="text-align: center;">0.805</td>
<td style="text-align: center;">0.798</td>
<td style="text-align: center;">0.616</td>
<td style="text-align: center;">0.758</td>
<td style="text-align: center;">0.761</td>
<td style="text-align: center;">0.757</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT (2-shot)</td>
<td style="text-align: center;">0.753</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.980</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.748</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">AugGPT</td>
<td style="text-align: center;">0.816</td>
<td style="text-align: center;">0.826</td>
<td style="text-align: center;">0.889</td>
<td style="text-align: center;">0.899</td>
<td style="text-align: center;">0.835</td>
<td style="text-align: center;">0.835</td>
</tr>
</tbody>
</table>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 2. Single-turn dialogue and multi-turn dialogues prompt
bounded by a range between 0 and 1 . Since the pre-trained language models without fine-tunning poorly to capture semantic meaning, we fine-tunning the pre-trained BERT on base dataset by BERT-flow [87] method, and finally apply the fine-tunned BERT to get smaple embedding. The cosine similarity metric is commonly used in NLP [88] and we follow this convention.</p>
<p>$$
\cos (\theta)=\frac{A \cdot B}{\left|A\right|<em 2="2">{2}|B|</em>
$$}</p>
<p>where A and B denote the two embedding vectors in comparison, respectively.</p>
<h3>4.6.2 TransRate</h3>
<p>TransRate is a metric that quantifies transferability based on the mutual information between the features extracted by a pre-trained model and their labels, with a single pass through the target data. The metric achieves a minimum value when the data covariance matrices of all classes are
identical, making it impossible to distinguish between the data from different classes and preventing any classifier from achieving better than random guessing. Thus, a higher TransRate could indicate better learnability of the data. More specifically, knowledge transfer from a source task $T_{s}$ to a target task $T_{t}$ is measured as shown below:</p>
<p>$$
\operatorname{Tr} R_{T_{s} \rightarrow T_{t}}(g)=H(Z)-H(Z \mid Y)
$$</p>
<p>where Y represents the labels of augmented examples, and $Z$ denotes the latency embedding features extracted by the pre-trained feature extractor $g . \operatorname{Tr} R$ means the TransRate value. $H(\cdot)$ denotes the Shannon entropy [89].</p>
<h3>4.7 Direct Classification Performance by ChatGPT</h3>
<p>An interesting and important question about the utilization of ChatGPT for text data augmentation would be how ChatGPT will perform when directly applied to FSL downstream tasks. Thus, we developed tailored prompts for ChatGPT to perform the classification tasks with integrated the API</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 3. We employed two evaluation metrics to assess the faithfulness and compactness of our newly augmented data. The top left plot displays the cosine similarity metric and final accuracy of all data augmentation methods on the Symptoms dataset, and the bottom left plot shows the TransRate metric and final accuracy of all data augmentation methods on the Symptoms dataset. In the middle and bottom panels, we plotted the cosine similarity and TransRate values of all data augmentation methods on the Amazon and PubMed20K datasets, respectively. On the right side of the picture, we listed all the augmented methods with different colors and shapes.</p>
<p>for prompting. For the Symptoms dataset, we employed the following prompt instruction: "Given a person's health description or symptom, predict the corresponding illness from the following categories: CLASSES." Additionally, we used "Description: DESCRIPTION. Typically, this symptom corresponds to CLASS" as the prompt for each example in the dataset. In this way, We can include few-shot examples (in this work, we used two) to facilitate the model's adaptation to downstream tasks. We used similarly-designed prompt instructions for the other two tasks and the corresponding example prompt to implement the few-shot in-context learning by ChatGPT.</p>
<h2>5 EXPERIMENT RESULTS</h2>
<p>In our experiments, we use BERT as the base model. Firstly, we train our model on the base dataset to produce the pre-trained model. Then we fine-tune the model with the combination of few-shot samples and the augmented samples generated from various data augmentation methods. Specifically, in all three FSL tasks, we perform 2-shot learning, i.e., there would be two real samples used for each class in the target domain. Afterward, We use those samples to fine-tune the pre-trained models. To evaluate the effectiveness of different data augmentation methods, we apply two different settings. The first one is the vanilla BERT model. In the second setting, we add a contrastive loss to the training objective function. In our experiments on the Symptoms dataset, we use a batch size of 8 for 150 epochs, set the maximum sequence length to 25, λ as 1, and use a learning rate of 4e-5. In our experiments on the PubMed20K dataset, we adopt the same training configuration, with the maximum sequence length set to 40. For all three tasks, we will generate six augmented samples per class. Examples of the augmented samples generated by AugGPT and other selected baseline methods can be found in the appendix.</p>
<h3>5.1 Classification Performance Comparison</h3>
<p>Table 2 shows the accuracy of different data augmentation methods. As shown in Table 2, AugGPT achieves the highest accuracy for Amazon, Symptoms and PubMed20K datasets. For the Amazon dataset, AugGPT and InsertWordByGoogleNewsEmbeddings achieve the best performance for BERT, and AugGPT achieve the best performance for BERT with contrastive loss. In the PubMed20K dataset, AugGPT achieves 83.5% accuracy for both BERT and BERT with contrastive loss, whereas without data augmentation, the accuracy values are only 79.2% and 79.8%, respectively. For the Symptoms dataset, the accuracy for BERT downstream augmentation is only 63.6%, and 60.6% with contrastive loss. However, our AugGPT approach significantly improves the accuracy to 88.9% and 89.9%, respectively. These results suggest that data augmentation using ChatGPT is more effective in enhancing the performance of machine learning models in various applications.</p>
<h3>5.2 Evaluation of Augmented Datasets</h3>
<p>In addition to the classification accuracy, we evaluate the augmented data in the latent space and visualize the results in Fig 3. Latent embeddings are evaluated using cosine similarity and the TransRate metric (see section 4.6 for more details). The horizontal axis represents the cosine similarity values and Transrate values, and the vertical axis describes the classification accuracy. Since embedded similarity measures the similarity between the generated data and the test dataset, high similarity means that the generated data are close to real input data and with higher faithfulness and compactness. Higher TransRate indicates better learnability of the data. Therefore, a higher TransRate score indicates that the augmented data are of higher quality. The most ideal candidate method should be positioned at the top-right of the visualization. As shown in Fig 3, AugGPT pro</p>
<p>duces high-quality samples in terms of both faithfulness and compactness on the Symptoms dataset and the PubMed20K dataset. On the open-domain Amazon dataset, AugGPT also produces high-quality samples with a higher TransRate.</p>
<h3>5.3 Performance Comparison with ChatGPT</h3>
<p>Furthermore, we used ChatGPT to directly perform the downstream text data classification tasks under a 5-shot learning scheme. We used in-house designed instructions with few-shot in-context examples to prompt ChatGPT as described in 4.7. The performance of ChatGPT for the downstream tasks is listed in Table 2. The result reveals that state-of-the-art large language models such as ChatGPT tend to perform better on relatively easier tasks, for example, identifying symptoms according to a one-sentence description. However, when it comes to complicated tasks such like PubMed, model fine-tuning is still needed and could achieve better performance compared to few-shot prompts.</p>
<h2>6 CONCLUSION AND DISCUSSION</h2>
<p>In this paper, we proposed a novel data augmentation approach for few-shot classification. Unlike other methods, our model expands the limited data at the semantic level to enhance data consistency and robustness, which results in a better performance than most of the current text data augmentation methods. With the advancement of LLM and its nature of a multi-task learner [77], we envision that a series of tasks in NLP can be enhanced or even replaced in a similar fashion.</p>
<p>Although AugGPT has shown promising results in data augmentation, it has certain limitations. For example, when recognizing and augmenting medical texts, AugGPT may produce incorrect augmentation results due to the lack of domain knowledge of ChatGPT. In future works, we will investigate adapting the general-domain LLMs, such as ChatGPT, to domain-specific data, such as medical texts, via model fine-tuning, in-context learning (prompt engineering), knowledge distillation, style transfer, etc.</p>
<p>AugGPT has demonstrated that the augmentation results can effectively improve the performance of the downstream classification task. A promising direction for future research is to investigate AugGPT against a wider range of downstream tasks. For example, given the strong ability of ChatGPT to extract key points and understand sentences, it can be utilized in tasks such as text summarization. Specifically, ChatGPT might be valuable for domain-specific science paper summarization [90] and clinical report summarization [91]. Publicly available domain-specific science paper summarization datasets and clinical report datasets are rare and often provided at small scales due to privacy concerns and the need for expert knowledge to generate annotated summaries. However, ChatGPT could address this challenge by generating diverse augmented summarization samples in different representation styles. The data generated from ChatGPT are typically concise, which can be valuable for further enhancing the generalization capabilities of the trained model.</p>
<p>The dramatic rise of generative image models such as DALLE2 [92] and Stable Diffusion [93] provides opportunities for applying AugGPT to few-shot learning tasks
in computer vision. For example, accurate language descriptions may be used to guide the generative model to generate images from text or to generate new images based on existing images as a data augmentation method for fewshot learning tasks, especially when combined with efficient fine-tuning methods [94], [95] such as LoRA for Stable Diffusion. Thus, prior knowledge from a large language model can facilitate faster domain adaptation and better few-shot learning of generative models in computer vision.</p>
<p>Recent research shows that large language models (LLMs), such as GPT-3 and ChatGPT, are capable of solving Theory of Mind (ToM) tasks, which were previously thought to be unique to humans [96]. While the ToM-like capabilities of LLMs may be an unintended byproduct of improved performance, the underlying connection between cognitive science and the human brain is an area ripe for exploration. Advancements in cognitive and brain science can also be used to inspire and optimize the design of LLMs. For example, it has been suggested that the activation patterns of the neurons in the BERT model and those in the human brain networks may share similarities and could be coupled together [97]. This presents a promising new direction for developing LLMs by utilizing prior knowledge from brain science. As researchers continue to investigate the connections between LLMs and the human brain, we may discover new means to enhance the performance and capabilities of AI systems, leading to exciting breakthroughs in the field.</p>
<h2>REFERENCES</h2>
<p>[1] Y. Wang, Q. Yao, J. T. Kwok, and L. M. Ni, "Generalizing from a few examples: A survey on few-shot learning," ACM computing surveys (csar), vol. 53, no. 3, pp. 1-34, 2020.
[2] W. Yin, "Meta-learning for few-shot natural language processing: A survey," arXiv preprint arXiv:2007.09604, 2020.
[3] C. Wang, J. Wang, M. Qiu, J. Huang, and M. Gao, "Transprompt: Towards an automatic transferable prompting framework for fewshot text classification," in Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, 2021, pp. 27922802.
[4] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, "Bert: Pretraining of deep bidirectional transformers for language understanding," arXiv preprint arXiv:1810.04805, 2018.
[5] H.-y. Lee, S.-W. Li, and N. T. Vu, "Meta learning for natural language processing: A survey," arXiv preprint arXiv:2205.01500, 2022.
[6] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., "Language models are few-shot learners," Advances in neural information processing systems, vol. 33, pp. 1877-1901, 2020.
[7] B. Lester, R. Al-Rfou, and N. Constant, "The power of scale for parameter-efficient prompt tuning," in Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, 2021, pp. 3045-3059.
[8] X. Han, W. Zhao, N. Ding, Z. Liu, and M. Sun, "Ptr: Prompt tuning with rules for text classification," AI Open, vol. 3, pp. 182-192, 2022.
[9] J. Wang, C. Wang, F. Luo, C. Tan, M. Qiu, F. Yang, Q. Shi, S. Huang, and M. Gao, "Towards unified prompt tuning for few-shot text classification," arXiv preprint arXiv:2205.05313, 2022.
[10] J. Wei and K. Zou, "Eda: Easy data augmentation techniques for boosting performance on text classification tasks," arXiv preprint arXiv:1901.11196, 2019.
[11] V. Kumar, H. Claude, C. de Lichy, and W. Campbell, "A closer look at feature space data augmentation for few-shot intent classification," in Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo 2019), 2019, pp. 1-10.</p>
<p>[12] S. Y. Feng, V. Gangal, J. Wei, S. Chandar, S. Vosoughi, T. Mitamura, and E. Hovy, "A survey of data augmentation approaches for nlp," arXiv preprint arXiv:2105.03075, 2021.
[13] R. Sennrich, B. Haddow, and A. Birch, "Improving neural machine translation models with monolingual data," arXiv preprint arXiv:1511.06709, 2015.
[14] A. Jindal, A. G. Chowdhury, A. Didolkar, D. Jin, R. Sawhney, and R. Shah, "Augmenting nlp models using latent feature interpolations," in Proceedings of the 28th International Conference on Computational Linguistics, 2020, pp. 6931-6936.
[15] C. Shorten, T. M. Khoshgoftaar, and B. Furht, "Text data augmentation for deep learning," Journal of big Data, vol. 8, pp. 1-34, 2021.
[16] M. Bayer, M.-A. Kauffold, and C. Reuter, "A survey on data augmentation for text classification," ACM Computing Surveys, vol. 55, no. 7, pp. 1-39, 2022.
[17] B. Min, H. Ross, E. Sulem, A. P. B. Veyseh, T. H. Nguyen, O. Sainz, E. Agirre, I. Heinz, and D. Roth, "Recent advances in natural language processing via large pre-trained language models: A survey," arXiv preprint arXiv:2111.01243, 2021.
[18] C. Zhou, Q. Li, C. Li, J. Yu, Y. Liu, G. Wang, K. Zhang, C. Ji, Q. Yan, L. He et al., "A comprehensive survey on pretrained foundation models: A history from bert to chatgpt," arXiv preprint arXiv:2302.09419, 2023.
[19] Z. Liu, M. He, Z. Jiang, Z. Wu, H. Dai, L. Zhang, S. Luo, T. Han, X. Li, X. Jiang et al., "Survey on natural language processing in medical image analysis." Zhong nan da xue xue bao. Yi xue ban= Journal of Central South University. Medical Sciences, vol. 47, no. 8, pp. 981-993, 2022.
[20] S. Wang, Z. Zhao, X. Ouyang, Q. Wang, and D. Shen, "Chatcad: Interactive computer-aided diagnosis on medical image using large language models," arXiv preprint arXiv:2302.07257, 2023.
[21] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Gray et al., "Training language models to follow instructions with human feedback," in Advances in Neural Information Processing Systems, 2022.
[22] Y. Belinkov and Y. Bisk, "Synthetic and natural noise both break neural machine translation," arXiv preprint arXiv:1711.02173, 2017.
[23] C. Coulombe, "Text Data Augmentation Made Simple By Leveraging NLP Cloud APIs," Dec. 2018.
[24] J. Wei and K. Zou, "EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks," in Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). Hong Kong, China: Association for Computational Linguistics, Nov. 2019, pp. 6382-6388.
[25] E. Pavlick, P. Rastogi, J. Ganitkevitch, B. Van Durme, and C. Callison-Burch, "Ppdb 2.0: Better paraphrase ranking, finegrained entailment relations, word embeddings, and style classification," in Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), 2015, pp. $425-430$.
[26] T. Niu and M. Bansal, "Adversarial Over-Sensitivity and OverStability Strategies for Dialogue Models," in Proceedings of the 22nd Conference on Computational Natural Language Learning. Brussels, Belgium: Association for Computational Linguistics, 2018, pp. $486-496$.
[27] G. A. Miller, "Wordnet: a lexical database for english," Communications of the ACM, vol. 38, no. 11, pp. 39-41, 1995.
[28] W. Y. Wang and D. Yang, "That's so annoying!!!: A lexical and frame-semantic embedding based data augmentation approach to automatic categorization of annoying behaviors using# petpeeve tweets," in Proceedings of the 2015 conference on empirical methods in natural language processing, 2015, pp. 2557-2563.
[29] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, "Distributed representations of words and phrases and their compositionality," Advances in neural information processing systems, vol. 26, 2013.
[30] N. Mrkšić, D. Ó Séaghdha, B. Thomson, M. Gašić, L. M. RojasBarahona, P.-H. Su, D. Vandyke, T.-H. Wen, and S. Young, "Counter-fitting Word Vectors to Linguistic Constraints," in Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. San Diego, California: Association for Computational Linguistics, Jun. 2016, pp. 142-148.
[31] M. Alzantot, Y. Sharma, A. Elgohary, B.-J. Ho, M. Srivastava, and K.-W. Chang, "Generating Natural Language Adversarial Exam-
ples," in Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Brussels, Belgium: Association for Computational Linguistics, 2018, pp. 2890-2896.
[32] S. Kobayashi, "Contextual Augmentation: Data Augmentation by Words with Paradigmatic Relations," in Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers). New Orleans, Louisiana: Association for Computational Linguistics, Jun. 2018, pp. 452-457.
[33] V. Kumar, A. Choudhary, and E. Cho, "Data Augmentation Using Pre-trained Transformer Models," arXiv preprint arXiv:2003.02245, 2020.
[34] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, "BERT: Pretraining of Deep Bidirectional Transformers for Language Understanding," May 2019.
[35] L. Sun, C. Xia, W. Yin, T. Liang, P. S. Yu, and L. He, "Mixuptransformer: dynamic data augmentation for nlp tasks," arXiv preprint arXiv:2010.02394, 2020.
[36] V. Sanh, L. Debut, J. Chaumond, and T. Wolf, "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter," ArXiv, vol. abs/1910.01108, 2019.
[37] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov, "Roberta: A robustly optimized bert pretraining approach," arXiv preprint arXiv:1907.11692, 2019.
[38] R. Sennrich, B. Haddow, and A. Birch, "Improving Neural Machine Translation Models with Monolingual Data," in Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Berlin, Germany: Association for Computational Linguistics, Aug. 2016, pp. 86-96.
[39] V. Gangal, S. Y. Feng, M. Alikhani, T. Mitamura, and E. Hovy, "Nanov: The narrative reordering problem," in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 36, no. 10, 2022, pp. $10645-10653$.
[40] L. Fei-Fei, R. Fergus, and P. Perona, "One-shot learning of object categories," IEEE transactions on pattern analysis and machine intelligence, vol. 28, no. 4, pp. 594-611, 2006.
[41] Y. Ge, Y. Guo, Y.-C. Yang, M. A. Al-Garadi, and A. Sarker, "Fewshot learning for medical text: A systematic review," arXiv preprint arXiv:2204.14081, 2022.
[42] J. Wei, C. Huang, S. Vosoughi, Y. Cheng, and S. Xu, "Few-shot text classification with triplet networks, data augmentation, and curriculum learning," in Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2021, pp. 5493-5500.
[43] T. Gao, A. Fisch, and D. Chen, "Making pre-trained language models better few-shot learners," in Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), 2021, pp. 3816-3830.
[44] A. Antoniou, H. Edwards, and A. Storkey, "How to train your maml," arXiv preprint arXiv:1810.09502, 2018.
[45] C. Finn, P. Abbeel, and S. Levine, "Model-agnostic meta-learning for fast adaptation of deep networks," in International conference on machine learning. PMLR, 2017, pp. 1126-1135.
[46] X. Yao, J. Zhu, G. Huo, N. Xu, X. Liu, and C. Zhang, "Modelagnostic multi-stage loss optimization meta learning," International Journal of Machine Learning and Cybernetics, vol. 12, no. 8, pp. 2349-2363, 2021.
[47] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever et al., "Improving language understanding by generative pre-training," 2018.
[48] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy, V. Stoyanov, and L. Zettlemoyer, "Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension," arXiv preprint arXiv:1910.13461, 2019.
[49] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu, "Exploring the limits of transfer learning with a unified text-to-text transformer," The Journal of Machine Learning Research, vol. 21, no. 1, pp. 5485-5551, 2020.
[50] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann et al., "Palm: Scaling language modeling with pathways," arXiv preprint arXiv:2204.02311, 2022.
[51] T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ilić, D. Hesslow, R. Castagné, A. S. Luccioni, F. Yvon, M. Gallé et al., "Bloom: A</p>
<p>176b-parameter open-access multilingual language model," arXiv preprint arXiv:2211.05100, 2022.
[52] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan, M. Diab, X. Li, X. V. Lin et al., "Opt: Open pre-trained transformer language models," arXiv preprint arXiv:2205.01068, 2022.
[53] S. Longpre, L. Hou, T. Vu, A. Webson, H. W. Chung, Y. Tay, D. Zhou, Q. V. Le, B. Zoph, J. Wei et al., "The flan collection: Designing data and methods for effective instruction tuning," arXiv preprint arXiv:2301.13688, 2023.
[54] Y. Gu, R. Tinn, H. Cheng, M. Lucas, N. Usuyama, X. Liu, T. Naumann, J. Gao, and H. Poon, "Domain-specific language model pretraining for biomedical natural language processing," ACM Transactions on Computing for Healthcare (HEALTH), vol. 3, no. 1, pp. 1-23, 2021.
[55] S. Rezayi, H. Dai, Z. Liu, Z. Wu, A. Hebbar, A. H. Burns, L. Zhao, D. Zhu, Q. Li, W. Liu et al., "Clinicalradiobert: Knowledge-infused few shot learning for clinical notes named entity recognition," in Machine Learning in Medical Imaging: 13th International Workshop, MLMI 2022, Held in Conjunction with MICCAI 2022, Singapore, September 18, 2022, Proceedings. Springer, 2022, pp. 269-278.
[56] C. Qin, A. Zhang, Z. Zhang, J. Chen, M. Yasunaga, and D. Yang, "Is chatgpt a general-purpose natural language processing task solver?" arXiv preprint arXiv:2302.06476, 2023.
[57] W. Jiao, W. Wang, J.-t. Huang, X. Wang, and Z. Tu, "Is chatgpt a good translator? a preliminary study," arXiv preprint arXiv:2301.08745, 2023.
[58] Y. Bang, S. Cahyawijaya, N. Lee, W. Dai, D. Su, B. Wilie, H. Lovenia, Z. Ji, T. Yu, W. Chung et al., "A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity," arXiv preprint arXiv:2302.04023, 2023.
[59] Y. Shen, L. Heacock, J. Elias, K. D. Hentel, B. Reig, G. Shih, and L. Moy, "Chatgpt and other large language models are doubleedged swords," p. 230163, 2023.
[60] F. Antaki, S. Touma, D. Milad, J. El-Khoury, and R. Duval, "Evaluating the performance of chatgpt in ophthalmology: An analysis of its successes and shortcomings," medRxiv, pp. 2023-01, 2023.
[61] T. H. Kung, M. Cheatham, A. Medenilla, C. Sillos, L. De Leon, C. Elepaño, M. Madriaga, R. Aggabao, G. Diaz-Candido, J. Maningo et al., "Performance of chatgpt on usmle: Potential for ai-assisted medical education using large language models," PLOS Digital Health, vol. 2, no. 2, p. e0000198, 2023.
[62] J. V. Pavlik, "Collaborating with chatgpt: Considering the implications of generative artificial intelligence for journalism and media education," Journalism $\mathcal{E}$ Mass Communication Educator, p. 10776958221149577, 2023.
[63] D. Baidoo-Ariu and L. Owusu Ansah, "Education in the era of generative artificial intelligence (ai): Understanding the potential benefits of chatgpt in promoting teaching and learning," Available at SSRN 4337484, 2023.
[64] S. Frieder, L. Pinchetti, R.-R. Griffiths, T. Salvatori, T. Lukasiewicz, P. C. Petersen, A. Chevalier, and J. Berner, "Mathematical capabilities of chatgpt," arXiv preprint arXiv:2301.13867, 2023.
[65] M. Dowling and B. Lucey, "Chatgpt for (finance) research: The bananarama conjecture," Finance Research Letters, p. 103662, 2023.
[66] E. A. van Dis, J. Bollen, W. Zuidema, R. van Rooij, and C. L. Bockting, "Chatgpt: five priorities for research," Nature, vol. 614, no. 7947, pp. 224-226, 2023.
[67] R. W. McGee, "Is chat gpt biased against conservatives? an empirical study," An Empirical Study (February 15, 2023), 2023.
[68] A. Blum, "Breaking chatgpt with dangerous questions understanding how chatgpt prioritizes safety, context, and obedience," 2022.
[69] H. Y. Jabotinsky and R. Sarel, "Co-authoring with an ai? ethical dilemmas and artificial intelligence," Ethical Dilemmas and Artificial Intelligence (December 15, 2022), 2022.
[70] T. Susnjak, "Chatgpt: The end of online exam integrity?" arXiv preprint arXiv:2212.09292, 2022.
[71] M. Khalil and E. Er, "Will chatgpt get you caught? rethinking of plagiarism detection," arXiv preprint arXiv:2302.04335, 2023.
[72] D. Castelvecchi, "Are chatgpt and alphacode going to replace programmers?" Nature, 2022.
[73] A. Zarithonarvar, "Economics of chatgpt: A labor market view on the occupational impact of artificial intelligence," Available at SSRN 4350925, 2023.
[74] R. He and J. McAuley, "Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering,"
in proceedings of the 25th international conference on world wide web, 2016, pp. 507-517.
[75] Y. Bao, M. Wu, S. Chang, and R. Barzilay, "Few-shot text classification with distributional signatures," arXiv preprint arXiv:1908.06039, 2019.
[76] S. Wang, X. Liu, B. Liu, and D. Dong, "Sentence-aware adversarial meta-learning for few-shot text classification," in Proceedings of the 29th International Conference on Computational Linguistics, 2022, pp. $4844-4852$.
[77] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever et al., "Language models are unsupervised multitask learners," OpenAI blog, vol. 1, no. 8, p. 9, 2019.
[78] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, "Attention is all you need," Advances in neural information processing systems, vol. 30, 2017.
[79] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, "Proximal policy optimization algorithms," arXiv preprint arXiv:1707.06347, 2017.
[80] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, "Bert: Pretraining of deep bidirectional transformers for language understanding," in Proceedings of NAACL-HLT, 2019, pp. 4171-4186.
[81] W. Liao, Z. Liu, H. Dai, Z. Wu, Y. Zhang, X. Huang, Y. Chen, X. Jiang, D. Zhu, T. Liu, S. Li, X. Li, and H. Cai, "Mask-guided bert for few shot text classification," arXiv preprint arXiv:2302.10447, 2023.
[82] S. Rezayi, Z. Liu, Z. Wu, C. Dhakal, B. Ge, C. Zhen, T. Liu, and S. Li, "Agribert: Knowledge-infused agricultural language models for matching food and nutrition," International Joint Conference on Artificial Intelligence, July 23-29, 2022, Vienna, Austria, 2022.
[83] E. Ma, "Nlp augmentation," https://github.com/makcedward/nlpaug, 2019.
[84] J. Morris, E. Lifland, J. Y. Yoo, J. Grigsby, D. Jin, and Y. Qi, "Textattack: A framework for adversarial attacks, data augmentation, and adversarial training in nlp," in Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, 2020, pp. 119-126.
[85] N. Ng, K. Yee, A. Baevski, M. Ott, M. Auli, and S. Edunov, "Facebook fair's wmt19 news translation task submission," in Proc. of WMT, 2020.
[86] L.-K. Huang, J. Huang, Y. Rong, Q. Yang, and Y. Wei, "Frustratingly easy transferability estimation," in International Conference on Machine Learning. PMLR, 2022, pp. 9201-9225.
[87] B. Li, H. Zhou, J. He, M. Wang, Y. Yang, and L. Li, "On the sentence embeddings from pre-trained language models," in Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2020, pp. 9119-9130.
[88] J. Wang and Y. Dong, "Measurement of text similarity: a survey," Information, vol. 11, no. 9, p. 421, 2020.
[89] T. M. Cover, Elements of information theory. John Wiley \&amp; Sons, 1999.
[90] X. Cai, S. Liu, J. Han, L. Yang, Z. Liu, and T. Liu, "Chestxraybert: A pretrained language model for chest radiology report summarization," IEEE Transactions on Multimedia, pp. 1-1, 2021.
[91] X. Cai, S. Liu, L. Yang, Y. Lu, J. Zhao, D. Shen, and T. Liu, "Covidsum: A linguistically enriched scibert-based summarization model for covid-19 scientific papers," Journal of Biomedical Informatics, vol. 127, p. 103999, 2022.
[92] A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen, "Hierarchical text-conditional image generation with clip latents," arXiv preprint arXiv:2204.06125, 2022.
[93] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, "High-resolution image synthesis with latent diffusion models," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 10684-10695.
[94] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen, "Lora: Low-rank adaptation of large language models," arXiv preprint arXiv:2106.09685, 2021.
[95] N. Ruiz, Y. Li, V. Jampani, Y. Pritch, M. Rubinstein, and K. Aberman, "Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation," arXiv preprint arXiv:2208.12242, 2022.
[96] M. Kosinski, "Theory of mind may have spontaneously emerged in large language models," arXiv preprint arXiv:2302.02083, 2023.
[97] X. Liu, M. Zhou, G. Shi, Y. Du, L. Zhao, Z. Wu, D. Liu, T. Liu, and X. Hu, "Coupling artificial neurons in bert and biological neurons in the human brain," in Proceedings of the 37th AAAI Conference on Artificial Intelligence, AAAI, 2023.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<ol>
<li>https://www.kaggle.com/datasets/paultimothymooney/medical-speech-transcription-and-intent</li>
</ol>
<p><a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
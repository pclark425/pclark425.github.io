<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5655 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5655</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5655</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-115.html">extraction-schema-115</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <p><strong>Paper ID:</strong> paper-261557284</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2309.02206v1.pdf" target="_blank">Language Models for Novelty Detection in System Call Traces</a></p>
                <p><strong>Paper Abstract:</strong> Due to the complexity of modern computer systems, novel and unexpected behaviors frequently occur. Such deviations are either normal occurrences, such as software updates and new user activities, or abnormalities, such as misconfigurations, latency issues, intrusions, and software bugs. Regardless, novel behaviors are of great interest to developers, and there is a genuine need for efficient and effective methods to detect them. Nowadays, researchers consider system calls to be the most fine-grained and accurate source of information to investigate the behavior of computer systems. Accordingly, this paper introduces a novelty detection methodology that relies on a probability distribution over sequences of system calls, which can be seen as a language model. Language models estimate the likelihood of sequences, and since novelties deviate from previously observed behaviors by definition, they would be unlikely under the model. Following the success of neural networks for language models, three architectures are evaluated in this work: the widespread LSTM, the state-of-the-art Transformer, and the lower-complexity Longformer. However, large neural networks typically require an enormous amount of data to be trained effectively, and to the best of our knowledge, no massive modern datasets of kernel traces are publicly available. This paper addresses this limitation by introducing a new open-source dataset of kernel traces comprising over 2 million web requests with seven distinct behaviors. The proposed methodology requires minimal expert hand-crafting and achieves an F-score and AuROC greater than 95% on most novelties while being data- and task-agnostic. The source code and trained models are publicly available on GitHub while the datasets are available on Zenodo.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5655.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5655.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LSTM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Long Short-Term Memory network</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A unidirectional multi-layer recurrent neural network trained as a left-to-right language model on sequences of system calls and their arguments to estimate conditional probabilities; used to score requests via perplexity and detect novel/out-of-distribution behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Long short-term memory</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LSTM (unidirectional multi-layer)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Standard multi-layer LSTM RNN trained autoregressively (left-to-right LM) on concatenated embeddings/encodings of system call name, return value, procname, entry flag, pid/tid positional encodings and elapsed timestamps; models were manually tuned (depth, width, embedding size, etc.) and trained with mixed precision.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_method</strong></td>
                            <td>Train LSTM as left-to-right language model; compute per-request perplexity (sequence-level normalized likelihood) and classify as novel if perplexity exceeds an empirically chosen threshold (threshold selected to maximize F-score on validation OOD).</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Sequences (system call traces with arguments; per-request sequences delimited by userspace events)</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Novel behaviors / out-of-distribution requests including misconfigurations, latencies, resource overloads and security changes (CPU overload, OPcache disabled, Dump IO, reduced Connection slots, KeepAlive disabled (Socket), SSL disabled, injected latencies).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Open-source kernel traces dataset (over 2 million web requests, 7 behaviors; Zenodo record 7378420)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Evaluated with per-behavior AuROC and F-score (Table III). LSTM (mean ± std over 5 seeds): Test Connection AuROC 79.9 ± 3.8, F1 74.8 ± 2.9; Test CPU AuROC 98.5 ± 0.6, F1 93.6 ± 2.1; Test IO AuROC 100.0 ± 0.0, F1 100.0 ± 0.0; Test OPcache AuROC 99.7 ± 0.1, F1 98.3 ± 0.2; Test Socket AuROC 98.8 ± 0.6, F1 94.7 ± 2.3; Test SSL AuROC 91.9 ± 1.7, F1 85.4 ± 2.0.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Greatly outperformed a 4-gram baseline on most behaviors (4-gram often failed except for IO). Compared to attention-based models, LSTM performed on par or better for 3 out of 6 behaviors due to its inductive bias toward local dependencies.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Biased toward local/short-range dependencies (may miss very long-range contextual cues); performance depends on large labeled/segmented trace dataset; black-box interpretability; computational/training cost; may be less effective if behavior requires modeling very long dependencies beyond LSTM effective context (200–400 tokens).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5655.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5655.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transformer (vanilla)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A self-attention-based sequence model trained as a left-to-right language model on system call sequences; models arbitrary-length dependencies but has quadratic complexity in sequence length, requiring sequence truncation during training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Attention is all you need</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer (vanilla, decoder-only/auto-regressive setup for LM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Vanilla Transformer architecture using self-attention and inter-attention components adapted to left-to-right language modeling; trained with mixed precision and gradient checkpointing; due to quadratic complexity, sequences longer than 2048 system calls were truncated.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_method</strong></td>
                            <td>Train Transformer as left-to-right LM; compute per-request perplexity and detect novelties by thresholding perplexity (thresholds chosen per-novelty using validation sets).</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Sequences (system call traces with arguments; per-request sequences)</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Same set of novelties as LSTM: misconfigurations, latencies, resource overloads, and security changes (CPU, OPcache, Dump IO, Connection, Socket, SSL, latency injections).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Open-source kernel traces dataset (over 2 million web requests, 7 behaviors; Zenodo record 7378420)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Transformer (Table III): Test Connection AuROC 97.8 ± 1.6, F1 94.3 ± 2.9; Test CPU AuROC 94.8 ± 1.8, F1 85.1 ± 3.4; Test IO AuROC 100.0 ± 0.0, F1 100.0 ± 0.0; Test OPcache AuROC 99.1 ± 0.2, F1 96.7 ± 0.4; Test Socket AuROC 99.9 ± 0.1, F1 99.1 ± 0.6; Test SSL AuROC 99.7 ± 0.2, F1 98.5 ± 0.9. Cross-entropy / token accuracy on ID/test sets reported in Table II (e.g., Test ID cross-entropy ~0.907 ± 0.038 in a smaller config).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Outperformed the 4-gram baseline across behaviors and in many cases matched or exceeded LSTM performance for some behaviors; however, large Transformer variants tended to assign high likelihoods across behaviors (reducing novelty separability), and the need to truncate long traces due to quadratic complexity is a practical drawback.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Quadratic complexity forces sequence truncation (potentially losing long-range signals); large models can over-generalize and assign high likelihood to OOD behavior; learned attention patterns were observed to be mostly local in this domain, reducing theoretical advantage; computationally intensive to train and deploy.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5655.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5655.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Longformer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Longformer (sparse-attention Transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Transformer variant that replaces dense self-attention with windowed local attention plus a small number of global tokens to achieve linear complexity with respect to sequence length; used here as a scalable LM for long system-call traces.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Longformer: The long-document transformer</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Longformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer variant using sliding-window local attention and a fixed set of global tokens to achieve linear time/space scaling in sequence length; trained autoregressively as a left-to-right LM with tunable window size, dilation, and number of global tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_method</strong></td>
                            <td>Train Longformer as left-to-right LM on per-request sequences; compute request perplexity and threshold to detect novelties.</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Sequences (long system call traces with arguments)</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Same catalog of novelties: misconfigurations, latency, IO overloads, CPU overload, socket/keepalive, SSL removal, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Open-source kernel traces dataset (over 2 million web requests, 7 behaviors; Zenodo record 7378420)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Longformer (Table III): Test Connection AuROC 93.6 ± 1.4, F1 87.6 ± 1.8; Test CPU AuROC 67.9 ± 7.7, F1 59.6 ± 4.1; Test IO AuROC 100.0 ± 0.0, F1 100.0 ± 0.0; Test OPcache AuROC 98.9 ± 0.2, F1 96.2 ± 0.4; Test Socket AuROC 99.1 ± 0.4, F1 96.4 ± 1.3; Test SSL AuROC 98.4 ± 0.5, F1 94.5 ± 0.9.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Better than 4-gram baseline on most behaviors; generally competitive with Transformer/LSTM except it performed significantly worse on CPU behavior due to window/global-token design limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Windowed attention can miss dependencies spanning beyond the window size unless many global tokens or larger windows are used; for CPU behavior attention patterns required dependencies beyond the chosen window leading to poor detection; requires tuning of window/global-token hyperparameters for best results.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5655.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5655.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>4-gram baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>4-gram Markov model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A classical n-gram (4-gram) Markov-style language model baseline that approximates P(w_i | history) using frequencies of length-4 n-grams computed from the training corpus; used as a simple baseline novelty detector.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>4-gram (n-gram) model</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Counts-based 4-gram conditional probability estimator: P(w_i | w_{i-1},...,w_{i-3}) computed from dataset frequencies; used to compute sequence likelihoods and derive novelty scores.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_method</strong></td>
                            <td>Compute per-token conditional probabilities via n-gram counts; aggregate into sequence-level score (joint probability or normalized measure) and threshold to detect anomalies.</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Sequences (system call names; no learned embeddings of args in baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Same novelty categories as neural models (misconfigurations, IO/CPU issues, latency, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Open-source kernel traces dataset (over 2 million web requests, 7 behaviors; Zenodo record 7378420)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>4-gram (Table III): Test Connection AuROC 51.5, F1 66.7; Test CPU AuROC 0.9, F1 53.2; Test IO AuROC 98.6, F1 94.7; Test OPcache AuROC 65.2, F1 67.5; Test Socket AuROC 22.6, F1 66.7; Test SSL AuROC 50.5, F1 66.7.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Served as a weak baseline; performed poorly on most novelty types except IO where distributional differences in syscall names made detection easy. Neural LMs (LSTM/Transformer/Longformer) substantially outperformed the 4-gram baseline on the majority of behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Cannot model long-range dependencies beyond fixed n context; ignores arguments/continuous features unless specially encoded; fragile when distributional shifts are subtle and require modeling anomalies in timing or arguments.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5655.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5655.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Perplexity scoring</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Perplexity-based novelty scoring</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that uses the per-sequence perplexity (normalized inverse likelihood) produced by a trained language model as a novelty score; sequences with higher perplexity than training distribution are classified as novel.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Perplexity (LM scoring)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Perplexity computed as P(w)^{-1/N} (sequence joint probability raised to -1/N) where N is sequence length; used to normalize for sequence length and compare sequence likelihoods across varying lengths and throughputs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_method</strong></td>
                            <td>Score each request sequence with the LM's perplexity; determine a threshold per novelty (using ID and OOD validation sets) that maximizes F-score; optionally report ROC/AUROC across thresholds.</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Sequences (system call traces), including variable-length requests and timing information</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Global sequence novelties (novel sequences/out-of-distribution behaviors), including point outliers when they affect conditional probabilities and length/timing anomalies (latency) because perplexity is normalized by length.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Open-source kernel traces dataset (over 2 million web requests, 7 behaviors; Zenodo record 7378420)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Perplexity served as the primary novelty score; thresholds chosen to maximize F-score per behavior; results reported as F-score and AuROC for each model (see Table III). Authors report F-score and AuROC >95% on most novelties for the proposed methodology overall.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Perplexity-based detection outperformed top-k misprediction schemes and raw joint probability thresholds in this domain because joint probability decreases strongly with sequence length and token-level top-k only finds point outliers.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Perplexity assumes ergodic/long sequences for entropy approximation and can still be sensitive to model calibration; thresholds are chosen per-novelty which may not transfer; extremely long or highly concurrent traces produce noise; definition of request boundaries impacts scores.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5655.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5655.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Kernel trace dataset</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Open-source kernel traces dataset (system call traces with arguments)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A newly introduced dataset of over 2 million web requests collected with LTTng on an Apache/MySQL/PHP server under seven realistic behaviors (ID + six OOD scenarios), including full system call names and arguments, per-request delimitation, and labels.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Kernel traces dataset (Fournier et al. 2022 collection)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Dataset collected using wrk2 clients and instrumented Apache (userspace events httpd:enter_event_handler/exit_event_handler) with LTTng capturing kernel events; includes sysname, timestamp (as elapsed times), return value, process name, tid/pid encodings, and entry/exit flag; balanced validation/test splits across behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_method</strong></td>
                            <td>Provides labeled in-distribution (train/val/test) and out-of-distribution (validation/test) sets for novelty detection experiments; used to train LMs and evaluate perplexity-based detection.</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Structured sequential data: per-request sequences of system call events with multiple arguments and timings</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Seven behaviors: nominal (ID) plus OOD: CPU overload, OPcache disabled, Dump IO (heavy logging), reduced Connection concurrency, Socket KeepAlive disabled, SSL disabled, and artificially injected latencies (microsecond to millisecond delays).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Open-source kernel traces dataset (over 2 million web requests; Zenodo record 7378420)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Used as training/validation/test splits to compute LM cross-entropy, token accuracy, per-request perplexity distributions, F-score and AuROC for novelty detection experiments reported in Tables II and III.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Enables fair comparison between classical n-gram baseline and neural LMs (LSTM/Transformer/Longformer); shows neural LMs generally outperform simplistic baselines on this modern large-scale trace dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Collected in a controlled environment and balanced across classes (real-world novelty incidence is much lower), so external generalization may be limited; other system configurations and workloads may differ.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Lstmbased system-call language modeling and robust ensemble method for designing host-based intrusion detection systems <em>(Rating: 2)</em></li>
                <li>Deeplog: Anomaly detection and diagnosis from system logs through deep learning <em>(Rating: 2)</em></li>
                <li>Recurrent neural network attention mechanisms for interpretable system log anomaly detection <em>(Rating: 2)</em></li>
                <li>Self-supervised anomaly detection from distributed traces <em>(Rating: 2)</em></li>
                <li>On improving deep learning trace analysis with system call arguments <em>(Rating: 2)</em></li>
                <li>A framework for anomaly detection in time-driven and event-driven processes using kernel traces <em>(Rating: 1)</em></li>
                <li>Anomaly detection and classification using distributed tracing and deep learning <em>(Rating: 1)</em></li>
                <li>Learning useful system call attributes for anomaly detection <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5655",
    "paper_id": "paper-261557284",
    "extraction_schema_id": "extraction-schema-115",
    "extracted_data": [
        {
            "name_short": "LSTM",
            "name_full": "Long Short-Term Memory network",
            "brief_description": "A unidirectional multi-layer recurrent neural network trained as a left-to-right language model on sequences of system calls and their arguments to estimate conditional probabilities; used to score requests via perplexity and detect novel/out-of-distribution behaviors.",
            "citation_title": "Long short-term memory",
            "mention_or_use": "use",
            "model_name": "LSTM (unidirectional multi-layer)",
            "model_description": "Standard multi-layer LSTM RNN trained autoregressively (left-to-right LM) on concatenated embeddings/encodings of system call name, return value, procname, entry flag, pid/tid positional encodings and elapsed timestamps; models were manually tuned (depth, width, embedding size, etc.) and trained with mixed precision.",
            "model_size": null,
            "anomaly_detection_method": "Train LSTM as left-to-right language model; compute per-request perplexity (sequence-level normalized likelihood) and classify as novel if perplexity exceeds an empirically chosen threshold (threshold selected to maximize F-score on validation OOD).",
            "data_type": "Sequences (system call traces with arguments; per-request sequences delimited by userspace events)",
            "anomaly_type": "Novel behaviors / out-of-distribution requests including misconfigurations, latencies, resource overloads and security changes (CPU overload, OPcache disabled, Dump IO, reduced Connection slots, KeepAlive disabled (Socket), SSL disabled, injected latencies).",
            "dataset_name": "Open-source kernel traces dataset (over 2 million web requests, 7 behaviors; Zenodo record 7378420)",
            "performance_metrics": "Evaluated with per-behavior AuROC and F-score (Table III). LSTM (mean ± std over 5 seeds): Test Connection AuROC 79.9 ± 3.8, F1 74.8 ± 2.9; Test CPU AuROC 98.5 ± 0.6, F1 93.6 ± 2.1; Test IO AuROC 100.0 ± 0.0, F1 100.0 ± 0.0; Test OPcache AuROC 99.7 ± 0.1, F1 98.3 ± 0.2; Test Socket AuROC 98.8 ± 0.6, F1 94.7 ± 2.3; Test SSL AuROC 91.9 ± 1.7, F1 85.4 ± 2.0.",
            "baseline_comparison": "Greatly outperformed a 4-gram baseline on most behaviors (4-gram often failed except for IO). Compared to attention-based models, LSTM performed on par or better for 3 out of 6 behaviors due to its inductive bias toward local dependencies.",
            "limitations_or_failure_cases": "Biased toward local/short-range dependencies (may miss very long-range contextual cues); performance depends on large labeled/segmented trace dataset; black-box interpretability; computational/training cost; may be less effective if behavior requires modeling very long dependencies beyond LSTM effective context (200–400 tokens).",
            "uuid": "e5655.0"
        },
        {
            "name_short": "Transformer",
            "name_full": "Transformer (vanilla)",
            "brief_description": "A self-attention-based sequence model trained as a left-to-right language model on system call sequences; models arbitrary-length dependencies but has quadratic complexity in sequence length, requiring sequence truncation during training.",
            "citation_title": "Attention is all you need",
            "mention_or_use": "use",
            "model_name": "Transformer (vanilla, decoder-only/auto-regressive setup for LM)",
            "model_description": "Vanilla Transformer architecture using self-attention and inter-attention components adapted to left-to-right language modeling; trained with mixed precision and gradient checkpointing; due to quadratic complexity, sequences longer than 2048 system calls were truncated.",
            "model_size": null,
            "anomaly_detection_method": "Train Transformer as left-to-right LM; compute per-request perplexity and detect novelties by thresholding perplexity (thresholds chosen per-novelty using validation sets).",
            "data_type": "Sequences (system call traces with arguments; per-request sequences)",
            "anomaly_type": "Same set of novelties as LSTM: misconfigurations, latencies, resource overloads, and security changes (CPU, OPcache, Dump IO, Connection, Socket, SSL, latency injections).",
            "dataset_name": "Open-source kernel traces dataset (over 2 million web requests, 7 behaviors; Zenodo record 7378420)",
            "performance_metrics": "Transformer (Table III): Test Connection AuROC 97.8 ± 1.6, F1 94.3 ± 2.9; Test CPU AuROC 94.8 ± 1.8, F1 85.1 ± 3.4; Test IO AuROC 100.0 ± 0.0, F1 100.0 ± 0.0; Test OPcache AuROC 99.1 ± 0.2, F1 96.7 ± 0.4; Test Socket AuROC 99.9 ± 0.1, F1 99.1 ± 0.6; Test SSL AuROC 99.7 ± 0.2, F1 98.5 ± 0.9. Cross-entropy / token accuracy on ID/test sets reported in Table II (e.g., Test ID cross-entropy ~0.907 ± 0.038 in a smaller config).",
            "baseline_comparison": "Outperformed the 4-gram baseline across behaviors and in many cases matched or exceeded LSTM performance for some behaviors; however, large Transformer variants tended to assign high likelihoods across behaviors (reducing novelty separability), and the need to truncate long traces due to quadratic complexity is a practical drawback.",
            "limitations_or_failure_cases": "Quadratic complexity forces sequence truncation (potentially losing long-range signals); large models can over-generalize and assign high likelihood to OOD behavior; learned attention patterns were observed to be mostly local in this domain, reducing theoretical advantage; computationally intensive to train and deploy.",
            "uuid": "e5655.1"
        },
        {
            "name_short": "Longformer",
            "name_full": "Longformer (sparse-attention Transformer)",
            "brief_description": "A Transformer variant that replaces dense self-attention with windowed local attention plus a small number of global tokens to achieve linear complexity with respect to sequence length; used here as a scalable LM for long system-call traces.",
            "citation_title": "Longformer: The long-document transformer",
            "mention_or_use": "use",
            "model_name": "Longformer",
            "model_description": "Transformer variant using sliding-window local attention and a fixed set of global tokens to achieve linear time/space scaling in sequence length; trained autoregressively as a left-to-right LM with tunable window size, dilation, and number of global tokens.",
            "model_size": null,
            "anomaly_detection_method": "Train Longformer as left-to-right LM on per-request sequences; compute request perplexity and threshold to detect novelties.",
            "data_type": "Sequences (long system call traces with arguments)",
            "anomaly_type": "Same catalog of novelties: misconfigurations, latency, IO overloads, CPU overload, socket/keepalive, SSL removal, etc.",
            "dataset_name": "Open-source kernel traces dataset (over 2 million web requests, 7 behaviors; Zenodo record 7378420)",
            "performance_metrics": "Longformer (Table III): Test Connection AuROC 93.6 ± 1.4, F1 87.6 ± 1.8; Test CPU AuROC 67.9 ± 7.7, F1 59.6 ± 4.1; Test IO AuROC 100.0 ± 0.0, F1 100.0 ± 0.0; Test OPcache AuROC 98.9 ± 0.2, F1 96.2 ± 0.4; Test Socket AuROC 99.1 ± 0.4, F1 96.4 ± 1.3; Test SSL AuROC 98.4 ± 0.5, F1 94.5 ± 0.9.",
            "baseline_comparison": "Better than 4-gram baseline on most behaviors; generally competitive with Transformer/LSTM except it performed significantly worse on CPU behavior due to window/global-token design limitations.",
            "limitations_or_failure_cases": "Windowed attention can miss dependencies spanning beyond the window size unless many global tokens or larger windows are used; for CPU behavior attention patterns required dependencies beyond the chosen window leading to poor detection; requires tuning of window/global-token hyperparameters for best results.",
            "uuid": "e5655.2"
        },
        {
            "name_short": "4-gram baseline",
            "name_full": "4-gram Markov model",
            "brief_description": "A classical n-gram (4-gram) Markov-style language model baseline that approximates P(w_i | history) using frequencies of length-4 n-grams computed from the training corpus; used as a simple baseline novelty detector.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "4-gram (n-gram) model",
            "model_description": "Counts-based 4-gram conditional probability estimator: P(w_i | w_{i-1},...,w_{i-3}) computed from dataset frequencies; used to compute sequence likelihoods and derive novelty scores.",
            "model_size": null,
            "anomaly_detection_method": "Compute per-token conditional probabilities via n-gram counts; aggregate into sequence-level score (joint probability or normalized measure) and threshold to detect anomalies.",
            "data_type": "Sequences (system call names; no learned embeddings of args in baseline)",
            "anomaly_type": "Same novelty categories as neural models (misconfigurations, IO/CPU issues, latency, etc.)",
            "dataset_name": "Open-source kernel traces dataset (over 2 million web requests, 7 behaviors; Zenodo record 7378420)",
            "performance_metrics": "4-gram (Table III): Test Connection AuROC 51.5, F1 66.7; Test CPU AuROC 0.9, F1 53.2; Test IO AuROC 98.6, F1 94.7; Test OPcache AuROC 65.2, F1 67.5; Test Socket AuROC 22.6, F1 66.7; Test SSL AuROC 50.5, F1 66.7.",
            "baseline_comparison": "Served as a weak baseline; performed poorly on most novelty types except IO where distributional differences in syscall names made detection easy. Neural LMs (LSTM/Transformer/Longformer) substantially outperformed the 4-gram baseline on the majority of behaviors.",
            "limitations_or_failure_cases": "Cannot model long-range dependencies beyond fixed n context; ignores arguments/continuous features unless specially encoded; fragile when distributional shifts are subtle and require modeling anomalies in timing or arguments.",
            "uuid": "e5655.3"
        },
        {
            "name_short": "Perplexity scoring",
            "name_full": "Perplexity-based novelty scoring",
            "brief_description": "A method that uses the per-sequence perplexity (normalized inverse likelihood) produced by a trained language model as a novelty score; sequences with higher perplexity than training distribution are classified as novel.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Perplexity (LM scoring)",
            "model_description": "Perplexity computed as P(w)^{-1/N} (sequence joint probability raised to -1/N) where N is sequence length; used to normalize for sequence length and compare sequence likelihoods across varying lengths and throughputs.",
            "model_size": null,
            "anomaly_detection_method": "Score each request sequence with the LM's perplexity; determine a threshold per novelty (using ID and OOD validation sets) that maximizes F-score; optionally report ROC/AUROC across thresholds.",
            "data_type": "Sequences (system call traces), including variable-length requests and timing information",
            "anomaly_type": "Global sequence novelties (novel sequences/out-of-distribution behaviors), including point outliers when they affect conditional probabilities and length/timing anomalies (latency) because perplexity is normalized by length.",
            "dataset_name": "Open-source kernel traces dataset (over 2 million web requests, 7 behaviors; Zenodo record 7378420)",
            "performance_metrics": "Perplexity served as the primary novelty score; thresholds chosen to maximize F-score per behavior; results reported as F-score and AuROC for each model (see Table III). Authors report F-score and AuROC &gt;95% on most novelties for the proposed methodology overall.",
            "baseline_comparison": "Perplexity-based detection outperformed top-k misprediction schemes and raw joint probability thresholds in this domain because joint probability decreases strongly with sequence length and token-level top-k only finds point outliers.",
            "limitations_or_failure_cases": "Perplexity assumes ergodic/long sequences for entropy approximation and can still be sensitive to model calibration; thresholds are chosen per-novelty which may not transfer; extremely long or highly concurrent traces produce noise; definition of request boundaries impacts scores.",
            "uuid": "e5655.4"
        },
        {
            "name_short": "Kernel trace dataset",
            "name_full": "Open-source kernel traces dataset (system call traces with arguments)",
            "brief_description": "A newly introduced dataset of over 2 million web requests collected with LTTng on an Apache/MySQL/PHP server under seven realistic behaviors (ID + six OOD scenarios), including full system call names and arguments, per-request delimitation, and labels.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Kernel traces dataset (Fournier et al. 2022 collection)",
            "model_description": "Dataset collected using wrk2 clients and instrumented Apache (userspace events httpd:enter_event_handler/exit_event_handler) with LTTng capturing kernel events; includes sysname, timestamp (as elapsed times), return value, process name, tid/pid encodings, and entry/exit flag; balanced validation/test splits across behaviors.",
            "model_size": null,
            "anomaly_detection_method": "Provides labeled in-distribution (train/val/test) and out-of-distribution (validation/test) sets for novelty detection experiments; used to train LMs and evaluate perplexity-based detection.",
            "data_type": "Structured sequential data: per-request sequences of system call events with multiple arguments and timings",
            "anomaly_type": "Seven behaviors: nominal (ID) plus OOD: CPU overload, OPcache disabled, Dump IO (heavy logging), reduced Connection concurrency, Socket KeepAlive disabled, SSL disabled, and artificially injected latencies (microsecond to millisecond delays).",
            "dataset_name": "Open-source kernel traces dataset (over 2 million web requests; Zenodo record 7378420)",
            "performance_metrics": "Used as training/validation/test splits to compute LM cross-entropy, token accuracy, per-request perplexity distributions, F-score and AuROC for novelty detection experiments reported in Tables II and III.",
            "baseline_comparison": "Enables fair comparison between classical n-gram baseline and neural LMs (LSTM/Transformer/Longformer); shows neural LMs generally outperform simplistic baselines on this modern large-scale trace dataset.",
            "limitations_or_failure_cases": "Collected in a controlled environment and balanced across classes (real-world novelty incidence is much lower), so external generalization may be limited; other system configurations and workloads may differ.",
            "uuid": "e5655.5"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Lstmbased system-call language modeling and robust ensemble method for designing host-based intrusion detection systems",
            "rating": 2,
            "sanitized_title": "lstmbased_systemcall_language_modeling_and_robust_ensemble_method_for_designing_hostbased_intrusion_detection_systems"
        },
        {
            "paper_title": "Deeplog: Anomaly detection and diagnosis from system logs through deep learning",
            "rating": 2,
            "sanitized_title": "deeplog_anomaly_detection_and_diagnosis_from_system_logs_through_deep_learning"
        },
        {
            "paper_title": "Recurrent neural network attention mechanisms for interpretable system log anomaly detection",
            "rating": 2,
            "sanitized_title": "recurrent_neural_network_attention_mechanisms_for_interpretable_system_log_anomaly_detection"
        },
        {
            "paper_title": "Self-supervised anomaly detection from distributed traces",
            "rating": 2,
            "sanitized_title": "selfsupervised_anomaly_detection_from_distributed_traces"
        },
        {
            "paper_title": "On improving deep learning trace analysis with system call arguments",
            "rating": 2,
            "sanitized_title": "on_improving_deep_learning_trace_analysis_with_system_call_arguments"
        },
        {
            "paper_title": "A framework for anomaly detection in time-driven and event-driven processes using kernel traces",
            "rating": 1,
            "sanitized_title": "a_framework_for_anomaly_detection_in_timedriven_and_eventdriven_processes_using_kernel_traces"
        },
        {
            "paper_title": "Anomaly detection and classification using distributed tracing and deep learning",
            "rating": 1,
            "sanitized_title": "anomaly_detection_and_classification_using_distributed_tracing_and_deep_learning"
        },
        {
            "paper_title": "Learning useful system call attributes for anomaly detection",
            "rating": 1,
            "sanitized_title": "learning_useful_system_call_attributes_for_anomaly_detection"
        }
    ],
    "cost": 0.016454749999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Language Models for Novelty Detection in System Call Traces</p>
<p>Quentin Fournier quentin.fournier@polymtl.ca 
Polytechnique Montreal Montreal
H3T 1J4Quebec</p>
<p>Daniel Aloise daniel.aloise@polymtl.ca 
Polytechnique Montreal Montreal
H3T 1J4Quebec</p>
<p>Leandro R Costa leandro.costa@polymtl.ca 
Polytechnique Montreal Montreal
H3T 1J4Quebec</p>
<p>Language Models for Novelty Detection in System Call Traces
Index Terms-AIOpsNovelty DetectionAnomaly DetectionNLPLanguage ModelsLSTMTransformer
Due to the complexity of modern computer systems, novel and unexpected behaviors frequently occur. Such deviations are either normal occurrences, such as software updates and new user activities, or abnormalities, such as misconfigurations, latency issues, intrusions, and software bugs. Regardless, novel behaviors are of great interest to developers, and there is a genuine need for efficient and effective methods to detect them. Nowadays, researchers consider system calls to be the most fine-grained and accurate source of information to investigate the behavior of computer systems. Accordingly, this paper introduces a novelty detection methodology that relies on a probability distribution over sequences of system calls, which can be seen as a language model. Language models estimate the likelihood of sequences, and since novelties deviate from previously observed behaviors by definition, they would be unlikely under the model. Following the success of neural networks for language models, three architectures are evaluated in this work: the widespread LSTM, the state-of-the-art Transformer, and the lower-complexity Longformer. However, large neural networks typically require an enormous amount of data to be trained effectively, and to the best of our knowledge, no massive modern datasets of kernel traces are publicly available. This paper addresses this limitation by introducing a new opensource dataset of kernel traces comprising over 2 million web requests with seven distinct behaviors. The proposed methodology requires minimal expert hand-crafting and achieves an F-score and AuROC greater than 95% on most novelties while being data-and task-agnostic. The source code and trained models are publicly available on GitHub 1 while the datasets are available on Zenodo 2 .</p>
<p>I. INTRODUCTION</p>
<p>Even though computer systems are virtually deterministic, complex interactions between hardware and software often result in novel and unexpected behaviors. Novel behaviors are deviations from what has been previously observed and may be common behaviors such as component upgrades, software updates, new users, and rare queries, or anomalies such as misconfigurations, latency, intrusions, hardware failures, and bugs. This research focuses on detecting novelties rather than anomalies since it is a broader problem and since normal yet novel behaviors are often of interest to practitioners. Moreover, anomaly detection methods may be ineffective in detecting clever attacks designed to resemble legitimate users, which may still be detected as novel behaviors.</p>
<p>A non-intrusive and lightweight approach to recording the behavior of computer systems is to trace them. Tracing is the act of collecting low-level events generated whenever a specific instruction called tracepoint is encountered at runtime. This research considers events generated by the operating system called kernel events since they expose the behavior of the whole system [1]. Furthermore, kernel events allow tracing virtually any Linux system without modifying the source code since tracepoints are already implemented in the Linux kernel. In particular, this paper focuses on a subset of the kernel events named system calls or syscall. System calls correspond to requests from applications running in the userspace to the kernel in order to access resources such as memory, network, or other devices that would otherwise be inaccessible. In short, system calls are the only way for an application to communicate with the operating system. As mentioned by Kim et al. [2], many researchers consider system calls to be the most fine-grained and accurate source of information to analyze computer systems.</p>
<p>Due to the computational speed of modern computers, operating systems often execute hundreds of system calls every second, making the manual analysis of a collection of kernel traces with tools such as Trace Compass extremely time-consuming. As a result, practitioners and researchers often analyze traces automatically [3]. Since novel behaviors are unexpected and unknown by definition, their detection is difficult to specify in practice. Consequently, novel behaviors are typically detected with machine learning techniques since they learn to solve the task from examples [4,5,6]. Nonetheless, most machine learning algorithms benefit from or require carefully hand-crafted features [7,8]. For the past decade, research has focused on neural networks [2,9,10] since they automatically learn to extract meaningful features for the task, thereby reducing the need for an expert and improving the performance. Since traces are sequences of discrete values comprising a syntax and a semantic akin to natural languages [11], deep learning techniques from natural language processing (NLP) are particularly well suited for traces.</p>
<p>Natural language processing is the use of natural language by a computer and includes various tasks such as ques-tion answering, machine translation, summarization, sentiment analysis, and image captioning. A wide range of NLP applications rely on a probability distribution over sequences of tokens, often words or characters, called a language model (LM) [12]. One of the most popular approaches to learning a language model is the left-to-right LM, whose objective is to predict the conditional probability of each token knowing the previous ones. Formally, given a sequence of N tokens w = {w 1 , . . . , w N }, the left-to-right language model computes for each token w i the conditional probability P (w i |w i−1 , . . . , w 1 ). The chain rule of probability states that the joint probability of the entire sequence is the product of all the conditional probabilities:
P (w 1 , . . . , w N ) = N i=1 P (w i |w i−1 , . . . , w 1 )(1)
Neural network language models typically minimize the crossentropy loss, which is equivalent to maximizing the joint probability of the sequence. In other words, such language models maximize the likelihood of the known behaviors. By definition, novel behaviors deviate from what has been previously observed and therefore have a low likelihood under a language model trained on known behaviors. Previous research studied log and trace language models for anomaly and novelty detection [2,13,14,15]. Our approach improves over and diverges from these existing approaches with three significant contributions: (1) the quality and quantity of the data are drastically improved, (2) neural networks that are able to learn extremely long dependencies are investigated, and (3) the novelty detection methodology takes into account the sequence length. We next expand on these three contributions.</p>
<p>First, deep learning approaches are known to greatly depend on the quality and quantity of the data [12]. Nonetheless, the public datasets considered by current research, such as UNM [16], KDD98 [17], and ADFA-LD [18], are small and obsolete, as explained by Creech and Hu [18] and Murtaza et al. [7]. Furthermore, these datasets lack the system call arguments, a valuable piece of information that has been shown to improve the performance of neural networks for language models [11]. As a result, these datasets are inadequate for effectively training large neural networks and are not representative of modern systems. As a solution, this paper introduces a massive dataset of kernel traces that includes system call arguments and comprises more than 2 million web requests from seven realistic scenarios, including misconfigurations and latencies. Notably, our dataset enables training larger neural networks, such as the Transformer, that have become state-of-the-art in other fields. The dataset has been made public, as well as the data collection methodology and the scripts for reproducibility.</p>
<p>Second, current anomaly and novelty detection approaches rely on recurrent neural networks (RNNs), most often the Long Short-Term Memory [19] (LSTM) network. However, recurrent networks are unable to efficiently model long-term dependencies due to their iterative nature. As explained by Khandelwal et al. [20], LSTM language models sharply distinguish recent positions but only vaguely remember the distant past. Dai et al. [21] estimated that the relative effective context length (RECL) of LSTMs on natural language is between 200 and 400 tokens, which is consistent with Khandelwal et al. [20] estimation. This inherent limitation of recurrent networks is analyzed in the case of kernel traces since they are typically much longer, comprising thousands of events. In particular, this paper investigates the state-of-the-art network for sequence processing called the Transformer [22], whose main advantage is the ability to model dependencies of arbitrary length. However, this flexibility comes at the expense of a quadratic complexity with respect to the sequence length. Consequently, a linear-complexity alternative called the Longformer [23] is also investigated.</p>
<p>Third, anomaly and novelty detections are typically performed with a top-k on the individual conditional probabilities [10,14,15] or a threshold on the joint probability of the sequence [2,13]. However, conditional probabilities are only able to detect deviations of single events, also known as point outliers, while the joint probability does not take into account the sequence length. As a solution, our methodology leverages the perplexity, a prevalent measure of how well a probability model predicts a sample [22,24].</p>
<p>The remainder of this paper is organized as follows. Section II surveys the related works. Section III introduces the proposed novelty detection methodology. Section IV presents the dataset collection methodology and analyzes the dataset. Section V details the experiments and reports the results of the proposed approach on the collected dataset. Section VI acknowledges internal and external validity threats. Section VII discusses the strengths and limitations of the proposed methodologies as well as suggests interesting future research avenues. Finally, Section VIII concludes this paper.</p>
<p>II. RELATED WORK</p>
<p>Let us preface the related works by discussing the distinction between anomaly and novelty. As defined in Section I, a novelty is any deviation from previously observed behaviors. Novelties include anomalies since they are typically unknown and unexpected, but not all novelties are anomalies. For instance, new users and rare queries are novel yet normal behaviors. The vast majority of the literature focuses on anomalies, and one of the most popular approaches is learning a "normal" behavior from the data and identifying any deviations from this behavior as abnormal [2,13,14,15,25]. We argue that these approaches would be better framed as novelty detection methods as an additional mechanism would be necessary to determine whether the novel behaviors are normal or abnormal. For that reason, even though this paper focuses on novelty detection, most of the approaches discussed in this section were published under the anomaly detection paradigm.</p>
<p>This section surveys the fundamental aspects of the relevant related works: the trace representation, the machine learning model, and the anomaly or novelty detection scheme.</p>
<p>A. Trace Representation</p>
<p>A trace usually comprises millions of low-level events, each containing multiple arguments, making them resourceintensive to handle. As a result, researchers have traded information for compactness in three ways: reducing the number of arguments, aggregating the events across time, and extracting higher-level features.</p>
<p>The first and foremost approach is to reduce the number of arguments. Current research often exclusively considers the event names and ignores the arguments, such as the process name and the return value. However, arguments are valuable data that allow the model to make more informed and, ultimately, more accurate predictions. Indeed, temporal information such as the response time [10,26], the timestamp [11], and the duration [14] has recently been considered with great success. Instead of reducing the number of arguments, Ezeme et al. [27] compressed the values of the arguments by encoding the characters using ASCII values and considering the frequency distribution of these values for each argument. Nonetheless, contemporary research demonstrated the benefit of considering the actual values of multiple system call arguments for neural language models [11].</p>
<p>The second approach is to aggregate the events across time. The main example of this approach is called bag-of-words, also known as system call counts vector [9], frequency counts of system call names [28] or bag of system calls [29]. A bag-of-words is a representation that describes the number of occurrences of each token within a document. For instance, consider a vocabulary V = {a, b, c} and a sequence w = {a, c, c, c, a, c}. The bag-of-word representation of this sequence is [2,0,4]. The aggregation trades the ordering and fine-grained temporal information for a more compact representation. However, temporal information may be critical to detecting some novelties, such as latency.</p>
<p>The third approach is to extract higher-level features from the trace, such as states of kernel modules [7] or execution states [8]. Although carefully hand-crafted higher-level features may deliver excellent performance, they discard the finegrained information that makes traces so valuable. Moreover, they are time-consuming, error-prone, and potentially suboptimal since they must often be hand-crafted specifically for the task considered.</p>
<p>B. Machine Learning Model</p>
<p>Due to the widespread use of computer systems and the importance of detecting anomalies and novel behaviors, a wide range of machine learning techniques have been explored, including rule-based algorithms [4,30], naive Bayes [6,5], decision trees [6,31], hidden Markov models [6,32], and support vector machines (SVM) [6]. Given the great success of deep learning, researchers have recently shifted toward a family of neural networks called recurrent neural networks (RNNs) [33].</p>
<p>Recurrent neural networks iteratively process variable-size sequences by sharing the parameters at each position. They have been successfully applied to a wide range of applications, including speech recognition [34,35], image captioning [36], machine translation [37], and anomaly detection [2,14]. RNNs have the advantage of iteratively storing information in their memory, also referred to as hidden representation, allowing information from prior input tokens to influence the current output. However, RNNs suffer from vanishing and exploding gradient issues [38]. As a solution, Hochreiter and Schmidhuber [19] introduced the now widely popular long short-term memory (LSTM) network, which mitigates these shortcomings with paths through time. Alternatively, Cho et al. [39] introduced the gated recurrent unit (GRU), which behaves and performs similarly to while requiring fewer parameters.</p>
<p>The LSTM and GRU have been at the core of numerous anomaly and novelty detection approaches. For instance, Kim et al. [2] detected host-based intrusions with an ensemble of LSTMs trained on sequences of system call names. Dymshits et al. [9] identified changes in the behavior of processes with unidirectional and bidirectional LSTMs trained on sequences of bag-of-words. Song et al. [31] identified and explained anomalies with an LSTM trained on time-series data obtained from traces. Nedelkoski et al. [10] detected anomalies with a multimodal network made of the concatenation of the hidden representations of two LSTMs trained on textual logs and realvalued response time. Lv et al. [40] detected intrusions by extending system-call sequences with a GRU.</p>
<p>Recurrent neural networks, including LSTMs, suffer from memory compression [41] and are unable to model very long-term dependencies. In particular, LSTM language models only vaguely remember the distant past [20] and are unable to model dependencies that span more than 200 to 400 tokens [20,21]. One solution is to augment the LSTM with an attention mechanism. Ezeme et al. [27] and Brown et al. [13] detected anomalies with LSTMs augmented with interattention. However, their inherently sequential nature prevents parallelizing them.</p>
<p>At the time of writing, LSTMs and GRUs have mostly been surpassed and replaced by the Transformer [22]. As illustrated by Figure 1, the Transformer is a simple network based solely on two attention mechanisms: the inter-attention and the self-attention. The latter computes a pairwise compatibility score between tokens corresponding to how much each token contributes to each output. As such, the self-attention replaces the role of recurrences in RNNs and enables modeling arbitrary length dependencies. Since this compatibility score is computed independently for each pair of tokens, the Transformer processes entire sequences simultaneously and can be efficiently parallelized. However, these major benefits come at the cost of quadratic complexity with respect to the sequence length. Consequently, a plethora of efficient alternatives have been proposed, such as the Longformer [23], the Linformer [42], and the Reformer [43].  Despite the clear advantages and successes of the Transformer, researchers have yet to investigate this architecture to detect novelties in traces. For additional information on the Transformers, we refer the reader to the many surveys that have been published [44,45,46,47].</p>
<p>C. Detection Scheme</p>
<p>Real-world datasets are often unlabeled regarding anomalies since they are typically unknown beforehand. Besides, manually labeling them afterward is generally time-consuming and error-prone. Consequently, the vast majority of approaches fall into the self-supervised or unsupervised setting. This paper focuses on language models such as the left-to-right LM that outputs the conditional probability of every possible token for each token in the sequence. In the literature, there are two distinct detection schemes based on the idea that novelties correspond to mispredictions.</p>
<p>The first scheme assumes that a misprediction occurs when the correct token does not appear in the top-k most likely predictions. Guo et al. [25] considered a sequence as anomalous if it contains more than a certain number of mispredictions, whereas Bogatinovski et al. [15] considered the ratio of mispredictions. Temporal information is decisive in detecting some novelties, such as latencies. Consequently, in addition to the token mispredictions, Du et al. [14] and Nedelkoski et al. [10] predicted the timestamp and the response time, respectively.</p>
<p>The second scheme considers that a misprediction occurs when the conditional or joint probability is lower than a given threshold. Notably, Kim et al. [2] and Brown et al. [13] detected anomalies with a threshold on the negative loglikelihood of the whole sequence.</p>
<p>III. METHODOLOGY TO DETECT NOVELTIES WITH</p>
<p>NEURAL LANGUAGE MODELS This section introduces the proposed methodology and follows the same structure as the literature review.</p>
<p>A. Trace Representation</p>
<p>Neural networks learn to extract the relevant features for a task and thus typically benefit from richer inputs. Accordingly, this paper follows the methodology proposed in [11] and relies on a joint representation of the system call name (sysname), the timestamp (timestamp), and five context fields that are added to all system calls by LTTng, namely the return value (ret), the process name (procname), the thread id (tid), the process id (pid), and whether the event corresponds to the start or the end of a system call execution (entry).</p>
<p>To determine how to represent the arguments, one must first identify the inherently meaningful ones -whose values convey meaning in themselves without any context. For explanatory purposes, let us consider a system call whose process name is mysql and whose process id is 15371. The process name indicates that a MySQL database emitted the call, while the process id does not provide knowledge but allows relating the events emitted by the same process in the context of the trace. Out of the considered arguments, the sysname, ret, entry, and procname are inherently meaningful, while the tid, pid, and timestamp are not 3 .</p>
<p>The semantic knowledge contained in the values of the inherently meaningful arguments is encapsulated in a compact vectorial representation called embedding. An embedding effectively acts as a lookup table and is defined by a dense matrix W ∈ R dv×de where d v is the size of the vocabulary and d e is a hyperparameter corresponding to the dimension of the embedding such that d e ≪ d v .</p>
<p>The values of the context-dependent arguments, such as the pid or tid, could be directly provided to the network as they are numerical values. However, it is best practice to normalize the input to mitigate potential numerical instabilities and speed up training [48]. As a result, the context-dependent arguments are encoded with a succession of cosine and sine functions, as proposed by Vaswani et al. [22]. Formally, the encoding of a numerical value x is a vector pe x of dimension d computed as pe x,i = sin
x × 10 −6i/d if i is even, otherwise pe x,i = cos x × 10 −6(i−1)/d) , where d is a hyperparameter.
In order to produce a joint representation of the system calls with their arguments and to provide a single input to the network, the embeddings and encodings must be combined. As mentioned in [11], the addition requires the vectors to have the same dimension and preserves that dimension, which may be too small to store all the information, thus creating a bottleneck. Consequently, the concatenation of the embeddings and encodings vectors is preferred.</p>
<p>Our methodology diverges from that of [11] and [22] in three aspects. First, the timestamps are converted into elapsed times between two consecutive system calls to avoid numerical instabilities as they exceed the largest value that can be stored on 32 bits. Second, the denominator of the encoding is increased from 10 4 to 10 6 , as the values encoded are larger than in the work of Vaswani et al. [22]. Finally, the embeddings and encodings are all concatenated since this empirically resulted in more effective models.</p>
<p>B. Neural Networks</p>
<p>The proposed methodology was evaluated on a simple n-gram baseline, the widespread LSTM, the state-of-the-art Transformer, and the lower-complexity Longformer. Let us briefly introduce and justify each method.</p>
<p>The n-gram model makes the Markov assumption that the conditional probability may be approximated by only considering the n − 1 tokens instead of all previous tokens. In other words, the n-gram model approximates the conditional probability P (w i |w i−1 , . . . , w 1 ) as P (w i |w i−1 , . . . , w i−(n−1) ), which is computed in practice as the number of times that
{w i−1 , . . . , w i−(n−1) } is followed by w i out of all the occur- rences of {w i−1 , . . . , w i−(n−1) } in the dataset.
Following the vast majority of literature, the proposed methodology was evaluated using a unidirectional multi-layer LSTM. Since this architecture is well known and ubiquitous in the literature, the reader is referred to the original paper by Hochreiter and Schmidhuber [19] and the reference book of Goodfellow et al. [12] for a comprehensive description and analysis of the model.</p>
<p>As discussed in the introduction, kernel traces are typically much longer than the effective context length of LSTMs. As such, they may contain dependencies that the LSTM is unable to model. In order to investigate this potential limitation, the proposed methodology was evaluated on a vanilla Transformer.</p>
<p>The quadratic complexity of the Transformer means that the model cannot handle entire kernel traces in practice, even with multiple GPUs and mixed precision. Consequently, sequences were truncated, and the joint probability was estimated. In order to determine whether truncating sequences is a potential issue for kernel traces and to propose a solution that is more easily deployable in practice, a lower-complexity Transformer called the Longformer [23] was selected based on the attention patterns learned by the Transformer.</p>
<p>The Longformer achieves a linear complexity by replacing the self-attention mechanism with a combination of two sparse attention mechanisms called global tokens and sliding windows. Since the objective of the left-to-right language model is to predict the next token given the previous ones, future tokens are masked to prevent the model from looking ahead at the solution. Instead of looking at all previous tokens, the sliding window attention only considers the past k tokens, similar to the n-gram model. Since only a fixed number of positions are considered for each token, the complexity of the window attention is linear with respect to the sequence length. Additionally, the global tokens are able to attend to every position and be attended by every position. Given that there is a fixed number of global tokens, each considering every token in the sequence, the complexity of global tokens is also linear with respect to the sequence length. Figure </p>
<p>C. Novelty Detection Scheme</p>
<p>The neural networks are trained with the left-to-right language model, whose task is to predict the next token given the previous ones. Formally, given an input sequence w = {w 1 , . . . , w N } comprising N tokens from a vocabulary V, a neural left-to-right language model outputs the conditional probability P (w * |w i−1 , . . . , w 1 ) for each w * in the vocabulary and for each position i = 1, . . . , N , such that w * ∈V P (w * |w i−1 , . . . , w 1 ) = 1. The joint probability P (w 1 , . . . , w N ) is given by the chain rule of probability as P (w 1 , . . . , w N ) = N i=1 P (w i |w i−1 , . . . , w 1 ). However, since each conditional probability is lower than 1 in practice, longer sequences tend to have a lower joint probability. Let us consider an operating system that produces 200 system calls per second and two requests of 80 ms and 120 ms. Let us assume that the variation in response time is normal and that the events are all equally likely, with a conditional probability of 95%. The two requests comprise 200 × 0.08 = 16 and 200 × 0.12 = 24 system calls, respectively. Consequently, their likelihood is 0.95 16 = 44% and 0.95 24 = 29%, respectively. As a result, the likelihood of sequences is not well suited for novelty detection, as the throughput of system calls is high, and the sequence lengths may greatly vary.</p>
<p>The perplexity of a language model is a widely popular metric [22,24] that measures its degree of uncertainty when a new token is generated, averaged over very long sequences. The per-word entropy H of a sequence of word w = {w 1 , . . . , w N } generated by a language model is:
H = lim N →∞ − 1 N w P (w) log 2 P (w)(2)
Assuming ergodicity and given a large enough value of N , the summation may be discarded, and the entropy can be approximated as:Ĥ
= − 1 N log 2 P (w)(3)
Finally, the perplexity is defined as:
P P = 2Ĥ = P (w) −1/N(4)
where N is the sequence length. In the above example, the perplexities of both requests are equal to 0.95 − 16 16 = 0.95 − 24 24 = 1.05. A sequence with a higher perplexity than the sequences in the training set is less likely under the model and can therefore be detected as a novelty. In practice, a simple threshold is efficient and provides excellent results. The threshold is empirically determined for each novel behavior with the indistribution and out-of-distribution datasets to maximize the F-score.</p>
<p>IV. DATA COLLECTION</p>
<p>Real-world traces are seldom released due to security and privacy concerns. Consequently, researchers often rely on the UNM [16] and KDD98 [17] datasets. Nonetheless, these two datasets are over twenty years old and thus fail to represent modern systems [18,7]. As a solution, Creech and Hu [18] introduced ADFA-LD for host-based intrusion detection. However, ADFA-LD comprises only a few thousand samples without the system call arguments, which is too small for training large neural networks. Alternatively, Murtaza et al. [7] introduced the much larger FirefoxDS dataset. Unfortunately, FirefoxDS is no longer available at the time of writing.</p>
<p>Neural networks greatly benefit from scaling as revealed by the current race toward ever-larger models [49,50] and large networks greatly benefit from massive datasets [51]. To the best of our knowledge, no modern and massive datasets of kernel traces are publicly available. This paper addresses this limitation by introducing a novel open-source dataset of kernel traces comprising over 2 million web requests with seven distinct behaviors. The dataset includes all the system calls arguments, and requests are well delimited by userspace events and labeled according to their behavior.</p>
<p>The remainder of this section explains the data collection methodology in detail and analyzes the collected dataset.</p>
<p>A. Methodology</p>
<p>Similar to the methodology of [11], a benchmark tool sends numerous concurrent requests from the client to the server via the hypertext transfer protocol (HTTP). A web server receives the requests and calls PHP to query an SQL database and create the requested dynamic web page. The simple clientserver architecture is depicted in Figure 3.</p>
<p>On the client side, the requests were emitted with the wrk2 benchmark tool, an open-source and multithreaded alternative to the Apache benchmark that guarantees a stable throughput for sufficiently long execution times. On the server side, the requests were handled with Apache2, a web server made popular thanks to its modular design. Apache2 was manually instrumented with two userspace events httpd:enter_event_handler and httpd:exit_event_handler to delimit requests. The requested dynamic web pages were created by querying MySQL with PHP installed as an Apache2 module. MySQL was chosen since it is an open-source relational database management system commonly used with Apache2. Finally, the database was filled with the Sakila sample database, as it is intended to provide a standard schema that can be used across numerous examples. Notably, this database comprises an author table with unique ids, first names, and last names.</p>
<p>As developers have limited access to the client side, this paper focuses on the server side, where most novelties originate. The kernel and userspace events were collected on the server with Linux Trace Toolkit: next generation (LTTng) [52] due to its lightweight and rapidity [53].</p>
<p>In order to identify novelties using a language model, the proposed methodology requires a set of known behaviors referred to as in-distribution (ID) and sets of novelties referred to as out-of-distribution (OOD). Accordingly, three ID sets were collected with a typical configuration under nominal load (train ID, validation ID, and test ID), and two OOD sets were collected for each of the following server-side novelties (validation OOD and test OOD). a) CPU: The CPU is overloaded using the stress-ng tool, which performs numerous matrix multiplications. This behavior simulates a compute-intensive process competing for resources with the web server, which may arise from a cryptocurrency-mining procedure deployed by an intruder. b) OPcache: The server is misconfigured by disabling PHP's OPcache, which stores precompiled script bytecode in memory to speed up the response time. This behavior may arise from a developer disabling the cache during the development phase and forgetting to re-enable it afterward.</p>
<p>c) Dump IO: The disk is overloaded by enabling the highest level of Apache2 log and storing them into a file using the dump io mod, which helps investigate the server's behavior but heavily uses storage resources and leads to a slower response time. Like OPcache, this behavior may arise from a developer enabling logging during development but forgetting to disable it afterward. d) Connection: Apache2 is configured to support 150 concurrent connections by default and will start dropping requests as the traffic increases if that number is not increased. Instead of increasing the traffic, this behavior was reproduced by reducing the number of concurrent connections to 25, as the server would be IO-bounded before requiring more connections. Additionally, this approach allows for keeping the traffic consistent with the other behaviors. e) Socket: The server is misconfigured by disabling Apache2 KeepAlive, which allows the web server and browsers to reuse the same socket for transferring multiple files, thereby reducing CPU usage at the cost of higher memory usage. By default, KeepAlive is enabled as CPU usage is often the main limiting factor. This behavior may arise when the server is redeployed from a memory-limited machine to a CPU-limited one.</p>
<p>f) SSL: The server's security is compromised by disabling the secure sockets layer (SSL), a protocol for establishing secure connections between web servers and browsers. This behavior may arise from a malicious intruder.</p>
<p>B. Dataset Analysis</p>
<p>The web server was deployed on an Ubuntu 21.04 machine with 16-core Intel E5640 (up to 2.67 GHz) and 192 GB of RAM, and the client sent 1000 requests per second to load the server properly. The web server was traced for 1,000s for the training set and 100s for all validation and test sets. The dataset is balanced, although positive samples are expected to be much rarer than negative ones in real-world applications. This decision is justified in the threat to validity section.</p>
<p>In this work, requests comprise all the system calls generated between their start and end as delimited by the userspace events httpd:enter_event_handler and httpd:exit_event_handler regardless of their thread ids. Consequently, system calls may be added to multiple requests since they are concurrent. The primary reason behind this decision is that we want requests to include the events associated with the root cause of the novelties. For instance, let us assume that an unexpected process is taking CPU time, thus creating latency. To detect the root cause of this behavior, the events generated by this abnormal process must be included in the request, even though they do not have the same thread id. The main drawback of this approach is that requests contain significantly more events, making them more resource-intensive to process and increasing the noise. Table I reports request statistics for each set after discarding the first second, corresponding to the initialization of LTTng. The lower number of requests for the CPU behavior is due to the server's inability to maintain the throughput due to the lack of CPU. Distributions of system call names, process names, length, and duration for each set are available on GitHub.</p>
<p>V. RESULTS</p>
<p>A. Language Models</p>
<p>The language models were trained on a server with 2 x Intel Gold 6148 Skylake @ 2.4 GHz, 4 x Nvidia V100SXM2 16G, and 64GB of RAM.</p>
<p>Neural networks were trained with mixed precision due to the simplicity of implementing NVIDIA's Automatic Mixed-Precision, which accelerates training and reduces memory consumption by storing and computing the weights, activations, and gradients in half-precision [54]. Furthermore, due to the quadratic complexity of the Transformer, they were trained with gradient checkpointing, which trades memory for computation by recomputing the activations during the backward pass instead of storing them in memory during the forward pass [55]. Additionally, the Transformer truncated the few sequences longer than 2048 system calls to avoid exceeding the GPUs memory.</p>
<p>The following hyperparameters were manually tuned in a greedy fashion for all networks: the depth and width of the models, the embedding size, the optimizer, the warmup steps, the label smoothing weight, the dropout probability, the number of updates without improvements before reducing the learning rate, and the number of updates before early stopping. Additionally, the number of heads, the SwiGLU activation function [56], and the T-fixup initialization [57] were considered for the Transformer and the Longformer. Furthermore, the window size, the dilation, and the number of global tokens were also considered for the latter. Overall, over 80 distinct network configurations were evaluated, each requiring about a day of computation on the aforementioned server. The exhaustive list of hyperparameters for each model is available on GitHub.</p>
<p>Table II reports the average cross-entropy and top-1 accuracy of the three networks on all sets. Note that the top-1 accuracy is defined as the proportion of accurately predicted tokens, that is, the number of times the most probable system call name corresponds to the actual one over the total number of predictions. Each experiment was reproduced five times with different seeds to mitigate the stochasticity. The crossentropy on the ID sets is consistently and significantly lower than on the OOD sets, indicating that the networks have a higher degree of uncertainty when modeling the novel behaviors. The LSTM outperforms the two attention-based networks in terms of cross-entropy and accuracy, although they have the advantage of learning arbitrary length dependencies. As expected, increasing the width and depth of the two attention-based models improved their performance. For instance, increasing the depth from 2 to 6 layers and the width from 672 to 896 allowed reducing the cross-entropy of the Transformer from 0.907 to 0.719 on the ID test set, outperforming the LSTM. However, although larger models were better at generalizing due to their higher flexibility, they performed poorly on our downstream novelty detection task since they assigned a high likelihood to all behaviors. Since our goal is to detect novelties, only the smaller models are reported in Table II and thereafter. Figure 4 shows the perplexity distribution of requests in the ID and OPcache validation sets computed with the LSTM. Although the length and duration distributions are similar, the network assigns a higher perplexity to OOD requests, indicating the ability to leverage complex interactions between  </p>
<p>B. Novelty Detection</p>
<p>Novelty detection is a binary classification problem since requests are either in-distribution or out-of-distribution. By convention, the outcomes of binary classification problems are referred to as positive and negative, with positive indicating the class of interest (i.e., novelties). Such problems are evaluated in terms of precision and recall, where the precision is the proportion of actual positives among the predicted positives, and the recall is the proportion of actual positives correctly predicted. Due to space constraints, the harmonic mean of the precision and recall, denoted F-score or F-measure, is instead reported. The classification is performed with a simple threshold on the perplexity, which acts as a novelty score, as described in Section-III-C. The threshold that maximizes the F-score is first empirically determined for each validation set, and the F-score is then computed for each test set with the corresponding threshold. Additionally, the area under the ROC curve (AuROC) is reported. The ROC curve evaluates the ratio of true positives against the ratio of false positives at different thresholds instead of selecting a threshold to optimize a specific metric. For more information on the ROC curve, please refer to Zou et al. [58]. Table III reports the AuROC and F-score of the 4-gram baseline and the three neural networks. The simple baseline could not detect the novel behaviors accurately, with the surprising exception of the IO behavior. The simplicity of detecting this behavior arises from the out-of-distribution requests having a wildly different distribution of system call names compared to the in-distribution request. Indeed, the two most common system calls in the training set are recvfrom (15%) and read (10%), while they are write (17%) and getpid (17%) for the IO dataset.</p>
<p>Due to the recent successes of the Transformer over the LSTM, one would have expected the former to outperform the latter. However, that is not the case: the LSTM performed on par or better than the attention-based networks in 3 out of 6 behaviors. Figure 5 illustrates the attention activation patterns of the Transformer on the in-distribution validation set. Interestingly, the dependencies modeled by the Transformer are mostly local since most of the attention learned is along the diagonal. This observation justifies the choice of the less complex Longformer, which relies on the window attention mechanism, thus assuming local dependencies. Furthermore, this observation explains the performance of the LSTM since RNNs are inherently biased toward local dependencies due to their iterative nature, making them well-suited for this use case. Nonetheless, in cases where longer-term dependencies must be modeled or a wide range of behaviors must be learned, we expect the Transformer or its lower-complexity alternative to perform better. The Longformer performed significantly worse than the LSTM and Transformer on the CPU behavior. The attention patterns learned by the Transformer on this behavior depicted in Figure 6 reveal that the Transformer learns dependencies that span further than the window size of the Longformer while remaining in the range of what LSTMs can model [20,21]. The Longformer has difficulty learning these dependencies with global tokens, which may be addressed by increasing their number or the window size. Delays from 1 microsecond (µs) to 1 millisecond (ms) were introduced randomly into an in-distribution sample to assess if the methodology can detect small latencies. The experiment involved first drawing a sample with N system calls from the in-distribution validation set and evaluating its perplexity as the baseline. Then, the sample is duplicated d×p times, where d is the number of delays and p is the number of positions considered, and each of the d delays is added to the duration of the system calls corresponding to each of the p positions, thereby allowing to compute of the average perplexity for a given delay. Figure 7 depicts the average perplexity as a function of the delay, which is always higher than the baseline and increases with the delay. Thus, the proposed methodology can detect small latencies, and its effectiveness increases as the delays increase. Additional figures for other in-distribution samples and language models are available on GitHub. </p>
<p>VI. THREATS TO VALIDITY</p>
<p>This section acknowledges the potential threats to internal and external validity. Internal validity relates to the soundness of the evaluation methodology and its ability to draw conclusions from the results, while external validity relates to the ability to generalize the approach to other use cases.</p>
<p>A. Threats to Internal Validity</p>
<p>Two threats arise from evaluating a novelty detection methodology on datasets collected for that purpose. First, indistribution and out-of-distribution sets may have unforeseen differences that facilitated the detection of novel behaviors, thus overestimating the methodology's performance. Second, in-distribution validation and test sets may contain behaviors not present in the training set, which can underestimate the methodology's performance. These threats are due to the high cost of manually investigating the dataset. They have been mitigated by collecting the datasets in a controlled environment with dedicated machines and comparing them with simple metrics, as shown in Table I. The datasets and scripts have been publicly released for further investigation.</p>
<p>Another threat arises from the balanced nature of the validation and test sets, as novelties are rare in practice, and the number of positive samples is expected to be smaller than that of negative samples. Since the ratio of novelties depends on the use case, we leave it to researchers to sample positive instances based on their use case, and the harmonic mean of the precision and recall is reported.</p>
<p>The last threat arises from manually tuning the networks instead of conducting a grid or random search [59], as it may lead to suboptimal hyperparameters. This approach is chosen to reduce the computational cost, thereby increasing reproducibility and minimizing the environmental impact. Despite the limited manual tuning, all the evaluated neural networks performed exceptionally well. As a mitigation, the code has been made publicly available and easily reproducible for researchers and practitioners to explore alternative architectures and hyperparameters.</p>
<p>B. Threats to External Validity</p>
<p>The leading threat arises from the low diversity of the datasets that may not represent real-world use cases, thus overestimating the generalization of the methodology. This is due to the lack of modern and massive public kernel trace datasets. Nonetheless, LSTMs and Transformers achieved comparable language model accuracy on a dataset similar to ours and a real-world dataset collected by Ciena [11], indicating that the collected dataset is representative of some real-world use cases. However, these two datasets contain a single behavior and are thus unsuitable for evaluating our novelty detection methodology. As a mitigation, the use cases were designed to be realistic and of genuine interest. Additionally, the trained models have been made public for researchers and practitioners to evaluate on their private datasets.</p>
<p>VII. DISCUSSION</p>
<p>This section briefly discusses the strengths and acknowledges the limitations of the proposed methodology.</p>
<p>A. Strengths and Benefits</p>
<p>First, the approach is data-agnostic and novelty-agnostic. Indeed, language models are probability distributions over sequences of tokens, and while this work focuses on system calls as they reveal the system behavior without requiring manual instrumentation, tokens need not be limited to them. Furthermore, the detection only depends on the perplexity of the sequence under the language model; thus, any divergence from previously observed behavior will likely increase the perplexity, including hardware upgrades or failures, software updates, new users, rare queries, misconfigurations, latency, intrusions, and bugs. Second, the approach does not heavily depend on an expert after the data collection to label the data or extract high-level features, which are often error-prone and suboptimal. While neural network hyperparameters must be tuned to achieve the best performance, techniques such as random search [59] have been developed to automate this process.</p>
<p>Finally, the approach is suitable for detecting novel behaviors in real-time as all three networks process a batch of 16 sequences in under 100ms on a single V100 GPU.</p>
<p>B. Limitations and Shortcomings</p>
<p>Neural networks are often considered to be black boxes, and their interpretability remains an active research topic. While the proposed approach cannot justify the detection, the predicted conditional probability of individual system calls may indicate the location of the root cause of the novelty.</p>
<p>Large language models may occasionally leak exact training samples [60]. This potential privacy issue is not severe as one would need access to the model, only sequences of system call names without their arguments can be generated, and there is no indication of which of them are from the training data.</p>
<p>The Transformer is known to be sensitive to false negatives that are samples containing repeated strings [61]. A wellcrafted attack that repeats a sequence of probable events may avoid detection. However, such attacks could be easily identified using simple metrics like the number of system calls or the request duration.</p>
<p>Finally, neural networks are computationally expensive and thus produce a large amount of carbon dioxide. Strubell et al. [62] estimated that training a Transformer with neural architecture search emits up to 284,000 kg of CO2. In comparison, the average American emits 16,400 kg annually, and the average car emits about 57,200 kg during its lifetime (fuel included).</p>
<p>VIII. CONCLUSION</p>
<p>This paper introduces a new open-source dataset of kernel traces comprising over 2 million web requests with seven distinct behaviors and a new approach for detecting novelties based on language models. Surprisingly, the inductive bias of the LSTM toward local dependencies enabled more accurate novelty detection on 3 out of 6 behaviors compared to the more flexible Transformer and Longformer.</p>
<p>An essential direction is to continuously update the language models as the behavior of computer systems constantly evolves. Previous research has shown that language models can effectively learn from a small number of samples when trained at a sufficient scale Tsimpoukelli et al. [63]. However, scaling neural networks increases the computation and memory required, preventing the detection of novelty in real-time. One promising avenue to scale the networks without increasing the amount of computation is a mixture of experts [64], which involves training multiple networks called experts and a router that forwards the input to a fixed number of relevant experts.</p>
<p>Fig. 1 .
1The computational graph of the Transformer, which comprises an encoder and a decoder. The encoder first processes the entire input sequence and produces a representation of each token attended by the decoder to generate the output sequence in an autoregressive manner.</p>
<p>Fig. 2 .
2The connectivity matrices of the Transformer's full attention (left) and the Longformer's sparse attention (right). The window attention and global tokens are depicted in blue and green, respectively. The i-th output position attends to the j-th input position if, and only if, the cell (i, j) is colored. The diagonal is highlighted to ease the reading. The matrices are lower triangular as future positions are masked to prevent the model from looking ahead at the solution.</p>
<p>Fig. 3 .
3The client-server architecture.</p>
<p>Fig. 4 .
4Distribution of the perplexity of requests in the ID (blue) and OPcache (orange) validation sets with respect to the length (left) and duration (right).</p>
<p>Fig. 5 .
5Attention activation patterns of the two layers of the Transformer on the ID validation set. For the sake of readability, the attention activation patterns are shown for the first 1024 positions and are compensated by multiplying each row with the number of unmasked positions.</p>
<p>Fig. 6 .
6Attention activation patterns of the second layer of the Transformer (left) and Longformer (right) on the CPU validation set. The Transformer leverages positions that the Longformer masks. For the sake of readability, the attention activation patterns are compensated by multiplying each row with the number of unmasked positions.</p>
<p>Fig. 7 .
7The average perplexity of an in-distribution request as 100 delays are introduced at 100 random positions each. The shade indicates the standard deviation, and the red horizontal line indicates the perplexity of the request without delays.</p>
<p>TABLE I STATISTICS
ION THE REQUESTS IN EACH DATASET.Behavior 
Dataset 
Number of Requests 
Request Length 
Request Duration (ms) 
min 
mean 
max 
min 
mean 
max </p>
<p>ID </p>
<p>Train 
999,063 
238 
1105.7 ± 244.8 
4,645 
0.28 
1.68 ± 0.65 
53.61 
Validation 
99,058 
30 
1107.3 ± 244.9 
2,803 
0.03 
1.67 ± 0.59 
11.69 
Test 
99,065 
240 
1108.7 ± 247.1 
2,683 
0.91 
1.67 ± 0.61 
12.36 </p>
<p>Connection 
Validation 
99,016 
246 
1125.7 ± 243.0 
2,882 
0.94 
1.66 ± 0.60 
15.02 
Test 
99,019 
158 
1125.0 ± 243.3 
2,792 
0.27 
1.66 ± 0.60 
11.83 </p>
<p>CPU 
Validation 
57,616 
258 
1910.6 ± 607.6 
6,221 
1.31 
13.25 ± 6.06 
52.10 
Test 
56,191 
222 
1913.8 ± 596.0 
6,363 
0.51 
13.58 ± 5.81 
35.69 </p>
<p>IO 
Validation 
98,974 
350 
1827.7 ± 323.4 
6,155 
1.27 
2.13 ± 3.23 
349.33 
Test 
98,980 
392 
1821.1 ± 321.0 
6,967 
1.25 
2.10 ± 1.23 
103.69 </p>
<p>OPcache 
Validation 
99,069 
256 
1162.9 ± 244.2 
2,824 
0.99 
1.78 ± 0.60 
14.79 
Test 
99,057 
250 
1160.6 ± 245.9 
2,896 
0.96 
1.77 ± 0.60 
11.94 </p>
<p>Socket 
Validation 
99,074 
216 
2082.0 ± 362.5 
8,463 
0.83 
6.89 ± 0.73 
48.79 
Test 
99,084 
679 
2081.8 ± 355.7 
7,032 
3.63 
6.89 ± 0.64 
19.61 </p>
<p>SSL 
Validation 
99,072 
16 
1058.1 ± 229.1 
3,230 
0.04 
1.48 ± 0.36 
15.92 
Test 
99,067 
238 
1054.8 ± 230.2 
3,855 
0.80 
1.47 ± 0.38 
22.23 </p>
<p>TABLE II 
TRAINING PERFORMANCE OF THE NEURAL NETWORKS. </p>
<p>LSTM 
Transformer 
Longformer 
Dataset 
Cross-Entropy 
Accuracy 
Cross-Entropy 
Accuracy 
Cross-Entropy 
Accuracy 
Train 
0.714 ± 0.002 
0.764 ± 0.000 
0.891 ± 0.039 
0.701 ± 0.013 
0.875 ± 0.008 
0.712 ± 0.003 
Test ID 
0.720 ± 0.002 
0.762 ± 0.000 
0.907 ± 0.038 
0.696 ± 0.012 
0.885 ± 0.010 
0.708 ± 0.004 
Test Connection 
0.812 ± 0.017 
0.737 ± 0.006 
1.274 ± 0.103 
0.605 ± 0.025 
1.105 ± 0.018 
0.651 ± 0.006 
Test CPU 
0.961 ± 0.027 
0.736 ± 0.010 
1.155 ± 0.056 
0.685 ± 0.012 
0.940 ± 0.022 
0.744 ± 0.005 
Test IO 
2.287 ± 0.185 
0.366 ± 0.037 
2.993 ± 0.307 
0.232 ± 0.042 
2.082 ± 0.150 
0.391 ± 0.036 
Test OPcache 
1.127 ± 0.019 
0.669 ± 0.005 
1.302 ± 0.052 
0.607 ± 0.013 
1.254 ± 0.024 
0.630 ± 0.007 
Test Socket 
1.008 ± 0.033 
0.699 ± 0.007 
1.573 ± 0.138 
0.549 ± 0.029 
1.223 ± 0.033 
0.636 ± 0.012 
Test SSL 
0.906 ± 0.018 
0.716 ± 0.007 
1.495 ± 0.105 
0.550 ± 0.030 
1.245 ± 0.027 
0.619 ± 0.010 </p>
<p>system calls. Similar figures for all datasets and models are 
available on GitHub. </p>
<p>TABLE III NOVELTY
IIIDETECTION PERFORMANCE OF THE LANGUAGE MODELS.4-gram 
LSTM 
Transformer 
Longformer 
Dataset 
AuROC 
F-score 
AuROC 
F-score 
AuROC 
F-score 
AuROC 
F-score 
Test Connection 
51.5 
66.7 
79.9 ± 3.8 
74.8 ± 2.9 
97.8 ± 1.6 
94.3 ± 2.9 
93.6 ± 1.4 
87.6 ± 1.8 
Test CPU 
0.9 
53.2 
98.5 ± 0.6 
93.6 ± 2.1 
94.8 ± 1.8 
85.1 ± 3.4 
67.9 ± 7.7 
59.6 ± 4.1 
Test IO) 
98.6 
94.7 
100.0 ± 0.0 
100.0 ± 0.0 
100.0 ± 0.0 
100.0 ± 0.0 
100.0 ± 0.0 
100.0 ± 0.0 
Test OPcache 
65.2 
67.5 
99.7 ± 0.1 
98.3 ± 0.2 
99.1 ± 0.2 
96.7 ± 0.4 
98.9 ± 0.2 
96.2 ± 0.4 
Test Socket 
22.6 
66.7 
98.8 ± 0.6 
94.7 ± 2.3 
99.9 ± 0.1 
99.1 ± 0.6 
99.1 ± 0.4 
96.4 ± 1.3 
Test SSL 
50.5 
66.7 
91.9 ± 1.7 
85.4 ± 2.0 
99.7 ± 0.2 
98.5 ± 0.9 
98.4 ± 0.5 
94.5 ± 0.9 </p>
<p>https://github.com/qfournier/syscall novelty detection 2 https://zenodo.org/record/7378420
There are exceptions, such as pid 0 and 1, which are meaningful.
ACKNOWLEDGMENTSWe gratefully acknowledge the Natural Sciences and Engineering Research Council of Canada (NSERC), Prompt, Ericsson, Ciena, AMD, and EfficiOS for funding this project.
Measuring and characterizing system behavior using kernel-level event logging. K Yaghmour, M R Dagenais, USENIX ATC. USENIX Association. K. Yaghmour and M. R. Dagenais, "Measuring and characterizing system behavior using kernel-level event logging," in USENIX ATC. USENIX Association, Jun. 2000.</p>
<p>Lstmbased system-call language modeling and robust ensemble method for designing host-based intrusion detection systems. G Kim, H Yi, J Lee, Y Paek, S Yoon, G. Kim, H. Yi, J. Lee, Y. Paek, and S. Yoon, "Lstm- based system-call language modeling and robust ensem- ble method for designing host-based intrusion detection systems," 2016.</p>
<p>Wait analysis of distributed systems using kernel tracing. F Giraldeau, M Dagenais, IEEE TPDS. 278F. Giraldeau and M. Dagenais, "Wait analysis of dis- tributed systems using kernel tracing," IEEE TPDS, vol. 27, no. 8, pp. 2450-2461, Aug. 2016.</p>
<p>Learning useful system call attributes for anomaly detection. G Tandon, P K Chan, FLAIRS. AAAI PressG. Tandon and P. K. Chan, "Learning useful system call attributes for anomaly detection," in FLAIRS. AAAI Press, 2005, pp. 405-411.</p>
<p>A machine learning approach for linux malware detection. K Asmitha, P Vinod, ICICTK. Asmitha and P. Vinod, "A machine learning approach for linux malware detection," ICICT, pp. 825-830, 2014.</p>
<p>On the comparison of user space and kernel space traces in identification of software anomalies. S S Murtaza, A Sultana, A Hamou-Lhadj, M Couture, CSMR. S. S. Murtaza, A. Sultana, A. Hamou-Lhadj, and M. Cou- ture, "On the comparison of user space and kernel space traces in identification of software anomalies," in CSMR, 2012, pp. 127-136.</p>
<p>A host-based anomaly detection approach by representing system calls as states of kernel modules. S S Murtaza, W Khreich, A Hamou-Lhadj, M Couture, ISSRE. S. S. Murtaza, W. Khreich, A. Hamou-Lhadj, and M. Couture, "A host-based anomaly detection approach by representing system calls as states of kernel modules," in ISSRE, 2013, pp. 431-440.</p>
<p>Automatic cause detection of performance problems in web applications. Q Fournier, N Ezzati-Jivan, D Aloise, M R Dagenais, ISSREWQ. Fournier, N. Ezzati-jivan, D. Aloise, and M. R. Dagenais, "Automatic cause detection of performance problems in web applications," in ISSREW, 2019, pp. 398-405.</p>
<p>Process monitoring on sequences of system call count vectors. M Dymshits, B Myara, D Tolpin, M. Dymshits, B. Myara, and D. Tolpin, "Process moni- toring on sequences of system call count vectors," 2017.</p>
<p>Anomaly detection from system tracing data using multimodal deep learning. S Nedelkoski, J Cardoso, O Kao, CLOUDS. Nedelkoski, J. Cardoso, and O. Kao, "Anomaly de- tection from system tracing data using multimodal deep learning," in CLOUD, Jul. 2019, pp. 179-186.</p>
<p>On improving deep learning trace analysis with system call arguments. Q Fournier, D Aloise, S V Azhari, F Tetreault, MSR. Q. Fournier, D. Aloise, S. V. Azhari, and F. Tetreault, "On improving deep learning trace analysis with system call arguments," in MSR, 2021, pp. 120-130.</p>
<p>Deep Learning. I Goodfellow, Y Bengio, A Courville, MIT PressI. Goodfellow, Y. Bengio, and A. Courville, Deep Learn- ing. MIT Press, 2016.</p>
<p>Recurrent neural network attention mechanisms for interpretable system log anomaly detection. A Brown, A Tuor, B Hutchinson, N Nichols, MLCS. ACMA. Brown, A. Tuor, B. Hutchinson, and N. Nichols, "Recurrent neural network attention mechanisms for in- terpretable system log anomaly detection," in MLCS. ACM, 2018.</p>
<p>Deeplog: Anomaly detection and diagnosis from system logs through deep learning. M Du, F Li, G Zheng, V Srikumar, CCS. ACMM. Du, F. Li, G. Zheng, and V. Srikumar, "Deeplog: Anomaly detection and diagnosis from system logs through deep learning," in CCS. ACM, 2017, p. 1285-1298.</p>
<p>Self-supervised anomaly detection from distributed traces. J Bogatinovski, S Nedelkoski, J Cardoso, O Kao, UCC. J. Bogatinovski, S. Nedelkoski, J. Cardoso, and O. Kao, "Self-supervised anomaly detection from distributed traces," in UCC, 2020, pp. 342-347.</p>
<p>A sense of self for unix processes. S Forrest, S Hofmeyr, A Somayaji, T Longstaff, Proceedings 1996 IEEE Symposium on Security and Privacy. 1996 IEEE Symposium on Security and PrivacyS. Forrest, S. Hofmeyr, A. Somayaji, and T. Longstaff, "A sense of self for unix processes," in Proceedings 1996 IEEE Symposium on Security and Privacy, 1996, pp. 120-128.</p>
<p>Evaluating intrusion detection systems: the 1998 darpa off-line intrusion detection evaluation. R Lippmann, D Fried, I Graf, J Haines, K Kendall, DISCEX. 2R. Lippmann, D. Fried, I. Graf, J. Haines, K. Kendall et al., "Evaluating intrusion detection systems: the 1998 darpa off-line intrusion detection evaluation," in DISCEX, vol. 2, 2000, pp. 12-26 vol.2.</p>
<p>Generation of a new ids test dataset: Time to retire the kdd collection. G Creech, J Hu, WCNC. G. Creech and J. Hu, "Generation of a new ids test dataset: Time to retire the kdd collection," in WCNC, 2013, pp. 4487-4492.</p>
<p>Long short-term memory. S Hochreiter, J Schmidhuber, Neural Computation. 98S. Hochreiter and J. Schmidhuber, "Long short-term memory," Neural Computation, vol. 9, no. 8, pp. 1735- 1780, 1997.</p>
<p>Sharp nearby, fuzzy far away: How neural language models use context. U Khandelwal, H He, P Qi, D Jurafsky, ACL. ACL. U. Khandelwal, H. He, P. Qi, and D. Jurafsky, "Sharp nearby, fuzzy far away: How neural language models use context," in ACL. ACL, Jul. 2018, pp. 284-294.</p>
<p>Transformer-XL: Attentive language models beyond a fixed-length context. Z Dai, Z Yang, Y Yang, J Carbonell, Q Le, ACL. ACL. Z. Dai, Z. Yang, Y. Yang, J. Carbonell, Q. Le et al., "Transformer-XL: Attentive language models beyond a fixed-length context," in ACL. ACL, Jul. 2019, pp. 2978-2988.</p>
<p>Attention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, NIPS. Curran Associates, IncA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones et al., "Attention is all you need," in NIPS. Curran Associates, Inc., 2017, pp. 5998-6008.</p>
<p>Longformer: The long-document transformer. I Beltagy, M E Peters, A Cohan, I. Beltagy, M. E. Peters, and A. Cohan, "Longformer: The long-document transformer," 2020.</p>
<p>BERT: Pre-training of deep bidirectional transformers for language understanding. J Devlin, M.-W Chang, K Lee, K Toutanova, ACL. ACL. J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, "BERT: Pre-training of deep bidirectional transformers for language understanding," in ACL. ACL, Jun. 2019, pp. 4171-4186.</p>
<p>Logbert: Log anomaly detection via bert. H Guo, S Yuan, X Wu, H. Guo, S. Yuan, and X. Wu, "Logbert: Log anomaly detection via bert," 2021.</p>
<p>Anomaly detection and classification using distributed tracing and deep learning. S Nedelkoski, J Cardoso, O Kao, CCGRID. S. Nedelkoski, J. Cardoso, and O. Kao, "Anomaly detec- tion and classification using distributed tracing and deep learning," in CCGRID, 2019, pp. 241-250.</p>
<p>A framework for anomaly detection in time-driven and event-driven processes using kernel traces. O M Ezeme, Q Mahmoud, A Azim, TKDE. 1O. M. Ezeme, Q. Mahmoud, and A. Azim, "A framework for anomaly detection in time-driven and event-driven processes using kernel traces," TKDE, p. 1, 2020.</p>
<p>A comparison of system call feature representations for insider threat detection. A Liu, C Martin, T Hetherington, S Matzner, IAW. A. Liu, C. Martin, T. Hetherington, and S. Matzner, "A comparison of system call feature representations for insider threat detection," in IAW, Jun. 2005, pp. 340-347.</p>
<p>Learning classifiers for misuse and anomaly detection using a bag of system calls representation. Dae-Ki Kang, D Fuller, V Honavar, IAW. Dae-Ki Kang, D. Fuller, and V. Honavar, "Learning classifiers for misuse and anomaly detection using a bag of system calls representation," in IAW, Jun. 2005, pp. 118-125.</p>
<p>On the learning of system call attributes for host-based anomaly detection. G Tandon, P K Chan, IJAIT. 1506G. Tandon and P. K. Chan, "On the learning of system call attributes for host-based anomaly detection," IJAIT, vol. 15, no. 06, pp. 875-892, 2006.</p>
<p>Exad: A system for explainable anomaly detection on big data traces. F Song, A Stiegler, Y Diao, J Read, A Bifet, ICDMWF. Song, A. Stiegler, Y. Diao, J. Read, and A. Bifet, "Exad: A system for explainable anomaly detection on big data traces," in ICDMW, Nov. 2018.</p>
<p>A multimodule anomaly detection scheme based on system call prediction. Z Xu, X Yu, Y Feng, J Hu, Z Tari, ICIEA. Z. Xu, X. Yu, Y. Feng, J. Hu, Z. Tari et al., "A multi- module anomaly detection scheme based on system call prediction," in ICIEA, 2013, pp. 1376-1381.</p>
<p>Schemata and Sequential Thought Processes in PDP Models. D E Rumelhart, P Smolensky, J L Mcclelland, G E Hinton, MIT PressD. E. Rumelhart, P. Smolensky, J. L. McClelland, and G. E. Hinton, Schemata and Sequential Thought Pro- cesses in PDP Models. MIT Press, 1986, p. 7-57.</p>
<p>Speech recognition with deep recurrent neural networks. A Graves, A Mohamed, G Hinton, ICASSP. A. Graves, A.-r. Mohamed, and G. Hinton, "Speech recognition with deep recurrent neural networks," in ICASSP, 2013, pp. 6645-6649.</p>
<p>Towards end-to-end speech recognition with recurrent neural networks. A Graves, N Jaitly, PMLRPMLR32A. Graves and N. Jaitly, "Towards end-to-end speech recognition with recurrent neural networks," in PMLR, vol. 32. PMLR, 22-24 Jun 2014, pp. 1764-1772.</p>
<p>Multimodal neural language models. R Kiros, R Salakhutdinov, R Zemel, PMLRPMLR32R. Kiros, R. Salakhutdinov, and R. Zemel, "Multimodal neural language models," in PMLR, vol. 32. PMLR, Jun. 2014, pp. 595-603.</p>
<p>Sequence to sequence learning with neural networks. I Sutskever, O Vinyals, Q V Le, NIPS. Curran Associates, IncI. Sutskever, O. Vinyals, and Q. V. Le, "Sequence to se- quence learning with neural networks," in NIPS. Curran Associates, Inc., 2014, pp. 3104-3112.</p>
<p>The problem of learning long-term dependencies in recurrent networks. Y Bengio, P Frasconi, P Simard, ICNN. 3Y. Bengio, P. Frasconi, and P. Simard, "The problem of learning long-term dependencies in recurrent networks," in ICNN, 1993, pp. 1183-1188 vol.3.</p>
<p>Learning phrase representations using RNN encoder-decoder for statistical machine translation. K Cho, B Van Merriënboer, C Gulcehre, D Bahdanau, F Bougares, EMNLP. ACL. K. Cho, B. van Merriënboer, C. Gulcehre, D. Bahdanau, F. Bougares et al., "Learning phrase representations using RNN encoder-decoder for statistical machine transla- tion," in EMNLP. ACL, Oct. 2014, pp. 1724-1734.</p>
<p>Intrusion prediction with system-call sequence-to-sequence model. S Lv, J Wang, Y Yang, J Liu, S. Lv, J. Wang, Y. Yang, and J. Liu, "Intrusion prediction with system-call sequence-to-sequence model," 2018.</p>
<p>Long short-term memory-networks for machine reading. J Cheng, L Dong, M Lapata, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. ACL. the 2016 Conference on Empirical Methods in Natural Language Processing. ACLJ. Cheng, L. Dong, and M. Lapata, "Long short-term memory-networks for machine reading," in Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. ACL, Nov. 2016, pp. 551-561.</p>
<p>Linformer: Self-attention with linear complexity. S Wang, B Z Li, M Khabsa, H Fang, H Ma, S. Wang, B. Z. Li, M. Khabsa, H. Fang, and H. Ma, "Linformer: Self-attention with linear complexity," Jun. 2020.</p>
<p>Reformer: The efficient transformer. N Kitaev, L Kaiser, A Levskaya, ICLR. N. Kitaev, L. Kaiser, and A. Levskaya, "Reformer: The efficient transformer," in ICLR, 2020.</p>
<p>Attention in natural language processing. A Galassi, M Lippi, P Torroni, TNNLS. A. Galassi, M. Lippi, and P. Torroni, "Attention in natural language processing," TNNLS, p. 1-18, 2020.</p>
<p>Attention? attention!. L Weng, L. Weng, "Attention? attention!" 2018. [Online].</p>
<p>Available: lilianweng.github.io/lil-log. Available: lilianweng.github.io/lil-log</p>
<p>Efficient transformers: A survey. Y Tay, M Dehghani, D Bahri, D Metzler, Y. Tay, M. Dehghani, D. Bahri, and D. Metzler, "Efficient transformers: A survey," 2020.</p>
<p>A practical survey on faster and lighter transformers. Q Fournier, G M Caron, D Aloise, Q. Fournier, G. M. Caron, and D. Aloise, "A practical survey on faster and lighter transformers," 2021.</p>
<p>. Y A Lecun, L Bottou, G B Orr, K.-R Müller, Efficient Backprop, SpringerBerlin HeidelbergY. A. LeCun, L. Bottou, G. B. Orr, and K.-R. Müller, Efficient BackProp. Springer Berlin Heidelberg, 2012, pp. 9-48.</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, NIPS. Curran Associates, Inc33T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan et al., "Language models are few-shot learners," in NIPS, vol. 33. Curran Associates, Inc., 2020, pp. 1877-1901.</p>
<p>Lamda: Language models for dialog applications. R Thoppilan, D Freitas, J Hall, N Shazeer, A Kulshreshtha, 2022R. Thoppilan, D. De Freitas, J. Hall, N. Shazeer, A. Kul- shreshtha et al., "Lamda: Language models for dialog applications," 2022.</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. C Raffel, N Shazeer, A Roberts, K Lee, S Narang, JMLR. 21140C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang et al., "Exploring the limits of transfer learning with a unified text-to-text transformer," JMLR, vol. 21, no. 140, pp. 1-67, 2020.</p>
<p>The lttng tracer: A low impact performance and behavior monitor for gnu/linux. M Desnoyers, M R Dagenais, OLS. M. Desnoyers and M. R. Dagenais, "The lttng tracer: A low impact performance and behavior monitor for gnu/linux," in OLS, vol. 2006, 2006, pp. 209-224.</p>
<p>Survey and analysis of kernel and userspace tracers on linux: Design, implementation, and overhead. M Gebai, M R Dagenais, ACM Computing Survey. 512M. Gebai and M. R. Dagenais, "Survey and analysis of kernel and userspace tracers on linux: Design, implemen- tation, and overhead," ACM Computing Survey, vol. 51, no. 2, Mar. 2018.</p>
<p>Mixed precision training. P Micikevicius, S Narang, J Alben, G Diamos, E Elsen, in ICLR. P. Micikevicius, S. Narang, J. Alben, G. Diamos, E. Elsen et al., "Mixed precision training," in ICLR, 2018.</p>
<p>Training deep nets with sublinear memory cost. T Chen, B Xu, C Zhang, C Guestrin, T. Chen, B. Xu, C. Zhang, and C. Guestrin, "Training deep nets with sublinear memory cost," 2016.</p>
<p>Glu variants improve transformer. N Shazeer, N. Shazeer, "Glu variants improve transformer," 2020.</p>
<p>Improving transformer optimization through better initialization. X S Huang, F Pérez, J Ba, M Volkovs, ICML. PMLR119X. S. Huang, F. Pérez, J. Ba, and M. Volkovs, "Improving transformer optimization through better initialization," in ICML, vol. 119. PMLR, 2020, pp. 4475-4483.</p>
<p>Receiveroperating characteristic analysis for evaluating diagnostic tests and predictive models. K H Zou, A J O&apos;malley, L Mauri, Circulation. 1155K. H. Zou, A. J. O'Malley, and L. Mauri, "Receiver- operating characteristic analysis for evaluating diagnostic tests and predictive models," Circulation, vol. 115, no. 5, pp. 654-657, 2007.</p>
<p>Random search for hyperparameter optimization. J Bergstra, Y Bengio, JMLR. 13J. Bergstra and Y. Bengio, "Random search for hyper- parameter optimization." JMLR, vol. 13, pp. 281-305, 2012.</p>
<p>Extracting training data from large language models. N Carlini, F Tramer, E Wallace, M Jagielski, A Herbert-Voss, N. Carlini, F. Tramer, E. Wallace, M. Jagielski, A. Herbert-Voss et al., "Extracting training data from large language models," 2020.</p>
<p>The curious case of neural text degeneration. A Holtzman, J Buys, L Du, M Forbes, Y Choi, A. Holtzman, J. Buys, L. Du, M. Forbes, and Y. Choi, "The curious case of neural text degeneration," 2019.</p>
<p>Energy and policy considerations for modern deep learning research. E Strubell, A Ganesh, A Mccallum, AAAI. 3409E. Strubell, A. Ganesh, and A. McCallum, "Energy and policy considerations for modern deep learning research," AAAI, vol. 34, no. 09, pp. 13 693-13 696, Apr. 2020.</p>
<p>Multimodal few-shot learning with frozen language models. M Tsimpoukelli, J Menick, S Cabi, S Eslami, O Vinyals, NIPS. M. Tsimpoukelli, J. Menick, S. Cabi, S. Eslami, O. Vinyals et al., "Multimodal few-shot learning with frozen language models," NIPS, 2021.</p>
<p>Adaptive mixtures of local experts. R A Jacobs, M I Jordan, S J Nowlan, G E Hinton, Neural Computation. 3R. A. Jacobs, M. I. Jordan, S. J. Nowlan, and G. E. Hinton, "Adaptive mixtures of local experts," Neural Computation, vol. 3, pp. 79-87, 1991.</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2245 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2245</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2245</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-61.html">extraction-schema-61</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <p><strong>Paper ID:</strong> paper-276226124</p>
                <p><strong>Paper Title:</strong> MTL-DoHTA: Multi-Task Learning-Based DNS over HTTPS Traffic Analysis for Enhanced Network Security</p>
                <p><strong>Paper Abstract:</strong> The adoption of DNS over HTTPS (DoH) has significantly enhanced user privacy and security by encrypting DNS queries. However, it also presents new challenges for detecting malicious activities, such as DNS tunneling, within encrypted traffic. In this study, we propose MTL-DoHTA, a multi-task learning-based framework designed to analyze DoH traffic and classify it into three tasks: (1) DoH vs. non-DoH traffic, (2) benign vs. malicious DoH traffic, and (3) the identification of DNS tunneling tools (e.g., dns2tcp, dnscat2, iodine). Leveraging statistical features derived from network traffic and a 2D-CNN architecture enhanced with GradNorm and attention mechanisms, MTL-DoHTA achieves a macro-averaging F1-score of 0.9905 on the CIRA-CIC-DoHBrw-2020 dataset. Furthermore, the model effectively handles class imbalance and mitigates overfitting using downsampling techniques while maintaining high classification performance. The proposed framework can serve as a reliable tool for monitoring and securing sensor-based network systems against sophisticated threats, while also demonstrating its potential to enhance multi-tasking capabilities in resource-constrained sensor environments.</p>
                <p><strong>Cost:</strong> 0.027</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2245.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2245.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MTL-DoHTA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-Task Learning-Based DNS over HTTPS Traffic Analysis (MTL-DoHTA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A compact multi-task 2D-CNN framework that combines a shared convolutional backbone with task-specific attention heads (static, feature-importance based) and dynamic GradNorm task-weighting to jointly solve: (1) DoH vs non-DoH, (2) benign vs malicious DoH, and (3) DNS tunneling tool identification.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MTL-DoHTA (2D-CNN + GradNorm + static task attention)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Input: 25 normalized flow features arranged as a 5×5 image into a 2D-CNN (three conv layers, GAP, FCs). Architecture uses a shared backbone and per-task attention layers where static attention weights (derived from Random Forest feature importance) are multiplied with shared representation; task losses are dynamically reweighted using GradNorm.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>105,546 parameters (~450 KB)</td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>Shared convolutional backbone + task-specific attention heads with static multiplicative weights derived from Random Forest feature importance; dynamic task-weight allocation using GradNorm (gradient-norm-based loss weighting).</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Multi-task network traffic classification (DNS over HTTPS detection and DNS tunneling tool identification) evaluated on CIC-DoHBrw-2020 and additional DoH datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td>Macro-averaging F1 = 0.9905 on CIC-DoHBrw-2020 when using 2D-CNN + GradNorm + static attention (feature importance); per-task (32-64-128 shared width): Task1 F1 = 0.9891, Task2 F1 = 0.9988, Task3 F1 = 0.9837.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td>Baseline 2D-CNN (shared-only, 32-64-128) macro F1 reported ≈ 0.9871–0.9881 (per-tables: Task1 0.9838, Task2 0.9988, Task3 0.9786).</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td>Model parameters = 105,546 (~450 KB). Average prediction time reported = 0.021379 s (averaged over 1000 repetitions on 231,822 flows using RTX 4090). No FLOPs reported.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td>Not separately quantified; baseline uses same backbone so parameter counts are similar; authors note small additional overhead for GradNorm and attention but do not provide FLOPs or memory deltas.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td>Robust to training-data reduction: 50% downsampling produced only a minor average F1 decrease (~0.003) relative to full dataset; pretraining/fine-tuning strategy used (pretrain on Dataset1 100 epochs, fine-tune on Dataset2 for 10 epochs) to improve adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td>Cross-dataset generalization: on Dataset1 (IEEE Dataport) average F1 = 0.9863 (Task1 0.9841, Task3 0.9907); on Dataset2 (CIC-DoHBrw-2020 + DoH-Tunnel-HKD combined) Task3 F1 = 0.9996 after 10-epoch fine-tuning, demonstrating strong adaptability to a novel tunneling technique.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td>Task-specific static attention weights are derived from Random Forest feature importance, providing explicit, interpretable feature weighting per task; authors cite interpretability as a design reason for static attention.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>MTL-DoHTA achieves high multi-task performance, matching or exceeding single-task baselines: Task2 was already strong in baseline, while Task1 and Task3 show noticeable gains when adding GradNorm + static attention; overall macro F1 improved to 0.9905.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td>Small model size (~450 KB) and modest inference time suggest suitability for embedded devices; authors note remaining challenges for real-time deployment due to reliance on pre-extracted features and non-trivial network compute and indicate future work on lightweight optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>A combination of shared representations with task-specific static attention (from RF feature importance) and dynamic GradNorm loss weighting yields better multi-task performance (macro F1 0.9905) and strong cross-dataset generalization while remaining compact.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>MTL-DoHTA shows that allocating task-specific representational emphasis (attention) and dynamically reallocating training pressure (GradNorm) improves multi-task accuracy and generalization compared to a uniform shared-only representation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2245.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2245.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GradNorm</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GradNorm: Gradient normalization for adaptive loss balancing</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An adaptive algorithm that balances multi-task learning by rescaling task loss weights using L2 norms of gradients in shared layers and relative inverse training rates so slower-learning tasks receive more emphasis.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gradnorm: Gradient normalization for adaptive loss balancing in deep multitask networks.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GradNorm (adaptive task-weighting algorithm)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Computes gradient L2 norms per task on a shared layer, computes relative inverse training rates r_i, and updates task weights w_i using a single hyperparameter α so that task gradient norms approach a common scale; incorporated in MTL-DoHTA to dynamically balance tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>Dynamic task-loss reweighting based on gradient-norm alignment (adjusts optimization resource allocation among tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Multi-task deep learning (used here for DoH multi-task traffic classification).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td>When added to 2D-CNN it improved multi-task performance compared to the baseline; authors report intermediate gains and that best results occur when GradNorm is combined with static attention (final macro F1 = 0.9905). Exact isolated numeric improvement due solely to GradNorm is reported as a modest increase in per-table comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td>Baseline without GradNorm had slightly lower macro F1 (~0.987–0.988) depending on configuration, per authors' comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td>Authors restrict gradient-norm computation to the last shared layer for efficiency and update GradNorm weights every 10 mini-batches; no FLOPs reported but noted as a modest overhead compared to baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td>Baseline lacks GradNorm overhead; no quantitative delta reported.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Helps balance learning rates across tasks and aids slower-learning tasks in catching up, improving overall multi-task performance when combined with attention.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td>Designed to be lightweight in this implementation by computing norms on the last shared layer only; no concrete resource numbers reported.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>GradNorm adaptively reallocates optimization focus among tasks, improving balance and contributing to higher multi-task performance in combination with task-specific attention.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>GradNorm is an explicit mechanism for dynamic resource allocation across tasks and in practice helped the MTL system achieve better balanced multi-task performance.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2245.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2245.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Static feature-importance attention</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Static task-specific attention weights derived from Random Forest feature importance</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A task-specific attention mechanism where per-feature weights are precomputed using Random Forest feature importance and applied multiplicatively to the shared representation as fixed attention during training and inference.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Static task-specific attention (feature-importance)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Task-specific fully-connected attention layers apply fixed (precomputed) attention weights, obtained from Random Forest feature importance per task, to the shared 64-d representation; attention weights are not learned during training.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>Static multiplicative attention gating per feature using precomputed feature-importance scores (no learned attention parameters).</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Multi-task DoH traffic classification</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td>When combined with 2D-CNN + GradNorm gave the best reported macro F1 = 0.9905; static feature-importance attention outperformed learned (dynamic) attention in the authors' experiments (see Table 8).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td>Improved over the uniform baseline 2D-CNN (baseline macro F1 ≈ 0.987–0.988) to final 0.9905 when used with GradNorm.</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td>Chosen for lower computational cost relative to learning attention weights; authors explicitly cite reduced compute as one rationale but do not provide FLOPs.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td>Provides human-interpretable per-feature importance per task because attention weights are directly derived from Random Forest importance scores.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Improved Task1 and Task3 notably versus baseline and versus learned attention in reported experiments; combined with GradNorm yields best multi-task results.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td>Authors argue static attention reduces computational overhead and is preferable for resource-constrained deployments.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>Precomputing and fixing task-specific attention from RF feature importance is both computationally cheaper and empirically more effective than learned attention for this DoH multi-task problem.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>Demonstrates that explicitly allocating representational emphasis per task (even statically) can improve performance and interpretability relative to uniform treatment.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2245.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2245.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>2D-CNN baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>2D Convolutional Neural Network baseline (shared-only)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline 2D-CNN with a shared convolutional backbone and per-task heads but without GradNorm dynamic reweighting or explicit task-specific attention; processes 5×5 images made from flow features.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>2D-CNN baseline (shared representation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Shared 2D-CNN (three conv layers 32/64/128 variants) with global average pooling and task heads; represents a uniform/shared representation baseline lacking task-specific representational allocations.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>Uniform shared representation across tasks; no task-gated or dynamically rebalanced components.</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Multi-task DoH traffic classification</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td>Macro F1 approximately 0.987–0.988 (for 32-64-128 configuration Task1 0.9838 Task2 0.9988 Task3 0.9786 as reported in tables).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td>Not separately reported; authors indicate it is slightly cheaper computationally than variants with learned attention/GradNorm but do not quantify.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Strong on Task2 but modestly weaker than MTL-DoHTA (with GradNorm + static attention) on Task1 and Task3.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>A uniform shared representation baseline attains high performance but is outperformed by architectures that introduce task-specific emphasis and dynamic task-weighting.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>Baseline demonstrates that uniform shared representations can be strong, but the observed gains from task-aligned mechanisms indicate advantages to task-specific abstraction.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2245.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2245.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Random Forest (for feature importance)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Random Forest (L. Breiman)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ensemble tree-based method used both as a high-performing single-task classifier in prior work and here to compute per-task feature importance that drives static attention weights.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Random forests.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Random Forest</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Ensemble of decision trees used here to compute feature importance per task (trained separately per task with 100 trees, default hyperparameters); top features common across tasks were selected to form the 25-feature input.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>Ensemble decision trees producing feature-importance scores; used as preprocessing to create task-specific static attention weights.</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Feature selection and single-task classification for network traffic</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td>Random Forest has been reported in prior work to achieve high F1 with 28 features (MontazeriShatoori et al.); in this paper RF was used to compute feature importances — exact RF classifier metrics within this paper are not enumerated.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td>Provides interpretable per-feature importance scores which the authors used directly to build static attention weights.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>Random Forest feature importances enabled a low-cost, interpretable task-specific attention mechanism that contributed to improved multi-task performance.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>By supplying explicit per-task feature importances, RF enabled task-aligned attention that improved downstream multi-task learning performance.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2245.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2245.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MFC-DoH (few-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DoH Tunnel Detection Based on the Fusion of MAML and F-CNN</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A few-shot meta-learning approach (MAML + F-CNN) for DoH tunnel detection cited for comparison; reported Task3 F1 = 0.9100 (few-shot 20) and has a much larger parameter count.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>DoH Tunnel Detection Based on the Fusion of MAML and F-CNN</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MFC-DoH (MAML + F-CNN few-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Meta-learning approach that uses MAML to enable fast adaptation of an F-CNN to novel DoH tunneling techniques with limited labeled examples (few-shot).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>1,147,904 parameters (reported in Table 10 of the paper)</td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>Meta-learning (MAML) enabling rapid per-task adaptation of network weights; few-shot learning enables task-conditioned representations.</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Few-shot DoH tunnel detection / DNS tunneling tool identification (Task3)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td>Reported Task3 F1 = 0.9100 (few-shot 20) as shown in the comparison table (Table 9) — authors state limited direct comparability to their approach.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td>Model is substantially larger (≈1.15M parameters) than MTL-DoHTA; no inference-time metrics reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td>Designed for few-shot sample efficiency (20-shot reported) but no direct sample-efficiency comparisons within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td>Large parameter count implies less suitable for constrained devices compared to compact MTL-DoHTA.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>Mentioned as an adaptive few-shot method that is more parameter-heavy and achieved lower Task3 F1 in the comparison table, illustrating trade-offs between adaptivity and compactness.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>MAML-based few-shot approaches exemplify adaptive, task-aligned representation learning intended to generalize quickly to new tasks, consistent with the Task-Aligned Abstraction Principle.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2245.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2245.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dynamic learned attention</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dynamic (learned) task-specific attention weights</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Learned attention weights for each task that are optimized end-to-end via backpropagation; compared experimentally to static, RF-derived attention and found inferior in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Dynamic learned attention (task-specific)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Task-specific attention mechanism where attention weights are parameterized and learned during training (softmax-normalized) and applied to shared representations.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>Learned per-task attention parameters applied multiplicatively to shared features.</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Multi-task DoH traffic classification</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td>Reported to perform worse than static feature-importance attention in experiments; e.g., for 32-64-128 configuration baseline+dynamic attention macro F1 reported as 0.9817 in Table 8 vs static feature-importance 0.9905.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td>In some configurations dynamic attention underperformed the uniform baseline or static attention (per Table 8).</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td>Higher computational and parameter cost than static attention due to learned parameters; authors cite increased computational cost as a reason static attention is preferable for resource-constrained settings.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td>Less interpretable than static attention derived from RF importance.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Did not improve multi-task performance in this application and was outperformed by static, RF-derived attention when combined with GradNorm.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td>Authors note that dynamic attention increases computational cost and is less suitable for constrained devices compared to static attention.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>Learned attention (dynamic) was more costly and produced lower multi-task F1 than static RF-based attention in this dataset and architecture.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>Dynamic attention is a task-aligned mechanism in principle, but in this empirical setting it did not yield better performance than the simpler static task alignment, so evidence is mixed.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Gradnorm: Gradient normalization for adaptive loss balancing in deep multitask networks. <em>(Rating: 2)</em></li>
                <li>DoH Tunnel Detection Based on the Fusion of MAML and F-CNN <em>(Rating: 2)</em></li>
                <li>CIRA-CIC-DoHBrw-2020 <em>(Rating: 2)</em></li>
                <li>Random forests. <em>(Rating: 1)</em></li>
                <li>MASiNet: Network Intrusion Detection for IoT Security Based on Meta-Learning Framework. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2245",
    "paper_id": "paper-276226124",
    "extraction_schema_id": "extraction-schema-61",
    "extracted_data": [
        {
            "name_short": "MTL-DoHTA",
            "name_full": "Multi-Task Learning-Based DNS over HTTPS Traffic Analysis (MTL-DoHTA)",
            "brief_description": "A compact multi-task 2D-CNN framework that combines a shared convolutional backbone with task-specific attention heads (static, feature-importance based) and dynamic GradNorm task-weighting to jointly solve: (1) DoH vs non-DoH, (2) benign vs malicious DoH, and (3) DNS tunneling tool identification.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "MTL-DoHTA (2D-CNN + GradNorm + static task attention)",
            "model_description": "Input: 25 normalized flow features arranged as a 5×5 image into a 2D-CNN (three conv layers, GAP, FCs). Architecture uses a shared backbone and per-task attention layers where static attention weights (derived from Random Forest feature importance) are multiplied with shared representation; task losses are dynamically reweighted using GradNorm.",
            "model_size": "105,546 parameters (~450 KB)",
            "uses_task_aligned_abstraction": true,
            "abstraction_mechanism": "Shared convolutional backbone + task-specific attention heads with static multiplicative weights derived from Random Forest feature importance; dynamic task-weight allocation using GradNorm (gradient-norm-based loss weighting).",
            "is_dynamic_or_adaptive": true,
            "task_domain": "Multi-task network traffic classification (DNS over HTTPS detection and DNS tunneling tool identification) evaluated on CIC-DoHBrw-2020 and additional DoH datasets.",
            "performance_task_aligned": "Macro-averaging F1 = 0.9905 on CIC-DoHBrw-2020 when using 2D-CNN + GradNorm + static attention (feature importance); per-task (32-64-128 shared width): Task1 F1 = 0.9891, Task2 F1 = 0.9988, Task3 F1 = 0.9837.",
            "performance_uniform_baseline": "Baseline 2D-CNN (shared-only, 32-64-128) macro F1 reported ≈ 0.9871–0.9881 (per-tables: Task1 0.9838, Task2 0.9988, Task3 0.9786).",
            "has_direct_comparison": true,
            "computational_efficiency_task_aligned": "Model parameters = 105,546 (~450 KB). Average prediction time reported = 0.021379 s (averaged over 1000 repetitions on 231,822 flows using RTX 4090). No FLOPs reported.",
            "computational_efficiency_baseline": "Not separately quantified; baseline uses same backbone so parameter counts are similar; authors note small additional overhead for GradNorm and attention but do not provide FLOPs or memory deltas.",
            "sample_efficiency_results": "Robust to training-data reduction: 50% downsampling produced only a minor average F1 decrease (~0.003) relative to full dataset; pretraining/fine-tuning strategy used (pretrain on Dataset1 100 epochs, fine-tune on Dataset2 for 10 epochs) to improve adaptation.",
            "transfer_generalization_results": "Cross-dataset generalization: on Dataset1 (IEEE Dataport) average F1 = 0.9863 (Task1 0.9841, Task3 0.9907); on Dataset2 (CIC-DoHBrw-2020 + DoH-Tunnel-HKD combined) Task3 F1 = 0.9996 after 10-epoch fine-tuning, demonstrating strong adaptability to a novel tunneling technique.",
            "interpretability_results": "Task-specific static attention weights are derived from Random Forest feature importance, providing explicit, interpretable feature weighting per task; authors cite interpretability as a design reason for static attention.",
            "multi_task_performance": "MTL-DoHTA achieves high multi-task performance, matching or exceeding single-task baselines: Task2 was already strong in baseline, while Task1 and Task3 show noticeable gains when adding GradNorm + static attention; overall macro F1 improved to 0.9905.",
            "resource_constrained_results": "Small model size (~450 KB) and modest inference time suggest suitability for embedded devices; authors note remaining challenges for real-time deployment due to reliance on pre-extracted features and non-trivial network compute and indicate future work on lightweight optimization.",
            "key_finding_summary": "A combination of shared representations with task-specific static attention (from RF feature importance) and dynamic GradNorm loss weighting yields better multi-task performance (macro F1 0.9905) and strong cross-dataset generalization while remaining compact.",
            "supports_or_challenges_theory": "supports",
            "supports_or_challenges_theory_explanation": "MTL-DoHTA shows that allocating task-specific representational emphasis (attention) and dynamically reallocating training pressure (GradNorm) improves multi-task accuracy and generalization compared to a uniform shared-only representation.",
            "uuid": "e2245.0"
        },
        {
            "name_short": "GradNorm",
            "name_full": "GradNorm: Gradient normalization for adaptive loss balancing",
            "brief_description": "An adaptive algorithm that balances multi-task learning by rescaling task loss weights using L2 norms of gradients in shared layers and relative inverse training rates so slower-learning tasks receive more emphasis.",
            "citation_title": "Gradnorm: Gradient normalization for adaptive loss balancing in deep multitask networks.",
            "mention_or_use": "use",
            "model_name": "GradNorm (adaptive task-weighting algorithm)",
            "model_description": "Computes gradient L2 norms per task on a shared layer, computes relative inverse training rates r_i, and updates task weights w_i using a single hyperparameter α so that task gradient norms approach a common scale; incorporated in MTL-DoHTA to dynamically balance tasks.",
            "model_size": null,
            "uses_task_aligned_abstraction": true,
            "abstraction_mechanism": "Dynamic task-loss reweighting based on gradient-norm alignment (adjusts optimization resource allocation among tasks).",
            "is_dynamic_or_adaptive": true,
            "task_domain": "Multi-task deep learning (used here for DoH multi-task traffic classification).",
            "performance_task_aligned": "When added to 2D-CNN it improved multi-task performance compared to the baseline; authors report intermediate gains and that best results occur when GradNorm is combined with static attention (final macro F1 = 0.9905). Exact isolated numeric improvement due solely to GradNorm is reported as a modest increase in per-table comparisons.",
            "performance_uniform_baseline": "Baseline without GradNorm had slightly lower macro F1 (~0.987–0.988) depending on configuration, per authors' comparisons.",
            "has_direct_comparison": true,
            "computational_efficiency_task_aligned": "Authors restrict gradient-norm computation to the last shared layer for efficiency and update GradNorm weights every 10 mini-batches; no FLOPs reported but noted as a modest overhead compared to baseline.",
            "computational_efficiency_baseline": "Baseline lacks GradNorm overhead; no quantitative delta reported.",
            "sample_efficiency_results": null,
            "transfer_generalization_results": null,
            "interpretability_results": null,
            "multi_task_performance": "Helps balance learning rates across tasks and aids slower-learning tasks in catching up, improving overall multi-task performance when combined with attention.",
            "resource_constrained_results": "Designed to be lightweight in this implementation by computing norms on the last shared layer only; no concrete resource numbers reported.",
            "key_finding_summary": "GradNorm adaptively reallocates optimization focus among tasks, improving balance and contributing to higher multi-task performance in combination with task-specific attention.",
            "supports_or_challenges_theory": "supports",
            "supports_or_challenges_theory_explanation": "GradNorm is an explicit mechanism for dynamic resource allocation across tasks and in practice helped the MTL system achieve better balanced multi-task performance.",
            "uuid": "e2245.1"
        },
        {
            "name_short": "Static feature-importance attention",
            "name_full": "Static task-specific attention weights derived from Random Forest feature importance",
            "brief_description": "A task-specific attention mechanism where per-feature weights are precomputed using Random Forest feature importance and applied multiplicatively to the shared representation as fixed attention during training and inference.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Static task-specific attention (feature-importance)",
            "model_description": "Task-specific fully-connected attention layers apply fixed (precomputed) attention weights, obtained from Random Forest feature importance per task, to the shared 64-d representation; attention weights are not learned during training.",
            "model_size": null,
            "uses_task_aligned_abstraction": true,
            "abstraction_mechanism": "Static multiplicative attention gating per feature using precomputed feature-importance scores (no learned attention parameters).",
            "is_dynamic_or_adaptive": false,
            "task_domain": "Multi-task DoH traffic classification",
            "performance_task_aligned": "When combined with 2D-CNN + GradNorm gave the best reported macro F1 = 0.9905; static feature-importance attention outperformed learned (dynamic) attention in the authors' experiments (see Table 8).",
            "performance_uniform_baseline": "Improved over the uniform baseline 2D-CNN (baseline macro F1 ≈ 0.987–0.988) to final 0.9905 when used with GradNorm.",
            "has_direct_comparison": true,
            "computational_efficiency_task_aligned": "Chosen for lower computational cost relative to learning attention weights; authors explicitly cite reduced compute as one rationale but do not provide FLOPs.",
            "computational_efficiency_baseline": null,
            "sample_efficiency_results": null,
            "transfer_generalization_results": null,
            "interpretability_results": "Provides human-interpretable per-feature importance per task because attention weights are directly derived from Random Forest importance scores.",
            "multi_task_performance": "Improved Task1 and Task3 notably versus baseline and versus learned attention in reported experiments; combined with GradNorm yields best multi-task results.",
            "resource_constrained_results": "Authors argue static attention reduces computational overhead and is preferable for resource-constrained deployments.",
            "key_finding_summary": "Precomputing and fixing task-specific attention from RF feature importance is both computationally cheaper and empirically more effective than learned attention for this DoH multi-task problem.",
            "supports_or_challenges_theory": "supports",
            "supports_or_challenges_theory_explanation": "Demonstrates that explicitly allocating representational emphasis per task (even statically) can improve performance and interpretability relative to uniform treatment.",
            "uuid": "e2245.2"
        },
        {
            "name_short": "2D-CNN baseline",
            "name_full": "2D Convolutional Neural Network baseline (shared-only)",
            "brief_description": "A baseline 2D-CNN with a shared convolutional backbone and per-task heads but without GradNorm dynamic reweighting or explicit task-specific attention; processes 5×5 images made from flow features.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "2D-CNN baseline (shared representation)",
            "model_description": "Shared 2D-CNN (three conv layers 32/64/128 variants) with global average pooling and task heads; represents a uniform/shared representation baseline lacking task-specific representational allocations.",
            "model_size": null,
            "uses_task_aligned_abstraction": false,
            "abstraction_mechanism": "Uniform shared representation across tasks; no task-gated or dynamically rebalanced components.",
            "is_dynamic_or_adaptive": false,
            "task_domain": "Multi-task DoH traffic classification",
            "performance_task_aligned": "Macro F1 approximately 0.987–0.988 (for 32-64-128 configuration Task1 0.9838 Task2 0.9988 Task3 0.9786 as reported in tables).",
            "performance_uniform_baseline": null,
            "has_direct_comparison": true,
            "computational_efficiency_task_aligned": null,
            "computational_efficiency_baseline": "Not separately reported; authors indicate it is slightly cheaper computationally than variants with learned attention/GradNorm but do not quantify.",
            "sample_efficiency_results": null,
            "transfer_generalization_results": null,
            "interpretability_results": null,
            "multi_task_performance": "Strong on Task2 but modestly weaker than MTL-DoHTA (with GradNorm + static attention) on Task1 and Task3.",
            "resource_constrained_results": null,
            "key_finding_summary": "A uniform shared representation baseline attains high performance but is outperformed by architectures that introduce task-specific emphasis and dynamic task-weighting.",
            "supports_or_challenges_theory": "mixed",
            "supports_or_challenges_theory_explanation": "Baseline demonstrates that uniform shared representations can be strong, but the observed gains from task-aligned mechanisms indicate advantages to task-specific abstraction.",
            "uuid": "e2245.3"
        },
        {
            "name_short": "Random Forest (for feature importance)",
            "name_full": "Random Forest (L. Breiman)",
            "brief_description": "An ensemble tree-based method used both as a high-performing single-task classifier in prior work and here to compute per-task feature importance that drives static attention weights.",
            "citation_title": "Random forests.",
            "mention_or_use": "use",
            "model_name": "Random Forest",
            "model_description": "Ensemble of decision trees used here to compute feature importance per task (trained separately per task with 100 trees, default hyperparameters); top features common across tasks were selected to form the 25-feature input.",
            "model_size": null,
            "uses_task_aligned_abstraction": false,
            "abstraction_mechanism": "Ensemble decision trees producing feature-importance scores; used as preprocessing to create task-specific static attention weights.",
            "is_dynamic_or_adaptive": false,
            "task_domain": "Feature selection and single-task classification for network traffic",
            "performance_task_aligned": "Random Forest has been reported in prior work to achieve high F1 with 28 features (MontazeriShatoori et al.); in this paper RF was used to compute feature importances — exact RF classifier metrics within this paper are not enumerated.",
            "performance_uniform_baseline": null,
            "has_direct_comparison": false,
            "computational_efficiency_task_aligned": null,
            "computational_efficiency_baseline": null,
            "sample_efficiency_results": null,
            "transfer_generalization_results": null,
            "interpretability_results": "Provides interpretable per-feature importance scores which the authors used directly to build static attention weights.",
            "multi_task_performance": null,
            "resource_constrained_results": null,
            "key_finding_summary": "Random Forest feature importances enabled a low-cost, interpretable task-specific attention mechanism that contributed to improved multi-task performance.",
            "supports_or_challenges_theory": "supports",
            "supports_or_challenges_theory_explanation": "By supplying explicit per-task feature importances, RF enabled task-aligned attention that improved downstream multi-task learning performance.",
            "uuid": "e2245.4"
        },
        {
            "name_short": "MFC-DoH (few-shot)",
            "name_full": "DoH Tunnel Detection Based on the Fusion of MAML and F-CNN",
            "brief_description": "A few-shot meta-learning approach (MAML + F-CNN) for DoH tunnel detection cited for comparison; reported Task3 F1 = 0.9100 (few-shot 20) and has a much larger parameter count.",
            "citation_title": "DoH Tunnel Detection Based on the Fusion of MAML and F-CNN",
            "mention_or_use": "mention",
            "model_name": "MFC-DoH (MAML + F-CNN few-shot)",
            "model_description": "Meta-learning approach that uses MAML to enable fast adaptation of an F-CNN to novel DoH tunneling techniques with limited labeled examples (few-shot).",
            "model_size": "1,147,904 parameters (reported in Table 10 of the paper)",
            "uses_task_aligned_abstraction": true,
            "abstraction_mechanism": "Meta-learning (MAML) enabling rapid per-task adaptation of network weights; few-shot learning enables task-conditioned representations.",
            "is_dynamic_or_adaptive": true,
            "task_domain": "Few-shot DoH tunnel detection / DNS tunneling tool identification (Task3)",
            "performance_task_aligned": "Reported Task3 F1 = 0.9100 (few-shot 20) as shown in the comparison table (Table 9) — authors state limited direct comparability to their approach.",
            "performance_uniform_baseline": null,
            "has_direct_comparison": false,
            "computational_efficiency_task_aligned": "Model is substantially larger (≈1.15M parameters) than MTL-DoHTA; no inference-time metrics reported in this paper.",
            "computational_efficiency_baseline": null,
            "sample_efficiency_results": "Designed for few-shot sample efficiency (20-shot reported) but no direct sample-efficiency comparisons within this paper.",
            "transfer_generalization_results": null,
            "interpretability_results": null,
            "multi_task_performance": null,
            "resource_constrained_results": "Large parameter count implies less suitable for constrained devices compared to compact MTL-DoHTA.",
            "key_finding_summary": "Mentioned as an adaptive few-shot method that is more parameter-heavy and achieved lower Task3 F1 in the comparison table, illustrating trade-offs between adaptivity and compactness.",
            "supports_or_challenges_theory": "supports",
            "supports_or_challenges_theory_explanation": "MAML-based few-shot approaches exemplify adaptive, task-aligned representation learning intended to generalize quickly to new tasks, consistent with the Task-Aligned Abstraction Principle.",
            "uuid": "e2245.5"
        },
        {
            "name_short": "Dynamic learned attention",
            "name_full": "Dynamic (learned) task-specific attention weights",
            "brief_description": "Learned attention weights for each task that are optimized end-to-end via backpropagation; compared experimentally to static, RF-derived attention and found inferior in this work.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Dynamic learned attention (task-specific)",
            "model_description": "Task-specific attention mechanism where attention weights are parameterized and learned during training (softmax-normalized) and applied to shared representations.",
            "model_size": null,
            "uses_task_aligned_abstraction": true,
            "abstraction_mechanism": "Learned per-task attention parameters applied multiplicatively to shared features.",
            "is_dynamic_or_adaptive": true,
            "task_domain": "Multi-task DoH traffic classification",
            "performance_task_aligned": "Reported to perform worse than static feature-importance attention in experiments; e.g., for 32-64-128 configuration baseline+dynamic attention macro F1 reported as 0.9817 in Table 8 vs static feature-importance 0.9905.",
            "performance_uniform_baseline": "In some configurations dynamic attention underperformed the uniform baseline or static attention (per Table 8).",
            "has_direct_comparison": true,
            "computational_efficiency_task_aligned": "Higher computational and parameter cost than static attention due to learned parameters; authors cite increased computational cost as a reason static attention is preferable for resource-constrained settings.",
            "computational_efficiency_baseline": null,
            "sample_efficiency_results": null,
            "transfer_generalization_results": null,
            "interpretability_results": "Less interpretable than static attention derived from RF importance.",
            "multi_task_performance": "Did not improve multi-task performance in this application and was outperformed by static, RF-derived attention when combined with GradNorm.",
            "resource_constrained_results": "Authors note that dynamic attention increases computational cost and is less suitable for constrained devices compared to static attention.",
            "key_finding_summary": "Learned attention (dynamic) was more costly and produced lower multi-task F1 than static RF-based attention in this dataset and architecture.",
            "supports_or_challenges_theory": "mixed",
            "supports_or_challenges_theory_explanation": "Dynamic attention is a task-aligned mechanism in principle, but in this empirical setting it did not yield better performance than the simpler static task alignment, so evidence is mixed.",
            "uuid": "e2245.6"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Gradnorm: Gradient normalization for adaptive loss balancing in deep multitask networks.",
            "rating": 2
        },
        {
            "paper_title": "DoH Tunnel Detection Based on the Fusion of MAML and F-CNN",
            "rating": 2
        },
        {
            "paper_title": "CIRA-CIC-DoHBrw-2020",
            "rating": 2
        },
        {
            "paper_title": "Random forests.",
            "rating": 1
        },
        {
            "paper_title": "MASiNet: Network Intrusion Detection for IoT Security Based on Meta-Learning Framework.",
            "rating": 1
        }
    ],
    "cost": 0.026617000000000002,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>MTL-DoHTA: Multi-Task Learning-Based DNS over HTTPS Traffic Analysis for Enhanced Network Security
7 February 2025</p>
<p>Rafal Kozik 
Division of Software
Hallym University
24252ChuncheonRepublic of Korea</p>
<p>Michal Choras 
Division of Software
Hallym University
24252ChuncheonRepublic of Korea</p>
<p>Marek Pawlicki 
Division of Software
Hallym University
24252ChuncheonRepublic of Korea</p>
<p>Woong Kyo 
Division of Software
Hallym University
24252ChuncheonRepublic of Korea</p>
<p>Byung Il Kwak 0000-0002-2009-4580
Division of Software
Hallym University
24252ChuncheonRepublic of Korea</p>
<p>MTL-DoHTA: Multi-Task Learning-Based DNS over HTTPS Traffic Analysis for Enhanced Network Security
7 February 202537E48663EC10CFA7F83A8DEB0148274810.3390/s25040993Received: 20 December 2024 Revised: 3 February 2025 Accepted: 5 February 2025DNS over HTTPSDNS covert channelmulti-task learningdeep learning
The adoption of DNS over HTTPS (DoH) has significantly enhanced user privacy and security by encrypting DNS queries.However, it also presents new challenges for detecting malicious activities, such as DNS tunneling, within encrypted traffic.In this study, we propose MTL-DoHTA, a multi-task learning-based framework designed to analyze DoH traffic and classify it into three tasks: (1) DoH vs. non-DoH traffic, (2) benign vs. malicious DoH traffic, and (3) the identification of DNS tunneling tools (e.g., dns2tcp, dnscat2, iodine).Leveraging statistical features derived from network traffic and a 2D-CNN architecture enhanced with GradNorm and attention mechanisms, MTL-DoHTA achieves a macroaveraging F1-score of 0.9905 on the CIRA-CIC-DoHBrw-2020 dataset.Furthermore, the model effectively handles class imbalance and mitigates overfitting using downsampling techniques while maintaining high classification performance.The proposed framework can serve as a reliable tool for monitoring and securing sensor-based network systems against sophisticated threats, while also demonstrating its potential to enhance multitasking capabilities in resource-constrained sensor environments.</p>
<p>Introduction</p>
<p>The increasing importance of internet security and privacy has made the Domain Name System (DNS) a critical but vulnerable target for cyberattacks.As the backbone of the Internet, DNS translates human-readable hostnames into machine-readable IP addresses, enabling seamless and efficient communication between users and websites.However, the traditional DNS design transmits data in plaintext, making it susceptible to threats like eavesdropping, data manipulation, and Man-in-the-Middle (MitM) attacks [1].Attackers have exploited these vulnerabilities to intercept sensitive information or manipulate DNS responses for malicious purposes, such as redirecting users to phishing websites or delivering malware.Recognizing the need to secure DNS communications, the Internet Engineering Task Force (IETF) introduced DNS over HTTPS (DoH) in 2018 [2].This standardized protocol encrypts DNS queries using HTTPS, ensuring that communication between users and DNS resolvers remains confidential and protected against interception, thus significantly enhancing user privacy and security [3].</p>
<p>Despite its benefits, DoH poses new challenges for network security.By encrypting DNS traffic and embedding it within HTTPS, DoH obscures the visibility of DNS queries, making them indistinguishable from other web traffic [4].This encryption prevents traditional packet inspection techniques from identifying DNS-related activities, complicating detecting and mitigating malicious behaviors.Attackers have exploited this characteristic to perform DNS tunneling, a method that uses DNS queries to covertly transfer data or communicate with command and control (C2) servers [4,5].Such misuse has been observed in high-profile cases, including Godlua malware and Oilrig (APT34), where DoH was used to exfiltrate data and maintain C2 channels [6,7].These incidents highlight a critical gap in existing DNS traffic analysis methods as they struggle to address the complexities introduced by encrypted DNS traffic.</p>
<p>In light of these challenges, research has shifted towards developing more robust detection mechanisms.Machine learning (ML) and deep learning (DL) approaches are increasingly employed to analyze encrypted DNS traffic.As noted by Jehad Ali et al. [8], advanced ML algorithms, such as anomaly detection and behavior analysis, have shown promise in identifying deviations from normal patterns within encrypted communications.Such approaches enable the detection of sophisticated threats while maintaining privacy safeguards.Ali et al. emphasize the importance of integrating AI-driven security frameworks, particularly in environments with significant IoT and network interconnectivity, such as smart cities.Their work highlights the role of adaptive AI systems in continuously learning and evolving to address emerging cyber threats.</p>
<p>Furthermore, while meta-learning frameworks have demonstrated efficacy in detecting intrusions and abnormal encrypted network traffic in IoT environments [9], their applicability to DoH remains underexplored.As DoH evolves, methodologies that enhance adaptability to new DNS tunneling technologies and attack vectors need to be incorporated.This adaptability can be achieved through multi-task learning frameworks that allow models to generalize across diverse scenarios without compromising detection accuracy or scalability.</p>
<p>Developing advanced detection mechanisms requires a multifaceted approach that combines machine learning algorithms with behavioral analysis to identify anomalies in DNS traffic patterns while maintaining strict privacy safeguards.These mechanisms must enhance model performance and preserve user privacy, a cornerstone of encrypted DNS traffic such as DoH.Furthermore, security solutions need to remain adaptable and effective even in diverse and dynamic network environments, providing a critical layer of resilience against the evolving landscape of cyber threats.</p>
<p>Motivated by these challenges, we propose multi-task learning (MTL)-based traffic classification model to address the complexities of DoH traffic detection and malicious activity identification.The MTL model is designed to tackle three interconnected tasks: (1) classifying network traffic into DoH and non-DoH categories, (2) distinguishing between benign and malicious DoH traffic, and (3) conducting a multi-class classification to identify specific DNS tunneling tools used in malicious activities.Leveraging time-series classification techniques, our MTL model offers comprehensive insight into network traffic, enabling rapid and accurate detection across all tasks.By achieving the high performance of single-task models while simultaneously addressing multiple objectives, this MTL-based approach enhances network security by effectively identifying malicious behaviors and specific DNS tunneling tools while preserving the privacy benefits of DoH.</p>
<p>The research contributions of the multi-task learning-based traffic classification model proposed in this study are as follows:</p>
<p>1.</p>
<p>It introduces a novel framework that integrates both classification tasks, enabling more efficient learning and improved accuracy in detecting threats within encrypted traffic.</p>
<p>2.</p>
<p>The model leverages shared representations across tasks, which not only reduces the computational burden but also enhances generalization capabilities by learning from diverse data patterns present in both benign and malicious traffic.</p>
<p>3.</p>
<p>The proposed methodology has been confirmed to achieve high accuracy and superior performance in both learning and testing.This represents a significant contribution to the research, demonstrating the effectiveness of multi-task processing.</p>
<p>4.</p>
<p>The framework employs downsampling techniques to address class imbalance in the dataset, ensuring that the model maintains high performance across underrepresented classes.This approach not only improves classification accuracy but also reduces the risk of overfitting, especially in multi-class tasks such as DNS tunneling tool identification.</p>
<p>The rest of this paper is organized as follows.Section 2 reviews the related studies.Section 3 presents our multi-task learning method based on DoH traffic analysis and classification.Section 4 describes the experimental results and evaluates our proposed approach.Finally, Section 5 provides limitations and the concluding remarks.</p>
<p>Related Work</p>
<p>To comprehensively understand the research themes, characteristics, and limitations of existing studies related to DoH traffic analysis, we added a comparative analysis table that examines whether Tasks 1, 2, and 3 were addressed in prior research, particularly emphasizing the usability of DoH (See Table 1).Facilitates elevated precision via swift adjustment within constrained data contexts.</p>
<p>The emphasis is placed on particular DNS tunneling instruments, resulting in the generalized detection paradigm lacking scalability.</p>
<p>V V X [6]</p>
<p>Research on methodologies for identifying nefarious DoH traffic through the application of machine learning algorithms.</p>
<p>Examine a range of machine learning frameworks and propose strategies for enhancing detection precision.</p>
<p>Concentrate on binary classification instead of multi-class classification.</p>
<p>V V X [11] Identify nefarious behavior within DoH traffic by employing ensemble machine learning methodologies.</p>
<p>Conduct a comparative analysis of the efficacy of various models, documenting elevated detection accuracy alongside minimal rates of false positives.</p>
<p>The emphasis is placed on binary classification as opposed to multiclass classification.</p>
<p>V V X [12] Proposed temporal series classification framework for the identification of DNS tunneling phenomena occurring within DoH traffic.</p>
<p>Enhance precision and operational efficacy through the recommendation of detection methodologies grounded in time series classification algorithms.</p>
<p>No experiments for Task 3 to identify specific DNS tunneling tools V V X The adoption of DoH has significantly enhanced DNS security through encryption, yet it also introduces new challenges for detecting malicious activities such as DNS tunneling.By obfuscating traffic within HTTPS, DoH complicates the ability of existing detection technologies to differentiate DNS requests from standard web traffic, allowing malicious entities to covertly transmit data or obscure communication with C2 servers [1].DNS tunneling is a technique used to hide malicious data within normal DNS queries.This method allows attackers to bypass firewalls and security measures, which can result in unauthorized data being extracted and malware being activated remotely.Instances of such exploitation underscore the inadequacies of conventional static rule-based detection systems in scrutinizing DoH traffic and accentuate the necessity for more advanced detection methodologies [4].</p>
<p>Recent studies have explored machine learning (ML) techniques to detect malicious DoH traffic, with particular emphasis on time-series analysis.Singh et al. [11] demonstrated that integrating ML algorithms with temporal attributes improves the detection of DNS tunneling activities.Ensemble learning methods such as Gradient Boosting and Random Forest have shown high precision in classifying DoH traffic based on packet dimensions, transmission velocity, and session length [13].Moreover, some studies have incorporated feature extraction techniques using machine learning and PCAP-based novel features to enhance model performance and improve malicious DoH detection [15].</p>
<p>Building on these techniques, MontazeriShatoori et al. [12] proposed a DoH detection approach together with the CIRA-CICDoHBrw-2020 dataset [17], which contained preextracted flow statistics.Their experiments compared multiple ML algorithms (Random Forest, Decision Tree, SVM, Naive Bayes), 2D-CNN, and LSTM; Random Forest achieved a notably high F1-score using 28 features.Furthermore, other research efforts leveraged deep learning architectures.In particular, Singh et al. [11] explored LSTM (Long Short-Term Memory) networks, demonstrating how temporal analysis can help to uncover malicious patterns in DoH traffic.</p>
<p>To further enhance the interpretability of DoH detection models, researchers have integrated visualization techniques.Mohammad et al. [10] performed visualization work on the CIRA-CIC-DoHBrw-2020 dataset (also referred to as CIC-DoHBrw-2020) using Eigen Centrality (EC) in graph/network theory, Principal Component Analysis (PCA), and a Gaussian Mixture Model (GMM).These methods analyzed specific clusters in the data to identify potential anomalies.Similarly, Zebin et al. [16] focused on classifying benign versus malicious DoH using a machine learning-based Random Forest algorithm, augmenting interpretability through Shapley additive explanations (SHAP) and the visualization of packet data.Furthermore, Jerabek et al. [18] performed a comparative analysis on both the CIC-DoHBrw-2020 dataset and a real-world dataset [19], examining the transferability, usability, and longevity of previously published malicious DoH detection machine learning models across these different data sources.</p>
<p>Stalder [15] proposed a three-layered framework to address three distinct classification tasks: DoH vs. non-DoH, Benign vs. Malicious DoH, and DNS tunneling tool classification.This framework integrates ML algorithms tailored for each classification task and employs feature importance analysis during preprocessing to enhance detection accuracy.However, the research lacked results for Task 3 (DNS tunneling tool classification), leaving this aspect unexplored.</p>
<p>Although previous research has contributed to DoH detection, recent studies indicate that existing datasets and models remain insufficient in addressing evolving threats comprehensively.The recent datasets require broader attack vector coverage and improved malicious behavior representations.Moreover, many ML/DL-based models must address task scalability, ensuring that existing models can be reused or extended when new malicious DNS tunneling tools emerge.In response to these gaps, this paper proposes a multi-task learning-based traffic analysis and classification approach which aims to enhance task scalability and adaptability in the face of evolving threats.</p>
<p>Methodology</p>
<p>In this section, we show the proposed method concerning multi-task learning-based DoH traffic analysis and classification (MTL-DoHTA).Figure 1 shows the overview of our MTL-DoHTA, which integrates various machine learning techniques to simultaneously analyze and classify DoH traffic patterns.By leveraging shared representations across multiple tasks, this approach not only improves the accuracy of detection but also enhances the model's ability to generalize across different types of network behaviors.The overview is composed three stages: first, the data preprocessing phase, where raw DoH traffic is cleaned and transformed into a suitable format for analysis; second, the feature extraction stage, which identifies key characteristics of the traffic that are critical for effective classification; and third, the model training phase, where machine learning algorithms are employed to learn from the extracted features and optimize performance across tasks.</p>
<p>DoHTANet Model Train
Client</p>
<p>Data Collection by Traffic Flow Unit</p>
<p>In this subsection, to integrate network traffic into the MTL algorithm, the raw traffic is transformed into a flow-based representation that encapsulates the essential attributes of individual data packets.This representation not only facilitates efficient processing but also enhances the model's ability to identify patterns and detect anomalies within the traffic, ultimately improving classification accuracy.Once the flow-based features are generated, they undergo a feature extraction process to select only the most critical features.These refined features are then utilized as inputs to the MTL algorithm, ensuring optimal performance in the classification tasks.</p>
<p>Feature Selection with Feature Importance</p>
<p>We extracted a total of 29 features from network traffic using the 'DoHlyzer' tool [12], generating flow-based statistical features.The complete list of these features is provided in Table 2.These features are categorized into attributes such as duration, number of bytes, packet length, packet time, and request/response time difference.To ensure model efficiency and lightweight processing, we selected 25 features with the highest feature importance as inputs for the model.This feature selection process is conducted only during the training phase and is not repeated during validation or testing.Instead, the features selected during training are directly used in the validation and testing phases.By focusing on the most relevant information, the selected features enhance the model's performance and ensure better generalization on unseen data.To identify the top 25 features out of the initial 29, we applied the Random Forest [20] algorithm as a single-task learning approach to each of the tasks (Task 1, Task 2, and Task 3).Based on the feature importance scores obtained for each task, we identified the top 25 features that were commonly ranked highly across all tasks (see Figure 2).These features, representing the most significant attributes overall, were then used as inputs to the 2D-CNN and attention-based models in this study.The 25 selected features undergo a MinMax normalization process, where their values are scaled between 0 and 1 according to Equation (1), as shown in Figure 3.After normalization, the features are arranged sequentially into a 5 × 5 vector, resulting in an image-like representation.
x ′ = x − x min x max − x min (1</p>
<p>MTL-DoHTA Model</p>
<p>To enable the simultaneous processing of multiple tasks, we designed the architecture of a MTL algorithm, as illustrated in Figure 4.The proposed MTL algorithm consists of three main components: 1.A shared network architecture in the neural network structure; 2. a task-specific attention architecture for each task; 3. an output layer that computes the outputs and dynamically updates task weights using GradNorm [21].The proposed architecture leverages a shared network to learn common features across tasks, thereby enhancing the model's generalization capability.Increasing the width of the shared network can further improve its ability to generalize; however, overly generalized shared features may lack robustness in capturing task-specific characteristics.To address this, the network incorporates dedicated task-specific layers, structured as a multi-task shared layer, a task-specific attention layer, and an output layer with GradNorm-based weight updates.The task-specific attention layer utilizes precomputed and fixed attention weights to assign feature importance for each task.These static attention weights guide the model in updating network parameters effectively, ensuring that each layer focuses on task-specific information based on pre-extracted features.Meanwhile, the output layer employs task-specific loss functions to compute final losses and predictions.In addition, the gradient norms of the final dense layer in the shared network are computed to compare the gradient norms across tasks.This enables dynamic updates of task weights using GradNorm, ensuring balanced learning among the tasks.The components and their detailed functionalities are described in the subsequent sections.</p>
<p>Multi-Task Shared Layer</p>
<p>The shared layer is responsible for extracting common features from the input data in the early stages of the network.In this study, the convolutional networks in the shared layer employ 3 × 3 filters and are structured to enhance feature extraction efficiency.The shared layer consists of three convolutional layers, a max-pooling layer, and two fully connected layers, with ReLU activation functions incorporated into the 2D-CNN structure to introduce non-linearity and enable the network to learn diverse features.</p>
<p>The input data, represented as a 2D image, sequentially pass through four shared layers before entering the task-specific layers.The first shared layer expands the 5 × 5 × 1 input into 32 feature maps and compresses it using a max-pooling layer, reducing the spatial dimensions to 2 × 2. The second and third shared layers further expand the number of feature maps to 64 and 128, respectively, progressively capturing more abstract representations.Following this, a global average pooling (GAP) layer computes the average values of the output feature maps.It transforms them into a 128-dimensional dense vector, a predefined size independent of the feature map dimensions.The flattened vector is then processed through the fully connected layers, where a 64-unit shared fully connected layer compresses the representation before passing it into the task-specific attention layer.</p>
<p>Task Specific Attention Layer</p>
<p>The task-specific attention layer consists of separate, fully connected layers, each with 29 units corresponding to the number of features used in this study.Each task-specific layer functions as an attention mechanism, where precomputed feature importance values are statically multiplied by fixed attention weights throughout the training process.This allows each task to focus on the most relevant features while preserving the structural integrity of the shared representation.</p>
<p>After passing through the attention layer, Task 1 and Task 2 employ the BCEWithLogits loss function for independent binary classification, generating probability values for classification decisions.In contrast, Task 3 utilizes the cross-entropy loss function to classify inputs into five categories, effectively capturing multi-class relationships.</p>
<p>Static Attention Mechanism Based on Feature Importance</p>
<p>In this study, the feature importance values for each task are precomputed using the Random Forest algorithm and utilized as prior knowledge during training.Instead of dynamically learning the attention weights, the model leverages task-specific feature importance to guide specialized training for each task.</p>
<p>The reasons for not dynamically learning attention weights are as follows:</p>
<p>• The robust weights learned through the shared layer provide generalized representation power across all tasks, while the additional attention layer further emphasizes this generalized representation.• Dynamically learning attention weights can significantly increase computational costs, especially when combined with the computations required for the shared layer.This consideration makes static attention weights a more efficient choice.• By explicitly reflecting the important features, the model ensures task-specific alignment, allowing each task to focus on its most relevant features without additional complexity.</p>
<p>Attention Mechanism</p>
<p>To enhance the learning effectiveness of our MTL-DoHTA model, the attention mechanism enables the model to focus on the most relevant parts of the input data, thereby improving its performance [22].This approach assigns attention weights to the encoder's hidden states, emphasizing the importance of each input token during decoding.</p>
<p>First, we compute a weighted sum of the encoder's hidden states, guided by these attention weights.Let this weighted sum be denoted by z, as defined in Equation ( 2):
z = W attention • x(2)
Here, x represents the input network weights, which are learned through backpropagation.Next, to ensure that the resulting attention weights sum to 1, we apply a softmax function to z (Equation ( 3)):
attention_scores i = exp(z i ) ∑ j exp(z j ) for i ∈ <a href="3">1, n</a>
producing a probability distribution that indicates how much attention is allocated to each token.Finally, we multiply the encoder's hidden states by these normalized attention scores in an element-wise manner to obtain the context vector X attended , as shown in Equation ( 4):
x attended = x ⊙ attention_scores(4)
This context vector highlights the most important features of the input data for predicting the current output, enabling the MTL-DoHTA model to adaptively focus on different tokens at each step.</p>
<p>Output Layer and GradNorm for Dynamic Task Weighting</p>
<p>After the fully connected layer for each task, the loss values are calculated using the respective loss functions.During the backward pass, the gradient norms of the fully connected network weights in the last shared layer (64 units) are computed.These gradient norms are normalized to a common scale and multiplied by the relative inverse training rate of each task.As a result, the task weights are dynamically adjusted during backpropagation based on these common-scale gradient norms.In multi-task learning, the final loss function is typically the sum of the loss values for each task.To account for differing learning speeds among tasks, task weights w i are introduced to regulate each task's contribution to the total loss.Equation (5) shows this weighted multi-task loss function, where i indexes each task:
L MTL = ∑ i w i • L i(5)
GradNorm is a normalization technique designed to balance the loss values across tasks by directly tuning w i based on the gradient magnitudes of the shared layer.Unlike grid search, GradNorm uses a single hyperparameter α to adjust task weights dynamically, allowing tasks with slower learning rates to catch up and train at a pace similar to other tasks.The GradNorm update rule for each task weight w i is shown in Equation ( 6):
w i (t + 1) = w i (t) + α • G i (t) − Ḡ(t)(6)
Here, G i (t) indicates the L2 norm of the gradients associated with task i.Although some approaches [22] suggest using the entire shared layer for this computation, we focus on the gradients in the last shared layer only for efficiency.Equation (7) defines G i (t):
G i (t) = ∥∇ W L i ∥ 2 (7)
The common scale Ḡ(t) is then computed as the average of these gradient norms across all T tasks (Equation ( 8)):
Ḡ(t) = 1 T T ∑ i=1 G i (t)(8)
In addition to gradient norms, GradNorm also calculates a loss ratio, L i (t), which represents how much the loss for task i has changed relative to its initial value.Equation (9) shows how L i (t) is derived:
L i (t) = L i (t) L i (0)(9)
A smaller L i (t) implies faster convergence (lower loss over time), whereas a larger L i (t) indicates slower learning.GradNorm then uses the relative inverse training rate, r i (t), to measure how a task's progress compares to the overall average loss, as shown in Equation ( 10):
r i (t) = L i (t) L(t)(10)
Here, L(t) is the average loss across all tasks.If r i (t) &gt; 1, it suggests that the task i is learning more slowly than average and therefore requires more attention (i.e., a higher weight).GradNorm leverages these metrics to adjust w i so that all tasks can maintain a balanced learning pace.</p>
<p>In addition to adjusting the task weights, GradNorm also updates the gradient norm for each task to reflect this inverse training rate.As shown in Equation ( 11), the gradient norm G i (t) is shifted closer to the common scale Ḡ(t) based on α and r i (t):
G i (t + 1) = G i (t) + α • r i (t) • Ḡ(t) − G i (t)(11)
Here, α is a hyperparameter that controls how strongly the gradient update prioritizes tasks with higher losses.Tasks displaying slower learning rates (r i (t) &gt; 1) thus receive proportionally larger adjustments in their gradient norms.Finally, GradNorm defines a gradient loss L grad that quantifies the discrepancy between the individual gradient magnitudes G i (t) and the rescaled common scale Ḡ(t).Equation (12) shows how L grad is computed:
L grad = ∑ i G i (t) − Ḡ(t)(12)
By minimizing L grad , the method encourages each task's gradient norm to remain close to the overall average, ensuring that all tasks progress at a similar pace.The task weights are normalized at each time step so that their sum equals T, the total number of tasks.The overall process of the MTL-DoHTA model is illustrated in Algorithms 1 and 2, detailing both the forward pass and backward propagation steps.</p>
<p>Algorithm 1 DoHTA Multi-task Learning Forward Pass 1: Input: X ∈ R B×5×5×1 2: Output: {y pred1 , y pred2 , y pred3 } 3: Permute X to (B, 1, 5, 5)</p>
<p>▷ Reorganize input tensor 4:
X conv1 ← ReLU(Conv2D 1→32 (X)) ▷ Output channels = 32 5: X pool ← MaxPool(X conv1 ) ▷ Downsample to 2 × 2 6: X conv2 ← ReLU(Conv2D 32→64 (X pool )) ▷ Output channels = 64 7: X conv3 ← ReLU(Conv2D 64→128 (X conv2 )) ▷ Output channels = 128 8: X gap ← Global Average Pooling(X conv3 ) ▷ Global Average Pooling 9: X f c1 ← Dropout(ReLU(X gap )) 10: X shared ← Dropout(ReLU(X f c1 ))
▷ Shared output for task-specific heads 11: for i ∈ {1, 2, 3} do 12:
α (i) ← Softmax(X shared A (i) ) ▷ Static feature importance A (i)
13:
X (i) task ← X shared ⊙ α (i)
▷ Attention applied for task i 14: end for 15: y pred1 ← BCEWithLogits(X task1 ) 16: y pred2 ← BCEWithLogits(X task2 ) 17: y pred3 ← CrossEntropy(X task3 ) return {y pred1 , y pred2 , y pred3 } Algorithm 2 DoHTA GradNorm Backward Propagation 1: Input: Task losses L task1 , L task2 , L task3 Shared output X shared Task weights w task1 , w task2 , w task3 2: Output:</p>
<p>Total loss L total Updated task weights w task1 , w task2 , w task3 3: Step 1: Compute Total Loss 4: L total ← w task1 L task1 + w task2 L task2 + w task3 L task3 5:</p>
<p>Step 2: Backpropagate Gradients for Shared Output 6: Compute gradients of total loss with respect to X shared : 7: X shared ← ∇ X shared (w task1 L task1 + w task2 L task2 + w task3 L task3 ) 8: Step 3: GradNorm Application 9: Compute updated task weights using GradNorm function: 10: w task1 , w task2 , w task3 ← GradNorm(L task1 , L task2 , L task3 , X shared ) 11:</p>
<p>Step 4: Return Values 12: return Total loss L total and updated task weights w task1 , w task2 , w task3</p>
<p>Experimental Evaluation</p>
<p>Dataset and Performance Metrics</p>
<p>The CIC-DoHBrw-2020 dataset, developed by the Canadian Institute for Cybersecurity Research, provides valuable insights, detailed in Table 3 [12].It includes DoH traffic generated using Google Chrome, Mozilla Firefox, and three DNS covert channel tools: iodine, dnscat2, and dns2tcp.This traffic interacts with four DoH servers, namely AdGuard, Cloudflare, Google DNS, and Quad9, to capture diverse behaviors.The dataset is organized into three categories: non-DoH (regular HTTPS traffic), benign-DoH (normal DoH traffic), and malicious-DoH (DoH-encrypted DNS covert channels).While non-DoH and benign-DoH traffic are created by accessing Alexa's top 10,000 domains, malicious DoH traffic is generated by covert channel tools using TLS-encrypted HTTPS requests to specific DoH servers.To train our model, we divided the CIC-DoHBrw-2020 dataset 8:2 in the experiment.The total train data comprised 927,419 (80%), and the test data comprised 231,822 (20%).The experiment was operated on a system with Windows 11 OS, an Intel(R) Core i9-14900KF processor, and a Geforce RTX 4090 GPU, using Python 3.9 (see Table 4).</p>
<p>Hyperparameter Settings</p>
<p>To calculate the importance for each single task, the Random Forest algorithm was used with default hyperparameter settings.Specifically, the number of decision trees was set to 100, the maximum tree depth was "unlimited", and Gini impurity was applied.For the MTL DNN algorithm, the following hyperparameters were used: a dropout rate of 0.3, a batch size of 32, and an Adam optimizer.The scaling factor α for GradNorm, which balances tasks, was set to 1.9.GradNorm weights were updated every 10 batch sequences to naturally integrate with the mini-batch gradient descent method.Training was conducted for 50 epochs with a learning rate of 0.001, and the final evaluation was based on the model achieving the highest total F1-score during these epochs.Additionally, the Optuna library [23] was used to optimize the hyperparameters, including the learning rate, batch size, and dropout rate.The search ranges were as follows: To obtain the optimal hyperparameters, we set another dataset that was a 50% downsampled version of the previous train dataset.To further analyze the model's performance, the structural parameters of the MTL-DoHTA model, which play a critical role in processing these datasets, are detailed in Table 5.</p>
<p>Performance Evaluation</p>
<p>To validate the performance of our proposed model, we conducted evaluations from three perspectives:</p>
<p>1.</p>
<p>We assessed task-specific performance based on changes in the layer width of the model's shared structure.</p>
<p>2.</p>
<p>We evaluated the performance improvements resulting from applying GradNorm and the attention mechanism to the baseline 2D-CNN architecture.</p>
<p>3.</p>
<p>We examined the model's performance when using downsampling to address class imbalance and prevent overfitting caused by data redundancy.</p>
<p>In the first evaluation, we analyzed the F1-score for each task based on changes in the convolutional structure of the shared layer (see Table 6).As shown in Table 6, increasing the width of the shared layer consistently improved performance across all tasks.For the second evaluation, we measured the performance improvements from applying GradNorm and the attention mechanism to the baseline 2D-CNN architecture.The results show that adding GradNorm to the 2D-CNN significantly improved the model's performance.Furthermore, applying both GradNorm and the attention mechanism yielded the highest performance.While the F1-score for Task 2 in the baseline 2D-CNN (32-64-128) was comparable to that of the MTL-DoHTA model, the F1-scores for Task 1 and Task 3 were noticeably higher with MTL-DoHTA.</p>
<p>In the third evaluation, we explored the performance of both the baseline model and MTL-DoHTA with varying downsampling rates (see Table 7).Downsampling offers advantages such as addressing class imbalance, reducing computational resources, and preventing overfitting.However, it can also lead to performance degradation due to insufficient training data.Despite this, as shown in Table 7, the MTL-DoHTA model maintained robust performance even with a downsampling percentage of 50%.Specifically, addressing class imbalance through downsampling resulted in only a minor decrease in F1-score, with an average difference of just 0.003 compared to using the full dataset.This demonstrates that MTL-DoHTA effectively mitigates the impact of downsampling.8, the highest macro-averaging F1-score of 0.9905 was achieved when applying the 2D-CNN + Grad-Norm + Attention mechanism with static attention weights based on feature importance.Furthermore, it can be observed that increasing the width of the shared layer consistently improved the model's performance.</p>
<p>The proposed model in this study was evaluated for prediction time performance.The dataset used for training and prediction did not undergo downsampling, and the full dataset was used for evaluation.From the full dataset, 20% (231,822 flows) was selected, and the prediction process was repeated 1000 times to calculate the average prediction time.The average prediction time was recorded as 0.021379 s.</p>
<p>Comparison with Other Methods</p>
<p>Along with our proposed MTL-DoHTA model with 2D-CNN, GradNorm, and Static attention weight (feature importance), we have also compared it with other study methods.As shown in Table 9, we confirmed that our proposed MTL-DoHTA model has subtle differences from different studies in terms of performance on Task 1 and Task 2. However, in other studies, the F1-score of Task 3, which was not focused on, was high at 0.9837.Since the study by Liu et al. [5] used an algorithm that applied few shots, it is somewhat limited to directly comparing the performance with this paper's algorithm.In addition, we confirmed the differences between our study and previous studies through a comparative analysis of model complexity, scalability, and the number of model parameters (see Table 10).MontazeriShatoori et al. [12] used approximately 37,000 parameters, showing that the model size is relatively smaller than the algorithms of other studies.Our proposed model uses 105,546 parameters, and the total model size is approximately 450KB, a small resource requirement that allows the model to be sufficiently executed on embedded devices.Although many methodologies and algorithms have been proposed in previous studies, they were not shown in Table 10 because the model size of machine learning algorithms varies depending on the learning criteria of the records.When looking at the complexity of the model, excluding the number of parameters, deep learning-based methods have a part where the complexity of the model changes depending on the layer width and depth settings.Accordingly, the model proposed in this study is relatively different.Compared to deep learning algorithms, it has low complexity, and in the case of deep learning algorithms with simple structures, the model complexity can be expressed as middle.In addition, since it is more free to update the model output layer in a new environment or when a new task appears, it has been shown to have high scalability in this study.However, general machine learning-based algorithms are performed as a single task even when performing multi-class classification, and there is a cumbersome part in that a new algorithm must be re-learned for other tasks, so in general, the model appears to have low scalability.</p>
<p>Performance Comparison in Two Datasets</p>
<p>To evaluate the generalization performance of the proposed MTL-DoHTA model, experiments were conducted on two datasets: the DNS Over HTTPS network traffic [25] and CIRA-CIC-DoHBrw-2020 and DoH-Tunnel-Traffic-HKD combined dataset [26,27].The first dataset, the IEEE Dataport Dataset, will be referred to as Dataset 1 for simplicity.Similarly, the second dataset, CIRA-CIC-DoHBrw-2020 and DoH-Tunnel-Traffic-HKD combined dataset will be referred to as Dataset 2 throughout the remainder of this section.</p>
<p>Dataset 1 was used to assess the model's performance across various tasks, while the DoH Tunnel Traffic HKD dataset introduced a new DNS tunneling technique to evaluate the model's adaptability to novel threats.For Dataset 1, we pre-trained the MTL-DoHTA model and fine-tuned it over 100 epochs to ensure sufficient training.The model achieved an average F1-score of 0.9863, with Task 1 scoring 0.9841 and Task 3 scoring 0.9907.This dataset, which supports flow-based processing with Pcap and includes diverse DNS resolvers, allowed comprehensive evaluations for Task 1 and Task 3.However, due to the absence of malicious DoH tunneling tools, Task 2 evaluations were limited in scope and feasibility.Dataset 2, which augments the CIC-DoHBRW-2020 dataset by including a new type of malicious DoH tunneling tool, was used to assess the model's performance specifically for Task 3. Since Task 1 and Task 2 of Dataset 2 align with the existing CIC-DoHBRW-2020 dataset, fine-tuning was limited to 10 epochs to preserve computational efficiency.The model achieved an impressive F1-score of 0.9996 for Task 3, demonstrating its effectiveness in identifying new tunneling techniques.</p>
<p>Conclusions</p>
<p>The proposed MTL-DoHTA framework effectively classifies DNS over HTTPS (DoH) traffic across three tasks: (1) differentiating DoH vs. non-DoH traffic, (2) classifying benign vs. malicious DoH traffic, and (3) identifying DNS tunneling tools such as dns2tcp, dnscat2, and iodine.By leveraging statistical features and a simple 2D-CNN architecture, MTL-DoHTA achieves a macro-averaging F1-score of 0.9905 on the CIC-DoHBrw-2020 dataset, outperforming GradNorm and static attention-based methods and thus demonstrating robustness and adaptability.</p>
<p>Despite these achievements, applying MTL-DoHTA in real-time environments presents challenges due to the reliance on pre-extracted features and the computational complexity of the network, potentially hindering deployment in latency-sensitive scenarios.Moreover, retraining is required to adapt to novel or evolving DNS tunneling tools.To address these limitations, future work will prioritize lightweight model optimization and explore continual learning approaches to enhance real-time detection, adaptability, and scalability.Our next experimental phase will also include testing with a real-nature dataset and additional tasks to further validate the model's performance under diverse conditions.</p>
<p>Figure 1 .
1
Figure 1.Overview of MTL-DoHTA framework.The * in '.pcap' represents all file names.</p>
<p>Figure 3 .
3
Figure 3. Statistical features to image.</p>
<p>•Figure 4 .
4
Figure 4. Structure of MTL-DoHTA.</p>
<p>•</p>
<p>Learning rate: [1 × 10 −4 , 1 × 10 −2 ]; • Batch size: [16, 32, 64]; • Dropout rate: [0.1, 0.5].</p>
<p>Table 1 .
1
Literature review via characteristics and limitations related to DoH.
Study Research ThemesCharacteristicsLimitationsTask 1Task 2Task 3Preliminary investigations exhibit[1]Investigating preliminary iden-tification methodologies for the recognition of DoH utilization.Acknowledge the significance of methodologies. early detection of DoH traffic and elucidate fundamental detectioninadequate detection precision, hensive examination of malevo-and there exists an absence of sys-tematic classification and compre-VXXlent traffic.[10]Centers on advanced visual-ization (Eigen Centrality, PCA, GMM) and dataset exploration for IDS enhancements in DoH-based cyber threats.Demonstrates the importance of Layer 3 data and realistic threat simulation to inform the IDS models. development of more effectiveDataset imbalances, inconsistent of task scalability classification performance across varied methods, and limitationsXVX[4]Analyze how encrypted DNS queries are used in botnets and other malicious activitiesPresenting techniques and coun-DoH termeasures for botnet activities and DNS tunneling exploitingDiscuss misuse cases rather than detection techniquesVVXProposal of a DNS tunnelingdetection framework that in-[5]tegrates model-agnostic meta-learning and Convolutional Neu-ral Networks.</p>
<p>Table 1 .
1
Cont.
Study Research ThemesCharacteristicsLimitationsTask 1Task 2Task 3Research on methodologies for[13]DNS covert channel detection with Multi-layer perceptron, Multi-Head Attention, and Resid-Feature fusion of session feature and sequence featureSingle tasks in multi-class classi-scalability fication and limitation of model'sVVVual Neural Networks[14]Focuses on simple recurrent neu-ral network multi-stage classifica-tion (Task 1 and Task 2) for mali-cious DoH detection.Employs the CIC-DoHBrw-2020 dataset with LSTM/GRU models, emphasizing preprocessing, class layer classification. imbalance handling, and two-Single tasks in RNNs algorithms scalability (LSTM, GRU, deepRNN, and biL-STM) and limitation of model'sVVXFocuses on machine-learning de-Implements a PCAP-based novelReduced accuracy across di-[15]tection of malicious DoH traffic, emphasizing a two-step classifica-feature extraction and ML (e.g., LGBM) to identify malicious DoHverse datasets, limited realism in browser settings, and noVVXtion (benign vs. malicious DoH).activity.evaluation of Task 3.[16]Proposes an explainable AI frame-work using a balanced Random Forest to accurately detect and classify malicious DoH traffic.Leverages the CIC-DoHBrw-2020 dataset, achieves high metrics, ent model decisions. and employs SHAP for transpar-Lacks of large-scale deployment model's task scalability considerations, and limitationsVVX</p>
<p>Table 2 .
2
List of statistical traffic features.
CategoryNumberStatistical Feature NameDuration1Flow duration2Number of flow bytes sentNumber of bytes3 4Rate of flow bytes sent Number of flow bytes received5Rate of flow bytes received6Mean packet length7Median packet length8Mode packet lengthPacket length9 10Variance of packet length Standard deviation of packet length11Coefficient of variation of packet length12Skew from median packet length13Skew from mode packet length14Mean packet time15Median packet time16Mode packet timePacket time17 18Variance of packet time Standard deviation of packet time19Coefficient of variation of packet time20Skew from median packet time21Skew from mode packet time22Mean request/response time difference23Median request/response time difference24Mode request/response time differenceRequest/response25Variance of request/response time differencetime difference26Standard deviation of request/response time difference27Coefficient of variation of request/response time difference28Skew from median request/response time difference29Skew from mode request/response time difference</p>
<p>5x5 2d Normalized Image Normalization of Statistical Image data Chrome DoH dns2tcp</p>
<p>)
5x5 2d Image54.000.42136.00 1996.00 0.120.000.010.020.010.000.10365.401.290.65 3654.800.000.060.280.890.01472.920.25 7139.00 0.210.10𝒙 ′ =𝒙 − 𝒙 𝒎𝒊𝒏 𝒙 𝒎𝒂𝒙 − 𝒙 𝒎𝒊𝒏0.050.000.000.100.100.0001.4502.2901.0200.690.000.880.320.660.0100.430.06 6783.43 0.000.040.040.010.000.000.04</p>
<p>Table 3 .
3
CIC-DoHBrw-2020 dataset.
Benign-DoHNon-DoHMalicious DoHBrowsers/ToolsGoogle Chrome/Mozilla FirefoxGoogle Chrome/Mozilla Firefoxiodinednscat2 dns2tcpNumber of Flows19,807897,49346,61335,622167,515</p>
<p>Table 4 .
4
Experimental settings.
CategoryExperimental EnvironmentOperating systemWindows 11ProcessorIntel (R) Core™ i9-14900KFGPUGeForce RTX 4090Programming language and versionPython 3.9LibraryPytorch, scikit-learn</p>
<p>Table 5 .
5
Structural parameters in MTL-DoHTA.
StructureLayerOperationInputOutputConv2D2D Convolution (32 filters, 3 kernels, 1 padding)5 × 5 × 15 × 5 × 32ReLU + MaxPoolReLU Activation + Max Pooling (2 kernels)5 × 5 × 322 × 2 × 32Conv2D2D Convolution (64 filters, kernel = 3, padding = 1)2 × 2 × 322 × 2 × 64ReLUReLU Activation2 × 2 × 642 × 2 × 64Shared LayersConv2D2D Convolution (128 filters, kernel = 3, padding = 1) 2 × 2 × 642 × 2 × 128ReLUReLU Activation2 × 2 × 128 2 × 2 × 128Global Average Pooling Pooling over spatial dimensions2 × 2 × 128 128 × 1Fully Connected (fc1)Linear transformation + ReLU + Dropout128 × 164 × 1Fully Connected (fc2)Linear transformation + ReLU + Dropout64 × 129 × 1Task 1 AttentionWeighted Attention using Task Importance29 × 129 × 1Task-Specific AttentionTask 2 AttentionWeighted Attention using Task Importance29 × 129 × 1Task 3 AttentionWeighted Attention using Task Importance29 × 129 × 1Task 1 HeadLinear transformation29 × 11 × 1Task-Specific HeadsTask 2 HeadLinear transformation29 × 11 × 1Task 3 HeadLinear transformation (Softmax)29 × 15 × 1</p>
<p>Table 6 .
6
Performance of F1-score comparison changing layer structure and function adaptation.
MTL-DoHTAShared layer2D-CNN2D-CNN + GradNorm(2D-CNN + GradNorm(layer 1-2-3)+ Attention)Each TaskAverageEach TaskAverageEach TaskAverageTask 1: 0.9785Task 1: 0.9765Task 1: 0.986816-32-64Task 2: 0.99510.9817Task 2: 0.99220.9775Task 2: 0.99680.9863Task 3: 0.9715Task 3: 0.9638Task 3: 0.9754Task 1: 0.9823Task 1: 0.9849Task 1: 0.983432-32-64Task 2: 0.99630.9852Task 2: 0.99780.9871Task 2: 0.99690.9860Task 3: 0.9769Task 3: 0.9785Task 3: 0.9777Task 1: 0.9838Task 1: 0.9864Task 1: 0.989132-64-128Task 2: 0.99880.9871Task 2: 0.99840.9881Task 2: 0.99880.9905Task 3: 0.9786Task 3: 0.9794Task 3: 0.9837</p>
<p>Table 7 .
7
Performance evaluation (F1-score) by downsampling rate.In the 2D-CNN, we choose the shared layer's width 16-32-64 to set the baseline.Table 8 presents the performance comparison results for different attention weight selection strategies applied in the MTL-DoHTA model.As shown in the Table
ModelTasks10%Downsampling Percentage 20% 30% 40%50%Task 10.98740.98570.98580.98600.98412D-CNNTask 20.99680.99750.98680.99790.9946Task 30.97760.97880.97400.97530.9699MTL-DoHTA (2D-CNN + GradNorm + Attention)Task 1 Task 2 Task 30.9853 0.9982 0.97860.9855 0.9977 0.97860.9852 0.9978 0.97630.9865 0.9981 0.97830.9855 0.9983 0.9787</p>
<p>Table 8 .
8
Macro-averaging F1-score of MTL-DoHTA with attention weights (baseline: 2D-CNN + GradNorm).
Shared Layer (Layer 1-2-3)BaselineBaseline + Dynamic Attention WeightBaseline + Static Attention Weight (Average Weight Value)Baseline + Static Attention Weight (Feature Importance)16-32-640.97750.98190.98320.986332-32-640.98710.98460.98660.986032-64-1280.98810.98170.98790.9905</p>
<p>Table 9 .
9
Performance comparison with other methods.
PaperBest AlgorithmTask 1F1-Score Task 2Task 3Singh et al. [6]RF1.00001.0000XSingh et al. [11]Ensemble ML0.9970.9970XMontazeriShatoori et al. [12] LSTM-based0.99800.999XCasanova et al. [24]BiLSTM0.98700.9990XZebin et al. [16]Balanced Stacked RF0.99900.9990XCasanova et al. [14]BiLSTM0.99500.9900XStalder [15]ML0.99800.9890XAggarwal et al. [13]Ensemble ML0.99860.9999XLiu et al. [5]MFC-DoH (few-shot 20)XX0.9100MTL-DoHTAMTL-DoHTA0.98910.99880.9837</p>
<p>Table 10 .
10
Parameter comparison with deep learning methods.
PaperModel ComplexityScalabilityNumber of Parameter (Model)MontazeriShatoori et al. [12] MiddleMiddleabout 37,000Casanova et al. [14]MiddleMiddle72,244Liu et al. [5]HighHigh1,147,904MTL-DoHTAHighHigh105,546
The effectiveness of the 2D-CNN-based MTL-DoHTA model was validated through comparisons with baseline models.However, the model's complex structure imposes limitations on real-time performance, which remains an area for improvement.Future research will focus on enhancing both the accuracy and real-time efficiency of the proposed approach by investigating more compact network designs and incremental training strategies.Data Availability Statement:The data presented in this study are available on request from the corresponding author.Funding: This research was supported by Hallym University Research Fund, 2021 (HRF-202110-009).Institutional Review Board Statement: Not applicable.Informed Consent Statement: Not applicable.Author Contributions: Conceptualization, B.I.K.; methodology, W.K.J.; investigation, W.K.J.; writing-original draft, W.K.J.; writing-review &amp; editing, B.I.K.; supervision, B.I.K.; project administration, B.I.K.All authors have read and agreed to the published version of the manuscript.Conflicts of Interest:The authors declare no conflicts of interest.
D Hjelm, A New Needle and Haystack: Detecting DNS over HTTPS Usage; SANS Institute, Information Security Reading Room. Bethesda, MD, USA2019</p>
<p>DNS Queries over HTTPS (DoH). P Hoffman, P Mcmanus, Internet Engineering Task Force. 2018Technical Report</p>
<p>A Longitudinal,{End-to-End} View of the {DNSSEC} Ecosystem. T Chung, R Van Rijswijk-Deij, B Chandrasekaran, D Choffnes, D Levin, B M Maggs, A Mislove, C Wilson, Proceedings of the 26th USENIX Security Symposium (USENIX Security 17). the 26th USENIX Security Symposium (USENIX Security 17)Vancouver, BC, CanadaAugust 2017</p>
<p>Encrypted and covert DNS queries for botnets: Challenges and countermeasures. C Patsakis, F Casino, V Katos, 10.1016/j.cose.2019.101614Comput. Secur. 882020. 101614</p>
<p>DoH Tunnel Detection Based on the Fusion of MAML and F-CNN. X Liu, Y Zhang, X Yang, W Gai, B Sun, Mfc-Doh, Proceedings of the 21st ACM International Conference on Computing Frontiers. the 21st ACM International Conference on Computing FrontiersIschia, ItalyMay 2024</p>
<p>Detecting malicious dns over https traffic using machine learning. S K Singh, P K Roy, Proceedings of the 2020 International Conference on Innovation and Intelligence for Informatics, Computing and Technologies (3ICT). the 2020 International Conference on Innovation and Intelligence for Informatics, Computing and Technologies (3ICT)Sakheer, Bahrain20-21 December 2020</p>
<p>Detecting DNS over HTTPS based data exfiltration. M Zhan, Y Li, G Yu, B Li, W Wang, Comput. Netw. 2022, 209, 108919</p>
<p>A deep dive into cybersecurity solutions for AI-driven IoT-enabled smart cities in advanced communication networks. J Ali, S K Singh, W Jiang, A M Alenezi, M Islam, Y I Daradkeh, A Mehmood, 10.1016/j.comcom.2024.108000Comput. Commun. 2292025. 108000</p>
<p>MASiNet: Network Intrusion Detection for IoT Security Based on Meta-Learning Framework. Y Wu, G Lin, L Liu, Z Hong, Y Wang, X Yang, Z L Jiang, S Ji, Z Wen, 10.1109/JIOT.2024.3395629IEEE Internet Things J. 2024</p>
<p>Visualizing realistic benchmarked IDS dataset: CIRA-CIC-DoHBrw-2020. M H M Yusof, A A Almohammedi, V Shepelev, O Ahmed, 10.1109/ACCESS.2022.3204690IEEE Access. 102022</p>
<p>Malicious traffic detection of DNS over https using ensemble machine learning. S K Singh, P K Roy, 10.12785/ijcds/110185Int. J. Comput. Digit. Syst. 2022</p>
<p>Detection of doh tunnels using time-series classification of encrypted traffic. M Montazerishatoori, L Davidson, G Kaur, A H Lashkari, Proceedings of the 2020 IEEE Intl Conf on Dependable, Autonomic and Secure Computing, Intl Conf on Pervasive Intelligence and Computing, Intl Conf on Cloud and Big Data Computing, Intl Conf on Cyber Science and Technology Congress. the 2020 IEEE Intl Conf on Dependable, Autonomic and Secure Computing, Intl Conf on Pervasive Intelligence and Computing, Intl Conf on Cloud and Big Data Computing, Intl Conf on Cyber Science and Technology CongressCalgary, AB, CanadaAugust 2020</p>
<p>An ensemble framework for detection of DNS-Over-HTTPS (DOH) traffic. A Aggarwal, M Kumar, 10.1007/s11042-023-16956-9Multimed. Tools Appl. 832024</p>
<p>Malicious Network Traffic Detection for DNS over HTTPS using Machine Learning Algorithms. L F G Casanova, L Po-Chiang, APSIPA Trans. Signal Inf. Process. 12e112023</p>
<p>Machine-Learning Based Detection of Malicious DNS-over-HTTPS (DoH) Traffic Based on Packet Captures. D Stalder, 2021Zürich, SwitzerlandUniversity of ZurichBachelor's Thesis</p>
<p>An explainable AI-based intrusion detection system for DNS over HTTPS (DoH) attacks. T Zebin, S Rezvy, Y Luo, 10.1109/TIFS.2022.3183390IEEE Trans. Inf. Forensics Secur. 172022</p>
<p>CIRA-CIC-DoHBrw-2020. M Montazerishatoori, L Davidson, G Kaur, A H Lashkari, 2020. January 2025</p>
<p>Comparative analysis of DNS over HTTPS detectors. K Jerabek, K Hynek, O Rysavy, 10.1016/j.comnet.2024.110452Comput. Netw. 2472024. 110452</p>
<p>Ryšav ỳ, O. Collection of datasets with DNS over HTTPS traffic. Data Brief. K Jeřábek, K Hynek, T Čejka, 10.1016/j.dib.2022.1083102022. 10831042</p>
<p>Random forests. L Breiman, 10.1023/A:1010933404324Mach. Learn. 452001</p>
<p>Gradnorm: Gradient normalization for adaptive loss balancing in deep multitask networks. Z Chen, V Badrinarayanan, C Y Lee, A Rabinovich, Proceedings of the International Conference on Machine Learning. the International Conference on Machine LearningStockholm, Sweden10-15 July 2018</p>
<p>Neural machine translation by jointly learning to align and translate. D Bahdanau, arXiv:1409.04732014</p>
<p>A next-generation hyperparameter optimization framework. T Akiba, S Sano, T Yanase, T Ohta, M Koyama, Optuna, Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data MiningAnchorage, AK, USA4-8 August 2019</p>
<p>Generalized classification of DNS over HTTPS traffic with deep learning. L F G Casanova, P C Lin, Proceedings of the 2021 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC). the 2021 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)Tokyo, JapanDecember 2021</p>
<p>. K Jeřábek, S Stuchlý, Network Traffic, 2021. January 2025</p>
<p>Malicious DNS tunnel tool recognition using persistent DoH traffic analysis. R Mitsuhashi, Y Jin, K Iida, T Shinagawa, Y Takai, 10.1109/TNSM.2022.3215681IEEE Trans. Netw. Serv. Manag. 202022</p>
<p>CIRA-CIC-DoHBrw-2020 and DoH-Tunnel-Traffic-HKD Combined Dataset. M Montazerishatoori, L Davidson, G Kaur, A H Lashkari, 2022. January 2025</p>
<p>Disclaimer/Publisher's Note: The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods. instructions or products referred to in the content</p>            </div>
        </div>

    </div>
</body>
</html>
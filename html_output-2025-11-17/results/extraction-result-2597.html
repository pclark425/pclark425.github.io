<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2597 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2597</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2597</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-67.html">extraction-schema-67</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <p><strong>Paper ID:</strong> paper-2e2089ae76fe914706e6fa90081a79c8fe01611e</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/2e2089ae76fe914706e6fa90081a79c8fe01611e" target="_blank">Practical Bayesian Optimization of Machine Learning Algorithms</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> This work describes new algorithms that take into account the variable cost of learning algorithm experiments and that can leverage the presence of multiple cores for parallel experimentation and shows that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization for many algorithms.</p>
                <p><strong>Paper Abstract:</strong> The use of machine learning algorithms frequently involves careful tuning of learning parameters and model hyperparameters. Unfortunately, this tuning is often a "black art" requiring expert experience, rules of thumb, or sometimes brute-force search. There is therefore great appeal for automatic approaches that can optimize the performance of any given learning algorithm to the problem at hand. In this work, we consider this problem through the framework of Bayesian optimization, in which a learning algorithm's generalization performance is modeled as a sample from a Gaussian process (GP). We show that certain choices for the nature of the GP, such as the type of kernel and the treatment of its hyperparameters, can play a crucial role in obtaining a good optimizer that can achieve expertlevel performance. We describe new algorithms that take into account the variable cost (duration) of learning algorithm experiments and that can leverage the presence of multiple cores for parallel experimentation. We show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization for many algorithms including latent Dirichlet allocation, structured SVMs and convolutional neural networks.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2597.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2597.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bayesian optimization (GP)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bayesian optimization with Gaussian process priors</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A global optimization framework for expensive-to-evaluate black-box functions that models the objective with a Gaussian process (GP) posterior and selects new evaluations by optimizing an acquisition function derived from that posterior.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Bayesian optimization with Gaussian process priors</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Maintains a Gaussian process posterior over the unknown objective f(x), computes predictive mean μ(x) and variance σ^2(x), and selects the next evaluation point x_next by maximizing an acquisition function a(x) (e.g., expected improvement, probability of improvement, or GP-UCB). The GP has a covariance (kernel) with hyperparameters (length-scales, amplitude, noise) which can be treated either by point-estimation (marginal likelihood optimization) or by full Bayesian marginalization (MCMC).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Hyperparameter tuning and general expensive black-box optimization in machine learning (e.g., neural networks, LDA, structured SVMs); generically applicable to experimental design where evaluations are costly.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>At each decision step, allocate the next experiment to the x that maximizes an acquisition function computed from the GP posterior (balances predicted improvement and uncertainty). The approach may be extended to account for evaluation cost and parallel pending evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Wall-clock time for evaluations / duration of each experiment (the paper models the duration function c(x) and often reports wall-clock days or hours); number of function evaluations is also tracked.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Proxy via acquisition functions derived from GP posterior (expected improvement, probability of improvement, or reduction in regret via confidence bounds); not explicit mutual information but uses predictive mean/variance to quantify potential utility.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Achieved via acquisition functions: EI uses σ(x) and predicted improvement (combines exploration and exploitation), PI focuses on probability of improvement, GP-UCB explicitly trades off mu and sigma via a tunable κ term.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>No explicit diversity-promoting mechanism beyond the intrinsic exploration term in acquisition functions (high predictive variance encourages trying diverse/hypothesis-different points).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Number of expensive evaluations, wall-clock time, and available parallel compute resources (cores/machines).</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Handles budget by selecting points that maximize acquisition under assumed constraints; extensions include modeling cost and optimizing acquisition per unit cost and Monte Carlo accounting for pending parallel evaluations to reduce wallclock time.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Improvement over the current best observed value (expected improvement); large EI indicates high potential breakthrough relative to current best.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported metrics include number of function evaluations to reach minima, wall-clock time (hours/days), and downstream model performance (e.g., CIFAR-10 test error 14.98% achieved by GP EI MCMC vs 18% expert baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Grid search / randomized grid, Tree Parzen Algorithm (TPA), human expert tuning, point-estimate GP EI, and GP-UCB.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>GP-based Bayesian optimization (particularly with fully Bayesian hyperparameter marginalization) outperforms grid search and TPA on benchmarks; e.g., on Branin-Hoo GP EI (marginalized) finds minima in fewer than half as many evaluations as TPA; on CIFAR-10 GP EI MCMC produced 14.98% test error vs 18% for expert tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Examples reported: 'fewer than half as many evaluations' on Branin-Hoo relative to TPA; grid search for LDA would require ~60–120 processor-days (288 configs × ~5–10 hours each) while Bayesian optimization reached better parameters using a fraction of that; GP EI MCMC improved test error by ≈3.02 percentage points absolute on CIFAR-10 relative to expert.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>The paper analyzes trade-offs between extra computation to choose the next point (GP inference and acquisition optimization) versus expensive evaluations, argues that extra computation is worthwhile. It studies how modeling cost (duration) changes selection behavior (prefers faster-evaluated promising points) and how parallelization using fantasies reduces wall-clock time at the cost of handling uncertainty about pending results.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Key recommendations: (1) Use a fully Bayesian treatment (marginalize GP hyperparameters) for robust acquisition; (2) model evaluation cost and optimize expected improvement per unit cost to improve wall-clock performance; (3) account for pending parallel evaluations by integrating acquisition under 'fantasies' (Monte Carlo) to avoid duplicate experiments; (4) prefer Matérn 5/2 ARD kernel over squared exponential for realistic function smoothness assumptions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Practical Bayesian Optimization of Machine Learning Algorithms', 'publication_date_yy_mm': '2012-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2597.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2597.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Expected Improvement (EI) acquisition function</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An acquisition function that selects the next evaluation by maximizing the GP-predicted expected improvement over the current best observed value, combining both predicted mean and uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Expected Improvement acquisition</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Computes the expected positive difference between the GP predictive distribution at x and the current best observation; closed-form for Gaussian predictive distribution: a_EI(x)=σ(x)[γ(x)Φ(γ(x))+φ(γ(x))], where γ uses f(x_best), μ(x) and σ(x). The paper implements EI as the principal acquisition function and integrates it over GP hyperparameters (Monte Carlo) to produce integrated EI.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Hyperparameter optimization and expensive black-box optimization tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocate experiments to the x maximizing expected improvement; when cost-aware, allocate to maximize EI per second.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>When extended, wall-clock duration modeled as c(x); base EI itself does not include cost but the paper extends EI to EI per second.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Expected improvement (a utility tied to likely reduction in objective value), computed from predictive mean and variance.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Balances exploitation (μ(x) low relative to best) and exploration (σ(x) large) via EI formula; no extra tuning parameter needed.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Implicit via exploration term σ(x) which encourages sampling uncertain (diverse) areas; no explicit diversity constraint.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Number of evaluations and (when extended) wall-clock time.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Extended by dividing EI by predicted cost (EI per second) to favor cheaper-to-evaluate promising points; when parallelizing, use Monte Carlo integration over pending outcomes to avoid wasted parallel resources.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Large expected improvement indicates potential for breakthrough over current best.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Used across experiments to drive selection; empirical comparisons show EI (with GP hyperparameter marginalization) outperforms alternatives in terms of evaluations to reach minima and leads to better hyperparameter selections.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Probability of Improvement (PI), GP-UCB, Tree Parzen Algorithm, grid search, human expert tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Integrated EI (marginalized over GP hyperparameters) outperforms point-estimate EI and TPA on Branin-Hoo and ML hyperparameter tasks; e.g., on Branin-Hoo integrated EI + GP marginalization found minima with fewer than half the evaluations of TPA.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>As above: fewer than half the number of evaluations vs TPA on Branin-Hoo; empirically superior sample efficiency in multiple ML tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper shows marginalizing GP hyperparameters changes EI behavior and is beneficial; also that EI per unit cost trades evaluation quality for speed, achieving better wall-clock performance at potential extra evaluation count.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Use EI integrated over hyperparameters (via MCMC) for robustness; when evaluation durations vary, use EI normalized by predicted evaluation time to improve wall-clock efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Practical Bayesian Optimization of Machine Learning Algorithms', 'publication_date_yy_mm': '2012-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2597.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2597.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GP EI MCMC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GP Expected Improvement with MCMC (Integrated EI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A specific implementation that marginalizes GP hyperparameters using MCMC (slice sampling) and computes an integrated expected improvement acquisition by Monte Carlo averaging EI over hyperparameter samples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GP EI MCMC (Integrated Expected Improvement)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Samples from the posterior p(θ | data) over GP hyperparameters using slice sampling (MCMC), computes EI( x | θ ) for each sample, and Monte Carlo-averages these to obtain an integrated acquisition â(x). This accounts for hyperparameter uncertainty and produces more robust acquisition functions and selection decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Hyperparameter optimization for machine learning models (neural nets, LDA, structured SVMs) and any expensive black-box experiment where GP hyperparameter uncertainty matters.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Select next experiment by maximizing integrated EI (Monte Carlo average of EI under posterior hyperparameter samples), thereby allocating resources to points with high expected improvement averaged over model uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Wall-clock time for evaluations; computational overhead of MCMC and GP inference (cubic in observations) is acknowledged but considered small relative to expensive function evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Integrated expected improvement (Monte Carlo average of EI across GP hyperparameter posterior).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Same EI tradeoff, but averaged over hyperparameter uncertainty which can increase exploration when hyperparameter posterior yields high variance predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>No separate diversity module; increased exploration may arise from hyperparameter uncertainty in the GP posterior.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Number of function evaluations and wall-clock time; can be parallelized (N x GP EI MCMC) to reduce wall-clock time.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Implements integrated EI to make more informed selections per evaluation; when parallelized, uses Monte Carlo fantasies to select further points while others are pending.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Integrated expected improvement reflects breakthrough potential averaged under model-hyperparameter uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Empirically shown to be most sample-efficient in many tasks (e.g., Branin-Hoo, logistic regression), and effective in exploratory ML hyperparameter tuning tasks; used as primary method in multiple experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>GP EI Opt (point-estimate EI), TP Algorithm, grid search, parallelized variants (N x GP EI MCMC), GP-UCB.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>GP EI MCMC outperforms GP EI Opt (point-estimate) and TPA on benchmark tasks; provides more robust behavior and better sample efficiency (e.g., Branin-Hoo results and ML tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Reported superior sample efficiency; concrete examples include fewer than half the evaluations vs TPA on Branin-Hoo and better final model performance on CIFAR-10 with fewer expensive tuning iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Demonstrates that marginalizing GP hyperparameters (instead of point-estimating) materially improves acquisition behavior and final performance; acknowledges increased computation for marginalization but deems it small versus evaluation cost.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Recommendation to marginalize GP hyperparameters (use MCMC) when evaluations are expensive because integrated acquisition leads to more robust and effective allocation of experimental resources.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Practical Bayesian Optimization of Machine Learning Algorithms', 'publication_date_yy_mm': '2012-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2597.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2597.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EI per Second</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Expected Improvement per Second (cost-aware EI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cost-aware acquisition that divides expected improvement by predicted evaluation duration, prioritizing points that yield good expected utility relative to their cost (wall-clock time).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Expected Improvement per Second (EI / predicted cost)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Models the log-duration c(x) of each candidate evaluation with a separate GP (assumed independent from f(x)), predicts expected inverse duration, and computes acquisition as EI(x) multiplied/divided by the predicted evaluation speed to prefer selections that are both promising and fast to evaluate.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Scenarios with variable evaluation costs (e.g., ML hyperparameter configurations where larger models take longer), where wall-clock time or monetary cost matters.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocate experiments by maximizing expected improvement per unit cost (per second), thereby biasing selection toward promising configurations that are fast to evaluate to improve wall-clock efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Wall-clock evaluation duration (seconds/hours); the paper models ln c(x) with a GP to predict durations.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Expected improvement (EI) over current best (as a proxy for information/value), combined with predicted evaluation time to yield EI per second.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Same as EI, but preference shifts toward cheaper points; encourages exploration of low-cost regions that still promise improvement, possibly trading sample efficiency for wall-clock speed.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>No explicit diversity mechanism; selection may become biased toward cheaper regions which could reduce diversity unless EI still favors diverse high-uncertainty points.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Wall-clock time and computational resource cost (implicitly monetary if translated).</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Incorporates predicted duration into acquisition via modeling ln c(x) with a GP and using EI per second to select points under wall-clock/time budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>High EI per second indicates good potential for impactful improvement within limited time budget.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Empirically, GP EI per Second finds better parameters faster in wall-clock time on tasks such as motif finding (3x GP EI per second found best parameters fastest in wall-clock despite requiring more function evaluations).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>GP EI MCMC (non-cost-aware), randomized grid search, 3x-parallel variants.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>GP EI per Second often achieves superior wall-clock performance (faster attainment of good parameters) compared to non-cost-aware GP EI MCMC even when it may use more function evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Example: In motif-finding experiments, GP EI per Second finds better parameters faster in wall-clock time than GP EI MCMC though it may perform more evaluations; exact numerical gains are presented in figures but not always given as single numeric summaries in text.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper explicitly analyzes the tradeoff: EI per second sacrifices raw evaluation-sample efficiency (may perform more evaluations) in exchange for better wall-clock progress by favoring fast-to-evaluate promising points.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>When evaluations vary widely in duration, optimizing EI per unit cost is a practical strategy to reduce wall-clock time to good solutions; the duration function can be learned with a GP on ln c(x).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Practical Bayesian Optimization of Machine Learning Algorithms', 'publication_date_yy_mm': '2012-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2597.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2597.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MC Parallel Acquisition</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Monte Carlo acquisition with fantasies for parallel/asynchronous evaluations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method to select new evaluation points while other evaluations are pending by integrating the acquisition function over the distribution of possible outcomes (fantasies) for pending evaluations using Monte Carlo sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Monte Carlo acquisition under pending evaluations (fantasy-based parallelization)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>When J evaluations are pending at locations {x_j}, compute the expected acquisition for a candidate x by integrating a(x) over the joint predictive distribution of the pending outcomes y_j (a J-dimensional Gaussian). Approximate this integral via Monte Carlo sampling ('fantasies') of pending y_j, compute acquisition conditioned on each fantasy, and average to obtain an acquisition that accounts for uncertainty in pending results, thus enabling asynchronous parallel selection without duplicate work.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Parallel/asynchronous experimental scheduling for expensive evaluations (ML hyperparameter tuning on multi-core/cloud resources).</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocates next experiments by maximizing the acquisition averaged over sampled 'fantasy' outcomes of pending evaluations, thereby accounting for possible future information and reducing redundant parallel experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Wall-clock time, utilization of multiple cores; Monte Carlo overhead for acquisition estimation is relatively small compared to evaluation time.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Expected acquisition (e.g., EI) under the posterior predictive distribution of pending evaluations; effectively expected improvement conditioned on possible pending outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Acquisition under fantasies balances exploration/exploitation as in the chosen base acquisition (EI, PI), but the fantasy averaging avoids selecting redundant points and can encourage exploration complementary to pending locations.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Implicitly promotes diversity by conditioning on pending points' possible outcomes, thereby discouraging repeated sampling of similar points; no explicit diversity metric.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Parallel compute cores / asynchronous evaluation slots and wall-clock time.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Handles parallel budgets by selecting non-redundant candidate points while others are running, using Monte Carlo sampling of pending outcomes to estimate expected acquisition and reduce wasted parallel resources.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Acquisition (e.g., integrated EI) averaged over pending outcomes indicates potential for breakthroughs given current and pending information.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Empirically shown to reduce wall-clock time to good solutions when parallelism is available (parallelized GP EI MCMC finds best parameters in significantly less time in experiments such as online LDA).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Sequential selection that ignores pending evaluations (which can duplicate experiments), naive batch selection, and previously-discussed approaches (e.g., Ginsbourger & Riche notionally).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Parallelized GP EI MCMC using Monte Carlo fantasies reaches good solutions in significantly less wall-clock time than sequential procedures; in online LDA experiments parallelized variants find best parameters faster.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Reported wall-clock speedups in experiments (e.g., parallelized GP EI MCMC finds best parameters 'in significantly less time' for online LDA compared to sequential); exact multipliers depend on available parallelism (e.g., 3x-parallel variants illustrated).</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Shows that accounting for pending evaluations via fantasies reduces wasted parallel work and accelerates wall-clock convergence; introduces Monte Carlo overhead but this is small compared to expensive evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>When experiments run in parallel, selecting new points by integrating acquisition over possible pending outcomes (via Monte Carlo fantasies) yields better use of parallel resources and faster attainment of good solutions than naive re-use of the same acquisition function.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Practical Bayesian Optimization of Machine Learning Algorithms', 'publication_date_yy_mm': '2012-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2597.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2597.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GP-UCB</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gaussian Process Upper / Lower Confidence Bound (GP-UCB / LCB)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An acquisition strategy that selects points by optimizing a linear combination of the GP predictive mean and standard deviation to control exploration-exploitation, typically μ(x) - κ σ(x) for minimization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GP Upper/Lower Confidence Bound acquisition</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Constructs acquisition a_LCB(x)=μ(x) - κ σ(x) (for minimization) with tunable κ that explicitly trades off exploitation (μ) and exploration (σ). It has theoretical regret guarantees in bandit settings and is discussed as an alternative to EI.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Theoretical and practical global optimization and bandit-style experimental design.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Selects the next point by maximizing (or minimizing) the confidence-bound acquisition, allocating trials to points with either low predicted mean or high uncertainty depending on κ.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Not explicitly cost-aware in base form; paper discusses GP-UCB as an alternative acquisition but focuses experiments on EI.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Uses predictive variance σ(x) as the exploration term; acquisition itself is not framed as mutual information but as a confidence-bound-based utility.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Explicit via the κ parameter: larger κ encourages exploration (higher weight on σ), smaller κ encourages exploitation.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Exploration via σ(x) can encourage diverse sampling; no explicit diversity mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Number of evaluations (not cost-aware by default).</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Base GP-UCB does not include cost modeling; paper compares conceptually but does not use GP-UCB in main experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Points with low μ(x) - κ σ(x) are targeted; not explicitly framed as 'breakthrough' metric but can locate low minima.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Discussed conceptually and compared in Section 4.1 in analyses; specific performance numbers vs EI are reported in experiments but EI (marginalized) is the main focus.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Expected Improvement, Probability of Improvement, grid search, TPA.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Authors note EI performed well empirically for minimization in their settings; GP-UCB has theoretical appeal but requires tuning κ and is not the primary method used.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Not quantified in this paper beyond conceptual discussion and occasional comparisons; paper focuses on EI variants.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>GP-UCB makes the exploration-exploitation tradeoff explicit via κ; the paper emphasizes EI's practical advantages (no extra tuning parameter) while acknowledging GP-UCB's regret formalization.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>GP-UCB is conceptually appropriate in regret-minimization settings, but for their hyperparameter optimization tasks EI (especially integrated EI) was preferred due to practical behavior and lack of extra tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Practical Bayesian Optimization of Machine Learning Algorithms', 'publication_date_yy_mm': '2012-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>The application of Bayesian methods for seeking the extremum <em>(Rating: 2)</em></li>
                <li>Gaussian process optimization in the bandit setting: No regret and experimental design <em>(Rating: 2)</em></li>
                <li>A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning <em>(Rating: 2)</em></li>
                <li>Algorithms for hyper-parameter optimization <em>(Rating: 2)</em></li>
                <li>Dealing with asynchronicity in parallel Gaussian process based global optimization <em>(Rating: 2)</em></li>
                <li>Slice sampling covariance hyperparameters of latent Gaussian models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2597",
    "paper_id": "paper-2e2089ae76fe914706e6fa90081a79c8fe01611e",
    "extraction_schema_id": "extraction-schema-67",
    "extracted_data": [
        {
            "name_short": "Bayesian optimization (GP)",
            "name_full": "Bayesian optimization with Gaussian process priors",
            "brief_description": "A global optimization framework for expensive-to-evaluate black-box functions that models the objective with a Gaussian process (GP) posterior and selects new evaluations by optimizing an acquisition function derived from that posterior.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Bayesian optimization with Gaussian process priors",
            "system_description": "Maintains a Gaussian process posterior over the unknown objective f(x), computes predictive mean μ(x) and variance σ^2(x), and selects the next evaluation point x_next by maximizing an acquisition function a(x) (e.g., expected improvement, probability of improvement, or GP-UCB). The GP has a covariance (kernel) with hyperparameters (length-scales, amplitude, noise) which can be treated either by point-estimation (marginal likelihood optimization) or by full Bayesian marginalization (MCMC).",
            "application_domain": "Hyperparameter tuning and general expensive black-box optimization in machine learning (e.g., neural networks, LDA, structured SVMs); generically applicable to experimental design where evaluations are costly.",
            "resource_allocation_strategy": "At each decision step, allocate the next experiment to the x that maximizes an acquisition function computed from the GP posterior (balances predicted improvement and uncertainty). The approach may be extended to account for evaluation cost and parallel pending evaluations.",
            "computational_cost_metric": "Wall-clock time for evaluations / duration of each experiment (the paper models the duration function c(x) and often reports wall-clock days or hours); number of function evaluations is also tracked.",
            "information_gain_metric": "Proxy via acquisition functions derived from GP posterior (expected improvement, probability of improvement, or reduction in regret via confidence bounds); not explicit mutual information but uses predictive mean/variance to quantify potential utility.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Achieved via acquisition functions: EI uses σ(x) and predicted improvement (combines exploration and exploitation), PI focuses on probability of improvement, GP-UCB explicitly trades off mu and sigma via a tunable κ term.",
            "diversity_mechanism": "No explicit diversity-promoting mechanism beyond the intrinsic exploration term in acquisition functions (high predictive variance encourages trying diverse/hypothesis-different points).",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Number of expensive evaluations, wall-clock time, and available parallel compute resources (cores/machines).",
            "budget_constraint_handling": "Handles budget by selecting points that maximize acquisition under assumed constraints; extensions include modeling cost and optimizing acquisition per unit cost and Monte Carlo accounting for pending parallel evaluations to reduce wallclock time.",
            "breakthrough_discovery_metric": "Improvement over the current best observed value (expected improvement); large EI indicates high potential breakthrough relative to current best.",
            "performance_metrics": "Reported metrics include number of function evaluations to reach minima, wall-clock time (hours/days), and downstream model performance (e.g., CIFAR-10 test error 14.98% achieved by GP EI MCMC vs 18% expert baseline).",
            "comparison_baseline": "Grid search / randomized grid, Tree Parzen Algorithm (TPA), human expert tuning, point-estimate GP EI, and GP-UCB.",
            "performance_vs_baseline": "GP-based Bayesian optimization (particularly with fully Bayesian hyperparameter marginalization) outperforms grid search and TPA on benchmarks; e.g., on Branin-Hoo GP EI (marginalized) finds minima in fewer than half as many evaluations as TPA; on CIFAR-10 GP EI MCMC produced 14.98% test error vs 18% for expert tuning.",
            "efficiency_gain": "Examples reported: 'fewer than half as many evaluations' on Branin-Hoo relative to TPA; grid search for LDA would require ~60–120 processor-days (288 configs × ~5–10 hours each) while Bayesian optimization reached better parameters using a fraction of that; GP EI MCMC improved test error by ≈3.02 percentage points absolute on CIFAR-10 relative to expert.",
            "tradeoff_analysis": "The paper analyzes trade-offs between extra computation to choose the next point (GP inference and acquisition optimization) versus expensive evaluations, argues that extra computation is worthwhile. It studies how modeling cost (duration) changes selection behavior (prefers faster-evaluated promising points) and how parallelization using fantasies reduces wall-clock time at the cost of handling uncertainty about pending results.",
            "optimal_allocation_findings": "Key recommendations: (1) Use a fully Bayesian treatment (marginalize GP hyperparameters) for robust acquisition; (2) model evaluation cost and optimize expected improvement per unit cost to improve wall-clock performance; (3) account for pending parallel evaluations by integrating acquisition under 'fantasies' (Monte Carlo) to avoid duplicate experiments; (4) prefer Matérn 5/2 ARD kernel over squared exponential for realistic function smoothness assumptions.",
            "uuid": "e2597.0",
            "source_info": {
                "paper_title": "Practical Bayesian Optimization of Machine Learning Algorithms",
                "publication_date_yy_mm": "2012-06"
            }
        },
        {
            "name_short": "EI",
            "name_full": "Expected Improvement (EI) acquisition function",
            "brief_description": "An acquisition function that selects the next evaluation by maximizing the GP-predicted expected improvement over the current best observed value, combining both predicted mean and uncertainty.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Expected Improvement acquisition",
            "system_description": "Computes the expected positive difference between the GP predictive distribution at x and the current best observation; closed-form for Gaussian predictive distribution: a_EI(x)=σ(x)[γ(x)Φ(γ(x))+φ(γ(x))], where γ uses f(x_best), μ(x) and σ(x). The paper implements EI as the principal acquisition function and integrates it over GP hyperparameters (Monte Carlo) to produce integrated EI.",
            "application_domain": "Hyperparameter optimization and expensive black-box optimization tasks.",
            "resource_allocation_strategy": "Allocate experiments to the x maximizing expected improvement; when cost-aware, allocate to maximize EI per second.",
            "computational_cost_metric": "When extended, wall-clock duration modeled as c(x); base EI itself does not include cost but the paper extends EI to EI per second.",
            "information_gain_metric": "Expected improvement (a utility tied to likely reduction in objective value), computed from predictive mean and variance.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Balances exploitation (μ(x) low relative to best) and exploration (σ(x) large) via EI formula; no extra tuning parameter needed.",
            "diversity_mechanism": "Implicit via exploration term σ(x) which encourages sampling uncertain (diverse) areas; no explicit diversity constraint.",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Number of evaluations and (when extended) wall-clock time.",
            "budget_constraint_handling": "Extended by dividing EI by predicted cost (EI per second) to favor cheaper-to-evaluate promising points; when parallelizing, use Monte Carlo integration over pending outcomes to avoid wasted parallel resources.",
            "breakthrough_discovery_metric": "Large expected improvement indicates potential for breakthrough over current best.",
            "performance_metrics": "Used across experiments to drive selection; empirical comparisons show EI (with GP hyperparameter marginalization) outperforms alternatives in terms of evaluations to reach minima and leads to better hyperparameter selections.",
            "comparison_baseline": "Probability of Improvement (PI), GP-UCB, Tree Parzen Algorithm, grid search, human expert tuning.",
            "performance_vs_baseline": "Integrated EI (marginalized over GP hyperparameters) outperforms point-estimate EI and TPA on Branin-Hoo and ML hyperparameter tasks; e.g., on Branin-Hoo integrated EI + GP marginalization found minima with fewer than half the evaluations of TPA.",
            "efficiency_gain": "As above: fewer than half the number of evaluations vs TPA on Branin-Hoo; empirically superior sample efficiency in multiple ML tasks.",
            "tradeoff_analysis": "Paper shows marginalizing GP hyperparameters changes EI behavior and is beneficial; also that EI per unit cost trades evaluation quality for speed, achieving better wall-clock performance at potential extra evaluation count.",
            "optimal_allocation_findings": "Use EI integrated over hyperparameters (via MCMC) for robustness; when evaluation durations vary, use EI normalized by predicted evaluation time to improve wall-clock efficiency.",
            "uuid": "e2597.1",
            "source_info": {
                "paper_title": "Practical Bayesian Optimization of Machine Learning Algorithms",
                "publication_date_yy_mm": "2012-06"
            }
        },
        {
            "name_short": "GP EI MCMC",
            "name_full": "GP Expected Improvement with MCMC (Integrated EI)",
            "brief_description": "A specific implementation that marginalizes GP hyperparameters using MCMC (slice sampling) and computes an integrated expected improvement acquisition by Monte Carlo averaging EI over hyperparameter samples.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "GP EI MCMC (Integrated Expected Improvement)",
            "system_description": "Samples from the posterior p(θ | data) over GP hyperparameters using slice sampling (MCMC), computes EI( x | θ ) for each sample, and Monte Carlo-averages these to obtain an integrated acquisition â(x). This accounts for hyperparameter uncertainty and produces more robust acquisition functions and selection decisions.",
            "application_domain": "Hyperparameter optimization for machine learning models (neural nets, LDA, structured SVMs) and any expensive black-box experiment where GP hyperparameter uncertainty matters.",
            "resource_allocation_strategy": "Select next experiment by maximizing integrated EI (Monte Carlo average of EI under posterior hyperparameter samples), thereby allocating resources to points with high expected improvement averaged over model uncertainty.",
            "computational_cost_metric": "Wall-clock time for evaluations; computational overhead of MCMC and GP inference (cubic in observations) is acknowledged but considered small relative to expensive function evaluations.",
            "information_gain_metric": "Integrated expected improvement (Monte Carlo average of EI across GP hyperparameter posterior).",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Same EI tradeoff, but averaged over hyperparameter uncertainty which can increase exploration when hyperparameter posterior yields high variance predictions.",
            "diversity_mechanism": "No separate diversity module; increased exploration may arise from hyperparameter uncertainty in the GP posterior.",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Number of function evaluations and wall-clock time; can be parallelized (N x GP EI MCMC) to reduce wall-clock time.",
            "budget_constraint_handling": "Implements integrated EI to make more informed selections per evaluation; when parallelized, uses Monte Carlo fantasies to select further points while others are pending.",
            "breakthrough_discovery_metric": "Integrated expected improvement reflects breakthrough potential averaged under model-hyperparameter uncertainty.",
            "performance_metrics": "Empirically shown to be most sample-efficient in many tasks (e.g., Branin-Hoo, logistic regression), and effective in exploratory ML hyperparameter tuning tasks; used as primary method in multiple experiments.",
            "comparison_baseline": "GP EI Opt (point-estimate EI), TP Algorithm, grid search, parallelized variants (N x GP EI MCMC), GP-UCB.",
            "performance_vs_baseline": "GP EI MCMC outperforms GP EI Opt (point-estimate) and TPA on benchmark tasks; provides more robust behavior and better sample efficiency (e.g., Branin-Hoo results and ML tasks).",
            "efficiency_gain": "Reported superior sample efficiency; concrete examples include fewer than half the evaluations vs TPA on Branin-Hoo and better final model performance on CIFAR-10 with fewer expensive tuning iterations.",
            "tradeoff_analysis": "Demonstrates that marginalizing GP hyperparameters (instead of point-estimating) materially improves acquisition behavior and final performance; acknowledges increased computation for marginalization but deems it small versus evaluation cost.",
            "optimal_allocation_findings": "Recommendation to marginalize GP hyperparameters (use MCMC) when evaluations are expensive because integrated acquisition leads to more robust and effective allocation of experimental resources.",
            "uuid": "e2597.2",
            "source_info": {
                "paper_title": "Practical Bayesian Optimization of Machine Learning Algorithms",
                "publication_date_yy_mm": "2012-06"
            }
        },
        {
            "name_short": "EI per Second",
            "name_full": "Expected Improvement per Second (cost-aware EI)",
            "brief_description": "A cost-aware acquisition that divides expected improvement by predicted evaluation duration, prioritizing points that yield good expected utility relative to their cost (wall-clock time).",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Expected Improvement per Second (EI / predicted cost)",
            "system_description": "Models the log-duration c(x) of each candidate evaluation with a separate GP (assumed independent from f(x)), predicts expected inverse duration, and computes acquisition as EI(x) multiplied/divided by the predicted evaluation speed to prefer selections that are both promising and fast to evaluate.",
            "application_domain": "Scenarios with variable evaluation costs (e.g., ML hyperparameter configurations where larger models take longer), where wall-clock time or monetary cost matters.",
            "resource_allocation_strategy": "Allocate experiments by maximizing expected improvement per unit cost (per second), thereby biasing selection toward promising configurations that are fast to evaluate to improve wall-clock efficiency.",
            "computational_cost_metric": "Wall-clock evaluation duration (seconds/hours); the paper models ln c(x) with a GP to predict durations.",
            "information_gain_metric": "Expected improvement (EI) over current best (as a proxy for information/value), combined with predicted evaluation time to yield EI per second.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Same as EI, but preference shifts toward cheaper points; encourages exploration of low-cost regions that still promise improvement, possibly trading sample efficiency for wall-clock speed.",
            "diversity_mechanism": "No explicit diversity mechanism; selection may become biased toward cheaper regions which could reduce diversity unless EI still favors diverse high-uncertainty points.",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Wall-clock time and computational resource cost (implicitly monetary if translated).",
            "budget_constraint_handling": "Incorporates predicted duration into acquisition via modeling ln c(x) with a GP and using EI per second to select points under wall-clock/time budgets.",
            "breakthrough_discovery_metric": "High EI per second indicates good potential for impactful improvement within limited time budget.",
            "performance_metrics": "Empirically, GP EI per Second finds better parameters faster in wall-clock time on tasks such as motif finding (3x GP EI per second found best parameters fastest in wall-clock despite requiring more function evaluations).",
            "comparison_baseline": "GP EI MCMC (non-cost-aware), randomized grid search, 3x-parallel variants.",
            "performance_vs_baseline": "GP EI per Second often achieves superior wall-clock performance (faster attainment of good parameters) compared to non-cost-aware GP EI MCMC even when it may use more function evaluations.",
            "efficiency_gain": "Example: In motif-finding experiments, GP EI per Second finds better parameters faster in wall-clock time than GP EI MCMC though it may perform more evaluations; exact numerical gains are presented in figures but not always given as single numeric summaries in text.",
            "tradeoff_analysis": "Paper explicitly analyzes the tradeoff: EI per second sacrifices raw evaluation-sample efficiency (may perform more evaluations) in exchange for better wall-clock progress by favoring fast-to-evaluate promising points.",
            "optimal_allocation_findings": "When evaluations vary widely in duration, optimizing EI per unit cost is a practical strategy to reduce wall-clock time to good solutions; the duration function can be learned with a GP on ln c(x).",
            "uuid": "e2597.3",
            "source_info": {
                "paper_title": "Practical Bayesian Optimization of Machine Learning Algorithms",
                "publication_date_yy_mm": "2012-06"
            }
        },
        {
            "name_short": "MC Parallel Acquisition",
            "name_full": "Monte Carlo acquisition with fantasies for parallel/asynchronous evaluations",
            "brief_description": "A method to select new evaluation points while other evaluations are pending by integrating the acquisition function over the distribution of possible outcomes (fantasies) for pending evaluations using Monte Carlo sampling.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Monte Carlo acquisition under pending evaluations (fantasy-based parallelization)",
            "system_description": "When J evaluations are pending at locations {x_j}, compute the expected acquisition for a candidate x by integrating a(x) over the joint predictive distribution of the pending outcomes y_j (a J-dimensional Gaussian). Approximate this integral via Monte Carlo sampling ('fantasies') of pending y_j, compute acquisition conditioned on each fantasy, and average to obtain an acquisition that accounts for uncertainty in pending results, thus enabling asynchronous parallel selection without duplicate work.",
            "application_domain": "Parallel/asynchronous experimental scheduling for expensive evaluations (ML hyperparameter tuning on multi-core/cloud resources).",
            "resource_allocation_strategy": "Allocates next experiments by maximizing the acquisition averaged over sampled 'fantasy' outcomes of pending evaluations, thereby accounting for possible future information and reducing redundant parallel experiments.",
            "computational_cost_metric": "Wall-clock time, utilization of multiple cores; Monte Carlo overhead for acquisition estimation is relatively small compared to evaluation time.",
            "information_gain_metric": "Expected acquisition (e.g., EI) under the posterior predictive distribution of pending evaluations; effectively expected improvement conditioned on possible pending outcomes.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Acquisition under fantasies balances exploration/exploitation as in the chosen base acquisition (EI, PI), but the fantasy averaging avoids selecting redundant points and can encourage exploration complementary to pending locations.",
            "diversity_mechanism": "Implicitly promotes diversity by conditioning on pending points' possible outcomes, thereby discouraging repeated sampling of similar points; no explicit diversity metric.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Parallel compute cores / asynchronous evaluation slots and wall-clock time.",
            "budget_constraint_handling": "Handles parallel budgets by selecting non-redundant candidate points while others are running, using Monte Carlo sampling of pending outcomes to estimate expected acquisition and reduce wasted parallel resources.",
            "breakthrough_discovery_metric": "Acquisition (e.g., integrated EI) averaged over pending outcomes indicates potential for breakthroughs given current and pending information.",
            "performance_metrics": "Empirically shown to reduce wall-clock time to good solutions when parallelism is available (parallelized GP EI MCMC finds best parameters in significantly less time in experiments such as online LDA).",
            "comparison_baseline": "Sequential selection that ignores pending evaluations (which can duplicate experiments), naive batch selection, and previously-discussed approaches (e.g., Ginsbourger & Riche notionally).",
            "performance_vs_baseline": "Parallelized GP EI MCMC using Monte Carlo fantasies reaches good solutions in significantly less wall-clock time than sequential procedures; in online LDA experiments parallelized variants find best parameters faster.",
            "efficiency_gain": "Reported wall-clock speedups in experiments (e.g., parallelized GP EI MCMC finds best parameters 'in significantly less time' for online LDA compared to sequential); exact multipliers depend on available parallelism (e.g., 3x-parallel variants illustrated).",
            "tradeoff_analysis": "Shows that accounting for pending evaluations via fantasies reduces wasted parallel work and accelerates wall-clock convergence; introduces Monte Carlo overhead but this is small compared to expensive evaluations.",
            "optimal_allocation_findings": "When experiments run in parallel, selecting new points by integrating acquisition over possible pending outcomes (via Monte Carlo fantasies) yields better use of parallel resources and faster attainment of good solutions than naive re-use of the same acquisition function.",
            "uuid": "e2597.4",
            "source_info": {
                "paper_title": "Practical Bayesian Optimization of Machine Learning Algorithms",
                "publication_date_yy_mm": "2012-06"
            }
        },
        {
            "name_short": "GP-UCB",
            "name_full": "Gaussian Process Upper / Lower Confidence Bound (GP-UCB / LCB)",
            "brief_description": "An acquisition strategy that selects points by optimizing a linear combination of the GP predictive mean and standard deviation to control exploration-exploitation, typically μ(x) - κ σ(x) for minimization.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "GP Upper/Lower Confidence Bound acquisition",
            "system_description": "Constructs acquisition a_LCB(x)=μ(x) - κ σ(x) (for minimization) with tunable κ that explicitly trades off exploitation (μ) and exploration (σ). It has theoretical regret guarantees in bandit settings and is discussed as an alternative to EI.",
            "application_domain": "Theoretical and practical global optimization and bandit-style experimental design.",
            "resource_allocation_strategy": "Selects the next point by maximizing (or minimizing) the confidence-bound acquisition, allocating trials to points with either low predicted mean or high uncertainty depending on κ.",
            "computational_cost_metric": "Not explicitly cost-aware in base form; paper discusses GP-UCB as an alternative acquisition but focuses experiments on EI.",
            "information_gain_metric": "Uses predictive variance σ(x) as the exploration term; acquisition itself is not framed as mutual information but as a confidence-bound-based utility.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Explicit via the κ parameter: larger κ encourages exploration (higher weight on σ), smaller κ encourages exploitation.",
            "diversity_mechanism": "Exploration via σ(x) can encourage diverse sampling; no explicit diversity mechanism.",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Number of evaluations (not cost-aware by default).",
            "budget_constraint_handling": "Base GP-UCB does not include cost modeling; paper compares conceptually but does not use GP-UCB in main experiments.",
            "breakthrough_discovery_metric": "Points with low μ(x) - κ σ(x) are targeted; not explicitly framed as 'breakthrough' metric but can locate low minima.",
            "performance_metrics": "Discussed conceptually and compared in Section 4.1 in analyses; specific performance numbers vs EI are reported in experiments but EI (marginalized) is the main focus.",
            "comparison_baseline": "Expected Improvement, Probability of Improvement, grid search, TPA.",
            "performance_vs_baseline": "Authors note EI performed well empirically for minimization in their settings; GP-UCB has theoretical appeal but requires tuning κ and is not the primary method used.",
            "efficiency_gain": "Not quantified in this paper beyond conceptual discussion and occasional comparisons; paper focuses on EI variants.",
            "tradeoff_analysis": "GP-UCB makes the exploration-exploitation tradeoff explicit via κ; the paper emphasizes EI's practical advantages (no extra tuning parameter) while acknowledging GP-UCB's regret formalization.",
            "optimal_allocation_findings": "GP-UCB is conceptually appropriate in regret-minimization settings, but for their hyperparameter optimization tasks EI (especially integrated EI) was preferred due to practical behavior and lack of extra tuning.",
            "uuid": "e2597.5",
            "source_info": {
                "paper_title": "Practical Bayesian Optimization of Machine Learning Algorithms",
                "publication_date_yy_mm": "2012-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "The application of Bayesian methods for seeking the extremum",
            "rating": 2
        },
        {
            "paper_title": "Gaussian process optimization in the bandit setting: No regret and experimental design",
            "rating": 2
        },
        {
            "paper_title": "A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Algorithms for hyper-parameter optimization",
            "rating": 2
        },
        {
            "paper_title": "Dealing with asynchronicity in parallel Gaussian process based global optimization",
            "rating": 2
        },
        {
            "paper_title": "Slice sampling covariance hyperparameters of latent Gaussian models",
            "rating": 1
        }
    ],
    "cost": 0.01647725,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>PRACTICAL BAYESIAN OPTIMIZATION OF MACHINE LEARNING ALGORITHMS</h1>
<p>By Jasper Snoek, Hugo Larochelle and Ryan P. Adams<br>University of Toronto, Université de Sherbrooke and Harvard University</p>
<h4>Abstract</h4>
<p>Machine learning algorithms frequently require careful tuning of model hyperparameters, regularization terms, and optimization parameters. Unfortunately, this tuning is often a "black art" that requires expert experience, unwritten rules of thumb, or sometimes brute-force search. Much more appealing is the idea of developing automatic approaches which can optimize the performance of a given learning algorithm to the task at hand. In this work, we consider the automatic tuning problem within the framework of Bayesian optimization, in which a learning algorithm's generalization performance is modeled as a sample from a Gaussian process (GP). The tractable posterior distribution induced by the GP leads to efficient use of the information gathered by previous experiments, enabling optimal choices about what parameters to try next. Here we show how the effects of the Gaussian process prior and the associated inference procedure can have a large impact on the success or failure of Bayesian optimization. We show that thoughtful choices can lead to results that exceed expert-level performance in tuning machine learning algorithms. We also describe new algorithms that take into account the variable cost (duration) of learning experiments and that can leverage the presence of multiple cores for parallel experimentation. We show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization on a diverse set of contemporary algorithms including latent Dirichlet allocation, structured SVMs and convolutional neural networks.</p>
<ol>
<li>Introduction. Machine learning algorithms are rarely parameter-free; whether via the properties of a regularizer, the hyperprior of a generative model, or the step size of a gradient-based optimization, learning procedures almost always require a set of high-level choices that significantly impact generalization performance. As a practitioner, one is usually able to specify the general framework of an inductive bias much more easily than the particular weighting that it should have relative to training data. As a result, these high-level parameters are often considered a nuisance, making it desirable to develop algorithms with as few of these "knobs" as possible.</li>
</ol>
<p>Another, more flexible take on this issue is to view the optimization of high-level parameters as a procedure to be automated. Specifically, we could view such tuning as the optimization of an unknown black-box function that reflects generalization performance and invoke algorithms developed for such problems. These optimization problems have a somewhat different flavor than the low-level objectives one often encounters as part of a training procedure: here function evaluations are very expensive, as they involve running the primary machine learning algorithm to completion. In this setting where function evaluations are expensive, it is desirable to spend computational time making better choices about where to seek the best parameters. Bayesian optimization (Mockus et al., 1978) provides an elegant approach and has been shown to outperform other state of the art global optimization algorithms on a number of challenging optimization benchmark functions (Jones, 2001). For continuous functions, Bayesian optimization typically works by assuming the unknown function was sampled from a</p>
<p>Gaussian process (GP) and maintains a posterior distribution for this function as observations are made. In our case, these observations are the measure of generalization performance under different settings of the hyperparameters we wish to optimize. To pick the hyperparameters of the next experiment, one can optimize the expected improvement (EI) (Mockus et al., 1978) over the current best result or the Gaussian process upper confidence bound (UCB)(Srinivas et al., 2010). EI and UCB have been shown to be efficient in the number of function evaluations required to find the global optimum of many multimodal black-box functions (Srinivas et al., 2010; Bull, 2011).</p>
<p>Machine learning algorithms, however, have certain characteristics that distinguish them from other black-box optimization problems. First, each function evaluation can require a variable amount of time: training a small neural network with 10 hidden units will take less time than a bigger network with 1000 hidden units. Even without considering duration, the advent of cloud computing makes it possible to quantify economically the cost of requiring large-memory machines for learning, changing the actual cost in dollars of an experiment with a different number of hidden units. It is desirable to understand how to include a concept of cost into the optimization procedure. Second, machine learning experiments are often run in parallel, on multiple cores or machines. We would like to build Bayesian optimization procedures that can take advantage of this parallelism to reach better solutions more quickly.</p>
<p>In this work, our first contribution is the identification of good practices for Bayesian optimization of machine learning algorithms. In particular, we argue that a fully Bayesian treatment of the GP kernel parameters is of critical importance to robust results, in contrast to the more standard procedure of optimizing hyperparameters (e.g. Bergstra et al. (2011)). We also examine the impact of the kernel itself and examine whether the default choice of the squared-exponential covariance function is appropriate. Our second contribution is the description of a new algorithm that accounts for cost in experiments. Finally, we also propose an algorithm that can take advantage of multiple cores to run machine learning experiments in parallel.
2. Bayesian Optimization with Gaussian Process Priors. As in other kinds of optimization, in Bayesian optimization we are interested in finding the minimum of a function $f(\mathbf{x})$ on some bounded set $\mathcal{X}$, which we will take to be a subset of $\mathbb{R}^{D}$. What makes Bayesian optimization different from other procedures is that it constructs a probabilistic model for $f(\mathbf{x})$ and then exploits this model to make decisions about where in $\mathcal{X}$ to next evaluate the function, while integrating out uncertainty. The essential philosophy is to use all of the information available from previous evaluations of $f(\mathbf{x})$ and not simply rely on local gradient and Hessian approximations. This results in a procedure that can find the minimum of difficult non-convex functions with relatively few evaluations, at the cost of performing more computation to determine the next point to try. When evaluations of $f(\mathbf{x})$ are expensive to perform - as is the case when it requires training a machine learning algorithm - it is easy to justify some extra computation to make better decisions. For an overview of the Bayesian optimization formalism, see, e.g., Brochu et al. (2010). In this section we briefly review the general Bayesian optimization approach, before discussing our novel contributions in Section 3.</p>
<p>There are two major choices that must be made when performing Bayesian optimization. First, one must select a prior over functions that will express assumptions about the function being optimized. For this we choose the Gaussian process prior, due to its flexibility and tractability. Second, we must choose an acquisition function, which is used to construct a utility function from the model posterior, allowing us to determine the next point to evaluate.</p>
<p>2.1. Gaussian Processes. The Gaussian process (GP) is a convenient and powerful prior distribution on functions, which we will take here to be of the form $f: \mathcal{X} \rightarrow \mathbb{R}$. The GP is defined by the property that any finite set of $N$ points $\left{\mathbf{x}<em n="1">{n} \in \mathcal{X}\right}</em>}^{N}$ induces a multivariate Gaussian distribution on $\mathbb{R}^{N}$. The $n$th of these points is taken to be the function value $f\left(\mathbf{x<em n="n">{n}\right)$, and the elegant marginalization properties of the Gaussian distribution allow us to compute marginals and conditionals in closed form. The support and properties of the resulting distribution on functions are determined by a mean function $m: \mathcal{X} \rightarrow \mathbb{R}$ and a positive definite covariance function $K: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}$. We will discuss the impact of covariance functions in Section 3.1. For an overview of Gaussian processes, see Rasmussen and Williams (2006).
2.2. Acquisition Functions for Bayesian Optimization. We assume that the function $f(\mathbf{x})$ is drawn from a Gaussian process prior and that our observations are of the form $\left{\mathbf{x}</em>\right}}, y_{n<em n="n">{n=1}^{N}$, where $y</em>} \sim \mathcal{N}\left(f\left(\mathbf{x<em _next="{next" _text="\text">{n}\right), \nu\right)$ and $\nu$ is the variance of noise introduced into the function observations. This prior and these data induce a posterior over functions; the acquisition function, which we denote by $a: \mathcal{X} \rightarrow \mathbb{R}^{+}$, determines what point in $\mathcal{X}$ should be evaluated next via a proxy optimization $\mathbf{x}</em>}}=\operatorname{argmax<em n="n">{\mathbf{x}} a(\mathbf{x})$, where several different functions have been proposed. In general, these acquisition functions depend on the previous observations, as well as the GP hyperparameters; we denote this dependence as $a\left(\mathbf{x} ;\left{\mathbf{x}</em>}, y_{n}\right}, \theta\right)$. There are several popular choices of acquisition function. Under the Gaussian process prior, these functions depend on the model solely through its predictive mean function $\mu\left(\mathbf{x} ;\left{\mathbf{x<em n="n">{n}, y</em>}\right}, \theta\right)$ and predictive variance function $\sigma^{2}\left(\mathbf{x} ;\left{\mathbf{x<em n="n">{n}, y</em>}\right}, \theta\right)$. In the proceeding, we will denote the best current value as $\mathbf{x<em _mathbf_x="\mathbf{x">{\text {best }}=\operatorname{argmin}</em><em n="n">{n}} f\left(\mathbf{x}</em>\right), \Phi(\cdot)$ will denote the cumulative distribution function of the standard normal, and $\phi(\cdot)$ will denote the standard normal density function.</p>
<p>Probability of Improvement. One intuitive strategy is to maximize the probability of improving over the best current value (Kushner, 1964). Under the GP this can be computed analytically as</p>
<p>$$
a_{\mathrm{PI}}\left(\mathbf{x} ;\left{\mathbf{x}<em n="n">{n}, y</em>}\right}, \theta\right)=\Phi(\gamma(\mathbf{x})) \quad \gamma(\mathbf{x})=\frac{f\left(\mathbf{x<em n="n">{\text {best }}\right)-\mu\left(\mathbf{x} ;\left{\mathbf{x}</em>}, y_{n}\right}, \theta\right)}{\sigma\left(\mathbf{x} ;\left{\mathbf{x<em n="n">{n}, y</em>
$$}\right}, \theta\right)</p>
<p>Expected Improvement. Alternatively, one could choose to maximize the expected improvement (EI) over the current best. This also has closed form under the Gaussian process:</p>
<p>$$
a_{\mathrm{EI}}\left(\mathbf{x} ;\left{\mathbf{x}<em n="n">{n}, y</em>}\right}, \theta\right)=\sigma\left(\mathbf{x} ;\left{\mathbf{x<em n="n">{n}, y</em>) ; 0,1)\right)
$$}\right}, \theta\right)\left(\gamma(\mathbf{x}) \Phi(\gamma(\mathbf{x}))+\mathcal{N}(\gamma(\mathbf{x</p>
<p>GP Upper Confidence Bound. A more recent development is the idea of exploiting lower confidence bounds (upper, when considering maximization) to construct acquisition functions that minimize regret over the course of their optimization (Srinivas et al., 2010). These acquisition functions have the form</p>
<p>$$
a_{\mathrm{LCB}}\left(\mathbf{x} ;\left{\mathbf{x}<em n="n">{n}, y</em>}\right}, \theta\right)=\mu\left(\mathbf{x} ;\left{\mathbf{x<em n="n">{n}, y</em>}\right}, \theta\right)-\kappa \sigma\left(\mathbf{x} ;\left{\mathbf{x<em n="n">{n}, y</em>\right}, \theta\right)
$$</p>
<p>with a tunable $\kappa$ to balance exploitation against exploration.
In this work we will focus on the expected improvement criterion, as it has been shown to be better-behaved than probability of improvement, but unlike the method of GP upper confidence bounds (GP-UCB), it does not require its own tuning parameter. We have found expected improvement to perform well in minimization problems, but wish to note that the regret formalization is more appropriate for many settings. We perform a direct comparison between our EI-based approach and GP-UCB in Section 4.1.</p>
<ol>
<li>Practical Considerations for Bayesian Optimization of Hyperparameters. Although an elegant framework for optimizing expensive functions, there are several limitations that have prevented it from becoming a widely-used technique for optimizing hyperparameters in machine learning problems. First, it is unclear for practical problems what an appropriate choice is for the covariance function and its associated hyperparameters. Second, as the function evaluation itself may involve a time-consuming optimization procedure, problems may vary significantly in duration and this should be taken into account. Third, optimization algorithms should take advantage of multi-core parallelism in order to map well onto modern computational environments. In this section, we propose solutions to each of these issues.
3.1. Covariance Functions and Treatment of Covariance Hyperparameters. The power of the Gaussian process to express a rich distribution on functions rests solely on the shoulders of the covariance function. While non-degenerate covariance functions correspond to infinite bases, they nevertheless can correspond to strong assumptions regarding likely functions. In particular, the automatic relevance determination (ARD) squared exponential kernel</li>
</ol>
<p>$$
K_{\mathrm{SE}}\left(\mathbf{x}, \mathbf{x}^{\prime}\right)=\theta_{0} \exp \left{-\frac{1}{2} r^{2}\left(\mathbf{x}, \mathbf{x}^{\prime}\right)\right} \quad r^{2}\left(\mathbf{x}, \mathbf{x}^{\prime}\right)=\sum_{d=1}^{D}\left(x_{d}-x_{d}^{\prime}\right)^{2} / \theta_{d}^{2}
$$</p>
<p>is often a default choice for Gaussian process regression. However, sample functions with this covariance function are unrealistically smooth for practical optimization problems. We instead propose the use of the ARD Matérn $5 / 2$ kernel:</p>
<p>$$
K_{\mathrm{MS} 2}\left(\mathbf{x}, \mathbf{x}^{\prime}\right)=\theta_{0}\left(1+\sqrt{5 r^{2}\left(\mathbf{x}, \mathbf{x}^{\prime}\right)}+\frac{5}{3} r^{2}\left(\mathbf{x}, \mathbf{x}^{\prime}\right)\right) \exp \left{-\sqrt{5 r^{2}\left(\mathbf{x}, \mathbf{x}^{\prime}\right)}\right}
$$</p>
<p>This covariance function results in sample functions which are twice differentiable, an assumption that corresponds to those made by, e.g., quasi-Newton methods, but without requiring the smoothness of the squared exponential.</p>
<p>After choosing the form of the covariance, we must also manage the hyperparameters that govern its behavior (Note that these "hyperparameters" are different than the ones which are being subjected to the overall Bayesian optimization.), as well as that of the mean function. For our problems of interest, typically we would have $D+3$ Gaussian process hyperparameters: $D$ length scales $\theta_{1: D}$, the covariance amplitude $\theta_{0}$, the observation noise $\nu$, and a constant mean $m$. The most commonly advocated approach is to use a point estimate of these parameters by optimizing the marginal likelihood under the Gaussian process</p>
<p>$$
p\left(\mathbf{y} \mid\left{\mathbf{x}<em n="1">{n}\right}</em>\right)
$$}^{N}, \theta, \nu, m\right)=\mathcal{N}\left(\mathbf{y} \mid m \mathbf{1}, \boldsymbol{\Sigma}_{\theta}+\nu \mathbf{I</p>
<p>where $\mathbf{y}=\left[y_{1}, y_{2}, \cdots, y_{n}\right]^{\top}$, and $\boldsymbol{\Sigma}_{\theta}$ is the covariance matrix resulting from the $N$ input points under the hyperparameters $\theta$.</p>
<p>However, for a fully-Bayesian treatment of hyperparameters (summarized here by $\theta$ alone), it is desirable to marginalize over hyperparameters and compute the integrated acquisition function:</p>
<p>$$
\hat{a}\left(\mathbf{x} ;\left{\mathbf{x}<em n="n">{n}, y</em>}\right}\right)=\int a\left(\mathbf{x} ;\left{\mathbf{x<em n="n">{n}, y</em>}\right}, \theta\right) p\left(\theta \mid\left{\mathbf{x<em n="n">{n}, y</em> \theta
$$}\right}_{n=1}^{N}\right) \mathrm{d</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" />
(a) Posterior samples under varying hyperparameters
<img alt="img-1.jpeg" src="img-1.jpeg" />
(b) Expected improvement under varying hyperparameters
<img alt="img-2.jpeg" src="img-2.jpeg" />
(c) Integrated expected improvement</p>
<p>Fig 1: Illustration of integrated expected improvement. (a) Three posterior samples are shown, each with different length scales, after the same five observations. (b) Three expected improvement acquisition functions, with the same data and hyperparameters. The maximum of each is shown. (c) The integrated expected improvement, with its maximum shown.
<img alt="img-3.jpeg" src="img-3.jpeg" />
(a) Posterior samples after three data
<img alt="img-4.jpeg" src="img-4.jpeg" />
(b) Expected improvement under three fantasies
<img alt="img-5.jpeg" src="img-5.jpeg" />
(c) Expected improvement across fantasies</p>
<p>Fig 2: Illustration of the acquisition with pending evaluations. (a) Three data have been observed and three posterior functions are shown, with "fantasies" for three pending evaluations. (b) Expected improvement, conditioned on the each joint fantasy of the pending outcome. (c) Expected improvement after integrating over the fantasy outcomes.
where $a(\mathbf{x})$ depends on $\theta$ and all of the observations. For probability of improvement and expected improvement, this expectation is the correct generalization to account for uncertainty in hyperparameters. We can therefore blend acquisition functions arising from samples from the posterior over GP hyperparameters and have a Monte Carlo estimate of the integrated expected improvement. These samples can be acquired efficiently using slice sampling, as described in Murray and Adams (2010). As both optimization and Markov chain Monte Carlo are computationally dominated by the cubic cost of solving an $N$-dimensional linear system (and our function evaluations are assumed to be much more expensive anyway), the fullyBayesian treatment is sensible and our empirical evaluations bear this out. Figure 1 shows how the integrated expected improvement changes the acquistion function.
3.2. Modeling Costs. Ultimately, the objective of Bayesian optimization is to find a good setting of our hyperparameters as quickly as possible. Greedy acquisition procedures such as expected improvement try to make the best progress possible in the next function evaluation. From a practial point of view, however, we are not so concerned with function evaluations as with wallclock time. Different regions of the parameter space may result in vastly different execution times, due to varying regularization, learning rates, etc. To improve our performance</p>
<p>in terms of wallclock time, we propose optimizing with the expected improvement per second, which prefers to acquire points that are not only likely to be good, but that are also likely to be evaluated quickly. This notion of cost can be naturally generalized to other budgeted resources, such as reagents or money.</p>
<p>Just as we do not know the true objective function $f(\mathbf{x})$, we also do not know the duration function $c(\mathbf{x}): \mathcal{X} \rightarrow \mathbb{R}^{+}$. We can nevertheless employ our Gaussian process machinery to model $\ln c(\mathbf{x})$ alongside $f(\mathbf{x})$. In this work, we assume that these functions are independent of each other, although their coupling may be usefully captured using GP variants of multi-task learning (e.g., Teh et al. (2005); Bonilla et al. (2008)). Under the independence assumption, we can easily compute the predicted expected inverse duration and use it to compute the expected improvement per second as a function of $\mathbf{x}$.
3.3. Monte Carlo Acquisition for Parallelizing Bayesian Optimization. With the advent of multi-core computing, it is natural to ask how we can parallelize our Bayesian optimization procedures. More generally than simply batch parallelism, however, we would like to be able to decide what $\mathbf{x}$ should be evaluated next, even while a set of points are being evaluated. Clearly, we cannot use the same acquisition function again, or we will repeat one of the pending experiments. We would ideally perform a roll-out of our acquisition policy, to choose a point that appropriately balanced information gain and exploitation. However, such roll-outs are generally intractable. Instead we propose a sequential strategy that takes advantage of the tractable inference properties of the Gaussian process to compute Monte Carlo estimates of the acquisiton function under different possible results from pending function evaluations.</p>
<p>Consider the situation in which $N$ evaluations have completed, yielding data $\left{\mathbf{x}<em n="n">{n}, y</em>\right}<em j="j">{n=1}^{N}$, and in which $J$ evaluations are pending at locations $\left{\mathbf{x}</em>$. Ideally, we would choose a new point based on the expected acquisition function under all possible outcomes of these pending evaluations:}\right}_{j=1}^{J</p>
<p>$$
\begin{aligned}
&amp; \hat{a}\left(\mathbf{x} ;\left{\mathbf{x}<em n="n">{n}, y</em>}\right}, \theta,\left{\mathbf{x<em _mathbb_R="\mathbb{R">{j}\right}\right)= \
&amp; \quad \int</em>}^{J}} a\left(\mathbf{x} ;\left{\mathbf{x<em n="n">{n}, y</em>}\right}, \theta,\left{\mathbf{x<em j="j">{j}, y</em>\right}}\right}\right) p\left(\left{y_{j<em j="j">{j=1}^{J} \mid\left{\mathbf{x}</em>\right}<em n="n">{j=1}^{J},\left{\mathbf{x}</em>\right}}, y_{n<em 1="1">{n=1}^{N}\right) \mathrm{d} y</em>
\end{aligned}
$$} \cdots \mathrm{~d} y_{J</p>
<p>This is simply the expectation of $a(\mathbf{x})$ under a $J$-dimensional Gaussian distribution, whose mean and covariance can easily be computed. As in the covariance hyperparameter case, it is straightforward to use samples from this distribution to compute the expected acquisition and use this to select the next point. Figure 2 shows how this procedure would operate with queued evaluations. We note that a similar approach is touched upon briefly by Ginsbourger and Riche (2010), but they view it as too intractable to warrant attention. We have found our Monte Carlo estimation procedure to be highly effective in practice, however, as will be discussed in Section 4.
4. Empirical Analyses. In this section, we empirically analyse ${ }^{1}$ the algorithms introduced in this paper and compare to existing strategies and human performance on a number of challenging machine learning problems. We refer to our method of expected improvement while marginalizing GP hyperparameters as "GP EI MCMC", optimizing hyperparameters as "GP EI Opt", EI per second as "GP EI per Second", and $N$ times parallelized GP EI MCMC as " $N$ x GP EI MCMC".</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Fig 3: A comparison of standard approaches compared to our GP EI MCMC approach on the Branin-Hoo function (3a) and training logistic regression on the MNIST data (3b).
4.1. Branin-Hoo and Logistic Regression. We first compare to standard approaches and the recent Tree Parzen Algorithm ${ }^{2}$ (TPA) of Bergstra et al. (2011) on two standard problems. The Branin-Hoo function is a common benchmark for Bayesian optimization techniques (Jones, 2001) that is defined over $x \in \mathbb{R}^{2}$ where $0 \leq x_{1} \leq 15$ and $-5 \leq x_{2} \leq 15$. We also compare to TPA on a logistic regression classification task on the popular MNIST data. The algorithm requires choosing four hyperparameters, the learning rate for stochastic gradient descent, on a log scale from 0 to 1 , the $\ell_{2}$ regularization parameter, between 0 and 1 , the mini batch size, from 20 to 2000 and the number of learning epochs, from 5 to 2000. Each algorithm was run on the Branin-Hoo and logistic regression problems 100 and 10 times respectively and mean and standard error are reported. The results of these analyses are presented in Figures 3a and 3b in terms of the number of times the function is evaluated. On Branin-Hoo, integrating over hyperparameters is superior to using a point estimate and the GP EI significantly outperforms TPA, finding the minimum in fewer than half as many evaluations, in both cases.
4.2. Online LDA. Latent Dirichlet allocation (LDA) is a directed graphical model for documents in which words are generated from a mixture of multinomial "topic" distributions. Variational Bayes is a popular paradigm for learning and, recently, Hoffman et al. (2010) proposed an online learning approach in that context. Online LDA requires two learning parameters, $\tau_{0}$ and $\kappa$, that control the learning rate $\rho_{t}=\left(\tau_{0}+t\right)^{-\kappa}$ used to update the variational parameters of LDA based on the $t^{\text {th }}$ minibatch of document word count vectors. The size of the minibatch is also a third parameter that must be chosen. Hoffman et al. (2010) relied on an exhaustive grid search of size $6 \times 6 \times 8$, for a total of 288 hyperparameter configurations.</p>
<p>We used the code made publically available by Hoffman et al. (2010) to run experiments with online LDA on a collection of Wikipedia articles. We downloaded a random set of 249,560 articles, split into training, validation and test sets of size $200,000,24,560$ and 25,000 respectively. The documents are represented as vectors of word counts from a vocabulary of 7,702 words. As reported in Hoffman et al. (2010), we used a lower bound on the per word perplixity of the validation set documents as the performance measure. One must also specify the number of topics and the hyperparameters $\eta$ for the symmetric Dirichlet prior over the topic</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Fig 4: Different strategies of optimization on the Online LDA problem compared in terms of function evaluations (4a), walltime (4b) and constrained to a grid or not (4c).
distributions and $\alpha$ for the symmetric Dirichlet prior over the per document topic mixing weights. We followed Hoffman et al. (2010) and used 100 topics and $\eta=\alpha=0.01$ in our experiments in order to emulate their analysis and repeated exactly the grid search reported in the paper ${ }^{3}$. Each online LDA evaluation generally took between five to ten hours to converge, thus the grid search requires approximately 60 to 120 processor days to complete.</p>
<p>In Figures 4a and 4b we compare our various strategies of optimization over the same grid on this expensive problem. That is, the algorithms were restricted to only the exact parameter settings as evaluated by the grid search. Each optimization was then repeated one hundred times (each time picking two different random experiments to initialize the optimization with) and the mean and standard error are reported. Figures 4 a and 4 b respectively show the average minimum loss (perplexity) achieved by each strategy compared to the number of times online LDA is evaluated with new parameter settings and the duration of the optimization in days. Figure 4c shows the average loss of 3 and 5 times parallelized GP EI MCMC which are restricted to the same grid as compared to a single run of the same algorithms where the algorithm can flexibly choose new parameter settings within the same range by optimizing the expected improvement.</p>
<p>In this case, integrating over hyperparameters is superior to using a point estimate. While</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>GP EI MCMC is the most efficient in terms of function evaluations, we see that parallelized GP EI MCMC finds the best parameters in significantly less time. Finally, in Figure 4c we see that the parallelized GP EI MCMC algorithms find a significantly better minimum value than was found in the grid search used by Hoffman et al. (2010) while running a fraction of the number of experiments.
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Fig 5: A comparison of various strategies for optimizing the hyperparameters of M3E models on the protein motif finding task in terms of wallclock time (5a), function evaluations (5b) and different covariance functions(5c).
4.3. Motif Finding with Structured Support Vector Machines. In this example, we consider optimizing the learning parameters of Max-Margin Min-Entropy (M3E) Models (Miller et al., 2012), which include Latent Structured Support Vector Machines (Yu and Joachims, 2009) as a special case. Latent structured SVMs outperform SVMs on problems where they can explicitly model problem-dependent hidden variables. A popular example task is the binary classification of protein DNA sequences (Miller et al., 2012; Yu and Joachims, 2009; Kumar et al., 2010). The hidden variable to be modeled is the unknown location of particular subsequences, or motifs, that are indicators of positive sequences.</p>
<p>Setting the hyperparameters, such as the regularisation term, $C$, of structured SVMs remains a challenge and these are typically set through a time consuming grid search procedure as is done in Miller et al. (2012) and Yu and Joachims (2009). Indeed, Kumar et al. (2010) report that hyperparameter selection was avoided for the motif finding task due to being too computationally expensive. However, Miller et al. (2012) demonstrate that classification</p>
<p>results depend highly on the setting of the parameters, which differ for each protein.
M3E models introduce an entropy term, parameterized by $\alpha$, which enables the model to significantly outperform latent structured SVMs. This additional performance, however, comes at the expense of an additional problem-dependent hyperparameter. We emulate the experiments of Miller et al. (2012) for one protein with approximately 40,000 sequences. We explore 25 settings of the parameter $C$, on a $\log$ scale from $10^{-1}$ to $10^{6}, 14$ settings of $\alpha$, on a $\log$ scale from 0.1 to 5 and the model convergence tolerance, $\epsilon \in\left{10^{-4}, 10^{-3}, 10^{-2}, 10^{-1}\right}$. We ran a grid search over the 1,400 possible combinations of these parameters, evaluating each over 5 random 50-50 training and test splits.</p>
<p>In Figures 5a and 5b, we compare the randomized grid search to GP EI MCMC, GP EI per Second and their 3x parallelized versions, all constrained to the same points on the grid, in terms of minimum validation error vs wallclock time and function evaluations. Each algorithm was repeated 100 times and the mean and standard error are shown. We observe that the Bayesian optimization strategies are considerably more efficient than grid search which is the status quo. In this case, GP EI MCMC is superior to GP EI per Second in terms of function evaluations but GP EI per Second finds better parameters faster than GP EI MCMC as it learns to use a less strict convergence tolerance early on while exploring the other parameters. Indeed, 3x GP EI per second is the least efficient in terms of function evaluations but finds better parameters faster than all the other algorithms.</p>
<p>Figure 5c compares the use of various covariance functions in GP EI MCMC optimization on this problem. The optimization was repeated for each covariance 100 times and the mean and standard error are shown. It is clear that the selection of an appropriate covariance significantly affects performance and the estimation of length scale parameters is critical. The assumption of the infinite differentiability of the underlying function as imposed by the commonly used squared exponential is too restrictive for this problem.
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Fig 6: Validation error on the CIFAR-10 data for different optimization strategies.
4.4. Convolutional Networks on CIFAR-10. Neural networks and deep learning methods notoriously require careful tuning of numerous hyperparameters. Multi-layer convolutional neural networks are an example of such a model for which a thorough exploration of architechtures and hyperparameters is beneficial, as demonstrated in Saxe et al. (2011), but often computationally prohibitive. While Saxe et al. (2011) demonstrate a methodology for efficiently exploring model architechtures, numerous hyperparameters, such as regularisation</p>
<p>parameters, remain. In this empirical analysis, we tune nine hyperparameters of a three-layer convolutional network, described in Krizhevsky (2009) on the CIFAR-10 benchmark dataset using the code provided ${ }^{4}$. This model has been carefully tuned by a human expert (Krizhevsky, 2009) to achieve a highly competitive result of $18 \%$ test error, which matches the published state of the art ${ }^{5}$ result (Coates and $\mathrm{Ng}, 2011$ ) on CIFAR-10. The parameters we explore include the number of epochs to run the model, the learning rate, four weight costs (one for each layer and the softmax output weights), and the width, scale and power of the response normalization on the pooling layers of the network.</p>
<p>We optimize over the nine parameters for each strategy on a withheld validation set and report the mean validation error and standard error over five separate randomly initialized runs. Results are presented in Figure 6 and contrasted with the average results achieved using the best parameters found by the expert. The best hyperparameters ${ }^{6}$ found by the GP EI MCMC approach achieve an error on the test set of $14.98 \%$, which is over $3 \%$ better than the expert and the state of the art on CIFAR-10.
5. Conclusion. In this paper we presented methods for performing Bayesian optimization of hyperparameters associated with general machine learning algorithms. We introduced a fully Bayesian treatment for expected improvement, and algorithms for dealing with variable time regimes and parallelized experiments. Our empirical analysis demonstrates the effectiveness of our approaches on three challenging recently published problems spanning different areas of machine learning. The code used will be made publicly available. The resulting Bayesian optimization finds better hyperparameters significantly faster than the approaches used by the authors. Indeed our algorithms surpassed a human expert at selecting hyperparameters on the competitive CIFAR-10 dataset and as a result beat the state of the art by over $3 \%$.</p>
<p>Acknowledgements. This work was supported by a grant from Amazon Web Services and by a DARPA Young Faculty Award.</p>
<h1>References.</h1>
<p>J Mockus, V Tiesis, and A Zilinskas. The application of Bayesian methods for seeking the extremum. Towards Global Optimization, 2:117-129, 1978.
D.R. Jones. A taxonomy of global optimization methods based on response surfaces. Journal of Global Optimization, 21(4):345-383, 2001.
Niranjan Srinivas, Andreas Krause, Sham Kakade, and Matthias Seeger. Gaussian process optimization in the bandit setting: No regret and experimental design. In ICML, 2010.
Adam D. Bull. Convergence rates of efficient global optimization algorithms. JMLR, (3-4):2879-2904, 2011.
James S. Bergstra, Rémi Bardenet, Yoshua Bengio, and Bálázs Kégl. Algorithms for hyper-parameter optimization. In NIPS. 2011.
Eric Brochu, Vlad M. Cora, and Nando de Freitas. A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning. pre-print, 2010. arXiv:1012.2599.
Carl E. Rasmussen and Christopher Williams. Gaussian Processes for Machine Learning. MIT Press, 2006.
H. J. Kushner. A new method for locating the maximum point of an arbitrary multipeak curve in the presence of noise. Journal of Basic Engineering, 86, 1964.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Iain Murray and Ryan Prescott Adams. Slice sampling covariance hyperparameters of latent Gaussian models. In NIPS, pages 1723-1731. 2010.
Yee Whye Teh, Matthias Seeger, and Michael I. Jordan. Semiparametric latent factor models. In AISTATS, 2005 .
Edwin V. Bonilla, Kian Ming A. Chai, and Christopher K. I. Williams. Multi-task Gaussian process prediction. In NIPS, 2008.
David Ginsbourger and Rodolphe Le Riche. Dealing with asynchronicity in parallel Gaussian process based global optimization. 2010.
Matthew Hoffman, David M. Blei, and Francis Bach. Online learning for latent Dirichlet allocation. In NIPS, 2010 .
Kevin Miller, M. Pawan Kumar, Benjamin Packer, Danny Goodman, and Daphne Koller. Max-margin minentropy models. In AISTATS, 2012.
Chun-Nam John Yu and Thorsten Joachims. Learning structural SVMs with latent variables. In ICML, 2009. M. Pawan Kumar, Benjamin Packer, and Daphne Koller. Self-paced learning for latent variable models. In NIPS. 2010.
Andrew Saxe, Pang Wei Koh, Zhenghao Chen, Maneesh Bhand, Bipin Suresh, and Andrew Ng. On random weights and unsupervised feature learning. In ICML, 2011.
Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, Department of Computer Science, University of Toronto, 2009.
Adam Coates and Andrew Y. Ng. Selecting receptive fields in deep networks. In NIPS. 2011.</p>
<p>Jasper Snoek
Department of Computer Science
University of Toronto
E-MAIL: jasper@cs.toronto.edu</p>
<p>Hugo Larochelle
Département d'informatique
Université de Sherbrooke
E-MAIL: hugo.larochelle@usherbrooke.ca</p>
<p>Ryan P. Adams
School of Engineering and Applied Sciences
Harvard University
E-MAIL: rpa@seas.harvard.edu</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{4}$ Available at: http://code.google.com/p/cuda-convnet/ using the architechture defined in http://code. google.com/p/cuda-convnet/source/browse/trunk/example-layers/layers-18pct.cfg
${ }^{5}$ Without augmenting the training data.
${ }^{6}$ The optimized parameters deviate interestingly from the expert-determined settings; e.g., the optimal weight costs are asymmetric (the weight cost of the second layer is approximately an order of magnitude smaller than the first layer), a learning rate two orders of magnitude smaller, a slightly wider response normalization, larger scale and much smaller power.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
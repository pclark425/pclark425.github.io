<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2219 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2219</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2219</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-60.html">extraction-schema-60</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that use proxy metrics, computational predictions, or surrogate objectives for scientific discovery, and how these compare to experimental or ground-truth validation, including quantitative measures of agreement or disagreement.</div>
                <p><strong>Paper ID:</strong> paper-281566529</p>
                <p><strong>Paper Title:</strong> Multimodal MBC-ATT: cross-modality attentional fusion of EEG-fNIRS for cognitive state decoding</p>
                <p><strong>Paper Abstract:</strong> With the rapid development of brain-computer interface (BCI) technology, the effective integration of multimodal biological signals to improve classification accuracy has become a research hotspot. However, existing methods often fail to fully exploit cross-modality correlations in complex cognitive tasks. To address this, this paper proposes a Multi-Branch Convolutional Neural Network with Attention (MBC-ATT) for BCI based cognitive tasks classification. MBC-ATT employs independent branch structures to process electroencephalography (EEG) and functional near-infrared spectroscopy (fNIRS) signals separately, thereby leveraging the advantages of each modality. To further enhance the fusion of multimodal features, we introduce a cross-modal attention mechanism to discriminate features, strengthening the model's ability to focus on relevant signals and thereby improving classification accuracy. We conducted experiments on the n-back and WG datasets. The results demonstrate that the proposed model outperforms conventional approaches in classification performance, further validating the effectiveness of MBC-ATT in brain-computer interfaces. This study not only provides novel insights for multimodal BCI systems but also holds great potential for various applications.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2219.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2219.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that use proxy metrics, computational predictions, or surrogate objectives for scientific discovery, and how these compare to experimental or ground-truth validation, including quantitative measures of agreement or disagreement.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MBC-ATT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-Branch Convolutional Neural Network with Attention (MBC-ATT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dual-branch deep learning model that independently extracts EEG and fNIRS features and fuses them with a cross-modal (Query-Key-Value) multi-head attention mechanism for cognitive-state classification.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>MBC-ATT</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Dual-branch CNN: one branch for temporal/spatial EEG feature extraction, one branch for fNIRS feature extraction; features are linearly projected to a common space and fused via a 4-head cross-modal Query-Key-Value attention mechanism, then classified with fully connected layers. Trained with standard supervised classification objectives (cross-entropy implied) on labeled cognitive task data.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>brain–computer interfaces / cognitive state decoding (neuroimaging)</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_name</strong></td>
                            <td>Classification metrics on held-out labeled data (Accuracy, Precision, Recall, F1-score)</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_description</strong></td>
                            <td>Performance of the trained model on test folds / holdout data measured by Accuracy (TP+TN / all), Precision (TP / (TP+FP)), Recall (TP / (TP+FN)) and F1-score (harmonic mean of precision and recall); these are computed against behaviorally-derived task labels (n-back levels, WG vs Baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_type</strong></td>
                            <td>data-driven ML prediction (supervised classification metrics on held-out labels)</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_metric</strong></td>
                            <td>Behavioral task labels collected during experiments (task condition labels)</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_description</strong></td>
                            <td>Ground truth is the experimenter-provided labels assigned per trial according to the cognitive task paradigm (n-back class labels, WG vs Baseline), derived from the experimental protocol and subject responses; model outputs are compared to these labels on held-out test sets and cross-validation folds.</td>
                        </tr>
                        <tr>
                            <td><strong>has_both_proxy_and_ground_truth</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_gap_measure</strong></td>
                            <td>Reported differences between model variants and baselines: ablation (no cross-modal attention) accuracy drop of 6.55 percentage points on n-back (98.13% -> 91.58%) and 2.07 points on WG (98.61% -> 96.54%); improvements over baseline methods on n-back: +15.13 (vs SVM 83.00%), +11.13 (vs DNN 87.00%), +9.72 (vs CNN-LSTM 88.41%), +3.03 (vs STFT-MDNF 95.10%); WG improvements over baselines: +24.62 (vs SVM 73.99%), +6.61 (vs DNN 92.00%), +5.51 (vs STFT-MDNF 93.10%), +2.32 (vs EF-Net 96.29%). Five-fold CV accuracy spread (n-back): folds = [0.9739,0.9775,0.9786,0.9739,0.9727], mean 0.9753 (≈97.53%), sample standard deviation ≈0.0023 (≈0.23 percentage points); WG folds = [0.9602,0.9639,0.9762,0.9709,0.9628], mean 0.9668 (≈96.68%), std ≈0.0059 (≈0.59 percentage points). Paired t-tests vs baselines reported p < 0.05.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_performance</strong></td>
                            <td>n-back: Accuracy 98.13%, Precision 98.24%, Recall 98.10%, F1 98.11%; WG: Accuracy 98.61%, Precision 99.79%, Recall 97.44%, F1 98.58%; five-fold CV averages: n-back Accuracy ≈97.53%, F1 ≈97.54%; WG Accuracy ≈96.68%, F1 ≈96.34%.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_performance</strong></td>
                            <td>Not a separate independent experimental validation beyond behavioral labels; model agreement with ground-truth labels equals the proxy performance above (e.g., 98.13% agreement with n-back labels).</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Can be derived from Recall: n-back average Recall 98.10% implies false negative rate ≈1.90%; WG Recall 97.44% implies false negative rate ≈2.56%. The paper does not report class-specific confusion-derived FPR across all classes numerically.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_characterization</strong></td>
                            <td>In-distribution / incremental within the multimodal BCI literature: uses established modalities (EEG, fNIRS) and established supervised objectives but introduces a novel cross-modal attention fusion — characterized as an incremental methodological advance rather than out-of-distribution discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_varies_with_novelty</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>gap_variation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>gap_reduction_method</strong></td>
                            <td>Architectural improvement — cross-modal attention fusion (Query-Key-Value across modalities) to reduce mismatch between modalities and improve predictive agreement with labels; five-fold CV and ablation studies used to quantify reduction of error relative to simple concatenation (ablation) and to existing baseline models.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_reduction_effectiveness</strong></td>
                            <td>Ablation shows removal of cross-modal attention reduces accuracy by 6.55 percentage points on n-back and 2.07 points on WG, demonstrating the method's effectiveness at reducing proxy error relative to simpler fusion. Paired t-tests versus baselines yield p < 0.05.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_comparison</strong></td>
                            <td>Paper discusses computational cost qualitatively (high computational cost challenges for real-time deployment) but does not provide quantitative cost/time/money comparisons between proxy evaluations (model training and inference) and any separate experimental validation (no wet-lab costs).</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_validation</strong></td>
                            <td>Not applicable — predictions are instantaneous trial classifications; no long-term temporal validation (stability/degradation over months/years) is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Emerging/application-focused: multimodal EEG–fNIRS fusion and deep-learning for BCI is an active, rapidly developing field with established datasets and growing application of ML methods, but with outstanding issues (inter-subject variability, synchronization, real-time deployment).</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_calibration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multiple_proxies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_correlation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_cascade</strong></td>
                            <td>Single-stage computational → evaluation against behavioral ground-truth labels via train/test splits and cross-validation; there is no additional intermediate experimental validation stage (e.g., physiological ground-truth beyond task labels) described. Errors are propagated from model predictions to performance metrics computed on labeled test sets.</td>
                        </tr>
                        <tr>
                            <td><strong>publication_bias_discussion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Limitations explicitly noted: requirement for tight temporal synchronization between EEG and fNIRS; untested generalization on small-sample patient datasets; relatively high computational cost for real-time deployment; variability across subjects and sessions may limit cross-subject generalization (needs transfer learning); no predictive uncertainty reported.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_factors</strong></td>
                            <td>EEG has millisecond temporal dynamics and is susceptible to artifacts and inter-subject/session variability; fNIRS measures slow hemodynamic responses (seconds) and has better spatial robustness but lower temporal resolution; neurovascular coupling differences and synchronization mismatch are key factors affecting proxy-to-ground-truth alignment and performance.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2219.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2219.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that use proxy metrics, computational predictions, or surrogate objectives for scientific discovery, and how these compare to experimental or ground-truth validation, including quantitative measures of agreement or disagreement.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Ablation (Concat) model</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ablated multimodal model without cross-modal attention (feature concatenation baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The same dual-branch feature extractors as MBC-ATT but without the cross-modal Query-Key-Value attention; high-level features from EEG and fNIRS are directly concatenated and fed into the classifier.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Ablation (concatenation) model</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Retains unimodal CNN branches but removes the attention-based interaction; concatenates feature vectors from EEG and fNIRS and passes them to classifier layers (serves as controlled baseline to measure effect of cross-modal attention).</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>brain–computer interfaces / neuroimaging</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_name</strong></td>
                            <td>Classification metrics on held-out labels (Accuracy, Precision, Recall, F1)</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_description</strong></td>
                            <td>Same metrics as MBC-ATT computed against the experimental trial labels.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_type</strong></td>
                            <td>data-driven ML prediction</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_metric</strong></td>
                            <td>Behavioral task labels (same ground truth as MBC-ATT)</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_description</strong></td>
                            <td>Experimental task labels used for supervised evaluation (n-back levels and WG/Baseline labels).</td>
                        </tr>
                        <tr>
                            <td><strong>has_both_proxy_and_ground_truth</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_gap_measure</strong></td>
                            <td>Accuracy drop relative to MBC-ATT: n-back −6.55 percentage points (98.13% -> 91.58%); WG −2.07 points (98.61% -> 96.54%).</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_performance</strong></td>
                            <td>n-back Accuracy 91.58% (after attention removal); WG Accuracy 96.54%. (Other metrics not explicitly tabulated in the text for ablation beyond accuracy.)</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_performance</strong></td>
                            <td>Agreement with trial labels equals the proxy numbers above (e.g., 91.58% agreement on n-back).</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_characterization</strong></td>
                            <td>Baseline/in-distribution (simple concatenation fusion).</td>
                        </tr>
                        <tr>
                            <td><strong>gap_varies_with_novelty</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>gap_variation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>gap_reduction_method</strong></td>
                            <td>N/A for ablation (it is the comparator showing gap vs full model).</td>
                        </tr>
                        <tr>
                            <td><strong>gap_reduction_effectiveness</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_comparison</strong></td>
                            <td>Not discussed quantitatively; ablation is computationally cheaper than full attention model but no numbers provided.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_validation</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Baseline architecture consistent with common fusion strategies in the field.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_calibration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multiple_proxies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_correlation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_cascade</strong></td>
                            <td>Single-stage evaluation against labels via train/test and cross-validation.</td>
                        </tr>
                        <tr>
                            <td><strong>publication_bias_discussion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Shows that simple concatenation may fail to capture cross-modal dependencies, leading to meaningful performance degradation; no per-class error breakdown provided in paper text.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_factors</strong></td>
                            <td>Same as MBC-ATT: EEG/fNIRS temporal and physiological mismatches reduce efficacy of naive fusion.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2219.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2219.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that use proxy metrics, computational predictions, or surrogate objectives for scientific discovery, and how these compare to experimental or ground-truth validation, including quantitative measures of agreement or disagreement.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Baseline methods (literature)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Baseline models reported from prior literature (SVM, DNN, CNN-LSTM, STFT-MDNF, EF-Net)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Collection of previously published classifiers for the same tasks used for comparative benchmarking; reported accuracies are taken from literature and compared to MBC-ATT.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SVM / DNN / CNN-LSTM / STFT-MDNF / EF-Net (as reported in prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Various classical and deep learning approaches used previously for EEG/fNIRS fusion or unimodal classification: SVM (classical ML), DNN (generic deep neural net), CNN-LSTM (recurrence-plot + CNN-LSTM), STFT-MDNF (STFT image + DenseNet multimodal fusion), EF-Net (EEG-fNIRS convolutional dual-stream model). The paper cites published accuracy numbers from those works for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>brain–computer interfaces / neuroimaging</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_name</strong></td>
                            <td>Reported classification accuracy (and in some sources other metrics) on the same or similar datasets/tasks</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_description</strong></td>
                            <td>Literature-reported performance (accuracy percentages) measured against task labels in respective studies; used as benchmark proxies for comparative effectiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_type</strong></td>
                            <td>data-driven ML prediction (literature-reported supervised metrics)</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_metric</strong></td>
                            <td>Behavioral task labels used in those original studies (not independently re-validated in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_description</strong></td>
                            <td>Original datasets' trial labels; the present paper compares reported numeric performance values to MBC-ATT numbers but does not re-run these baselines under identical evaluation pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>has_both_proxy_and_ground_truth</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_gap_measure</strong></td>
                            <td>Reported gaps (as provided in the paper): n-back: MBC-ATT (98.13%) vs SVM (83.00%) +15.13 pp; vs DNN (87.00%) +11.13 pp; vs CNN-LSTM (88.41%) +9.72 pp; vs STFT-MDNF (95.10%) +3.03 pp. WG: MBC-ATT (98.61%) vs SVM (73.99%) +24.62 pp; vs DNN (92.00%) +6.61 pp; vs STFT-MDNF (93.10%) +5.51 pp; vs EF-Net (96.29%) +2.32 pp.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_performance</strong></td>
                            <td>Literature-reported accuracies: SVM 83.00% (n-back) / 73.99% (WG); DNN 87.00% / 92.00%; CNN-LSTM 88.41% (n-back); STFT-MDNF 95.10% / 93.10%; EF-Net 96.29% (WG).</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_performance</strong></td>
                            <td>Ground-truth agreement as reported in each original study; not re-evaluated under identical conditions in this paper, so cross-study differences may reflect dataset, preprocessing, or evaluation protocol differences.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_characterization</strong></td>
                            <td>These are prior-art methods; their novelty ranges from classical (SVM) to recent deep-learning fusion models (EF-Net, STFT-MDNF).</td>
                        </tr>
                        <tr>
                            <td><strong>gap_varies_with_novelty</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>gap_variation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>gap_reduction_method</strong></td>
                            <td>Not applicable (these are comparators).</td>
                        </tr>
                        <tr>
                            <td><strong>gap_reduction_effectiveness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_comparison</strong></td>
                            <td>Not discussed in the present paper in quantitative terms; only accuracy comparisons are tabulated.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_validation</strong></td>
                            <td>Not discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Varies by method; overall the field contains both established classical methods (SVM) and newer deep-learning approaches; considered an actively developing domain.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_calibration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multiple_proxies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_correlation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_cascade</strong></td>
                            <td>Not described — comparisons are cross-study numeric comparisons rather than controlled re-evaluations under a single pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>publication_bias_discussion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Cross-study performance comparisons may be affected by differences in preprocessing, dataset splits, hyperparameter tuning, and evaluation protocols; the paper does not reimplement all baselines under a unified pipeline so reported gaps may partly reflect methodological differences rather than purely model superiority.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_factors</strong></td>
                            <td>Differences in data preprocessing (filtering, downsampling, artifact removal), segmentation (window length/step), and subject-level partitioning can cause substantial variance in reported proxy metrics across studies.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Simultaneous acquisition of EEG and nirs during cognitive tasks for an open access dataset <em>(Rating: 2)</em></li>
                <li>FGANET: fNIRS-guided attention network for hybrid EEG-fNIRS brain-computer interfaces <em>(Rating: 2)</em></li>
                <li>EEG-fNIRS-based hybrid image construction and classification using cnn-lstm <em>(Rating: 2)</em></li>
                <li>Simultaneous functional near-infrared spectroscopy and electroencephalography for monitoring of human brain activity and oxygenation: a review <em>(Rating: 1)</em></li>
                <li>An electroencephalographic signature predicts antidepressant response in major depression <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2219",
    "paper_id": "paper-281566529",
    "extraction_schema_id": "extraction-schema-60",
    "extracted_data": [
        {
            "name_short": "MBC-ATT",
            "name_full": "Multi-Branch Convolutional Neural Network with Attention (MBC-ATT)",
            "brief_description": "A dual-branch deep learning model that independently extracts EEG and fNIRS features and fuses them with a cross-modal (Query-Key-Value) multi-head attention mechanism for cognitive-state classification.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "MBC-ATT",
            "system_description": "Dual-branch CNN: one branch for temporal/spatial EEG feature extraction, one branch for fNIRS feature extraction; features are linearly projected to a common space and fused via a 4-head cross-modal Query-Key-Value attention mechanism, then classified with fully connected layers. Trained with standard supervised classification objectives (cross-entropy implied) on labeled cognitive task data.",
            "domain": "brain–computer interfaces / cognitive state decoding (neuroimaging)",
            "proxy_metric_name": "Classification metrics on held-out labeled data (Accuracy, Precision, Recall, F1-score)",
            "proxy_metric_description": "Performance of the trained model on test folds / holdout data measured by Accuracy (TP+TN / all), Precision (TP / (TP+FP)), Recall (TP / (TP+FN)) and F1-score (harmonic mean of precision and recall); these are computed against behaviorally-derived task labels (n-back levels, WG vs Baseline).",
            "proxy_metric_type": "data-driven ML prediction (supervised classification metrics on held-out labels)",
            "ground_truth_metric": "Behavioral task labels collected during experiments (task condition labels)",
            "ground_truth_description": "Ground truth is the experimenter-provided labels assigned per trial according to the cognitive task paradigm (n-back class labels, WG vs Baseline), derived from the experimental protocol and subject responses; model outputs are compared to these labels on held-out test sets and cross-validation folds.",
            "has_both_proxy_and_ground_truth": true,
            "quantitative_gap_measure": "Reported differences between model variants and baselines: ablation (no cross-modal attention) accuracy drop of 6.55 percentage points on n-back (98.13% -&gt; 91.58%) and 2.07 points on WG (98.61% -&gt; 96.54%); improvements over baseline methods on n-back: +15.13 (vs SVM 83.00%), +11.13 (vs DNN 87.00%), +9.72 (vs CNN-LSTM 88.41%), +3.03 (vs STFT-MDNF 95.10%); WG improvements over baselines: +24.62 (vs SVM 73.99%), +6.61 (vs DNN 92.00%), +5.51 (vs STFT-MDNF 93.10%), +2.32 (vs EF-Net 96.29%). Five-fold CV accuracy spread (n-back): folds = [0.9739,0.9775,0.9786,0.9739,0.9727], mean 0.9753 (≈97.53%), sample standard deviation ≈0.0023 (≈0.23 percentage points); WG folds = [0.9602,0.9639,0.9762,0.9709,0.9628], mean 0.9668 (≈96.68%), std ≈0.0059 (≈0.59 percentage points). Paired t-tests vs baselines reported p &lt; 0.05.",
            "proxy_performance": "n-back: Accuracy 98.13%, Precision 98.24%, Recall 98.10%, F1 98.11%; WG: Accuracy 98.61%, Precision 99.79%, Recall 97.44%, F1 98.58%; five-fold CV averages: n-back Accuracy ≈97.53%, F1 ≈97.54%; WG Accuracy ≈96.68%, F1 ≈96.34%.",
            "ground_truth_performance": "Not a separate independent experimental validation beyond behavioral labels; model agreement with ground-truth labels equals the proxy performance above (e.g., 98.13% agreement with n-back labels).",
            "false_positive_rate": null,
            "false_negative_rate": "Can be derived from Recall: n-back average Recall 98.10% implies false negative rate ≈1.90%; WG Recall 97.44% implies false negative rate ≈2.56%. The paper does not report class-specific confusion-derived FPR across all classes numerically.",
            "novelty_characterization": "In-distribution / incremental within the multimodal BCI literature: uses established modalities (EEG, fNIRS) and established supervised objectives but introduces a novel cross-modal attention fusion — characterized as an incremental methodological advance rather than out-of-distribution discovery.",
            "gap_varies_with_novelty": null,
            "gap_variation_details": null,
            "gap_reduction_method": "Architectural improvement — cross-modal attention fusion (Query-Key-Value across modalities) to reduce mismatch between modalities and improve predictive agreement with labels; five-fold CV and ablation studies used to quantify reduction of error relative to simple concatenation (ablation) and to existing baseline models.",
            "gap_reduction_effectiveness": "Ablation shows removal of cross-modal attention reduces accuracy by 6.55 percentage points on n-back and 2.07 points on WG, demonstrating the method's effectiveness at reducing proxy error relative to simpler fusion. Paired t-tests versus baselines yield p &lt; 0.05.",
            "validation_cost_comparison": "Paper discusses computational cost qualitatively (high computational cost challenges for real-time deployment) but does not provide quantitative cost/time/money comparisons between proxy evaluations (model training and inference) and any separate experimental validation (no wet-lab costs).",
            "temporal_validation": "Not applicable — predictions are instantaneous trial classifications; no long-term temporal validation (stability/degradation over months/years) is reported.",
            "domain_maturity": "Emerging/application-focused: multimodal EEG–fNIRS fusion and deep-learning for BCI is an active, rapidly developing field with established datasets and growing application of ML methods, but with outstanding issues (inter-subject variability, synchronization, real-time deployment).",
            "uncertainty_quantification": false,
            "uncertainty_calibration": null,
            "multiple_proxies": true,
            "proxy_correlation": null,
            "validation_cascade": "Single-stage computational → evaluation against behavioral ground-truth labels via train/test splits and cross-validation; there is no additional intermediate experimental validation stage (e.g., physiological ground-truth beyond task labels) described. Errors are propagated from model predictions to performance metrics computed on labeled test sets.",
            "publication_bias_discussion": false,
            "limitations_challenges": "Limitations explicitly noted: requirement for tight temporal synchronization between EEG and fNIRS; untested generalization on small-sample patient datasets; relatively high computational cost for real-time deployment; variability across subjects and sessions may limit cross-subject generalization (needs transfer learning); no predictive uncertainty reported.",
            "domain_specific_factors": "EEG has millisecond temporal dynamics and is susceptible to artifacts and inter-subject/session variability; fNIRS measures slow hemodynamic responses (seconds) and has better spatial robustness but lower temporal resolution; neurovascular coupling differences and synchronization mismatch are key factors affecting proxy-to-ground-truth alignment and performance.",
            "uuid": "e2219.0"
        },
        {
            "name_short": "Ablation (Concat) model",
            "name_full": "Ablated multimodal model without cross-modal attention (feature concatenation baseline)",
            "brief_description": "The same dual-branch feature extractors as MBC-ATT but without the cross-modal Query-Key-Value attention; high-level features from EEG and fNIRS are directly concatenated and fed into the classifier.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Ablation (concatenation) model",
            "system_description": "Retains unimodal CNN branches but removes the attention-based interaction; concatenates feature vectors from EEG and fNIRS and passes them to classifier layers (serves as controlled baseline to measure effect of cross-modal attention).",
            "domain": "brain–computer interfaces / neuroimaging",
            "proxy_metric_name": "Classification metrics on held-out labels (Accuracy, Precision, Recall, F1)",
            "proxy_metric_description": "Same metrics as MBC-ATT computed against the experimental trial labels.",
            "proxy_metric_type": "data-driven ML prediction",
            "ground_truth_metric": "Behavioral task labels (same ground truth as MBC-ATT)",
            "ground_truth_description": "Experimental task labels used for supervised evaluation (n-back levels and WG/Baseline labels).",
            "has_both_proxy_and_ground_truth": true,
            "quantitative_gap_measure": "Accuracy drop relative to MBC-ATT: n-back −6.55 percentage points (98.13% -&gt; 91.58%); WG −2.07 points (98.61% -&gt; 96.54%).",
            "proxy_performance": "n-back Accuracy 91.58% (after attention removal); WG Accuracy 96.54%. (Other metrics not explicitly tabulated in the text for ablation beyond accuracy.)",
            "ground_truth_performance": "Agreement with trial labels equals the proxy numbers above (e.g., 91.58% agreement on n-back).",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_characterization": "Baseline/in-distribution (simple concatenation fusion).",
            "gap_varies_with_novelty": null,
            "gap_variation_details": null,
            "gap_reduction_method": "N/A for ablation (it is the comparator showing gap vs full model).",
            "gap_reduction_effectiveness": "N/A",
            "validation_cost_comparison": "Not discussed quantitatively; ablation is computationally cheaper than full attention model but no numbers provided.",
            "temporal_validation": "Not applicable.",
            "domain_maturity": "Baseline architecture consistent with common fusion strategies in the field.",
            "uncertainty_quantification": false,
            "uncertainty_calibration": null,
            "multiple_proxies": true,
            "proxy_correlation": null,
            "validation_cascade": "Single-stage evaluation against labels via train/test and cross-validation.",
            "publication_bias_discussion": false,
            "limitations_challenges": "Shows that simple concatenation may fail to capture cross-modal dependencies, leading to meaningful performance degradation; no per-class error breakdown provided in paper text.",
            "domain_specific_factors": "Same as MBC-ATT: EEG/fNIRS temporal and physiological mismatches reduce efficacy of naive fusion.",
            "uuid": "e2219.1"
        },
        {
            "name_short": "Baseline methods (literature)",
            "name_full": "Baseline models reported from prior literature (SVM, DNN, CNN-LSTM, STFT-MDNF, EF-Net)",
            "brief_description": "Collection of previously published classifiers for the same tasks used for comparative benchmarking; reported accuracies are taken from literature and compared to MBC-ATT.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "SVM / DNN / CNN-LSTM / STFT-MDNF / EF-Net (as reported in prior work)",
            "system_description": "Various classical and deep learning approaches used previously for EEG/fNIRS fusion or unimodal classification: SVM (classical ML), DNN (generic deep neural net), CNN-LSTM (recurrence-plot + CNN-LSTM), STFT-MDNF (STFT image + DenseNet multimodal fusion), EF-Net (EEG-fNIRS convolutional dual-stream model). The paper cites published accuracy numbers from those works for comparison.",
            "domain": "brain–computer interfaces / neuroimaging",
            "proxy_metric_name": "Reported classification accuracy (and in some sources other metrics) on the same or similar datasets/tasks",
            "proxy_metric_description": "Literature-reported performance (accuracy percentages) measured against task labels in respective studies; used as benchmark proxies for comparative effectiveness.",
            "proxy_metric_type": "data-driven ML prediction (literature-reported supervised metrics)",
            "ground_truth_metric": "Behavioral task labels used in those original studies (not independently re-validated in this paper).",
            "ground_truth_description": "Original datasets' trial labels; the present paper compares reported numeric performance values to MBC-ATT numbers but does not re-run these baselines under identical evaluation pipelines.",
            "has_both_proxy_and_ground_truth": null,
            "quantitative_gap_measure": "Reported gaps (as provided in the paper): n-back: MBC-ATT (98.13%) vs SVM (83.00%) +15.13 pp; vs DNN (87.00%) +11.13 pp; vs CNN-LSTM (88.41%) +9.72 pp; vs STFT-MDNF (95.10%) +3.03 pp. WG: MBC-ATT (98.61%) vs SVM (73.99%) +24.62 pp; vs DNN (92.00%) +6.61 pp; vs STFT-MDNF (93.10%) +5.51 pp; vs EF-Net (96.29%) +2.32 pp.",
            "proxy_performance": "Literature-reported accuracies: SVM 83.00% (n-back) / 73.99% (WG); DNN 87.00% / 92.00%; CNN-LSTM 88.41% (n-back); STFT-MDNF 95.10% / 93.10%; EF-Net 96.29% (WG).",
            "ground_truth_performance": "Ground-truth agreement as reported in each original study; not re-evaluated under identical conditions in this paper, so cross-study differences may reflect dataset, preprocessing, or evaluation protocol differences.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_characterization": "These are prior-art methods; their novelty ranges from classical (SVM) to recent deep-learning fusion models (EF-Net, STFT-MDNF).",
            "gap_varies_with_novelty": null,
            "gap_variation_details": null,
            "gap_reduction_method": "Not applicable (these are comparators).",
            "gap_reduction_effectiveness": null,
            "validation_cost_comparison": "Not discussed in the present paper in quantitative terms; only accuracy comparisons are tabulated.",
            "temporal_validation": "Not discussed.",
            "domain_maturity": "Varies by method; overall the field contains both established classical methods (SVM) and newer deep-learning approaches; considered an actively developing domain.",
            "uncertainty_quantification": null,
            "uncertainty_calibration": null,
            "multiple_proxies": true,
            "proxy_correlation": null,
            "validation_cascade": "Not described — comparisons are cross-study numeric comparisons rather than controlled re-evaluations under a single pipeline.",
            "publication_bias_discussion": false,
            "limitations_challenges": "Cross-study performance comparisons may be affected by differences in preprocessing, dataset splits, hyperparameter tuning, and evaluation protocols; the paper does not reimplement all baselines under a unified pipeline so reported gaps may partly reflect methodological differences rather than purely model superiority.",
            "domain_specific_factors": "Differences in data preprocessing (filtering, downsampling, artifact removal), segmentation (window length/step), and subject-level partitioning can cause substantial variance in reported proxy metrics across studies.",
            "uuid": "e2219.2"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Simultaneous acquisition of EEG and nirs during cognitive tasks for an open access dataset",
            "rating": 2
        },
        {
            "paper_title": "FGANET: fNIRS-guided attention network for hybrid EEG-fNIRS brain-computer interfaces",
            "rating": 2
        },
        {
            "paper_title": "EEG-fNIRS-based hybrid image construction and classification using cnn-lstm",
            "rating": 2
        },
        {
            "paper_title": "Simultaneous functional near-infrared spectroscopy and electroencephalography for monitoring of human brain activity and oxygenation: a review",
            "rating": 1
        },
        {
            "paper_title": "An electroencephalographic signature predicts antidepressant response in major depression",
            "rating": 1
        }
    ],
    "cost": 0.0146015,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Multimodal MBC-ATT: cross-modality attentional fusion of EEG-fNIRS for cognitive state decoding
24 September 2025</p>
<p>Xiaosu Hu 
Rossella Capotorto 
Yu Li 
HDU-ITMO Joint Institute
Hangzhou Dianzi University
HangzhouChina</p>
<p>Lei Zhu zhulei@hdu.edu.cn 
School of Automation
Hangzhou Dianzi University
HangzhouChina</p>
<p>Provincial Key Laboratory of Soft Matter &amp; Biomedical Materials
Wenzhou Institute
University of Chinese Academy of Sciences (WIUCAS)
WenzhouChina</p>
<p>Aiai Huang 
School of Automation
Hangzhou Dianzi University
HangzhouChina</p>
<p>Jianhai Zhang 
School of Computer Science
Hangzhou Dianzi University
HangzhouChina</p>
<p>Peng Yuan 
The Affiliated Wuxi People's Hospital of Nanjing Medical University
WuxiChina</p>
<p>Wuxi Medical Center
Nanjing Medical University
WuxiChina</p>
<p>Wuxi People's Hospital
WuxiChina</p>
<p>University of Michigan
United States</p>
<p>Sapienza University of Rome
Italy RekhaYadav Gillala</p>
<p>KLEF Deemed to be University
India</p>
<p>Multimodal MBC-ATT: cross-modality attentional fusion of EEG-fNIRS for cognitive state decoding
24 September 20258F8650B0FB3877546C3B85B8F10FE0E010.3389/fnhum.2025.1660532RECEIVED 06 July 2025 ACCEPTED 04 September 2025brain-computer interfacecognitive taskdeep learningmultimodal signalsmultimodal fusion
With the rapid development of brain-computer interface (BCI) technology, the effective integration of multimodal biological signals to improve classification accuracy has become a research hotspot.However, existing methods often fail to fully exploit cross-modality correlations in complex cognitive tasks.To address this, this paper proposes a Multi-Branch Convolutional Neural Network with Attention (MBC-ATT) for BCI based cognitive tasks classification.MBC-ATT employs independent branch structures to process electroencephalography (EEG) and functional near-infrared spectroscopy (fNIRS) signals separately, thereby leveraging the advantages of each modality.To further enhance the fusion of multimodal features, we introduce a cross-modal attention mechanism to discriminate features, strengthening the model's ability to focus on relevant signals and thereby improving classification accuracy.We conducted experiments on the n-back and WG datasets.The results demonstrate that the proposed model outperforms conventional approaches in classification performance, further validating the effectiveness of MBC-ATT in brain-computer interfaces.This study not only provides novel insights for multimodal BCI systems but also holds great potential for various applications.</p>
<p>Introduction</p>
<p>Brain-Computer Interface (BCI) represents a cutting-edge human-machine interaction paradigm that establishes direct neural pathways between the brain and external devices, enabling users to bypass traditional neuromuscular channels for device control (Sharma and Meena, 2024).Based on the mode of signal acquisition, BCI systems are generally categorized into invasive and non-invasive approaches.Compared to invasive methods, non-invasive techniques offer superior clinical applicability due to their enhanced safety and user tolerance.These approaches eliminate the risks associated with surgical implantation, making them suitable for long-term monitoring and mobile applications (Jafari et al., 2023).Moreover, they avoid the ethical concerns linked to intracranial implants, rendering them more appropriate for large-scale population studies and translational clinical research (Värbu et al., 2022).Accordingly, non-invasive acquisition methods were adopted in the present study.Electroencephalography (EEG) and functional near-infrared spectroscopy (fNIRS) are two representative non-invasive brain signal acquisition techniques that have been widely adopted in BCI research due to their safety and ease of use (Liu et al., 2021).EEG offers millisecond-level temporal resolution, enabling real-time recording of cortical neural activity.This makes it particularly suitable for investigating high-level cognitive functions such as attention and memory.However, as EEG signals are collected via scalp electrodes, they are highly susceptible to artifacts from electromyographic activity, eye movements, and environmental noise (Li et al., 2022).Additionally, EEG exhibits considerable inter-subject variability, and the same subject's signal characteristics may change across sessions or experimental contexts.This variability hinders the cross-subject generalizability of EEG-based BCI systems and often necessitates a laborious calibration process for new users, which severely limits practical deployment (Wu et al., 2020).In contrast, fNIRS measures hemodynamic responses associated with neural activity by detecting changes in oxygenated and deoxygenated hemoglobin concentrations in the brain.It offers better spatial resolution and is more robust against motion artifacts, making it well-suited for experiments in more naturalistic settings (Chiarelli et al., 2017).However, fNIRS suffers from limited temporal resolution and is less capable of capturing fast neural dynamics.</p>
<p>To overcome this limitation, researchers have focused in recent years on developing multimodal fusion techniques.Among these, the combined application of EEG and fNIRS has garnered particular attention.The high temporal resolution of EEG complements the spatial localization capability of fNIRS, which indirectly reflects neural activity by detecting changes in cortical blood oxygen levels.Moreover, the strong resistance of fNIRS to artifacts such as eye movements effectively compensates for the inherent limitations of EEG (Lin et al., 2023).This fusion strategy not only deepens the multidimensional analysis of brain activity but also demonstrates significant value in areas such as motor imagery decoding, cognitive state assessment, and the diagnosis of neurological disorders.</p>
<p>In recent years, an increasing number of studies have attempted to enhance brain state recognition performance through multimodal fusion.For example, a method combining handcrafted features and traditional machine learning techniques was proposed (Cao et al., 2022) to classify multi-level brain load, but it heavily relies on complex preprocessing and feature extraction processes.To overcome the limitations of traditional approaches, some studies have begun to incorporate deep learning techniques.A novel recurrence plot (RP)-based timedistributed convolutional neural network and long short-term memory (CNN-LSTM) framework (Mughal et al., 2022) has been introduced for the integrated classification of EEG and fNIRS signals in hybrid BCI applications, demonstrating an effective approach for capturing spatiotemporal patterns across modalities.Additionally, short-time Fourier transform (STFT) has been employed to convert EEG signals into time-frequency images, which are subsequently integrated with the frequency-domain features of fNIRS using the Dense Convolutional Network (DenseNet) architecture, offering a complementary strategy for enhancing multimodal representation and classification performance in hybrid BCI systems (Bunterngchit et al., 2024).However, these methods largely depend on simple concatenation or stacking fusion strategies and fail to fully exploit the complementary and synergistic relationships between modalities.</p>
<p>In terms of fusion strategies, existing studies have explored different fusion timings and methods, which are generally categorized into early fusion-where multimodal features are combined at the input or low-level feature stage and late fusion, where decisions or high-level features from each modality are integrated at a later stage.For example, a comparison between early and late fusion approaches (Li et al., 2023) showed that early fusion, where multimodal features are combined before classification, can somewhat improve model performance.In contrast, a polynomial fusion method was proposed in Sun et al. (2020), which operates at a deeper semantic level and thus falls under the category of late fusion.Similarly, the FGANet model (Kwak et al., 2022) employs spatial mapping and attention mechanisms to extract high-level cross-modal features, serving as another example of a late fusion strategy that provides novel insights into improving representational capacity.However, current fusion methods still face several challenges: firstly, the ability to model complementary relationships between modalities is limited, lacking mechanisms for deeply exploring the dynamic dependencies between EEG and fNIRS (Bourguignon et al., 2022;Khan et al., 2021); secondly, most fusion methods rely on feature concatenation or static weighting, making it difficult to automatically focus on key modalities or brain region signals based on different task states (Nour et al., 2021;Chen et al., 2023).</p>
<p>To address these issues, this paper proposes a crossmodal attention fusion framework, Multimodal MBC-ATT.This framework is based on a late fusion strategy and incorporates a modality-guided attention mechanism aimed at selectively integrating information through the joint modeling of cross-modal features, thereby enhancing the decoding ability of cognitive states and overcoming the limitations of current methods in dynamic dependency modeling and task adaptability.The innovations of this study are primarily reflected in the following aspects: 2 Dataset and method</p>
<p>In this section, we describe the multimodal dataset that integrates EEG and fNIRS signals, outline the preprocessing steps necessary for effective signal representation, and finally present the MBC-ATT framework designed for cross-modal brain activity decoding.</p>
<p>Experimental dataset</p>
<p>This study used an open-access multimodal brain imaging dataset, which simultaneously recorded EEG and fNIRS signals, aiming to promote the development of neuroimaging analysis and BCI research (Shin et al., 2018).The dataset was collected from 26 healthy participants while performing three cognitive tasks, with the aim of providing high-quality multimodal signal data for BCI and neuroscience research.All participants were right-handed adults (nine males, 17 females) aged 17-33 years (M = 26.1,SD = 3.5).Each participant provided written informed consent and confirmed absence of neurological or psychiatric history through standardized screening questionnaires.</p>
<p>During the experiment, participants sat on a comfortable chair approximately 1.2 meters from a 24-inch LCD monitor, with their right index and middle fingers positioned on a numeric keypad for keypress responses.Three cognitive tasks were employed: the n-back task (Dataset A) to assess working memory load, the discrimination/selection response (DSR) task (Dataset B) to examine neural responses to target versus non-target stimuli, and the word generation (WG) task (Dataset C) to investigate brain activity related to language processing.</p>
<p>In this study, participants completed the n-back task (Dataset A) and the WG task (Dataset C) to investigate the multimodal neural signal characteristics under different cognitive task conditions.</p>
<p>In the n-back task, participants made responses based on the task type: in the 0-back task, participants pressed the right index finger (target button) or the right middle finger (nontarget button); in the 2-back and 3-back tasks, participants determined whether the currently displayed number matched the number shown 2 or 3 trials earlier and pressed the corresponding button.As shown in Figure 1, each task block consists of a 2s instruction display, followed by a 40-s task period.During the task period, a random one-digit number is displayed every 2 s for 0.5 s, followed by a 1.5-s display of a fixed cross.After the task period, participants enter a 20-s rest period, during which they focus on a fixed cross displayed on the screen.Each participant performed a total of 180 trials (20 trials × 3 series × 3 sessions) to ensure the adequacy and reliability of the data.</p>
<p>The WG task is a spontaneous word generation task in which participants are required to think of and silently generate words starting with a specific letter within a limited time, aiming to investigate the neural activity characteristics of brain regions related to language.The experiment consists of 3 sessions, each containing 10 WG tasks and 10 Baseline (BL) tasks.Participants perform a total of 30 WG tasks and 30 BL tasks.As shown in Figure 2, each trial consists of a 2-s task prompt, a 10-s task execution period, and a 13-15 s rest period.During the WG task period, a random letter is displayed on the screen, and participants are required to quickly generate and silently list as many words as possible starting with that letter within 10 s, while avoiding repetition.In the BL task period, participants are instructed to focus on a fixed point at the center of the screen to maintain a low cognitive load, serving as a control condition for the WG task.</p>
<p>The EEG-fNIRS multimodal dataset used in this study underwent basic preprocessing prior to its release to ensure data quality.The EEG data were sampled at 1,000 Hz, consisting of 30 EEG channels and 2 Electrooculography (EOG) channels, and were downsampled to 200 Hz during the data processing stage.A 1-40 Hz bandpass filter was applied to remove lowfrequency drift and high-frequency noise, as this frequency range preserves task-related neural oscillations (theta, alpha, beta, and low gamma) while suppressing irrelevant artifacts.Eye movement artifacts were removed using the EEGLAB toolbox (Martínez-Cancino et al., 2021).The fNIRS data consist of 36 channels with a sampling rate of 10.4 Hz.The raw optical intensity measurements were converted to changes in concentrations of oxyhemoglobin (HbO) and deoxyhemoglobin (HbR), and the sampling rate was downsampled to 10 Hz.Additionally, basic artifact removal and noise filtering were applied during data acquisition.</p>
<p>To meet the requirements of this experiment, further processing was performed on the data.First, the EEG and fNIRS signals were synchronized and segmented based on the task time markers.The 2 s before the task start and the rest period after the task were discarded, retaining only the valid data following the task onset.Each task data was segmented into multiple time windows using a sliding window approach (with a window length of 5 s and a step size of 1 s), which was chosen to ensure that each segment contains sufficient task-related neural activity while increasing the number of samples and maintaining temporal continuity for more robust analysis.</p>
<p>Methods</p>
<p>The paper proposes a decoding framework that integrates a multi-branch convolutional neural network with a crossmodal attention mechanism (MBC-ATT), aimed at efficient joint modeling of EEG and fNIRS signals.The model independently extracts temporal and spatial features of EEG and fNIRS signals through separate branches and introduces a modality-guided attention mechanism to achieve dynamic fusion and selective enhancement of cross-modal features, thereby improving decoding performance for complex cognitive tasks.The network architecture is illustrated in Figure 3.</p>
<p>The model employs a dual-branch architecture, with the upper branch corresponding to the EEG feature extraction module and the lower branch dedicated to the fNIRS feature extraction module.The features extracted from both modalities are subsequently fed into a cross-modal attention fusion mechanism to accomplish the final classification task.</p>
<p>EEG feature extraction network</p>
<p>This branch uses a convolutional neural network (CNN) as the core architecture for the EEG feature extraction branch.The branch aims to automatically extract meaningful features from raw EEG signals using deep learning methods, thereby enhancing the performance of multimodal fusion.This branch consists of multiple convolutional layers, pooling layers, and fully connected layers.In the initial stage, three convolutional layers with kernel sizes of (7, 1) are used to gradually extract local features from the EEG signals, with the ReLU activation function introducing non-linearity to enhance the feature representation capability.This kernel configuration is primarily based on the temporal characteristics of EEG signals, where the (7,1) kernel slides along the time dimension to capture short-term local temporal dependencies while maintaining the independence across channels.This design was inspired by Yu et al. (2025), which demonstrated that short temporal receptive fields are effective for EEG decoding; in our work, the (7,1) kernel was further adapted to our data characteristics (sampling rate and channel configuration) to balance temporal locality with computational efficiency.Subsequently, a max pooling layer is applied to downsample the feature map, reducing its spatial dimensions while preserving key information.This operation effectively reduces computational complexity and enhances the robustness of the features.Additionally, batch normalization (Ogundokun et al., 2022) is applied to standardize the features, accelerating the model training and improving its stability.In the deeper layers of the network, three two-dimensional convolutional layers with kernel sizes of (4×4) are used to extract global EEG features, a configuration designed to capture spatial correlations across multiple channels, which helps identify more complex interchannel patterns and reflect the spatial distribution of brain activity.Max pooling is again applied to further downsample the feature map for increased feature abstraction, while batch normalization continues to optimize the training process.Finally, the feature maps are flattened and passed through two fully connected layers to extract high-level features and perform feature mapping, thereby enhancing discriminability and providing optimized feature representations for multimodal fusion in classification tasks.</p>
<p>fNIRS feature extraction network</p>
<p>Unlike EEG signals, fNIRS signals reflect changes in blood oxygen concentration, with feature extraction focusing more on local fluctuations and temporal changes.Despite their differences in physiological characteristics, both utilize CNN as the core architecture.This design ensures consistency in multimodal data processing, while also reducing the complexity of module design, thereby facilitating subsequent multimodal fusion.</p>
<p>The initial stage of the network consists of two convolutional layers with kernel sizes of (4, 1), designed to extract local spatial features of the fNIRS signals, particularly modeling the temporal dynamics of blood oxygen concentration.Subsequently, the pooling layer downsamples the feature map, reducing its spatial dimensions, while preserving key features and lowering computational complexity.In the subsequent layers, the network further extracts complex spatial features through two convolutional layers with kernel sizes of (2, 2), identifying blood oxygen concentration variation patterns between different regions.These convolutional layers progressively expand the receptive field, capturing broader spatial information.Finally, the fully connected layers merge and transform the extracted spatiotemporal features of blood oxygen concentration, generating high-level features that serve as input for subsequent classification and multimodal fusion.This design enables the fNIRS feature extraction branch to efficiently capture the spatial distribution and temporal dynamics of oxygen hemodynamics and map them to a feature space suitable for multimodal tasks.</p>
<p>Multimodal feature fusion network</p>
<p>In this study, to fully leverage the complementarity of the two modalities, we employed an attention mechanism (Vaswani et al., 2017) to dynamically capture the dependencies between them.</p>
<p>First, the input features of EEG and fNIRS are mapped onto a unified hidden space using independent linear transformation layers.This step ensures that both modalities, which may have different original dimensions or feature distributions, are projected into a common representational space, facilitating effective interaction and comparison.Subsequently, the features of the EEG and fNIRS modalities are passed through independent linear transformation layers to generate their corresponding Query, Key, and Value representations, which are used to assess both intra-modal and inter-modal dependencies.These additional transformations allow the model to learn optimal representations tailored for attention computation, enabling it to focus on the most relevant parts of the input.To enhance the interaction between modalities, the model employs a cross-modal attention mechanism for fusion.Specifically, the Query from the EEG modality interacts with the Key and Value from the fNIRS modality, and similarly, the fNIRS modality interacts with the EEG modality.This cross-modal fusion allows each modality to dynamically reference the features of the other, thereby fully leveraging the complementary nature of the high temporal resolution of EEG and the spatial resolution of fNIRS.In addition, the model employs a 4-head attention mechanism, where the Query, Key, and Value are divided into four independent subspaces.Each head captures the inter-modal dependencies from a different perspective.This multi-head design further enhances the expressive power of the attention mechanism, enabling it to model the complex the complex relationships between EEG and fNIRS more comprehensively.</p>
<p>Experiments and results</p>
<p>In the experiments, this study employs the n-back task and the WG task to validate the effectiveness of the proposed MBC-ATT method across different cognitive tasks.The experimental procedure strictly follows standardized protocols for data splitting, feature extraction, and model training to ensure the reproducibility of the results.</p>
<p>All experiments were conducted on a system equipped with an Intel Core processor and an NVIDIA GeForce RTX 4060 Laptop GPU.The software environment included Python 3.10 and PyTorch 2.1 with CUDA 11.8 support, all implemented within an Anaconda-managed virtual environment.For the nback experiment, the model was trained for 60 epochs using the Adam optimizer with an initial learning rate of 1e-3; for the WG experiment, training lasted 70 epochs with the same optimizer and learning rate.Other training hyperparameters were kept consistent across both experiments.</p>
<p>Experimental plan</p>
<p>This study adopts a within-subject partitioning strategy to evaluate the applicability of the model at the individual level.</p>
<p>By performing independent training and testing on the dataset</p>
<p>Frontiers in Human Neuroscience 05 frontiersin.orgLi et al. 10.3389/fnhum.2025.1660532 of each subject, confounding effects arising from inter-individual variability are effectively mitigated.This approach not only enhances recognition accuracy but also ensures robust adaptation to subject-specific physiological signal characteristics.</p>
<p>For each subject, the dataset is randomly partitioned into training (80%) and testing (20%) subsets, with the former dedicated to model development and the latter reserved for final performance assessment.To ensure robustness against partitioning randomness, the training subset undergoes five-fold cross-validation.This strategy not only mitigates overfitting but also enhances the model's generalizability to unseen data.</p>
<p>To further ensure the robustness and generalizability of the model, we additionally conducted a complementary evaluation using five-fold cross-validation on the entire dataset.This crossvalidation procedure mitigates overfitting and reduces potential bias caused by data partitioning, providing a more comprehensive assessment of the model's performance.</p>
<p>Evaluation metrics</p>
<p>To provide a comprehensive evaluation of the classification model's performance, this study utilizes four key metrics: Accuracy, Precision, Recall, and the F1-score.</p>
<p>Accuracy serves as a cornerstone evaluation metric in deep learning-based classification tasks, formally defined as the ratio between correctly predicted instances and the total number of test samples.The computation follows the standard formulation:
Accuracy = TP + TN TP + TN + FP + FN (1)
In the classification framework, True Positives (TP) correspond to the number of positive instances correctly identified by the model, True Negatives (TN) represent correctly classified negative cases, False Positives (FP) indicate negative samples erroneously predicted as positive, and False Negatives (FN) signify positive samples inaccurately classified as negative.</p>
<p>Precision quantifies the model's predictive reliability for the positive class, representing the proportion of true positives among all positive predictions.The metric is formally defined as:
Precision = TP TP + FP (2)
Elevated precision demonstrates the model's enhanced accuracy in positive-class identification, characterized by reduced false positive predictions.This metric proves especially critical in applications where misclassification entails significant consequences.</p>
<p>Recall quantifies the model's sensitivity in detecting positiveclass instances, defined as the ratio of true positives to all actual positives in the population.The formal computation is expressed as:
Recall = TP TP + FN (3)
An elevated recall rate demonstrates the model's enhanced detection capability for positive instances, albeit with a potential compromise in specificity through increased false positives.This performance metric assumes critical importance in highstakes applications where false negatives may incur substantial costs, such as medical diagnosis, security surveillance, or fault detection systems.</p>
<p>The F1-score represents the harmonic mean of precision and recall, providing a balanced metric that reconciles the tradeoff between these two competing objectives.Formally, it is computed as:
F1-score = 2 × Precision × Recall Precision + Recall (4)
Under class-imbalanced conditions, the F1-score serves as a robust composite metric for evaluating overall model performance, mitigating potential assessment bias induced by over-reliance on individual metrics such as precision or recall in isolation.</p>
<p>Furthermore, to enable intuitive cross-class performance analysis, this investigation incorporates confusion matrix visualization (Lei et al., 2016).This diagnostic tool explicitly maps the correspondence between ground-truth and predicted labels, uncovering category-specific classification biases that inform targeted model refinement.</p>
<p>Experimental results</p>
<p>n-back dataset experimental results</p>
<p>In the n-back experiment, we followed the established experimental plan for data partitioning and model training, and evaluated the model performance on the test set.</p>
<p>We computed multiple evaluation metrics for each subject, as shown in Table 1.The results indicate that the proposed method demonstrates excellent classification performance at the individual level.Although there are differences in classification performance across different subjects, the method is able to adapt to inter-individual variations in neural signals and maintains a high classification accuracy in most subjects, demonstrating good generalization ability.</p>
<p>To further analyze the overall performance of the model, we calculated the average results of all subjects.The classification accuracy reached 98.13%, precision was 98.24%, recall was 98.10%, and the F1 score was 98.11%.This result validates the effectiveness and robustness of the model in a within-subject partition scenario.Furthermore, the high consistency observed across different subjects further demonstrates the model's robustness to individual neural signal variations, as it can stably extract common features and perform classification tasks.</p>
<p>In addition, we plotted the confusion matrix (as shown in Figure 4) to further analyze the classification performance of the model.From the figure, it can be seen that the model's prediction results are well-balanced across categories with a high accuracy, indicating that the proposed method has strong discriminative ability across different task categories.Confusion matrix of n-back experiment results.</p>
<p>WG dataset experimental results</p>
<p>Similar to the n-back dataset, we followed the same experimental procedure for data partitioning, model training, and performance evaluation on the test set.As shown in Table 2, the proposed method achieves high classification accuracy for each subject.The average classification accuracy reaches 98.61%, the precision is 99.79%, the recall is 97.44%, and the F1-score is 98.58%.The model's classification performance was further analyzed using the confusion matrix (as shown in Figure 5).The results indicate that the recognition outcomes are well-balanced across different categories, with diagonal elements significantly larger than non-diagonal elements.This demonstrates a high correct classification rate and a low misclassification rate, highlighting the model's robustness.</p>
<p>Model evaluation with cross-validation and results</p>
<p>To enhance the robustness and generalizability of the model evaluation, the original 80:20 train-test split strategy was replaced with a five-fold cross-validation approach.This adjustment aims to better address potential overfitting issues and provide a more comprehensive assessment of the model's performance.Specifically, the dataset was randomly divided into five equally sized subsets.In each fold, one subset was used as the test set while the remaining four were used for training.This process was repeated five times, ensuring that each sample was tested exactly once.The final performance metrics, including accuracy, precision, recall, and F1-score, were averaged across all folds to obtain a more reliable and stable evaluation.</p>
<p>Table 3 presents the mean accuracy, precision, recall, and F1-score of the model evaluated via five-fold crossvalidation on the n-back and WG datasets.The results demonstrate that the model achieves consistently high and stable performance across both datasets, with accuracies of approximately 97.53 and 96.68%, and F1-scores of 97.54 and 96.34%, respectively.The close alignment between accuracy and F1-score indicates the model's balanced capability across classes, ensuring both precise classification and overall performance stability.</p>
<p>Effectiveness of the cross-modal attention mechanism</p>
<p>To evaluate the contribution of the multimodal fusion module to overall model performance, this study designed and conducted an ablation experiment.Specifically, in the ablated model, the cross-modal attention mechanism originally proposed in the model was removed, and the Query-Key-Value structure used to model the dependency between EEG and fNIRS modalities was no longer employed.Accordingly, the high-level features extracted from the two modalities were directly concatenated and fed into the classifier for decisionmaking.This modification retained the unimodal feature extraction structures but omitted the explicit feature interaction mechanism between modalities, serving as a baseline for comparison against the proposed multimodal fusion strategy in the controlled experiments.</p>
<p>On the nback task, the classification accuracy of the original fusion model reached 98.13%, whereas it dropped to 91.58% after removing the fusion module.On the WG dataset, the accuracy decreased from 98.61 to 96.54%.These results demonstrate that the proposed fusion module effectively enhances model performance across both datasets.Notably, the accuracy improvement of 6.55 percentage points on the nback dataset highlights a more pronounced advantage of the fusion mechanism in capturing complementary information between modalities.</p>
<p>In summary, the designed fusion mechanism plays a critical role in leveraging the complementary information between EEG and fNIRS modalities and enhancing feature  Confusion matrix of WG experiment results.representation capabilities, thereby effectively improving the model's discriminative power and generalization performance.</p>
<p>Statistical verification of results</p>
<p>To quantitatively assess the significance of the performance improvements introduced by MBC-ATT, paired t-tests were conducted comparing MBC-ATT with three baseline models: the ablation model, the EEG-only model, and the fNIRS-only model.The results, presented in Figure 6, indicate that MBC-ATT consistently outperforms all baseline models.The obtained p-values are statistically significant (p &lt; 0.05), confirming that the multimodal fusion contributes substantially to improvements in classification accuracy.</p>
<p>On the nback and WG tasks, MBC-ATT effectively captures complementary features from EEG and fNIRS, thereby enhancing feature representation.The paired t-test results further confirm that these improvements are statistically significant, indicating that the high accuracies are robust and reliable.</p>
<p>Comparative analysis of methods</p>
<p>To comprehensively evaluate the performance of the proposed method, we conducted a comparative analysis with methods from relevant literature for both the n-back and WG tasks.Given the differences in cognitive load and signal characteristics between these two tasks, the selected comparison methods also vary accordingly.The following sections provide a The bold font represents the experimental results of the MBC-ATT method proposed in this manuscript.The bold font represents the experimental results of the MBC-ATT method proposed in this manuscript.</p>
<p>detailed discussion of the comparisons for the n-back and WG tasks, respectively.</p>
<p>Performance comparison of the n-back task</p>
<p>To comprehensively evaluate the performance of the proposed method, we compared it with four existing approaches, including Support Vector Machine (SVM) (Wu et al., 2020), Deep Neural Network (DNN) (Vaswani et al., 2017), a time-distributed CNN-LSTM method based on recurrence plots (CNN-LSTM) (Chiarelli et al., 2017), and a multimodal DenseNet fusion model based on Short-Time Fourier Transform (STFT-MDNF) (Lin et al., 2023).</p>
<p>As shown in Table 4, this study systematically compares the classification performance of five methods in the n-back task.Compared to SVM (83.00%),DNN (87.00%),CNN-LSTM (88.41%), and STFT-MDNF (95.10%), the MBC-ATT method (98.13%) achieves accuracy improvements of 15.13%, 11.13%, 9.72%, and 3.03%, respectively, significantly outperforming existing methods.These results strongly demonstrate the superior performance of the MBC-ATT method in cognitive load recognition and classification for the n-back task.</p>
<p>Performance comparison of the WG task</p>
<p>This study focuses on the characteristics of the WG task and compares four methods: Support Vector Machine (SVM) (Wu et al., 2020), Deep Neural Network (DNN) (Vaswani et al., 2017), EEG-fNIRS Convolutional Network (EF-Net) (Arif et al., 2024), and a Short-Time Fourier Transform-based Multimodal DenseNet Fusion Model (STFT-MDNF) (Lin et al., 2023).The primary focus is to evaluate the performance advantages of the proposed MBC-ATT model in the WG task.</p>
<p>As shown in Table 5, the proposed MBC-ATT method demonstrates superior performance in the WG task, achieving a classification accuracy of 98.61%.The performance comparison indicates that MBC-ATT improves accuracy by 24.62 percentage points over the traditional SVM method (73.99%), 6.61 percentage points over the deep neural network (DNN) (92.00%), 5.51 percentage points over STFT-MDNF (93.10%), and 2.32 percentage points over EF-Net (96.29%).These results strongly confirm the superior effectiveness of the MBC-ATT method in the WG task.</p>
<p>Discussion</p>
<p>The results of this study indicate that the Multimodal MBC-ATT framework significantly enhances classification accuracy in both the n-back and WG tasks, effectively demonstrating the feasibility and advantages of its cross-modal attention mechanism for EEG and fNIRS signal fusion.The framework achieved classification accuracies of 98.13 and 98.61% in the two tasks, respectively, outperforming existing fusion methods (as shown in Figure 7).</p>
<p>The superior performance achieved in this study can be primarily attributed to the proposed framework's innovative cross-modal attention mechanism.This mechanism leverages the Query-Key-Value interaction strategy across modalities to adaptively balance the contributions of EEG signals, which are characterized by high temporal resolution, and fNIRS signals, which provide high spatial resolution.In doing so, it enables precise feature-level alignment and effective complementary fusion.The mechanism effectively mitigates the inherent discrepancies between EEG and fNIRS in terms of both temporal dynamics and physiological representations.Specifically, EEG reflects electrophysiological activity on a millisecond scale, while fNIRS captures hemodynamic responses on a second scale.Additionally, EEG represents neural electrical activity, while fNIRS reflects blood oxygen metabolism.By preserving the unique information of each modality, this approach significantly enhances the representation of task-relevant features, thereby offering a novel and effective fusion strategy to optimize the performance of multimodal brain-computer interface systems.</p>
<p>Compared with existing multimodal fusion approaches, the proposed MBC-ATT model demonstrates superior performance.In contrast to early fusion strategies, which typically concatenate features from different modalities and are prone to mutual interference and loss of modality-specific information, MBC-ATT employs a branch-specific feature extraction architecture that effectively preserves the unique characteristics of each modality.Moreover, unlike late fusion methods that often perform simple integration at the decision level and thus overlook crossmodal feature interactions, MBC-ATT incorporates an attention mechanism to enable dynamic fusion at the feature level.This design not only facilitates more effective inter-modal information exchange but also aligns well with the physiological basis of neurovascular coupling.The proposed approach demonstrates significant application potential across various domains.Achieving a classification accuracy exceeding 98%, the model provides a robust foundation for the development of real-time BCI systems, particularly in scenarios such as cognitive workload monitoring and neurofeedback training.The attention weight distributions offer a novel methodology for quantifying functional connectivity strength across brain regions under different task states, thereby facilitating deeper investigations into multimodal brain network dynamics.Despite these promising results, several limitations remain to be addressed.The current model requires a high degree of temporal synchronization between modalities, highlighting the need for future research into fusion strategies that can accommodate asynchronous signals.Additionally, the model's generalization ability on small-sample patient datasets has yet to be thoroughly validated and may benefit from the integration of techniques such as transfer learning.Furthermore, the relatively high computational cost poses challenges for real-time deployment.To address this, future work will explore lightweight solutions, including knowledge distillation, to enhance system efficiency.These advancements are expected to strengthen the practical applicability of the proposed method in real-world BCI applications.</p>
<p>Conclusion</p>
<p>This study introduces MBC-ATT for cognitive state classification utilizing EEG-fNIRS multimodal data.Extensive experimental evaluations demonstrate that MBC-ATT consistently achieves superior classification performance in both n-back and WG tasks, compared to traditional machine learning models and deep learning models.</p>
<p>The proposed MBC-ATT framework employs a multi-branch convolutional architecture to effectively extract spatiotemporal features from EEG and fNIRS signals.An integrated attention mechanism further enhances feature fusion by selectively emphasizing salient neural patterns.This design not only improves the model's discriminative capability for different cognitive states but also strengthens its generalization performance across subjects.</p>
<p>The experimental results substantiate the effectiveness of MBC-ATT in cognitive load recognition and spontaneous language generation tasks.Future research will focus on further optimizing the model architecture to enhance adaptability to individual variability, as well as exploring its potential applications in a wider range of cognitive paradigms and real-time BCI systems.</p>
<p>FIGURE 1 n
1
FIGURE 1n-back task paradigm flowchart.</p>
<p>FIGURE 2 WG
2
FIGURE 2WG task paradigm flowchart.</p>
<p>FIGURE 3MBC-ATT network architecture.</p>
<p>Frontiers</p>
<p>FIGURE 5
5
FIGURE 5</p>
<p>FIGURE 6
6
FIGURE 6 Multimodal MBC-ATT: statistical validation.(a) Statistical validation for n-back task.(b) Statistical validation for WG task.</p>
<p>FIGURE 7
7
FIGURE 7 Comparison of experimental results.(a) Classification performance on the n-back task.(b) Classification performance on the WG task.</p>
<p>TABLE 1 Classification performance metrics of each subject in the n-back task.
1Li et al.</p>
<p>TABLE 2 Classification performance metrics of each subject in the WG task.
2Li et al.</p>
<p>TABLE 3 Performance of the model on n-back and WG datasets under five-fold cross-validation.
3
The bold font represents the average value of the previous five fold cross validation.
Dataset MetricFold 1Fold 2Fold 3Fold 4Fold 5AverageAccuracy0.97390.97750.97860.97390.97270.9753n-backPrecision0.97500.97760.97900.97400.97230.9756Recall0.97320.97720.97850.97410.97340.9753F1-score0.97390.97740.97870.97400.97280.9754Accuracy0.96020.96390.97620.97090.96280.9668WGPrecision0.95500.96050.97500.96980.96000.9640Recall0.95200.95850.97480.96960.95840.9627F1-score0.95350.95950.97490.96970.95920.9634</p>
<p>TABLE 4 Performance comparison of different methods on the n-back task.
4Frontiers in Human Neuroscience10frontiersin.org</p>
<p>TABLE 5 Performance comparison of different methods on the WG task.
5MethodAccuracy (%)SVM73.99%DNN92.00%STFT-MDNF93.10%EF-Net96.29%MBC-ATT98.61%
Frontiers in Human Neuroscience
Data availability statementThe original contributions presented in the study are included in the article/supplementary material, further inquiries can be directed to the corresponding author.FundingThe author(s) declare that financial support was received for the research and/or publication of this article.This work was funded by the Key Research and Development Project of Zhejiang Province (2020C04009).Conflict of interestThe authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.Generative AI statementThe author(s) declare that no Gen AI was used in the creation of this manuscript.Any alternative text (alt text) provided alongside figures in this article has been generated by Frontiers with the support of artificial intelligence and reasonable efforts have been made to ensure accuracy, including review by the authors wherever possible.If you identify any issues, please contact us.Publisher's noteAll claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers.Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher.
EF-Net: mental state recognition by analyzing multimodal EEG-fNIRS via CNN. A Arif, Y Wang, R Yin, X Zhang, A Helmy, 10.3390/s24061889Sensors. 2418892024</p>
<p>. N J Bourguignon, S L Bue, C Guerrero-Mosquera, G Borragán, 2022</p>
<p>Bimodal EEG-fNIRS in neuroergonomics. current evidence and prospects for future research. 10.3389/fnrgo.2022.934234Front. Neuroergon. 3934234</p>
<p>Simultaneous EEG-fNIRS data classification through selective channel representation and spectrogram imaging. C Bunterngchit, J Wang, Z G Hou, 10.1109/JTEHM.2024.3448457IEEE J. Transl. Eng. Health Med. 122024</p>
<p>EEG/fNIRS based workload classification using functional brain connectivity and machine learning. J Cao, E M Garro, Y Zhao, 10.3390/s22197623Sensors. 2276232022</p>
<p>fNIRS-EEG bcis for motor rehabilitation: a review. J Chen, Y Xia, X Zhou, E Vidal Rosas, A Thomas, R Loureiro, 10.3390/bioengineering10121393Bioengineering. 1013932023</p>
<p>Simultaneous functional near-infrared spectroscopy and electroencephalography for monitoring of human brain activity and oxygenation: a review. A M Chiarelli, F Zappasodi, F Di Pompeo, Merla , A , 10.1117/1.NPh.4.4.041411Neurophotonics. 4414112017</p>
<p>Emotion recognition in EEG signals using deep learning methods: a review. M Jafari, A Shoeibi, M Khodatars, S Bagherzadeh, A Shalbaf, D L García, 10.1016/j.compbiomed.2023.107450Comput. Biol. Med. 1651074502023</p>
<p>Analysis of human gait using hybrid EEG-fNIRS-based BCI system: a review. H Khan, N Naseer, A Yazidi, P K Eide, H W Hassan, P Mirtaheri, 10.3389/fnhum.2020.613254Front. Hum. Neurosci. 146132542021</p>
<p>FGANET: fNIRS-guided attention network for hybrid EEG-fNIRS brain-computer interfaces. Y Kwak, W.-J Song, S.-E Kim, 10.1109/TNSRE.2022.3149899IEEE Trans. Neural Syst. Rehabil. Eng. 302022</p>
<p>An intelligent fault diagnosis method using unsupervised feature learning towards mechanical big data. Y Lei, F Jia, J Lin, S Xing, S X Ding, 10.1109/TIE.2016.2519325IEEE Trans. Ind. Electron. 632016</p>
<p>10.3389/fnhum.2025.1660532Frontiers in Human Neuroscience 13 frontiersin.org Li et. </p>
<p>Concurrent fNIRS and EEG for brain function investigation: a systematic, methodology-focused review. R Li, D Yang, F Fang, K S Hong, A L Reiss, Y Zhang, 10.3390/s22155865Sensors. 2258652022</p>
<p>Early-stage fusion of EEG and fNIRS improves classification of motor imagery. Y Li, X Zhang, M Dong, 10.3389/fnins.2022.1062889Front. Neurosci. 1610628892023</p>
<p>An EEG-fNIRS neurovascular coupling analysis method to investigate cognitive-motor interference. J Lin, J Lu, Z Shu, N Yu, J Han, 10.1016/j.compbiomed.2023.106968Comput. Biol. Med. 1601069682023</p>
<p>A systematic review on hybrid EEG/fNIRS in brain-computer interface. Z Liu, J Shore, M Wang, F Yuan, A Buss, X Zhao, 10.1016/j.bspc.2021.102595Biomed. Signal Process. Control. 681025952021</p>
<p>The open EEGlab portal interface: high-performance computing with EEGlab. R Martínez-Cancino, A Delorme, D Truong, F Artoni, K Kreutz-Delgado, S Sivagnanam, 10.1016/j.neuroimage.2020.116778Neuroimage. 2241167782021</p>
<p>EEG-fNIRS-based hybrid image construction and classification using cnn-lstm. N E Mughal, M J Khan, K Khalil, K Javed, H Sajid, N Naseer, 10.3389/fnbot.2022.873239Front. Neurorobot. 168732392022</p>
<p>A novel classification framework using multiple bandwidth method with optimized cnn for brain-computer interfaces with EEG-fNIRS signals. M Nour, Ş Öztürk, K Polat, 10.1007/s00521-021-06202-4Neural Comput. Appl. 332021</p>
<p>Simultaneous acquisition of EEG and nirs during cognitive tasks for an open access dataset. R O Ogundokun, R Maskeliunas, S Misra, R Damaševicius, R Sharma, H K Meena, 10.1038/sdata.2018.3doi: 10.1038/sdata.2018.3International Conference on Computational Science and Its Applications. J Von Lühmann, A Kim, D W Mehnert, J Hwang, H J Müller, K R , ChamSpringer International Publishing2022. 2024. 20185Sci. Data</p>
<p>A novel multimodal approach for hybrid brain-computer interface. Z Sun, Z Huang, F Duan, Y Liu, 10.1109/ACCESS.2020.2994226IEEE Access. 82020</p>
<p>Past, present, and future of EEG-based bci applications. K Värbu, N Muhammad, Y Muhammad, 10.3390/s22093331Sensors. 2233312022</p>
<p>Attention is all you needm. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, Advances in Information Processing Systems. Long Beach, CA201730</p>
<p>An electroencephalographic signature predicts antidepressant response in major depression. W Wu, Y Zhang, J Jiang, M V Lucas, G A Fonzo, C E Rolle, 10.1038/s41587-019-0397-3Nat. Biotechnol. 382020</p>
<p>E-FNet: a EEG-fNIRS dual-stream model for brain-computer interfaces. B Yu, L Cao, J Jia, C Fan, Y Dong, C Zhu, 10.1016/j.bspc.2024.106943Biomed. Signal Process. Control. 1001069432025</p>
<p>. Frontiers in Human Neuroscience. 14</p>            </div>
        </div>

    </div>
</body>
</html>
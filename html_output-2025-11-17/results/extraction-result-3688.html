<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3688 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3688</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3688</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-91.html">extraction-schema-91</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-260165117</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2307.10930v2.pdf" target="_blank">MediaGPT : A Large Language Model For Chinese Media</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have shown remarkable capabilities in generating high-quality text and making predictions based on large amounts of data, including the media domain. However, in practical applications, the differences between the media's use cases and the general-purpose applications of LLMs have become increasingly apparent, especially Chinese. This paper examines the unique characteristics of media-domain-specific LLMs compared to general LLMs, designed a diverse set of task instruction types to cater the specific requirements of the domain and constructed unique datasets that are tailored to the media domain. Based on these, we proposed MediaGPT, a domain-specific LLM for the Chinese media domain, training by domain-specific data and experts SFT data. By performing human experts evaluation and strong model evaluation on a validation set, this paper demonstrated that MediaGPT outperforms mainstream models on various Chinese media domain tasks and verifies the importance of domain data and domain-defined prompt types for building an effective domain-specific LLM.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3688.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3688.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BioMedLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BioMedLM</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 2.7B-parameter GPT-style domain-specific language model trained on biomedical literature (PubMed) and reported to achieve state-of-the-art performance on medical question answering tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Biomedlm: a domain-specific large language model for biomedical text</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>BioMedLM</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>Domain-specific LLM trained on biomedical literature to capture biomedical knowledge for downstream medical QA and related tasks; described in the paper as an example of DS-LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Biomedical text from PubMed (exact size/number of papers not specified in this citing paper); model size reported as ~2.7B parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Training on domain corpus (pretraining/fine-tuning) using PubMed biomedical text; the cited description in this paper only states the model was trained on PubMed.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Natural-language answers for medical question-answering tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Evaluated on medical question answering benchmarks (the citing paper states it achieves SOTA on medical QA; exact benchmarks/metrics not provided here).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Reported state-of-the-art performance on medical question answering according to the referenced work; no quantitative metrics are given in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>Stated to outperform prior models on medical QA (described as achieving SOTA in the referenced work).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3688.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3688.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BloombergGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BloombergGPT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 50-billion-parameter domain-specific LLM for finance trained using Bloomberg's proprietary financial data along with general-purpose datasets, reported to perform strongly on standard language model and financial benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Bloomberggpt: A large language model for finance.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>BloombergGPT</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>Domain-specific LLM leveraging Bloomberg's rich financial data and additional general datasets to handle various financial tasks and benchmarks; presented as an example of DS-LLMs in related work.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Bloomberg's proprietary financial data plus general-purpose datasets; model size reported as ~50B parameters. Specific corpus statistics are not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Pretraining on proprietary financial corpora combined with general-purpose data and subsequent fine-tuning for financial tasks (details are referenced but not elaborated here).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Natural-language outputs and predictions for financial benchmarks and internal financial tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Reported evaluation on standard LM benchmarks and open financial benchmarks as well as internal tasks (specific metrics not reported in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Reported strong performance on LM and financial benchmarks according to the cited BloombergGPT work; quantitative results are not reproduced here.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>Reported to perform well relative to baselines on language-model and domain-specific financial benchmarks in the referenced work.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3688.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3688.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FinGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>FinGPT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source set of financial large language models (referred to as FinGPT/Fingpt) intended for financial domain applications and tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Fingpt: Open-source financial large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>FinGPT (Fingpt)</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>Open-source financial LLM(s) developed for finance domain tasks; cited as an example of domain-specific LLM efforts in finance.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Financial-domain data (specific sources, sizes, and composition are not described in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Domain-specific training/fine-tuning on financial datasets (exact procedure not detailed in this citing paper).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Natural-language outputs for financial tasks and benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3688.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3688.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DomainSpecSurvey</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Domain specialization as the key to make large language models disruptive: A comprehensive survey</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A systematic taxonomy and review of domain-specialization techniques for large language models, organized around accessibility to LLMs and availability of domain data; cited as contextual background for DS-LLM methods.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Domain specialization as the key to make large language models disruptive: A comprehensive survey</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>DS-LLM survey / taxonomy</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>A survey paper that catalogs approaches to building domain-specific LLMs (e.g., pretraining on domain corpora, fine-tuning, prompt methods) and discusses benefits and challenges; used in the paper as related work grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Survey summarizes methods (pretraining, fine-tuning, prompt engineering, etc.); the citing paper does not provide further specifics from the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Survey/taxonomy (review article) rather than a single model output.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Provides a taxonomy and review of DS-LLM techniques (details not reproduced here).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3688.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3688.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (as evaluator)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A strong large language model used in this paper as an automated evaluator (judge) with a finetuned evaluator prompt set to rank outputs from candidate models; the paper reports high agreement between GPT-4 rankings and human expert judgments when using prompt tuning techniques.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>GPT-4 evaluator with finetuned evaluator prompts</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>Used as a strong-model judge to rank and compare outputs from multiple LLMs; the paper proposes and applies a finetuned evaluator prompt set and several procedural controls (empty placeholder, randomized ordering, normalized references, ranking instead of scoring) to improve stability and precision of automated evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Candidate model outputs (answers) generated for the paper's domain-specific evaluation set (each main category contains ~100 questions; answers from ChatGPT-3.5, ERNIE Bot, MediaGPT-generalSFT, MediaGPT-domainSFT).</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Evaluation tasks and prompts mirroring human expert criteria for media-domain tasks; evaluator prompts are tuned to follow those criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Not used to synthesize scholarly literature; used for ranking via prompt-engineered evaluation. Techniques used: prompt tuning, insertion of an empty placeholder answer, randomization of answer order, normalization of answer references, and ranking-based outputs to reduce bias and instability.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Rankings of candidate model outputs (rank positions and pairwise 'win' probabilities), aggregated rank statistics (Avg.rank, Rank-n rates, Compared win rate).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Comparison against human expert double-blind evaluation; the paper reports similarity between GPT-4 judgments (with tuned prompts) and human judgments, and presents rank/win-rate tables.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Finetuned evaluator prompts improved stability and precision of GPT-4 evaluations; GPT-4 rankings with tuned prompts closely resembled human expert rankings in this media-domain evaluation and placed MediaGPT-domainSFT highest among tested models (tables provided).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Authors note instability and potential bias in strong-model evaluation despite prompt tuning; strong-model evaluations carry risk due to inherent instability of the evaluator model.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>Automated GPT-4 rankings (with finetuned prompts) were reported to be similar to human expert rankings; the paper presents comparative rank and win-rate statistics between models.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Biomedlm: a domain-specific large language model for biomedical text <em>(Rating: 2)</em></li>
                <li>Bloomberggpt: A large language model for finance. <em>(Rating: 2)</em></li>
                <li>Fingpt: Open-source financial large language models. <em>(Rating: 1)</em></li>
                <li>Domain specialization as the key to make large language models disruptive: A comprehensive survey <em>(Rating: 1)</em></li>
                <li>Training language models to follow instructions with human feedback <em>(Rating: 1)</em></li>
                <li>Gpteval: Nlg evaluation using gpt-4 with better human alignment <em>(Rating: 1)</em></li>
                <li>Gptscore: Evaluate as you desire <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3688",
    "paper_id": "paper-260165117",
    "extraction_schema_id": "extraction-schema-91",
    "extracted_data": [
        {
            "name_short": "BioMedLM",
            "name_full": "BioMedLM",
            "brief_description": "A 2.7B-parameter GPT-style domain-specific language model trained on biomedical literature (PubMed) and reported to achieve state-of-the-art performance on medical question answering tasks.",
            "citation_title": "Biomedlm: a domain-specific large language model for biomedical text",
            "mention_or_use": "mention",
            "system_or_method_name": "BioMedLM",
            "system_or_method_description": "Domain-specific LLM trained on biomedical literature to capture biomedical knowledge for downstream medical QA and related tasks; described in the paper as an example of DS-LLMs.",
            "input_corpus_description": "Biomedical text from PubMed (exact size/number of papers not specified in this citing paper); model size reported as ~2.7B parameters.",
            "topic_or_query_specification": null,
            "distillation_method": "Training on domain corpus (pretraining/fine-tuning) using PubMed biomedical text; the cited description in this paper only states the model was trained on PubMed.",
            "output_type_and_format": "Natural-language answers for medical question-answering tasks.",
            "evaluation_or_validation_method": "Evaluated on medical question answering benchmarks (the citing paper states it achieves SOTA on medical QA; exact benchmarks/metrics not provided here).",
            "results_summary": "Reported state-of-the-art performance on medical question answering according to the referenced work; no quantitative metrics are given in this paper.",
            "limitations_or_challenges": null,
            "comparison_to_baselines_or_humans": "Stated to outperform prior models on medical QA (described as achieving SOTA in the referenced work).",
            "uuid": "e3688.0"
        },
        {
            "name_short": "BloombergGPT",
            "name_full": "BloombergGPT",
            "brief_description": "A 50-billion-parameter domain-specific LLM for finance trained using Bloomberg's proprietary financial data along with general-purpose datasets, reported to perform strongly on standard language model and financial benchmarks.",
            "citation_title": "Bloomberggpt: A large language model for finance.",
            "mention_or_use": "mention",
            "system_or_method_name": "BloombergGPT",
            "system_or_method_description": "Domain-specific LLM leveraging Bloomberg's rich financial data and additional general datasets to handle various financial tasks and benchmarks; presented as an example of DS-LLMs in related work.",
            "input_corpus_description": "Bloomberg's proprietary financial data plus general-purpose datasets; model size reported as ~50B parameters. Specific corpus statistics are not provided in this paper.",
            "topic_or_query_specification": null,
            "distillation_method": "Pretraining on proprietary financial corpora combined with general-purpose data and subsequent fine-tuning for financial tasks (details are referenced but not elaborated here).",
            "output_type_and_format": "Natural-language outputs and predictions for financial benchmarks and internal financial tasks.",
            "evaluation_or_validation_method": "Reported evaluation on standard LM benchmarks and open financial benchmarks as well as internal tasks (specific metrics not reported in this paper).",
            "results_summary": "Reported strong performance on LM and financial benchmarks according to the cited BloombergGPT work; quantitative results are not reproduced here.",
            "limitations_or_challenges": null,
            "comparison_to_baselines_or_humans": "Reported to perform well relative to baselines on language-model and domain-specific financial benchmarks in the referenced work.",
            "uuid": "e3688.1"
        },
        {
            "name_short": "FinGPT",
            "name_full": "FinGPT",
            "brief_description": "An open-source set of financial large language models (referred to as FinGPT/Fingpt) intended for financial domain applications and tasks.",
            "citation_title": "Fingpt: Open-source financial large language models.",
            "mention_or_use": "mention",
            "system_or_method_name": "FinGPT (Fingpt)",
            "system_or_method_description": "Open-source financial LLM(s) developed for finance domain tasks; cited as an example of domain-specific LLM efforts in finance.",
            "input_corpus_description": "Financial-domain data (specific sources, sizes, and composition are not described in this paper).",
            "topic_or_query_specification": null,
            "distillation_method": "Domain-specific training/fine-tuning on financial datasets (exact procedure not detailed in this citing paper).",
            "output_type_and_format": "Natural-language outputs for financial tasks and benchmarks.",
            "evaluation_or_validation_method": null,
            "results_summary": null,
            "limitations_or_challenges": null,
            "comparison_to_baselines_or_humans": null,
            "uuid": "e3688.2"
        },
        {
            "name_short": "DomainSpecSurvey",
            "name_full": "Domain specialization as the key to make large language models disruptive: A comprehensive survey",
            "brief_description": "A systematic taxonomy and review of domain-specialization techniques for large language models, organized around accessibility to LLMs and availability of domain data; cited as contextual background for DS-LLM methods.",
            "citation_title": "Domain specialization as the key to make large language models disruptive: A comprehensive survey",
            "mention_or_use": "mention",
            "system_or_method_name": "DS-LLM survey / taxonomy",
            "system_or_method_description": "A survey paper that catalogs approaches to building domain-specific LLMs (e.g., pretraining on domain corpora, fine-tuning, prompt methods) and discusses benefits and challenges; used in the paper as related work grounding.",
            "input_corpus_description": null,
            "topic_or_query_specification": null,
            "distillation_method": "Survey summarizes methods (pretraining, fine-tuning, prompt engineering, etc.); the citing paper does not provide further specifics from the survey.",
            "output_type_and_format": "Survey/taxonomy (review article) rather than a single model output.",
            "evaluation_or_validation_method": null,
            "results_summary": "Provides a taxonomy and review of DS-LLM techniques (details not reproduced here).",
            "limitations_or_challenges": null,
            "comparison_to_baselines_or_humans": null,
            "uuid": "e3688.3"
        },
        {
            "name_short": "GPT-4 (as evaluator)",
            "name_full": "GPT-4",
            "brief_description": "A strong large language model used in this paper as an automated evaluator (judge) with a finetuned evaluator prompt set to rank outputs from candidate models; the paper reports high agreement between GPT-4 rankings and human expert judgments when using prompt tuning techniques.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_or_method_name": "GPT-4 evaluator with finetuned evaluator prompts",
            "system_or_method_description": "Used as a strong-model judge to rank and compare outputs from multiple LLMs; the paper proposes and applies a finetuned evaluator prompt set and several procedural controls (empty placeholder, randomized ordering, normalized references, ranking instead of scoring) to improve stability and precision of automated evaluation.",
            "input_corpus_description": "Candidate model outputs (answers) generated for the paper's domain-specific evaluation set (each main category contains ~100 questions; answers from ChatGPT-3.5, ERNIE Bot, MediaGPT-generalSFT, MediaGPT-domainSFT).",
            "topic_or_query_specification": "Evaluation tasks and prompts mirroring human expert criteria for media-domain tasks; evaluator prompts are tuned to follow those criteria.",
            "distillation_method": "Not used to synthesize scholarly literature; used for ranking via prompt-engineered evaluation. Techniques used: prompt tuning, insertion of an empty placeholder answer, randomization of answer order, normalization of answer references, and ranking-based outputs to reduce bias and instability.",
            "output_type_and_format": "Rankings of candidate model outputs (rank positions and pairwise 'win' probabilities), aggregated rank statistics (Avg.rank, Rank-n rates, Compared win rate).",
            "evaluation_or_validation_method": "Comparison against human expert double-blind evaluation; the paper reports similarity between GPT-4 judgments (with tuned prompts) and human judgments, and presents rank/win-rate tables.",
            "results_summary": "Finetuned evaluator prompts improved stability and precision of GPT-4 evaluations; GPT-4 rankings with tuned prompts closely resembled human expert rankings in this media-domain evaluation and placed MediaGPT-domainSFT highest among tested models (tables provided).",
            "limitations_or_challenges": "Authors note instability and potential bias in strong-model evaluation despite prompt tuning; strong-model evaluations carry risk due to inherent instability of the evaluator model.",
            "comparison_to_baselines_or_humans": "Automated GPT-4 rankings (with finetuned prompts) were reported to be similar to human expert rankings; the paper presents comparative rank and win-rate statistics between models.",
            "uuid": "e3688.4"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Biomedlm: a domain-specific large language model for biomedical text",
            "rating": 2,
            "sanitized_title": "biomedlm_a_domainspecific_large_language_model_for_biomedical_text"
        },
        {
            "paper_title": "Bloomberggpt: A large language model for finance.",
            "rating": 2,
            "sanitized_title": "bloomberggpt_a_large_language_model_for_finance"
        },
        {
            "paper_title": "Fingpt: Open-source financial large language models.",
            "rating": 1,
            "sanitized_title": "fingpt_opensource_financial_large_language_models"
        },
        {
            "paper_title": "Domain specialization as the key to make large language models disruptive: A comprehensive survey",
            "rating": 1,
            "sanitized_title": "domain_specialization_as_the_key_to_make_large_language_models_disruptive_a_comprehensive_survey"
        },
        {
            "paper_title": "Training language models to follow instructions with human feedback",
            "rating": 1,
            "sanitized_title": "training_language_models_to_follow_instructions_with_human_feedback"
        },
        {
            "paper_title": "Gpteval: Nlg evaluation using gpt-4 with better human alignment",
            "rating": 1,
            "sanitized_title": "gpteval_nlg_evaluation_using_gpt4_with_better_human_alignment"
        },
        {
            "paper_title": "Gptscore: Evaluate as you desire",
            "rating": 1,
            "sanitized_title": "gptscore_evaluate_as_you_desire"
        }
    ],
    "cost": 0.01464825,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>MEDIAGPT : A LARGE LANGUAGE MODEL FOR CHINESE MEDIA</p>
<p>Zhonghao Wang 
State Key Laboratory of Media Convergence Production Technology and Systems
China</p>
<p>Zijia Lu 
State Key Laboratory of Media Convergence Production Technology and Systems
China</p>
<p>Bo Jin jinbo@xinhua.org 
State Key Laboratory of Media Convergence Production Technology and Systems
China</p>
<p>Haiying Deng denghaiying@xinhuaskl.com 
State Key Laboratory of Media Convergence Production Technology and Systems
China</p>
<p>MEDIAGPT : A LARGE LANGUAGE MODEL FOR CHINESE MEDIA
Artificial Intelligence · Large Language Model · Media
Large language models (LLMs) have shown remarkable capabilities in generating high-quality text and making predictions based on large amounts of data, including the media domain. However, in practical applications, the differences between the media's use cases and the general-purpose applications of LLMs have become increasingly apparent, especially Chinese. This paper examines the unique characteristics of media-domain-specific LLMs compared to general LLMs, designed a diverse set of task instruction types to cater the specific requirements of the domain and constructed unique datasets that are tailored to the media domain. Based on these, we proposed MediaGPT, a domain-specific LLM for the Chinese media domain, training by domain-specific data and experts SFT data. By performing human experts evaluation and strong model evaluation on a validation set, this paper demonstrated that MediaGPT outperforms mainstream models on various Chinese media domain tasks and verifies the importance of domain data and domain-defined prompt types for building an effective domain-specific LLM.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs) have gained significant attention in recent years for their ability to generate high-quality text and make predictions based on large amounts of data. OpenAI's InstructGPT research [1] laid the foundation for this technology, and subsequent products like chatGPT [2] have demonstrated its effectiveness in diverse fields such as natural language processing, machine translation, and content creation.</p>
<p>The potential of LLM technology has been increasingly realized by the public. However, in order for LLMs to be effectively applied in industries and assist in improving work efficiency, they need to be tailored to meet specific domain demands. There are two main approaches to enhancing LLMs for domain-specific applications. One approach is to use prompts to improve model accuracy [3], while another approach is to train the model using domain-specific data, such as BloombergGPT and FinGPT [4,5].</p>
<p>In the media domain, for example, many professionals have found that generic LLMs often fail to meet their expectations due to the domain's particular characteristics, such as unique writing styles, narrative structures, and even differing political stances among media outlets [6,7]. Consequently, there is a growing need for LLMs specifically designed for the media domain.</p>
<p>This paper takes the media domain as the entry point and highlights the uniqueness of domain-specific large models compared to general large models. These unique features are not only reflected in pre-training samples, but also in various aspects such as prompts, SFT data instructions, and verification methods. Therefore, MediaGPT, an domain-specific large model designed for the Chinese media domain is proposed in this paper. It is trained using domain-specific data and fine-tuned using experts SFT data that capture media business requirements. With the help of experts, we constructed a validation set for the Chinese media domain, and performed human experts evaluation and strong model evaluation on it. Ultimately, MediaGPT demonstrates superior performance in Chinese media domain tasks and verified the importance of domain data.</p>
<p>Related Work</p>
<p>Large language models (LLMs) are neural network models that are trained on massive amounts of text data, and can generate natural language for various tasks and domains. In this section, we review some of the related work on LLMs, focusing on two aspects: domain-specific large language models (DS-LLMs) and Chinese large language models (CLLMs). DS-LLMs are LLMs that are trained or adapted on data from a specific domain, such as biomedical, legal, or financial text. CLLMs are LLMs that are trained or adapted on data from the Chinese language, which is one of the most widely spoken and written languages in the world. We discuss the potential and challenges of developing and applying DS-LLMs and CLLMs for various domains and applications.</p>
<p>Domain-specific Large Language Models</p>
<p>Domain-specific large language models (DS-LLMs) are LLMs that are trained or adapted on data from a specific domain, such as biomedical, legal, or financial text. DS-LLMs aim to capture the domain knowledge, terminology, and style of the target domain, and improve the performance of various downstream tasks in that domain. DS-LLMs can be obtained by either training a new LLM from scratch on domain data, or fine-tuning a pre-trained LLM on domain data. The latter approach is a form of transfer learning, where the pre-trained LLM serves as a general-purpose model that provides a good initialization for the domain-specific model.</p>
<p>Several studies have shown the benefits of DS-LLMs for different domains and tasks. For example, BioMedLM is a 2.7B parameter GPT model trained on biomedical data from PubMed, which achieves state-of-the-art results on medical question answering [8]. [9] provides a systematic taxonomy and review of DS-LLM techniques based on the accessibility to LLMs and the availability of domain data. BloombergGPT is a large language model that is specifically designed for the financial domain, with 50 billion parameters and the ability to handle various financial data and tasks [4]. It is an improvement over Bloom model, using Bloomberg's rich data sources and general-purpose datasets for training. It performs well on standard language model benchmarks and open financial benchmarks, as well as some internal financial tasks. These works illustrate the potential and challenges of developing and applying DS-LLMs for various domains and applications.</p>
<p>Chinese Large Language Models</p>
<p>Chinese large language models (CLLMs) are LLMs that are trained or adapted on data from the Chinese language, which is one of the most widely spoken and written languages in the world. CLLMs aim to capture the linguistic and cultural diversity, complexity, and richness of the Chinese language, and improve the performance of various downstream tasks in Chinese natural language processing. CLLMs can be obtained by either training a new LLM from scratch on Chinese data, or fine-tuning a pre-trained LLM on Chinese data. The latter approach is a form of transfer learning, where the pre-trained LLM serves as a general-purpose model that provides a good initialization for the Chinese-specific model.</p>
<p>Several studies have shown the benefits of CLLMs for different domains and tasks. For example, Safety Assessment of Chinese Large Language Models [10] is a technical report that evaluates and analyzes 15 CLLMs including the OpenAI GPT series and other well-known Chinese LLMs, where they observe some interesting findings on their safety issues and challenges. [11] is a paper that presents a practice on training large-scale autoregressive language models named PanGu, with up to 200 billion parameters, which demonstrates superior capabilities of performing various tasks under few-shot or zero-shot settings. An open source CLLM named Wu Dao 2.0 was trained using FastMoEs by Beijing Academy of Artificial Intelligence (BAAI), which claims to be 10 times larger than GPT-3 and can handle both natural language and images [12]. These works illustrate the potential and challenges of developing and applying CLLMs for various domains and applications.</p>
<p>Dataset</p>
<p>Unlabeled Pretrain Data</p>
<p>Unlabeled pretrain data is the data used for pre-training LLM before fine-tuning it on the target domain of Chinese media. We selected all the published data from influential Chinese media outlets since about 2000, as well as the published data from influential English media outlets with official backgrounds , totally about 250GB. These data sources are produced by highly professional media practitioners, and have high quality and credibility. The data provides a rich and diverse source of text data for LLM to learn from. It covers a wide range of topics and domains related to Chinese media, such as politics, economy, culture, society, sports, entertainment, science, technology, health, education, and international affairs, etc. It also includes some English texts from official media outlets that can help our LLM learn cross-lingual knowledge and skills. The unlabeled pretrain data can enhance the generality and robustness of the LLM's representations and knowledge for various tasks or domains in Chinese media.</p>
<p>Supervised Fine-tuning(SFT) Data</p>
<p>Supervised fine-tuning (SFT) is a technique that involves training or adapting a pre-trained large language model on a specific task or domain, using labeled data. SFT can be used to improve the performance of a language model on natural language generation, question answering, or text summarization tasks, by fine-tuning it on data that contains input-output pairs for the desired task or domain.</p>
<p>To enable MediaGPT to better learn the main tasks in the media domain, we surveyed the expert opinions of more than ten media companies or departments in China, and designed four major categories of SFT based on the main needs of Chinese media practitioners in the workflow of editing, writing and publishing, see Table 1. These categories contain over 80 specific types of SFT data:</p>
<p>• Opinion creation, which requires the MediaGPT to generate more content without deviating from the facts given by the prompt, such as adding opinions and presenting positions. Typical business scenarios include: outline generation based on topics, comment article generation, etc.</p>
<p>• Article transcription, which requires the MediaGPT to create content based on the information given by the prompt, without adding opinions or modifying facts, such as news script generation based on manuscripts, news style transcription, summary article writing, etc.</p>
<p>• Media understanding, which requires the MediaGPT to understand and extract the content or elements of the information given by the prompt, without adding opinions or modifying facts, such as headline generation, news element extraction, etc.</p>
<p>• Other QA, which are some additional general knowledge QA samples to improve the generalization and emergence abilities of the MediaGPT.</p>
<p>In the process of constructing these instruction samples, we aimed to make the MediaGPT's creation more professional. For the SFT samples that mainly focus on media creation, we used real-world excellent media articles as outputs, and let humans or code generate inputs. This way, the difficulty of constructing SFT samples was greatly reduced, and the professional outputs also improved the quality of the MediaGPT's generation. </p>
<p>Model</p>
<p>In recent times, it has come to our attention that LLMs have opened up new possibilities and sparked a revolution in various domains. The LLMs in media domain differ primarily in their application areas and the datasets they are trained on.This paper aims to improve the performance of LLMs in the media domain through the implementation of domain-specific pre-training and the precise definition of datasets for supervised fine-tuning. Additionally, we build upon the structure of popular generative open-source LLMs' structure to complete our model. Initially, we employed two popular generative open-source LLMs: BLOOMZ-7B [13] and LLaMA-7B [14] respectively as our base models. However, as our experiments progressed, it became evident that LLaMA-7B consistently outperformed BLOOMZ-7B. Therefore, we propose MediaGPT, a novel approach that involves supervised fine-tuning of the pre-trained model: LLaMA-7B. This decision was driven by LLaMA's superior performance and suitability for media domain tasks.</p>
<p>Architecture</p>
<p>LLaMA is a collection of LLMs trained on publicly available datasets and achieving efficient performance on various benchmarks. A variety of model sizes were trained ranging from 7 billion to 65 billion parameters. The underlying code of LLaMa is made available to researchers, enabling them to adjust the model according to their needs for various research, without any requirement for commercial licenses. LLaMA is designed as a multi-functional model that is suitable for various use cases, rather than just fine-tuning models for specific tasks. Furthermore, its requirements for computational power are relatively low.</p>
<p>LLaMA is based on the transformer architecture with various improvements that were subsequently proposed. Compared to GPT-3 [15], LLaMA incorporates RMSNorm as a normalizing function to enhance training stability by normalizing the input of each transformer sub-layer, replaces ReLU non-linearity with the SwiGLU activation function for improved performance, and replaces absolute positional embeddings with rotary positional embeddings (RoPE) added at each layer of the network.</p>
<p>Scale</p>
<p>MediaGPT is based on LLaMA-7B so the parameters of the model is about 7 billion. The number of hidden layers is 32, the number of attention heads is 32 and the hidden size is 4096. LLaMA possesses an impressive multilingual and cross-lingual understanding capability, especially within European languages. Its training dataset consists of 1.4 trillion tokens, primarily in English, along with some other European languages in Latin or Cyrillic scripts [14]. As a result, its ability to generate Chinese text is limited. To address this issue and improve encoding efficiency, we propose augmenting the LLaMA vocabulary with additional Chinese tokens and employing the model for the extended vocabulary [16]. By combining the Chinese tokenizer with the original LLaMA tokenizer, we have created the Chinese LLaMA tokenizer, which now encompasses about 50k tokenized words.</p>
<p>Training</p>
<p>In this paper, we present the MediaGPT model, which is built upon the open-source generative model LLaMA-7B. Initially, the model was pre-trained on unlabeled data from the Chinese media domain, as described in Section 3.1. Subsequently, we conducted fine-tuning using two different SFT datasets. The version fine-tuned on the open-source dataset is referred to MediaGPT-generalSFT, while the one fine-tuned on the Chinese media domain dataset described in Section 3.2 is referred to MediaGPT-domainSFT.</p>
<p>Evaluation</p>
<p>Quantitative evaluation of generative large language models is challenging, as there is no single dimension that can directly measure their performance. Most objective methods use multiple-choice or judgment questions, but the main goal of the Chinese media large language model is to handle subjective and open-ended questions, such as writing and creativity. These kinds of questions are best evaluated by human judges, but this approach is time-consuming and costly. Some recent works have used strong models as judges [17], where the models select which model is better, which slightly reduces the evaluation cost, but also severely affects the credibility.</p>
<p>We collected opinions from dozens of media domain experts, and selected several typical cases for evaluation in the Chinese media scenario, including human evaluation and strong model evaluation. We developed domainspecific dataset for the evaluation task, which is about main categories mentioned in Section 3.2. Each main category contains about 100 questions. Answers were generated by four different LLMs: ChatGPT-3.5 [2], ERNIE Bot [18], MediaGPT-generalSFT and MediaGPT-domainSFT. ChatGPT-3.5 and ERNIE Bot are mainstream English and Chinese 100B-level models respectively, and we use api for testing. It should be noted that about 2% of ernie's answers refused to answer because of some security issues. The architecture, scale and pre-training data of MediaGPT-generalSFT and MediaGPT-domainSFT are exactly the same. The only difference between them is the SFT data, as mentioned in Section 4.3.</p>
<p>We proposed three performance evaluation metrics to quantify the quality and relevance of the model's outputs for each case: Avg.rank, Rank-n rate, and Compared win rate. We calculated the Rank-n rate for each model, which is the number of times the model is ranked 1st, 2nd, 3rd, or 4th. Avg.rank is the average of Rank-n. Compared win rate is the winning probability under the situation of pairwise comparison.</p>
<p>Human experts evaluation</p>
<p>In human evaluation, we enlisted the participation of a dozen journalists and editors as media domain experts to assess the results of the random half evaluation set, which answers generated by four different LLMs.</p>
<p>Before the evaluation process, we firstly asked some of the most senior of the dozen experts to devise evaluation criteria based on the news values and editorial standards. For example, for Opinion Creation tasks, the criteria encompassed five general evaluation dimensions and specific dimensions for each subtask. As for Article Transcription tasks, the criteria primarily comprised four general evaluation dimensions and specific dimensions for respective subtask.</p>
<p>In the formal evaluation process, we formulated a series of questions for all types of tasks and had the ChatGPT-3.5, ERNIE Bot, MediaGPT-generalSFT and MediaGPT-domainSFT generate answers to these questions. The media domain experts conducted a double-blind test of the answers provided by the four LLMs, and throughout this process, the experts were required to assign scores and rank the answers strictly following predetermined criteria.</p>
<p>The human evaluation results are as follows ,see Table 2. It can be observed that under the criteria of news expertise, MediaGPT-domainSFT achieves the highest average rate and the Rank-1 rate, and the win-rate. MediaGPT-domainSFT performs well in the following two aspects: 1) Understanding the question within the realm of news and consistently crafting the answers in the role of mainstream media. 2) Producing outputs that align better with the format of news, avoiding excessive stiffness. However, MediaGPT-domainSFT still has some shortcomings, such as sometimes missing the details of the question and writing in an overconceptualization way.</p>
<p>Known as main stream models, ChatGPT-3.5 and ERNIE Bot's most notable features lie in their meticulous comprehension of the nuances of the questions and clear presentation logic. But it's like a double-edged sword, ChatGPT-3.5 and ERNIE Bot tend to be overly formulaic in their logic and expressions, and they sometimes fail to contextualize certain abstract topics within the domain of news and current affairs. Besides, ERNIE Bot falls short compared to ChatGPT-3.5 in terms of shorter and less logical answer, and it sometimes tends to avoid addressing sensitive political topics, which is critical for news writing and may not meet the necessary usage requirements.</p>
<p>Regarding MediaGPT-general SFT, its biggest problem is often generating results in the wrong format. This indicates the LLM couldn't tell the difference of each news genre and emphasizes the significance of domain SFT. </p>
<p>Strong model evaluation</p>
<p>Conventional reference-based metrics, such as BLEU and ROUGE, have demonstrated relatively low correlation with human judgments. [19] discussed the potential of employing a more strong model to evaluate the performance of less strong models. Additionally, [20] delved into methods for reducing bias and increasing the alignment with human judgment in the strong models evaluation.</p>
<p>We use GPT-4 as the evaluation model and have proposed an evaluator prompt set for media-domain LLMs. Based on [21], strong model evaluation has the problem of validity and reliability. We present our Finetuned Evaluator Prompts Set, using four techniques to improve the stability and precision of evaluation. 1) Insert empty answer text in the first position of the answer list generated by LLMs given to the evaluation model to prevent evaluation biases caused by the order of the answers. 2) Randomly mix the order of the other answers for the same reason mentioned above. 3)</p>
<p>Reference each answer with normal names to avoid chaos caused by the additional meaning of the reference name. 4) Rank the answers instead of scoring them to minimize the instability caused by the evaluation model. Through our experiments, we found that by using finely-tuned prompts, the accuracy and stability of the evaluation were significantly improved.</p>
<p>The evaluator prompt set is the same as human experts evaluation. We monitored the performance of our models MediaGPT-generalSFT and MediaGPT-domainSFT with two other mainstream models ChatGPT-3.5 and ERNIE Bot. The results demonstrate the MediaGPT's superior performance in Chinese media domain tasks. The comparison of MediaGPT-generalSFT and MediaGPT-domainSFT proves that the performance of DS-LLMs improves steadily, and correlates with the using of SFT data, see Table3.</p>
<p>In our experiments, we saw that finetuned evaluator prompts can enable strong model evaluation to achieve similar results to human experts judgement. The final judgement results3 also shows a great similarity with those of humans2, which further demonstrated the value of prompt tuning. However, it cannot be denied that the unstable performance of Strong Model itself [22] still poses a certain risk to the use of Strong model evaluation, even though it has the advantage of being fast and scalable. </p>
<p>Some results</p>
<p>In the appendix4, we show some typical questions in the working scenarios of the Chinese media domain, as well as the answers given by MediaGPT.</p>
<p>Conclusion</p>
<p>In this paper, we presented MediaGPT, a large language model for the Chinese media domain, which can generate high-quality and relevant outputs for various tasks in the Chinese media domain. We propose and construct a set of media instructions to cover the main demands of the Chinese media workflow, and built a system of SFT data based on these. Due to the difficulty of evaluation in the media domain, we proposed a adversarial evaluation method that combines human evaluation and strong model evaluation to measure the quality of the generative tasks. Under this evaluation framework, we demonstrated the superior performance of MediaGPT over existing models on main media domain cases, and verified the importance of domain data and domain-defined prompt types for building an effective domain-specific large language models. We hope that MediaGPT can be a valuable resource and tool for Chinese media practitioners and researchers, and inspire more innovations and applications in this domain. -te ⌘?ËÙõé9ÑúQ0:E⌘Ñ ; ( †:aQ 0 ·o °å"≥ §⌫LÔae ƒ✓ πøIÓòπb€LÜ ÔÅ¢"⇥:·≥'ú⌘ §⌫Â ûÑ˙L Bå·o ° Å ⌘?Ë®®íTQ0˛s g˝∂⌥∆ƒ⇤⌥ËaQ0 q°⌥Ë0 670Y⌥<em> µ÷Üh˝200⇢</em>0:É:∆-Ñ∫":OE⇥d&gt; ≈∫Ï˝ øw0(K:I˚®»Ô⌦∑ ÷¯s·o ÿ H"≥ÜO‹0:Ñ §⇢⌥ aeò ◊0Ü 'ú⌘↵ÀÑ"OEå/ ⇥</p>
<p>Table 1 :
1Rules and examples for constructing the Chinese media instruction datasetMajor categories 
Examples of specific instruc-
tion type </p>
<p>Example rules of specific samples 
{} means referenced content </p>
<p>Opinion creation 
Theme-based review article 
writing </p>
<p>Q:˜˙é{ theme};ò ⇣ «ƒ∫{á ‡ 
A:{published high-quality commentary news} </p>
<p>Outline creation </p>
<p>Q:˜˙éÂ↵Öπ ⇣ «á ‡'≤: 
{theme, information or something} 
A:{article outline} </p>
<p>Article transcription 
Summary generation </p>
<p>Q:˜`9nÂ↵∞˚ ⇣ µ100WÊÛ¸ 
e ?ˆ ‡sÑ·o: {a published article} 
A:{the abstract article that meets the requirements} </p>
<p>New media style transcrip-
tion </p>
<p>Q:˜`⌃Â↵%Éá ‡lô: «l⌫˜OE&lt;?ˆ: 
{a published article} 
A:{corresponding article published on WeChat} </p>
<p>Media understanding Headline generation </p>
<p>Q:˜:↵bá ‡ ⇣ *C íSOE&lt;Ñ⌥ò: 
{a published article} 
A:{headlines of the article in Q} </p>
<p>News element extraction </p>
<p>Q:˜-÷↵bá ‡Ñ∞˚Å : 
{a published article} 
A:{5W1H of the article in Q} </p>
<p>Other QA 
Self-awareness instruction 
Q:`Î¿H? 
A:⌘ÎMediaGPT </p>
<p>Table 2 :
2Results of human experts evaluationModel name Avg. rank 
Rank-n rate 
Compared win rate </p>
<p>Rank-
1 </p>
<p>Rank-
2 </p>
<p>Rank-
3 </p>
<p>Rank-
4 </p>
<p>ChatGPT-
3.5 </p>
<p>ERNIE-
Bot </p>
<p>MediaGPT-
generalSFT </p>
<p>MediaGPT-
domainSFT 
ChatGPT-
3.5[2] 
2.19 
29.9% 35.8% 19.4% 14.9% 
-
65.7% 
70.1% 
44.8% </p>
<p>ERNIE-
Bot[18] 
2.60 
14.9% 32.8% 29.9% 22.4% 
34.3% 
-
68.7% 
37.3% </p>
<p>MediaGPT-
generalSFT 
3.18 
10.5% 13.4% 23.9% 52.2% 
29.9% 
31.3% 
-
20.9% </p>
<p>MediaGPT-
domainSFT 
2.03 
44.8% 17.9% 26.9% 10.5% 
55.2% 
62.7% 
79.1% 
-</p>
<p>Table 3 :
3Results of strong model (GPT-4) evaluationModel name Avg. rank 
Rank-n rate 
Compared win rate </p>
<p>Rank-
1 </p>
<p>Rank-
2 </p>
<p>Rank-
3 </p>
<p>Rank-
4 </p>
<p>ChatGPT-
3.5 </p>
<p>ERNIE-
Bot </p>
<p>MediaGPT-
generalSFT </p>
<p>MediaGPT-
domainSFT 
ChatGPT-
3.5[2] 
2.62 
6.7% 
46.2% 25.2% 21.9% 
-
38.7% 
60.5% 
38.7% </p>
<p>ERNIE-
Bot[18] 
2.37 
24.4% 24.4% 41.2% 10.1% 
61.3% 
-
63.0% 
38.7% </p>
<p>MediaGPT-
generalSFT 
2.87 
14.3% 24.4% 21.9% 39.5% 
39.5% 
37.0% 
-
37.0% </p>
<p>MediaGPT-
domainSFT 
2.14 
54.6% 5.0% 11.8% 28.6% 
61.3% 
61.3% 
63.0% 
-</p>
<p>Table 5 :
5Some article transcription resultsQuestions by humanAnswers by MediaGPT˘Â↵µ=€L ¶rå9€ ÌÂ ⇢z (Õaen⇢Ÿ ‡t ⌘?Ë Ù ( †:aQ0 ˙aeÑ·o ° sË 0 ~"˙L ⇧ Î✓~ 00πÑ ∞ûÓò ⌘?Ë©íTQ0˛s ƒ⇤⌥ËaQ0 670⇢⌥a ∞û † Ñ0 Ü÷Üh˝200⇢<em>'É⇢∫O ÑúQ0π Ÿõ∞ûÑ0 /°πoe ‡Aø∫! Ÿà}0√€ÜúQ ∫⌘˙L¸</em> Î✓ 0QÃåúß¡ 0OEÃ⇥
A Some results≈%Õ%OÜ&gt;⇢OE Ù©Ü"baKÀ q°K ⇣"Ñ o&gt;⌘⇥ "ΩPoeN"/⇥:oeNÑßiK ü,(e™ı1˝◊∫ Hü⇥6 -te (F⇢) q®↵ ΩP⇣⇣ÿÜsS⇢ ∫⇤ΩPt pâ Ö?Ñ∫äΩPSˆfl †Ç60fl(⇢ÛLZ✏à9 Ù ⇢⇧ ⌃ΩP \:% Kµ :í\Çı€L"e% ""‹-;I". . . . . . ÕÕqa¸ÙΩPO ªÜ,I ÃªÜ √⇥ OE‰Û ‡∫/⇧ ⌦≥˛-⇧@ΩP8W:⌫ ÿ/ÔHÇ⇥~ˆ÷≤[≈2´ ë /~"Í∂⇧ΩPm`HV ˝t+@-N ⌘oe˘é} ;Ñ⌘Ä ¸⇥6 è@i( ;Â 0Ã ∫ÏäeäËÕ⇤ OLÑ¡(-G ΩPŸÕ"✏∂ßâ"⇣⇣´∑=⇥Ÿv-˙6 flá Â⇣✏ AEÑ ‡ F_ OD,⇣)å"/)/˛"Ñü≥⇥F∂↵∆Ü∫Ï˘é} ;Ñ⌘ Äå˘ flá Ñ ı 1Cπ~°OE à9⇧√⌃ ®˙⇧≈aeé ˜&lt;ÿ⇥Ñ ΩP&lt;" d·≥Ë⌃∫ÑZc√å G√⌃ OE b⇣ ÕxbÑ"ΩPoeN"⇥ Å9O"ΩPoeN"jOE™ 1ÅZ≥R⇤xb°é¬ Z ⌥OEûE˙-oeõ L oee:˙ M˙ j9⇥ ˆ sËËî †:-°õ ¶ %âS˚ï: ª%L: ˘"ΩPoeN"-Ñ›ƒ›'L:%È 7 ˆÿî!¸e∑á Ñnf Ô ¸' §⌫⌃'à9 Í…µ6N◊∏◊ö◊∞a q % ⌥ÌëbÑ &gt;⇢ Ù⇥ Í "›"ΩPoeN"« ¶⇧≈ M˝ c©ΩPfiR"üs" û∞ cÑˇrà9 á à9⇥
Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in Neural Information Processing Systems. 35Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730-27744, 2022.</p>
<p>. OpenAI. Introducing chatgpt. 2022OpenAI. Introducing chatgpt. https://openai.com/blog/chatgpt, 2022.</p>
<p>A prompt pattern catalog to enhance prompt engineering with chatgpt. Jules White, Quchen Fu, Sam Hays, Michael Sandborn, Carlos Olea, Henry Gilbert, Ashraf Elnashar, Jesse Spencer-Smith, Douglas C Schmidt, arXiv:2302.11382arXiv preprintJules White, Quchen Fu, Sam Hays, Michael Sandborn, Carlos Olea, Henry Gilbert, Ashraf Elnashar, Jesse Spencer-Smith, and Douglas C Schmidt. A prompt pattern catalog to enhance prompt engineering with chatgpt. arXiv preprint arXiv:2302.11382, 2023.</p>
<p>Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prabhanjan Kambadur, David Rosenberg, Gideon Mann, arXiv:2303.17564Bloomberggpt: A large language model for finance. arXiv preprintShijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prabhanjan Kambadur, David Rosenberg, and Gideon Mann. Bloomberggpt: A large language model for finance. arXiv preprint arXiv:2303.17564, 2023.</p>
<p>Fingpt: Open-source financial large language models. Hongyang Yang, Xiao-Yang Liu, Christina Dan Wang, arXiv:2306.06031arXiv preprintHongyang Yang, Xiao-Yang Liu, and Christina Dan Wang. Fingpt: Open-source financial large language models. arXiv preprint arXiv:2306.06031, 2023.</p>
<p>The function of chat gpt in social media: According to chat gpt. Available at SSRN 4405389. Som Biswas, Som Biswas. The function of chat gpt in social media: According to chat gpt. Available at SSRN 4405389, 2023.</p>
<p>Gpt-3: What's it good for?. Robert Dale, Natural Language Engineering. 271Robert Dale. Gpt-3: What's it good for? Natural Language Engineering, 27(1):113-118, 2021.</p>
<p>Biomedlm: a domain-specific large language model for biomedical text. A Venigalla, M Frankle, Carbin, MosaicML. Accessed. 233A Venigalla, J Frankle, and M Carbin. Biomedlm: a domain-specific large language model for biomedical text. MosaicML. Accessed: Dec, 23(3):2, 2022.</p>
<p>Domain specialization as the key to make large language models disruptive: A comprehensive survey. Xujiang Zhao, Chen Ling, arXiv:2305.18703arXiv preprintXujiang Zhao Chen Ling et al. Domain specialization as the key to make large language models disruptive: A comprehensive survey. arXiv preprint arXiv:2305.18703, 2023.</p>
<p>Safety assessment of chinese large language models. Hao Sun, Zhexin Zhang, Jiawen Deng, Jiale Cheng, Minlie Huang, arXiv:2304.10436arXiv preprintHao Sun, Zhexin Zhang, Jiawen Deng, Jiale Cheng, and Minlie Huang. Safety assessment of chinese large language models. arXiv preprint arXiv:2304.10436, 2023.</p>
<p>Wei Zeng, Xiaozhe Ren, Teng Su, Hui Wang, Yi Liao, Zhiwei Wang, Xin Jiang, Zhenzhang Yang, Kaisheng Wang, Xiaoda Zhang, arXiv:2104.12369Large-scale autoregressive pretrained chinese language models with auto-parallel computation. arXiv preprintWei Zeng, Xiaozhe Ren, Teng Su, Hui Wang, Yi Liao, Zhiwei Wang, Xin Jiang, ZhenZhang Yang, Kaisheng Wang, Xiaoda Zhang, et al. Pangu : Large-scale autoregressive pretrained chinese language models with auto-parallel computation. arXiv preprint arXiv:2104.12369, 2021.</p>
<p>Fastmoe: A fast mixture-of-expert training system. Jiaao He, Jiezhong Qiu, Aohan Zeng, Zhilin Yang, Jidong Zhai, Jie Tang, arXiv:2103.13262arXiv preprintJiaao He, Jiezhong Qiu, Aohan Zeng, Zhilin Yang, Jidong Zhai, and Jie Tang. Fastmoe: A fast mixture-of-expert training system. arXiv preprint arXiv:2103.13262, 2021.</p>
<p>Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, Sheng Bari, Zheng-Xin Shen, Hailey Yong, Schoelkopf, arXiv:2211.01786Crosslingual generalization through multitask finetuning. arXiv preprintNiklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, et al. Crosslingual generalization through multitask finetuning. arXiv preprint arXiv:2211.01786, 2022.</p>
<p>Llama: Open and efficient foundation language models. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Naman Baptiste Rozière, Eric Goyal, Faisal Hambro, Aurelien Azhar, Armand Rodriguez, Edouard Joulin, Guillaume Grave, Lample, Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. 2023.</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 33Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Nee- lakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.</p>
<p>Cino: A chinese minority pre-trained language model. Ziqing Yang, Zihang Xu, Yiming Cui, Baoxin Wang, Min Lin, Dayong Wu, Zhigang Chen, Ziqing Yang, Zihang Xu, Yiming Cui, Baoxin Wang, Min Lin, Dayong Wu, and Zhigang Chen. Cino: A chinese minority pre-trained language model. 2022.</p>
<p>Xuanwei Zhang and others from SuperCLUE team. Superclue: A benchmark for foundation models in chinese. Kangkang Zhao Lei Zhu Liang Xu, Kangkang Zhao Lei Zhu Liang Xu, Xuanwei Zhang and others from SuperCLUE team. Superclue: A benchmark for foundation models in chinese. https://github.com/CLUEbench/SuperCLUE, 2023.</p>
<p>. Ernie Baidu, Bot, Baidu. Ernie bot. https://yiyan.baidu.com/, 2023.</p>
<p>Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, Pengfei Liu, Gptscore, arXiv:2302.04166Evaluate as you desire. arXiv preprintJinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. Gptscore: Evaluate as you desire. arXiv preprint arXiv:2302.04166, 2023.</p>
<p>Large language models are not fair evaluators. Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, Zhifang Sui, arXiv:2305.17926arXiv preprintPeiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. Large language models are not fair evaluators. arXiv preprint arXiv:2305.17926, 2023.</p>
<p>Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, Chenguang Zhu, Gpteval, arXiv:2303.16634Nlg evaluation using gpt-4 with better human alignment. arXiv preprintYang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. Gpteval: Nlg evaluation using gpt-4 with better human alignment. arXiv preprint arXiv:2303.16634, 2023.</p>
<p>How is chatgpt's behavior changing over time?. Lingjiao Chen, Matei Zaharia, James Zou, arXiv:2307.09009arXiv preprintLingjiao Chen, Matei Zaharia, and James Zou. How is chatgpt's behavior changing over time? arXiv preprint arXiv:2307.09009, 2023.</p>
<p>⌃%é∞˚•s9ô, ∞íSS¡⇢ 2023t-˝ ‡∫:fi l. wWÍ⌃%É∞˚•S9ô:∞íSS¡⇢ 2023t-˝ ‡∫:fi l [ wWÍ</p>
<p>/Ÿ é7 13Â-15Â(wW w" ⇥&gt;L⇥nƒ'⇢ÀÕ 65 ¬[ K -ÖJp˝/R⌘tfiK ✏Ñ¬[ Kt≈6Å⇥\: y∞tÑz˝-Ä-® ‡∫:fi äeä◊0R⌘t å∂Ñ"OE⇥∞: ⌘ K∂h: √<code>‡∫:fi ≈0ÃÜiPÑ˛Y. /Ÿ é7 13Â-15Â(wW w" ⇥&gt;L⇥nƒ'⇢ÀÕ 65 ¬[ K -ÖJp˝/R⌘tfiK ✏Ñ¬[ Kt≈6Å⇥\: y∞tÑz˝- Ä-® ‡∫:fi äeä◊0R⌘t å∂Ñ"OE⇥∞: ⌘ K∂h: √</code> ‡∫:fi ≈0ÃÜiPÑ˛Y</p>
<p>≥ÓòÑ˝õ⇥dÜ◆⇢ [ãÂ ,!'[ÿaenÜ[SSå fiL8Ì⇢ ˜<code>‡∫:fi ⌘⇠. Ù-Güipñ®k˝õ Óã Õåïà, I ˚⌫0ÃÑhπ;®⇥[⇢;ûπ h: È(ë &gt;ûŸ7Ñ˝∂ß ‡ ∫:'[ / (0ÃiPÏÑëÙ-GÜiPÑ®K˝õ óã õåÏÀ"≥ÓòÑ˝õ⇥dÜ◆⇢ [ãÂ ,!'[ÿaenÜ[SSå fiL8Ì⇢ ˜</code> ‡∫:fi ⌘⇠[ I ˚⌫0ÃÑhπ;®⇥[⇢;ûπ h: È(ë &gt;ûŸ7Ñ˝∂ß ‡ ∫:'[ / (0ÃiPÏÑë</p>
<p><em>z</em>)-ÄŸ7Ñ<em>e-ÄÑf -e⇥ 2023h˝ ‡∫:fi l [ wWÍ8/Ÿ 7 13ÂÛ15Â(w"⇥&gt;L eÍh˝⌅065M¬[ K v-tÑ 'Ñ18Å ✏ÑÍ 6Å oe«$)Ñ¿»'" ⌘t:⇡˝:" ⌘tÏU∞˙^·ÑÕßÄ/ ∞: ⌘ K∂Ù ⇢«√<code>‡∫:fi iP ≈ÿó«bú≠ ÿ˝˘{÷ÏÑ⇣√å≈õ f} ‡∫:ŸyÄi PÏeÙà ✏I -te è@-ÄÑ-U n äeä⇢Ñ∫•Ê0Ü ‡∫: vú"⌦ÜÉ à⇢∫ §: Ÿy‚:¿»w´Ñ-® ⌃⇣:÷Ï*eÑÂ8 Ì1-Ä-®Ñ⌘tÏ Îe wfi⌦›. *z*)-ÄŸ7Ñ*e-ÄÑf -e⇥ 2023h˝ ‡∫:fi l [ wWÍ8/Ÿ 7 13ÂÛ15Â(w"⇥&gt;L eÍh˝⌅065M¬[ K v-tÑ 'Ñ18Å ✏ÑÍ 6Å oe«$)Ñ¿»'" ⌘t:⇡˝:" ⌘tÏU∞˙^·ÑÕßÄ/ ∞: ⌘ K∂Ù ⇢«√</code> ‡∫:fi iP ≈ÿó«bú≠ ÿ˝˘{÷ÏÑ⇣√å≈õ f} ‡∫:ŸyÄi PÏeÙà ✏I -te è@-ÄÑ-U n äeä⇢Ñ∫•Ê0Ü ‡∫: vú"⌦ÜÉ à⇢∫ §: Ÿy‚:¿»w´Ñ-® ⌃⇣:÷Ï</em>eÑÂ8 Ì1-Ä-®Ñ⌘tÏ Îe wfi⌦›)'</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5983 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5983</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5983</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-120.html">extraction-schema-120</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <p><strong>Paper ID:</strong> paper-259924919</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2307.07221v3.pdf" target="_blank">Software Testing With Large Language Models: Survey, Landscape, and Vision</a></p>
                <p><strong>Paper Abstract:</strong> Pre-trained large language models (LLMs) have recently emerged as a breakthrough technology in natural language processing and artificial intelligence, with the ability to handle large-scale datasets and exhibit remarkable performance across a wide range of tasks. Meanwhile, software testing is a crucial undertaking that serves as a cornerstone for ensuring the quality and reliability of software products. As the scope and complexity of software systems continue to grow, the need for more effective software testing techniques becomes increasingly urgent, making it an area ripe for innovative approaches such as the use of LLMs. This paper provides a comprehensive review of the utilization of LLMs in software testing. It analyzes 102 relevant studies that have used LLMs for software testing, from both the software testing and LLMs perspectives. The paper presents a detailed discussion of the software testing tasks for which LLMs are commonly used, among which test case preparation and program repair are the most representative. It also analyzes the commonly used LLMs, the types of prompt engineering that are employed, as well as the accompanied techniques with these LLMs. It also summarizes the key challenges and potential opportunities in this direction. This work can serve as a roadmap for future research in this area, highlighting potential avenues for exploration, and identifying gaps in our current understanding of the use of LLMs in software testing.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>A survey of large language models <em>(Rating: 2)</em></li>
                <li>Large language models for software engineering: A systematic literature review <em>(Rating: 2)</em></li>
                <li>Large language models for software engineering: Survey and open problems <em>(Rating: 1)</em></li>
                <li>Large language models meet nl2code: A survey <em>(Rating: 1)</em></li>
                <li>Prompt engineering guide <em>(Rating: 1)</em></li>
                <li>Talking about large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5983",
    "paper_id": "paper-259924919",
    "extraction_schema_id": "extraction-schema-120",
    "extracted_data": [],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "A survey of large language models",
            "rating": 2,
            "sanitized_title": "a_survey_of_large_language_models"
        },
        {
            "paper_title": "Large language models for software engineering: A systematic literature review",
            "rating": 2,
            "sanitized_title": "large_language_models_for_software_engineering_a_systematic_literature_review"
        },
        {
            "paper_title": "Large language models for software engineering: Survey and open problems",
            "rating": 1,
            "sanitized_title": "large_language_models_for_software_engineering_survey_and_open_problems"
        },
        {
            "paper_title": "Large language models meet nl2code: A survey",
            "rating": 1,
            "sanitized_title": "large_language_models_meet_nl2code_a_survey"
        },
        {
            "paper_title": "Prompt engineering guide",
            "rating": 1,
            "sanitized_title": "prompt_engineering_guide"
        },
        {
            "paper_title": "Talking about large language models",
            "rating": 1,
            "sanitized_title": "talking_about_large_language_models"
        }
    ],
    "cost": 0.012019249999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Software Testing with Large Language Models: Survey, Landscape, and Vision
4 Mar 2024</p>
<p>Junjie Wang junjie@iscas.ac.cn 
Yuchao Huang yuchao2019@iscas.ac.cn 
Chunyang Chen chunyang.chen@monash.edu 
S Wang </p>
<p>Zhe Liu
Song WangQing Wang</p>
<p>Institute of Software Chinese Academy of Sciences
State Key Laboratory of Intelligent Game
University of Chinese Academy of Sciences
BeijingChina. J. Wang</p>
<p>Monash University
MelbourneAustralia</p>
<p>York University
TorontoCanada</p>
<p>Software Testing with Large Language Models: Survey, Landscape, and Vision
4 Mar 2024C3CBB15AF7279FD6C43D2CCEA957961BarXiv:2307.07221v3[cs.SE]Pre-trained Large Language ModelSoftware TestingLLMGPT
Pre-trained large language models (LLMs) have recently emerged as a breakthrough technology in natural language processing and artificial intelligence, with the ability to handle large-scale datasets and exhibit remarkable performance across a wide range of tasks.Meanwhile, software testing is a crucial undertaking that serves as a cornerstone for ensuring the quality and reliability of software products.As the scope and complexity of software systems continue to grow, the need for more effective software testing techniques becomes increasingly urgent, making it an area ripe for innovative approaches such as the use of LLMs.This paper provides a comprehensive review of the utilization of LLMs in software testing.It analyzes 102 relevant studies that have used LLMs for software testing, from both the software testing and LLMs perspectives.The paper presents a detailed discussion of the software testing tasks for which LLMs are commonly used, among which test case preparation and program repair are the most representative.It also analyzes the commonly used LLMs, the types of prompt engineering that are employed, as well as the accompanied techniques with these LLMs.It also summarizes the key challenges and potential opportunities in this direction.This work can serve as a roadmap for future research in this area, highlighting potential avenues for exploration, and identifying gaps in our current understanding of the use of LLMs in software testing.</p>
<p>INTRODUCTION</p>
<p>Software testing is a crucial undertaking that serves as a cornerstone for ensuring the quality and reliability of software products.Without the rigorous process of software testing, software enterprises would be reluctant to release their products into the market, knowing the potential consequences of delivering flawed software to end-users.By conducting thorough and meticulous testing procedures, software enterprises can minimize the occurrence of critical software failures, usability issues, or security breaches that could potentially lead to financial losses or jeopardize user trust.Additionally, software testing helps to reduce maintenance costs by identifying and resolving issues early in the development lifecycle, preventing more significant complications down the line [1], [2].</p>
<p>The significance of software testing has garnered substantial attention within the research and industrial communities.In the field of software engineering, it stands as an immensely popular and vibrant research area.One can observe the undeniable prominence of software testing by simply examining the landscape of conferences and symposiums focused on software engineering.Amongst these events, topics related to software testing consistently dominate the submission numbers and are frequently selected for publication.</p>
<p>While the field of software testing has gained significant popularity, there remain dozens of challenges that have not been effectively addressed.For example, one such challenge is automated unit test case generation.Although various approaches, including search-based [3], [4], constraintbased [5] or random-based [6] techniques to generate a suite of unit tests, the coverage and the meaningfulness of the generated tests are still far from satisfactory [7], [8].Similarly, when it comes to mobile GUI testing, existing studies with random-/rule-based methods [9], [10], model-based methods [11], [12], and learning-based methods [13] are unable to understand the semantic information of the GUI page and often fall short in achieving comprehensive coverage [14], [15].Considering these limitations, numerous research efforts are currently underway to explore innovative techniques that can enhance the efficacy of software testing tasks, among which large language models are the most promising ones.</p>
<p>Large language models (LLMs) such as T5 and GPT-3 have revolutionized the field of natural language processing (NLP) and artificial intelligence (AI).These models, initially pre-trained on extensive corpora, have exhibited remarkable performance across a wide range of NLP tasks including question-answering, machine translation, and text generation [16]- [19].In recent years, there has been a significant advancement in LLMs with the emergence of models capable of handling even larger-scale datasets.This expansion in model size has not only led to improved performance but also opened up new possibilities for applying LLMs as Artificial General Intelligence.Among these advanced LLMs, models like ChatGPT 1 and LLaMA 2 boast billions of parameters.Such models hold tremendous potential for tackling complex practical tasks in domains like code generation and artistic creation.With their expanded capacity and enhanced capabilities, LLMs have become game-changers in NLP and AI, and are driving advancements in other fields like coding and software testing.</p>
<p>LLMs have been used for various coding-related tasks including code generation and code recommendation [20]- [23].On one hand, in software testing, there are many tasks related to code generation, such as unit test generation [7], where the utilization of LLMs is expected to yield good performance.On the other hand, software testing possesses unique characteristics that differentiate it from code generation.For example, code generation primarily focuses on producing a single, correct code snippet, whereas software testing often requires generating diverse test inputs to ensure better coverage of the software under test [1].The existence of these differences introduces new challenges and opportunities when employing LLMs for software testing.Moreover, people have benefited from the excellent performance of LLMs in generation and inference tasks, leading to the emergence of dozens of new practices that use LLMs for software testing.</p>
<p>This article presents a comprehensive review of the utilization of LLMs in software testing.We collect 102 relevant papers and conduct a thorough analysis from both software testing and LLMs perspectives, as roughly summarized in Figure 1.</p>
<p>From the viewpoint of software testing, our analysis involves an examination of the specific software testing tasks for which LLMs are employed.Results show that LLMs are commonly used for test case preparation (including unit test case generation, test oracle generation, and system test input generation), program debugging, and bug repair, while we do not find the practices for applying LLMs in the tasks of early testing life-cycle (such as test requirement, test plan, etc).For each test task, we would provide detailed illustrations showcasing the utilization of LLMs in addressing the task, highlighting commonly-used practices, tracking technology evolution trends, and summarizing achieved performance, so as to facilitate readers in gaining a thorough overview of how LLMs are employed across various testing tasks.</p>
<p>From the viewpoint of LLMs, our analysis includes the commonly used LLMs in these studies, the types of prompt engineering, the input of the LLMs, as well as the accompanied techniques with these LLMs.Results show that about one-third of the studies utilize the LLMs through pre-training or fine-tuning schema, while the others employ prompt engineering to communicate with LLMs to steer their behavior for desired outcomes.For prompt engineering, the zero-shot learning and few-shot learning strategies are most commonly used, while other advances like chain-of-thought promoting and self-consistency are rarely utilized.Results also show that traditional testing techniques like differential testing and mutation testing are usually accompanied by LLMs to help generate more diversified tests.</p>
<p>Furthermore, we summarize the key challenges and potential opportunities in this direction.Although software testing with LLMs has undergone significant growth in the This paper makes the following contributions:</p>
<p>• We thoroughly analyze 102 relevant studies that used LLMs for software testing, regarding publication trends, distribution of publication venues, etc.</p>
<p>• We conduct a comprehensive analysis from the perspective of software testing to understand the distribution of software testing tasks with LLM and present a thorough discussion about how these tasks are solved with LLM.</p>
<p>• We conduct a comprehensive analysis from the perspective of LLMs, and uncover the commonly-used LLMs, the types of prompt engineering, input of the LLMs, as well as the accompanied techniques with these LLMs.</p>
<p>• We highlight the challenges in existing studies and present potential opportunities for further studies.We believe that this work will be valuable to both researchers and practitioners in the field of software engineering, as it provides a comprehensive overview of the current state and future vision of using LLMs for software testing.For researchers, this work can serve as a roadmap for future research in this area, highlighting potential avenues for exploration and identifying gaps in our current understanding of the use of LLMs in software testing.For practitioners, this work can provide insights into the potential benefits and limitations of using LLMs for software testing, as well as practical guidance on how to effectively integrate them into existing testing processes.By providing a detailed landscape of the current state and future vision of using LLMs for software testing, this work can help accelerate the adoption of this technology in the software engineering community and ultimately contribute to improving the quality and reliability of software systems.</p>
<p>BACKGROUND</p>
<p>Large Language Model (LLM)</p>
<p>Recently, pre-trained language models (PLMs) have been proposed by pretraining Transformer-based models over large-scale corpora, showing strong capabilities in solving various natural language processing (NLP) tasks [16]- [19].</p>
<p>Studies have shown that model scaling can lead to improved model capacity, prompting researchers to investigate the scaling effect through further parameter size increases.Interestingly, when the parameter scale exceeds a certain threshold, these larger language models demonstrate not only significant performance improvements but also special abilities such as in-context learning, which are absent in smaller models such as BERT.</p>
<p>To discriminate the language models in different parameter scales, the research community has coined the term large language models (LLM) for the PLMs of significant size.LLMs typically refer to language models that have hundreds of billions (or more) of parameters and are trained on massive text data such as GPT-3, PaLM, Codex, and LLaMA.LLMs are built using the Transformer architecture, which stacks multi-head attention layers in a very deep neural network.Existing LLMs adopt similar model architectures (Transformer) and pre-training objectives (language modeling) as small language models, but largely scale up the model size, pre-training data, and total compute power.This enables LLMs to better understand natural language and generate high-quality text based on given context or prompts.</p>
<p>Note that, in existing literature, there is no formal consensus on the minimum parameter scale for LLMs, since the model capacity is also related to data size and total compute.In a recent survey of LLMs [17], the authors focus on discussing the language models with a model size larger than 10B.Under their criteria, the first LLM is T5 released by Google in 2019, followed by GPT-3 released by OpenAI in 2020, and there are more than thirty LLMs released between 2021 and 2023 indicating its popularity.In another survey of unifying LLMs and knowledge graphs [24], the authors categorize the LLMs into three types: encoder-only (e.g., BERT), encoder-decoder (e.g., T5), and decoder-only network architecture (e.g., GPT-3).In our review, we take into account the categorization criteria of the two surveys and only consider the encoder-decoder and decoder-only network architecture of pre-training language models, since they can both support generative tasks.We do not consider the encoder-only network architecture because they cannot handle generative tasks, were proposed relatively early (e.g., BERT in 2018), and there are almost no models using this architecture after 2021.In other words, the LLMs discussed in this paper not only include models with parameters of over 10B (as mentioned in [17]) but also include other models that use the encoder-decoder and decoder-only network architecture (as mentioned in [24]), such as BART with 140M parameters and GPT-2 with parameter sizes ranging from 117M to 1.5B.This is also to potentially include more studies to demonstrate the landscape of this topic.</p>
<p>Software Testing</p>
<p>Software testing is a crucial process in software development that involves evaluating the quality of a software product.The primary goal of software testing is to identify defects or errors in the software system that could potentially lead to incorrect or unexpected behavior.The whole life cycle of software testing typically includes the following tasks (demonstrated in Figure 4):</p>
<p>• Requirement Analysis: analyze the software requirements and identify the testing objectives, scope, and criteria.</p>
<p>• Test Plan: develop a test plan that outlines the testing strategy, test objectives, and schedule.</p>
<p>• Test Design and Review: develop and review the test cases and test suites that align with the test plan and the requirements of the software application.</p>
<p>• Test Case Preparation: the actual test cases are prepared based on the designs created in the previous stage.</p>
<p>• Test Execution: execute the tests that were designed in the previous stage.The software system is executed with the test cases and the results are recorded.</p>
<p>• Test Reporting: analyze the results of the tests and generate reports that summarize the testing process and identify any defects or issues that were discovered.</p>
<p>• Bug Fixing and Regression Testing: defects or issues identified during testing are reported to the development team for fixing.Once the defects are fixed, regression testing is performed to ensure that the changes have not introduced new defects or issues.</p>
<p>• Software Release: once the software system has passed all of the testing stages and the defects have been fixed, the software can be released to the customer or end user.</p>
<p>The testing process is iterative and may involve multiple cycles of the above stages, depending on the complexity of the software system and the testing requirements.</p>
<p>During the testing phase, various types of tests may be performed, including unit tests, integration tests, system tests, and acceptance tests.</p>
<p>• Unit Testing involves testing individual units or components of the software application to ensure that they function correctly.</p>
<p>• Integration Testing involves testing different modules or components of the software application together to ensure that they work correctly as a system.</p>
<p>• System Testing involves testing the entire software system as a whole, including all the integrated components and external dependencies.</p>
<p>• Acceptance Testing involves testing the software application to ensure that it meets the business requirements and is ready for deployment.In addition, there can be functional testing, performance testing, unit testing, security testing, accessibility testing, etc, which explores various aspects of the software under test [25].</p>
<p>Automatic Search</p>
<p>To ensure that we collect papers from diverse research areas, we conduct an extensive search using four popular scientific databases: ACM digital library, IEEE Xplore digital library, arXiv, and DBLP.We search for papers whose title contains keywords related to software testing tasks and testing techniques (as shown below) in the first three databases.In the case of DBLP, we use additional keywords related to LLMs (as shown below) to filter out irrelevant studies, as relying solely on testingrelated keywords would result in a large number of candidate studies.While using two sets of keywords for DBLP may result in overlooking certain related studies, we believe it is still a feasible strategy.This is due to the fact that a substantial number of studies present in this database can already be found in the first three databases, and the fourth database only serves as a supplementary source for collecting additional papers.The above search strategy based on the paper title can recall a large number of papers, and we further conduct the automatic filtering based on the paper content.Specifically, we filter the paper whose content contains "LLM" or "language model" or "generative model" or "large model" or the name of the LLMs (using the LLMs in [17], [24] except those in our exclusion criteria).This can help eliminate the papers that do not involve the neural models.</p>
<p>Manual Search</p>
<p>To compensate for the potential omissions that may result from automated searches, we also conduct manual searches.In order to make sure we collect highly relevant papers, we conduct a manual search within the conference proceedings and journal articles from top-tier software engineering venues (listed in Table 2).</p>
<p>In addition, given the interdisciplinary nature of this work, we also include the conference proceedings of the artificial intelligence field.We select the top ten venues based on the h5 index from Google Scholar, and exclude three computer vision venues, i.e., CVPR, ICCV, ECCV, as listed in Table 2.</p>
<p>Inclusion and Exclusion Criteria</p>
<p>The search conducted on the databases and venue is, by design, very inclusive.This allows us to collect as many papers as possible in our pool.However, this generous inclusivity results in having papers that are not directly related to the scope of this survey.Accordingly, we define a set of specific inclusion and exclusion criteria and then we apply them to each paper in the pool and remove papers not meeting the criteria.This ensures that each collected paper aligns with our scope and research questions.</p>
<p>Inclusion Criteria.We define the following criteria for including papers:</p>
<p>• The paper proposes or improves an approach, study, or tool/framework that targets testing specific software or systems with LLMs.</p>
<p>• The paper applies LLMs to software testing practice, including all tasks within the software testing lifecycle as demonstrated in Section 2.2.</p>
<p>• The paper presents an empirical or experimental study about utilizing LLMs in software testing practice.</p>
<p>• The paper involves specific testing techniques (e.g., fuzz testing) employing LLMs.If a paper satisfies any of the following criteria, we will include it.</p>
<p>Exclusion Criteria.The following studies would be excluded during study selection:</p>
<p>• The paper does not involve software testing tasks, e.g., code comment generation.</p>
<p>• The paper does not utilize LLMs, e.g., using recurrent neural networks.</p>
<p>• The paper mentions LLMs only in future work or discussions rather than using LLMs in the approach.</p>
<p>• The paper utilizes language models with encoder-only architecture, e.g., BERT, which can not directly be utilized for generation tasks (as demonstrated in Section 2.1).</p>
<p>• The paper focuses on testing the performance of LLMs, such as fairness, stability, security, etc. [125]- [127].</p>
<p>• The paper focuses on evaluating the performance of LLM-enabled tools, e.g., evaluating the code quality of the code generation tool Copilot [128]- [130].For the papers collected through automatic search and manual search, we conduct a manual inspection to check whether they satisfy our inclusion criteria and filter those following our exclusion criteria.Specifically, the first two authors read each paper to carefully determine whether it  should be included based on the inclusion criteria and exclusion criteria, and any paper with different decisions will be handed over to the third author to make the final decision.</p>
<p>Quality Assessment</p>
<p>In addition, we establish quality assessment criteria to exclude low-quality studies as shown below.For each question, the study's quality is rated as "yes", "partial" or "no" which are assigned values of 1, 0.5, and 0, respectively.Papers with a score of less than eight will be excluded from our study.</p>
<p>• Is there a clearly stated research goal related to software testing?</p>
<p>• Is there a defined and repeatable technique?</p>
<p>• Is there any explicit contribution to software testing?</p>
<p>• Is there an explicit description of which LLMs are utilized?</p>
<p>• Is there an explicit explanation about how the LLMs are utilized?</p>
<p>• Is there a clear methodology for validating the technique?</p>
<p>• Are the subject projects selected for validation suitable for the research goals?</p>
<p>• Are there control techniques or baselines to demonstrate the effectiveness of the proposed technique?</p>
<p>• Are the evaluation metrics relevant (e.g., evaluate the effectiveness of the proposed technique) to the research objectives?</p>
<p>• Do the results presented in the study align with the research objectives and are they presented in a clear and relevant manner?</p>
<p>Snowballing</p>
<p>At the end of searching database repositories and conference proceedings and journals, and applying inclusion/exclusion criteria and quality assessment, we obtain the initial set of papers.Next, to mitigate the risk of omitting relevant literature from this survey, we also perform backward</p>
<p>3XEOLFDWLRQ&lt;HDU 3XEOLFDWLRQV</p>
<p>Fig. 3: Trend in the number of papers with year snowballing [131] by inspecting the references cited by the collected papers so far.Note that, this procedure did not include new studies, which might because the surveyed topic is quite new and the reference studies tend to published previously, and we already include a relatively comprehensive automatic and manual search.</p>
<p>Collection Results</p>
<p>As shown in Figure 2, the collection process started with a total of 14,623 papers retrieved from four academic databases employing keyword searching.Then after automated filtering, manual search, applying inclusion/exclusion criteria, and quality assessment, we finally collected a total of 102 papers involving software testing with LLMs.Table 1 shows the details of the collected papers.Besides, we also use Table 5 (at the end of the paper) to provide a more comprehensive overview of these papers regarding the specific characteristics which will be illustrated in Section 4 and Section 5. Note that, there are two studies which are respectively the extension of a previously published paper by the same authors ( [46] and [132], [68] and [133]), and we only keep the extended version to avoid duplicate.</p>
<p>General Overview of Collected Paper</p>
<p>Among the papers, 47% papers are published in software engineering venues, among which 19 papers are from ICSE, 5 papers are from FSE, 5 papers are from ASE, and 3 papers are from ISSTA.2% papers are published in artificial intelligence venues such as EMNLP and ICLR, and 5% papers are published in program analysis or security venues like PLDI and S&amp;P.Besides, 46% of the papers have not yet been published via peer-reviewed venues, i.e., they are disclosed on arXiv.This is understandable because this field is emerging and many works are just completed and in the process of submission.Although these papers did not undergo peer review, we have a quality assessment process that eliminates papers with low quality, which potentially ensures the quality of this survey.</p>
<p>Figure 3 demonstrates the trend of our collected papers per year.We can see that as the years go by, the number of papers in this field is growing almost exponentially.In and 2021, there were only 1 and 2 papers, respectively.In 2022, there were 19 papers, and in 2023, there have been Fig. 4: Distribution of testing tasks with LLMs (aligned with software testing life cycle [134]- [136], the number in bracket indicates the number of collected studies per task, and one paper might involve multiple tasks) papers.It is conceivable that there will be even more papers in the future, which indicates the popularity and attention that this field is receiving.</p>
<p>ANALYSIS FROM SOFTWARE TESTING PER-SPECTIVE</p>
<p>This section presents our analysis from the viewpoint of software testing and organizes the collected studies in terms of testing tasks.Figure 4 lists the distribution of each involved testing task, aligned with the software testing life cycle.We first provide a general overview of the distribution, followed by further analysis for each task.Note that, for each following subsection, the cumulative total of subcategories may not always match the total number of papers since a paper might belong to more than one subcategory.</p>
<p>We can see that LLMs have been effectively used in both the mid to late stages of the software testing lifecycle.In the test case preparation phase, LLMs have been utilized for tasks such as generating unit test cases, test oracle generation, and system test input generation.These tasks are crucial in the mid-phase of software testing to help catch issues and prevent further development until issues are resolved.Furthermore, in later phases such as the test report/bug reports and bug fix phase, LLMs have been employed for tasks such as bug analysis, debugging, and repair.These tasks are critical towards the end of the testing phase when software bugs need to be resolved to prepare for the product's release.</p>
<p>Unit Test Case Generation</p>
<p>Unit test case generation involves writing unit test cases to check individual units/components of the software independently and ensure that they work correctly.For a method under test (i.e., often called the focal method), its corresponding unit test consists of a test prefix and a test oracle.In particular, the test prefix is typically a series of method invocation statements or assignment statements, which aims at driving the focal method to a testable state; and then the test oracle serves as the specification to check whether the current behavior of the focal method satisfies the expected one, e.g., the test assertion.</p>
<p>To alleviate manual efforts in writing unit tests, researchers have proposed various techniques to facilitate automated unit test generation.Traditional unit test generation techniques leverage search-based [3], [4], constraint-based [5] or random-based strategies [6] to generate a suite of unit tests with the main goal of maximizing the coverage in the software under test.</p>
<p>Nevertheless, the coverage and the meaningfulness of the generated tests are still far from satisfactory.</p>
<p>Since LLMs have demonstrated promising results in tasks such as code generation, and given that both code generation and unit test case generation involve generating source code, recent research has extended the domain of code generation to encompass unit test case generation.Despite initial success, there are nuances that set unit test case generation apart from general code generation, signaling the need for more tailored approaches.</p>
<p>Pre-training or fine-tuning LLMs for unit test case generation.</p>
<p>Due to the limitations of LLMs in their earlier stages, a majority of the earlier published studies adopt this pre-training or fine-tuning schema.Moreover, in some recent studies, this schema continues to be employed to increase the LLMs' familiarity with domain knowledge.Alagarsamy et al. [29] first pre-trained the LLM with the focal method and asserted statements to enable the LLM to have a stronger foundation knowledge of assertions, then fine-tuned the LLM for the test case generation task where the objective is to learn the relationship between the focal method and the corresponding test case.Tufano et al. [26] utilized a similar schema by pre-training the LLM on a large unsupervised Java corpus, and supervised fine-tuning a downstream translation task for generating unit tests.Hashtroudi et al. [32] leveraged the existing developerwritten tests for each project to generate a project-specific dataset for domain adaptation when fine-tuning the LLM, which can facilitate generating human-readable unit tests.Rao et al. [35] trained a GPT-style language model by utilizing a pre-training signal that explicitly considers the mapping between code and test files.Steenhoek et al. [42] utilizes reinforcement learning to optimize models by providing rewards based on static quality metrics that can be automatically computed for the generated unit test cases.</p>
<p>Designing effective prompts for unit test case generation.The advancement of LLMs has allowed them to excel at targeted tasks without pre-training or fine-tuning.Therefore most later studies typically focus on how to design the prompt, to make the LLM better at understanding the context and nuances of this task.Xie et al. [36] generated unit test cases by parsing the project, extracting essential information, and creating an adaptive focal context that includes a focal method and its dependencies within the predefined maximum prompt token limit of the LLM, and incorporating these context into a prompt to query the LLM.Dakhel et al. [38] introduced MuTAP for improving the effectiveness of test cases generated by LLMs in terms of revealing bugs by leveraging mutation testing.They augment prompts with surviving mutants, as those mutants highlight the limitations of test cases in detecting bugs.Zhang et al. [40] generated security tests with vulnerable dependencies with LLMs.</p>
<p>Yuan et al. [7] first performed an empirical study to evaluate ChatGPT's capability of unit test generation with both a quantitative analysis and a user study in terms of correctness, sufficiency, readability, and usability.And results show that the generated tests still suffer from correctness issues, including diverse compilation errors and execution failures.They further propose an approach that leveraged the ChatGPT itself to improve the quality of its generated tests with an initial test generator and an iterative test refiner.Specifically, the iterative test refiner iteratively fixed the compilation errors in the tests generated by the initial test generator, which follows a validate-and-fix paradigm to prompt the LLM based on the compilation error messages and additional code context.Guilherme et al. [31] and Li et al. [41] respectively evaluated the quality of the generated unit tests by LLM using different metrics and different prompts.</p>
<p>Test generation with additional documentation.</p>
<p>Vikram et al. [34] went a step further by investigating the potential of using LLMs to generate property-based tests when provided API documentation.They believe that the documentation of an API method can assist the LLM in producing logic to generate random inputs for that method and deriving meaningful properties of the result to check.Instead of generating unit tests from the source code, Plein et al. [33] generated the tests based on user-written bug reports.</p>
<p>LLM and search-based method for unit test generation.</p>
<p>The aforementioned studies utilize LLMs for the whole unit test case generation task, while Lemieux et al. [37] focus on a different direction, i.e., first letting the traditional searchbased software testing techniques (e.g., Pynguin [137]) in generating unit test case until its coverage improvements stall, then asking the LLM to provide the example test cases for under-covered functions.These examples can help the original test generation redirect its search to more useful areas of the search space.</p>
<p>Tang et al. [8] conducts a systematic comparison of test suites generated by the LLM and the state-of-the-art searchbased software testing tool EvoSuite, by considering the correctness, readability, code coverage, and bug detection capability.Similarly, Bhatia [43] experimentally investigates the quality of unit tests generated by LLM compared to a commonly-used test generator Pynguin.</p>
<p>Performance of unit test case generation.Since the aforementioned studies of unit test case generation are based on different datasets, one can hardly derive a fair comparison and we present the details in Table 3 to let the readers obtain a general view.We can see that in the SF110 benchmark, all three evaluated LLMs have quite low performance, i.e., 2% coverage [39].SF110 is an Evosuite (a search-based unit test case generation technique) benchmark consisting of 111 open-source Java projects retrieved from SourceForge, containing 23,886 classes, over 800,000 bytecode-level branches, and 6.6 million lines of code.The authors did not present detailed reasons for the low performance which can be further explored in the future.</p>
<p>Test Oracle Generation</p>
<p>A test oracle is a source of information about whether the output of a software system (or program or function or method) is correct or not [138].Most of the collected studies in this category target the test assertion generation, which is inside a unit test case.Nevertheless, we opted to treat these studies as separate sections to facilitate a more thorough analysis.</p>
<p>Test assertion, which is to indicate the potential issues in the tested code, is an important aspect that can distinguish the unit test cases from the regular code.This is why some studies specifically focus on the generation of effective test assertions.Actually, before using LLMs, researchers have proposed RNN-based approaches that aim at learning from thousands of unit test methods to generate meaningful assert statements [139], yet only 17% of the generated asserts can exactly match with the ground truth asserts.Subsequently, to improve the performance, several researchers utilized the LLMs for this task.</p>
<p>Mastropaolo et al. [46], [132] pre-trained a T5 model on a dataset composed of natural language English text and source code.Then, it fine-tuned such a model by reusing datasets used in four previous works that used deep learning techniques (such as RNN as mentioned before) including test assertion generation and program repair, etc. Results showed that the extract match rate of the generated test assertion is 57%.Tufano et al. [44] proposed a similar approach which separately pre-trained the LLM with English corpus and code corpus, and then fine-tuned it on the asserts dataset (with test methods, focal methods, and asserts).This further improved the performance to 62% of the exact match rate.Besides the syntax-level data as previous studies, Nie et al. [45] fine-tuned the LLMs with six kinds of code semantics data, including the execution result (e.g., types of the local variables) and execution context (e.g., the last called method in the test method), which enabled LLMs to learn to understand the code execution information.The exact match rate is 17% (note that this paper is based on a different dataset from all other studies mentioned under this topic).</p>
<p>The aforementioned studies utilized the pre-training and fine-tuning schema when using LLMs, and with the increasingly powerful capabilities of LLMs, they can perform well on specific tasks without these specialized pre-training or fine-tuning datasets.Subsequently, Nashid et al. [47] utilized prompt engineering for this task, and proposed a technique for prompt creation that automatically retrieves code demonstrations similar to the task, based on embedding or frequency analysis.They also present evaluations about the few-shot learning with various numbers (e.g., zero-shot, one-shot, or n-shot) and forms (e.g., random vs. systematic, or with vs. without natural language descriptions) of the prompts, to investigate its feasibility on test assertion generation.With only a few relevant code demonstrations, this approach can achieve an accuracy of 76% for exact matches in test assertion generation, which is the state-of-the-art performance for this task.</p>
<p>System Test Input Generation</p>
<p>This category encompasses the studies related to creating test input of system testing for enabling the automation of test execution.We employ three subsections to present the analysis from three different orthogonal viewpoints, and each of the collected studies may be analyzed in one or more of these subsections.</p>
<p>The first subsection is input generation in terms of software types.The generation of system-level test inputs for software testing varies for specific types of software being tested.For example, for mobile applications, the test input generation requires providing a diverse range of text inputs or operation combinations (e.g., click a button, long press a list) [14], [49], which is the key to testing the application's functionality and user interface; while for Deep Learning (DL) libraries, the test input is a program which covers diversified DL APIs [58], [59].This subsection will demonstrate how the LLMs are utilized to generate inputs for different types of software.</p>
<p>The second subsection input generation in terms of testing techniques.We have observed that certain approaches serve as specific types of testing techniques.For example, dozens of our collected studies specifically focus on using LLMs for fuzz testing.Therefore, this subsection would provide an analysis of the collected studies in terms of testing techniques, showcasing how the LLMs are employed to enhance traditional testing techniques.</p>
<p>The third subsection input generation in terms of input and output.While most of the collected studies take the source code or the software itself as the input and directly output the software's test input, there are studies that utilize alternative forms of input and output.This subsection would provide an analysis of such studies, highlighting different approaches and their input-output characteristics.Test input generation for mobile apps.For mobile app testing, one difficulty is to generate the appropriate text inputs to proceed to the next page, which remains a prominent obstacle for testing coverage.Considering the diversity and semantic requirement of valid inputs (e.g., flight departure, movie name), traditional techniques with heuristic-based or constraint-based techniques [10], [140] are far from generating meaningful text input.Liu et al. [49] employ the LLM to intelligently generate the semantic input text according to the GUI context.In detail, their proposed QTypist automatically extracts the component information related to the EditText for generating the prompts, and then inputs the prompts into the LLM to generate the input text.</p>
<p>Input Generation in</p>
<p>Besides the text input, there are other forms of input for mobile apps, i.e., operations like 'click a button' and 'select a list'.To fully test an app, it is required to cover more GUI pages and conduct more meaningful exploration traces through the GUI operations, yet existing studies with random-/rule-based methods [9], [10], model-based methods [11], [12], and learning-based methods [13] are unable to understand the semantic information of the GUI page thus could not conduct the trace planning effectively.Liu et al. [14] formulates the test input generation of mobile GUI testing problem as a Q&amp;A task, which asks LLM to chat with the mobile apps by passing the GUI page information to LLM to elicit testing scripts (i.e., GUI operation), and executing them to keep passing the app feedback to LLM, iterating the whole process.The proposed GPTDroid extracts the static context of the GUI page and the dynamic context of the iterative testing process, and designs prompts for inputting this information to LLM which enables the LLM to better understand the GUI page as well as the whole testing process.It also introduces a functionality-aware memory prompting mechanism that equips the LLM with the ability to retain testing knowledge of the whole process and conduct long-term, functionality-based reasoning to guide exploration.Similarly, Zimmermann et al. utilize the LLM to interpret natural language test cases and programmatically navigate through the application under test [54].</p>
<p>Yu et al. [61] investigate the LLM's capabilities in the mobile app test script generation and migration task, including the scenario-based test generation, and the crossplatform/app test migration.</p>
<p>Test input generation for DL libraries.The input for testing DL libraries is DL programs, and the difficulty in generating the diversified input DL programs is that they need to satisfy both the input language (e.g., Python) syntax/semantics and the API input/shape constraints for tensor computations.Traditional techniques with API-level fuzzing [141], [142] or model-level fuzzing [143], [144] suffer from the following limitations: 1) lack of diverse API sequence thus cannot reveal bugs caused by chained API sequences; 2) cannot generate arbitrary code thus cannot explore the huge search space that exists when using the DL libraries.Since LLMs can include numerous code snippets invoking DL library APIs in their training corpora, they can implicitly learn both language syntax/semantics and intricate API constraints for valid DL program generation.Taken in this sense, Deng et al. [59] used both generative and infilling LLMs to generate and mutate valid/diverse input DL programs for fuzzing DL libraries.In detail, it first uses a generative LLM (CodeX) to generate a set of seed programs (i.e., code snippets that use the target DL APIs).Then it replaces part of the seed program with masked tokens using different mutation operators and leverages the ability of infilling LLM (InCoder) to perform code infilling to generate new code that replaces the masked tokens.Their follow-up study [58] goes a step further to prime LLMs to synthesize unusual programs for the fuzzing DL libraries.It is built on the well-known hypothesis that historical bug-triggering programs may include rare/valuable code ingredients important for bug finding and show improved bug detection performance.</p>
<p>Test input generation for other types of software.There are also dozens of studies that address testing tasks in various other domains, due to space limitations, we will present a selection of representative studies in these domains.</p>
<p>Finding bugs in a commercial cyber-physical system (CPS) development tool such as Simulink is even more challenging.Given the complexity of the Simulink language, generating valid Simulink model files for testing is an ambitious task for traditional machine learning or deep learning techniques.Shrestha et al. [51] employs a small set of Simulink-specific training data to fine-tune the LLM for generating Simulink models.Results show that it can create Simulink models quite similar to the open-source models, and can find a super-set of the bugs traditional fuzzing approaches found.</p>
<p>Sun et al. [63] utilize LLM to generate test formulas for fuzzing SMT solvers.It retrains the LLMs on a large corpus of SMT formulas to enable them to acquire SMT-specific domain knowledge.Then it further fine-tunes the LLMs on historical bug-triggering formulas, which are known to involve structures that are more likely to trigger bugs and solver-specific behaviors.The LLM-based compiler fuzzer proposed by Yang et al. [69] adopts a dual-model framework: (1) an analysis LLM examines the low-level optimization source code and produces requirements on the high-level test programs that can trigger the optimization;</p>
<p>(2) a generation LLM produces test programs based on the summarized requirements.Ye et al. [48] utilize the LLM for generating the JavaScript programs and then use the well-structured ECMAScript specifications to automatically generate test data along with the test programs, after that they apply differential testing to expose bugs.</p>
<p>Input Generation in Terms of Testing Techniques</p>
<p>By utilizing system test inputs generated by LLMs, the collected studies aim to enhance traditional testing techniques and make them more effective.Among these techniques, fuzz testing is the most commonly involved one.Fuzz testing, as a general concept, revolves around generating invalid, unexpected, or random data as inputs to evaluate the behavior of software.LLMs play a crucial role in improving traditional fuzz testing by facilitating the generation of diverse and realistic input data.This enables fuzz testing to uncover potential bugs in the software by subjecting it to a wide range of input scenarios.In addition to fuzz testing, LLMs also contribute to enhancing other testing techniques, which will be discussed in detail later.</p>
<p>Universal fuzzing framework.Xia et al. [67] present Fuzz4All that can target many different input languages and many different features of these languages.The key idea behind it is to leverage LLMs as an input generation and mutation engine, which enables the approach to produce diverse and realistic inputs for any practically relevant language.To realize this potential, they present a novel auto-prompting technique, which creates LLM prompts that are well-suited for fuzzing, and a novel LLM-powered fuzzing loop, which iteratively updates the prompt to create new fuzzing inputs.They experiment with six different languages (C, C++, Go, SMT2, Java and Python) as inputs and demonstrate higher coverage than existing language-specific fuzzers.Hu et al. [52] propose a greybox fuzzer augmented by the LLM, which picks a seed in the fuzzer's seed pool and prompts the LLM to produce the mutated seeds that might trigger a new code region of the software.They experiment with three categories of input formats, i.e., formatted data files (e.g., json, xml), source code in different programming languages (e.g., JS, SQL, C), text with no explicit syntax rules (e.g., HTTP response, md5 checksum).In addition, effective fuzzing relies on the effective fuzz driver, and Zhang et al. [66] utilize LLMs on the fuzz driver generation, in which five query strategies are designed and analyzed from basic to enhanced.</p>
<p>Fuzzing techniques for specific software.There are studies that focus on the fuzzing techniques tailored to specific software, e.g., the deep learning library [58], [59], compiler [69], SMT solvers [63], input widget of mobile app [65], cyber-physical system [51], etc.One key focus of these fuzzing techniques is to generate diverse test inputs so as to achieve higher coverage.This is commonly achieved by combining the mutation technique with LLM-based generation, where the former produces various candidates while the latter is responsible for generating the executable test inputs [59], [63].Another focus of these fuzzing techniques is to generate the risky test inputs that can trigger bugs earlier.To achieve this, a common practice is to collect the historical bug-triggering programs to fine-tune the LLM [63] or treat them as the demonstrations when querying the LLM [58], [65].</p>
<p>Other testing techniques.There are studies that utilize LLMs for enhancing GUI testing for generating meaningful text input [49] and functionality-oriented exploration traces [14], which has been introduced in Test input generation for mobile apps part of Section 4.3.1.</p>
<p>Besides, Deng et al. [62] leverage the LLMs to carry out penetration testing tasks automatically.It involves setting a penetration testing goal for the LLM, soliciting it for the appropriate operation to execute, implementing it in the testing environment, and feeding the test outputs back to the LLM for next-step reasoning.</p>
<p>Input Generation in Terms of Input and Output</p>
<p>Other output format of test generation.Although most works use LLM to generate test cases directly, there are also some works generating indirect inputs like testing code, test scenarios, metamorphic relations, etc. Liu et al. [65] propose InputBlaster which leverages the LLM to automatically generate unusual text inputs for fuzzing the text input widgets in mobile apps.It formulates the unusual inputs generation problem as a task of producing a set of test generators, each of which can yield a batch of unusual text inputs under the same mutation rule.In detail, InputBlaster leverages LLM to produce the test generators together with the mutation rules serving as the reasoning chain and utilizes the in-context learning schema to demonstrate the LLM with examples for boosting the performance.Deng et al. [64] use LLM to extract key information related to the test scenario from a traffic rule, and represent the extracted information in a test scenario schema, then synthesize the corresponding scenario scripts to construct the test scenario.Luu et al. [56] examine the effectiveness of LLM in generating metamorphic relations (MRs) for metamorphic testing.Their results show that ChatGPT can be used to advance software testing intelligence by proposing MRs candidates that can be later adapted for implementing tests, but human intelligence should still inevitably be involved to justify and rectify their correctness.</p>
<p>Other input format of test generation.The aforementioned studies primarily take the source code or the software as the input of LLM, yet there are also studies that take natural language description as the input for test generation.Mathur et al. [53] propose to generate test cases from the natural language described requirements.Ackerman et al. [60] generate the instances from natural language described requirements recursively to serve as the seed examples for a mutation fuzzer.</p>
<p>Bug Analysis</p>
<p>This category involves analyzing and categorizing the identified software bugs to enhance understanding of the bug, and facilitate subsequent debug and bug repair.Mukherjee et al. [73] generate relevant answers to follow-up questions for deficient bug reports to facilitate bug triage.Su et al. [76] transform the bug-component triaging into a multiclassification task and a generation task with LLM, then ensemble the prediction results from them to improve the performance of bug-component triaging further.Zhang et al. [72] first leverage the LLM under the zero-shot setting to get essential information on bug reports, then use the essential information as the input to detect duplicate bug reports.Mahbub et al. [74] proposes to explain software bugs with LLM, which generates natural language explanations for software bugs by learning from a large corpus of bug-fix commits.Zhang et al. [70] target to automatically generate the bug title from the descriptions of the bug, which aims to help developers write issue titles and facilitate the bug triaging and follow-up fixing process.</p>
<p>Debug</p>
<p>This category refers to the process of identifying and locating the cause of a software problem (i.e., bug).It involves analyzing the code, tracing the execution flow, collecting error information to understand the root cause of the issue, and fixing the issue.Some studies concentrate on the comprehensive debug process, while others delve into specific sub-activities within the process.</p>
<p>Overall debug framework.Bui et al. [77] proposes a unified Detect-Localize-Repair framework based on the LLM for debugging, which first determines whether a given code snippet is buggy or not, then identifies the buggy lines, and translates the buggy code to its fixed version.Kang et al. [83] proposes automated scientific debugging, a technique that given buggy code and a bug-revealing test, prompts LLMs to automatically generate hypotheses, uses debuggers to actively interact with buggy code, and thus automatically reaches conclusions prior to patch generation.Chen et al. [88] demonstrate that self-debugging can teach the LLM to perform rubber duck debugging; i.e., without any human feedback on the code correctness or error messages, the model is able to identify its mistakes by investigating the execution results and explaining the generated code in natural language.Cao et al. [89] conducts a study of LLM's debugging ability for deep learning programs, including fault detection, fault localization and program repair.</p>
<p>Bug localization.Wu et al. [85] compare the two LLMs (ChatGPT and GPT-4) with the existing fault localization techniques, and investigate the consistency of LLMs in fault localization, as well as how prompt engineering and the length of code context affect the results.Kang et al. [79] propose AutoFL, an automated fault localization technique that only requires a single failing test, and during its fault localization process, it also generates an explanation about why the given test fails.Yang et al. [84] propose LLMAO to overcome the left-to-right nature of LLMs by fine-tuning a small set of bidirectional adapter layers on top of the representations learned by LLMs, which can locate buggy lines of code without any test coverage information.Tu et al. [86] propose LLM4CBI to tame LLMs to generate effective test programs for finding suspicious files.</p>
<p>Bug reproduction.There are also studies focusing on a sub-phase of the debugging process.For example, Kang et al. [78] and Plein et al. [81] respectively propose the framework to harness the LLM to reproduce bugs, and suggest bug reproducing test cases to the developer for facilitating debugging.Li et al. [87] focus on a similar aspect of finding the failure-inducing test cases whose test input can trigger the software's fault.It synergistically combines LLM and differential testing to do that.</p>
<p>There are also studies focusing on the bug reproduction of mobile apps to produce the replay script.Feng et al. [75] propose AdbGPT, a new lightweight approach to automatically reproduce the bugs from bug reports through prompt engineering, without any training and hard-coding effort.It leverages few-shot learning and chain-of-thought reasoning to elicit human knowledge and logical reasoning from LLMs to accomplish the bug replay in a manner similar to a developer.Huang et al. [71] propose CrashTranslator to automatically reproduce bugs directly from the stack trace.It accomplishes this by leveraging the LLM to predict the exploration steps for triggering the crash, and designing a reinforcement learning based technique to mitigate the inaccurate prediction and guide the search holistically.Taeb et al. [55] convert the manual accessibility test instructions into replayable, navigable videos by using LLM and UI element detection models, which can also help reveal accessibility issues.</p>
<p>Error explanation.Taylor et al. [82] integrates the LLM into the Debugging C Compiler to generate unique, novicefocused explanations tailored to each error.Widjojo et al. [80] study the effectiveness of Stack Overflow and LLMs at explaining compiler errors.</p>
<p>Program Repair</p>
<p>This category denotes the task of fixing the identified software bugs.The high frequency of repair-related studies can be attributed to the close relationship between this task and the source code.With their advanced natural language processing and understanding capabilities, LLM are well-equipped to process and analyze source code, making them an ideal tool for performing code-related tasks such as fixing bugs.</p>
<p>There have been template-based [145], heuristic-based [146], and constraint-based [147], [148] automatic program repair techniques.And with the development of deep learning techniques in the past few years, there have been several studies employing deep learning techniques for program repair.They typically adopt deep learning models to take a buggy software program as input and generate a patched program.Based on the training data, they would build a neural network model that learns the relations between the buggy code and the corresponding fixed code.Nevertheless, these techniques still fail to fix a large portion of bugs, and they typically have to generate hundreds to thousands of candidate patches and take hours to validate these patches to fix enough bugs.Furthermore, the deep learning based program repair models need to be trained with huge amounts of labeled training data (typically pairs of buggy and fixed code), which is time-and effortconsuming to collect the high-quality dataset.Subsequently, with the popularity and demonstrated capability of the LLMs, researchers begin to explore the LLMs for program repair.</p>
<p>Patch single-line bugs.In the early era of program repair, the focus was mainly on addressing defects related to single-line code errors, which are relatively simple and did not require the repair of complex program logic.Lajk ó et al. [95] propose to fine-tune the LLM with JavaScript code snippets to serve as the purpose for the JavaScript program repair.Zhang et al. [116] employs program slicing to extract contextual information directly related to the given buggy statement as repair ingredients from the corresponding program dependence graph, which makes the fine-tuning more focused on the buggy code.Zhang et al. [121] propose a stage-wise framework STEAM for patching single-line bugs, which simulates the interactive behavior of multiple programmers involved in bug management, e.g., bug reporting, bug diagnosis, patch generation, and patch verification.</p>
<p>Since most real-world bugs would involve multiple lines of code, and later studies explore these more complex situations (although some of them can also patch the single-line bugs).</p>
<p>Patch multiple-lines bugs.The studies in this category would input a buggy function to the LLM, and the goal is to output the patched function, which might involve complex semantic understanding, code hunk modification, as well as program refactoring.Earlier studies typically employ the fine-tuning strategy to enable the LLM to better understand the code semantics.Fu et al. [123] fine-tune the LLM by employing BPE tokenization to handle Out-Of-Vocabulary (OOV) issues which makes the approach generate new tokens that never appear in a training function but are newly introduced in the repair.Wang et.al. [120] train the LLM based on both buggy input and retrieved bug-fix examples which are retrieved in terms of the lexical and semantical similarities.The aforementioned studies (including the ones in patching single-line bugs) would predict the fixed programs directly, and Hu et al. [92] utilize a different setup that predicts the scripts that can fix the bugs when executed with the delete and insert grammar.For example, it predicts whether an original line of code should be deleted, and what content should be inserted.</p>
<p>Nevertheless, fine-tuning may face limitations in terms of its reliance on abundant high-quality labeled data, significant computational resources, and the possibility of overfitting.To approach the program repair problem more effectively, later studies focus on how to design an effective prompt for program repair.Several studies empirically investigate the effectiveness of prompt variants of the latest LLMs for program repair under different repair settings and commonly-used benchmarks (which will be explored in depth later), while other studies focus on proposing new techniques.Ribeiro et al. [109] take advantage of LLM to conduct the code completion in a buggy line for patch generation, and elaborate on how to circumvent the open-ended nature of code generation to appropriately fit the new code in the original program.Xia et al. [115] propose the conversation-driven program repair approach that interleaves patch generation with instant feedback to perform the repair in a conversational style.They first feed the LLM with relevant test failure information to start with, and then learns from both failures and successes of earlier patching attempts of the same bug for more powerful repair.For earlier patches that failed to pass all tests, they combine the incorrect patches with their corresponding relevant test failure information to construct a new prompt for the LLM to generate the next patch, in order to avoid making the same mistakes.For earlier  [103] propose Repilot to copilot the AI "copilots" (i.e., LLMs) by synthesizing more valid patches during the repair process.Its key insight is that many LLMs produce outputs autoregressively (i.e., token by token), and by resembling human writing programs, the repair can be significantly boosted and guided through a completion engine.Brownlee et al. [105] propose to use the LLM as mutation operators for the search-based techniques of program repair.</p>
<p>Repair with static code analyzer.Most of the program repair studies would suppose the bug has been detected, while Jin et al. [114] propose a program repair framework paired with a static analyzer to first detect the bugs, and then fix them.In detail, the static analyzer first detects an error (e.g., null pointer dereference) and the context information provided by the static analyzer will be sent into the LLM for querying the patch for this specific error.Wadhwa et al. [110] focus on a similar task, and additionally employ an LLM as the ranker to assess the likelihood of acceptance of generated patches which can effectively catch plausible but incorrect fixes and reduce developer burden.</p>
<p>Repair for specific bugs.The aforementioned studies all consider the buggy code as the input for the automatic program repair, while other studies conduct program repairing in terms of other types of bug descriptions, specific types of bugs, etc. Fakhoury et al. [122] focus on program repair from natural language issue descriptions, i.e., generating the patch with the bug and fix-related information described in the issue reports.Garg et al. [119] aim at repairing performance issues, in which they first retrieve a prompt instruction from a pre-constructed knowledge-base of previous performance bug fixes and then generate a repair prompt using the retrieved instruction.There are studies focusing on the bug fixing of Rust programs [108] or OCaml programs (an industrial-strength programming language) [111].</p>
<p>Empirical study about program repair.</p>
<p>There are several studies related to the empirical or experimental evaluation of the various LLMs on program repair, and we summarize the performance in Table 4. Jiang et al. [113], Xia et al. [93], and Zhang et.al. [118] respectively conduct comprehensive experimental evaluations with various LLMs and on different automated program repair benchmarks, while other researchers [89], [96], [98], [100] focus on a specific LLM and on one dataset, e.g., QuixBugs.In addition, Gao et al. [124] empirically investigate the impact of in-context demonstrations for bug fixing, including the selection, order, and number of demonstration examples.Prenner et al. [117] empirically study how the local context (i.e., code that comes before or after the bug location) affects the repair performance.Horváth et al. [99] empirically study the impact of program representation and model architecture on the repair performance.</p>
<p>There are two commonly-used repair settings when using LLMs to generate patches: 1) complete function generation (i.e., generating the entire patch function), 2) correct code infilling (i.e., filling in a chunk of code given the prefix and suffix), and different studies might utilize different settings which are marked in Table 4.The commonlyused datasets are QuixBugs, Defects4J, etc.These datasets only involve the fundamental functionalities such as sorting algorithms, each program's average number of lines ranging from 13 to 22, implementing one functionality, and involving few dependencies.To tackle this, Cao et al. [89] conducts an empirical study on a more complex dataset with DL programs collected from StackOverflow.Every program contains about 46 lines of code on average, implementing several functionalities including data preprocessing, DL model construction, model training, and evaluation.And the dataset involves more than 6 dependencies for each program, including TensorFlow, Keras, and Pytorch.Their results demonstrate a much lower rate of correct patches than in other datasets, which again reveals the potential difficulty of this task.Similarly, Haque et al. [106] introduce a dataset comprising of buggy code submissions and their corresponding fixes collected from online judge platforms, in which it offers an extensive collection of unit tests to enable the evaluations about the correctness of fixes and further information regarding time, memory constraints, and acceptance based on a verdict.</p>
<p>ANALYSIS FROM LLM PERSPECTIVE</p>
<p>This section discusses the analysis based on the viewpoints of LLM, specifically, it's unfolded from the viewpoints of utilized LLMs, types of prompt engineering, input of the LLMs, as well as the accompanied techniques when utilizing LLM.</p>
<p>LLM Models</p>
<p>As shown in Figure 6, the most commonly utilized LLM in software testing tasks is ChatGPT, which was released on Nov. 2022 by OpenAI.It is trained on a large corpus of natural language text data, and primarily designed for natural language processing and conversation.ChatGPT is the most widely recognized and popular LLM up until now, known for its exceptional performance across various tasks.Therefore, it comes as no surprise that it ranks in the top position in terms of our collected studies.</p>
<p>Codex, an LLM based on GPT-3, is the second most commonly used LLM in our collected studies.It is trained on a massive code corpus containing examples from many programming languages such as JavaScript, Python, C/C++, and Java.Codex was released on Sep.2021 by OpenAI and powers GitHub Copilot-an AI pair programmer that generates whole code snippets, given a natural language description as a prompt.Since a large portion of our collected studies involve the source code (e.g., repair, unit test case generation), it is not surprising that researchers choose Codex as the LLM in assisting them in accomplishing the codingrelated tasks.</p>
<p>The third-ranked LLM is CodeT5, which is an opensourced LLM developed by salesforce 3 .Thanks to its open source, researchers can easily conduct the pre-training and fine-tuning with domain-specific data to achieve better performance.Similarly, CodeGen is also open-sourced and ranked relatively higher.Besides, for CodeT5 and CodeGen, there are more than half of the related studies involve the empirical evaluations (which employ multiple LLMs), e.g., program repair [112], [113], unit test case generation [39].</p>
<ol>
<li>https://blog.salesforceairesearch.com/codet5/There are already 14 studies that utilize GPT-4, ranking at the fourth place, which is launched on March 2023.Several studies directly utilize this state-of-the-art LLM of Ope-nAI, since it demonstrates excellent performance across a wide range of generation and reasoning tasks.For example, Xie et al. utilize GPT-4 to generate fuzzing inputs [67], while Vikram et al. employ it to generate property-based tests with the assistance of API documentation [34].In addition, some studies conduct experiments using both GPT-4 and Chat-GPT or other LLMs to provide a more comprehensive evaluation of these models' performance.In their proposed LLMempowered automatic penetration testing technique, Deng et al. find that GPT-4 surpasses ChatGPT and LaMDA from Google [62].Similarly, Zhang et al. find that GPT-4 shows its performance superiority over ChatGPT when generating the fuzz drivers with both the basic query strategies and enhanced query strategies [66].Furthermore, GPT-4, as a multi-modal LLM, sets itself apart from the other mentioned LLMs by showcasing additional capabilities such as generating image narratives and answering questions based on images [149].Yet we have not come across any studies that explore the utilization of GPT-4's image-related features (e.g., UI screenshots, programming screencasts) in software testing tasks.</li>
</ol>
<p>Types of Prompt Engineering</p>
<p>As shown in Figure 7, among our collected studies, 38 studies utilize the LLMs through pre-training or finetuning schema, while 64 studies employ the prompt engineering to communicate with LLMs to steer its behavior for desired outcomes without updating the model weights.When using the early LLMs, their performances might not be as impressive, so researchers often use pre-training or fine-tuning techniques to adjust the models for specific domains and tasks in order to improve their performance.Then with the upgrading of LLM technology, especially with the introduction of GPT-3 and later LLMs, the knowledge contained within the models and their understanding/inference capability has increased significantly.Therefore, researchers will typically rely on prompt engineering to consider how to design appropriate prompts to stimulate the model's knowledge.</p>
<p>Among the 64 studies with prompt engineering, 51 studies involve zero-shot learning, and 25 studies involve fewshot learning (a study may involve multiple types).There are also studies involving the chain-of-though (7 studies), self-consistency (1 study), and automatic prompt (1 study).</p>
<p>Zero-shot learning is to simply feed the task text to the model and ask for results.Many of the collected studies employ the Codex, CodeT5, and CodeGen (as shown in Section 5.1), which is already trained on source code.Hence, for the tasks dealing with source code like unit test case generation and program repair as demonstrated in previous sections, directly querying the LLM with prompts is the common practice.There are generally two types of manners of zeroshot learning, i.e., with and without instructions.For example, Xie et al. [36] would provide the LLMs with the instructions as "please help me generate a JUnit test for a specific Java method ..." to facilitate the unit test case generation.In contrast, Siddiq et al. [39] only provide the code header Fig. 7: Distribution about how LLM is used (Note that, a study can involve multiple types of prompt engineering) of the unit test case (e.g., "class ${className}${suffix}Test {"), and the LLMs would carry out the unit test case generation automatically.Generally speaking, prompts with clear instructions will yield more accurate results, while prompts without instructions are typically suitable for very specific situations.</p>
<p>Few-shot learning presents a set of high-quality demonstrations, each consisting of both input and desired output, on the target task.As the model first sees the examples, it can better understand human intention and criteria for what kinds of answers are wanted, which is especially important for tasks that are not so straightforward or intuitive to the LLM.For example, when conducting the automatic test generation from general bug reports, Kang et al. [78] provide examples of bug reports (questions) and the corresponding bug reproducing tests (answers) to the LLM, and their results show that two examples can achieve the highest performance than no examples or other number of examples.Another example of test assertion generation, Nashid et al. [47] provide demonstrations of the focal method, the test method containing an <AssertPlaceholder>, and the expected assertion, which enables the LLMs to better understand the task.</p>
<p>Chain-of-thought (CoT) prompting generates a sequence of short sentences to describe reasoning logics step by step (also known as reasoning chains or rationales) to the LLMs for generating the final answer.For example, for program repair from the natural language issue descriptions [122], given the buggy code and issue report, the authors first ask the LLM to localize the bug, and then they ask it to explain why the localized lines are buggy, finally, they ask the LLM to fix the bug.Another example is for generating unusual programs for fuzzing deep learning libraries, Deng et al. [58] first generate a possible "bug" (bug description) before generating the actual "bug-triggering" code snippet that invokes the target API.The predicted bug description provides an additional hint to the LLM, indicating that the generated code should try to cover specific potential buggy behavior.</p>
<p>Self-consistency involves evaluating the coherence and consistency of the LLM's responses on the same input in different contexts.There is one study with this prompt type, and it is about debugging.Kang et al. [83] employ a hypothesize-observe-conclude loop, which first generates a hypothesis about what the bug is and constructs an experiment to verify, using an LLM, then decide whether the hypothesis is correct based on the experiment result (with a debugger or code execution) using an LLM, after that, depending on the conclusion, it either starts with a new hypothesis or opts to terminate the debugging process and generate a fix.</p>
<p>Automatic prompt aims to automatically generate and select the appropriate instruction for the LLMs, instead of requiring the user to manually engineer a prompt.Xia et al. [67] introduce an auto-prompting step that automatically distils all user-provided inputs into a concise and effective prompt for fuzzing.Specifically, they first generate a list of candidate prompts by incorporating the user inputs and auto prompting instruction while setting the LLM at high temperature, then a small-scale fuzzing experiment is conducted to evaluate each candidate prompt, and the best one is selected.</p>
<p>Note that there are fourteen studies that apply the iterative prompt design when using zero-shot or few-shot learning, in which the approach continuously refines the prompts with the running information of the testing task, e.g., the test failure information.For example, for program repair, Xia et al. [115] interleave patch generation with test validation feedback to prompt future generation iteratively.In detail, they incorporate various information from a failing test including its name, the relevant code line(s) triggering the test failure, and the error message produced in the next round of prompting which can help the model understand the failure reason and provide guidance towards generating the correct fix.Another example is for mobile GUI testing, Liu et al. [14] iteratively query the LLM about the operation (e.g., click a button, enter a text) to be conducted in the mobile app, and at each iteration, they would provide the LLM with current context information like which GUI pages and widgets have just explored.</p>
<p>Mapping between testing tasks and how LLMs are used.Figure 8 demonstrates the mapping between the testing tasks (mentioned in Section 4) and how LLMs are used (as introduced in this subsection).The unit test case generation and program repair share similar patterns of communicating with the LLMs, since both tasks are closely related to the source code.Typically, researchers utilize pretraining and/or fine-tuning and zero-shot learning methods for these two tasks.Zero-shot learning is suitable because these tasks are relatively straightforward and can be easily understood by LLMs.Moreover, since the training data for these two tasks can be automatically collected from source code repositories, pre-training and/or fine-tuning methods In comparison, for system test input generation, zeroshot learning and few-shot learning methods are commonly used.This might be because this task often involves generating specific types of inputs, and demonstrations in fewshot learning can assist the LLMs in better understanding what should be generated.Besides, for this task, the utilization of pre-training and/or fine-tuning methods are not as widespread as in unit test case generation and program repair.This might be attributed to the fact that training data for system testing varies across different software and is relatively challenging to collect automatically.</p>
<p>Input of LLM</p>
<p>We also find that different testing tasks or software under test might involve diversified input when querying the LLM, as demonstrated in Figure 9.</p>
<p>The most commonly utilized input is the source code since a large portion of collected studies relate to program repair or unit test case generation whose input are source code.For unit test case generation, typical code-related information would be (i) the complete focal method, including the signature and body; (ii) the name of the focal class (i.e., the class that the focal method belongs to); (iii) the field in the focal class; and (iv) the signatures of all methods defined in the focal class [7], [26].For program repair, there can be different setups and involve different inputs, including (i) inputting a buggy function with the goal of outputting the patched function, (ii) inputting the buggy location with the goal of generating the correct replacement code (can be a single line change) given the prefix and suffix of the buggy function [93].Besides, there can be variations for the buggy location input, i.e., (i) does not contain the buggy lines (but the bug location is still known), (ii) give the buggy lines as lines of comments.</p>
<p>There are also 12 studies taking the bug description as input for the LLM.For example, Kang et al. [78] take the bug description as input when querying LLM and let the LLM generate the bug-reproducing test cases.Fakhoury et al. [122] input the natural language descriptions of bugs to the LLM, and generate the correct code fixes.</p>
<p>There are 7 studies that would provide the intermediate error information, e.g., test failure information, to the LLM, and would conduct the iterative prompt (as described in Section 5.2) to enrich the context provided to the LLM.These studies are related to the unit test case generation and program repair, since in these scenarios, the running information can be acquired easily.</p>
<p>When testing mobile apps, since the utilized LLM could not understand the image of the GUI page, the view hierarchy file which represents the details of the GUI page usually acts as the input to LLMs.Nevertheless, with the emergence of GPT-4 which is a multimodal model and accepts both image and text inputs for model input, the GUI screenshots might be directly utilized for LLM's input.</p>
<p>Incorporating Other Techniques with LLM</p>
<p>There are divided opinions on whether LLM has reached an all-powerful status that requires no other techniques.As shown in Figure 10, among our collected studies, 67 of them utilize LLMs to address the entire testing task, while 35 studies incorporate additional techniques.These techniques include mutation testing, differential testing, syntactic checking, program analysis, statistical analysis, etc. .The reason why researchers still choose to combine LLMs with other techniques might be because, despite exhibiting enormous potential in various tasks, LLMs still possess limitations such as comprehending code semantics and handling complex program structures.Therefore, combining LLMs with other techniques optimizes their strengths and weaknesses to achieve better outcomes in specific scenarios.In addition, it is important to note that while LLMs are capable of generating correct code, they may not necessarily produce sufficient test cases to check for edge cases or rare scenarios.This is where mutation and other testing techniques come into play, as they allow for the generation of more diverse and complex code that can better simulate real-world scenarios.Taken in this sense, a testing approach can incorporate a combination of different techniques, including both LLMs and other testing strategies, to ensure comprehensive coverage and effectiveness.</p>
<p>LLM + statistical analysis.As LLMs can often generate a multitude of outputs, manually sifting through and identifying the correct output can be overwhelmingly laborious.As such, researchers have turned to statistical analysis techniques like ranking and clustering [28], [45], [78], [93], [116]  LLM + program analysis.When utilizing LLMs to accomplish tasks such as generating unit test cases and repairing software code, it is important to consider that software code inherently possesses structural information, which may not be fully understood by LLMs.Hence, researchers often utilize program analysis techniques, including code abstract syntax trees (ASTs) [74], to represent the structure of code more effectively and increase the LLM's ability to comprehend the code accurately.Researchers also perform the structure-based subsetting of code lines to narrow the focus for LLM [94], or extract additional code context from other code files [7], to enable the models to focus on the most task-relevant information in the codebase and lead to more accurate predictions.</p>
<p>LLM + mutation testing.It is mainly targeting at generating more diversified test inputs.For example, Deng et al. [59] first use LLM to generate the seed programs (e.g., code snippets using a target DL API) for fuzzing deep learning libraries.To enrich the pool of these test programs, they replace parts of the seed program with masked tokens using mutation operators (e.g., replaces the API call arguments with the span token) to produce masked inputs, and again utilize the LLMs to perform code infilling to generate new code that replaces the masked tokens.</p>
<p>LLM + syntactic checking.Although LLMs have shown remarkable performance in various natural language processing tasks, the generated code from these models can sometimes be syntactically incorrect, leading to potential errors and reduced usability.Therefore, researchers have proposed to leverage syntax checking to identify and correct errors in the generated code.For example, in their work for unit test case generation, Alagarsamy et al. [29] additionally introduce a verification method to check and repair the naming consistency (i.e., revising the test method name to be consistent with the focal method name) and the test signatures (i.e., adding missing keywords like public, void, or @test annotations).Xie et al. [36] also validates the generated unit test case and employs rule-based repair to fix syntactic and simple compile errors.</p>
<p>LLM + differential testing.Differential testing is wellsuited to find semantic or logic bugs that do not exhibit explicit erroneous behaviors like crashes or assertion failures.In this category of our collected studies, the LLM is mainly responsible for generating valid and diversified inputs, while the differential testing helps to determine whether there is a triggered bug based on the software's output.For example, Ye et al. [48] first uses LLM to produce random JavaScript programs, and leverages the language specification document to generate test data, then conduct the differential testing on JavaScript engines such as JavaScriptCore, ChakraCore, SpiderMonkey, QuickJS, etc.There are also studies utilizing the LLMs to generate test inputs and then conduct differential testing for fuzzing DL libraries [58], [59] and SAT solvers [63].Li et al. [87] employs the LLM in finding the failure-inducing test cases.In detail, given a program under test, they first request the LLM to infer the intention of the program, then request the LLM to generate programs that have the same intention, which are alternative implementations of the program, and are likely free of the program's bug.Then they perform the differential testing with the program under test and the generated programs to find the failure-inducing test cases.</p>
<p>CHALLENGES AND OPPORTUNITIES</p>
<p>Based on the above analysis from the viewpoints of software testing and LLM, we summarize the challenges and opportunities when conducting software testing with LLM.</p>
<p>Challenges</p>
<p>As indicated by this survey, software testing with LLMs has undergone significant growth in the past two years.However, it is still in its early stages of development, and numerous challenges and open questions need to be addressed.</p>
<p>Challenges for Achieving High Coverage</p>
<p>Exploring the diverse behaviors of the software under test to achieve high coverage is always a significant concern in software testing.In this context, test generation differs from code generation, as code generation primarily focuses on producing a single, correct code snippet, whereas software testing requires generating diverse test inputs to ensure better coverage of the software.Although setting a high temperature can facilitate the LLMs in generating different outputs, it remains challenging for LLMs to directly achieve the required diversity.For example, for unit test case generation, in SF110 dataset, the line coverage is merely 2% and the branch coverage is merely 1% [39].For system test input generation, in terms of fuzzing DL libraries, the API coverage for TensorFlow is reported to be 66% (2215/3316) [59].</p>
<p>From our collected studies, we observe that the researchers often utilize mutation testing together with the LLMs to generate more diversified outputs.For example, when fuzzing a DL library, instead of directly generating the code snippet with LLM, Deng et al. [59] replace parts of the selected seed (code generated by LLM) with masked tokens using different mutation operators to produce masked inputs.They then leverage the LLM to perform code infilling to generate new code that replaces the masked tokens, which can significantly increase the diversity of the generated tests.Liu et al. [65] leverage LLM to produce the test generators (each of which can yield a batch of unusual text inputs under the same mutation rule) together with the mutation rules for text-oriented fuzzing, which reduces the human effort required for designing mutation rules.</p>
<p>A potential research direction could involve utilizing testing-specific data to train or fine-tune a specialized LLM that is specifically designed to understand the nature of testing.By doing so, the LLM can inherently acknowledge the requirements of testing and autonomously generate diverse outputs.</p>
<p>Challenges in Test Oracle Problem</p>
<p>The oracle problem has been a longstanding challenge in various testing applications, e.g., testing machine learning systems [150] and testing deep learning libraries [59].To alleviate the oracle problem to the overall testing activities, a common practice in our collected studies is to transform it into a more easily derived form, often by utilizing differential testing [63] or focusing on only identifying crash bugs [14].</p>
<p>There are successful applications of differential testing with LLMs, as shown in Figure 10.For instance, when testing the SMT solvers, Sun et al. adopt differential testing which involves comparing the results of multiple SMT solvers (i.e., Z3, cvc5, and Bitwuzla) on the same generated test formulas by LLM [63].However, this approach is limited to systems where counterpart software or running environment can easily be found, potentially restricting its applicability.Moreover, to mitigate the oracle problem, other studies only focus on the crash bugs which are easily observed automatically.This is particularly the case for mobile applications testing, in which the LLMs guide the testing in exploring more diversified pages, conducting more complex operational actions, and covering more meaningful operational sequences [14].However, this significantly restricts the potential of utilizing the LLMs for uncovering various types of software bugs.</p>
<p>Exploring the use of LLMs to derive other types of test oracles represents an interesting and valuable research direction.Specifically, metamorphic testing is also widely used in software testing practices to help mitigate the oracle problem, yet in most cases, defining metamorphic relations relies on human ingenuity.Luu et al. [56] have examined the effectiveness of LLM in generating metamorphic relations, yet they only experiment with straightforward prompts by directly querying ChatGPT.Further exploration, potentially incorporating human-computer interaction or domain knowledge, is highly encouraged.Another promising avenue is exploring the capability of LLMs to automatically generate test cases based on metamorphic relations, covering a wide range of inputs.</p>
<p>The advancement of multi-model LLMs like GPT-4 may open up possibilities for exploring their ability to detect bugs in software user interfaces and assist in deriving test oracles.By leveraging the image understanding and reasoning capabilities of these models, one can investigate their potential to automatically identify inconsistencies, errors, or usability issues in user interfaces.</p>
<p>Challenges for Rigorous Evaluations</p>
<p>The lack of benchmark datasets and the potential data leakage issues associated with LLM-based techniques present challenges in conducting rigorous evaluations and comprehensive comparisons of proposed methods.</p>
<p>For program repair, there are only two well-known and commonly-used benchmarks, i.e., Defect4J and QuixBugs, as demonstrated in Table 4. Furthermore, these datasets are not specially designed for testing the LLMs.For example, as reported by Xia et al. [93], 39 out of 40 Python bugs in the QuixBugs dataset can be fixed by Codex, yet in real-world practice, the successful fix rate can be nowhere near as high.For unit test case generation, there are no widely recognized benchmarks, and different studies would utilize different datasets for performance evaluation, as demonstrated in Table 3.This indicates the need to build more specialized and diversified benchmarks.</p>
<p>Furthermore, the LLMs may have seen the widely-used benchmarks in their pre-training data, i.e., data leakage issues.Jiang et al. [113] check the CodeSearchNet and BigQuery, which are the data sources of common LLMs, and the results show that four repositories used by the Defect4J benchmark are also in CodeSearchNet, and the whole Defects4J repository is included by BigQuery.Therefore, it is very likely that existing program repair benchmarks are seen by the LLMs during pre-training.This data leakage issue has also been investigated in machine learning-related studies.For example, Tu et al. [151] focus on the data leakage in issue tracking data, and results show that information leaked from the "future" makes prediction models misleadingly optimistic.This reminds us that the performance of LLMs on software testing tasks may not be as good as reported in previous studies.It also suggests that we need more specialized datasets that are not seen by LLMs to serve as benchmarks.One way is to collect it from specialized sources, e.g., user-generated content from niche online communities.</p>
<p>Challenges in Real-world Application of LLMs in Software Testing</p>
<p>As we mentioned in Section 5.2, in the early days of using LLMs, pre-training and fine-tuning are commonly used practice, considering the model parameters are relatively few resulting in weaker model capabilities (e.g., T5).As time progressed, the number of model parameters increased significantly, leading to the emergence of models with greater capabilities (e.g., ChatGPT).And in recent studies, prompt engineering has become a common approach.However, due to concerns regarding data privacy, when considering realworld practice, most software organizations tend to avoid using commercial LLMs and would prefer to adopt opensource ones with training or fine-tuning using organizationspecific data.Furthermore, some companies also consider the current limitations in terms of computational power or pay close attention to energy consumption, they tend to fine-tune medium-sized models.It is quite challenging for these models to achieve similar performance to what our collected papers have reported.For instance, in the widelyused QuixBugs dataset, it has been reported that 39 out of 40 Python bugs and 34 out of 40 Java bugs can be automatically fixed [93].However, when it comes to DL programs collected from Stack Overflow, which represent real-world coding practice, only 16 out of 72 Python bugs can be automatically fixed [89].</p>
<p>Recent research has highlighted the importance of highquality training data in improving the performance of models for code-related tasks [152], yet manually building highquality organization-specific datasets for training or finetuning is time-consuming and labor-intensive.To address this, one is encouraged to utilize the automated techniques of mining software repositories to build the datasets, for example, techniques like key information extraction techniques from Stack Overflow [153] offer potential solutions for automatically gathering relevant data.</p>
<p>In addition, exploring the methodology for better finetuning the LLMs with software-specific data is worth considering because software-specific data differs from natural language data as it contains more structural information, such as data flow and control flow.Previous research on code representations has shown the benefits of incorporating data flow, which captures the semantic-level structure of code and represents the relationship between variables in terms of "whether-value-comes-from" [154].These insights can provide valuable guidance for effectively fine-tuning LLMs with software-specific data.</p>
<p>Opportunities</p>
<p>There are also many research opportunities in software testing with LLMs, which can greatly benefit developers, users, and the research community.While not necessarily challenges, these opportunities contribute to advancements in software testing, benefiting practitioners and the wider research community.</p>
<p>Exploring LLMs in the Early Stage of Testing</p>
<p>As shown in Figure 4, LLMs have not been used in the early stage of testing, e.g., test requirements, and test planning.There might be two main reasons behind that.The first is the subjectivity in early-stage testing tasks.Many tasks in the early stages of testing, such as requirements gathering, test plan creation, and design reviews, may involve subjective assessments that require significant input from human experts.This could make it less suitable for LLMs that rely heavily on data-driven approaches.The second might be the lack of open-sourced data in the early stages.Unlike in later stages of testing, there may be limited data available online during early-stage activities.This could mean that LLMs may not have seen much of this type of data, and therefore may not perform well on these tasks.</p>
<p>Adopting a human-computer interaction schema for tackling early-stage testing tasks would harness the domainspecific knowledge of human developers and leverage the general knowledge embedded in LLMs.Additionally, it is highly encouraged for software development companies to record and provide access to early-stage testing data, allowing for improved training and performance of LLMs in these critical testing activities.</p>
<p>Exploring LLMs in Other Testing Phases</p>
<p>We have analyzed the distribution of testing phases for the collected studies.As shown in Fig 11, we can observe that LLMs are most commonly used in unit testing, followed by system testing.However, there is still no research on the use of LLMs in integration testing and acceptance testing.For integration testing, it involves testing the interfaces between different software modules.In some software organizations, integration testing might be merged with unit testing, which can be a possible reason why LLM is rarely utilized in integration testing.Another reason might be that the size and complexity of the input data in this circumstance may exceed the capacity of the LLM to process and analyze (e.g., the source code of all involved software modules), which can lead to errors or unreliable results.To tackle this, a potential reference can be found in Section 4.1, where Xie et al. [36] design a method to organize the necessary information into the pre-defined maximum prompt token limit of the LLM.Furthermore, integration testing requires diversified data to be generated to sufficiently test the interface among multiple modules.As mentioned in Section 4.3, previous work has demonstrated the LLM's capability in generating diversified test input for system testing, in conjunction with mutation testing techniques [48], [59].And these can provide insights about generating the diversified interface data for integration testing.</p>
<p>Acceptance testing is usually conducted by business analysts or end-users to validate the system's functionality and usability, which requires more non-technical language and domain-specific knowledge, thus making it challenging to apply LLM effectively.Since acceptance testing involves humans, it is well-suited for the use of human-in-the-loop schema with LLMs.This has been studied in traditional machine learning [155], but has not yet been explored with LLMs.Specifically, the LLMs can be responsible for automatically generating test cases, evaluating test coverage, etc, while human testers are responsible for checking the program's behavior and verifying test oracle.</p>
<p>Exploring LLMs for More Types of Software</p>
<p>We analyze what types of software have been explored in the collected studies, as shown in Figure 5.Note that, since a large portion of studies are focused on unit testing or program repair, they are conducted on publicly available datasets and do not involve specific software types.</p>
<p>From the analysis in Section 4.3, the LLM can generate not only the source code for testing DL libraries but also the textual input for testing mobile apps, even the models for testing CPS.Overall, the LLM provides a flexible and powerful framework for generating test inputs for a wide range of applications.Its versatility would make it useful for testing the software in other domains.</p>
<p>From one point of view, some proposed techniques can be applied to other types of software.For example, in the paper proposed for testing deep learning libraries [58], since it proposes techniques for generating diversified, complicated, and human-like DL programs, the authors state that the approach can be easily extended to test software systems from other application domains, e.g., interpreters, database systems, and other popular libraries.More than that, there are already studies that focus on universal fuzzing techniques [52], [67] which are designed to be adaptable and applicable to different types of test inputs and software.</p>
<p>From another point of view, other types of software can also benefit from the capabilities of LLMs to design the testing techniques that are better suited to their specific domain and characteristics.For instance, the metaverse, with its immersive virtual environments and complex interactions, presents unique challenges for software testing.LLMs can be leveraged to generate diverse and realistic inputs that mimic user behavior and interactions within the metaverse, which are never explored.</p>
<p>Exploring LLMs for Non-functional Testing</p>
<p>In our collected studies, LLMs are primarily used for functional testing, and no practice in performance testing, usability testing or others.One possible reason for the prevalence of LLM-based solutions in functional testing is that they can convert functional testing problems into code generation or natural language generation problems [14], [59], which LLMs are particularly adept at solving.</p>
<p>On the other hand, performance testing and usability testing may require more specialized models that are designed to detect and analyze specific types of data, handle complex statistical analyses, or determine the buggy criteria.Moreover, there have been dozens of performance testing tools (e.g., LoadRunner [156]) that can generate a workload that simulates real-world usage scenarios and achieve relatively satisfactory performance.</p>
<p>The potential opportunities might let the LLM integrate the performance testing tools and acts like the LangChain [157], to better simulate different types of workloads based on real user behavior.Furthermore, the LLMs can identify the parameter combinations and values that have the highest potential to trigger performance problems.It is essentially a way to rank and prioritize different parameter settings based on their impact on performance and improve the efficiency of performance testing.</p>
<p>Exploring Advanced Prompt Engineering</p>
<p>There are a total of 11 commonly used prompt engineering techniques as listed in a popular prompt engineering guide [158], as shown in Figure 12.Currently, in our collected studies, only the first five techniques are being utilized.The more advanced techniques have not been employed yet, and can be explored in the future for prompt design.For instance, multimodal chain of thought prompting involves using diverse sensory and cognitive cues to stimulate thinking and creativity in LLMs [159].By providing images (e.g., GUI screenshots) or audio recordings related to the software under test can help the LLM better understand the software's context and potential issues.Besides, try to prompt the LLM to imagine itself in different roles, such as a developer, user, or quality assurance specialist.This perspective-shifting exercise enables the LLM to approach software testing from multiple viewpoints and uncover different aspects that might require attention or investigation.</p>
<p>Graph prompting [160] involves the representation of information using graphs or visual structures to facilitate understanding and problem-solving.Graph prompting can be a natural match with software engineering, consider it involves various dependencies, control flow, data flow, state transitions, or other relevant graph structure.Graph prompting can be beneficial in analyzing this structural information, and enabling the LLMs to comprehend the software under test effectively.For instance, testers can use graph prompts to visualize test coverage, identify untested areas or paths, and ensure adequate test execution.</p>
<p>Incorporating LLMs with Traditional Techniques</p>
<p>There is currently no clear consensus on the extent to which LLMs can solve software testing problems.From the analysis in Section 5.4, we have seen some promising results from studies that have combined LLMs with traditional software testing techniques.This implies the LLMs are not the sole silver bullet for software testing.Considering the availability of many mature software testing techniques and tools, and the limited capabilities of LLMs, it is necessary to explore other better ways to combine LLMs with traditional testing or program analysis techniques and tools for better software testing.</p>
<p>Based on the collected studies, the LLMs have been successfully utilized together with various techniques such as differential testing (e.g., [63]), mutation testing (e.g., [59]), program analysis (e.g., [104], as shown in Figure 10.From one perspective, future studies can explore improved integration of these traditional techniques with LLMs.Take mutation testing as an example, current practices mainly rely on the human-designed mutation rules to mutate the candidate tests, and let the LLMs re-generate new tests [38], [59], [67], while Liu et al. directly utilize the LLMs for producing the mutation rules alongside the mutated tests [65].Further explorations in this direction are of great interest.</p>
<p>From another point of view, more traditional techniques can be incorporated in LLMs for software testing.For instance, besides the aforementioned traditional techniques, the LLMs have been combined with formal verification for self-healing software detection in the field of software security [161].More attempts are encouraged.Moreover, considering the existence of numerous mature software testing tools, one can explore the integration of LLMs with these tools, allowing them to act as a "LangChain" to better explore the potential of these tools.</p>
<p>RELATED WORK</p>
<p>The systematic literature review is a crucial manner for gaining insights into the current trends and future directions within a particular field.It enables us to understand and stay updated on the developments in that domain.</p>
<p>Wang et al. surveyed the machine learning and deep learning techniques for software engineering [162].Yang et al. and Watson et al. respectively carried out surveys about the use of deep learning in software engineering domain [163], [164].Bajammal et al. surveyed the utilization of computer vision techniques to improve software engineering tasks [165].Zhang et al. provided a survey of techniques for testing machine learning systems [150] With the advancements of artificial intelligence and LLMs, researchers also conduct systematic literature reviews about LLMs, and their applications in various fields (e.g., software engineering).Zhao et al. [17] reviewed recent advances in LLMs by providing an overview of their background, key findings, and mainstream techniques.They focused on four major aspects of LLMs, namely pre-training, adaptation tuning, utilization, and capacity evaluation.Additionally, they summarized the available resources for developing LLMs and discuss the remaining issues for future directions.Hou et al. conducted a systematic literature review on using LLMs for software engineering, with a particular focus on understanding how LLMs can be exploited to optimize processes and outcomes [166].Fan et al. conducted a survey of LLMs for software engineering, and set out open research challenges for the application of LLMs to technical problems faced by software engineers [167].Zan et al. conducted a survey of existing LLMs for NL2Code task (i.e., generating code from a natural language description), and reviewed benchmarks and metrics [168].</p>
<p>While these studies either targeted the broader software engineering domain (with a limited focus on software testing tasks) or focused on other software development tasks (excluding software testing), this paper specifically focuses on the use of LLMs for software testing.It surveys related studies, summarizes key challenges and potential opportunities, and serves as a roadmap for future research in this area.</p>
<p>CONCLUSION</p>
<p>This paper provides a comprehensive review of the use of LLMs in software testing.We have analyzed relevant studies that have utilized LLMs in software testing from both the software testing and LLMs perspectives.This paper also highlights the challenges and potential opportunities in this direction.Results of this review demonstrate that LLMs have been successfully applied in a wide range of testing tasks, including unit test case generation, test oracle generation, system test input generation, program debugging, and program repair.However, challenges still exist in achieving high testing coverage, addressing the test oracle problem, conducting rigorous evaluations, and applying LLMs in real-world scenarios.Additionally, it is observed that LLMs are commonly used in only a subset of the entire testing lifecycle, for example, they are primarily utilized in the middle and later stages of testing, only serving the unit and system testing phases, and only for functional testing.This highlights the research opportunities for exploring the uncovered areas.Regarding how the LLMs are utilized, we find that various pre-training/fine-tuning and prompt engineering methods have been developed to enhance the capabilities of LLMs in addressing testing tasks.However, more advanced techniques in prompt design have yet to be explored and can be an avenue for future research.</p>
<p>It can serve as a roadmap for future research in this area, identifying gaps in our current understanding of the use of LLMs in software testing and highlighting potential avenues for exploration.We believe that the insights provided in this paper will be valuable to both researchers and practitioners in the field of software engineering, assisting them in leveraging LLMs to improve software testing practices and ultimately enhance the quality and reliability of software systems.</p>
<p>Fig. 1 :
1
Fig. 1: Structure of the contents in this paper (the numbers in bracket indicates the number of involved papers, and a paper might involve zero or multiple items)</p>
<p>Fig. 2 :Figure 2
22
Fig. 2: Overview of the paper collection process</p>
<p>Fig. 8 :Fig. 9 :
89
Fig. 8: Mapping between testing tasks and how LLMs are used</p>
<p>Fig. 10 :
10
Fig. 10: Distribution about other techniques incorporated with LLMs (Note that, a study can involve multiple types)</p>
<p>Fig. 11 :
11
Fig.11: Distribution of testing phases (note that we omit the studies which do not explicitly specify the testing phases, e.g., program repair)</p>
<p>Fig. 12 :
12
Fig. 12: List of advanced prompt engineering practices and those utilized in the collected papers</p>
<p>TABLE 1 :
1
Details of the collected papers
IDTopicPaper title</p>
<p>TABLE 2 :
2
Conference proceedings and journals considered for manual search
AcronymVenueICSEInternational Conference on Software EngineeringESEC/FSEJoint European Software Engineering Conference and Symposium on theSE ConferenceASE ISSTA ICST ESEM MSRFoundations of Software Engineering International Conference on Automated Software Engineering International Symposium on Software Testing and Analysis International Conference on Software Testing, Verification and Validation International Symposium on Empirical Software Engineering and Mea-surement International Conference on Mining Software RepositoriesQRSInternational Conference on Software Quality, Reliability and SecurityICSMEInternational Conference on Software Maintenance and EvolutionISSREInternational Symposium on Software Reliability EngineeringTSETransactions on Software EngineeringTOSEMTransactions on Software Engineering and MethodologyEMSEEmpirical Software EngineeringSE JournalASE JSS JSEP STVR IEEE SOFTW. IEEE Software Automated Software Engineering Journal of Systems and Software Journal of Software: Evolution and Process Software Testing, Verification and ReliabilityIET SOFTW.IET SoftwareISTInformation and Software TechnologySQJSoftware Quality JournalICLRInternational Conference on Learning RepresentationsAI VenuesNeurIPS ICML AAAI EMNLP ACLConference on Neural Information Processing Systems International Conference on Machine Learning AAAI Conference on Artificial Intelligence Conference on Empirical Methods in Natural Language Processing Annual Meeting of the Association for Computational LinguisticsIJCAIInternational Joint Conference on Artificial Intelligence</p>
<p>TABLE 3 :
3
[39]ormance of unit test case generation Note that,[39]experiments with Codex, CodeGen, and ChatGPT, and the best performance was achieved by Codex.
DatasetCorrectnessCoverageLLMPaper5 Java projects from Defects4J16.21%5%-13% (line coverage)BART[26]10 Jave projects40%89% (line coverage), 90% (branch coverage)ChatGPT[36]CodeSearchNet41%N/AChatGPT[7]HumanEval78%87% (line coverage), 92% (branch coverage)Codex[39]SF1102%2% (line coverage), 1% (branch coverage)Codex[39]</p>
<p>Terms of Software TypesFigure5demonstrates the types of software under test in our collected studies.It is evident that the most prominent category is mobile apps, with five studies utilizing LLMs for testing, possibly due to their prevalence and importance
0RELOHDSS'HHSOHDUQLQJOLEUDU\6RIWZDUH8QGHU7HVW&amp;RPSLOHU 607VROYHU $XWRQRPRXVGULYLQJV\VWHP &amp;\EHUSK\VLFDOV\VWHP *2WRROFKDLQ -DYD6FULSWHQJLQH4XDQWXPFRPSXWLQJSODWIRUP9LGHRJDPH3DSHU&amp;RXQWFig. 5: Distribution of software under testin today's business and daily life. Additionally, there arerespectively two studies focusing on testing deep learninglibraries, compilers, and SMT solvers. Moreover, LLM-basedtesting techniques have also been applied to domains suchas cyber-physical systems, quantum computing platforms,and more. This widespread adoption of LLMs demonstratestheir effectiveness in handling diverse test inputs and en-hancing testing activities across various software domains.A detailed analysis is provided below.</p>
<p>TABLE 4 :
4
[102]rmance of program repairNote that, for studies with multiple datasets or LLMs, we only present the best performance or in the most commonly utilized dataset.patches that passed all the tests (i.e., plausible patches), they further ask the LLM to generate alternative variations of the original plausible patches.This can further build on and learn from earlier successes to generate more plausible patches to increase the chance of having correct patches.Zhang et al.[94]propose a similar approach design by leveraging multimodal prompts (e.g., natural language description, error message, input-output-based test cases), iterative querying, test-case-based few-shot selection to produce repairs.Moon et al.[102]propose for bug fixing with feedback.It consists of a critic model to generate feedback, an editor to edit codes based on the feedback, and a feedback selector to choose the best possible feedback from the critic.Wei et.al.
Dataset% Correct patchesLLMPaperDefects4J v1.2, Defects4J22/40 Jave bugs (QuixBugs dataset, with InCoder-6B, correctPLBART, CodeT5, CodeGen, In-[113]v2.0,QuixBugs,code infilling setting)Coder (each with variant pa-HumanEval-Javarameters, 10 LLMs in total)QuixBugs23/40 Python bugs, 14/40 Java bugs (complete function genera-Codex-12B[100]tion setting)Defects4J v1.2, Defects4J39/40 Python bugs, 34/40 Java bugs (QuixBugs dataset, withCodex, GPT-Neo, CodeT5, In-[93]v2.0, QuixBugs, Many-Codex-12B, correct code infilling setting); 37/40 Python bugs,Coder (each with variant pa-Bugs32/40 Java bugs (QuixBugs dataset, with Codex-12B, completerameters, 9 LLMs in total)function generation setting)QuixBugs31/40 Python bugs (completion function generation setting)ChatGPT-175B[96]DL programs from Stack-16/72 Python bugs (complete function generation setting)ChatGPT-175B[89]Overflow</p>
<p>G J Myers, The art of software testing. Wiley2004</p>
<p>Software testing and analysis -process, principles and techniques. M Pezzè, M Young, 2007Wiley</p>
<p>A theoretical and empirical study of search-based testing: Local, global, and hybrid search. M Harman, P Mcminn, 201036</p>
<p>Interevo-tr: Interactive evolutionary test generation with readability assessment. P Delgado-Pérez, A Ramírez, K J Valle-G Ómez, I Medina-Bulo, J R Romero, IEEE Trans. Software Eng. 4942023</p>
<p>Characteristic studies of loop problems for structural test generation via symbolic execution. X Xiao, S Li, T Xie, N Tillmann, 2013 28th IEEE/ACM International Conference on Automated Software Engineering, ASE 2013. E Denney, T Bultan, A Zeller, Silicon Valley, CA, USAIEEENovember 11-15, 2013. 2013</p>
<p>Feedbackdirected random test generation. C Pacheco, S K Lahiri, M D Ernst, T Ball, 29th International Conference on Software Engineering (ICSE 2007). Minneapolis, MN, USAIEEE Computer SocietyMay 20-26, 2007. 2007</p>
<p>No more manual tests? evaluating and improving chatgpt for unit test generation. Z Yuan, Y Lou, M Liu, S Ding, K Wang, Y Chen, X Peng, arXiv:2305.042072023arXiv preprint</p>
<p>Chatgpt vs SBST: A comparative assessment of unit test suite generation. Y Tang, Z Liu, Z Zhou, X Luo, 10.48550/arXiv.2307.00588abs/2307.00588CoRR. 2023</p>
<p>Ui/application exerciser monkey. A Developers, 2012</p>
<p>Droidbot: a lightweight uiguided test input generator for android. Y Li, Z Yang, Y Guo, X Chen, 2017ICSE. IEEE</p>
<p>Guided, stochastic model-based gui testing of android apps. T Su, G Meng, Y Chen, K Wu, W Yang, Y Yao, G Pu, Y Liu, Z Su, Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering. the 2017 11th Joint Meeting on Foundations of Software Engineering2017</p>
<p>Timetravel testing of android apps. Z Dong, M Öhme, L Cojocaru, A Roychoudhury, ICSE. IEEE2020</p>
<p>Reinforcement learning based curiosity-driven testing of android applications. M Pan, A Huang, G Wang, T Zhang, X Li, Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis. the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis2020</p>
<p>Make LLM a testing expert: Bringing humanlike interaction to mobile GUI testing via functionality-aware decisions. Z Liu, C Chen, J Wang, M Chen, B Wu, X Che, D Wang, Q Wang, 10.48550/arXiv.2310.15780CoRR. 2310.15780, 2023</p>
<p>Benchmarking automated GUI testing for android against real-world bugs. T Su, J Wang, Z Su, ESEC/FSE '21: 29th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering. Athens, GreeceACMAugust 23-28, 2021. 2021</p>
<p>Talking about large language models. M Shanahan, 10.48550/arXiv.2212.03551abs/2212.035512022</p>
<p>A survey of large language models. W X Zhao, K Zhou, J Li, T Tang, X Wang, Y Hou, Y Min, B Zhang, J Zhang, Z Dong, Y Du, C Yang, Y Chen, Z Chen, J Jiang, R Ren, Y Li, X Tang, Z Liu, P Liu, J Nie, J Wen, 10.48550/arXiv.2303.18223CoRR. 2303.18223, 2023</p>
<p>Large language models are zeroshot reasoners. T Kojima, S S Gu, M Reid, Y Matsuo, Y Iwasawa, NeurIPS. 2022</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, B Ichter, F Xia, E H Chi, Q V Le, D Zhou, NeurIPS. 2022</p>
<p>Structured chain-of-thought prompting for code generation. J Li, G Li, Y Li, Z Jin, 2023258615421</p>
<p>Skcoder: A sketch-based approach for automatic code generation. J Li, Y Li, G Li, Z Jin, Y Hao, X Hu, 2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE). 2023</p>
<p>Acecoder: Utilizing existing code to enhance code generation. J Li, Y Zhao, Y Li, G Li, Z Jin, 2023257901190</p>
<p>Self-collaboration code generation via chatgpt. Y Dong, X Jiang, Z Jin, G Li, 10.48550/arXiv.2304.07590abs/2304.07590CoRR. 2023</p>
<p>Unifying large language models and knowledge graphs: A roadmap. S Pan, L Luo, Y Wang, C Chen, J Wang, X Wu, 10.48550/arXiv.2306.08302abs/2306.08302CoRR. 2023</p>
<p>The art of software testing. G J Myers, T Badgett, T M Thomas, C Sandler, 2004Wiley Online Library2</p>
<p>Unit test case generation with transformers and focal context. M Tufano, D Drain, A Svyatkovskiy, S K Deng, N Sundaresan, arXiv:2009.056172020arXiv preprint</p>
<p>Codet: Code generation with generated tests. B Chen, F Zhang, A Nguyen, D Zan, Z Lin, J.-G Lou, W Chen, arXiv:2207.103972022arXiv preprint</p>
<p>Interactive code generation via test-driven user-intent formalization. S K Lahiri, A Naik, G Sakkas, P Choudhury, C Veh, M Musuvathi, J P Inala, C Wang, J Gao, arXiv:2208.059502022arXiv preprint</p>
<p>A3test: Assertion-augmented automated test case generation. S Alagarsamy, C Tantithamthavorn, A Aleti, arXiv:2302.103522023arXiv preprint</p>
<p>An empirical evaluation of using large language models for automated unit test generation. M Schäfer, S Nadi, A Eghbali, F Tip, IEEE Transactions on Software Engineering. 2023</p>
<p>An initial investigation of chatgpt unit test generation capability. V Guilherme, A Vincenzi, A L Fontão, D M B Paiva, H Borges, M I Cagnin, P G Fernandes, V Borges, S M Melo, 10.1145/3624032.36240358th Brazilian Symposium on Systematic and Automated Software Testing, SAST 2023. V H S Durelli, E D Canedo, Campo Grande, MS, BrazilACMSeptember 25-29, 2023. 2023</p>
<p>Automated test case generation using code models and domain adaptation. S Hashtroudi, J Shin, H Hemmati, S Wang, 10.48550/arXiv.2308.08033abs/2308.08033CoRR. 2023</p>
<p>Automatic generation of test cases based on bug reports: a feasibility study with large language models. L Plein, W C Ouédraogo, J Klein, T F Bissyandé, 10.48550/arXiv.2310.06320abs/2310.06320CoRR. 2023</p>
<p>Can large language models write good property-based tests. V Vikram, C Lemieux, R Padhye, 10.48550/arXiv.2307.04346abs/2307.04346CoRR. 2023</p>
<p>CAT-LM training language models on aligned code and tests. N Rao, K Jain, U Alon, C L Goues, V J Hellendoorn, 10.1109/ASE56229.2023.0019338th IEEE/ACM International Conference on Automated Software Engineering, ASE 2023. LuxembourgSeptember 11-15, 2023. IEEE, 2023</p>
<p>Chatunitest: a chatgpt-based automated unit test generation tool. Z Xie, Y Chen, C Zhi, S Deng, J Yin, arXiv:2305.047642023arXiv preprint</p>
<p>Codamosa: Escaping coverage plateaus in test generation with pre-trained large language models. C Lemieux, J P Inala, S K Lahiri, S Sen, International conference on software engineering (ICSE). 2023</p>
<p>Effective test generation using pre-trained large language models and mutation testing. A M Dakhel, A Nikanjam, V Majdinasab, F Khomh, M C Desmarais, 10.48550/arXiv.2308.16557CoRR. 2308.16557, 2023</p>
<p>Exploring the effectiveness of large language models in generating unit tests. M L Siddiq, J Santos, R H Tanvir, N Ulfat, F A Rifat, V C Lopes, arXiv:2305.004182023arXiv preprint</p>
<p>How well does LLM generate security tests. Y Zhang, W Song, Z Ji, D Yao, N Meng, 10.48550/arXiv.2310.00710abs/2310.00710CoRR. 2023</p>
<p>Prompting code interpreter to write better unit tests on quixbugs functions. V Li, N Doiron, 10.48550/arXiv.2310.00483abs/2310.00483CoRR. 2023</p>
<p>Reinforcement learning from automatic feedback for highquality unit test generation. B Steenhoek, M Tufano, N Sundaresan, A Svyatkovskiy, 2023</p>
<p>Unit test generation using generative ai : A comparative performance analysis of autogeneration tools. S Bhatia, T Gandhi, D Kumar, P Jalote, 2023</p>
<p>Generating accurate assert statements for unit test cases using pretrained transformers. M Tufano, D Drain, A Svyatkovskiy, N Sundaresan, Proceedings of the 3rd ACM/IEEE International Conference on Automation of Software Test. the 3rd ACM/IEEE International Conference on Automation of Software Test2022</p>
<p>Learning deep semantics for test completion. P Nie, R Banerjee, J J Li, R J Mooney, M Gligoric, arXiv:2302.101662023arXiv preprint</p>
<p>Using transfer learning for code-related tasks. A Mastropaolo, N Cooper, D Nader-Palacio, S Scalabrino, D Poshyvanyk, R Oliveto, G Bavota, 10.1109/TSE.2022.3183297IEEE Trans. Software Eng. 4942023</p>
<p>Retrieval-based prompt selection for code-related few-shot learning. N Nashid, M Sintaha, A Mesbah, Proceedings of the 45th International Conference on Software Engineering (ICSE'23). the 45th International Conference on Software Engineering (ICSE'23)2023</p>
<p>Automated conformance testing for javascript engines via deep compiler fuzzing. G Ye, Z Tang, S H Tan, S Huang, D Fang, X Sun, L Bian, H Wang, Z Wang, Proceedings of the 42nd ACM SIGPLAN international conference on programming language design and implementation. the 42nd ACM SIGPLAN international conference on programming language design and implementation2021</p>
<p>Fill in the blank: Context-aware automated text input generation for mobile gui testing. Z Liu, C Chen, J Wang, X Che, Y Huang, J Hu, Q Wang, arXiv:2212.047322022arXiv preprint</p>
<p>Large language models are pretty good zero-shot video game bug detectors. M R Taesiri, F Macklon, Y Wang, H Shen, C.-P Bezemer, arXiv:2210.025062022arXiv preprint</p>
<p>Slgpt: using transfer learning to directly generate simulink model files and find bugs in the simulink toolchain. S L Shrestha, C Csallner, Evaluation and Assessment in Software Engineering. 2021</p>
<p>Augmenting greybox fuzzing with generative AI. J Hu, Q Zhang, H Yin, 10.48550/arXiv.2306.06782abs/2306.06782CoRR. 2023</p>
<p>Automated test case generation using t5 and gpt-3. A Mathur, S Pradhan, P Soni, D Patel, R Regunathan, 2023 9th International Conference on Advanced Computing and Communication Systems (ICACCS). 20231</p>
<p>Automating gui-based software testing with gpt-3. D Zimmermann, A Koziolek, 2023 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW). 2023</p>
<p>Axnav: Replaying accessibility tests from natural language. M Taeb, A Swearngin, E Schoop, R Cheng, Y Jiang, J Nichols, 10.48550/arXiv.2310.02424abs/2310.02424CoRR. 2023</p>
<p>Can chatgpt advance software testing intelligence? an experience report on metamorphic testing. Q Luu, H Liu, T Y Chen, 10.48550/arXiv.2310.19204CoRR. 2310.19204, 2023</p>
<p>Efficient mutation testing via pre-trained language models. A Khanfir, R Degiovanni, M Papadakis, Y L Traon, arXiv:2301.035432023arXiv preprint</p>
<p>Large language models are edge-case fuzzers: Testing deep learning libraries via fuzzgpt. Y Deng, C S Xia, C Yang, S D Zhang, S Yang, L Zhang, arXiv:2304.020142023arXiv preprint</p>
<p>Large language models are zero shot fuzzers: Fuzzing deep learning libraries via large language models. arXiv:2209.115152023arXiv preprint</p>
<p>Large language models for fuzzing parsers (registered report). J Ackerman, G Cybenko, 10.1145/3605157.3605173Proceedings of the 2nd International Fuzzing Workshop, FUZZING 2023. M Öhme, Y Noller, B Ray, L Szekeres, the 2nd International Fuzzing Workshop, FUZZING 2023Seattle, WA, USAACM17 July 2023. 2023</p>
<p>LLM for test script generation and migration: Challenges, capabilities, and opportunities. S Yu, C Fang, Y Ling, C Wu, Z Chen, 10.48550/arXiv.2309.13574abs/2309.13574CoRR. 2023</p>
<p>Pentestgpt: An llm-empowered automatic penetration testing tool. G Deng, Y Liu, V M Vilches, P Liu, Y Li, Y Xu, T Zhang, Y Liu, M Pinzger, S Rass, 10.48550/arXiv.2308.06782abs/2308.06782CoRR. 2023</p>
<p>SMT solver validation empowered by large pre-trained language models. M Sun, Y Yang, Y Wang, M Wen, H Jia, Y Zhou, 10.1109/ASE56229.2023.0018038th IEEE/ACM International Conference on Automated Software Engineering, ASE 2023. LuxembourgSeptember 11-15, 2023. IEEE, 2023</p>
<p>Target: Automated scenario generation from traffic rules for testing autonomous vehicles. Y Deng, J Yao, Z Tu, X Zheng, M Zhang, T Zhang, 2023258588387</p>
<p>Testing the limits: Unusual text inputs generation for mobile app crash detection with large language model. Z Liu, C Chen, J Wang, M Chen, B Wu, X Che, D Wang, Q Wang, 10.48550/arXiv.2310.15657CoRR. 2310.15657, 2023</p>
<p>Understanding large language model based fuzz driver generation. C Zhang, M Bai, Y Zheng, Y Li, X Xie, Y Li, W Ma, L Sun, Y Liu, 10.48550/arXiv.2307.12469CoRR. 2307.12469, 2023</p>
<p>Universal fuzzing via large language models. C Xia, M Paltenghi, J Tian, M Pradel, L Zhang, abs/2308.04748ArXiv. 2607355982023</p>
<p>Variable discovery with large language models for metamorphic testing of scientific software. C Tsigkanos, P Rani, S , T Kehrer, 10.1007/978-3-031-35995-8_23Computational Science -ICCS 2023 -23rd International Conference. J Mikyska, C De Mulatier, M Paszynski, V V Krzhizhanovskaya, J J Dongarra, P M A Sloot, Prague, Czech RepublicSpringerJuly 3-5, 2023. 202314073Proceedings, Part I, ser. Lecture Notes in Computer Science</p>
<p>White-box compiler fuzzing empowered by large language models. C Yang, Y Deng, R Lu, J Yao, J Liu, R Jabbarvand, L Zhang, 2310.15991, 2023CoRR</p>
<p>. 10.48550/arXiv.2310.15991</p>
<p>itiger: an automatic issue title generation tool. T Zhang, I C Irsan, F Thung, D Han, D Lo, L Jiang, Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering. the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering2022</p>
<p>Crashtranslator: Automatically reproducing mobile application crashes directly from stack trace. Y Huang, J Wang, Z Liu, Y Wang, S Wang, C Chen, Y Hu, Q Wang, 10.48550/arXiv.2310.07128abs/2310.07128CoRR. 2023</p>
<p>Cupid: Leveraging chatgpt for more accurate duplicate bug report detection. T Zhang, I C Irsan, F Thung, D Lo, 10.48550/arXiv.2308.10022CoRR. 2308.10022, 2023</p>
<p>Employing deep learning and structured information retrieval to answer clarification questions on bug reports. U Mukherjee, M M Rahman, 2023259501524</p>
<p>Explaining software bugs leveraging code structures in neural machine translation. P Mahbub, O Shuvo, M M Rahman, arXiv:2212.045842022arXiv preprint</p>
<p>Prompting is all your need: Automated android bug replay with large language models. S Feng, C Chen, 10.48550/arXiv.2306.01987CoRR. 2306.01987, 2023</p>
<p>Still confusing for bug-component triaging? deep feature learning and ensemble setting to rescue. Y Su, Z Han, Z Gao, Z Xing, Q Lu, X Xu, 10.1109/ICPC58990.2023.0004631st IEEE/ACM International Conference on Program Comprehension, ICPC 2023. Melbourne, AustraliaMay 15-16, 2023. IEEE, 2023</p>
<p>Detect-localize-repair: A unified framework for learning to debug with codet5. N D Bui, Y Wang, S Hoi, arXiv:2211.148752022arXiv preprint</p>
<p>Large language models are few-shot testers: Exploring llm-based general bug reproduction. S Kang, J Yoon, S Yoo, arXiv:2209.115152022arXiv preprint</p>
<p>A preliminary evaluation of llm-based fault localization. S Kang, G An, S Yoo, 10.48550/arXiv.2308.05487abs/2308.05487CoRR. 2023</p>
<p>Addressing compiler errors: Stack overflow or large language models. P Widjojo, C Treude, 10.48550/arXiv.2307.10793CoRR. 2307.10793, 2023</p>
<p>Can llms demystify bug reports. L Plein, T F Bissyandé, 10.48550/arXiv.2310.06310abs/2310.06310CoRR. 2023</p>
<p>Dcc -help: Generating context-aware compiler error explanations with large language models. A Taylor, A Vassar, J Renzella, H A Pearce, 2023261076439</p>
<p>Explainable automated debugging via large language model-driven scientific debugging. S Kang, B Chen, S Yoo, J.-G Lou, arXiv:2304.021952023arXiv preprint</p>
<p>Large language models for test-free fault localization. A Z H Yang, R Martins, C L Goues, V J Hellendoorn, abs/2310.01726CoRR. 2023</p>
<p>. 10.48550/arXiv.2310.01726</p>
<p>Large language models in fault localisation. Y Wu, Z Li, J M Zhang, M Papadakis, M Harman, Y Liu, 10.48550/arXiv.2308.15276CoRR. 2308.15276, 2023</p>
<p>LLM4CBI: taming llms to generate effective test programs for compiler bug isolation. H Tu, Z Zhou, H Jiang, I N B Yusuf, Y Li, L Jiang, 10.48550/arXiv.2307.00593abs/2307.00593CoRR. 2023</p>
<p>Nuances are the key: Unlocking chatgpt to find failure-inducing tests with differential prompting. T.-O Li, W Zong, Y Wang, H Tian, Y Wang, S.-C Cheung, J Kramer, 2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE). 2023</p>
<p>Teaching large language models to self-debug. X Chen, M Lin, N Schärli, D Zhou, 10.48550/arXiv.2304.05128abs/2304.05128CoRR. 2023</p>
<p>A study on prompt design, advantages and limitations of chatgpt for deep learning program repair. J Cao, M Li, M Wen, S.-C Cheung, arXiv:2304.081912023arXiv preprint</p>
<p>Examining zero-shot vulnerability repair with large language models. H Pearce, B Tan, B Ahmad, R Karri, B Dolan-Gavitt, 2023 IEEE Symposium on Security and Privacy (SP). IEEE Computer Society2022</p>
<p>Automated repair of programs from large language models. Z Fan, X Gao, A Roychoudhury, S H Tan, arXiv:2205.105832022arXiv preprint</p>
<p>Fix bugs with transformer through a neural-symbolic edit grammar. Y Hu, X Shi, Q Zhou, L Pike, arXiv:2204.066432022arXiv preprint</p>
<p>Practical program repair in the era of large pre-trained language models. C S Xia, Y Wei, L Zhang, arXiv:2210.141792022arXiv preprint</p>
<p>Repairing bugs in python assignments using large language models. J Zhang, J Cambronero, S Gulwani, V Le, R Piskac, G Soares, G Verbruggen, arXiv:2209.148762022arXiv preprint</p>
<p>Towards javascript program repair with generative pre-trained transformer (gpt-2). M Lajk Ó, V Csuvik, L Vidács, Proceedings of the Third International Workshop on Automated Program Repair. the Third International Workshop on Automated Program Repair2022</p>
<p>An analysis of the automatic bug fixing performance of chatgpt. D Sobania, M Briesch, C Hanna, J Petke, arXiv:2301.086532023arXiv preprint</p>
<p>An empirical study on fine-tuning large language models of code for automated program repair. K Huang, X Meng, J Zhang, Y Liu, W Wang, S Li, Y Zhang, 10.1109/ASE56229.2023.0018138th IEEE/ACM International Conference on Automated Software Engineering, ASE 2023. LuxembourgSeptember 11-15, 2023. IEEE, 2023</p>
<p>An evaluation of the effectiveness of openai's chatgpt for automated python program bug fixing using quixbugs. M C Wuisang, M Kurniawan, K A Wira Santosa, A Agung Santoso Gunawan, K E Saputra, 2023 International Seminar on Application for Technology of Information and Communication. 2023</p>
<p>An extensive study on model architecture and program representation in the domain of learning-based automated program repair. D Horváth, V Csuvik, T Gyim, L Vidács, 10.1109/APR59189.2023.00013IEEE/ACM International Workshop on Automated Program Repair, APR@ICSE 2023. Melbourne, AustraliaMay 16, 2023. IEEE, 2023</p>
<p>Can openai's codex fix bugs? an evaluation on quixbugs. J A Prenner, H Babii, R Robbes, Proceedings of the Third International Workshop on Automated Program Repair. the Third International Workshop on Automated Program Repair2022</p>
<p>Circle: continual repair across programming languages. W Yuan, Q Zhang, T He, C Fang, N Q V Hung, X Hao, H Yin, Proceedings of the 31st ACM SIGSOFT International Symposium on Software Testing and Analysis. the 31st ACM SIGSOFT International Symposium on Software Testing and Analysis2022</p>
<p>Coffee: Boost your code llms by fixing bugs with feedback. S Moon, Y Song, H Chae, D Kang, T Kwon, K T Ong, S Hwang, J Yeo, 2023</p>
<p>Copiloting the copilots: Fusing large language models with completion engines for automated program repair. Y Wei, C S Xia, L Zhang, 10.1145/3611643.3616271Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering, ESEC/FSE 2023. K Chandra, P Blincoe, Tonella, the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering, ESEC/FSE 2023San Francisco, CA, USAACMDecember 3-9, 2023. 2023</p>
<p>Domain knowledge matters: Improving prompts with fix templates for repairing python type errors. Y Peng, S Gao, C Gao, Y Huo, M R Lyu, 10.48550/arXiv.2306.01394CoRR. 2306.01394, 2023</p>
<p>Enhancing genetic improvement mutations using large language models. A E I Brownlee, J Callan, K Even-Mendoza, A Geiger, C Hanna, J Petke, F Sarro, D Sobania, 10.1007/978-3-031-48796-5_13Search-Based Software Engineering -15th International Symposium, SSBSE 2023. P Arcaini, T Yue, E M Fredericks, San Francisco, CA, USASpringerDecember 8, 2023. 202314415Proceedings, ser. Lecture Notes in Computer Science</p>
<p>Fixeval: Execution-based evaluation of program fixes for programming problems. M M A Haque, W U Ahmad, I Lourentzou, C Brown, 10.1109/APR59189.2023.00009APR@ICSE 2023. Melbourne, AustraliaMay 16, 2023. IEEE, 2023</p>
<p>Fixing hardware security bugs with large language models. B Ahmad, S Thakur, B Tan, R Karri, H Pearce, arXiv:2302.012152023arXiv preprint</p>
<p>Fixing rust compilation errors using llms. P Deligiannis, A Lal, N Mehrotra, A Rastogi, 10.48550/arXiv.2308.05177abs/2308.05177CoRR. 2023</p>
<p>Framing program repair as code completion. F Ribeiro, R Abreu, J Saraiva, Proceedings of the Third International Workshop on Automated Program Repair. the Third International Workshop on Automated Program Repair2022</p>
<p>Frustrated with code quality issues? llms can help. N Wadhwa, J Pradhan, A Sonwane, S P Sahu, N Natarajan, A Kanade, S Parthasarathy, S K Rajamani, 10.48550/arXiv.2309.12938abs/2309.129382023</p>
<p>Gpt-3-powered type error debugging: Investigating the use of large language models for code repair. F Ribeiro, J N C De Macedo, K Tsushima, R Abreu, J Saraiva, 10.1145/3623476.3623522Proceedings of the 16th ACM SIGPLAN International Conference on Software Language Engineering, SLE 2023. J Saraiva, T Degueule, E Scott, the 16th ACM SIGPLAN International Conference on Software Language Engineering, SLE 2023Cascais, PortugalACMOctober 23-24, 2023. 2023</p>
<p>How effective are neural networks for fixing security vulnerabilities. Y Wu, N Jiang, H V Pham, T Lutellier, J Davis, L Tan, P Babkin, S Shah, arXiv:2305.186072023arXiv preprint</p>
<p>Impact of code language models on automated program repair. N Jiang, K Liu, T Lutellier, L Tan, arXiv:2302.050202023arXiv preprint</p>
<p>Inferfix: End-to-end program repair with llms. M Jin, S Shahriar, M Tufano, X Shi, S Lu, N Sundaresan, A Svyatkovskiy, arXiv:2303.072632023arXiv preprint</p>
<p>Keep the conversation going: Fixing 162 out of 337 bugs for $0.42 each using chatgpt. C S Xia, L Zhang, arXiv:2304.003852023arXiv preprint</p>
<p>Neural program repair with program dependence analysis and effective filter mechanism. Y Zhang, G Li, Z Jin, Y Xing, arXiv:2305.093152023arXiv preprint</p>
<p>Out of context: How important is local context in neural program repair. J A Prenner, R Robbes, 2023</p>
<p>Pre-trained model-based automated software vulnerability repair: How far are we. Q Zhang, C Fang, B Yu, W Sun, T Zhang, Z Chen, 10.48550/arXiv.2308.12533CoRR. 2308.12533, 2023</p>
<p>Rapgen: An approach for fixing code inefficiencies in zero-shot. S Garg, R Z Moghaddam, N Sundaresan, </p>
<p>. Corr, 10.48550/arXiv.2306.170772306.17077, 2023</p>
<p>Rapgen: Retrieval-augmented patch generation with codet5 for automatic program repair. W Wang, Y Wang, S Joty, S C H Hoi, 10.1145/3611643.3616256Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering, ESEC/FSE 2023. K Chandra, P Blincoe, Tonella, the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering, ESEC/FSE 2023San Francisco, CA, USAACMDecember 3-9, 2023. 2023</p>
<p>STEAM: simulating the interactive behavior of programmers for automatic bug fixing. Y Zhang, Z Jin, Y Xing, G Li, 10.48550/arXiv.2308.14460CoRR. 2308.14460, 2023</p>
<p>Towards generating functionally correct code edits from natural language issue descriptions. S Fakhoury, S Chakraborty, M Musuvathi, S K Lahiri, arXiv:2304.038162023arXiv preprint</p>
<p>Vulrepair: a t5-based automated software vulnerability repair. M Fu, C Tantithamthavorn, T Le, V Nguyen, D Phung, Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering. the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering2022</p>
<p>What makes good in-context demonstrations for code intelligence tasks with llms?. S Gao, X Wen, C Gao, W Wang, H Zhang, M R Lyu, 10.1109/ASE56229.2023.0010938th IEEE/ACM International Conference on Automated Software Engineering, ASE 2023. LuxembourgSeptember 11-15, 2023. IEEE, 2023</p>
<p>She elicits requirements and he tests: Software engineering gender bias in large language models. C Treude, H Hata, 10.48550/arXiv.2303.10131CoRR. 2303.10131, 2023</p>
<p>Autobiastest: Controllable sentence generation for automated and open-ended social bias testing in language models. R Kocielnik, S Prabhumoye, V Zhang, R M Alvarez, A Anandkumar, 10.48550/arXiv.2302.07371abs/2302.07371CoRR. 2023</p>
<p>To what extent do deep learning-based code recommenders generate predictions by cloning code from the training set?. M Ciniselli, L Pascarella, G Bavota, 10.1145/3524842.352844019th IEEE/ACM International Conference on Mining Software Repositories, MSR 2022. Pittsburgh, PA, USAACMMay 23-24, 2022. 2022</p>
<p>Measuring the runtime performance of code produced with github copilot. D Erhabor, S Udayashankar, M Nagappan, S Al-Kiswany, abs/2305.06439CoRR. 2023</p>
<p>. 10.48550/arXiv.2305.06439</p>
<p>Investigating and designing for trust in ai-powered code generation tools. R Wang, R Cheng, D Ford, T Zimmermann, 10.48550/arXiv.2305.11248CoRR. 2305.11248, 2023</p>
<p>Evaluating the code quality of ai-assisted code generation tools: An empirical study on github copilot, amazon codewhisperer, and chatgpt. B Yetistiren, I Özsoy, M Ayerdem, E Üz Ün, 10.48550/arXiv.2304.10778CoRR. 2304.10778, 2023</p>
<p>Guidelines for snowballing in systematic literature studies and a replication in software engineering. C Wohlin, 10.1145/2601248.260126838:1018th International Conference on Evaluation and Assessment in Software Engineering, EASE '14. M J Shepperd, T Hall, I Myrtveit, London, England, United KingdomACMMay 13-14, 2014. 2014381</p>
<p>Studying the usage of text-to-text transfer transformer to support code-related tasks. A Mastropaolo, S Scalabrino, N Cooper, D Nader-Palacio, D Poshyvanyk, R Oliveto, G Bavota, 43rd IEEE/ACM International Conference on Software Engineering, ICSE 2021. Madrid, SpainIEEEMay 2021. 2021</p>
<p>Large language models: The next frontier for variable discovery within metamorphic testing?. C Tsigkanos, P Rani, S , T Kehrer, 10.1109/SANER56733.2023.00070IEEE International Conference on Software Analysis, Evolution and Reengineering, SANER 2023. T Zhang, X Xia, N Novielli, Taipa, MacaoIEEEMarch 21-24, 2023. 2023</p>
<p>G J Myers, The art of software testing. Wiley2004</p>
<p>Manage software testing. P Farrell-Vinay, 2008Auerbach Publ</p>
<p>A Mili, F Tchier, Software testing: Concepts and operations. John Wiley &amp; Sons2015</p>
<p>Pynguin: Automated unit test generation for python. S Lukasczyk, G Fraser, 10.1145/3510454.351682944th IEEE/ACM International Conference on Software Engineering: Companion Proceedings, ICSE Companion 2022. Pittsburgh, PA, USAACM/IEEEMay 22-24, 2022. 2022</p>
<p>The oracle problem in software testing: A survey. E T Barr, M Harman, P Mcminn, M Shahbaz, S Yoo, IEEE transactions on software engineering. 4152014</p>
<p>On learning meaningful assert statements for unit test cases. C Watson, M Tufano, K Moran, G Bavota, D Poshyvanyk, ICSE '20: 42nd International Conference on Software Engineering. D Rothermel, Bae, Seoul, South KoreaACM27 June -19 July, 2020. 2020</p>
<p>Textexerciser: Feedback-driven text input exercising for android applications. Y He, L Zhang, Z Yang, Y Cao, K Lian, S Li, W Yang, Z Zhang, M Yang, Y Zhang, H Duan, 2020 IEEE Symposium on Security and Privacy. San Francisco, CA, USAIEEEMay 18-21, 2020. 20202020</p>
<p>Free lunch for testing: Fuzzing deep-learning libraries from open source. A Wei, Y Deng, C Yang, L Zhang, 44th IEEE/ACM 44th International Conference on Software Engineering, ICSE 2022. Pittsburgh, PA, USAACMMay 25-27, 2022. 2022</p>
<p>Docter: documentation-guided fuzzing for testing deep learning API functions. D Xie, Y Li, M Kim, H V Pham, L Tan, X Zhang, M W Godfrey, ISSTA '22: 31st ACM SIGSOFT International Symposium on Software Testing and Analysis, Virtual Event, South Korea. S Ryu, Y Smaragdakis, ACMJuly 18 -22, 2022. 2022</p>
<p>Audee: Automated testing for deep learning frameworks. Q Guo, X Xie, Y Li, X Zhang, Y Liu, X Li, C Shen, 35th IEEE/ACM International Conference on Automated Software Engineering, ASE 2020. Melbourne, AustraliaIEEESeptember 21-25, 2020. 2020</p>
<p>Deep learning library testing via effective model generation. Z Wang, M Yan, J Chen, S Liu, D Zhang, ESEC/FSE '20: 28th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering. P Devanbu, M B Cohen, T Zimmermann, ACMNovember 8-13, 2020. 2020Virtual Event, USA</p>
<p>Shaping program repair space with existing patches and similar code. J Jiang, Y Xiong, H Zhang, Q Gao, X Chen, 10.1145/3213846.3213871Proceedings of the 27th ACM SIGSOFT International Symposium on Software Testing and Analysis, ser. ISSTA. the 27th ACM SIGSOFT International Symposium on Software Testing and Analysis, ser. ISSTANew York, NY, USAAssociation for Computing Machinery2018. 2018</p>
<p>Contextaware patch generation for better automated program repair. M Wen, J Chen, R Wu, D Hao, S.-C Cheung, 10.1145/3180155.3180233Proceedings of the 40th International Conference on Software Engineering, ser. ICSE '18. the 40th International Conference on Software Engineering, ser. ICSE '18New York, NY, USAAssociation for Computing Machinery2018</p>
<p>Precise condition synthesis for program repair. Y Xiong, J Wang, R Yan, J Zhang, S Han, G Huang, L Zhang, 2017 IEEE/ACM 39th International Conference on Software Engineering (ICSE. 2017</p>
<p>Nopol: Automatic repair of conditional statement bugs in java programs. J Xuan, M Martinez, F Demarco, M Clément, S L Marcote, T Durieux, D Le Berre, M Monperrus, IEEE Transactions on Software Engineering. 4312017</p>
<p>How to bridge the gap between modalities: A comprehensive survey on multimodal large language model. S Song, X Li, S Li, abs/2311.07594CoRR. 2023</p>
<p>Machine learning testing: Survey, landscapes and horizons. J M Zhang, M Harman, L Ma, Y Liu, IEEE Trans. Software Eng. 4822022</p>
<p>Be careful of when: an empirical study on time-related misuse of issue tracking data. F Tu, J Zhu, Q Zheng, M Zhou, Proceedings of the 2018 ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, ESEC/SIGSOFT FSE 2018. G T Leavens, A Garcia, C S Pasareanu, the 2018 ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, ESEC/SIGSOFT FSE 2018Buena Vista, FL, USAACMNovember 04-09. 2018. 2018</p>
<p>. 10.1145/3236024.3236054</p>
<p>On the importance of building high-quality training datasets for neural code search. Z Sun, L Li, Y Liu, X Du, L Li, 10.1145/3510003.351016044th IEEE/ACM 44th International Conference on Software Engineering, ICSE 2022. Pittsburgh, PA, USAACMMay 25-27, 2022. 2022</p>
<p>ISPY: automatic issue-solution pair extraction from community live chats. L Shi, Z Jiang, Y Yang, X Chen, Y Zhang, F Mu, H Jiang, Q Wang, 10.1109/ASE51524.2021.967889436th IEEE/ACM International Conference on Automated Software Engineering, ASE 2021. Melbourne, AustraliaNovember 15-19, 2021. IEEE, 2021</p>
<p>Graphcodebert: Pre-training code representations with data flow. D Guo, S Ren, S Lu, Z Feng, D Tang, S Liu, L Zhou, N Duan, A Svyatkovskiy, S Fu, M Tufano, S K Deng, C B Clement, D Drain, N Sundaresan, J Yin, D Jiang, M Zhou, 9th International Conference on Learning Representations, ICLR 2021, Virtual Event. AustriaMay 3-7, 2021. OpenReview.net, 2021</p>
<p>Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop. F Yu, A Seff, Y Zhang, S Song, T Funkhouser, J Xiao, arXiv:1506.033652015arXiv preprint</p>
<p>. Inc Loadrunner, Loadrunner," 2023, microfocus.com</p>
<p>. Inc Langchain, Langchain, 2023</p>
<p>Prompt engineering guide. 2023Prompt engineering</p>
<p>Multimodal chain-of-thought reasoning in language models. Z Zhang, A Zhang, M Li, H Zhao, G Karypis, A Smola, abs/2302.00923CoRR. 2023</p>
<p>Graphprompt: Unifying pre-training and downstream tasks for graph neural networks. Z Liu, X Yu, Y Fang, X Zhang, Proceedings of the ACM Web Conference 2023, WWW 2023. J Ding, J F Tang, L Sequeda, C Aroyo, G Castillo, Houben, the ACM Web Conference 2023, WWW 2023Austin, TX, USAACM30 April 2023 -4 May 2023. 2023</p>
<p>A new era in software security: Towards selfhealing software via large language models and formal verification. Y Charalambous, N Tihanyi, R Jain, Y Sun, M A Ferrag, L C Cordeiro, 2023</p>
<p>Machine/deep learning for software engineering: A systematic literature review. S Wang, L Huang, A Gao, J Ge, T Zhang, H Feng, I Satyarth, M Li, H Zhang, V Ng, 10.1109/TSE.2022.3173346IEEE Trans. Software Eng. 4932023</p>
<p>A survey on deep learning for software engineering. Y Yang, X Xia, D Lo, J C Grundy, 10.1145/3505243ACM Comput. Surv. 5410s2022</p>
<p>A systematic literature review on the use of deep learning in software engineering research. C Watson, N Cooper, D Nader-Palacio, K Moran, D Poshyvanyk, 10.1145/3485275ACM Trans. Softw. Eng. Methodol. 3122022</p>
<p>A survey on the use of computer vision to improve software engineering tasks. M Bajammal, A Stocco, D Mazinanian, A Mesbah, 10.1109/TSE.2020.3032986IEEE Trans. Software Eng. 4852022</p>
<p>Large language models for software engineering: A systematic literature review. X Hou, Y Zhao, Y Liu, Z Yang, K Wang, L Li, X Luo, D Lo, J C Grundy, H Wang, 10.48550/arXiv.2308.10620CoRR. 2308.10620, 2023</p>
<p>Large language models for software engineering: Survey and open problems. A Fan, B Gokkaya, M Harman, M Lyubarskiy, S Sengupta, S Yoo, J M Zhang, 10.48550/arXiv.2310.03533abs/2310.03533CoRR. 2023</p>
<p>Large language models meet nl2code: A survey. D Zan, B Chen, F Zhang, D Lu, B Wu, B Guan, Y Wang, J Lou, 10.18653/v1/2023.acl-long.411Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. A Rogers, J L Boyd-Graber, N Okazaki, the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational LinguisticsJuly 9-14, 2023. 20231ACL 2023</p>            </div>
        </div>

    </div>
</body>
</html>
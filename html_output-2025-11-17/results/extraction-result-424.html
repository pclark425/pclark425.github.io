<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-424 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-424</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-424</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-18.html">extraction-schema-18</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <p><strong>Paper ID:</strong> paper-274859617</p>
                <p><strong>Paper Title:</strong> Is This You, LLM? Recognizing AI-written Programs with Multilingual Code Stylometry</p>
                <p><strong>Paper Abstract:</strong> With the increasing popularity of LLM-based code completers, like GitHub Copilot, the interest in automatically detecting AI-generated code is also increasing-in particular in contexts where the use of LLMs to program is forbidden by policy due to security, intellectual property, or ethical concerns. We introduce a novel technique for AI code stylometry, i.e., the ability to distinguish code generated by LLMs from code written by humans, based on a transformer-based encoder classifier. Differently from previous work, our classifier is capable of detecting AI-written code across 10 different programming languages with a single machine learning model, maintaining high average accuracy across all languages (84.1% ± 3.8%). Together with the classifier we also release H-AIRosettaMP, a novel open dataset for AI code stylometry tasks, consisting of 121 247 code snippets in 10 popular programming languages, labeled as either human-written or AI-generated. The experimental pipeline (dataset, training code, resulting models) is the first fully reproducible one for the AI code stylometry task. Most notably our experiments rely only on open LLMs, rather than on proprietary/closed ones like ChatGPT.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e424.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e424.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>reproducibility_underreporting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reproducibility issues due to underreported experimental details</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The authors report that prior studies underreport implementation and data-generation details needed to fully replicate experiments, causing mismatches between paper descriptions and actual code/data artifacts and impairing replication of results.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AI code stylometry experimental pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>End-to-end experimental pipeline for generating datasets of human vs AI-written code (H-AIRosettaMP), training transformer-based classifiers, and evaluating detection accuracy across languages and provenances.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>research paper methods section / dataset & pipeline description</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>dataset generation scripts, training scripts, evaluation code</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>omitted implementation & data-generation details (incomplete specification)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Previous works often omitted artifacts and low-level implementation choices (e.g., exact prompts, model versions, data selection and filtering heuristics), so reimplementations had to reconstruct missing pieces; this produces observable discrepancies between the described methodology and the reimplemented code/pipeline, and can yield different empirical results.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>dataset construction and experimental pipeline (data generation, prompt details, model/version choices, preprocessing)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>reproducibility study / reimplementation and direct comparison of results (authors attempted to reimplement baseline methods on their dataset and compared accuracies)</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>comparison of reproduced baseline accuracies vs reported baseline accuracies and statistical tests (reported absolute accuracy gaps and t-tests / ANOVA); authors report numerical gaps when reimplementing baselines on their dataset</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Substantial — reimplemented baselines performed worse than originally reported: negative gaps of -18.5% (Java), -3.5% (C++), and -5.2% (Python) compared to original reports; authors attribute these differences partly to dataset differences and missing artifacts, affecting ability to compare methods fairly.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Described as common in empirical software engineering and observed across the compared prior works; in this study the authors encountered missing artifacts when attempting to replicate several prior works (no per-paper percentage provided).</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>under-specified natural language descriptions in papers (omitted low-level details), closed/undisclosed artifacts (proprietary LLMs, dataset curation choices), and implicit assumptions not written in the methods text.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Provide a fully open, reproducible pipeline: publish dataset (H-AIRosettaMP), release training/evaluation code, release model checkpoints, use open-weight LLMs (StarCoder2) and document prompts and preprocessing choices.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Effective in this study: authors produce a fully reproducible benchmark and models; quantifiably, their open pipeline enabled consistent cross-language experiments and direct re-training of baselines on the same data, though it does not retroactively fix closed prior work results.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning / empirical software engineering / AI code stylometry</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Is This You, LLM? Recognizing AI-written Programs with Multilingual Code Stylometry', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e424.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e424.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>provenance_mismatch</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Provenance (source-language) mismatch effect</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>AI-generated snippets' source (provenance) language used in code-translation data generation significantly affects classifier accuracy, causing large gaps when train/test provenances differ.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>dataset generation via cross-language code translation and classifier training/evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>H-AIRosettaMP dataset where AI-written examples are produced by translating Rosetta Code snippets from a provenance/source language (src) to a target language (dst) using StarCoder2; monolingual classifiers are trained on dst data with a fixed provenance distribution and tested on dst snippets coming from different provenances.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>dataset generation description / provenance specification</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>code translation scripts and model training/evaluation scripts</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>dataset provenance mismatch / out-of-distribution generation (different input distribution than described or assumed)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>When classifiers are trained and tested on datasets where AI examples come from one provenance language (e.g., Kotlin) but are evaluated on data where AI examples were translated from different provenance languages, performance degrades considerably — i.e., a mismatch between the assumed provenance distribution in the natural-language dataset description and the actual provenance of test examples leads to misaligned results.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>data generation / training vs test data distribution (dataset provenance)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>empirical evaluation across 90 {src,dst} sub-datasets: authors trained monolingual models per dst with different src provenances and measured accuracy differences; statistical analysis (t-tests, ANOVA) confirmed significance.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Measured as change in classification accuracy; reported average loss of -17.93% in accuracy when using different provenance at test time (with t-statistic = 8.82, 95% CI 12.4 to 21.3, p < 0.001). Table II and Figure 6 detail per-language provenance accuracies.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Large — average accuracy loss of ~17.9% when provenance differs; this materially alters conclusions about detector performance and indicates strong sensitivity to provenance.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Observed consistently across the 90 sub-datasets and multiple languages; authors treat it as a pervasive phenomenon in their translation-based data-generation methodology.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>data-generation method (code translation) introduces provenance-dependent artifacts and distributional shifts; classifiers learn provenance-specific signals rather than generalizable author-style features.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Train a multilingual classifier on a dataset sampled to include uniform provenance distribution (multi-provenance training), and apply preprocessing safeguards (e.g., remove trivial artifacts like leading/trailing spaces).</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Partially effective: multilingual model achieves consistent performance across languages (average accuracy 84.1% ± 3.8) and reduces extreme outliers, but still underperforms the best monolingual models by an average of -5.17% (t-test avg. loss 2.7 to 8.7, p < 0.001).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning / dataset curation / AI code stylometry</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Is This You, LLM? Recognizing AI-written Programs with Multilingual Code Stylometry', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e424.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e424.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>generation_mode_mismatch</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mismatch between dataset generation modes (code translation vs natural-language prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Different data-generation modes (translating existing code vs generating code from natural-language task descriptions) yield qualitatively different AI outputs, causing classifiers trained on one mode to perform differently on data from another mode.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AI-generated code dataset creation procedures</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Two alternative ways to generate AI-written code: (1) code translation (provide code in src language, ask LLM to translate to dst), and (2) natural-language prompting (provide task description and request solution).</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>dataset generation method description / prompt specification</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>prompt templates and generative-model invocation code</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>different generation variant (mode mismatch) / incomplete mapping between described generation mode and real-world generation modes</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>The paper explains that translation-based generation was chosen because natural-language prompting or function-driven prompting often produced incomplete 'skeleton' code; classifiers trained on translation-generated snippets may not generalize to examples produced by natural-language prompts (e.g., ChatGPT outputs), leading to performance gaps between description of dataset generation and the variety of real-world generation modes.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>data-generation method / upstream prompt engineering</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Comparative evaluation: authors tested their classifier (trained on translation-generated data) on an external dataset (Oedingen et al.) created via natural-language prompts with ChatGPT and measured drop in accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Measured accuracy drop: multilingual classifier achieved 72.8% accuracy on Oedingen et al.'s ChatGPT-generated Python dataset, representing a -6.3% drop relative to their in-dataset result; relative to Oedingen et al.'s reported results the gap was larger (-25.2%).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Moderate-to-significant — classifiers trained on one generation mode (translation) see lower accuracy on data generated by another mode (natural-language prompting), which affects external validity of conclusions and practical deployment.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Observed at least in cross-dataset evaluation (this paper vs Oedingen et al.); authors treat this as a limitation and note it is a shared threat across related work.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>generation-mode dependent output distributions (LLMs produce different code when translating vs when generating from description); lack of coverage of generation modes in dataset descriptions and training.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Propose future work to analyze multiple generation modes (prompt engineering, different LLMs, provenance languages) and encourage datasets that include mixed-generation modes; in this study they partially address it by training a multilingual, multi-provenance model and by testing on external datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Partial: multilingual model was still able to detect ChatGPT-generated code above chance (72.8%) without fine-tuning, but performance was reduced compared to in-domain results; authors quantify the drop (-6.3% vs their dataset, -25.2% vs Oedingen reported).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning / dataset generation / empirical evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Is This You, LLM? Recognizing AI-written Programs with Multilingual Code Stylometry', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e424.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e424.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>translation_artifacts</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Translation-induced artifacts (e.g., leading/trailing spaces and length differences)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Code produced by automated translation (StarCoder2) exhibits systematic low-level artifacts (spacing patterns, differing snippet lengths) that can be exploited by detectors, causing mismatches between the claimed balanced dataset and actual detectable differences.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>code translation-based dataset construction</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Generating AI-written code by asking a code LLM to translate human-authored code from a source language into a target language, producing translated snippets that sometimes include consistent formatting or length patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>dataset construction description / expected data equivalence claim</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>translation prompt templates and post-processing scripts</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>formatting/artifact leakage (unintended trivial signals) / dataset imbalance in low-level features</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>AI-translated snippets showed recognizable leading/trailing spacing patterns and systematic differences in snippet length between AI and human groups for several languages. If left in the dataset, these trivial artifacts could make the classification task artificially easy, misaligning the intended evaluation (style-based detection) with the actual code inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>data preprocessing and artifact removal stages</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Data inspection and statistical testing: authors measured snippet length differences (t-tests per language) and observed systematic spacing patterns; they removed leading/trailing spaces as a corrective action.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>T-tests on snippet length between AI and human groups per language (α = 0.05); six out of ten languages had statistically significant length differences (p < 0.05) as reported in Table I. Qualitative observation of spacing patterns led to removal of leading/trailing spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Potentially large if not mitigated — such trivial artifacts could give classifiers an unfair advantage. The authors removed these artifacts to avoid bias; no explicit numeric delta provided for artifact removal effect, but presence was judged sufficient risk to preprocess them out.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Observed across translation-generated snippets in multiple languages (six languages with significant length differences; spacing artifacts observed and cleaned across the dataset).</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>systematic behavior of the generative model during translation (formatting/whitespace conventions) and differences in original human snippets' formatting conventions; omission of such cleaning in dataset descriptions allows artifact leakage.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Remove trivial artifacts (e.g., strip leading/trailing spaces) and perform balancing/undersampling so that trivial low-level signals cannot drive classification.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Not reported as a numeric improvement, but authors state they removed the artifacts specifically to avoid unfair advantage and to better match real-world conditions; dataset after cleaning used for all reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>dataset curation / machine learning</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Is This You, LLM? Recognizing AI-written Programs with Multilingual Code Stylometry', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e424.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e424.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>baseline_dataset_hardness</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dataset-dependent performance gaps when reimplementing baselines</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline methods reported in prior literature yield substantially different accuracies when reimplemented and evaluated on the new H-AIRosettaMP dataset, indicating a misalignment between prior paper-reported performance and actual performance under different dataset conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>baseline algorithm reimplementation and comparison</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Authors reimplemented previous baseline classifiers (Random Forest, J48, XGB with TF-IDF) and trained/evaluated them on H-AIRosettaMP to compare against transformer-based models.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>related-work reported performance / method description</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>reimplemented baseline classifier code</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>dataset shift / mismatch between reported baseline performance and reimplementation results</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Baselines reported high accuracies in original papers, but when reimplemented and run on the authors' dataset, they performed worse (negative accuracy gaps), suggesting that claims in prior papers may not generalize to different or harder datasets and that paper descriptions did not fully specify dataset characteristics causing the discrepancy.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>evaluation stage / cross-dataset generalization</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Empirical reimplementation and evaluation on H-AIRosettaMP; direct numeric comparison of accuracies reported in prior work vs reimplementation results.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Reported absolute accuracy gaps: -18.5% (Java baseline), -3.5% (C++ baseline), -5.2% (Python baseline) when compared to original reported results; also compared monolingual vs multilingual classifier gaps (e.g., multilingual worse than monolingual by -5.17% avg).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Substantive: shows that dataset choice and undocumented preprocessing materially affect reported performance and the fairness of comparisons; motivated authors to offer an open, harder benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Observed for multiple baselines and languages in this study; authors interpret H-AIRosettaMP as a harder benchmark than prior datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>differences in dataset difficulty, selection biases, and underreported preprocessing/data-splitting choices in original papers (natural-language descriptions not sufficiently precise about dataset characteristics).</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Provide open benchmark with detailed artifact release, re-train baselines on the same dataset to enable fair comparison, and emphasize reproducible artifact publication.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Effective for enabling fair comparisons in this work: authors re-trained baselines on H-AIRosettaMP and present direct comparisons; however, it cannot retroactively change prior published claims.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning / empirical evaluation / benchmarking</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Is This You, LLM? Recognizing AI-written Programs with Multilingual Code Stylometry', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Discriminating human-authored from chatgpt-generated code via discernable feature analysis <em>(Rating: 2)</em></li>
                <li>Chatgpt code detection: Techniques for uncovering the source of code <em>(Rating: 2)</em></li>
                <li>Detecting chatgpt-generated code in a CS1 course <em>(Rating: 2)</em></li>
                <li>Gptsniffer: A codebertbased classifier to detect source code written by chatgpt <em>(Rating: 1)</em></li>
                <li>Zero-shot detection of machinegenerated codes <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-424",
    "paper_id": "paper-274859617",
    "extraction_schema_id": "extraction-schema-18",
    "extracted_data": [
        {
            "name_short": "reproducibility_underreporting",
            "name_full": "Reproducibility issues due to underreported experimental details",
            "brief_description": "The authors report that prior studies underreport implementation and data-generation details needed to fully replicate experiments, causing mismatches between paper descriptions and actual code/data artifacts and impairing replication of results.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "AI code stylometry experimental pipeline",
            "system_description": "End-to-end experimental pipeline for generating datasets of human vs AI-written code (H-AIRosettaMP), training transformer-based classifiers, and evaluating detection accuracy across languages and provenances.",
            "nl_description_type": "research paper methods section / dataset & pipeline description",
            "code_implementation_type": "dataset generation scripts, training scripts, evaluation code",
            "gap_type": "omitted implementation & data-generation details (incomplete specification)",
            "gap_description": "Previous works often omitted artifacts and low-level implementation choices (e.g., exact prompts, model versions, data selection and filtering heuristics), so reimplementations had to reconstruct missing pieces; this produces observable discrepancies between the described methodology and the reimplemented code/pipeline, and can yield different empirical results.",
            "gap_location": "dataset construction and experimental pipeline (data generation, prompt details, model/version choices, preprocessing)",
            "detection_method": "reproducibility study / reimplementation and direct comparison of results (authors attempted to reimplement baseline methods on their dataset and compared accuracies)",
            "measurement_method": "comparison of reproduced baseline accuracies vs reported baseline accuracies and statistical tests (reported absolute accuracy gaps and t-tests / ANOVA); authors report numerical gaps when reimplementing baselines on their dataset",
            "impact_on_results": "Substantial — reimplemented baselines performed worse than originally reported: negative gaps of -18.5% (Java), -3.5% (C++), and -5.2% (Python) compared to original reports; authors attribute these differences partly to dataset differences and missing artifacts, affecting ability to compare methods fairly.",
            "frequency_or_prevalence": "Described as common in empirical software engineering and observed across the compared prior works; in this study the authors encountered missing artifacts when attempting to replicate several prior works (no per-paper percentage provided).",
            "root_cause": "under-specified natural language descriptions in papers (omitted low-level details), closed/undisclosed artifacts (proprietary LLMs, dataset curation choices), and implicit assumptions not written in the methods text.",
            "mitigation_approach": "Provide a fully open, reproducible pipeline: publish dataset (H-AIRosettaMP), release training/evaluation code, release model checkpoints, use open-weight LLMs (StarCoder2) and document prompts and preprocessing choices.",
            "mitigation_effectiveness": "Effective in this study: authors produce a fully reproducible benchmark and models; quantifiably, their open pipeline enabled consistent cross-language experiments and direct re-training of baselines on the same data, though it does not retroactively fix closed prior work results.",
            "domain_or_field": "machine learning / empirical software engineering / AI code stylometry",
            "reproducibility_impact": true,
            "uuid": "e424.0",
            "source_info": {
                "paper_title": "Is This You, LLM? Recognizing AI-written Programs with Multilingual Code Stylometry",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "provenance_mismatch",
            "name_full": "Provenance (source-language) mismatch effect",
            "brief_description": "AI-generated snippets' source (provenance) language used in code-translation data generation significantly affects classifier accuracy, causing large gaps when train/test provenances differ.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "dataset generation via cross-language code translation and classifier training/evaluation",
            "system_description": "H-AIRosettaMP dataset where AI-written examples are produced by translating Rosetta Code snippets from a provenance/source language (src) to a target language (dst) using StarCoder2; monolingual classifiers are trained on dst data with a fixed provenance distribution and tested on dst snippets coming from different provenances.",
            "nl_description_type": "dataset generation description / provenance specification",
            "code_implementation_type": "code translation scripts and model training/evaluation scripts",
            "gap_type": "dataset provenance mismatch / out-of-distribution generation (different input distribution than described or assumed)",
            "gap_description": "When classifiers are trained and tested on datasets where AI examples come from one provenance language (e.g., Kotlin) but are evaluated on data where AI examples were translated from different provenance languages, performance degrades considerably — i.e., a mismatch between the assumed provenance distribution in the natural-language dataset description and the actual provenance of test examples leads to misaligned results.",
            "gap_location": "data generation / training vs test data distribution (dataset provenance)",
            "detection_method": "empirical evaluation across 90 {src,dst} sub-datasets: authors trained monolingual models per dst with different src provenances and measured accuracy differences; statistical analysis (t-tests, ANOVA) confirmed significance.",
            "measurement_method": "Measured as change in classification accuracy; reported average loss of -17.93% in accuracy when using different provenance at test time (with t-statistic = 8.82, 95% CI 12.4 to 21.3, p &lt; 0.001). Table II and Figure 6 detail per-language provenance accuracies.",
            "impact_on_results": "Large — average accuracy loss of ~17.9% when provenance differs; this materially alters conclusions about detector performance and indicates strong sensitivity to provenance.",
            "frequency_or_prevalence": "Observed consistently across the 90 sub-datasets and multiple languages; authors treat it as a pervasive phenomenon in their translation-based data-generation methodology.",
            "root_cause": "data-generation method (code translation) introduces provenance-dependent artifacts and distributional shifts; classifiers learn provenance-specific signals rather than generalizable author-style features.",
            "mitigation_approach": "Train a multilingual classifier on a dataset sampled to include uniform provenance distribution (multi-provenance training), and apply preprocessing safeguards (e.g., remove trivial artifacts like leading/trailing spaces).",
            "mitigation_effectiveness": "Partially effective: multilingual model achieves consistent performance across languages (average accuracy 84.1% ± 3.8) and reduces extreme outliers, but still underperforms the best monolingual models by an average of -5.17% (t-test avg. loss 2.7 to 8.7, p &lt; 0.001).",
            "domain_or_field": "machine learning / dataset curation / AI code stylometry",
            "reproducibility_impact": true,
            "uuid": "e424.1",
            "source_info": {
                "paper_title": "Is This You, LLM? Recognizing AI-written Programs with Multilingual Code Stylometry",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "generation_mode_mismatch",
            "name_full": "Mismatch between dataset generation modes (code translation vs natural-language prompting)",
            "brief_description": "Different data-generation modes (translating existing code vs generating code from natural-language task descriptions) yield qualitatively different AI outputs, causing classifiers trained on one mode to perform differently on data from another mode.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "AI-generated code dataset creation procedures",
            "system_description": "Two alternative ways to generate AI-written code: (1) code translation (provide code in src language, ask LLM to translate to dst), and (2) natural-language prompting (provide task description and request solution).",
            "nl_description_type": "dataset generation method description / prompt specification",
            "code_implementation_type": "prompt templates and generative-model invocation code",
            "gap_type": "different generation variant (mode mismatch) / incomplete mapping between described generation mode and real-world generation modes",
            "gap_description": "The paper explains that translation-based generation was chosen because natural-language prompting or function-driven prompting often produced incomplete 'skeleton' code; classifiers trained on translation-generated snippets may not generalize to examples produced by natural-language prompts (e.g., ChatGPT outputs), leading to performance gaps between description of dataset generation and the variety of real-world generation modes.",
            "gap_location": "data-generation method / upstream prompt engineering",
            "detection_method": "Comparative evaluation: authors tested their classifier (trained on translation-generated data) on an external dataset (Oedingen et al.) created via natural-language prompts with ChatGPT and measured drop in accuracy.",
            "measurement_method": "Measured accuracy drop: multilingual classifier achieved 72.8% accuracy on Oedingen et al.'s ChatGPT-generated Python dataset, representing a -6.3% drop relative to their in-dataset result; relative to Oedingen et al.'s reported results the gap was larger (-25.2%).",
            "impact_on_results": "Moderate-to-significant — classifiers trained on one generation mode (translation) see lower accuracy on data generated by another mode (natural-language prompting), which affects external validity of conclusions and practical deployment.",
            "frequency_or_prevalence": "Observed at least in cross-dataset evaluation (this paper vs Oedingen et al.); authors treat this as a limitation and note it is a shared threat across related work.",
            "root_cause": "generation-mode dependent output distributions (LLMs produce different code when translating vs when generating from description); lack of coverage of generation modes in dataset descriptions and training.",
            "mitigation_approach": "Propose future work to analyze multiple generation modes (prompt engineering, different LLMs, provenance languages) and encourage datasets that include mixed-generation modes; in this study they partially address it by training a multilingual, multi-provenance model and by testing on external datasets.",
            "mitigation_effectiveness": "Partial: multilingual model was still able to detect ChatGPT-generated code above chance (72.8%) without fine-tuning, but performance was reduced compared to in-domain results; authors quantify the drop (-6.3% vs their dataset, -25.2% vs Oedingen reported).",
            "domain_or_field": "machine learning / dataset generation / empirical evaluation",
            "reproducibility_impact": true,
            "uuid": "e424.2",
            "source_info": {
                "paper_title": "Is This You, LLM? Recognizing AI-written Programs with Multilingual Code Stylometry",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "translation_artifacts",
            "name_full": "Translation-induced artifacts (e.g., leading/trailing spaces and length differences)",
            "brief_description": "Code produced by automated translation (StarCoder2) exhibits systematic low-level artifacts (spacing patterns, differing snippet lengths) that can be exploited by detectors, causing mismatches between the claimed balanced dataset and actual detectable differences.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "code translation-based dataset construction",
            "system_description": "Generating AI-written code by asking a code LLM to translate human-authored code from a source language into a target language, producing translated snippets that sometimes include consistent formatting or length patterns.",
            "nl_description_type": "dataset construction description / expected data equivalence claim",
            "code_implementation_type": "translation prompt templates and post-processing scripts",
            "gap_type": "formatting/artifact leakage (unintended trivial signals) / dataset imbalance in low-level features",
            "gap_description": "AI-translated snippets showed recognizable leading/trailing spacing patterns and systematic differences in snippet length between AI and human groups for several languages. If left in the dataset, these trivial artifacts could make the classification task artificially easy, misaligning the intended evaluation (style-based detection) with the actual code inputs.",
            "gap_location": "data preprocessing and artifact removal stages",
            "detection_method": "Data inspection and statistical testing: authors measured snippet length differences (t-tests per language) and observed systematic spacing patterns; they removed leading/trailing spaces as a corrective action.",
            "measurement_method": "T-tests on snippet length between AI and human groups per language (α = 0.05); six out of ten languages had statistically significant length differences (p &lt; 0.05) as reported in Table I. Qualitative observation of spacing patterns led to removal of leading/trailing spaces.",
            "impact_on_results": "Potentially large if not mitigated — such trivial artifacts could give classifiers an unfair advantage. The authors removed these artifacts to avoid bias; no explicit numeric delta provided for artifact removal effect, but presence was judged sufficient risk to preprocess them out.",
            "frequency_or_prevalence": "Observed across translation-generated snippets in multiple languages (six languages with significant length differences; spacing artifacts observed and cleaned across the dataset).",
            "root_cause": "systematic behavior of the generative model during translation (formatting/whitespace conventions) and differences in original human snippets' formatting conventions; omission of such cleaning in dataset descriptions allows artifact leakage.",
            "mitigation_approach": "Remove trivial artifacts (e.g., strip leading/trailing spaces) and perform balancing/undersampling so that trivial low-level signals cannot drive classification.",
            "mitigation_effectiveness": "Not reported as a numeric improvement, but authors state they removed the artifacts specifically to avoid unfair advantage and to better match real-world conditions; dataset after cleaning used for all reported experiments.",
            "domain_or_field": "dataset curation / machine learning",
            "reproducibility_impact": true,
            "uuid": "e424.3",
            "source_info": {
                "paper_title": "Is This You, LLM? Recognizing AI-written Programs with Multilingual Code Stylometry",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "baseline_dataset_hardness",
            "name_full": "Dataset-dependent performance gaps when reimplementing baselines",
            "brief_description": "Baseline methods reported in prior literature yield substantially different accuracies when reimplemented and evaluated on the new H-AIRosettaMP dataset, indicating a misalignment between prior paper-reported performance and actual performance under different dataset conditions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "baseline algorithm reimplementation and comparison",
            "system_description": "Authors reimplemented previous baseline classifiers (Random Forest, J48, XGB with TF-IDF) and trained/evaluated them on H-AIRosettaMP to compare against transformer-based models.",
            "nl_description_type": "related-work reported performance / method description",
            "code_implementation_type": "reimplemented baseline classifier code",
            "gap_type": "dataset shift / mismatch between reported baseline performance and reimplementation results",
            "gap_description": "Baselines reported high accuracies in original papers, but when reimplemented and run on the authors' dataset, they performed worse (negative accuracy gaps), suggesting that claims in prior papers may not generalize to different or harder datasets and that paper descriptions did not fully specify dataset characteristics causing the discrepancy.",
            "gap_location": "evaluation stage / cross-dataset generalization",
            "detection_method": "Empirical reimplementation and evaluation on H-AIRosettaMP; direct numeric comparison of accuracies reported in prior work vs reimplementation results.",
            "measurement_method": "Reported absolute accuracy gaps: -18.5% (Java baseline), -3.5% (C++ baseline), -5.2% (Python baseline) when compared to original reported results; also compared monolingual vs multilingual classifier gaps (e.g., multilingual worse than monolingual by -5.17% avg).",
            "impact_on_results": "Substantive: shows that dataset choice and undocumented preprocessing materially affect reported performance and the fairness of comparisons; motivated authors to offer an open, harder benchmark.",
            "frequency_or_prevalence": "Observed for multiple baselines and languages in this study; authors interpret H-AIRosettaMP as a harder benchmark than prior datasets.",
            "root_cause": "differences in dataset difficulty, selection biases, and underreported preprocessing/data-splitting choices in original papers (natural-language descriptions not sufficiently precise about dataset characteristics).",
            "mitigation_approach": "Provide open benchmark with detailed artifact release, re-train baselines on the same dataset to enable fair comparison, and emphasize reproducible artifact publication.",
            "mitigation_effectiveness": "Effective for enabling fair comparisons in this work: authors re-trained baselines on H-AIRosettaMP and present direct comparisons; however, it cannot retroactively change prior published claims.",
            "domain_or_field": "machine learning / empirical evaluation / benchmarking",
            "reproducibility_impact": true,
            "uuid": "e424.4",
            "source_info": {
                "paper_title": "Is This You, LLM? Recognizing AI-written Programs with Multilingual Code Stylometry",
                "publication_date_yy_mm": "2025-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Discriminating human-authored from chatgpt-generated code via discernable feature analysis",
            "rating": 2,
            "sanitized_title": "discriminating_humanauthored_from_chatgptgenerated_code_via_discernable_feature_analysis"
        },
        {
            "paper_title": "Chatgpt code detection: Techniques for uncovering the source of code",
            "rating": 2,
            "sanitized_title": "chatgpt_code_detection_techniques_for_uncovering_the_source_of_code"
        },
        {
            "paper_title": "Detecting chatgpt-generated code in a CS1 course",
            "rating": 2,
            "sanitized_title": "detecting_chatgptgenerated_code_in_a_cs1_course"
        },
        {
            "paper_title": "Gptsniffer: A codebertbased classifier to detect source code written by chatgpt",
            "rating": 1,
            "sanitized_title": "gptsniffer_a_codebertbased_classifier_to_detect_source_code_written_by_chatgpt"
        },
        {
            "paper_title": "Zero-shot detection of machinegenerated codes",
            "rating": 1,
            "sanitized_title": "zeroshot_detection_of_machinegenerated_codes"
        }
    ],
    "cost": 0.014690249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Is This You, LLM? Recognizing AI-written Programs with Multilingual Code Stylometry</p>
<p>Andrea Gurioli andrea.gurioli5@unibo.it 
DISI University of Bologna Bologna
Italy</p>
<p>Maurizio Gabbrielli maurizio.gabbrielli@unibo.it 
DISI University of Bologna Bologna
Italy</p>
<p>Stefano Zacchiroli stefano.zacchiroli@telecom-paris.fr 
LTCI
Télécom Paris Institut Polytechnique de Paris Palaiseau
France</p>
<p>Is This You, LLM? Recognizing AI-written Programs with Multilingual Code Stylometry
E2BD40FED4858E8AB507F6A1C32FC11D10.1109/SANER64311.2025.00044code stylometrylarge language modelsAI detectioncode generationdata provenancedeep learning
With the increasing popularity of LLM-based code completers, like GitHub Copilot, the interest in automatically detecting AI-generated code is also increasing-in particular in contexts where the use of LLMs to program is forbidden by policy due to security, intellectual property, or ethical concerns.We introduce a novel technique for AI code stylometry, i.e., the ability to distinguish code generated by LLMs from code written by humans, based on a transformer-based encoder classifier.Differently from previous work, our classifier is capable of detecting AI-written code across 10 different programming languages with a single machine learning model, maintaining high average accuracy across all languages (84.1% ± 3.8%).Together with the classifier we also release H-AIRosettaMP, a novel open dataset for AI code stylometry tasks, consisting of 121 247 code snippets in 10 popular programming languages, labeled as either human-written or AI-generated.The experimental pipeline (dataset, training code, resulting models) is the first fully reproducible one for the AI code stylometry task.Most notably our experiments rely only on open LLMs, rather than on proprietary/closed ones like ChatGPT.</p>
<p>I. INTRODUCTION</p>
<p>LLM-based code completers [8], [24], [39] (or code LLMs for short in this paper), as exemplified by GitHub Copilot 1 , are becoming popular automatic programming tools among software developers.Preliminary evaluations of code LLM results show that they can produce either correct or buggy code [11], [40], depending on how they are used.Specifically, code LLMs can be useful assets for expert programmers who quickly learn to use them well or a liability for novice developers who lack the experience to skip misleading answers quickly.The real impact of code LLMs on developer productivity also remains unclear, with growing interest in defining proper metrics to evaluate it [1].</p>
<p>Similarly, policy-wise, the use of code LLMs can be frowned upon or outright forbidden, depending on the context.Security-and privacy-sensitive environments might forbid the use of code LLMs hosted by 3rd parties-like Copilot, hosted by GitHub, or ChatGPT 2 by OpenAI-to avoid leaking internal code in prompts.(Self-hosted open-weight LLMs, like Code Llama [39] and StarCoder [24] mitigate this issue.)In teaching contexts, such as schools and universities, the use of code LLMs can be considered cheating (depending on the assignment goals), with severe consequences for the students who use them [18].</p>
<p>Legal and licensing risks are also ongoing concerns when using code LLMs [38].Even leaving aside the hot legal topic of whether training LLMs on third-party unlicensed material is allowed (or ethical), code LLMs can output verbatim parts of their training datasets, a phenomenon known as recitation [44], which might expose their users to legal liabilities [7] if generated code is integrated into a product put on the market.</p>
<p>A. Problem statement</p>
<p>These practical needs have spawned an interest in automatically recognizing code generated by LLMs, distinguishing it from code written by humans.This is an instance of the more general task of automatically detecting who wrote a given piece of code, known in the literature as code stylometry (or code authorship attribution, or code author recognition) [6], [20], [34].</p>
<p>Previous work [7], [18], [19], [23], [33] has already applied code stylometry techniques to the recognition of "AI authors" (i.e., code LLMs), with three recurrent characteristics: (1) detection is possible on a single programming language at a time; (2) the tested code LLM is a proprietary, non-open tool or model (e.g., Copilot, or ChatGPT), which hinders scientific reproducibility and replicability; (3) detection is based on traditional machine learning techniques (e.g., random forest classifications, LSTM, or code2vec embeddings).</p>
<p>The goal of this paper is to improve the state-of-the-art of the detection of AI-written programs, by addressing the following research question:</p>
<p>RQ1: Is it possible to detect source code generated by code LLMs, achieving high accuracy across several different programming languages?Answering this question affirmatively would improve over limitation (1) above, which can be particularly annoying in projects where multiple programming languages are in use, as it is often the case.Methodologically, we aim to answer RQ1 following a fully reproducible experimental approach (addressing limitation 2 above) and using more recent machine learning techniques (point 3 above) that, as we will see, are 1) Programming language selection: As a starting point for human-written code snippets, we used Rosetta Code [9], a programming chrestomathy project that collects and publishes solutions to the same programming tasks in as many different languages as possible, to showcase similarities and differences across languages.We retrieved a version of the Rosetta Code timestamped as July 1st, 2022.The retrieved dataset contained 79 013 code snippets, each representing a solution to one among 1203 programming tasks in total, written in one among 883 programming languages.</p>
<p>In order to both respond to real-world use cases and maximize data availability for the later training phase, we selected our target programming languages for AI code stylometry based on their popularity.To rank languages by popularity, we retrieved the TIOBE index [35] ranking, as of May 2024.From the TIOBE ranking we removed all languages not present in the training dataset of the open code LLM used in our experiments, namely StarCoder2-15B [27] (see later in this section for a discussion of our choice of LLM).To conclude this step (Popular languages filtering in Figure 1), we selected the top-10 remaining languages by ranking order-10 languages being a very significant step forward w.r.t. the state of the art of AI code stylometry performed on at most 2 languages at a time.</p>
<p>We hence obtained a set of 10 popular programming languages, together with hand-written snippets in those languages from Rosetta code, that are also well-known to the code LLM used later to generated AI-authored snippets: C++, C, C#, Go, Java, JavaScript, Kotlin, Python, Ruby, Rust.</p>
<p>Before dwelling into details, here are the two key intuitions behind the next steps:</p>
<p>1) Human-written snippets in the target dataset are unmodified snippets coming from Rosetta Code: they were all contributed as task solutions by humans participating in the initiative.2) AI-written snippets in the target dataset are generated by a code LLM (specifically: StarCoder2 [27]) using cross-language translation form a source programming language src (called the provenance language in the following) to a destination language dst, as previously done by Li et al. [23].Input to the translation is a humanwritten snippet coming from Rosetta Code (as per (1) above); output of the translation is an AI-written snippet (by StarCoder2) that will be integrated into the target dataset.Details on the translation step are provided later in this section.2) Task balancing: To avoid skewing the AI-written part of the dataset by translating from a single source programming language (which might be more affine to one target language than another), all human-written snippets in the dataset for a given programming language src have been translated to all other 9 languages among the 10 selected languages.This constitute a total of 90 (= 10 × 9) sub-datasets, each formed by a {src, dst} unordered language pair, where src ̸ = dst.</p>
<p>The initial 90 (sub-)datasets (before the Process Fig. 1.Experimental methodology.The process is divided into two main steps: (1) The Dataset construction, which starts from the filtered Rosetta Code dataset and terminates in the H-AIRosettaMP, obtained via code translation, comprising 90 (sub-)datasets.Each dataset is labeled by the author (Human or AI) and is represented by dst (the language of the dataset) and src language (the provenance of the AI-generated part of the dataset); (2) The model training, that shows the process leading to 90 monolingual models (one per dataset) and 1 multilingual model.</p>
<p>in Figure 1) have been obtained by selecting snippets from Rosetta Code in a way that created balanced datasets.Specifically, in each {src, dst} dataset, we only kept Rosetta Code snippets pertaining to the same task.That is, each solution written in programming language src is kept in the dataset {src, dst} if and only if a solution for the same task exists also for programming language dst, and vice-versa.</p>
<p>After this step, we obtained the balanced 90 (sub-)datasets shown in Figure 1 just before the Translation step that we describe next.</p>
<p>3) Translation: Li et al. [23] pioneered using code translation for building the AI-generated part of datasets for AI code stylometry.They discussed three alternative methodologies to do so:</p>
<p>1) Code translation involves providing the generative model with a code snippet in one programming language, asking the model to translate it into a different language; 2) Functional translation involves providing a natural language description of the desired task, asking to generate a solution; 3) Functional customization involves providing an existing snippet of code, asking to provide an explanation of what it does first, and then asking to generate a solution based on the description.We considered all three options for our needs and concluded that (2) and (3) are not suitable option, because in our evaluation (using various code LLMs) they often end up producing "skeleton code", with holes that remain to be filled by the user.Keeping those incomplete snippets in the target dataset would give an unfair advantage to the AI detector, because they will be fairly easy to distinguish from complete code snippets from Rosetta Code (written by humans).We then settled for code translation (1) and applied it to the 90 balanced datasets obtained from the previous step.</p>
<p>Specifically, for each (sub-)dataset {src, dst}, we took all the snippets in it written in the dst programming language, and used the StarCoder2 [27] generative model to translate the snippet into the src programming language (Translation step in Figure 1, further detailed in Figure 2).</p>
<p>The choice of StarCoder2 as code completion model is due to its being an open model, both in its weights (available for download and reuse under the terms of the Open RAIL-M v1 license) and in its training dataset (obtained from the Software Heritage archive [10]).Openness is a strong requirement to achieve our goal of full reproducibility of the experimental pipeline, which would not be achievable using closed models such as Copilot or ChatGPT (and indeed has not been achieved in previous work in the literature).StarCoder2 achieves 46.3% accuracy in the HumanEval benchmark [8], a widely used benchmark for assessing the coding abilities of generative models, making it outperform coding models like CodeLlama and DeepSeekCoder [16], [39].In summary: StarCoder2 is the best performing code LLM among those that are open enough to satisfy the reproducibility requirement.</p>
<p>To translate a snippet from language src to dst, we give to StarCoder2 the prompt whose synopsis is shown in Figure 3.When reading the generated output we select each next token by taking the one with the highest likelihood (greedy search), as it is commonplace in translation tasks.</p>
<p>Due to memory and computational limitations, we excluded snippet pairs {src, dst} that would result in prompts longer than 1024 tokens, and we set 2048 as the maximum length in the generative phase.We have also excluded snippet pairs for which StarCoder2 returned malformed outputs (e.g., empty or lacking the closing ''' delimiter).</p>
<p>After translation, each of the 90 {src, dst} sub-datasets is now composed of snippets in a single programming language Here is the translated code\n\n''' Fig. 2. Code translation step.The human-labeled part of the dataset (the ArrayLength Java class here) is a solution to a task from Rosetta Code (Array concatenation).The AI-labeled part is obtained via code translation (from Python to Java) using StarCoder 2. Input to the translation is a human-written solution for the same task, in a different programming language.</p>
<p>Translate this '''\n CODE_SNIPPET \n''' from SOURCE_LANGUAGE to TARGET_LANGUAGE.Here is the translated code\n\n''' Fig. 3. Synopsis of the prompt given to StarCoder2 for translating a given code snippet (CODE_SNIPPET in the text) from a source programming language (SOURCE_LANGUAGE) to a target one (TARGET_LANGUAGE).A prefix of the desired answer (Here is the...) is provided because StarCoder2 has not been fine-tuned for chat-based interaction and is strictly a completion model.</p>
<p>(dst), labeled as either human-written (by a Rosetta Code contributor) or AI-written (by StarCoder2 via code translation).</p>
<p>All together, the 90 sub-datasets form a single reproducible dataset, called H-AIRosettaMP, which we release publicly as open data for others to experiment with.H-AIRosettaMP comprises 121 247 snippets, with 1127 unique tasks in 10 popular programming languages.</p>
<p>Note how the H-AIRosettaMP dataset satisfies multiple important requirements for the AI code stylometry task.As discussed by Caliskan-Islam [20] as an important feature: it contains multiple snippets, authored by multiple authors (grouped in two, in our case: humans vs AI), implementing different tasks.Specifically, it is not the case that the two authors are partitioned by task: for each task we have both a human-authored solution and an AI-authored one.This avoids the risk that the classifier will learn to distinguish tasks, rather than authors.Additionally, the dataset satisfies all the requirements associated with RQ1, namely: it is multilingual (with 10 languages), it is openly available and fully reproducible.</p>
<p>B. Model training</p>
<p>1) Classifier architecture: Until now, only classical machine learning techniques (discussed in detail in Section VII) have been applied to the AI code stylometry task.On the other hand, Niu et al. [32] showed how transformer-based architectures are state-of-the-art for several code understanding tasks.
CodeT5+ 770M Encoder <s> </s> V 0 V N V N-1 } Classification head (class probability) Code input 0.97 public V 1
In the context of natural language (as opposed to code), recent works obtained successful results on AI recognition [25], [30] using analogous architectures.</p>
<p>In this work, we apply, to the best of our knowledge for the first time, a transformer-based machine learning architecture to the task of AI code stylometry.Specifically, we use CodeT5plus-770M [42], a pre-trained code transformer architecture, in an encoder setup, as shown in Figure 4. We provide the model with a tokenized text input and obtain several vectorial representations as the number of tokens.To distinguish human-from AI-written code, we add, following Niu et al. [32], a classification head to the first output repre-sentation of the model, corresponding to an always present classification token (see Figure 4).The classification head consists of a linear layer with a ReLU activation function, 20% dropout, followed by a final linear layer for binary classification.</p>
<p>2) Undersampling: As we describe below, we have trained 91 classifier models in total: one monolingual model for each of the 90 {src, dst} sub-datasets + one multilingual model on a multilingual dataset sampled from the entire H-AIRosettaMP.</p>
<p>Before training the monolingual models, we undersampled each sub-dataset to the threshold of 470 code snippets for each Human/AI class, corresponding to the minimum amount of snippets across all sub-datasets.</p>
<p>Before training the multilingual model, we want to make sure that: (1) AI-written snippets, generated via code translation, come from a uniform distribution of source languages before the translation; (2) for both AI-written and humanwritten snippets, only a single solution for a given task is present (to avoid learning about the task, rather than learning about the author style).To ensure these properties, we processed the 10 languages one by one.For each language dst, we collect 470+470 = 940 snippets (half AI-written, half humanwritten).When collecting AI-written snippets, we sample across the other 9 provenance languages src, with a uniform distribution.When collecting both AI-written and humanwritten snippets, we never select more than one solution for the same task; at most, the solution to the same task can hence appear twice in a given programming language, once as human-written and once as AI-written.</p>
<p>As an additional data cleaning step, we also removed all leading and trailing spaces from all code snippets (both human-written and AI-written) because AI-translated snippets exhibit recognizable heading/trailing spacing patterns, and we wanted to avoid unfairly advantaging classifiers that might learn from them (in real-world use cases those spaces would most likely not be preserved as is).</p>
<p>3) Training: We adjusted the model hyperparameters, starting from the setup proposed by Wang et al. [42], picking a subset of the dataset (all languages with Python provenance except for the Python language, translated from C++) and validated the model, obtaining a hyper-parameter setup for the rest of the experiments.We used AdamW [26] as an optimizer with a weight decay of 0.01.We trained each model for 15 epochs, multiplying the learning rate after 10 epochs by a 0.1 factor, with an initial learning rate of 2e-05.</p>
<p>After training the 90 monolingual models, we observed different results for the same dst language, coming from datasets with different provenance language src (see Section IV for details).We inspected this phenomenon by testing the best model for a language dst on datasets with different src provenance.</p>
<p>To obtain a model capable of handling several different languages as input, we trained the multilingual model using the entire H-AIRosettaMP dataset (after undersampling).</p>
<p>All models were trained with an 80%/20% training/test split.</p>
<p>We then compared our results with the best-performing results in the literature [23], [33].Our classifiers were trained on the novel H-AIRosettaMP dataset, which is different from datasets used in previous works.Thus, for a fair comparison, we re-trained Li et al. and Oedingen et al. [23], [33] classifiers over our dataset.</p>
<p>We trained at first four baseline models following Li et al. [23] two methodologies (Random Forest and J48) over our Java and C++ sub-datasets with Kotlin provenance (because it obtained the best performances across all src provenances).The models were trained and evaluated using the same methodology of Li et al., with 10-fold cross-validation.</p>
<p>As our last comparison, we trained a baseline model following the best-performing methodology of Oedingen et al [33] over our Python dataset (also with Kotlin provenance).</p>
<p>4) Evaluation: We evaluated each of the trained monolingual classifiers on the respective dataset (in-distribution test), noting down the resulting accuracy.We evaluated in the same way the trained multilingual classifier on its own dataset (indistribution test), which contains snippets in all languages and translated from all src languages (for the AI-labeled part).</p>
<p>We evaluated the 5 baseline models on different datasets (see Table IV for reference): RF Java and J48 Java baselines on the Java sub-dataset with translation from Kotlin (best average accuracy among all src provenance languages for Java); RF C++ and J48 C++ on the C++ sub-dataset (provenance: Kotlin); XGB-TF-IDF Python on the Python sub-dataset (provenance: Kotlin).Finally, we evaluated the monolingual models needed for comparison with baselines on out-distribution languages with Kotlin provenance, except for Kotlin itself, where Go provenance was used.</p>
<p>III. DATASET</p>
<p>The H-AIRosettaMP dataset is released publicly as part of our replication package (on Zenodo, see Data availability statement at the end of the paper) and also mirrored on Hugging Face. 4 The dataset comes in tabular form, with one row per snippet, for a total of 121 247 rows.Each column in the table provides some information about the snippet:</p>
<p>• task_name, task_url, task_description: information about the Rosetta Code [9] task that the snippet implements, respectively: the task name (e.g., Array concatenation), URL on the Rosetta Code website (e.g., http://rosettacode.org/wiki/Arrayconcatenation), and natural language description of the task.• language_name: the programming language in which the code snippet is written, one of: C++, C, C#, Go, Java, JavaScript, Kotlin, Python, Ruby, Rust.• code: the actual, full source code of the snippet as a string.</p>
<p>• target: a binary label denoting whether the snippet is human-or AI-written.• set: the name of the specific sub-dataset, e.g., "Java_from_C++" for the Java snippet dataset whose Fig. 5. Distribution of unique tasks for which solutions are present in the dataset per language.For each task, both a human-written and an AI-written snippet is always provided.Overlapping tasks denote the number of tasks for which multiple AI-written solutions are present, with a guarantee that they have been translated from all other programming languages in the dataset.</p>
<p>AI-written parts were obtained via translation from C++ (the human-written snippets, on the other hand, were natively written in Java).As a simple descriptive statistics, Table I shows the average length of code snippets in the entire dataset by programming language, measured as the number of characters.We conducted a t-test for each language between the Human and AI-labeled groups of snippets with α = 0.05 after having tested data normality and variance.When looking at the breakdown between AI-and human-written snippets, we see that six languages out of ten significantly differ in number of characters (p &lt; 0.05 Table I).The noticeable differences in snippet lengths between AI-and human-written code suggest that length could be a predictive feature in code detection models.</p>
<p>We recall from Section II-A that the generation of AIlabeled snippets in the dataset is followed by task balancing, ensuring that each language-specific sub-dataset contains pairs of human-written/AI-written snippets solving the same task.Figure 5 shows the number of unique tasks (or, equivalently, the number of snippet pairs) for which solutions are present in each sub-dataset, aggregated by programming language.It also shows, for each language, the number of tasks for which there are solutions coming from all other provenance languages ("Overlapping tasks" in the figure).</p>
<p>Due to the uneven distribution of task solutions across languages in Rosetta Code, different languages in the dataset show a different number of tasks present.On the other hand, dataset users interested in avoiding the effect of translation provenance on AI-generated snippets can safely work in the subset of overlapping tasks.</p>
<p>In comparison to previous work in the literature [18], [23], [33], [43], this dataset provides the ability to develop and test AI stylometry classifiers along multiple snippet dimensions-(1) programming language of the snippet, (2) provenance language (for AI-written snippets, obtained via translation), (3) snippet length, (4) task implemented by the snippet-allowing to isolate how each of them influences the performances of AI code stylometry.</p>
<p>IV. RESULTS</p>
<p>We show the results of our experiments in two separate tables.We first report, in Table II, accuracy results for all our models, both monolingual and multilingual.Then we compare, in Table IV, our best models to the best baselines from the literature.</p>
<p>We present all results in terms of overall accuracy, which is defined as the ratio of correctly classified snippets to the total number of snippets to be classified.In order to establish the statistical significance of our results, we also conducted t-tests (see Table III) and ANOVA tests with α = 0.05 after testing data normality and variance.</p>
<p>In Table II, we depict the results of the experiments across all languages and provenance.Each monolingual classifier is tested only on snippets of the same language (in-distribution results), making the provenance language (i.e., the language AI-written snippets in the dataset were translated from) vary across all other languages.Therefore, we show the provenance language (src) of the snippets in rows, while in the columns, we display the target language dst, which is the language the models have been trained on.The table also shows the average accuracy by provenance language (Prov.accuracy column) and the average accuracy by tested language (Language accuracy row).The Multilingual model row provides results for the multilingual classifier, which has been trained on the multilingual dataset sampled from H-AIRosettaMP and tested separately on each language.Finally, the bottom-right cell of Table II contains the average accuracy of the multilingual model across the 10 programming languages considered.</p>
<p>The results in Table II show significant differences both in rows (same provenance language and different destinations) and columns (different provenances and same destination).Table III confirms that these differences among the models are significant, since we observed both for average provenance accuracies (F−statistic = 2.00 with p = 0.04 in the table) and average language accuracy (F−statistic = 3.56 with p &lt; 0.001) relevant values.</p>
<p>In Table IV we compare our models with the baseline ones, namely J48 and Random forest algorithms from Li et al. [23] and XGB from Oedingen et al. [33].All the baselines and our monolingual models (Java, C++, Python) are trained with our best dataset, namely the one that provides the best value for column Prov.accuracy in Table II, that is Kotlin.The test datasets are also all with Kotlin provenance, except for Kotlin itself which has Go provenance.</p>
<p>For monolingual models-when analyzing in-distribution tests-we highlight a substantial positive gap (+7.2% for the Java language, +8.9% for the C++ language, and +1.5% for the Python model) compared to the baselines.We notice the positive gap between the C++ model tested on the Java (outdistribution test) dataset (+2.4%), showing how, even when trained on a different language, this methodology performs better than the one adopted by Li et al. [23].Out-distribution   tests for the baselines are not shown as these architectures employ predefined features-lexical, syntactical (extracted from the abstract syntax tree of the source code), or deriving from source code layout-strictly linked to the language used during training, making the model specific to the designed language (a net advantage for the approach proposed in this paper).We notice how the multilingual classifier performs worse than monolingual classifiers in both Table II and Table IV.In particular, Table III shows that the multilingual classifier has an accuracy that is worse than that one of the monolingual model in the average language case (-5.17% avg.t−statistic = 4.01 with p &lt; 0.001).The multilingual classifier Table IV, however, does not present outliers in terms of accuracies, obtaining a model with consistent results, effective in handling multiple languages and the provenance phenomenon.In addition to the practical benefits of having a single model, this is another reason, as we will discuss in Section V, why the multilingual model is preferable for detecting AI-generated code in practice.</p>
<p>We also observe that our reimplementations of the baselines perform worse than the results reported in the original papers by both Li et al. [23] and Oedingen et al. [33].Specifically, we obtained negative gaps of -18.5% with Java [23], -3.5% with the C++ baseline [23] and -5.2% for the Python baseline [33].Since we reimplemented the same methodologies and applied them on our dataset (to perform a fair comparison on an identical dataset), we attribute these differences to the dataset itself, suggesting that H-AIRosettaMP is a harder benchmark than the ones used in previous work-which also makes intuitive sense, given the fast-paced advances in LLM-based code generators.V. DISCUSSION a) Findings: Based on the results presented in Section IV we can answer the stated research question affirmatively: it is possible to recognize AI-written programs with high average accuracy (84.1%), across 10 different programming language (multilingual code stylometry), with a single trained classifier based on a transformer-based architecture, novel for this task.</p>
<p>To achieve this, we devised and implemented a fully open and reproducible methodology and also replicated previous experiments in the literature [23], [33].We observe significant performance differences not only across different datasets (which is to be expected), but also between accuracies previously reported in the literature and our replications of the same experiments with the same architectures.The following factors might be the cause of these discrepancies:</p>
<p>(1) The language in which the snippets to be recognized are written in plays an important role.For example, we see a pattern in our results comparing C snippets (accuracy 93.2%± 3.3) with Rust snippets (86.1% ± 2.3), for a difference of 7 points.It is entirely possible that AI-written code in some programming languages is intrinsically easier to recognize as such than code written in other languages.Establishing this conclusively is an interesting direction for future work.</p>
<p>(2) We also observe significant accuracy differences between datasets with different provenance languages (i.e., the languages AI-written code snippets where translated from).In order to test the impact of this factor, we analyzed models in the same language but with different provenances for the testing datasets (Figure 6).More precisely, since for the same language we have different models (depending on the provenance language), we first selected the models with the best provenance accuracy (last column of Table II), namely the models with Kotlin provenance.We depicted in Figure 6, with the dashed orange line, the accuracy of these models on the various languages (that is, we reported in the Kotlin row of Table II).Then, we compared these accuracies to those obtained for each language L ̸ = Kotlin by averaging the accuracies of the L model on all the training datasets generated in L from the other provenances.This is reported in Figure 6 with the blue line.II (in-distribution).The solid blue line shows the accuracy of the same models on test sets with a different language provenance (potentially out-distribution, as is general translating solutions for the same task from different languages to the same language will result in different snippets).Model performances degrade significantly in the latter case.</p>
<p>As the picture shows, we obtain a significant loss in accuracy (-17.93% in avg.t−statistic = 8.82 with 95% CI 12.4 to 21.3 and p &lt; 0.001).We believe this phenomenon is tightly coupled with the methodological choice of producing AI-written snippets via code translation and would not be observable under different conditions.We consider that we have properly mitigated this by training our main classifier to be multilingual across multiple translation provenances.</p>
<p>Still, the accuracy differences are interesting per se and pave the way to dedicated future work.We hypothesize that different ways of inputting-and prompting-the generative model, both to generate the training datasets and to use the obtained classifiers, can influence its performance, possibly leading to detection evasion.</p>
<p>(3) Reproducibility issues.As is way too common in empirical software engineering [15], studies tend to underreport the details needed to fully replicate their empirical findings, particularly so when AI-training is involved.When replicating baseline results from previous work, we tried our best, but some artifacts were not available and had to be reimplemented from scratch.This work contributes to raising the bar of replicability for AI code stylometry by relying on and producing only openly available artifacts.Aside from the above, in Section III, we highlighted two other factors that can influence detection accuracy: dataset size and task balancing.We did not explore systematically how either of them influence the results; it is left as future work.</p>
<p>b) What about ChatGPT?: ChatGPT is one of the most popular generative AI tools on the market, for both generating natural language text and program code.We could not use ChatGPT in our experiments without undermining their replicability-which is one of our goals and differentiating aspects w.r.t.previous work.Still, it is interesting to verify how our reproducible classifiers, trained using only openly available data, fare against ChatGPT (whose model and training data are undisclosed).</p>
<p>We tested our multilingual classifier (trained on the H-AIRosettaMP dataset) on the Human-AI dataset by Oedingen et al. [33], whose AI part consists of Python code snippets generated by ChatGPT (at the time and version of their experiments) starting from natural language prompts.Without any fine-tuning on ChatGPT data, our multilingual classifier achieved 72.8% accuracy, with a -6.3% drop from the results obtained on our dataset.The accuracy gap is more significant when compared to the results reported by Oedingen et al. [33]: -25.2% with our model on their dataset.Still, our classifier results are way above the chance (50% for 2 classes), multilingual, and reproducible.</p>
<p>The accuracy gap can be due to several factors: the LLM used to generate the AI snippets, the different inputs and prompts used during generation, hyperparameter tuning, etc.Ultimately, the important question here is whether, in the upcoming arm race between AI code generation and AI code stylometry (to detect it), we can rely on closed models and datasets or not.We argue we should not and propose a new baseline for reproducible, multilingual, AI code stylometry with this work.</p>
<p>VI. LIMITATIONS</p>
<p>External validity: Our approach to the generation of AIwritten snippets in the novel H-AIRosettaMP dataset is reproducible code translation.Our results could be impacted by that choice.We have explored the possibility only superficially, by verifying how our classifier performs on the dataset of Oedingen et al. [33], generated using ChatGPT on natural language prompts (hence: not code translation), with good results (cf.Section V).A more thorough analysis of how our results generalize to other prompting techniques (e.g., natural language prompts) and LLMs is left as future work.Note that we share this threat with all related work and that it is impossible to fully mitigate this threat encompassing closed LLMs (like ChatGPT) without sacrificing reproducibility.</p>
<p>Reliability: We address reliability threats in the usual way by releasing a comprehensive replication package covering all experiments discussed in the paper (see the Data availability statement at the end).In this respect, we fare better than all previous works by relying only on openly available datasets and components, including third-party LLMs.</p>
<p>VII. RELATED WORK</p>
<p>a) Code stylometry: Oman and Cook [34] were the first to introduce the notion of code stylometry.They hypothesized that each author is recognizable by a unique coding style called "fingerprint".In their pioneering work, they approached the task using cluster-based classification, introducing an unsupervised technique for inferring the code author.</p>
<p>Caliskan-Islam et al. [20] were the first to use both syntactical features (from ASTs) and lexical features (from concrete syntax trees) for code stylometry.They showed how a random forest classifier can take advantage of both kinds of information to achieve an accuracy of 53.91% for the Python language across 229 different authors.Other works followed the Caliskan approach, e.g., Dauber et al. [12].</p>
<p>More recently, the emergence of word embeddings [28] introduced a shift in the representation of author fingerprints from classical machine-learning techniques to deep-learning ones.Deep learning approaches [3], [6], [22] led to better author style representation capabilities, most notably by leveraging LSTM and code2vec [2] architectures.In terms of code stylometry accuracy, this resulted in a bump up to 95.90% with 70 different authors [6].These architectures represent source code using both syntactical and lexical features.</p>
<p>Our work in this paper is a specific instance of the code stylometry task, where we aim to distinguish a specific "AI author" (a code LLM) from human authors.To that end we introduce the use of a transformer-based architecture [41], novel for the code stylometry task.Contrary to more traditional code stylometry work, we do not rely on syntactical features, but solely on lexical features (token stream).</p>
<p>b) AI detection for natural language: Köbis and Mossink [21] observed first how the generative capabilities of LLMs make it difficult to distinguish their (natural language, in this case) output from human-authored text, paving the way to research on the topic.</p>
<p>Early studies [5], [14], [17] approached the problem of AIgenerated natural language using stochastic approaches.They generated AI-labeled samples with GPT-2 [36] and reached accuracies up to 93% [5].</p>
<p>Liao et al. [25] introduced the use of BERT [13] for recognizing AI-generated natural language.They demonstrated that this approach results in superior accuracy (96.7%) compared to traditional machine learning approaches: +7% w.r.t.XGBoost (decision tree) to a fine-tuned BERT architecture.</p>
<p>Mitchell et al. [29] used an approach based solely on probabilities sampled from a generative model, reaching 86% AUROC.Mitrović et al. [30] compared the performance of different approaches that do not need fine-tuning.They show that supervised techniques perform better, with a 14% accuracy increase from a perplexity-based approach (84%) to DistillBERT (98%).</p>
<p>With respect to these works, we focus on AI code stylometry, rather than natural language.We adopt an LLM-based approach (like [25], [30]), fine tuning the T5plus [42] LLM.For dataset generation, we use the open-weight StarCoder2 [27] code LLM.c) AI code stylometry: Hoq et al. [18] looked at the problem of AI code stylometry, for educational plagiarism detection in the context of university computer science class.Their dataset consists of: (1) student-written Java code from a publicly-available dataset encompassing multiple problems with human solutions, and (2) solutions to the same problems generated by ChatGPT.Their approach relied on both syntactical and lexical features extracted from the code, fed to both traditional machine learning techniques (random forest) and deep learning ones, such as code2vec [2].They reached accuracies up to 95% (with code2vec).</p>
<p>Both Bukhari et al. [7] and Idialu et al. [19] followed the same approach on different datasets and programming languages.The former focused on the C language using the Lost at C dataset [40] and Codex [8] for AI-generated snippets.The latter looked at Python code from the CodeChef learning platform, and generated the AI solutions with GPT-4.Both studies used random forest classification, reaching respectively 92% accuracy and 91% F1-score.</p>
<p>Yang et al. [43] replicated for AI code stylometry the probability-based methodology introduced by Mitchell et al. [29] for natural language.They focused on Java and Python, data coming from multiple LLMs from OpenAI, reaching AUCs up to 86.01% for Python and 77.42% for Java.</p>
<p>Oedingen et al. [33] analyzed the discrepancies between fine-tuned methodologies and zero-shot approaches (like Yang et al. [43]), showing how the latter struggle to achieve competitive performances.Using traditional machine learning techniques (XGB with TF-IDF features), they achieved impressive results (98% accuracy) on the detection of Python code generated by ChatGPT.</p>
<p>Rahman et al. [37] followed a similar approach, using a different (but still proprietary) LLM for code generation: Claude 3 haiku [4].They reached 82% accuracy on Python.</p>
<p>Li et al. [23] introduced the use of translations (from either natural language specifications or existing code) to generate the AI-labeled part of training datasets for AI code stylometry.Using translation, they aim to reduce the chance of producing code already present in the training dataset of the code LLM that is to be recognized as an AI author.They considered C++ and Java languages (separately), using both ChatGPT and GPT-4 as generative models.They used random forest classification, with only lexical features, achieving 93% and 97.8% accuracy for C++ and Java, respectively.We depart from previous AI code stylometry work in three ways: (1) we apply for the first time a transformer architecture to the AI code stylometry task; (2) we are able to recognize AI-written code across 10 different programming languages with a single model, achieving an average accuracy of 84.1%;</p>
<p>(3) we rely only on openly-available data and code, enabling scientific reproducibility and future reuse of our work.d) GPTSniffer: Independently and in parallel to our work, Nguyen et al. [31] introduced GPT Sniffer, that tackles the task of detecting AI-generated coming from a different perspective than code stylometry, but also using a transformerbased classifier.Their work is focused on the Java language, uses ChatGPT as a generative model, and considers the impact of different data source domains, such as programming books and data representative of real use-case scenarios that encompass a mixture of snippets from GitHub repositories and generated from ad-hoc queries.GPTSniffer performs really well in domain (100% F1 score training and testing with data provenance from a Java programming book), degrades a lot on out-of-domain (56% F1 with real use-case data for testing), and improves again on a mixed training set (94% F1) and data alteration techniques (96% F1).In comparison to GPTSniffer, we rely exclusively on open models for data generation and classification, ensuring experiment reproducibility, and supports a larger classification scope of 10 distinct programming languages with a single model.</p>
<p>VIII. CONCLUSIONS</p>
<p>AI code stylometry consists of automatically detecting whether an input piece of source code was authored by an AI (e.g., Copilot or ChatGPT) or a human.In this paper, we took a fresh look at the problem by revisiting assumptions made in previous work.</p>
<p>First, we solved the problem in a multilingual setting, supporting 10 different popular programming languages achieving high average accuracy (84.1%) with a single transformedbased classifier, a novel architecture for this task.</p>
<p>Second, our experiments are fully reproducible.As building blocks, we use only openly available data and components, including our code LLM: StarCoder2.We release openly all our artifacts: the novel H-AIRosettaMP dataset consisting of 121 247 code snippets in 10 languages, partly human-written (from Rosetta Code) and partly AI-written (via cross-language code translation); checkpoint of our trained model; and an open source CLI tool to use it in practice.</p>
<p>As future work we plan to analyze how snippet generation impacts detection accuracy, covering: prompt engineering, used code LLM, the provenance language for code translation, as well as starting from natural language prompts.DATA AVAILABILITY A full replication package containing the dataset, a checkpoint of our multilingual model, a command-line tool for using it on selected code snippets, as well as the source code used to run all the experiments presented in this paper is available from Zenodo at https://doi.org/10.5281/zenodo.13908858.</p>
<p>Fig. 4 .
4
Fig. 4. Transformer-based architecture of the Human/AI stylometry classifier.Input source code is tokenized and provided as input to the CodeT5plus encoder, which produces as outputs multiple vectorial representations.The first token (<s> in the figure) is used as input for the classification head, which produces the final class probability.</p>
<p>Fig. 6 .
6
Fig.6.Accuracy of our best-performing monolingual models.The dashed orange line corresponds to the Kotlin row in TableII(in-distribution).The solid blue line shows the accuracy of the same models on test sets with a different language provenance (potentially out-distribution, as is general translating solutions for the same task from different languages to the same language will result in different snippets).Model performances degrade significantly in the latter case.</p>
<p>Translation step
DatasetHumanDATASET CONSTRUCTIONH-AIRosettaMPMODEL TRAININGMONOLINGUALRosetta Code ModelAIbalanced Java ⨆ C++Java ⨆ Java (from C++)undersampled Java ⨆ Java (from C++)MODELS Java with C++ provenance(883 lang.)90ModelsPopular languages filteringRosetta Code (10 lang.)Task balancing90 DatasetsTranslation90 DatasetsUndersampling90 DatasetsTrainingKotlin with Rust provenanceTIOBE index ∩ Starcoder 2 top languagesbalanced Rust ⨆ KotlinRust ⨆ Rust (from Kotlin)undersampled Rust ⨆ Rust (from Kotlin)MULTILINGUAL MODEL Multilingual multi-provenance</p>
<p>TABLE I AVERAGE
I
LENGTH OF CODE SNIPPETS IN THE H-AIROSETTAMP DATASET, BY PROGRAMMING LANGUAGE, MEASURED IN CHARACTERS.T-TEST (α = 0.05) IS COMPUTED BETWEEN THE AI-WRITTEN AND HUMAN-WRITTEN GROUPS OF ALL TRAINED CLASSIFIERS WHEN SOLVING THE AI CODE STYLOMETRY TASK.
LanguageSnippet length (character avg. ± std)t-testAllAI-writtenHuman-written t-statistic95% CIp-valueC++1183 ± 671061 ± 731306 ± 826.72168 to 323&lt; 0.01C1094 ± 571077 ± 911112 ± 441.04-37 to 1070.31C#1248 ± 69 1240 ± 1031257 ± 760.39-74 to 1070.69Go977 ± 63883 ± 791072 ± 725.30114 to 265&lt; 0.01Java1262 ± 88 1245 ± 1021278 ± 101−0.67-134 to 690.51JavaScript933 ± 60787 ± 821079 ± 877.32207 to 377&lt; 0.01Kotlin920 ± 56872 ± 91969 ± 672.6018 to 1780.02Python744 ± 57729 ± 86761 ± 510.96-38 to 1020.35Ruby584 ± 42650 ± 62518 ± 36−5.52 -183 to -82&lt; 0.01Rust992 ± 54909 ± 811076 ± 714.6390 to 243&lt; 0.01TABLE IIACCURACY (%) Language testedProv. languageC++CC#GoJavaJavascript KotlinPython RubyRustProv. accuracy (avg. ± std)Monolingual modelsC++-88.387.891.081.486.291.087.286.284.087.0 ± 2.9C89.9-90.991.592.090.994.189.988.888.890.8 ± 2.0C#75.587.8-93.688.391.592.587.888.388.888.2 ± 4.9Go94.794.787.2-90.995.294.784.090.485.690.8 ± 4.1Java91.592.086.792.0-89.492.587.888.883.089.3 ± 2.9Javascript90.995.288.391.587.7-93.185.690.484.089.6 ± 3.4Kotlin98.492.094.792.694.196.8-94.194.789.994.1 ± 2.4Python92.595.281.990.990.990.996.3-77.685.189.0 ± 5.9Ruby90.496.386.793.187.789.494.178.2-86.289.1 ± 5.0Rust88.397.890.987.394.791.587.888.889.4-90.7 ± 3.3Language accuracy90.293.288.3 91.589.791.392.987.088.386.1-(avg. ± std)±±±±±±±±±±5.93.33.41.73.83.02.34.14.32.3Multilingual modelMultilingual model accuracy88.888.384.089.482.479.387.879.380.881.484.1 ± 3.8Multilingual model F188.888.884.089.382.478.987.878.880.581.184.0 ± 4.0Multilingual model AUC94.194.991.893.989.990.494.690.888.693.992.3 ± 2.1</p>
<p>TABLE II :
II
ANOVA TEST RESULTS (α = 0.05) FOR LANGUAGE ACCURACY; ANOVA TEST FOR PROVENANCE ACCURACY; T-TEST (α = 0.05) FOR MULTILINGUAL MODEL AND (MONOLINGUAL) LANGUAGE ACCURACY.
Valuest/F-statistic95% CIp-valueLang. accuracy (ANOVA)3.56-&lt; 0.001Prov. accuracy (ANOVA)2.00-0.04Multilingual comp. (t-test)4.012.7 to 8.7&lt; 0.001</p>
<p>TABLE IV COMPARISON
IV
[33]EEN THE ACCURACY OF BASELINES CLASSIFIERS[23],[33]AND CLASSIFIERS INTRODUCED IN THIS WORK.BEST RESULTS FOR EACH COLUMN ARE SHOWN IN BOLD.
Tested languageModelJavaC++ PythonJavascriptC#CGoRuby Rust KotlinBaseline modelsRF Java [23]79.3---------J48 Java [23]86.9---------RF C++ [23]-85.5--------J48 C++ [23]-89.5--------XGB-TF-IDF Python [33]--92.6-------Proposed modelsJava (monolingual)94.183.987.790.688.980.4 86.786.685.031.3C++ (monolingual)89.398.488.090.786.789.586.187.086.224.1Python (monolingual)63.182.494.190.258.5 83.6 58.790.784.044.1Multilingual82.488.879.379.384.0 88.389.480.881.487.8
https://github.com/features/copilot/, accessed
-09-24 2 https://openai.com/chatgpt/, accessed 2024-09-24
https://huggingface.co/spaces/isThisYouLLM/Human-Ai
Authorized licensed use limited to the terms of the applicable license agreement with IEEE. Restrictions apply.
https://huggingface.co/datasets/isThisYouLLM/H-AIRosettaMP</p>
<p>Copilot evaluation harness: Evaluating llm-guided software programming. Anisha Agarwal, Aaron Chan, Shubham Chandel, Jinu Jang, Shaun Miller, Roshanak Zilouchian Moghaddam, Yevhen Mohylevskyy, Neel Sundaresan, Michele Tufano, CoRR, abs/2402.142612024</p>
<p>code2vec: learning distributed representations of code. Uri Alon, Meital Zilberstein, Omer Levy, Eran Yahav, 40:1-40:29Proc. ACM Program. Lang. 32019</p>
<p>Source code authorship attribution using long short-term memory based networks. Bander Alsulami, Edwin Dauber, Richard E Harang, Spiros Mancoridis, Rachel Greenstadt, Computer Security -ESORICS 2017 -22nd European Symposium on Research in Computer Security. Lecture Notes in Computer Science. N Simon, Dieter Foley, Einar Gollmann, Snekkenes, Oslo, NorwaySpringerSeptember 11-15, 2017. 201710492Proceedings, Part I</p>
<p>The claude 3 model family: Opus, sonnet. Anthropic, 2024</p>
<p>Real or fake? learning to discriminate machine from human generated text. Anton Bakhtin, Sam Gross, Myle Ott, Yuntian Deng, Marc ' , Aurelio Ranzato, Arthur Szlam, CoRR, abs/1906.033512019</p>
<p>Authorship attribution of source code: a languageagnostic approach and applicability in software engineering. Egor Bogomolov, Vladimir Kovalenko, Yurii Rebryk, Alberto Bacchelli, Timofey Bryksin, ESEC/FSE '21: 29th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering. Marsha Chechik, Massimiliano Di, Penta , Athens, GreeceACMAugust 23-28, 2021. 2021Diomidis Spinellis, Georgios Gousios</p>
<p>Distinguishing AI-and human-generated code: A case study. Sufiyan Bukhari, Benjamin Tan, Lorenzo De, Carli , Proceedings of the 2023 Workshop on Software Supply Chain Offensive Research and Ecosystem Defenses, SCORED 2023. Santiago Torres-Arias, Marcela S Melara, Laurent Simon, Nikos Vasilakis, Kathleen Moriarty, the 2023 Workshop on Software Supply Chain Offensive Research and Ecosystem Defenses, SCORED 2023Copenhagen, DenmarkACM30 November 2023. 2023</p>
<p>. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pondé De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such ; Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech ZarembaJan Leike. 2021Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang,; Andrew N. Carr; Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrewEvaluating large language models trained on code. CoRR, abs/2107.03374</p>
<p>Software heritage: Why and how to preserve software source code. Roberto Di, Cosmo , Stefano Zacchiroli, Proceedings of the 14th International Conference on Digital Preservation. the 14th International Conference on Digital PreservationKyoto, Japan2017. September 25-29, 2017, 2017Shoichiro Hara, Shigeo Sugimoto, and Makoto Goto</p>
<p>GitHub Copilot AI pair programmer: Asset or liability?. Arghavan Moradi Dakhel, Vahid Majdinasab, Amin Nikanjam, Foutse Khomh, Michel C Desmarais, Zhen Ming, J. Syst. Softw. 2031117342023</p>
<p>Git blame who?: Stylistic authorship attribution of small, incomplete source code fragments. Edwin Dauber, Aylin Caliskan, Richard E Harang, Gregory Shearer, Michael J Weisman, Frederica Free-Nelson, Rachel Greenstadt, Proc. Priv. Enhancing Technol. 201932019</p>
<p>BERT: pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019. Long and Short Papers. Jill Burstein, Christy Doran, Thamar Solorio, the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019Minneapolis, MN, USAAssociation for Computational LinguisticsJune 2-7, 2019. 20191</p>
<p>GLTR: statistical detection and visualization of generated text. Sebastian Gehrmann, Hendrik Strobelt, Alexander M Rush, 2019</p>
<p>Revisiting the reproducibility of empirical software engineering studies based on data retrieved from development repositories. M Jesús, Gregorio González-Barahona, Robles, Inf. Softw. Technol. 1641073182023</p>
<p>Deepseek-coder: When the large language model meets programming -the rise of code intelligence. Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y Wu, Y K Li, Fuli Luo, Yingfei Xiong, Wenfeng Liang, CoRR, abs/2401.141962024</p>
<p>Discrimination of human-written and human and machine written sentences using text consistency. Atsumu Harada, Danushka Bollegala, Naiwala P Chandrasiri, 2021 International Conference on Computing, Communication, and Intelligent Systems (ICCCIS). 2021</p>
<p>Detecting chatgpt-generated code in a CS1 course. Muntasir Hoq, Yang Shi, Juho Leinonen, Damilola Babalola, Collin F Lynch, Bita Akram ; Steven Moore, John C Stamper, Richard Jiarui Tong, Chen Cao, Zitao Liu, Xiangen Hu, Yu Lu, Joleen Liang, Hassan Khosravi, Paul Denny, Anjali Singh, Christopher Brooks, Proceedings of the Workshop on Empowering Education with LLMs -the Next-Gen Interface and Content Generation 2023 co-located with 24th International Conference on Artificial Intelligence in Education (AIED 2023). the Workshop on Empowering Education with LLMs -the Next-Gen Interface and Content Generation 2023 co-located with 24th International Conference on Artificial Intelligence in Education (AIED 2023)Tokyo, JapanJuly 7, 2023. 20233487CEUR Workshop Proceedings</p>
<p>Whodunit: Classifying code as human authored or GPT-4 generated-A case study on codechef problems. Oseremen Joy Idialu, Noble Saji Mathews, Rungroj Maipradit, Joanne M Atlee, Meiyappan Nagappan, 21st IEEE/ACM International Conference on Mining Software Repositories, MSR 2024. Lisbon, PortugalACMApril 15-16, 2024. 2024Diomidis Spinellis, Alberto Bacchelli, and Eleni Constantinou</p>
<p>De-anonymizing programmers via code stylometry. Caliskan Aylin, Richard E Islam, Andrew Harang, Arvind Liu, Clare R Narayanan, Fabian Voss, Rachel Yamaguchi, Greenstadt, 24th USENIX Security Symposium, USENIX Security 15. Jaeyeon Jung, Thorsten Holz, Washington, D.C., USAUSENIX AssociationAugust 12-14, 2015. 2015</p>
<p>Artificial intelligence versus maya angelou: Experimental evidence that people cannot differentiate ai-generated from human-written poetry. Nils C Köbis, Luca Mossink, Comput. Hum. Behav. 1141065532021</p>
<p>Building implicit vector representations of individual coding style. Vladimir Kovalenko, Egor Bogomolov, Timofey Bryksin, Alberto Bacchelli, ICSE '20: 42nd International Conference on Software Engineering. Workshops, Seoul, Republic of KoreaACMJune -19 July, 2020. 202027</p>
<p>Discriminating human-authored from chatgpt-generated code via discernable feature analysis. Ke Li, Sheng Hong, Cai Fu, Yunhe Zhang, Ming Liu, 34th IEEE International Symposium on Software Reliability Engineering, ISSRE 2023 -Workshops. Florence, ItalyIEEEOctober 9-12, 2023. 2023</p>
<p>. Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, João Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy, V , Jason T Stillerman, Sankalp Siva, Dmitry Patel, Marco Abulkhanov, Manan Zocca, Zhihan Dey, Nour Zhang, Urvashi Fahmy, Wenhao Bhattacharyya, Swayam Yu, Sasha Singh, Paulo Luccioni, Maxim Villegas, Fedor Kunakov, Manuel Zhdanov, Tony Romero, Nadav Lee, Jennifer Timor, Claire Ding, Hailey Schlesinger, Schoelkopf, Mach. Learn. Res. Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy2023Jan. 2023Arjun GuhaLeandro von Werra, and Harm de Vries. Starcoder: may the source be with you! Trans</p>
<p>Tianming 404 Authorized licensed use limited to the terms of the applicable license agreement with IEEE. Restrictions apply. Liu, and Xiang Li. Differentiate chatgpt-generated and human-written medical texts. Wenxiong Liao, Zhengliang Liu, Haixing Dai, Shaochen Xu, Zihao Wu, Yiyang Zhang, Xiaoke Huang, Dajiang Zhu, Hongmin Cai, CoRR, abs/2304.115672023</p>
<p>Decoupled weight decay regularization. Ilya Loshchilov, Frank Hutter, ICLR 20197th International Conference on Learning Representations. New Orleans, LA, USAMay 6-9, 2019. OpenReview.net, 2019</p>
<p>. Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, Tianyang Liu, Max Tian, Denis Kocetkov, Arthur Zucker, Younes Belkada, Zijian Wang, Qian Liu, Dmitry Abulkhanov, Indraneil Paul, Zhuang Li, Wen-Ding Li, Megan Risdal, Jia Li, Jian Zhu, Terry Yue Zhuo, Evgenii Zheltonozhskii, Nii Osae Osae Dade, Wenhao Yu, Lucas Krauß, Naman Jain, Yixuan Su, Xuanli He, Manan Dey, Edoardo Abati, Yekun Chai, Niklas Muennighoff, Xiangru Tang, Muhtasham Oblokulov, Christopher Akiki, Marc Marone, Chenghao Mou, Mayank Mishra, Alex Gu, Binyuan Hui, Tri Dao, Armel Zebaze, Olivier Dehaene, Nicolas Patry, Canwen Xu, Julian J Mcauley, Han Hu, Torsten Scholak, Sébastien Paquet, Jennifer Robinson, Carolyn Jane Anderson, Nicolas Chapados, 2024Starcoder 2 and the stack v2: The next generation. CoRR, abs/2402.19173</p>
<p>Efficient estimation of word representations in vector space. Tomás Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean, 1st International Conference on Learning Representations, ICLR 2013. Scottsdale, Arizona, USAMay 2-4, 2013. 2013Workshop Track Proceedings</p>
<p>Detectgpt: Zero-shot machine-generated text detection using probability curvature. Eric Mitchell, Yoonho Lee, Alexander Khazatsky, Christopher D Manning, Chelsea Finn, International Conference on Machine Learning, ICML 2023. Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, Jonathan Scarlett, Honolulu, Hawaii, USAPMLR23-29 July 2023. 2023202</p>
<p>Chatgpt or human? detect and explain. explaining decisions of machine learning model for detecting short chatgpt-generated text. Sandra Mitrovic, Davide Andreoletti, Omran Ayoub, CoRR, abs/2301.138522023</p>
<p>Gptsniffer: A codebertbased classifier to detect source code written by chatgpt. Phuong T Nguyen, Juri Di Rocco, Claudio Di Sipio, Riccardo Rubei, Davide Di Ruscio, Massimiliano Di, Penta , J. Syst. Softw. 2141120592024</p>
<p>An empirical comparison of pre-trained models of source code. Changan Niu, Chuanyi Li, Vincent Ng, Dongxiao Chen, Jidong Ge, Bin Luo, 45th IEEE/ACM International Conference on Software Engineering, ICSE 2023. Melbourne, AustraliaIEEEMay 14-20, 2023. 2023</p>
<p>Chatgpt code detection: Techniques for uncovering the source of code. Marc Oedingen, Raphael C Engelhardt, Robin Denz, Maximilian Hammer, Wolfgang Konen, CoRR, abs/2405.155122024</p>
<p>Programming style authorship analysis. Paul W Oman, Curtis R Cook, Computer Trends in the 1990s -Proceedings of the 1989 ACM 17th Annual Computer Science Conference. Arthur M Riehl, Louisville, Kentucky, USAACMFebruary 21-23, 1989. 1989</p>
<p>Tiobe -the software quality company. Marvin Wener, Paul Jansen, Rob Goud, </p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI blog. 1892019</p>
<p>Automatic detection of llm-generated code: A case study of claude 3 haiku. Musfiqur Rahman, Sayedhassan Khatoonabadi, Ahmad Abdellatif, Emad Shihab, arXiv:2409.013822024arXiv preprint</p>
<p>Copyright protection in generative AI: A technical perspective. Jie Ren, Han Xu, Pengfei He, Yingqian Cui, Shenglai Zeng, Jiankun Zhang, Hongzhi Wen, Jiayuan Ding, Hui Liu, Yi Chang, Jiliang Tang, CoRR, abs/2402.023332024</p>
<p>Code llama: Open foundation models for code. Jonas Baptiste Rozière, Fabian Gehring, Sten Gloeckle, Itai Sootla, Gat, Ellen Xiaoqing, Yossi Tan, Jingyu Adi, Tal Liu, Jérémy Remez, Artyom Rapin, Ivan Kozhevnikov, Joanna Evtimov, Manish Bitton, Cristian Bhatt, Aaron Canton-Ferrer, Wenhan Grattafiori, Alexandre Xiong, Jade Défossez, Faisal Copet, Hugo Azhar, Louis Touvron, Nicolas Martin, Thomas Usunier, Gabriel Scialom, Synnaeve, CoRR, abs/2308.129502023</p>
<p>Lost at C: A user study on the security implications of large language model code assistants. Gustavo Sandoval, Hammond Pearce, Teo Nys, Ramesh Karri, Siddharth Garg, Brendan Dolan-Gavitt, 32nd USENIX Security Symposium, USENIX Security 2023. Joseph A Calandrino, Carmela Troncoso, Anaheim, CA, USAUSENIX AssociationAugust 9-11, 2023. 2023</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, Illia Polosukhin, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017. Isabelle Guyon, Samy Ulrike Von Luxburg, Hanna M Bengio, Rob Wallach, S V N Fergus, Roman Vishwanathan, Garnett, Long Beach, CA, USADecember 4-9, 2017. 2017</p>
<p>Codet5+: Open code large language models for code understanding and generation. Yue Wang, Hung Le, Akhilesh Gotmare, D Q Nghi, Junnan Bui, Steven C H Li, Hoi, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023. Houda Bouamor, Juan Pino, Kalika Bali, the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023SingaporeAssociation for Computational LinguisticsDecember 6-10, 2023. 2023</p>
<p>Zero-shot detection of machinegenerated codes. Xianjun Yang, Kexun Zhang, Haifeng Chen, Linda R Petzold, William Yang, Wang , Wei Cheng, CoRR, abs/2310.051032023</p>
<p>GitHub Copilot research recitation -parrot or crow? a first look at rote learning in GitHub Copilot suggestions. Albert Ziegler, 2021</p>            </div>
        </div>

    </div>
</body>
</html>
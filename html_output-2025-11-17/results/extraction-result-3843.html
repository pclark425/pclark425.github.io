<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3843 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3843</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3843</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-93.html">extraction-schema-93</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including agreement rates, qualitative differences, limitations, failure cases, and any aspects that are lost or degraded when using LLMs as judges instead of humans.</div>
                <p><strong>Paper ID:</strong> paper-267069482</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2401.10910v2.pdf" target="_blank">Metacognition is all you need? Using Introspection in Generative Agents to Improve Goal-directed Behavior</a></p>
                <p><strong>Paper Abstract:</strong> Recent advances in Large Language Models (LLMs) have shown impressive capabilities in various applications, yet LLMs face challenges such as limited context windows and difficulties in generalization. In this paper, we introduce a metacognition module for generative agents, enabling them to observe their own thought processes and actions. This metacognitive approach, designed to emulate System 1 and System 2 cognitive processes, allows agents to significantly enhance their performance by modifying their strategy. We tested the metacognition module on a variety of scenarios, including a situation where generative agents must survive a zombie apocalypse, and observe that our system outperform others, while agents adapt and improve their strategies to complete tasks over time.</p>
                <p><strong>Cost:</strong> 0.006</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3843.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3843.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including agreement rates, qualitative differences, limitations, failure cases, and any aspects that are lost or degraded when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-as-a-Judge</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-as-a-Judge evaluation methodology</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of large language models to automatically score and judge agent behavior and conversational outputs in place of full human evaluations; the paper adopts this method (citing Zheng et al.) and performs spot-checking against humans.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_setting</strong></td>
                            <td>LLM-as-a-judge to evaluate generative-agent performance across simulated scenarios (spot-checked against human judgments).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_domain</strong></td>
                            <td>Evaluation of generative agents in simulation (zombie apocalypse, murder mystery, Christmas party) across metrics such as believability, learning, goal performance, higher-level cognition, and scenario outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Mistral 7B (primary); additionally tested with Phi1, Phi2, Llama2, Mixtral, GPT-3.5-turbo, GPT-4 and others.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_rate</strong></td>
                            <td>Cited prior result: 80% agreement with human judges (Zheng et al.); authors' internal spot-checking reported that LLM evaluations were 'just as good as a human judge' but no numeric agreement reported for their own experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_differences</strong></td>
                            <td>Not reported in detail in this paper — authors state spot-checking indicated parity with humans but do not describe qualitative differences or specific aspects that LLM judges miss or degrade (no examples of lost subtleties such as humor, empathy, or cultural context are provided).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>The paper does not report specific failure cases or systematic limitations of using LLMs as judges; the only evaluation-related limitation explicitly noted is that full human evaluation was not used due to resource constraints. No biases, sensitivity failures, or missed content categories are documented.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_strengths</strong></td>
                            <td>Authors report practical strengths: reduced resource/cost needs and acceptable performance; cite Zheng et al.'s 80% agreement and state that spot-checked LLM judgments were 'just as good as a human judge.' They also note choosing Mistral 7B for speed, model size, and strong performance as a pragmatic benefit.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Implicitly reported practices: rely on LLM-as-a-judge when human evaluation is infeasible, standardize on a performant local model (authors standardized on Mistral 7B), and perform spot-checking against human judgments to validate LLM judgments. No formal best-practice protocol or mitigation strategies are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Metacognition is all you need? Using Introspection in Generative Agents to Improve Goal-directed Behavior (Toy et al., 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Metacognition is all you need? Using Introspection in Generative Agents to Improve Goal-directed Behavior', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena <em>(Rating: 2)</em></li>
                <li>Generative Agents: Interactive Simulacra of Human Behavior <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3843",
    "paper_id": "paper-267069482",
    "extraction_schema_id": "extraction-schema-93",
    "extracted_data": [
        {
            "name_short": "LLM-as-a-Judge",
            "name_full": "LLM-as-a-Judge evaluation methodology",
            "brief_description": "Use of large language models to automatically score and judge agent behavior and conversational outputs in place of full human evaluations; the paper adopts this method (citing Zheng et al.) and performs spot-checking against humans.",
            "citation_title": "Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena",
            "mention_or_use": "use",
            "evaluation_setting": "LLM-as-a-judge to evaluate generative-agent performance across simulated scenarios (spot-checked against human judgments).",
            "task_or_domain": "Evaluation of generative agents in simulation (zombie apocalypse, murder mystery, Christmas party) across metrics such as believability, learning, goal performance, higher-level cognition, and scenario outcomes.",
            "llm_model_name": "Mistral 7B (primary); additionally tested with Phi1, Phi2, Llama2, Mixtral, GPT-3.5-turbo, GPT-4 and others.",
            "agreement_rate": "Cited prior result: 80% agreement with human judges (Zheng et al.); authors' internal spot-checking reported that LLM evaluations were 'just as good as a human judge' but no numeric agreement reported for their own experiments.",
            "qualitative_differences": "Not reported in detail in this paper — authors state spot-checking indicated parity with humans but do not describe qualitative differences or specific aspects that LLM judges miss or degrade (no examples of lost subtleties such as humor, empathy, or cultural context are provided).",
            "limitations_or_failure_cases": "The paper does not report specific failure cases or systematic limitations of using LLMs as judges; the only evaluation-related limitation explicitly noted is that full human evaluation was not used due to resource constraints. No biases, sensitivity failures, or missed content categories are documented.",
            "counterexamples_or_strengths": "Authors report practical strengths: reduced resource/cost needs and acceptable performance; cite Zheng et al.'s 80% agreement and state that spot-checked LLM judgments were 'just as good as a human judge.' They also note choosing Mistral 7B for speed, model size, and strong performance as a pragmatic benefit.",
            "recommendations_or_best_practices": "Implicitly reported practices: rely on LLM-as-a-judge when human evaluation is infeasible, standardize on a performant local model (authors standardized on Mistral 7B), and perform spot-checking against human judgments to validate LLM judgments. No formal best-practice protocol or mitigation strategies are provided.",
            "citation": "Metacognition is all you need? Using Introspection in Generative Agents to Improve Goal-directed Behavior (Toy et al., 2024)",
            "uuid": "e3843.0",
            "source_info": {
                "paper_title": "Metacognition is all you need? Using Introspection in Generative Agents to Improve Goal-directed Behavior",
                "publication_date_yy_mm": "2024-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena",
            "rating": 2,
            "sanitized_title": "judging_llmasajudge_with_mtbench_and_chatbot_arena"
        },
        {
            "paper_title": "Generative Agents: Interactive Simulacra of Human Behavior",
            "rating": 1,
            "sanitized_title": "generative_agents_interactive_simulacra_of_human_behavior"
        }
    ],
    "cost": 0.006482999999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Metacognition is all you need? Using Introspection in Generative Agents to Improve Goal-directed Behavior
March 4, 2024</p>
<p>Jason Toy 
Phil Tabor 
Josh Macadam 
Metacognition is all you need? Using Introspection in Generative Agents to Improve Goal-directed Behavior
March 4, 2024C29B5AC5D5A5518A46709F4B229EB26D
Recent advances in Large Language Models (LLMs) have shown impressive capabilities in various applications, yet LLMs face challenges such as limited context windows and difficulties in generalization.In this paper, we introduce a metacognition module for generative agents, enabling them to observe their own thought processes and actions.This metacognitive approach, designed to emulate System 1 and System 2 cognitive processes, allows agents to significantly enhance their performance by modifying their strategy.We tested the metacognition module on a variety of scenarios, including a situation where generative agents must survive a zombie apocalypse, and observe that our system outperform others, while agents adapt and improve their strategies to complete tasks over time.All code can be found at https://replicantlife.com</p>
<p>Introduction</p>
<p>Metacognition refers to the higher-order cognitive processes that involve thinking about one's own thinking.It encompasses a range of mental activities related to monitoring, regulating, and organizing cognitive processes to achieve specific goals.Metacognitive abilities enable individuals to reflect on their knowledge, problem-solving strategies, and learning experiences and therefore play a crucial role in shaping and modifying one's habits.For example, if one is studying for an exam, metacognitive processes might involve setting goals, choosing appropriate study strategies, monitoring their comprehension, and adjusting their approach if they are not understanding the material.</p>
<p>The concept of System 1 and System 2 thinking, popularized by psychologist Daniel Kahneman, provides a framework for understanding metacognition [1].System 1 represents fast, automatic, and intuitive thinking, while system 2 involves slower, deliberate, and reflective thinking.In this framework, metacognition can be thought of as a specific System 2 process that examines actions from both System 1 and System 2 processing.Another analogy is system 1 is subconscious thinking and system 2 is conscious thinking (the voice you hear in your head).Metacognition is a slow, expensive, and methodical thought process and is therefore better suited for introspective or strategic thinking, rather than immediate problem solving.</p>
<p>Metacognition in essence is "thinking about thinking" and requires the ability to look at one's own thoughts and thought processes from different points of view.Reflection, on the other hand, is typically characterized as looking at past experiences and deriving insights for future actions.Metacognition allows one to adjust their thought process and strategy based on asking relevant questions.Due to the dynamic nature of metacognition, no one strategy will be set in stone.Metacognition typically involves asking oneself different questions as a probing mechanism to further understand.Some questions one may see: "What do I know about this topic?","Why do I want to achieve this goal?","How can I monitor my progress towards my goal?", "How can I adjust my strategy to overcome current challenges?", "Metacognition is all you need?" etc. Metacognition is often applied to different types of thinking such as: problem solving, goal setting, reflection, learning, monitoring and evaluation, emotional regulation and other types of cognitive processes.As artificial agents are usually given a specific task, we focus on metacognition related to problem solving, monitoring, and evaluating progress towards their specific task.</p>
<p>Recent work with large language models has incor-arXiv:2401.10910v2[q-bio.NC] 29 Feb 2024 porated human cognitive processes into simulations of interacting generative agents tasked with cooperating to achieve strategic objectives [2].In particular, planning, memory, and reflection have been implemented in an effort to elicit human-like behaviors such as long term planning and cooperation among agents [3].The success of this work raises the question of what role metacognition may play in further enhancing the believability of behaviors of generative agents.</p>
<p>Large Language Models with Cognitive Modules as Generative Agents</p>
<p>Multiple experiments have incorporated metacognition into computational frameworks.Cox et al. [4] outlines a general computational architecture in lisp.Mustafa et al. [5] provides a framework for autonomous vehicles that adds a metacognition layer to monitor safety violations on top of generic reward accumulation.</p>
<p>Krueger, Lieder, and Griffiths [6] created a generic deep reinforcement learning (DRL) framework that incorporates metacognition into traditional reinforcement learning frameworks.</p>
<p>Park et al. [3] demonstrated that LLMs equipped with reflection, observation, and planning modules on agents can successfully mimic believable human behavior in a simulated town environment.Seeding the idea of an agent wanting to host a valentine's party resulted in the agent successfully organizing a party which other agents discussed and attended.</p>
<p>We propose a metacognize module which allows agents to broadly contemplate their circumstances in order to create alternative strategies and improve performance.This subsumes the more tactical reflect and plan functions as presented in Park et al. [3].We show through ablation that agents can learn to continually adapt their strategies depending on the situation.Their overall strategy for dealing with problems affects the specific actions they take.</p>
<p>Architecture</p>
<p>We implement many of the same modules from Park et al. [3], with the addition of a group of modules dubbed meta_cognize.As an agent progresses through the simulation, it accumulates a history of observations, memories, and thoughts.Agents are given goals but can optionally be left blank.When an agent starts towards its goal, it is not given an explicit strategy to follow.Instead, each agent periodically evaluates how it is progressing towards its goals by reviewing memories, thoughts, and past actions.The agent assigns itself a numeric score as well as a text statement for its reasoning for providing that score.This evaluation is stored in its memory as a meta-thought.</p>
<p>If the agent finds that it is not making enough progress, the agent calls its meta_cognize module.When metacognition occurs, the agent asks itself how it might improve its performace in light of what it has learned.Additionally, the agent will periodically selfgenerate new introspective questions to think about its goals from different perspectives.For example, in the zombie apocalypse scenario, an agent initially starts with no goal or strategy, but after some time, we observe an agent contemplate these thoughts:</p>
<p>"How can I survive this zombie apocalypse?What resources do I need?Where should I go for safety?How can I learn from both successes and failures to improve survival strategies?" Depending on the current task and goals of the agent, those questions will change over time, influencing how the agent responds and acts in the environment.Agents have memory that is stored outside of the LLM, where each memory stores content, timestamp, location, importance score, and type of memory.Each time an agent reviews memories for higher cogntive function, memories are ranked by relevance to the speficic question it is considering.Relevance is calculated by cosine similarity of the question and memory embeddings.</p>
<p>Agents have two kinds of memories, a short term and long term memory.Short term memory stores a maximum of seven recent memories and is forgotten after approximately 30 seconds, modeled after human short term memoryMiller [7].Long term storage memory is essentially unlimited and stored in system RAM.Due to limited context window sizes, agents cannot process every memory when making decisions.This is similar to human memory where we may store large amounts of data, but only certain memories can be recalled at a time.</p>
<p>Retrieval-Augmented Generation (RAG) Lewis et al. [8] is a technique created to give LLMs the ability to surface and reference content that the model was not explicitly trained on.Our memory recall system can be seen as a dynamic RAG system.As the agent's system acquires new memories and metacognizes over time, the relevant memories sent to the LLM change.</p>
<p>Anytime an agent makes a decision, relevant memories are retrieved to prime the agent on what to do.</p>
<p>There are several memory types such as observance memories like "John saw a cat" and conversation memories like "John said 'How are you doing Paul?".We explicitly store metacognition memories where an agent looks at its past memories and actions and asks a metaquestion.That thought is stored as a meta-memory and inserted into the memory stream of the agent.In future actions and conversations, these meta-memories are recalled along with other memories to prime the agent to think about these meta-thoughts when in conversation and taking action.For each step in the simulation, we allow the agent to choose an action from a list of possible actions which also includes the meta_cognize function.</p>
<p>Simulation Framework</p>
<p>To conduct these experiments we built a framework dubbed ReplicantLife, where agents can be run standalone or within a simulated town environment.Repli-cantLife has a pluggable architecture which can utilize any popular LLM with an http interface, including locally hosted models through ollama.There is preliminary support for concurrency using threads.Concurrency is limited by GPUs and LLM calls, so max concurrency should be set to the total available GPUs.Feature flags are provided to toggle various functionality including LLM call limits.</p>
<p>Each simulation is created through two JSON environment files which constitute the world and the senario.The world file contains the layout of the map, where static objects are, and boundaries of structures.The scenario files describes the agents, their personalities, goals, locations, meta questions, interview questions, and other attributes.All atttributes can be left out and agents will be initialized with randomized values.Adding a new situation to simulate can be added by defining a new scenario file.</p>
<p>Interview questions are used to evaluate agents at the end of a simulation.Interview questions can be directed to all agents or specific agents.Agents are asked to evaluate their performance with questions such as: "Did you accomplish your goal?", "Who do you suspect is the murderer?"or "What did you learn recently?".Code for the framework can be obtained at https://replicantlife.com.</p>
<p>Experiments</p>
<p>We tested our simulation framework in a variety of different situations including a Christmas party, zombie apocalypse, and murder mystery.In the Christmas party simulation agents hosted a party where multiple other agents were invited and arrived at the specified time to attend.This is similar to the earlier work in generative agents [3] where agents had to coordinate a social activity.</p>
<p>In the zombie apocalypse simulation, zombies are non playable characters that are allowed to kill nonzombie agents.Agents initially have no goal but can develop them over time.Zombies randomly walk and move towards non-zombie agents when seen.Survivors most often self-discovered a strategy of hiding in zombie-free areas.We found that in 73% of zombie scenarios, agents would not survive.</p>
<p>In the murder mystery scenario, one agent is a murderer tasked with killing as many agents as possible.Another agent is a detective, and other agents are common bystanders.We found when using gpt3.5-turboand GPT4, we could not use prompts relating to simulated murder without heavily modifying prompts to bypass safety mechanisms [9].When using Mistral 7B [10] and other open models, we had no prompt blocking issue.</p>
<p>Performance of our cognitive models is shown through ablation.Evaluation metrics are composed of five criteria: believability (how believable and human sounding do the conversations look), learning (are the agents learning over time), individual goal performance (are agents are able to achieve their goals), higher level cognitive performance (are agents observations and conversations converting to higher level thoughts), and overall scenario performance (how many agents survived the zombie apocalypse).We expect the agents to meet new agents, learn about their preferences, learn new locations, and obtain new knowledge through conversations.Additionally, they should pick up new insights, draw conclusions from previous memories, and use these insights for future actions.</p>
<p>To measure performance, we opted not to use full human evaluations due to resource constraints.Instead we opted to use LLMS to assist us in evaluating performance, a technique similary described as LLMas-a-Judge in Zheng et al. [11].In their paper, they found that using an LLM to judge evaluations is 80% in agreement with human judges.We found in our own spot checking of evaluations that LLM performance was just as good as a human judge.The majority of our tests were run using Mistral 7B [10], but we also did extensive testing with Phi1 [12], Phi2 [13] Llama2 [14], Mixtral [15], GPT-3.5-turbo,GPT4 [16], and other models.We standardized on Mistral due to its combination of speed, small model size, and excellent performance.While we did build support for ChatGPT models, we primarily focused on local LLMs for cost and performance reasons.We built out test infrastucture to spin up LLM nodes on public GPU clouds when needed for faster simulations.</p>
<p>To measure performance of our cognitive modules, we ran our scenarios 3 times each for 1000 steps with different cognitive modules turned on.Our experiments show that the metacognition module outperforms all other modules by 33</p>
<p>We also did experiments with realtime systems.With a single agent, we were able to cut down runtime to 2 seconds to process a full request on an RTX 4090 making metacognition generative agents suitable for near realtime systems.With multiple agents, time grows linearly and with 25 agents a single step in game time takes 50 seconds.We believe that multiple agents can be run in near realtime with further optimizations.</p>
<p>Discussion</p>
<p>LLMs are being widely applied across a variety of domains, especially with interactive agents and chat-bots.Agents can elicit disparate functional strengths of LLMs, and their orchestration can result in further higher-order capabilities.The combination of LLMs with a metacognition module allows agents to monitor and adjust strategies to deal with changes that occur over time.This allows for much more powerful agents.</p>
<p>Use Cases</p>
<p>These agents have been developed to work in simulation or used standalone.We see generative agents having widespread use and potential as there is already widespread testing of LLMs nearly every industry.</p>
<p>In the field of psychology, generative agents are being tested to assist individuals in addressing personal problems through conversational interactions.Educational applications involve chatbots that adapt to user preferences, tailoring the learning experience over time.Several companies have integrated chatbots into userfacing customer interactions, such as support chats and customer success conversations.</p>
<p>Multiple organizations are testing generative agents to create friends, companions, and romantic partners to interact with humans.</p>
<p>For generative agents to be successful, they must be believable by acting smarter and able to make similar decisions to a human.Generative agents that have access to their own internal thoughts to improve their actions could potentially improve the realism of humanagent and agent-agent interactions.Unlike humans, these agents operate within a constrained scope, deprived of access to a substantial portion of human sensory data including the nuanced sense of touch.Current integrations of LLMs including ours is a "text in, text out" interface.So all interactions with the world must be converted to text descriptions than an LLM can understand, and then output to a text format that software can interpret.Moreover an absence of tactile perception restricts their ability to comprehend and respond to the physical world in a manner analogous to human experiences.Given these inherent limitations, it becomes imperative to approach interactions with generative agents with a discerning awareness of their boundaries.There is notable progress in multimodal models being combined with vision such as GPT4-V and LLaVA [17] that give LLMs the ability to process images along with text.</p>
<p>Interactive Media As generative AI enters mainstream computation, their use in visual media and video games is increasing rapidly.The integration of generative agents with metacognition modules holds Incorporating generative agents with metacognition into non-player characters (NPCs) can dynamically adapt their strategies within the gaming environment.As players navigate through diverse and unpredictable scenarios, the agents can observe and analyze their own decision-making processes, leading to real-time adjustments in gameplay strategies and developing unique behaviors over time.This adaptability enhances the overall gaming experience, making it more challenging and engaging for players while also creating more immersive and realistic virtual worlds.These dynamic responses also allow agents to influence the storyline providing players with a dynamic and responsive storytelling experience.</p>
<p>Simulation Generative Agents inside a simulation engine can be used for testing and simulating both personal and business cases.Generative agents can serve as valuable tools in personal development simulations.Individuals can engage in simulated conversations to enhance communication skills, receive constructive feedback, and practice decision-making in various scenarios.The metacognition module allows the agent to adapt its coaching strategies based on the user's progress, providing personalized and effective self-improvement experiences.</p>
<p>Agents could also be deployed as teachers for education and training purposes where the agent tailors its teaching method based on the individual's learning style and progress.</p>
<p>For businesses, generative agents have a broad range of potential applications.In one paper from Qian et al. [2], the authors create teams of generative agents with the goal of simulating typical software development process.Agents work together to write specifications, write software, and doing quality assurance testing to build products for end users.</p>
<p>Future Directions</p>
<p>With our current metacognition implementation, we built a base framework that shows increased performance of generative agents to achieve their tasks.We believe our architecture can be improved along several dimensions.</p>
<p>Improved memory retrieval</p>
<p>We found several issues with our memory structure that could be improved.Through our experimentation, we observed that when employing cosine similarity for vector comparison, numerous memories that should be related were, in fact, not pertinent.</p>
<p>In high dimensional spaces, vectors that are similar may not be sementically related.Returning irrelavent memories would effect the output of the LLM and in turn the actions the agent takes.Cosine similarity is what is used in most RAG [8] implementations and so many of these systems will potentially have similar issues of non-relevant context being included.Changing out embedding models could improve performance, but the cosine similarity issue would still remain.As recently adopted in Min et al. [18], improvements can be made to relevance scoring by searching a database based on the LLM's output embedding and employing a K-nearest neighbors (KNN) search algorithm.This process selectively adjusts the output embedding vector prior to token generation.This reverse sequence of operations, wherein LLM embeddings are utilized, has demonstrated superior performance compared to existing methods like RAG, as reported in the literature.Retrieving more relevant memories would likely improve performance in all elements of the system.Further experiments would have us test different memory augmentation and retrieval models.</p>
<p>Inference Speed Testing with smaller models produced inferior results.We believe with time spent on prompt tuning, smaller models may still work efficiently and provide an improvement in inference speed.We would focus on Phi2 and TinyLLama [19], as initial tests showed promising results.We also explored different inference engines.We tested PowerInfer from textcitesong2023powerinfer, an inference engine that exploits high locality in LLM inference.The authors convert common models that use ReLU into format that uses another predictive model to read the input query and selectively choose which neurons to activate.In effect, this reduces the total amount of neurons and computation needed to process a prompt.We did not see speed improvements on the models we tested with.We also tested with vLLM from Kwon et al. [20], an inference engine that uses PagedAttention and saw a 35% speed increase when using concurrency, but further testing is needed.We would want to continue testing with other models, different concurrency systems, and inference speed up techniques to get large simulations to run in realtime.Large groups of simulated agents on non-cloud based machines could be interesting for sandbox games and simulations.</p>
<p>Model optimization Improving models is not just about performance: we are also interested in accuracy and sophistication of responses.Better responses often require a tradeoff with performance as sophisticated responses typically require more data and larger context windows.We have been constrained by GPU memory as we primarily tested on an RTX 4090 with 24 GB of RAM and would like to test with larger models.Another future test would have us dynamically switch out models for different tasks where we use as many smaller models as possible and reserve larger computations for larger models.For example, Phi2, a 2.7 million parameter model uses only 1.7 GB of RAM.Phi2 could be used for simpler prompts such as scoring memory importance, while a larger model such as Llama2 could be reserved for metacognition functions.In Anonymous [21], the authors trained a hybrid LLM that is able to route queries to different LLMs resulting in up to 40% fewer calls.Another approach is to finetune a smaller foundation model that would be optimized for chat and simulations.</p>
<p>Broader Metacognition abilities Our current model predominantly focuses on metacognition in the context of immediate goal achievement.However, metacognitive processes extend well beyond this scope, encompassing a diverse range of aspects such as emotional wellbeing, balancing overarching life goals with immediate objectives, knowledge management, effective time management, adapting to various learning styles, among other aspects.</p>
<p>The complexity and diversity of metacognitive processes in humans are evidenced by the extensive efforts dedicated to understanding and optimizing these processes.This is exemplified by the growing self-help book industry, which aims to aid individuals in developing effective mental frameworks for improved life management.</p>
<p>Further research would be directed towards developing a more broader metacognition framework that would enable an agent to inspect and modify any part of its cognitive processes.We have laid the groundwork necessary to allow the agent to focus on various metacognitive processes.Future experiments would have us guide agents to use different metacognitive processes and inspect if they adopt them sufficiently such as time management in a busy schedule or decision making in the context of multiple conflicting goals.</p>
<p>Metacognition directly in a LLM Our current implementation of metacognition uses Python to essentially graft on metacogntion on top of an LLM.Future investigations may delve into building metacognition like capabilities directly into the LLM, allowing them to introspect and enhance their own decision-making processes.This introspective capability could contribute potentially pave the way for more adaptive and self- aware systems.One interesting view of the human brain is as a coordinate transformation engine: "A brain is a well-designed machine for the frame conversion to internalize the external world" [22], "the egocentric representations of the primary sensory cortical areas must be transformed into an allocentric representation in the hippocampus, and then transformed back to an egocentric motor representation for behavioral output" [23], "Our findings provide compelling evidence that the reference frame of neural representations is not static and can be powerfully modulated by task instructions."[24], "Conjunctive representations among input variables appear in many theoretical models for neural systems that perform coordinate transformations" [25], and "This picture also implies operation of different representations over different timescales... a process of translation between the systems" [26].The human brain has been found to store over 10 different coordinate representations along with orientations or points of view, such as allocentric and egocentric orientations.</p>
<p>A large portion of computational neuroscience studies are devoted to decoding how the brain transforms and integrates between the myriad of neural representations.</p>
<p>Seen through this lens, metacognition can be thought of as neural synapse activity being transformed to other coordinate systems for further inspection.One interesting candidate for this type of computation is the grid cell, located in the Entorhinal Cortex of mammals.</p>
<p>The discovery of grid cells led to a Nobel Prize in medicine in 2014.Grid cells exhibit scale-invariant firing patterns, meaning that the same cells can represent generalization of spatial and non-spatial information across various contexts [27].</p>
<p>Research by Banino et al. [28], Leadholm, Lewis, and Ahmad [29], and others has sought to integrate grid cell computation into neural network architectures.The transformer architecture Vaswani et al. [30] is what powers LLMs.In a paper from Whittington, Warren, and Behrens [31], the authors have shown that when a small modification is made to the transformer architecture, they learn and act like grid cells.</p>
<p>Our hypothesis posits that by adapting the underlying architecture to accommodate more dynamic representation transformations, a neural network can be trained to represent a wider range of data and facilitate metacognitive processes.Achieving this would likely involve formulating a hybrid objective function wherein the model learns to not only predict the next token, but also to evaluate the token's quality in relation to the input query.</p>
<p>Conclusion</p>
<p>We show that metacognition significantly improves performance for task oriented generative agents.Furthermore, we illustrate the potency of combining large language models with traditional programming methods as effective tools for prototyping cognitive systems.</p>
<p>As generative agents integrated with metacogntive abilities approach ubiquity in daily human life, taking on increasingly sophisticated tasks, their proliferation across diverse domains marks a paradigm shift in both lay human-computer interactions and programmercomputer interactions.This shift paves the way for the emergence of more intelligent, adaptive, and contextaware systems.With these advancements in mind, the strategic interplay of metacognition, LLMs, and traditional programming methodologies emerges as a powerful technique for the productionization of intelligent generative agents.While metacognition and system 2 thinking are often hailed as the pinacle of human intelligence, achieving human-level intelligence in computers remains an elusive and unsolved goal.As humans are the sole known species capable of metacognition, further exploration of this dynamic cognitive process becomes a compelling and promising avenue for advancing progress in Artificial General Intelligence.</p>
<p>Figure 1 :
1
Figure 1: Zombie Apocalypse Simulation</p>
<p>Figure 2 :
2
Figure 2: A timeline showing an agent's goals and thoughts change as it interacts in the simulation.</p>
<p>Figure 3 :
3
Figure 3: Graphical representation of metacognition process.</p>
<p>Figure 4 :
4
Figure 4: Graphical representation of the generative agent's cognitive map.</p>
<p>Figure 5 :
5
Figure 5: Comparison of different cognitive modules turned on</p>
<p>Figure 6 :
6
Figure 6: Coordinate Representations in the Brain</p>
<p>Jason Toy, replicantlife.com -jasontoy@gmail.com
Phil Tabor, neuralnet.ai -phil@neuralnet.ai
https://replicantlife.com
https://replicantlife.com
https://replicantlife.com
https://replicantlife.com
https://replicantlife.com
https://replicantlife.com</p>
<p>Thinking, fast and slow. Daniel Kahneman, 2011Macmillan</p>
<p>Communicative Agents for Software Development. Chen Qian, arXiv:2307.07924[cs.SE]2023</p>
<p>Generative Agents: Interactive Simulacra of Human Behavior. Joon Sung, Park , arXiv:2304.03442[cs.HC]2023</p>
<p>Computational Metacognition. Michael Cox, arXiv:2201.12885[cs.AI]2022</p>
<p>Assured Learning-enabled Autonomy: A Metacognitive Reinforcement Learning Framework. Aquib Mustafa, arXiv:2103.12558[cs.AI]2021</p>
<p>Enhancing metacognitive reinforcement learning using reward structures and feedback. M Paul, Falk Krueger, Thomas L Lieder, Griffiths, These authors contributed equally. 2022Department of Psychology, University of California Berkeley</p>
<p>The magical number seven plus or minus two: some limits on our capacity for processing information. George A Miller, 1956</p>
<p>Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. Patrick Lewis, arXiv:2005.11401[cs.CL]2021</p>
<p>Breaking Bad: Unraveling Influences and Risks of User Inputs to ChatGPT for Game Story Generation. Pittawat Taveekitworachai, Lissa Holloway-Attaway and John T. Murray2023Springer Nature SwitzerlandCham</p>
<p>Mistral 7B. Albert Q Jiang, arXiv:2310.06825[cs.CL]2023</p>
<p>Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena. Lianmin Zheng, arXiv:2306.05685[cs.CL]2023</p>
<p>Textbooks Are All You Need II: phi-1.5 technical report. Yuanzhi Li, arXiv:2309.05463[cs.CL]2023</p>
<p>Phi-2: The surprising power of small language models. Marah Abdin, Dec. 2023Microsoft Research Blog</p>
<p>Llama 2: Open Foundation and Fine-Tuned Chat Models. Hugo Touvron, arXiv:2307.09288[cs.CL]2023</p>
<p>Albert Q Jiang, arXiv:2401.04088[cs.LG]Mixtral of Experts. 2024</p>
<p>GPT-4. Openai, arXiv:2303.08774[cs.CL]2023Technical Report</p>
<p>Improved Baselines with Visual Instruction Tuning. Haotian Liu, 2023</p>
<p>SILO Language Models: Isolating Legal Risk In a Nonparametric Datastore. Sewon Min, arXiv:2308.04430[cs.CL]2023</p>
<p>TinyLlama: An Open-Source Small Language Model. Peiyuan Zhang, arXiv:2401.02385[cs.CL]2024</p>
<p>Efficient Memory Management for Large Language Model Serving with PagedAttention. Woosuk Kwon, Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles. 2023. the ACM SIGOPS 29th Symposium on Operating Systems Principles. 2023</p>
<p>Anonymous, Submitted to The Twelfth International Conference on Learning Representations. under review. 2023Hybrid LLM: Cost-Efficient and Quality-Aware Query Routing</p>
<p>Grand unified theory of mind and brain-part i: Space-time approach to dynamic connectomes of c. elegans and human brains by mepmos. Katsushi Arisaka, 2022</p>
<p>Remembering the past and imagining the future: A neural model of spatial memory and imagery. Peter Byrne, Suzanna Becker, Neil Burgess, 10.1037/0033-295X.114.2.340Psychological Review. 1142007</p>
<p>Flexible coding of object motion in multiple reference frames by parietal cortex neurons. R Sasaki, A Anzai, D E Angelaki, 10.1038/s41593-020-0656-0Nature Neuroscience. 23Aug. 2020</p>
<p>Conjunctive representation of position, direction, and velocity in entorhinal cortex. Francesca Sargolini, 10.1126/science.1125572Science. 312May 2006</p>
<p>Spatial memory: how egocentric and allocentric combine. Neil Burgess, 10.1016/j.tics.2006.10.005Trends in Cognitive Sciences. 1012Dec. 2006. 2006 Oct 30</p>
<p>Grid cells and their potential application in AI. Jason Toy, arXiv:2210.120682022q-bio.NC</p>
<p>Vector-based navigation using grid-like representations in artificial agents. Andrea Banino, Nature. 5572018</p>
<p>and Subutai Ahmad. Grid Cell Path Integration For Movement-Based Visual Object Recognition. Niels Leadholm, Marcus Lewis, arXiv:2102.09076[cs.AI]2021</p>
<p>Attention Is All You Need. Ashish Vaswani, arXiv:1706.03762[cs.CL]2017</p>
<p>One-sentence Summary: Transformers learn brain representations and they are algorithmically related to models of the hippocampal formation. C R James, Joseph Whittington, Tim E J Warren, Behrens, International Conference on Learning Representations (ICLR). 2022Relating transformers to models and neural representations of the hippocampal formation</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1913 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1913</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1913</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-39.html">extraction-schema-39</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <p><strong>Paper ID:</strong> paper-282057262</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2510.10642v1.pdf" target="_blank">UniCoD: Enhancing Robot Policy via Unified Continuous and Discrete Representation Learning</a></p>
                <p><strong>Paper Abstract:</strong> Building generalist robot policies that can handle diverse tasks in open-ended environments is a central challenge in robotics. To leverage knowledge from large-scale pretraining, prior work has typically built generalist policies either on top of vision-language understanding models (VLMs) or generative models. However, both semantic understanding from vision-language pretraining and visual dynamics modeling from visual-generation pretraining are crucial for embodied robots. Recent unified models of generation and understanding have demonstrated strong capabilities in both comprehension and generation through large-scale pretraining. We posit that robotic policy learning can likewise benefit from the combined strengths of understanding, planning and continuous future representation learning. Building on this insight, we introduce UniCoD, which acquires the ability to dynamically model high-dimensional visual features through pretraining on over 1M internet-scale instructional manipulation videos. Subsequently, UniCoD is fine-tuned on data collected from the robot embodiment, enabling the learning of mappings from predictive representations to action tokens. Extensive experiments show our approach consistently outperforms baseline methods in terms of 9\% and 12\% across simulation environments and real-world out-of-distribution tasks.</p>
                <p><strong>Cost:</strong> 0.024</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1913.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1913.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>UniCoD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Unified Continuous and Discrete Representation Learning (UniCoD)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A vision-language-action (VLA) model that jointly learns discrete language representations and continuous predictive visual features via a two-stage pretrain→fine-tune pipeline; fuses VLM backbone and a continuous-future prediction expert in a Mixture-of-Transformers to produce action policies for multi-embodiment robotic manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>UniCoD</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Mixture-of-Transformers (MoT) multimodal model with a pretrained Vision-Language Model (VLM) backbone (discrete/text-image branch) plus a generative/prediction expert that forecasts continuous future visual features; an action expert (flow-matching) is added during embodiment-specific fine-tuning. Processes image (single third-person or first-person), text instructions, proprioception, and produces continuous actions (action chunks).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>multimodal: vision-language on image-text / embodied VQA plus multimodal video-text continuous visual prediction (video-based pretraining)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Pretraining used large-scale embodied and web data: reported subsets include ~320k robot videos with fine-grained subtasks (VQA/TI2E), ~870k robot+human operation videos (TI2E), and ~560k generic vision-language QA examples (co-training) — overall >1M instructional manipulation videos; data contains task instructions, subtask descriptions, planning text, scene descriptions and temporal video sequences enabling action-verb and affordance-relevant signals.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Robotic manipulation (language-conditioned vision-language-action policies)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Long-horizon, language-conditioned manipulation across simulated benchmarks (Calvin ABC→D, SimplerEnv WindowX and Google Robot) and two real-world platforms: a 7-DoF Franka Emika Panda arm (7-dim continuous action vector: 6D end-effector delta + binary gripper) and a 12-DoF XArm + dexterous 5-DoF hand; tasks include pick/place, drawer open/close, button press, cable routing, stacking, tool use; action execution uses action-chunks (e.g., 4 or 10 steps) and continuous control outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Yes — pretraining explicitly includes embodied VQA and planning text so discrete language tokens and continuous visual futures are co-trained; the paper emphasizes preserving VLM vision-language alignment during prediction by using a dual-encoder and MoT so semantic concepts (objects, sub-tasks, planning steps) overlap between pretraining and downstream tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Reported SOTA: SimplerEnv-WindowX average success 71.0% (Table 1), SimplerEnv-Google Robot average success 78.4% (Table 3); Calvin long-horizon avg length 4.11 subtasks completed (Table 2). Ablation: pretraining adds ≈+2% absolute success rate in SimplerEnv (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Ablation w/o pretraining: UniCoD (no pretrain) reported lower avg success ~60.8% (Table 4; row 'w/o Pretrain UniCoD' vs 'UniCoD (Ours)' which is 63.0 w/ pretrain — see table for variants); compared baseline π0 reproduced at 49.8% avg (Table 1). (Note: numbers are those reported in paper tables for the specific simulated benchmarks.)</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Paper reports that large-scale planning & prediction pretraining 'accelerates convergence' of the model and future-prediction loss during fine-tuning and yields ≈2% absolute performance gain; no explicit episode/trajectory counts or learning-curve point-to-point sample-efficiency ratios are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>No explicit attention attribution maps or per-head attention visualizations are provided. The model design uses block-wise masking in MoT for modality-specific causal/bidirectional attention, but the paper does not analyze attention patterns on semantic regions or affordances.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Yes — qualitative analyses via PCA + t-SNE in Appendix A.1 compare raw pixels, ViT continuous features, and VQ-GAN discrete tokens for future-observation encodings; findings: pixel features show high variance and temporal mixing; VQ features show temporal collapse ('circling phenomenon'); ViT continuous features yield best temporal separability and clearer frame clustering, motivating continuous-feature prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Moderate evidence. The model jointly predicts continuous visual futures and actions during fine-tuning (flow-matching objective) and ablations show that adding continuous-future prediction boosts downstream action performance (~20% relative uplift vs removing continuous predictive branch), indicating the predictive visual features carry action-relevant dynamics and help ground language-conditioned action selection. Qualitative examples show successful grasps of unseen objects and correct handling of OOD language instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Yes — ablations contrast low-level pixel prediction vs high-level continuous ViT features vs discrete VQ tokens: pixels are noisy for temporal prediction, VQ tokens collapse temporally, and ViT continuous features (higher-level semantics) outperform for prediction and policy learning — implying higher-level semantic features are most beneficial for action learning.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Transfer is improved when pretraining data contains embodied videos and planning/VQA text (domain similarity); choice of continuous encoder matters (SigLIP features, which are 'native' to the VLM, transferred best in experiments); distillation-style architectures helped when encoders were not pre-trained. Poor transfer occurs if VLM alignment is degraded by naive fine-tuning on small robot datasets (motivating two-stage pretrain->fine-tune).</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Paper reports experiments on 'unseen' tasks (novel objects, distractors, visual variations) and claims substantial generalization advantages, with qualitative demos of grasping completely unseen objects; numeric per-task breakdown for seen vs unseen not provided in main text, but authors state UniCoD outperforms baselines on both seen and unseen variants.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>No explicit zero-shot claims. The model is pre-trained on large datasets and then fine-tuned on embodiment-specific trajectories; reported generalization to OOD objects after fine-tuning, but not zero-shot from pretraining alone.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Component-level ablations: removal of continuous-prediction modules (w/o Continuous) and removal of pretraining are evaluated; choice of continuous encoder (UniCoD-Distill / UniCoD-Dino / UniCoD-Siglip) is compared — demonstrating which modules/encoders contribute most to downstream performance. No fine-grained per-transformer-layer freezing/probing analysis is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Paper cites prior observations that naive fine-tuning of VLMs on limited robot data can degrade foundational capabilities (cited literature); their own ablation does not show negative transfer when using their two-stage scheme — instead pretraining helps. They do not report quantified cases within their experiments where language pretraining hurts performance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Yes — compared to video-pretraining / prediction-based baselines (GR-1, VPP) and VLA baselines: UniCoD outperforms GR-1 (Calvin avg length: UniCoD 4.11 vs GR-1 3.06, Table 2) and VPP (VPP 3.58 avg length) showing that combining semantic VLM alignment with continuous future prediction yields better transfer than video-only predictive baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Authors state the UniCoD improvement is consistent 'at every training checkpoint' and that pretraining accelerates convergence of the future-prediction loss during fine-tuning (qualitative convergence observation); no fine-grained training curves with numeric epoch comparisons are given in text.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Partial: PCA is used to reduce features before t-SNE visualizations (Appendix A.1) to visualize temporal structure; no explicit measurements of intrinsic dimensionality or PCA-variance fractions are reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1913.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1913.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>pi0</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>pi 0 (pi-zero): A vision-language-action flow model for general robot control</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A VLA flow-model baseline that uses discrete-token predictive generation (VQ quantization in prior work) to map vision-language inputs to actions; reproduced in this paper for direct architectural comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>pi 0: A vision-language-action flow model for general robot control.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>π0 (pi0)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Vision-language-action flow model that integrates VLM representations and predictive generation (VQ quantization style) to produce actions; uses discrete-token style predictive framework and flow-modeling for action generation in prior work and is reproduced here under the same training/eval settings.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>VLA-style pretraining referenced (uses VQ quantization predictive tasks / video or generative pretraining in prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Prior pi0 approaches incorporate predictive generation tasks; paper does not enumerate pi0's original pretraining data but references it as an architecturally-similar VLA baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Robotic manipulation (generalist VLA)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Evaluated in the same simulated benchmarks (Calvin long-horizon and SimplerEnv) and reproduced under the paper's standardized single-view setup; uses discrete action chunking.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Mentioned: pi0 uses VQ quantization to incorporate predictive generation into VLA policies; however, the paper argues these discrete-only schemes may compromise robust VLM vision-language alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Reproduced performance: SimplerEnv-WindowX average 49.8% (Table 1); Calvin avg length 3.65 subtasks (Table 2); SimplerEnv-Google Robot 51.9% average (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not reported within this paper for pi0 specifically; used as baseline reproduction under same pretraining/settings as UniCoD for fair comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>No explicit sample-efficiency numbers reported for pi0 in this paper; comparison notes UniCoD shows consistent improvements across checkpoints but no sample-count metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>No attention analysis for pi0 in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>No embedding-space analysis for pi0 in this paper; the paper contrasts pi0's discrete prediction approach conceptually with UniCoD's continuous-feature prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>pi0 is designed to predict action sequences (flow-based), but the paper does not provide additional grounding analyses for pi0; UniCoD authors claim continuous features better support action grounding than discrete VQ-style methods used by pi0.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not analyzed for pi0 in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>pi0 performance reported under same data/visual input; the paper finds UniCoD consistently outperforms pi0, implying pi0 is less effective when continuous predictive signals and VLM-alignment are combined as in UniCoD.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>No per-seen/unseen breakdown for pi0 provided here; aggregate metrics include unseen generalization as part of benchmark evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not demonstrated in this paper for pi0.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>No layer-level ablations for pi0 here; pi0 was reproduced as a baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>No explicit negative-transfer experiments for pi0 reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>pi0 is compared to several generation/prediction baselines and VLA models; UniCoD improves >20% relative over reproduced pi0 under identical training/eval according to the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>No temporal-dynamics-of-training analysis specifically for pi0 in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>No dimensionality measurements for pi0.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1913.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1913.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RT-1-X</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RT-1 / RT-1-X (Robotics Transformer family)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer-based robotics policy that transfers vision-language knowledge to real-world control at scale; used as a baseline VLA in simulated and real robot comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Rt-1: Robotics transformer for real-world control at scale.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RT-1 / RT-1-X</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based vision-to-action policy (vision-language-action family) designed for large-scale real-world robot control; processes images and language instructions to output continuous actions; used as a baseline in the experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Vision-language-action style pretraining referenced (web and robot data transfer) in prior work</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not enumerated in this paper; RT-1 historically trained on large datasets of image-action pairs and web-scale supervision (paper only cites it as baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Robotic manipulation (language-conditioned policies)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Benchmarked on SimplerEnv and Calvin under identical single-view input constraints; evaluated on pick/place and other manipulation tasks in simulation and on real robots for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>RT-1 is cited as a VLA approach that transfers web knowledge into robotic control; no quantitative analysis of overlap with downstream task objects/actions in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Reported baseline values in tables: SimplerEnv-WindowX per-task metrics provided; aggregate average in Table 1 shows RT-1-X much lower than UniCoD (per-task numbers in table; overall average low — Table 1 entries). In SimplerEnv-Google Robot Table 3 RT-1-X average reported as 2.4% in that table's formatting (per-task numbers vary); the paper highlights UniCoD's superior averages.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not reported here specifically for RT-1-X.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>No explicit comparative sample-efficiency figures vs UniCoD provided for RT-1-X.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>No attention analysis for RT-1-X in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>No embedding-space analysis for RT-1-X here.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>RT-1-X is used as a baseline; the paper does not re-analyze RT-1-X grounding mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not analyzed here.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>RT-1 family is discussed in context of transferring web knowledge; paper notes more careful post-training is needed for robot tasks to avoid VLM degradation.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>No explicit seen/unseen split results for RT-1-X presented beyond aggregated baseline numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not discussed for RT-1-X in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>No.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>The paper references general problems of VLM fine-tuning on small robotic data (background discussion) but does not present explicit RT-1-X negative transfer experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>RT-1-X is a VLA baseline; UniCoD is reported to outperform it on the evaluated benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>No.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>No.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1913.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1913.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GR-1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GR-1 (video generative pretraining for robot manipulation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A video-pretraining / generative visual foresight baseline that initializes action policies from large-scale video generative pre-training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Unleashing large-scale video generative pre-training for visual robot manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GR-1</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Video generative pretraining model used to initialize action policies; focuses on visual foresight and predictive generation from video to aid policy learning.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Video generative pretraining (vision-only or video-text predictive pretraining depending on prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Large-scale video datasets used for visual foresight; paper cites GR-1 as leveraging video pretraining to initialize policy encoders (no granular dataset breakdown in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Robotic manipulation via video-initialized policies</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Evaluated on Calvin long-horizon benchmark and in comparisons on SimplerEnv as a prediction-based baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>GR-1 emphasizes dynamics modeling from video but lacks large language grounding according to authors' discussion; therefore semantic alignment with text instructions is limited compared to VLM-based methods.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Calvin avg length: GR-1 reported 3.06 (Table 2) vs UniCoD 4.11; GR-1 performs worse than UniCoD on long-horizon language-conditioned tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not separately enumerated; GR-1 itself is primarily video-pretrained (vision/video-focused).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>No explicit sample-efficiency numbers provided for GR-1 in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>No</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>No</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>GR-1 emphasizes modeling dynamics; the paper argues such models lack semantic grounding present in VLM-based approaches and hence are less effective for language-conditioned tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>No</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Works well for dynamics transfer but less well for tasks requiring semantic language grounding; combined strategies (understanding+prediction) are preferred per authors.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not reported in detail here.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>No</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>GR-1 (video-only) is outperformed by UniCoD which combines VLM alignment and continuous prediction (quantified on Calvin and SimplerEnv benchmarks).</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>No</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>No</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1913.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1913.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VPP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>VPP (Video Prediction Policy)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A video foundation model used as a visual encoder for action policies (visual-foresight style); included as a baseline prediction-based method.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Video prediction policy: A generalist robot policy with predictive visual representations.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>VPP</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Uses a video foundation model as the visual encoder for downstream action policies; focuses on continuous visual prediction to support control.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Video foundation model pretraining (video-prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Video datasets for visual foresight; specific datasets not enumerated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Robotic manipulation with predictive visual encoders</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Evaluated in Calvin and SimplerEnv benchmarks as a predictive-visual-representation baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Primarily models dynamics and prediction; lacks VLM-level language grounding according to the paper's critique of pure prediction methods.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Calvin avg length: VPP* 3.58 (Table 2) vs UniCoD 4.11; UniCoD outperforms VPP on long-horizon language-conditioned tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not separately reported.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>No numeric sample-efficiency data provided for VPP in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>No</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>No</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>VPP emphasizes predictive visual features aiding action, but the paper claims that without explicit language grounding these features are less effective for instruction-conditioned tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>No</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Works better for dynamics-heavy tasks; authors suggest combining with language understanding for better transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>No</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>No</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>VPP is a prediction/video-based baseline; UniCoD that integrates VLM alignment outperforms it on language-conditioned manipulation benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>No</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>No</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1913.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1913.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>UP-VLA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>UP-VLA (Unified Understanding and Prediction for Embodied Agent)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A unified understanding-and-prediction model for embodied agents that jointly models language understanding and visual prediction; referenced and reproduced as a related approach.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>UP-VLA: A unified understanding and prediction model for embodied agent.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>UP-VLA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A unified model combining understanding and prediction objectives for embodied agents; uses VLA design ideas similar to UniCoD but differing in prediction/representation specifics.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Multimodal (understanding + prediction) pretraining referenced in prior work</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not explicitly enumerated in this paper; referenced as prior embodied datasets and tasks for co-training.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Language-conditioned robotic manipulation / embodied agent tasks</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Included in Calvin comparison and other VLA baselines under standardized evaluation in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>UP-VLA uses combined understanding and prediction; authors position UniCoD as an evolution focusing on continuous predictions to better preserve VLM alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Calvin avg length: UP-VLA* 4.08 (Table 2) vs UniCoD 4.11 — UP-VLA performs close to UniCoD on this metric under reproduction.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not reported here specifically.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>No explicit sample-efficiency numbers reported.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>No</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>No</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>UP-VLA's unified objectives aim at grounding but this paper does not present additional analysis beyond citing UP-VLA's approach.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>No</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Not deeply analyzed here; compared as a baseline to show UniCoD's marginal gains using continuous-feature prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>No</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>No</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>UP-VLA unifies understanding/prediction whereas pure video methods lack VLM grounding; UniCoD outperforms or matches UP-VLA depending on setting (slight improvement reported).</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>No</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>No</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1913.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1913.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OpenVLA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenVLA (Open-source Vision-Language-Action Model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source VLA architecture used as a baseline that maps vision-language features to robotic actions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Openvla: An open-source vision-language-action model.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenVLA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source vision-language-action model that extracts VLM features and predicts actions for robotic manipulation; used as a comparative baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Vision-language-action pretraining (referenced prior work).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not specified in this paper; cited as an open-source VLA baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Robotic manipulation (language-conditioned)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Benchmarked on SimplerEnv and real robot tasks in comparative evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>OpenVLA leverages VLM representations; paper positions UniCoD as improving over such baselines by adding continuous prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>SimperEnv numbers: Table 1/3 show OpenVLA per-task entries and lower average than UniCoD (e.g., Table 1 WindowX per-task numbers and averages; Table 3 for Google Robot shows OpenVLA avg 24.5%).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>No</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>No</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>No</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Not analyzed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>No</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>No detailed analysis provided.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not reported separately.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>No</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>No</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>No</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>OpenVLA is a VLA baseline; UniCoD outperforms it on the evaluated benchmarks according to tables.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>No</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>No</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1913.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e1913.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Octo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Octo (Open-source Generalist Robot Policy)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source generalist robot policy referenced as a baseline; included in the SimplerEnv comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>An open-source generalist robot policy.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Octo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source generalist robot policy (team paper); treated as a baseline VLA/policy in empirical comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Not detailed in this paper; treated as an externally developed baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Robotic manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Evaluated on SimplerEnv WindowX/Google Robot benchmarks as part of baseline suite.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Not analyzed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Table entries show Octo performance on SimplerEnv (e.g., Table 1 WindowX per-task numbers and overall lower average than UniCoD; Table 3 Google Robot average 11.0%).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>No</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>No</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>No</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>No</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>No</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>No</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>No</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>No</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>No</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>No</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>No direct claim; baseline comparison shows UniCoD outperforms Octo on reported benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>No</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>No</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1913.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e1913.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RoboVLMs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RoboVLMs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A set of VLM-based methods for robot imitation/control referenced and used as a baseline family in SimplerEnv evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RoboVLMs</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Vision-language foundation models adapted for robot imitation/control; cited as baseline(s) in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Vision-language pretraining adapted to robotics (referenced prior work).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not detailed in this paper; treated as externally developed baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Robotic manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Evaluated on SimplerEnv benchmarks (WindowX and Google Robot) with per-task metrics reported in tables.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Not analyzed in detail here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>SimplerEnv-Google Robot Table 3 lists RoboVLMs avg 51.7% (per-table aggregate), SimplerEnv-WindowX shows task-level numbers with mixed performance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>No</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>No</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>No</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Not analyzed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>No</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>No</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>No</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>No</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>No</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>No</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>RoboVLMs are VLM-based; UniCoD claims improvements over these baselines on the reported benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>No</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>No</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1913.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e1913.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Villa-x</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Villa-x (latent action modeling enhancement for VLA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A VLA baseline that enhances latent action modeling and is included in SimplerEnv comparisons; shows strong performance on some tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Villa-x: enhancing latent action modeling in vision-language-action models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Villa-x</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Vision-language-action model with latent-action modeling improvements; used as a state-of-the-art baseline in SimplerEnv evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>VLA pretraining (referenced prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not enumerated here.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Robotic manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Evaluated on SimplerEnv WindowX and Google Robot benchmarks; per-task metrics in Tables 1 and 3.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Not deeply analyzed here; presented as a high-performing VLA baseline on some sub-tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>SimplerEnv-Google Robot Table 3 Villa-x avg 59.6% (Table entry); On WindowX Villa-x reported per-task figures and overall ~62.5% in table formatting.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>No</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>No</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>No</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Not analyzed here.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>No</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>No</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>No</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>No</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>No</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>No</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Villa-x is a VLA baseline; UniCoD outperforms or is competitive across evaluated benchmarks according to reported tables.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>No</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>No</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Rt-1: Robotics transformer for real-world control at scale. <em>(Rating: 2)</em></li>
                <li>pi 0: A vision-language-action flow model for general robot control. <em>(Rating: 2)</em></li>
                <li>Unleashing large-scale video generative pre-training for visual robot manipulation. <em>(Rating: 2)</em></li>
                <li>Video prediction policy: A generalist robot policy with predictive visual representations. <em>(Rating: 2)</em></li>
                <li>UP-VLA: A unified understanding and prediction model for embodied agent. <em>(Rating: 2)</em></li>
                <li>Openvla: An open-source vision-language-action model. <em>(Rating: 1)</em></li>
                <li>Villa-x: enhancing latent action modeling in vision-language-action models. <em>(Rating: 1)</em></li>
                <li>An open-source generalist robot policy. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1913",
    "paper_id": "paper-282057262",
    "extraction_schema_id": "extraction-schema-39",
    "extracted_data": [
        {
            "name_short": "UniCoD",
            "name_full": "Unified Continuous and Discrete Representation Learning (UniCoD)",
            "brief_description": "A vision-language-action (VLA) model that jointly learns discrete language representations and continuous predictive visual features via a two-stage pretrain→fine-tune pipeline; fuses VLM backbone and a continuous-future prediction expert in a Mixture-of-Transformers to produce action policies for multi-embodiment robotic manipulation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "UniCoD",
            "model_description": "Mixture-of-Transformers (MoT) multimodal model with a pretrained Vision-Language Model (VLM) backbone (discrete/text-image branch) plus a generative/prediction expert that forecasts continuous future visual features; an action expert (flow-matching) is added during embodiment-specific fine-tuning. Processes image (single third-person or first-person), text instructions, proprioception, and produces continuous actions (action chunks).",
            "pretraining_type": "multimodal: vision-language on image-text / embodied VQA plus multimodal video-text continuous visual prediction (video-based pretraining)",
            "pretraining_data_description": "Pretraining used large-scale embodied and web data: reported subsets include ~320k robot videos with fine-grained subtasks (VQA/TI2E), ~870k robot+human operation videos (TI2E), and ~560k generic vision-language QA examples (co-training) — overall &gt;1M instructional manipulation videos; data contains task instructions, subtask descriptions, planning text, scene descriptions and temporal video sequences enabling action-verb and affordance-relevant signals.",
            "target_task_name": "Robotic manipulation (language-conditioned vision-language-action policies)",
            "target_task_description": "Long-horizon, language-conditioned manipulation across simulated benchmarks (Calvin ABC→D, SimplerEnv WindowX and Google Robot) and two real-world platforms: a 7-DoF Franka Emika Panda arm (7-dim continuous action vector: 6D end-effector delta + binary gripper) and a 12-DoF XArm + dexterous 5-DoF hand; tasks include pick/place, drawer open/close, button press, cable routing, stacking, tool use; action execution uses action-chunks (e.g., 4 or 10 steps) and continuous control outputs.",
            "semantic_alignment": "Yes — pretraining explicitly includes embodied VQA and planning text so discrete language tokens and continuous visual futures are co-trained; the paper emphasizes preserving VLM vision-language alignment during prediction by using a dual-encoder and MoT so semantic concepts (objects, sub-tasks, planning steps) overlap between pretraining and downstream tasks.",
            "performance_with_language_pretraining": "Reported SOTA: SimplerEnv-WindowX average success 71.0% (Table 1), SimplerEnv-Google Robot average success 78.4% (Table 3); Calvin long-horizon avg length 4.11 subtasks completed (Table 2). Ablation: pretraining adds ≈+2% absolute success rate in SimplerEnv (Table 4).",
            "performance_without_language_pretraining": "Ablation w/o pretraining: UniCoD (no pretrain) reported lower avg success ~60.8% (Table 4; row 'w/o Pretrain UniCoD' vs 'UniCoD (Ours)' which is 63.0 w/ pretrain — see table for variants); compared baseline π0 reproduced at 49.8% avg (Table 1). (Note: numbers are those reported in paper tables for the specific simulated benchmarks.)",
            "sample_efficiency_comparison": "Paper reports that large-scale planning & prediction pretraining 'accelerates convergence' of the model and future-prediction loss during fine-tuning and yields ≈2% absolute performance gain; no explicit episode/trajectory counts or learning-curve point-to-point sample-efficiency ratios are provided.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "No explicit attention attribution maps or per-head attention visualizations are provided. The model design uses block-wise masking in MoT for modality-specific causal/bidirectional attention, but the paper does not analyze attention patterns on semantic regions or affordances.",
            "embedding_space_analysis": "Yes — qualitative analyses via PCA + t-SNE in Appendix A.1 compare raw pixels, ViT continuous features, and VQ-GAN discrete tokens for future-observation encodings; findings: pixel features show high variance and temporal mixing; VQ features show temporal collapse ('circling phenomenon'); ViT continuous features yield best temporal separability and clearer frame clustering, motivating continuous-feature prediction.",
            "action_grounding_evidence": "Moderate evidence. The model jointly predicts continuous visual futures and actions during fine-tuning (flow-matching objective) and ablations show that adding continuous-future prediction boosts downstream action performance (~20% relative uplift vs removing continuous predictive branch), indicating the predictive visual features carry action-relevant dynamics and help ground language-conditioned action selection. Qualitative examples show successful grasps of unseen objects and correct handling of OOD language instructions.",
            "hierarchical_features_evidence": "Yes — ablations contrast low-level pixel prediction vs high-level continuous ViT features vs discrete VQ tokens: pixels are noisy for temporal prediction, VQ tokens collapse temporally, and ViT continuous features (higher-level semantics) outperform for prediction and policy learning — implying higher-level semantic features are most beneficial for action learning.",
            "transfer_conditions": "Transfer is improved when pretraining data contains embodied videos and planning/VQA text (domain similarity); choice of continuous encoder matters (SigLIP features, which are 'native' to the VLM, transferred best in experiments); distillation-style architectures helped when encoders were not pre-trained. Poor transfer occurs if VLM alignment is degraded by naive fine-tuning on small robot datasets (motivating two-stage pretrain-&gt;fine-tune).",
            "novel_vs_familiar_objects": "Paper reports experiments on 'unseen' tasks (novel objects, distractors, visual variations) and claims substantial generalization advantages, with qualitative demos of grasping completely unseen objects; numeric per-task breakdown for seen vs unseen not provided in main text, but authors state UniCoD outperforms baselines on both seen and unseen variants.",
            "zero_shot_or_few_shot": "No explicit zero-shot claims. The model is pre-trained on large datasets and then fine-tuned on embodiment-specific trajectories; reported generalization to OOD objects after fine-tuning, but not zero-shot from pretraining alone.",
            "layer_analysis": "Component-level ablations: removal of continuous-prediction modules (w/o Continuous) and removal of pretraining are evaluated; choice of continuous encoder (UniCoD-Distill / UniCoD-Dino / UniCoD-Siglip) is compared — demonstrating which modules/encoders contribute most to downstream performance. No fine-grained per-transformer-layer freezing/probing analysis is reported.",
            "negative_transfer_evidence": "Paper cites prior observations that naive fine-tuning of VLMs on limited robot data can degrade foundational capabilities (cited literature); their own ablation does not show negative transfer when using their two-stage scheme — instead pretraining helps. They do not report quantified cases within their experiments where language pretraining hurts performance.",
            "comparison_to_vision_only": "Yes — compared to video-pretraining / prediction-based baselines (GR-1, VPP) and VLA baselines: UniCoD outperforms GR-1 (Calvin avg length: UniCoD 4.11 vs GR-1 3.06, Table 2) and VPP (VPP 3.58 avg length) showing that combining semantic VLM alignment with continuous future prediction yields better transfer than video-only predictive baselines.",
            "temporal_dynamics": "Authors state the UniCoD improvement is consistent 'at every training checkpoint' and that pretraining accelerates convergence of the future-prediction loss during fine-tuning (qualitative convergence observation); no fine-grained training curves with numeric epoch comparisons are given in text.",
            "dimensionality_analysis": "Partial: PCA is used to reduce features before t-SNE visualizations (Appendix A.1) to visualize temporal structure; no explicit measurements of intrinsic dimensionality or PCA-variance fractions are reported.",
            "uuid": "e1913.0"
        },
        {
            "name_short": "pi0",
            "name_full": "pi 0 (pi-zero): A vision-language-action flow model for general robot control",
            "brief_description": "A VLA flow-model baseline that uses discrete-token predictive generation (VQ quantization in prior work) to map vision-language inputs to actions; reproduced in this paper for direct architectural comparison.",
            "citation_title": "pi 0: A vision-language-action flow model for general robot control.",
            "mention_or_use": "use",
            "model_name": "π0 (pi0)",
            "model_description": "Vision-language-action flow model that integrates VLM representations and predictive generation (VQ quantization style) to produce actions; uses discrete-token style predictive framework and flow-modeling for action generation in prior work and is reproduced here under the same training/eval settings.",
            "pretraining_type": "VLA-style pretraining referenced (uses VQ quantization predictive tasks / video or generative pretraining in prior work)",
            "pretraining_data_description": "Prior pi0 approaches incorporate predictive generation tasks; paper does not enumerate pi0's original pretraining data but references it as an architecturally-similar VLA baseline.",
            "target_task_name": "Robotic manipulation (generalist VLA)",
            "target_task_description": "Evaluated in the same simulated benchmarks (Calvin long-horizon and SimplerEnv) and reproduced under the paper's standardized single-view setup; uses discrete action chunking.",
            "semantic_alignment": "Mentioned: pi0 uses VQ quantization to incorporate predictive generation into VLA policies; however, the paper argues these discrete-only schemes may compromise robust VLM vision-language alignment.",
            "performance_with_language_pretraining": "Reproduced performance: SimplerEnv-WindowX average 49.8% (Table 1); Calvin avg length 3.65 subtasks (Table 2); SimplerEnv-Google Robot 51.9% average (Table 3).",
            "performance_without_language_pretraining": "Not reported within this paper for pi0 specifically; used as baseline reproduction under same pretraining/settings as UniCoD for fair comparison.",
            "sample_efficiency_comparison": "No explicit sample-efficiency numbers reported for pi0 in this paper; comparison notes UniCoD shows consistent improvements across checkpoints but no sample-count metrics.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "No attention analysis for pi0 in this paper.",
            "embedding_space_analysis": "No embedding-space analysis for pi0 in this paper; the paper contrasts pi0's discrete prediction approach conceptually with UniCoD's continuous-feature prediction.",
            "action_grounding_evidence": "pi0 is designed to predict action sequences (flow-based), but the paper does not provide additional grounding analyses for pi0; UniCoD authors claim continuous features better support action grounding than discrete VQ-style methods used by pi0.",
            "hierarchical_features_evidence": "Not analyzed for pi0 in this paper.",
            "transfer_conditions": "pi0 performance reported under same data/visual input; the paper finds UniCoD consistently outperforms pi0, implying pi0 is less effective when continuous predictive signals and VLM-alignment are combined as in UniCoD.",
            "novel_vs_familiar_objects": "No per-seen/unseen breakdown for pi0 provided here; aggregate metrics include unseen generalization as part of benchmark evaluation.",
            "zero_shot_or_few_shot": "Not demonstrated in this paper for pi0.",
            "layer_analysis": "No layer-level ablations for pi0 here; pi0 was reproduced as a baseline.",
            "negative_transfer_evidence": "No explicit negative-transfer experiments for pi0 reported here.",
            "comparison_to_vision_only": "pi0 is compared to several generation/prediction baselines and VLA models; UniCoD improves &gt;20% relative over reproduced pi0 under identical training/eval according to the paper.",
            "temporal_dynamics": "No temporal-dynamics-of-training analysis specifically for pi0 in this paper.",
            "dimensionality_analysis": "No dimensionality measurements for pi0.",
            "uuid": "e1913.1"
        },
        {
            "name_short": "RT-1-X",
            "name_full": "RT-1 / RT-1-X (Robotics Transformer family)",
            "brief_description": "A transformer-based robotics policy that transfers vision-language knowledge to real-world control at scale; used as a baseline VLA in simulated and real robot comparisons.",
            "citation_title": "Rt-1: Robotics transformer for real-world control at scale.",
            "mention_or_use": "use",
            "model_name": "RT-1 / RT-1-X",
            "model_description": "Transformer-based vision-to-action policy (vision-language-action family) designed for large-scale real-world robot control; processes images and language instructions to output continuous actions; used as a baseline in the experiments.",
            "pretraining_type": "Vision-language-action style pretraining referenced (web and robot data transfer) in prior work",
            "pretraining_data_description": "Not enumerated in this paper; RT-1 historically trained on large datasets of image-action pairs and web-scale supervision (paper only cites it as baseline).",
            "target_task_name": "Robotic manipulation (language-conditioned policies)",
            "target_task_description": "Benchmarked on SimplerEnv and Calvin under identical single-view input constraints; evaluated on pick/place and other manipulation tasks in simulation and on real robots for comparison.",
            "semantic_alignment": "RT-1 is cited as a VLA approach that transfers web knowledge into robotic control; no quantitative analysis of overlap with downstream task objects/actions in this paper.",
            "performance_with_language_pretraining": "Reported baseline values in tables: SimplerEnv-WindowX per-task metrics provided; aggregate average in Table 1 shows RT-1-X much lower than UniCoD (per-task numbers in table; overall average low — Table 1 entries). In SimplerEnv-Google Robot Table 3 RT-1-X average reported as 2.4% in that table's formatting (per-task numbers vary); the paper highlights UniCoD's superior averages.",
            "performance_without_language_pretraining": "Not reported here specifically for RT-1-X.",
            "sample_efficiency_comparison": "No explicit comparative sample-efficiency figures vs UniCoD provided for RT-1-X.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "No attention analysis for RT-1-X in this paper.",
            "embedding_space_analysis": "No embedding-space analysis for RT-1-X here.",
            "action_grounding_evidence": "RT-1-X is used as a baseline; the paper does not re-analyze RT-1-X grounding mechanisms.",
            "hierarchical_features_evidence": "Not analyzed here.",
            "transfer_conditions": "RT-1 family is discussed in context of transferring web knowledge; paper notes more careful post-training is needed for robot tasks to avoid VLM degradation.",
            "novel_vs_familiar_objects": "No explicit seen/unseen split results for RT-1-X presented beyond aggregated baseline numbers.",
            "zero_shot_or_few_shot": "Not discussed for RT-1-X in this paper.",
            "layer_analysis": "No.",
            "negative_transfer_evidence": "The paper references general problems of VLM fine-tuning on small robotic data (background discussion) but does not present explicit RT-1-X negative transfer experiments.",
            "comparison_to_vision_only": "RT-1-X is a VLA baseline; UniCoD is reported to outperform it on the evaluated benchmarks.",
            "temporal_dynamics": "No.",
            "dimensionality_analysis": "No.",
            "uuid": "e1913.2"
        },
        {
            "name_short": "GR-1",
            "name_full": "GR-1 (video generative pretraining for robot manipulation)",
            "brief_description": "A video-pretraining / generative visual foresight baseline that initializes action policies from large-scale video generative pre-training.",
            "citation_title": "Unleashing large-scale video generative pre-training for visual robot manipulation.",
            "mention_or_use": "use",
            "model_name": "GR-1",
            "model_description": "Video generative pretraining model used to initialize action policies; focuses on visual foresight and predictive generation from video to aid policy learning.",
            "pretraining_type": "Video generative pretraining (vision-only or video-text predictive pretraining depending on prior work)",
            "pretraining_data_description": "Large-scale video datasets used for visual foresight; paper cites GR-1 as leveraging video pretraining to initialize policy encoders (no granular dataset breakdown in this paper).",
            "target_task_name": "Robotic manipulation via video-initialized policies",
            "target_task_description": "Evaluated on Calvin long-horizon benchmark and in comparisons on SimplerEnv as a prediction-based baseline.",
            "semantic_alignment": "GR-1 emphasizes dynamics modeling from video but lacks large language grounding according to authors' discussion; therefore semantic alignment with text instructions is limited compared to VLM-based methods.",
            "performance_with_language_pretraining": "Calvin avg length: GR-1 reported 3.06 (Table 2) vs UniCoD 4.11; GR-1 performs worse than UniCoD on long-horizon language-conditioned tasks.",
            "performance_without_language_pretraining": "Not separately enumerated; GR-1 itself is primarily video-pretrained (vision/video-focused).",
            "sample_efficiency_comparison": "No explicit sample-efficiency numbers provided for GR-1 in this paper.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "No",
            "embedding_space_analysis": "No",
            "action_grounding_evidence": "GR-1 emphasizes modeling dynamics; the paper argues such models lack semantic grounding present in VLM-based approaches and hence are less effective for language-conditioned tasks.",
            "hierarchical_features_evidence": "No",
            "transfer_conditions": "Works well for dynamics transfer but less well for tasks requiring semantic language grounding; combined strategies (understanding+prediction) are preferred per authors.",
            "novel_vs_familiar_objects": "Not reported in detail here.",
            "zero_shot_or_few_shot": "Not reported.",
            "layer_analysis": "No",
            "negative_transfer_evidence": "Not reported.",
            "comparison_to_vision_only": "GR-1 (video-only) is outperformed by UniCoD which combines VLM alignment and continuous prediction (quantified on Calvin and SimplerEnv benchmarks).",
            "temporal_dynamics": "No",
            "dimensionality_analysis": "No",
            "uuid": "e1913.3"
        },
        {
            "name_short": "VPP",
            "name_full": "VPP (Video Prediction Policy)",
            "brief_description": "A video foundation model used as a visual encoder for action policies (visual-foresight style); included as a baseline prediction-based method.",
            "citation_title": "Video prediction policy: A generalist robot policy with predictive visual representations.",
            "mention_or_use": "use",
            "model_name": "VPP",
            "model_description": "Uses a video foundation model as the visual encoder for downstream action policies; focuses on continuous visual prediction to support control.",
            "pretraining_type": "Video foundation model pretraining (video-prediction)",
            "pretraining_data_description": "Video datasets for visual foresight; specific datasets not enumerated in this paper.",
            "target_task_name": "Robotic manipulation with predictive visual encoders",
            "target_task_description": "Evaluated in Calvin and SimplerEnv benchmarks as a predictive-visual-representation baseline.",
            "semantic_alignment": "Primarily models dynamics and prediction; lacks VLM-level language grounding according to the paper's critique of pure prediction methods.",
            "performance_with_language_pretraining": "Calvin avg length: VPP* 3.58 (Table 2) vs UniCoD 4.11; UniCoD outperforms VPP on long-horizon language-conditioned tasks.",
            "performance_without_language_pretraining": "Not separately reported.",
            "sample_efficiency_comparison": "No numeric sample-efficiency data provided for VPP in this paper.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "No",
            "embedding_space_analysis": "No",
            "action_grounding_evidence": "VPP emphasizes predictive visual features aiding action, but the paper claims that without explicit language grounding these features are less effective for instruction-conditioned tasks.",
            "hierarchical_features_evidence": "No",
            "transfer_conditions": "Works better for dynamics-heavy tasks; authors suggest combining with language understanding for better transfer.",
            "novel_vs_familiar_objects": "Not reported.",
            "zero_shot_or_few_shot": "Not reported.",
            "layer_analysis": "No",
            "negative_transfer_evidence": "No",
            "comparison_to_vision_only": "VPP is a prediction/video-based baseline; UniCoD that integrates VLM alignment outperforms it on language-conditioned manipulation benchmarks.",
            "temporal_dynamics": "No",
            "dimensionality_analysis": "No",
            "uuid": "e1913.4"
        },
        {
            "name_short": "UP-VLA",
            "name_full": "UP-VLA (Unified Understanding and Prediction for Embodied Agent)",
            "brief_description": "A unified understanding-and-prediction model for embodied agents that jointly models language understanding and visual prediction; referenced and reproduced as a related approach.",
            "citation_title": "UP-VLA: A unified understanding and prediction model for embodied agent.",
            "mention_or_use": "use",
            "model_name": "UP-VLA",
            "model_description": "A unified model combining understanding and prediction objectives for embodied agents; uses VLA design ideas similar to UniCoD but differing in prediction/representation specifics.",
            "pretraining_type": "Multimodal (understanding + prediction) pretraining referenced in prior work",
            "pretraining_data_description": "Not explicitly enumerated in this paper; referenced as prior embodied datasets and tasks for co-training.",
            "target_task_name": "Language-conditioned robotic manipulation / embodied agent tasks",
            "target_task_description": "Included in Calvin comparison and other VLA baselines under standardized evaluation in this paper.",
            "semantic_alignment": "UP-VLA uses combined understanding and prediction; authors position UniCoD as an evolution focusing on continuous predictions to better preserve VLM alignment.",
            "performance_with_language_pretraining": "Calvin avg length: UP-VLA* 4.08 (Table 2) vs UniCoD 4.11 — UP-VLA performs close to UniCoD on this metric under reproduction.",
            "performance_without_language_pretraining": "Not reported here specifically.",
            "sample_efficiency_comparison": "No explicit sample-efficiency numbers reported.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "No",
            "embedding_space_analysis": "No",
            "action_grounding_evidence": "UP-VLA's unified objectives aim at grounding but this paper does not present additional analysis beyond citing UP-VLA's approach.",
            "hierarchical_features_evidence": "No",
            "transfer_conditions": "Not deeply analyzed here; compared as a baseline to show UniCoD's marginal gains using continuous-feature prediction.",
            "novel_vs_familiar_objects": "Not reported.",
            "zero_shot_or_few_shot": "Not reported.",
            "layer_analysis": "No",
            "negative_transfer_evidence": "No",
            "comparison_to_vision_only": "UP-VLA unifies understanding/prediction whereas pure video methods lack VLM grounding; UniCoD outperforms or matches UP-VLA depending on setting (slight improvement reported).",
            "temporal_dynamics": "No",
            "dimensionality_analysis": "No",
            "uuid": "e1913.5"
        },
        {
            "name_short": "OpenVLA",
            "name_full": "OpenVLA (Open-source Vision-Language-Action Model)",
            "brief_description": "An open-source VLA architecture used as a baseline that maps vision-language features to robotic actions.",
            "citation_title": "Openvla: An open-source vision-language-action model.",
            "mention_or_use": "use",
            "model_name": "OpenVLA",
            "model_description": "Open-source vision-language-action model that extracts VLM features and predicts actions for robotic manipulation; used as a comparative baseline.",
            "pretraining_type": "Vision-language-action pretraining (referenced prior work).",
            "pretraining_data_description": "Not specified in this paper; cited as an open-source VLA baseline.",
            "target_task_name": "Robotic manipulation (language-conditioned)",
            "target_task_description": "Benchmarked on SimplerEnv and real robot tasks in comparative evaluations.",
            "semantic_alignment": "OpenVLA leverages VLM representations; paper positions UniCoD as improving over such baselines by adding continuous prediction.",
            "performance_with_language_pretraining": "SimperEnv numbers: Table 1/3 show OpenVLA per-task entries and lower average than UniCoD (e.g., Table 1 WindowX per-task numbers and averages; Table 3 for Google Robot shows OpenVLA avg 24.5%).",
            "performance_without_language_pretraining": "Not provided.",
            "sample_efficiency_comparison": "No",
            "has_sample_efficiency_data": false,
            "attention_analysis": "No",
            "embedding_space_analysis": "No",
            "action_grounding_evidence": "Not analyzed in this paper.",
            "hierarchical_features_evidence": "No",
            "transfer_conditions": "No detailed analysis provided.",
            "novel_vs_familiar_objects": "Not reported separately.",
            "zero_shot_or_few_shot": "No",
            "layer_analysis": "No",
            "negative_transfer_evidence": "No",
            "comparison_to_vision_only": "OpenVLA is a VLA baseline; UniCoD outperforms it on the evaluated benchmarks according to tables.",
            "temporal_dynamics": "No",
            "dimensionality_analysis": "No",
            "uuid": "e1913.6"
        },
        {
            "name_short": "Octo",
            "name_full": "Octo (Open-source Generalist Robot Policy)",
            "brief_description": "An open-source generalist robot policy referenced as a baseline; included in the SimplerEnv comparisons.",
            "citation_title": "An open-source generalist robot policy.",
            "mention_or_use": "use",
            "model_name": "Octo",
            "model_description": "Open-source generalist robot policy (team paper); treated as a baseline VLA/policy in empirical comparisons.",
            "pretraining_type": "Not detailed in this paper; treated as an externally developed baseline.",
            "pretraining_data_description": "Not specified here.",
            "target_task_name": "Robotic manipulation",
            "target_task_description": "Evaluated on SimplerEnv WindowX/Google Robot benchmarks as part of baseline suite.",
            "semantic_alignment": "Not analyzed in this paper.",
            "performance_with_language_pretraining": "Table entries show Octo performance on SimplerEnv (e.g., Table 1 WindowX per-task numbers and overall lower average than UniCoD; Table 3 Google Robot average 11.0%).",
            "performance_without_language_pretraining": "Not provided.",
            "sample_efficiency_comparison": "No",
            "has_sample_efficiency_data": false,
            "attention_analysis": "No",
            "embedding_space_analysis": "No",
            "action_grounding_evidence": "No",
            "hierarchical_features_evidence": "No",
            "transfer_conditions": "No",
            "novel_vs_familiar_objects": "No",
            "zero_shot_or_few_shot": "No",
            "layer_analysis": "No",
            "negative_transfer_evidence": "No",
            "comparison_to_vision_only": "No direct claim; baseline comparison shows UniCoD outperforms Octo on reported benchmarks.",
            "temporal_dynamics": "No",
            "dimensionality_analysis": "No",
            "uuid": "e1913.7"
        },
        {
            "name_short": "RoboVLMs",
            "name_full": "RoboVLMs",
            "brief_description": "A set of VLM-based methods for robot imitation/control referenced and used as a baseline family in SimplerEnv evaluations.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "RoboVLMs",
            "model_description": "Vision-language foundation models adapted for robot imitation/control; cited as baseline(s) in experiments.",
            "pretraining_type": "Vision-language pretraining adapted to robotics (referenced prior work).",
            "pretraining_data_description": "Not detailed in this paper; treated as externally developed baselines.",
            "target_task_name": "Robotic manipulation",
            "target_task_description": "Evaluated on SimplerEnv benchmarks (WindowX and Google Robot) with per-task metrics reported in tables.",
            "semantic_alignment": "Not analyzed in detail here.",
            "performance_with_language_pretraining": "SimplerEnv-Google Robot Table 3 lists RoboVLMs avg 51.7% (per-table aggregate), SimplerEnv-WindowX shows task-level numbers with mixed performance.",
            "performance_without_language_pretraining": "Not provided.",
            "sample_efficiency_comparison": "No",
            "has_sample_efficiency_data": false,
            "attention_analysis": "No",
            "embedding_space_analysis": "No",
            "action_grounding_evidence": "Not analyzed in this paper.",
            "hierarchical_features_evidence": "No",
            "transfer_conditions": "No",
            "novel_vs_familiar_objects": "No",
            "zero_shot_or_few_shot": "No",
            "layer_analysis": "No",
            "negative_transfer_evidence": "No",
            "comparison_to_vision_only": "RoboVLMs are VLM-based; UniCoD claims improvements over these baselines on the reported benchmarks.",
            "temporal_dynamics": "No",
            "dimensionality_analysis": "No",
            "uuid": "e1913.8"
        },
        {
            "name_short": "Villa-x",
            "name_full": "Villa-x (latent action modeling enhancement for VLA)",
            "brief_description": "A VLA baseline that enhances latent action modeling and is included in SimplerEnv comparisons; shows strong performance on some tasks.",
            "citation_title": "Villa-x: enhancing latent action modeling in vision-language-action models.",
            "mention_or_use": "use",
            "model_name": "Villa-x",
            "model_description": "Vision-language-action model with latent-action modeling improvements; used as a state-of-the-art baseline in SimplerEnv evaluations.",
            "pretraining_type": "VLA pretraining (referenced prior work)",
            "pretraining_data_description": "Not enumerated here.",
            "target_task_name": "Robotic manipulation",
            "target_task_description": "Evaluated on SimplerEnv WindowX and Google Robot benchmarks; per-task metrics in Tables 1 and 3.",
            "semantic_alignment": "Not deeply analyzed here; presented as a high-performing VLA baseline on some sub-tasks.",
            "performance_with_language_pretraining": "SimplerEnv-Google Robot Table 3 Villa-x avg 59.6% (Table entry); On WindowX Villa-x reported per-task figures and overall ~62.5% in table formatting.",
            "performance_without_language_pretraining": "Not provided.",
            "sample_efficiency_comparison": "No",
            "has_sample_efficiency_data": false,
            "attention_analysis": "No",
            "embedding_space_analysis": "No",
            "action_grounding_evidence": "Not analyzed here.",
            "hierarchical_features_evidence": "No",
            "transfer_conditions": "No",
            "novel_vs_familiar_objects": "No",
            "zero_shot_or_few_shot": "No",
            "layer_analysis": "No",
            "negative_transfer_evidence": "No",
            "comparison_to_vision_only": "Villa-x is a VLA baseline; UniCoD outperforms or is competitive across evaluated benchmarks according to reported tables.",
            "temporal_dynamics": "No",
            "dimensionality_analysis": "No",
            "uuid": "e1913.9"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Rt-1: Robotics transformer for real-world control at scale.",
            "rating": 2
        },
        {
            "paper_title": "pi 0: A vision-language-action flow model for general robot control.",
            "rating": 2
        },
        {
            "paper_title": "Unleashing large-scale video generative pre-training for visual robot manipulation.",
            "rating": 2
        },
        {
            "paper_title": "Video prediction policy: A generalist robot policy with predictive visual representations.",
            "rating": 2
        },
        {
            "paper_title": "UP-VLA: A unified understanding and prediction model for embodied agent.",
            "rating": 2
        },
        {
            "paper_title": "Openvla: An open-source vision-language-action model.",
            "rating": 1
        },
        {
            "paper_title": "Villa-x: enhancing latent action modeling in vision-language-action models.",
            "rating": 1
        },
        {
            "paper_title": "An open-source generalist robot policy.",
            "rating": 1
        }
    ],
    "cost": 0.0235765,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>UNICOD: ENHANCING ROBOT POLICY VIA UNI-FIED CONTINUOUS AND DISCRETE REPRESENTATION LEARNING
12 Oct 2025</p>
<p>Jianke Zhang 
Yucheng Hu 
Yanjiang Guo 
Xiaoyu Chen 
Yichen Liu 
Institute for Interdisciplinary Information Sciences
Tsinghua University</p>
<p>Wenna Chen 
Peking University</p>
<p>Chaochao Lu 
Shanghai AI Lab</p>
<p>Jianyu Chen 
Equal Contribution </p>
<p>Shanghai Qizhi Institute</p>
<p>UNICOD: ENHANCING ROBOT POLICY VIA UNI-FIED CONTINUOUS AND DISCRETE REPRESENTATION LEARNING
12 Oct 2025E9BAFF2518B55B115A3B716EFC202EE5arXiv:2510.10642v1[cs.RO]
Building generalist robot policies that can handle diverse tasks in open-ended environments is a central challenge in robotics.To leverage knowledge from largescale pretraining, prior work has typically built generalist policies either on top of vision-language understanding models (VLMs) or generative models.However, both semantic understanding from vision-language pretraining and visual dynamics modeling from visual-generation pretraining are crucial for embodied robots.Recent unified models of generation and understanding have demonstrated strong capabilities in both comprehension and generation through large-scale pretraining.We posit that robotic policy learning can likewise benefit from the combined strengths of understanding, planning and continuous future representation learning.Building on this insight, we introduce UniCoD, which acquires the ability to dynamically model high-dimensional visual features through pretraining on over 1M internet-scale instructional manipulation videos.Subsequently, UniCoD is fine-tuned on data collected from the robot embodiment, enabling the learning of mappings from predictive representations to action tokens.Extensive experiments show our approach consistently outperforms baseline methods in terms of 9% and</p>
<p>INTRODUCTION</p>
<p>Constructing generalist foundation models (Zitkovich et al., 2023;Kim et al., 2024b) for robots manipulation in the physical world has emerged as a rapidly growing frontier within embodied AI.Vision-language-action (VLA) models aim to learn robotic policies from data annotated with vision, linguistic, and action signals.However, the scarcity of robotic data and the heterogeneity across embodiments present substantial challenges, particularly in achieving generalization to novel scenes and task instructions, and in accurately predicting actions.</p>
<p>To mitigate these limitations, recent studies have explored mapping Vision-Language Models (VLMs) into the action space (Black et al., 2024;Team et al., 2024).This strategy provides robot policies with alignment priors across language and vision modalities.Nevertheless, these approaches often overlook the fundamental discrepancies between robotic action tasks and vision-language tasks.Unlike the abundance of internet-scale vision-language data, fine-tuning VLMs on limited robotic datasets frequently leads to degradation of their foundational capabilities (Xing et al., 2025).Complementary lines of work have investigated leveraging generation models as intermediaries for action policy learning (Hu et al., 2024;Wen et al., 2024).While such visual foresight approaches facilitate dynamic representation learning and enable the use of heterogeneous data sources, they typically fail to preserve vision-language alignment inherent to pretrained VLMs.These observations highlight a central insight: it is crucial to design robot-specific post-training paradigms tailored to embodied scenarios.Upon re-examining this line of approaches, we observe that both language understanding and future state prediction can provide preliminary guidance for general manipulation tasks.The unified learning strategy further enables the model to acquire representations beneficial for robotic tasks from a broader range of data.</p>
<p>Building upon these insights and prior advances in vision-language-action (VLA) research (Zhang et al., 2025;Wang et al., 2025b), we propose UniCoD, which follows an understanding-generation-execution paradigm that integrates discrete task comprehension with continuous prediction of future robotic states.To address heterogeneous modalities, UniCoD employs a MOT architecture (Liang et al., 2024) with modality-specialized experts.UniCoD is trained in two stages to introduce continuous feature forecasting to action learning while maintaining general capabilities within VLM.In the first stage, we curate and label a diverse collection of Embodied QA data sourced from both robots (Khazatsky et al., 2024;Bu et al., 2025;Wu et al., 2024) and human demonstrations (Hoque et al., 2025;Grauman et al., 2022).We enable the model to learn discrete language representations for understanding of embodied scenes and continuous visual representations for world modeling.In the second stage, we introduce embodiment-specific robotic data annotated with action behaviors.By jointly predicting continuous visual futures and actions, the model learns to utilize semantically aligned features that are rich in dynamic information.This, in turn, equips the VLA policy with better generalization capabilities for new objects and scenes.</p>
<p>In experiments, UniCoD achieves a 9% improvement in the Simpler benchmark compared to existing SOTA approach and demonstrates strong semantic generalization for real-world robots for complex tasks on both robot arms and dexterous hands.In summary, our contributions are as follows:</p>
<p>• We propose a novel vision-language-action (VLA) that integrates both discrete and continuous representations for understanding and learning dynamics, which is pre-trained on large-scale data from both robot and human demonstrations, enabling effective transfer to embodied tasks.</p>
<p>• We propose a two-stage training framework that aligns action representations while preserving the aligned intermediate representations.</p>
<p>• Our best-performing model achieves state-of-the-art results across both simulated and real-world environments, and we further analyze the impact of different feature design choices on the model's capabilities.</p>
<p>RELATED WORKS</p>
<p>Vision-Language-Action Models Vision-Language-Action (VLA) models introduce multimodal large language models (Dai et al., 2024;Touvron et al., 2023;Wang et al., 2025a;Bai et al., 2025) into robot policy models to enhance their generalization ability (Brohan et al., 2023;Kim et al., 2024a;Black et al., 2024;Guo et al., 2025).This line of work either utilizes the VLM and an action head for end-to-end action prediction (Li et al., 2023;Wen et al., 2025) or uses the VLM to extract key information to condition downstream policy (Zhang et al., 2024;Li et al., 2025).Some recent works have introduced additional auxiliary tasks to VLAs, including enhancing spatial understanding (Qu et al., 2025), QA reasoning (Zhou et al., 2025), visual reasoning (Zhao et al., 2025) and prediction (Zhang et al., 2025), demonstrating that both general-purpose understanding and generation capabilities can promote action learning.However, these methods are primarily limited to unifying generative tasks within a discrete token prediction framework, which may compromise the robust vision-language alignment inherent in the pre-trained VLM.In this work, we incorporate a continuous-space visual prediction task to aid downstream action learning.</p>
<p>Generalist Robot Policies with Joint Prediction Explorations into generalist robot policies have considered using world models (Blattmann et al., 2023;Assran et al., 2025;Chen et al., 2024;Guo et al., 2024) to learn physical dynamics and subsequently predict actions (Du et al., 2024;Black et al., 2023).Many recent methods have incorporated prediction into larger-scale data and models: GR-1 (Wu et al., 2023) utilizes video pre-training to initialize the action policy; VPP (Hu et al., 2024) uses a video foundation model as the visual encoder for action policy.While these methods fully leverage the rich information from video data, they lack semantic grounding capabilities due to the absence of large language models.Recent works (Zhang et al., 2025;Wang et al., 2025b) use VQ quantization to incorporate predictive generation tasks into VLA policies, demonstrating the potential for unifying understanding and prediction.In contrast, we utilize continuous visual features as the prediction supervision signal and pre-train our model on large-scale language prediction and continuous visual prediction tasks.In this section, we present the overall framework design and the two-stage training strategy of UniCoD, as illustrated in Figure 2. In the first stage, UniCoD is trained to learn joint text-image representations across diverse manipulation datasets, including understanding, planning, and continuous future prediction tasks.In the subsequent stage, an action expert is employed to integrate the multimodal inputs and predicted future states with action.In the subsequent subsections, we will respectively describe: (1) the joint visual-language embedding learning for pre-training in Sec 3.1, (2) our policy learning method in Sec 3.2, and (3) the implementation details and training data in Sec 3.3.</p>
<p>METHODOLOGY</p>
<p>UNIFIED VISION LANGUAGE JOINT EMBEDDING MODELING</p>
<p>Before introducing the robot action space, we first establish a cross-embodiment pre-training paradigm for robots.In this stage, a subset of the model parameters U v,l is jointly optimized via the Text-Image to Embedding(TI2E) (examples can be found in A.5). Concretely, given a language instruction l and the current view observations o t at time t, UniCoD is trained to predict the joint visual-text embedding: ôt+h , l = U v,l (o t , l) , where ôt+h = V (o t+h ) = {c 1 , c 2 , . . ., c n } denotes the predicted continuous future representation encoded by the visual encoder V , while l = {d 1 , d 2 , . . ., d m } corresponds to the m-token textual sequence.</p>
<p>Discrete Representation Learning.To enhance vision-language alignment, the parameters U v,l are initialized from a pre-trained vision-language model.Fine-grained language representations are derived from large-scale vision-language datasets, as well as planning and scene descriptions from embodied tasks, which are annotated using pre-trained MLLM into a VQA-style format.This target enables the agent to gain a better understanding of diverse instructions and scenes, thereby facilitating the learning of continuous representations for visual prediction and action.</p>
<p>World Modeling under Continuous Space.In the pre-training stage, to acquire dynamic representations associated with the action space, we introduce additional attention weights dedicated to future state prediction, which are integrated with the original VLM within the mixture-of-transformers framework.Unlike prior approaches that directly predict image pixels, we leverage a frozen visual encoder to represent future observations in a continuous high-dimensional space, capturing high-level information across different semantics.A more detailed discussion can be found in Appendix A.1.</p>
<p>For the visual inputs, we employ a dual-encoder design that combines the VLM visual encoder with a generator encoder.The tokens generated by the latter are processed by the generative expert in the mixture-of-transformers and, together with the language tokens and VLM visual tokens, jointly participate in the attention computation.This design preserves the pretrained model's vision-language alignment while enabling the prediction process to benefit from richer semantic understanding.</p>
<p>Training Objective.The visual and language inputs are processed respectively through the MoT framework, then autoregresively generate lpred t+h = d pred 1:m , while the generation expert obtains the ôpred t+h = c pred 1:n .We follow the standard setup of generative-understanding models, employing cross-entropy loss for the language branch and mean squared error loss for the generative branch.This optimaze progress can be formulated as:
L 1 = λ 1 • 1 n n i=1 c pred i − c i 2 2 − (1 − λ 1 ) • 1 m m j=1 logP θ (d j | d &lt;j , l, o t )(1)
where λ 1 serves as a weighting factor to balance the loss contributions of the discrete and continuous representations.</p>
<p>UNIFIED ACTION MODELING</p>
<p>In the previous stage, we obtained U v,l through pre-training, which endowed the model with basic capabilities in future state prediction and vision-language alignment.However, U v,l cannot yet be directly mapped to the action space.To address this limitation, in the second stage we fine-tune U v,l on embodiment data comprising visual, language, and action modalities, while simultaneously training an action expert from scratch to construct U v,l,a .</p>
<p>Action &amp; State Expert.Similar to the generation and understanding experts, we employ distinct attention weights to project actions and states (i.e., proprioception) into a shared attention space.Unlike the other experts, the action expert leverages flow matching to capture the continuous and inherently multi-modal distribution of the action space.Proprioceptive signals s t are processed by an MLP-based state expert encoder, enabling fusion within the unified model.Given an action sequence A t = (a t , a t+1 , . . ., a t+h ) to be executed, along with the observation o t and instruction l, the unified model U v,l,a is trained to approximate vector fields as:
L flow = E τ ∼U (0,1) E {At,ot,st,l}∼D ∥U v,l,a (A τ t , o t , s t , l, τ ) − (A t − A τ t )∥ 2 2 ,(2)
where A τ t = (1 − τ )ϵ + τ A t denotes the interpolated actions at step τ , and ϵ ∼ N (0, I).</p>
<p>In this action training stage, we also jointly optimize the generation expert by predicting the future observation states c 1:n , yielding the following objective:
L 2 = λ 2 • 1 n n i=1 c pred i − c i 2 2 + (1 − λ 2 )L flow .(3</p>
<p>EXPERIMENT</p>
<p>To comprehensively evaluate our proposed method, UniCoD, we conduct extensive experiments across two simulation benchmarks and on two distinct real-world robotic platforms.Our experiments are designed to assess the performance of UniCoD and validate the effectiveness of our proposed modules.</p>
<p>EXPERIMENTAL SETUP</p>
<p>Our experiments are conducted and deployed across four distinct environments.Figure 3 illustrates a selection of tasks from both our simulation and real-world settings.</p>
<p>Figure 3: Our evaluation environments, including 2 simulation benchmarks and 2 real-world embodiments.</p>
<p>Calvin Benchmark Calvin is a simulation benchmark designed for evaluating long-horizon, language-conditioned manipulation policies.We employ the ABC-D split to evaluate the single-view generalization capabilities of the models.The evaluation suite includes 1,000 long-horizon sequences, each of length 5. We report the average length of completed sub-task sequences.</p>
<p>SimplerEnv Benchmark SimplerEnv is a simulation benchmark designed to evaluate policies trained on real-world datasets, such as Bridge-V2 and Fractal.The benchmark supports two types of robot arms: WindowX and Google Robot.For our evaluation, we conduct 240 runs for each task and report the average success rate.</p>
<p>Real-World Franka Emika Panda Arm We deploy models on a Franka Emika arm for real-world task comparison.We first collected a dataset of 2,000 trajectories spanning over 20 distinct tasks, encompassing six fundamental skills: picking, placing, opening a drawer, closing a drawer, pressing a button, and routing a cable.We evaluate performance on both seen and unseen task variations.</p>
<p>The unseen category primarily involves grasping novel objects not present in the training data and introducing misleading objects.More details can be found in Appendix A.3.1.</p>
<p>Real-World XArm with 12-DOF X-Hand On our dexterous manipulation platform, we train different models using a dataset of 4,000 trajectories across more than 100 tasks.The models are then evaluated in a variety of seen and unseen scenarios, which cover 13 distinct skills in 9 categories.More details can be found in Appendix A.3.2.</p>
<p>SIMULATION EXPERIMENTS</p>
<p>Implementation Details We first pre-train UniCoD following the methodology described in Section 3. Subsequently, we fine-tune the model on 8 A100 GPUs for 22k steps, using a learning rate of 5 × 10 −5 and a batch size of 1024.For all simulation training, we consistently use a single, third-person-view image of size 224 × 224 as the visual input.In Calvin, we use an action chunk size of 10, and during deployment, the full 10-step chunk is executed at each inference step.In SimplerEnv, we use an action chunk size of 4; for the WindowX environment (corresponding to the Bridge dataset), the full 4-step chunk is executed, whereas for the Google Robot environment (corresponding to the Fractal dataset), half of the action chunk is executed.</p>
<p>Baselines We compare UniCoD against several state-of-the-art VLAs and prediction-based policies.</p>
<p>On SimplerEnv, we benchmark UniCoD against RT-1-X (Brohan et al., 2022), Octo (Team et al., 2024), OpenVLA (Kim et al., 2024a), RoboVLMs (Liu et al., 2025), SpatialVLA (Qu et al., 2025), π 0 (Black et al., 2024), CogAct (Li et al., 2024) and Villa-x (Chen et al., 2025).On Calvin, we compare UniCoD against several policies that leverage visual generation tasks, including GR-1 (Wu et al., 2023), π 0 (Black et al., 2024), VPP (Hu et al., 2024), and UP-VLA (Zhang et al., 2025).To ensure a fair comparison, we reproduce these baselines and standardize their visual input to a single third-person view.For π 0 , we specifically use the implementation from the open-pi-zero and report its performance under the same training and evaluation setup used in UniCoD for a direct comparison.Tables 1 and 3 present the performance of our method on the SimplerEnv-WindowX and SimplerEnv-Google Robot benchmarks, respectively.We report the officially published results of other methods for comparison.On both robotic platforms, our method achieves the highest success rates of 71.0% and 78.4%, attaining state-of-the-art (SOTA) performance.We highlight the top-performing and second-best methods for each task category in bold and with an underline.It is evident that UniCoD demonstrates consistently high success rates across all sub-tasks.This contrasts with other methods, which often exhibit "spiky" performance profiles-excelling on some tasks while performing poorly on others.This finding underscores the superior multi-task learning capabilities of our approach.</p>
<p>PERFORMANCE ON SIMULATION BENCHMARKS</p>
<p>Furthermore, for a fair, apple-to-apple comparison with the architecturally similar π 0 baseline, we reproduced it within our identical training and evaluation framework.Across both environments, we found that the novel components in UniCoD yield a significant performance uplift of over 20%.We also observed that this improvement is consistently present at every training checkpoint, indicating that the stable gains can be attributed to our method's ability to learn continuous future features and discrete representations simultaneously.Moreover, when compared to the baseline pi0, our method again exhibits a performance improvement, consistent with the results on SimplerEnv.Implementation Details We fine-tune the pre-trained UniCoD model separately on the datasets collected from our two real-world robotic platforms to evaluate its performance on a variety of seen and unseen tasks.The fine-tuning process is conducted for 10 epochs using a batch size of 1024 and a learning rate of 5 × 10 −5 , with both the prediction horizon and action chunk length set to 10.For the Franka Emika Panda arm, the model is fine-tuned on 2,000 trajectories, and during deployment, we evaluate both full and half action chunk execution, reporting the superior result.On the XArm with a 12-DOF dexterous hand, we use a larger dataset of 4,000 trajectories and execute the full 10-step action chunk at each inference step.We test on seen tasks, which involve familiar objects in novel, randomized positions, and unseen tasks, which introduce novel color, objects, and background.For each task configuration, we conduct 20 trials from randomized initial configurations and report the average task success rate.More details can be found in Appendix A.3.</p>
<p>REAL WORLD EXPERIMENTS</p>
<p>PERFORMANCE ON REAL WORLD EXPERIMENTS</p>
<p>We compare UniCoD against OpenVLA (Kim et al., 2024a), GR-1 (Wu et al., 2023), π 0 (Black et al., 2024), UP-VLA (Zhang et al., 2025) and VPP (Hu et al., 2024) in two environments, visualizing the results in Figure 4 and 5. Our method achieves the highest overall task success rates on both real-world robotic platforms.Specifically, on the Franka Panda arm, UniCoD attains the best performance across all four task categories, outperforming baselines on both seen and unseen tasks.This demonstrates that our approach effectively enhances both multi-task learning and generalization capabilities.Consistent with our findings in the Simpler simulation environment, our method again shows superior performance over the architecturally similar π 0 baseline across a majority of these real-world tasks.Furthermore, on the more complex 12-DoF dexterous hand platform, UniCoD achieves the highest average success rate across all nine skill categories.Notably, we observe that our method exhibits a significant generalization advantage when dealing with novel objects and scenes.</p>
<p>Figure 4: Results on real-world 7DOF robotarm experiment.More detailed quantitative results are provided in Table 6.</p>
<p>Figure 5: Results on real-world 12-DOF dexterous hands experiment.More detailed quantitative results can be found in Table 7.</p>
<p>We provide several illustrative examples in Appendix A.4, where the model successfully grasps completely unseen objects and correctly interprets out-of-distribution (OOD) language descriptions.</p>
<p>These consistent, state-of-the-art results across two morphologically distinct robots validate the effectiveness and broad applicability of our proposed method.</p>
<p>ABLATION STUDY</p>
<p>In this section, we conduct a series of ablation studies to validate the effectiveness of the different components within UniCoD.These experiments investigate the role of our continuous visual representations, the impact of our large-scale pre-training phase involving both language and visual prediction, and a comparison of several continuous vision encoding methods proposed in Sec 3.All ablation studies are conducted in the Simpler simulation environment, following the same training and evaluation protocols described in Sec 4.2.4. We evaluate the following without using pretraining: (1) w/o Continuous (π 0 ), where the modules for predicting continuous future features (including the auxiliary prediction expert and its corresponding encoder/decoder) are removed.</p>
<p>(2) w/Pred, which predicts future raw pixels using a two-layer MLP.This helps us elucidate the trade-offs between using high-level visual features versus raw pixels as the predictive signal.The results in w/o Pretrain section of the table show that our proposed continuous visual feature prediction boosts performance by approximately 20%.Furthermore, the comparison with w/Pred reveals that continuous features are indeed a more effective signal for future prediction, enabling the model to extract dynamic information crucial for action generation.</p>
<p>Effectiveness of Large-Scale Planning and Prediction Pre-training Table 4 also presents a comparison between UniCoD with and without pre-training.Overall, pre-training improves the success rate across all tasks, yielding a performance gain of approximately 2%.During fine-tuning, we observe that leveraging large-scale external data for future and language prediction accelerates the model's convergence on the robotics dataset.This effect is particularly pronounced in the convergence of the future prediction loss.This indicates that our joint pre-training scheme, which combines continuous and discrete prediction, provides a superior model initialization, especially for the prediction expert module, which translates to tangible benefits during downstream fine-tuning.Table 5: Ablation study on choice of continuous vision features.</p>
<p>Choice of Continuous Visual Prediction</p>
<p>We further compare the different encoding methods for future prediction proposed in our methodology.Specifically, we evaluate three distinct approaches (all without pre-training), with results on both Simpler environments shown in Table 5: (1) UniCoD-Distill, which takes the input embeddings of the ViT (from the current frame) as input to the prediction expert and predicts the output features of ViT for the future frame.This approach is analogous to distilling knowledge from the ViT encoder itself.</p>
<p>(2) UniCoD-Dino and (3) UniCoD-Siglip, which take the output features of their respective vision encoders (DINO Siméoni et al. (2025) or SigLIP Tschannen et al. ( 2025)) for the current frame as input to predict the corresponding features for the future frame.</p>
<p>The results show that UniCoD-Siglip demonstrates better performance on both benchmarks, and consequently, we select SigLIP as the vision encoder for our UniCoD model.Notably, on Google Robot environment, UniCoD-Distill achieves better performance than the UniCoD-Siglip when neither is pre-trained.This suggests that the distillation-style architecture has inherent advantages.In contrast, UniCoD-Dino performs significantly worse than the other two.This is likely because the DINO feature space is not aligned with the VLM backbone.Conversely, since SigLIP is the native vision encoder for Paligemma, its feature space is naturally more aligned with that of the VLM expert, facilitating more effective integration within the prediction expert.</p>
<p>CONCLUSION</p>
<p>In this paper, we introduce UniCoD, a Vision-Language-Action (VLA) framework that enhances policy learning by integrating discrete token prediction with continuous visual prediction.During the pre-training stage, we leverage embodied VQA and robotic planning tasks to align the discrete language features of a Vision-Language Model (VLM).Concurrently, we train a predictive module on large-scale video data to forecast future continuous visual features.These two components-the VLM backbone and the prediction module-are effectively fused using a Mixture-of-Experts (MoE) Transformer architecture.In the subsequent action fine-tuning stage, an action expert is incorporated, and the entire model is fine-tuned on a joint objective of continuous action generation and future feature prediction.Our method achieves state-of-the-art (SOTA) performance in two distinct simulation environments.Furthermore, on real-world hardware, including a 7-DoF robot arm and a 12-DoF dexterous hand, our model demonstrates superior performance and stronger semantic generalization, particularly when handling novel objects not encountered during training.</p>
<p>A APPENDIX A.1 QUALITATIVE COMPARISON OF ENCODED FUTURE VISUAL REPRESENTATIONS</p>
<p>To qualitatively analyze the characteristics of different encoding methods, we visualize the features they produce.Specifically, we compare features from a single robot trajectory encoded in three ways: raw image pixels, continuous visual features from a ViT encoder, and discrete visual tokens from a VQ-GAN.We selected a trajectory from the Fractal dataset corresponding to the instruction pick the coffee bag from the drawer onto the table.For each frame, the resulting features-raw pixels (flattened from 224 × 224 × 3), ViT features (flattened from 256 × 1152), and VQ-VAE tokens (2048-dim)-are first reduced to 50 dimensions via PCA and then projected into a 2D space using t-SNE for visualization.Figure 6 illustrates the t-SNE visualizations for the trajectory encoded by these three methods.To highlight the temporal evolution, feature points from adjacent frames are connected by lines.</p>
<p>• Pixel Features (Left): This encoding preserves the most low-level information.We observe that despite small visual changes between consecutive frames, the corresponding pixel-level features exhibit high variance, often jumping into regions occupied by features from distant timesteps.This suggests that using raw pixel values as a predictive signal could mislead the policy by causing it to over-emphasize low-level, high-frequency changes.• ViT vs. VQ Features (Center and Right): A comparison reveals a distinct "circling phenomenon" in the VQ-GAN visualization, where features from many different timesteps collapse into a dense central region.This indicates poor temporal separability in the context of manipulation trajectories.In contrast, the ViT features provide the best separation of the three methods, organizing features from different frames into distinct, minimally overlapping clusters.</p>
<p>This qualitative analysis supports our insight that continuous features, by virtue of focusing on high-level semantic information, serve as a more stable and suitable predictive signal for robot action policies within our framework.</p>
<p>A.2 DETAILS ABOUT SIMULATION BENCHMARKS</p>
<p>Calvin Benchmark Calvin is a simulation benchmark designed for evaluating long-horizon, language-conditioned manipulation policies.It comprises four distinct environments (A, B, C, and D) and offers evaluation splits such as ABC-D and ABCD-D.In our experiments, we employ the ABC-D split to evaluate the single-view generalization capabilities of the models.Models are trained on data collected from environments A, B, and C, and subsequently evaluated in the unseen  Table 8 summarizes the datasets employed during pre-training.To creating the robot vqa data, we employ Gemini 2.5 (Comanici et al., 2025) to annotate text descriptions and task planning for a subset of video data.The RoboMind dataset inherently contains overall task descriptions and sub-tasks, which can be directly utilized as vision-language question-answer pairs.</p>
<p>Table 8: Datasets and the number of samples used for TI2E task and VQA task.</p>
<p>Task name Dataset name Number of samples</p>
<p>AgibotWorld (Bu et al., 2025) 120k Galaxea Open-World (Jiang et al., 2025) 99k Robomind (Wu et al., 2024) 20k TI2E</p>
<p>Droid (Khazatsky et al., 2024) 76k Bridge (Walke et al., 2023) 55k Egodex (Hoque et al., 2025) 320k Ego4D (Grauman et al., 2022) 500k</p>
<p>AgibotWorld VQA 120k Galaxea Open-World VQA 99k VQA Robomind VQA 20k Droid VQA 76k LLaVA-Pretrain (Liu et al., 2023) 558k  For part of the embodied datasets (e.g., Agibot and RoboMIND), which contain precise instruction descriptions, we can directly construct QA pairs.For other datasets, we employ Gemini to decompose and annotate instruction descriptions according to the following prompt in Figure 9, 10.</p>
<p>B USAGE OF LLMS</p>
<p>In the final stages of preparing this manuscript, the authors used a Large Language Model (LLM) solely for grammar checking and language polishing.The model assisted in improving sentence structure and correcting grammatical errors to enhance readability.</p>
<p>Figure 1 :
1
Figure 1: Overview of UniCoD.Our proposed UniCoD, which utilizes both understanding and prediction tasks under discrete and continuous representation space, demonstrates strong semantic generalization capabilities on real-world manipulation tasks, particularly in its ability to handle completely novel objects not seen during training.The upper right displays benchmark evaluations across several simulations and 2 real-world robots.</p>
<p>Figure 2 :
2
Figure 2: Illustration of the UniCoD framework.UniCoD adopts a MoT framework to handle text understanding and planning, continuous visual prediction, and action execution.The continuous features are derived from future observations using a frozen vision encoder.</p>
<p>Figure 6 :
6
Figure 6: t-SNE Visualization of Different Future Representations.</p>
<p>Put the green apple into the blue plate Put the toy into the blue plate Put the apple into the transparent plate Lift up the transparent cup</p>
<p>Figure 7 :
7
Figure 7: Examples of Semantic Generalization to OOD objects</p>
<p>A. 6
6
VQA DATA DESIGH We present several examples of embodied VQA question-answer pairs in Figure 8. Question: Based on current observations and instruction &lt; Put apples, oranges and peaches in plastic bags.&gt;,what subtask does the robot need to do next?Answer: Next subtask is: place the held red apple into the plastic bag in the shopping cart.Question: Based on current observations and instruction &lt; Using the TV Remote.&gt;,what subtask does the robot need to do next?Answer: Next subtask is: Pick up the remote control from the table with left arm.Question: Based on current observations and instruction &lt; Stacking the blue, yellow, and orange cups.&gt;,planning for the task the robot need to do? Answer: Planning: Pick the blue cup; Place the blue cup on the yellow cup; Pick up the yellow cup; Place the yellow cup on the orange cup</p>
<p>Figure 8 :
8
Figure 8: Example of VQA.</p>
<p>Siméoni et al. (2025)mma Beyer et al. (2024)as the VLM expert.For future observation encoding, we experiment with SigLIP Tschannen et al. (2025), DINOv3Siméoni et al. (2025), and direct pixel-level prediction.Considering the information flow across modalities, we adopt a block-wise masking mechanism in the MoT attention: within each modality, bidirectional attention is applied, while across modalities a causal mask is enforced following the order of image, language, image prediction, state information, and action.Pre-training Data.In the pretraining stage, we utilize three categories of data to acquire joint text-image representations: (1) 320k robot videos paired with fine-grained subtask descriptions and overall task instructions, which yield VQA and TI2E data for the generation-understanding task; (2) 870k robot and human operation videos accompanied by task instructions, which are used as TI2E data; and (3) 560k generic vision-language question answering data, employed for co-training to preserve the fundamental capabilities of the VLM.In the action modeling stage, we exclusively adopt VLA data collected in both simulation and real-world robotic environments.Further details regarding the datasets are provided in Appendix A.5.
)3.3 IMPLEMENTATION DETAILSModel Setting.</p>
<p>Table 1 :
1
Results on SimplerEnv-WindowsX (visual matching).Entries marked with * are methods reproduced with our training and test settings.
ModelCarrot on PlateEggplant in BasketSpoon on TowelStack CubeSuccessGraspSuccessGraspSuccessGraspSuccessGraspSuccessAverageRT-1-X20.84.20.00.016.70.08.30.01.1Octo-Base52.88.366.743.134.712.531.90.016.0OpenVLA33.30.08.34.14.10.012.50.01.0RoboVLMs33.320.891.779.270.845.854.24.237.5SpatialVLA29.225.0100.0100.020.816.762.529.242.7π 0 *58.548.878.864.683.373.362.512.549.8CogAct/58.3/45.8/29.2/95.857.3Villa-x/46.3/64.6/77.9/61.362.5UniCoD (Ours)75.063.0100.089.683.378.891.752.571.0</p>
<p>Table 2 :
2
Long-horizon evaluation on the Calvin ABC→D benchmark.Entries marked with * are methods reproduced with our training and test settings.We only use a single 224x224 third-view image as input in all methods.
MethodTasks completed in a rowAvg. Len ↑12345RT-1<em>0.5330.2220.0940.0380.0130.900GR-10.8540.7120.5960.4970.4013.06π0</em>0.9370.8320.7400.6290.5103.65VPP<em>0.9090.8150.7130.6200.5183.58UP-VLA</em>0.9280.8650.8150.7690.6994.08UniCoD (Ours)0.9730.8950.8230.7520.6704.11</p>
<p>Table 3 :
3
Results on SimplerEnv-Google Robot (visual matching).Entries marked with * are methods reproduced with our training and test settings.
ModelPick CokeMove NearO./C. DrawerPut in DrawerAVG↑RT-1-X56.731.759.721.342.4Octo-Base17.04.222.70.011.0OpenVLA16.346.235.60.024.5RoboVLMs77.361.743.524.151.7π0*93.378.123.612.551.9CogACT91.385.071.850.974.8Villa-x98.775.059.35.659.6UniCod (Ours)98.781.563.270.078.4</p>
<p>Table 4 :
4
Ablation study on unified pretraining paradigm and continuous feature for prediction.
Effectiveness of Continuous Predic-tive Visual Representations To val-idate the effectiveness of predictionusing continuous representations, wecompare a version of UniCoD withoutModelCarrotEggplantSpoonCubeAVG↑pre-training against two baselines, asw/o Pretrainshown in Tablew/o Continuous48.864.673.312.549.8w/o Continuous w/ Pred52.579.279.630.060.3UniCoD60.887.178.850.469.3w/ PretrainUniCoD (Ours)63.089.678.852.571.0
Project Page: https://sites.google.com/view/unicod1
environment D. This evaluation suite includes 34 different manipulation tasks organized into 1,000 long-horizon sequences, each of length 5. We report the average length of successfully completed sub-task sequences.SimplerEnv Benchmark SimplerEnv is a simulation benchmark designed to evaluate policies trained on large-scale real-world datasets, such as Bridge-V2 and Fractal.It procedurally generates scenes that mimic real-world environments using texturing techniques, allowing models trained on real data to be tested directly in simulation without requiring physical deployment.The benchmark supports two types of robot arms: the WindowX and the Google Robot.For our evaluation, we conduct 240 runs for each task and report the average success rate.A.3 DETAILS ON REAL WORLD EXPERIMENTSA.3.1 FRANKA PANDA ROBOT ARMReal-World Franka Emika Panda Arm We deploy several models on a Franka Emika Panda arm for real-world task comparison.The robot arm features 7 degrees of freedom (DoF).Its action space is defined by a 7-dimensional vector, where the first six dimensions specify the relative change in the end-effector's 6D pose (3D position and 3D orientation), and the final dimension controls the binary state of the gripper (open or closed).In our experiments, the policy takes images from an on-board, first-person-view camera as visual input and outputs these relative actions.We first collected a dataset of 2,000 trajectories spanning over 20 distinct tasks, encompassing six fundamental skills: picking, placing, opening a drawer, closing a drawer, pressing a button, and routing a cable.We evaluate performance on both seen and unseen task variations.The unseen category primarily involves grasping novel objects not present in the training data.The task suite for the Franka Panda arm includes:• Pick &amp; Place: Grasping and placing a variety of objects.The training set includes items such as a toy banana, a toy eggplant, red/green/blue blocks, and red/yellow/black plates.• Press Button: Pressing a toy button using a grasped black block as a tool.• Route Cable: Routing a thin black rubber cable into a narrow slot.• Drawer Operation: Opening a toy drawer.Unseen Tasks These are designed to evaluate generalization: Novel Objects: Grasping objects not seen during training (e.g., toy chili, toy strawberry, yellow block, large toy eggplant, arrow sticker, marker pen).Distractors: Operating in the presence of irrelevant distractor objects.Visual Variations: Adapting to changes in background color and object color.We tested UniCoD, OpenVLA(Kim et al., 2024a), GR-1(Wu et al., 2023), π 0(Black et al., 2024), UP-VLA(Zhang et al., 2025)and VPP(Hu et al., 2024)on this environment.The detailed results are shown in Table6(corresponding to Figure4).A.3.2 XARM DEXTEROUS MANIPULATIONReal-World XArm with 12-DOF X-Hand Our 12-DoF single-arm dexterous manipulation platform, which comprises a 7-DoF XArm and a 5-DoF hand, is controlled using a dual-view visual input from both first-person and third-person cameras.During evaluation, we test pick-and-place capabilities across 5 distinct task variations for a total of 50 trials.For all other skills, we conduct 20 trials per task.The final performance is reported as the average success rate for each skill.We train different models using a dataset of 4,000 trajectories across more than 100 tasks.The models are then evaluated in a variety of seen and unseen scenarios, which cover 13 distinct skills, e.g., picking, placing, stacking, and pouring.To specifically test for visual generalization, we alter the background colors and novel objects during evaluation in the unseen scenarios.The task suite for the XArm platform includes:• Dexterous Pick &amp; Place: Dexterously grasping and placing a wide range of objects.The training set includes a toy banana, a toy eggplant, a toy orange, small and large toy soccer balls, a computer mouse, a toy drawer, and more.• Move Cup: Grasping and moving a cup to a different location.• Relocate: Grasping an object and placing it adjacent to another target object.• Stack Cube: Placing one block on top of another.• Pass: Grasping an object and handing it to a human operator.• Press Button: Directly actuating a toy button with a finger.• Unplug: Extracting a rubber cable from a socket.• Drawer Operation: Opening or closing a toy drawer.• Tool Use: Using various tools, such as a spoon (e.g., for scooping) and a toy hammer (e.g., for striking).Unseen Tasks These are designed to evaluate generalization: Novel Objects: Grasping unseen objects and placing them to not-seen targets during training (e.g., apple, lemon, glass cup, glass plate, blue plate, toy kapibla, transparent plate, green apple, big ball, and various of novel objects).Distractors: Operating in the presence of irrelevant distractor objects.Visual Variations: Adapting to changes in background color and object color.A.4 EXAMPLES OF DEMOS ON OOD-TASKSExamples of video on unseen objects are shown in Figure7, where unseen objects are bold in the instructions.More demos can be found in our anonymous website.Prompt ExampleSystem Message:"You are an expert in video analysis and robotic task understanding.You will be given an image sequence representing a video and a reference description.Your task is to decompose the total task into several steps which are needed to complete the task, and label each step with a frame range."User Message:## Task Description You will analyze an image sequence of a robotic arm performing a specific task.Your task is to make the overall task description more detailed with the help of the video clip, extract the necessary steps, and specify the frame range for each step.## TargetStep Extraction: Extract the key steps required to complete the task.Each step includes: -Specific actions decomposed from video and description -Frame window: Specify the start and end frame for each step ## Requirements: 1. Different steps must correspond to different action types.2. A step cannot contain two or more actions.3. Two similar steps need to be merged into one.4. The first step must start at frame 0, and the last step cannot exceed {frame_num-1}.## Output Format Return output in JSON: { "task_summary": "...", "steps": [ {"step_description": "...", "start_frame": 0, "end_frame": 6}, {"step_description": "...", "start_frame": 7, "end_frame": 12} ] } ## Example Input "task description": "Moving colored blocks into a container.""video": image sequence with length {frame_num} 1 Figure 9: prompt for Gemini.## Example Output {"task_summary": "Moving the red and yellow blocks into a container.","steps": [ {"step_description": "pick the red block.","start_frame": 0, "end_frame": 6}, {"step_description": "place the red block into container.","start_frame": 7, "end_frame": 12}, {"step_description": "pick the yellow block.","start_frame": 13, "end_frame": 15}, {"step_description": "place the yellow block into container.","start_frame": 16, "end_frame": {frame_num-1}} ] } 2 Figure 10: prompt for Gemini.
V-jepa 2: Self-supervised video models enable understanding, prediction and planning. Adrien Mido Assran, David Bardes, Quentin Fan, Russell Garrido, Matthew Howes, Ammar Muckley, Claire Rizvi, Koustuv Roberts, Artem Sinha, Zholus, arXiv:2506.099852025arXiv preprint</p>
<p>Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, arXiv:2502.13923arXiv:2407.07726A versatile 3b vlm for transfer. 2025. 2024arXiv preprint</p>
<p>Zero-shot robotic manipulation with pretrained image-editing diffusion models. Kevin Black, Mitsuhiko Nakamoto, Pranav Atreya, Homer Walke, Chelsea Finn, Aviral Kumar, Sergey Levine, arXiv:2310.106392023arXiv preprint</p>
<p>pi 0: A vision-language-action flow model for general robot control. Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, arXiv:2410.241642024arXiv preprint</p>
<p>Stable video diffusion: Scaling latent video diffusion models to large datasets. Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, arXiv:2311.151272023arXiv preprint</p>
<p>Rt-1: Robotics transformer for real-world control at scale. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, arXiv:2212.068172022arXiv preprint</p>
<p>Rt-2: Vision-language-action models transfer web knowledge to robotic control. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, arXiv:2307.158182023arXiv preprint</p>
<p>Agibot world colosseo: A large-scale manipulation platform for scalable and intelligent embodied systems. Qingwen Bu, Jisong Cai, Li Chen, Xiuqi Cui, Yan Ding, Siyuan Feng, Shenyuan Gao, Xindong He, Xuan Hu, Xu Huang, arXiv:2503.066692025arXiv preprint</p>
<p>Igor: Image-goal representations are the atomic control units for foundation models in embodied ai. Xiaoyu Chen, Junliang Guo, Tianyu He, Chuheng Zhang, Pushi Zhang, Derek Cathera Yang, Li Zhao, Jiang Bian, 2024</p>
<p>Villa-x: enhancing latent action modeling in vision-language-action models. Xiaoyu Chen, Hangxing Wei, Pushi Zhang, Chuheng Zhang, Kaixin Wang, Yanjiang Guo, Rushuai Yang, Yucen Wang, Xinquan Xiao, Li Zhao, arXiv:2507.236822025arXiv preprint</p>
<p>Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, arXiv:2507.062612025arXiv preprint</p>
<p>Instructblip: Towards general-purpose visionlanguage models with instruction tuning. Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng, Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale N Fung, Steven Hoi, Advances in Neural Information Processing Systems. 362024</p>
<p>Learning universal policies via text-guided video generation. Yilun Du, Sherry Yang, Bo Dai, Hanjun Dai, Ofir Nachum, Josh Tenenbaum, Dale Schuurmans, Pieter Abbeel, Advances in Neural Information Processing Systems. 202436</p>
<p>Ego4d: Around the world in 3,000 hours of egocentric video. Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2022</p>
<p>Prediction with action: Visual policy learning via joint denoising process. Yanjiang Guo, Yucheng Hu, Jianke Zhang, Yen-Jen Wang, Xiaoyu Chen, Chaochao Lu, Jianyu Chen, arXiv:2411.181792024arXiv preprint</p>
<p>Improving vision-language-action model with online reinforcement learning. Yanjiang Guo, Jianke Zhang, Xiaoyu Chen, Xiang Ji, Yen-Jen Wang, Yucheng Hu, Jianyu Chen, arXiv:2501.166642025arXiv preprint</p>
<p>Ryan Hoque, Peide Huang, David J Yoon, Mouli Sivapurapu, Jian Zhang, Egodex, arXiv:2505.11709Learning dexterous manipulation from large-scale egocentric video. 2025arXiv preprint</p>
<p>Yucheng Hu, Yanjiang Guo, Pengchao Wang, Xiaoyu Chen, Yen-Jen Wang, Jianke Zhang, Koushil Sreenath, Chaochao Lu, Jianyu Chen, arXiv:2412.14803Video prediction policy: A generalist robot policy with predictive visual representations. 2024arXiv preprint</p>
<p>Tao Jiang, Tianyuan Yuan, Yicheng Liu, Chenhao Lu, Jianning Cui, Xiao Liu, Shuiqi Cheng, Jiyang Gao, Huazhe Xu, Hang Zhao, arXiv:2509.00576Galaxea open-world dataset and g0 dual-system vla model. 2025arXiv preprint</p>
<p>Alexander Khazatsky, Karl Pertsch, Suraj Nair, Ashwin Balakrishna, Sudeep Dasari, Siddharth Karamcheti, Soroush Nasiriany, Mohan Kumar Srirama, Lawrence Yunliang Chen, Kirsty Ellis, arXiv:2403.12945A large-scale in-the-wild robot manipulation dataset. 2024arXiv preprint</p>
<p>Jin Moo, Karl Kim, Siddharth Pertsch, Ted Karamcheti, Ashwin Xiao, Suraj Balakrishna, Rafael Nair, Ethan Rafailov, Grace Foster, Pannag Lam, Quan Sanketi, Thomas Vuong, Benjamin Kollar, Russ Burchfiel, Dorsa Tedrake, Sergey Sadigh, Percy Levine, Chelsea Liang, Finn, arXiv:2406.09246Openvla: An open-source vision-language-action model. 2024aarXiv preprint</p>
<p>Openvla: An open-source vision-language-action model. Jin Moo, Karl Kim, Siddharth Pertsch, Ted Karamcheti, Ashwin Xiao, Suraj Balakrishna, Rafael Nair, Ethan Rafailov, Grace Foster, Pannag Lam, Sanketi, arXiv:2406.092462024barXiv preprint</p>
<p>Bridgevla: Input-output alignment for efficient 3d manipulation learning with vision-language models. Peiyan Li, Yixiang Chen, Hongtao Wu, Xiao Ma, Xiangnan Wu, Yan Huang, Liang Wang, Tao Kong, Tieniu Tan, arXiv:2506.079612025arXiv preprint</p>
<p>Cogact: A foundational vision-language-action model for synergizing cognition and action in robotic manipulation. Qixiu Li, Yaobo Liang, Zeyu Wang, Lin Luo, Xi Chen, Mozheng Liao, Fangyun Wei, Yu Deng, Sicheng Xu, Yizhong Zhang, arXiv:2411.196502024arXiv preprint</p>
<p>Vision-language foundation models as effective robot imitators. Xinghang Li, Minghuan Liu, Hanbo Zhang, Cunjun Yu, Jie Xu, Hongtao Wu, Chilam Cheang, Ya Jing, Weinan Zhang, Huaping Liu, arXiv:2311.013782023arXiv preprint</p>
<p>Mixture-of-transformers: A sparse and scalable architecture for multi-modal foundation models. Weixin Liang, Lili Yu, Liang Luo, Srinivasan Iyer, Ning Dong, Chunting Zhou, Gargi Ghosh, Mike Lewis, Wen-Tau Yih, Luke Zettlemoyer, arXiv:2411.049962024arXiv preprint</p>
<p>Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee, Visual instruction tuning. 2023</p>
<p>Towards generalist robot policies: What matters in building vision-language-action models. Huaping Liu, Xinghang Li, Peiyan Li, Minghuan Liu, Dong Wang, Jirong Liu, Bingyi Kang, Xiao Ma, Tao Kong, Hanbo Zhang, arXiv:2412.140582025arXiv preprint</p>
<p>Delin Qu, Haoming Song, Qizhi Chen, Yuanqi Yao, Xinyi Ye, Yan Ding, Zhigang Wang, Jiayuan Gu, Bin Zhao, Dong Wang, arXiv:2501.15830Exploring spatial representations for visual-languageaction model. 2025arXiv preprint</p>
<p>. Oriane Siméoni, V Huy, Maximilian Vo, Federico Seitzer, Maxime Baldassarre, Cijo Oquab, Vasil Jose, Marc Khalidov, Seungeun Szafraniec, Michaël Yi, Ramamonjisoa, arXiv:2508.101042025arXiv preprint</p>
<p>Octo Model Team, Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey Hejna, Tobias Kreiman, Charles Xu, arXiv:2405.12213An open-source generalist robot policy. 2024arXiv preprint</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Hambro, arXiv:2302.13971Faisal Azhar, et al. Llama: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>Siglip 2: Multilingual vision-language encoders with improved semantic understanding, localization, and dense features. Alexey Michael Tschannen, Xiao Gritsenko, Muhammad Ferjad Wang, Ibrahim Naeem, Nikhil Alabdulmohsin, Talfan Parthasarathy, Lucas Evans, Ye Beyer, Basil Xia, Mustafa, arXiv:2502.147862025arXiv preprint</p>
<p>Bridgedata v2: A dataset for robot learning at scale. Homer Rich Walke, Kevin Black, Tony Z Zhao, Quan Vuong, Chongyi Zheng, Philippe Hansen-Estruch, Andre Wang He, Vivek Myers, Jin Moo, Max Kim, Du, Conference on Robot Learning. PMLR2023</p>
<p>Weiyun Wang, Zhangwei Gao, Lixin Gu, Hengjun Pu, Long Cui, Xingguang Wei, Zhaoyang Liu, Linglin Jing, Shenglong Ye, Jie Shao, arXiv:2508.18265Advancing open-source multimodal models in versatility, reasoning, and efficiency. 2025a5arXiv preprint</p>
<p>Unified vision-language-action model. Yuqi Wang, Xinghang Li, Wenxuan Wang, Junbo Zhang, Yingyan Li, Yuntao Chen, Xinlong Wang, Zhaoxiang Zhang, arXiv:2506.19850arXiv:2502.05855Junjie Wen, Yichen Zhu, Jinming Li, Zhibin Tang, Chaomin Shen, and Feifei Feng. Dexvla: Vision-language model with plug-in diffusion expert for general robot control. 2025b. 2025arXiv preprint</p>
<p>Vidman: Exploiting implicit dynamics from video diffusion model for effective robot manipulation. Youpeng Wen, Junfan Lin, Yi Zhu, Jianhua Han, Hang Xu, Shen Zhao, Xiaodan Liang, Advances in Neural Information Processing Systems. 202437</p>
<p>Unleashing large-scale video generative pre-training for visual robot manipulation. Hongtao Wu, Ya Jing, Chilam Cheang, Guangzeng Chen, Jiafeng Xu, Xinghang Li, Minghuan Liu, Hang Li, Tao Kong, arXiv:2312.131392023arXiv preprint</p>
<p>Kun Wu, Chengkai Hou, Jiaming Liu, Zhengping Che, Xiaozhu Ju, Zhuqin Yang, Meng Li, Yinuo Zhao, Zhiyuan Xu, Guang Yang, arXiv:2412.13877Benchmark on multi-embodiment intelligence normative data for robot manipulation. 2024arXiv preprint</p>
<p>Youguang Xing, Xu Luo, Junlin Xie, Lianli Gao, Hengtao Shen, Jingkuan Song, arXiv:2508.06426Shortcut learning in generalist robot policies: The role of dataset diversity and fragmentation. 2025arXiv preprint</p>
<p>Jianke Zhang, Yanjiang Guo, Xiaoyu Chen, Yen-Jen Wang, Yucheng Hu, Chengming Shi, Jianyu Chen, arXiv:2410.05273Hirt: Enhancing robotic control with hierarchical robot transformers. 2024arXiv preprint</p>
<p>Up-vla: A unified understanding and prediction model for embodied agent. Jianke Zhang, Yanjiang Guo, Yucheng Hu, Xiaoyu Chen, Xiang Zhu, Jianyu Chen, arXiv:2501.188672025arXiv preprint</p>
<p>Cot-vla: Visual chain-of-thought reasoning for vision-language-action models. Qingqing Zhao, Yao Lu, Jin Moo, Zipeng Kim, Zhuoyang Fu, Yecheng Zhang, Zhaoshuo Wu, Qianli Li, Song Ma, Chelsea Han, Finn, Proceedings of the Computer Vision and Pattern Recognition Conference. the Computer Vision and Pattern Recognition Conference2025</p>
<p>Zhongyi Zhou, Yichen Zhu, Minjie Zhu, Junjie Wen, Ning Liu, Zhiyuan Xu, Weibin Meng, Ran Cheng, Yaxin Peng, Chaomin Shen, arXiv:2502.14420Unified multimodal understanding and robot control with vision-language-action model. 2025arXiv preprint</p>
<p>Rt-2: Vision-language-action models transfer web knowledge to robotic control. Brianna Zitkovich, Tianhe Yu, Sichun Xu, Peng Xu, Ted Xiao, Fei Xia, Jialin Wu, Paul Wohlhart, Stefan Welker, Ayzaan Wahid, Conference on Robot Learning. PMLR2023</p>            </div>
        </div>

    </div>
</body>
</html>
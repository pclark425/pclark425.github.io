<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4521 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4521</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4521</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-99.html">extraction-schema-99</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <p><strong>Paper ID:</strong> paper-267522843</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.04268v1.pdf" target="_blank">ProtAgents: protein discovery via large language model multi-agent collaborations combining physics and machine learning</a></p>
                <p><strong>Paper Abstract:</strong> Designing de novo proteins beyond those found in nature holds significant promise for advancements in both scientific and engineering applications. Current methodologies for protein design often rely on AI-based models, such as surrogate models that address end-to-end problems by linking protein structure to material properties or vice versa. However, these models frequently focus on specific material objectives or structural properties, limiting their flexibility when incorporating out-of-domain knowledge into the design process or comprehensive data analysis is required. In this study, we introduce ProtAgents, a platform for de novo protein design based on Large Language Models (LLMs), where multiple AI agents with distinct capabilities collaboratively address complex tasks within a dynamic environment. The versatility in agent development allows for expertise in diverse domains, including knowledge retrieval, protein structure analysis, physics-based simulations, and results analysis. The dynamic collaboration between agents, empowered by LLMs, provides a versatile approach to tackling protein design and analysis problems, as demonstrated through diverse examples in this study. The problems of interest encompass designing new proteins, analyzing protein structures and obtaining new first-principles data – natural vibrational frequencies – via physics simulations. The concerted effort of the system allows for powerful automated and synergistic design of de novo proteins with targeted mechanical properties. The flexibility in designing the agents, on one hand, and their capacity in autonomous collaboration through the dynamic LLM-based multi-agent environment on the other hand, unleashes great potentials of LLMs in addressing multi-objective materials problems and opens up new avenues for autonomous materials discovery and design.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4521.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4521.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ProtAgents</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ProtAgents: LLM-based Multi-Agent Platform for Protein Design</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-agent platform introduced in this paper that uses GPT-4-powered conversational agents (planner, assistant, critic, manager, user_proxy) together with tool-backed functions (design, fold, analyze, simulate, retrieval) to perform automated protein design, analysis, and literature retrieval workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>ProtAgents multi-agent LLM platform</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>A pipeline in which distinct agent roles (planner, assistant, critic, user_proxy, group chat manager) are implemented as GPT-4-based conversational agents orchestrated by the AutoGen framework. The planner translates user queries into a multi-step plan and suggests function calls; the assistant executes tool-backed Python functions (e.g., design_protein_from_length, fold_protein, analyze_protein_structure, calc_protein_ANM, save_to_csv_file, retrieve_content) and external model APIs; the critic inspects plans/results and suggests corrections; the manager dynamically selects speaking agents and mediates the group chat. The system integrates retrieval-augmented generation (RAG) via Llama Index, generative design models (Chroma), folding models (OmegaFold), a physics simulator (ANM via ProDy) and a special-purpose protein language model (ProteinForceGPT) for predicting mechanical force-extension behavior. Inputs and outputs are exchanged as structured JSON/dictionaries and CSVs.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>GPT-4 (as the conversational agent backbone); integrates a domain-specialist autoregressive protein language model 'ProteinForceGPT' (NeoGPT-X style) for mechanical predictions</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computational protein design / materials science / computational biophysics</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Structure–property relationships (e.g., sequence/secondary-structure ↔ natural frequencies; sequence ↔ unfolding force-extension curves; numerical mechanical properties such as max unfolding force and unfolding energy)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Structured JSON dictionaries and CSV files containing numerical arrays (e.g., lists of natural frequencies, force‑extension curves), scalar mechanical metrics (max force, energy), and secondary structure percentages</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Internal cross-tool comparisons and physics-based computation: e.g., predictions from generative/folding models compared to ANM simulations and analyses by critic; functions validated via execution (no formal external benchmark reported)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not reported (no quantitative accuracy metrics for extraction-of-laws from literature were provided in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>None reported for literature-extraction tasks (comparisons are made between design/folding outputs from Chroma vs OmegaFold and physics computations, but no formal baseline ML approach for literature-to-law extraction was benchmarked)</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Documented issues include poor performance of an implemented retrieve_content function (incorrect PDB id matches), JSON formatting errors when assembling outputs, and the general limitation that LLMs alone are not ideal for physics-based forward/inverse design — hence the need to integrate specialized simulation and surrogate models. The paper also notes Chroma's failures on some conditional design tasks (e.g., producing beta-rich proteins) and that external-tool/database access quality constrains results.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ProtAgents: protein discovery via large language model multi-agent collaborations combining physics and machine learning', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4521.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4521.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG (Llama Index)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation implemented with Llama Index</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval-augmented generation approach used to provide the agent-team with access to full-text papers and external knowledge sources in real time via Llama Index, enabling the LLMs to retrieve and condition on documents during planning/analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Retrieval-Augmented Generation via Llama Index</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>The assistant/agents are given a RAG tool implemented with Llama Index to query an indexed collection of full-text papers (the references cited) as external sources; retrieved passages are supplied to GPT-4 agents to inform planning, retrieval of examples (e.g., proteins with experimentally measured mechanical properties), and knowledge-grounded reasoning. The RAG tool is called via a custom function (retrieve_content) integrated into the Assistant's function_map.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>GPT-4 (agents) used in conjunction with Llama Index as the retrieval backend; no separate retriever LLM size specified</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Materials science / protein mechanics / computational biology literature</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Empirical/experimental data extraction (e.g., protein names, PDB identifiers, experimentally studied mechanical properties) and fact retrieval rather than symbolic law-discovery; supports extraction of numeric experimental properties (percent secondary structure, frequencies, max forces)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Retrieved text passages; structured JSON fields created by agents from retrieved information; CSV exports</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Agent-level checks and critic feedback (agents cross-check retrieved items and detect mismatches); no external ground-truth benchmark evaluation for retrieval quality reported</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not reported; empirical failure example given where retrieve_content returned PDB ids that did not correctly match protein names (error attributed to the retrieve_content function and/or database access)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>None reported</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Dependence on the quality and coverage of the indexed database; documented mismatches between retrieved PDB IDs and protein names caused incorrect downstream operations; no formal IR evaluation (precision/recall) provided.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ProtAgents: protein discovery via large language model multi-agent collaborations combining physics and machine learning', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4521.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4521.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ProteinForceGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ProteinForceGPT (autoregressive transformer for force‑extension predictions)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A domain-specialist autoregressive GPT-style protein language model (NeoGPT-X architecture) fine-tuned to predict mechanical unfolding force‑extension curves and summary mechanical metrics (max force, unfolding energy) from amino-acid sequences, and to support inverse design of sequences matching target mechanical behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>ProteinForceGPT autoregressive sequence-to-mechanics model</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>A NeoGPT-X-style autoregressive transformer with rotary positional embeddings (RoPE), 36 hidden layers, 16 attention heads, hidden size 1024 and intermediate size 4096. Pre-trained on ~800,000 amino-acid sequences using next-token prediction, then fine-tuned bidirectionally using datasets derived from molecular dynamics simulations to map sequences ↔ force-extension curves and scalar mechanical descriptors. In ProtAgents the model is invoked via the function calucalte_force_energy_from_seq (and related functions) to compute max unfolding force, unfolding energy, and predicted force‑separation histories from de novo sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>ProteinForceGPT (NeoGPT-X style): 36 layers, 16 attention heads, hidden size 1024; rotary positional embeddings (RoPE).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Protein mechanics / computational biophysics / materials mechanics</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Quantitative mechanical relationships: full force‑extension curves (time/extension → force), scalar mechanical metrics (maximum unfolding force, unfolding energy = area under curve), mapping from primary sequence to mechanical response</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Numerical arrays for force‑extension histories, scalar values for max force and energy; CSV/JSON structured outputs</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Model validation performed against MD-simulation-derived data (validation results referred to and shown in supplemental Figure S2); in-system use validated by downstream computation and saving outputs, but no external experimental validation reported in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not reported in main text (sample validation results are referenced in Figure S2 but no explicit numeric performance metrics such as RMSE, R², or MAE are listed in the paper)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>No formal baseline comparison reported within this paper (other surrogate models are referenced in related work but no head-to-head metrics presented here)</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>The model relies on training data derived from MD simulations (domain and fidelity limitations); forward predictions are used but inverse-design capabilities are mentioned without full evaluation in this work. Specific numeric performance summaries are not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ProtAgents: protein discovery via large language model multi-agent collaborations combining physics and machine learning', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4521.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4521.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AutoGen multi-agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AutoGen framework for multi-agent LLM collaboration</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source framework (AutoGen) used to implement and orchestrate the dynamic multi-agent chat system, enabling creation of UserProxyAgent, AssistantAgent, Planner, Critic, and GroupChatManager with function maps and tool integration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>AutoGen-driven LLM multi-agent orchestration</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>AutoGen provides APIs/classes used to create conversational agents with role-specific system prompts, function maps (tool call interfaces), and a GroupChatManager to select speaking agents and mediate multi-agent chats. In ProtAgents, AutoGen is used to implement the dynamic loop where planner suggests function calls, assistant executes functions, critic reviews outputs and suggests corrections, and manager orchestrates the run.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>GPT-4 (agents instantiated via AutoGen); AutoGen is framework-level (no model size)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General (here applied to protein design and materials science workflows)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Enables extraction and assembly of quantitative outputs (structured data, simulation outputs, numeric descriptors) via tool calls rather than directly discovering symbolic laws from literature</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Supports structured JSON, CSV, and function-call outputs integrated into agent workflows</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Agent-level critic checks and automated reruns (debugging JSON formatting, plan corrections); no formal statistical validation of literature-to-law extraction reported</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not reported</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>None reported</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Quality of function implementations, data sources, and the LLM's reasoning can limit outcomes; errors encountered included malformed JSON inputs and downstream function failures tied to data retrieval quality.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ProtAgents: protein discovery via large language model multi-agent collaborations combining physics and machine learning', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Generative retrieval-augmented ontologic graph and multi-agent strategies for interpretive large language model-based materials design <em>(Rating: 2)</em></li>
                <li>Mechagents: Large language model multi-agent collaborations can solve mechanics problems, generate new data, and integrate knowledge <em>(Rating: 2)</em></li>
                <li>Melm, a generative pretrained language modeling framework that solves forward and inverse mechanics problems <em>(Rating: 2)</em></li>
                <li>Retrieval-augmented generation for knowledge-intensive nlp tasks <em>(Rating: 2)</em></li>
                <li>Autogen: Enabling next-gen LLM applications via multi-agent conversation <em>(Rating: 2)</em></li>
                <li>Rapid prediction of protein natural frequencies using graph neural networks <em>(Rating: 1)</em></li>
                <li>End-to-end protein normal mode frequency predictions using language and graph models and application to sonification <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4521",
    "paper_id": "paper-267522843",
    "extraction_schema_id": "extraction-schema-99",
    "extracted_data": [
        {
            "name_short": "ProtAgents",
            "name_full": "ProtAgents: LLM-based Multi-Agent Platform for Protein Design",
            "brief_description": "A multi-agent platform introduced in this paper that uses GPT-4-powered conversational agents (planner, assistant, critic, manager, user_proxy) together with tool-backed functions (design, fold, analyze, simulate, retrieval) to perform automated protein design, analysis, and literature retrieval workflows.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "ProtAgents multi-agent LLM platform",
            "method_description": "A pipeline in which distinct agent roles (planner, assistant, critic, user_proxy, group chat manager) are implemented as GPT-4-based conversational agents orchestrated by the AutoGen framework. The planner translates user queries into a multi-step plan and suggests function calls; the assistant executes tool-backed Python functions (e.g., design_protein_from_length, fold_protein, analyze_protein_structure, calc_protein_ANM, save_to_csv_file, retrieve_content) and external model APIs; the critic inspects plans/results and suggests corrections; the manager dynamically selects speaking agents and mediates the group chat. The system integrates retrieval-augmented generation (RAG) via Llama Index, generative design models (Chroma), folding models (OmegaFold), a physics simulator (ANM via ProDy) and a special-purpose protein language model (ProteinForceGPT) for predicting mechanical force-extension behavior. Inputs and outputs are exchanged as structured JSON/dictionaries and CSVs.",
            "llm_model_used": "GPT-4 (as the conversational agent backbone); integrates a domain-specialist autoregressive protein language model 'ProteinForceGPT' (NeoGPT-X style) for mechanical predictions",
            "scientific_domain": "Computational protein design / materials science / computational biophysics",
            "number_of_papers": null,
            "type_of_quantitative_law": "Structure–property relationships (e.g., sequence/secondary-structure ↔ natural frequencies; sequence ↔ unfolding force-extension curves; numerical mechanical properties such as max unfolding force and unfolding energy)",
            "extraction_output_format": "Structured JSON dictionaries and CSV files containing numerical arrays (e.g., lists of natural frequencies, force‑extension curves), scalar mechanical metrics (max force, energy), and secondary structure percentages",
            "validation_method": "Internal cross-tool comparisons and physics-based computation: e.g., predictions from generative/folding models compared to ANM simulations and analyses by critic; functions validated via execution (no formal external benchmark reported)",
            "performance_metrics": "Not reported (no quantitative accuracy metrics for extraction-of-laws from literature were provided in this paper)",
            "baseline_comparison": "None reported for literature-extraction tasks (comparisons are made between design/folding outputs from Chroma vs OmegaFold and physics computations, but no formal baseline ML approach for literature-to-law extraction was benchmarked)",
            "challenges_limitations": "Documented issues include poor performance of an implemented retrieve_content function (incorrect PDB id matches), JSON formatting errors when assembling outputs, and the general limitation that LLMs alone are not ideal for physics-based forward/inverse design — hence the need to integrate specialized simulation and surrogate models. The paper also notes Chroma's failures on some conditional design tasks (e.g., producing beta-rich proteins) and that external-tool/database access quality constrains results.",
            "requires_human_in_loop": false,
            "fully_automated": true,
            "uuid": "e4521.0",
            "source_info": {
                "paper_title": "ProtAgents: protein discovery via large language model multi-agent collaborations combining physics and machine learning",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "RAG (Llama Index)",
            "name_full": "Retrieval-Augmented Generation implemented with Llama Index",
            "brief_description": "A retrieval-augmented generation approach used to provide the agent-team with access to full-text papers and external knowledge sources in real time via Llama Index, enabling the LLMs to retrieve and condition on documents during planning/analysis.",
            "citation_title": "",
            "mention_or_use": "use",
            "method_name": "Retrieval-Augmented Generation via Llama Index",
            "method_description": "The assistant/agents are given a RAG tool implemented with Llama Index to query an indexed collection of full-text papers (the references cited) as external sources; retrieved passages are supplied to GPT-4 agents to inform planning, retrieval of examples (e.g., proteins with experimentally measured mechanical properties), and knowledge-grounded reasoning. The RAG tool is called via a custom function (retrieve_content) integrated into the Assistant's function_map.",
            "llm_model_used": "GPT-4 (agents) used in conjunction with Llama Index as the retrieval backend; no separate retriever LLM size specified",
            "scientific_domain": "Materials science / protein mechanics / computational biology literature",
            "number_of_papers": null,
            "type_of_quantitative_law": "Empirical/experimental data extraction (e.g., protein names, PDB identifiers, experimentally studied mechanical properties) and fact retrieval rather than symbolic law-discovery; supports extraction of numeric experimental properties (percent secondary structure, frequencies, max forces)",
            "extraction_output_format": "Retrieved text passages; structured JSON fields created by agents from retrieved information; CSV exports",
            "validation_method": "Agent-level checks and critic feedback (agents cross-check retrieved items and detect mismatches); no external ground-truth benchmark evaluation for retrieval quality reported",
            "performance_metrics": "Not reported; empirical failure example given where retrieve_content returned PDB ids that did not correctly match protein names (error attributed to the retrieve_content function and/or database access)",
            "baseline_comparison": "None reported",
            "challenges_limitations": "Dependence on the quality and coverage of the indexed database; documented mismatches between retrieved PDB IDs and protein names caused incorrect downstream operations; no formal IR evaluation (precision/recall) provided.",
            "requires_human_in_loop": null,
            "fully_automated": true,
            "uuid": "e4521.1",
            "source_info": {
                "paper_title": "ProtAgents: protein discovery via large language model multi-agent collaborations combining physics and machine learning",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "ProteinForceGPT",
            "name_full": "ProteinForceGPT (autoregressive transformer for force‑extension predictions)",
            "brief_description": "A domain-specialist autoregressive GPT-style protein language model (NeoGPT-X architecture) fine-tuned to predict mechanical unfolding force‑extension curves and summary mechanical metrics (max force, unfolding energy) from amino-acid sequences, and to support inverse design of sequences matching target mechanical behavior.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "ProteinForceGPT autoregressive sequence-to-mechanics model",
            "method_description": "A NeoGPT-X-style autoregressive transformer with rotary positional embeddings (RoPE), 36 hidden layers, 16 attention heads, hidden size 1024 and intermediate size 4096. Pre-trained on ~800,000 amino-acid sequences using next-token prediction, then fine-tuned bidirectionally using datasets derived from molecular dynamics simulations to map sequences ↔ force-extension curves and scalar mechanical descriptors. In ProtAgents the model is invoked via the function calucalte_force_energy_from_seq (and related functions) to compute max unfolding force, unfolding energy, and predicted force‑separation histories from de novo sequences.",
            "llm_model_used": "ProteinForceGPT (NeoGPT-X style): 36 layers, 16 attention heads, hidden size 1024; rotary positional embeddings (RoPE).",
            "scientific_domain": "Protein mechanics / computational biophysics / materials mechanics",
            "number_of_papers": null,
            "type_of_quantitative_law": "Quantitative mechanical relationships: full force‑extension curves (time/extension → force), scalar mechanical metrics (maximum unfolding force, unfolding energy = area under curve), mapping from primary sequence to mechanical response",
            "extraction_output_format": "Numerical arrays for force‑extension histories, scalar values for max force and energy; CSV/JSON structured outputs",
            "validation_method": "Model validation performed against MD-simulation-derived data (validation results referred to and shown in supplemental Figure S2); in-system use validated by downstream computation and saving outputs, but no external experimental validation reported in this paper",
            "performance_metrics": "Not reported in main text (sample validation results are referenced in Figure S2 but no explicit numeric performance metrics such as RMSE, R², or MAE are listed in the paper)",
            "baseline_comparison": "No formal baseline comparison reported within this paper (other surrogate models are referenced in related work but no head-to-head metrics presented here)",
            "challenges_limitations": "The model relies on training data derived from MD simulations (domain and fidelity limitations); forward predictions are used but inverse-design capabilities are mentioned without full evaluation in this work. Specific numeric performance summaries are not provided here.",
            "requires_human_in_loop": false,
            "fully_automated": true,
            "uuid": "e4521.2",
            "source_info": {
                "paper_title": "ProtAgents: protein discovery via large language model multi-agent collaborations combining physics and machine learning",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "AutoGen multi-agent",
            "name_full": "AutoGen framework for multi-agent LLM collaboration",
            "brief_description": "An open-source framework (AutoGen) used to implement and orchestrate the dynamic multi-agent chat system, enabling creation of UserProxyAgent, AssistantAgent, Planner, Critic, and GroupChatManager with function maps and tool integration.",
            "citation_title": "",
            "mention_or_use": "use",
            "method_name": "AutoGen-driven LLM multi-agent orchestration",
            "method_description": "AutoGen provides APIs/classes used to create conversational agents with role-specific system prompts, function maps (tool call interfaces), and a GroupChatManager to select speaking agents and mediate multi-agent chats. In ProtAgents, AutoGen is used to implement the dynamic loop where planner suggests function calls, assistant executes functions, critic reviews outputs and suggests corrections, and manager orchestrates the run.",
            "llm_model_used": "GPT-4 (agents instantiated via AutoGen); AutoGen is framework-level (no model size)",
            "scientific_domain": "General (here applied to protein design and materials science workflows)",
            "number_of_papers": null,
            "type_of_quantitative_law": "Enables extraction and assembly of quantitative outputs (structured data, simulation outputs, numeric descriptors) via tool calls rather than directly discovering symbolic laws from literature",
            "extraction_output_format": "Supports structured JSON, CSV, and function-call outputs integrated into agent workflows",
            "validation_method": "Agent-level critic checks and automated reruns (debugging JSON formatting, plan corrections); no formal statistical validation of literature-to-law extraction reported",
            "performance_metrics": "Not reported",
            "baseline_comparison": "None reported",
            "challenges_limitations": "Quality of function implementations, data sources, and the LLM's reasoning can limit outcomes; errors encountered included malformed JSON inputs and downstream function failures tied to data retrieval quality.",
            "requires_human_in_loop": null,
            "fully_automated": true,
            "uuid": "e4521.3",
            "source_info": {
                "paper_title": "ProtAgents: protein discovery via large language model multi-agent collaborations combining physics and machine learning",
                "publication_date_yy_mm": "2024-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Generative retrieval-augmented ontologic graph and multi-agent strategies for interpretive large language model-based materials design",
            "rating": 2
        },
        {
            "paper_title": "Mechagents: Large language model multi-agent collaborations can solve mechanics problems, generate new data, and integrate knowledge",
            "rating": 2
        },
        {
            "paper_title": "Melm, a generative pretrained language modeling framework that solves forward and inverse mechanics problems",
            "rating": 2
        },
        {
            "paper_title": "Retrieval-augmented generation for knowledge-intensive nlp tasks",
            "rating": 2
        },
        {
            "paper_title": "Autogen: Enabling next-gen LLM applications via multi-agent conversation",
            "rating": 2
        },
        {
            "paper_title": "Rapid prediction of protein natural frequencies using graph neural networks",
            "rating": 1
        },
        {
            "paper_title": "End-to-end protein normal mode frequency predictions using language and graph models and application to sonification",
            "rating": 1
        }
    ],
    "cost": 0.015153,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>PROTAGENTS: PROTEIN DISCOVERY VIA LARGE LANGUAGE MODEL MULTI-AGENT COLLABORATIONS COMBINING PHYSICS AND MACHINE LEARNING *
27 Jan 2024</p>
<p>Alireza Ghafarollahi 
Laboratory for Atomistic and Molecular Mechanics (LAMM)
Massachusetts Institute of Technology
77 Massachusetts Ave02139CambridgeMAUSA</p>
<p>Markus J Buehler mbuehler@mit.edu 
Center for Computational Science and Engineering Schwarzman College of Computing
Laboratory for Atomistic and Molecular Mechanics (LAMM
Massachusetts Institute of Technology
77 Massachusetts Ave02139CambridgeMAUSA</p>
<p>PROTAGENTS: PROTEIN DISCOVERY VIA LARGE LANGUAGE MODEL MULTI-AGENT COLLABORATIONS COMBINING PHYSICS AND MACHINE LEARNING *
27 Jan 20243332BF8578B89D2F92BBBDE8A3C37D5BarXiv:2402.04268v1[cond-mat.soft]Multi-agent modelinglarge language model (LLM)physics-inspired machine learningprotein design
Designing de novo proteins beyond those found in nature holds significant promise for advancements in both scientific and engineering applications.Current methodologies for protein design often rely on AI-based models, such as surrogate models that address end-to-end problems by linking protein structure to material properties or vice versa.However, these models frequently focus on specific material objectives or structural properties, limiting their flexibility when incorporating out-of-domain knowledge into the design process or comprehensive data analysis is required.In this study, we introduce ProtAgents, a platform for de novo protein design based on Large Language Models (LLMs), where multiple AI agents with distinct capabilities collaboratively address complex tasks within a dynamic environment.The versatility in agent development allows for expertise in diverse domains, including knowledge retrieval, protein structure analysis, physics-based simulations, and results analysis.The dynamic collaboration between agents, empowered by LLMs, provides a versatile approach to tackling protein design and analysis problems, as demonstrated through diverse examples in this study.The problems of interest encompass designing new proteins, analyzing protein structures and obtaining new first-principles data -natural vibrational frequencies -via physics simulations.The concerted effort of the system allows for powerful automated and synergistic design of de novo proteins with targeted mechanical properties.The flexibility in designing the agents, on one hand, and their capacity in autonomous collaboration through the dynamic LLM-based multi-agent environment on the other hand, unleashes great potentials of LLMs in addressing multi-objective materials problems and opens up new avenues for autonomous materials discovery and design.</p>
<p>Introduction</p>
<p>Proteins, the building blocks of life, serve as the fundamental elements of many biological materials emerging from natural evolution over the span of 300 million years.Protein-base biomaterials like silk, collagen and tissue assemblies such as skin exhibit diverse structural features and showcase unique combinations of material properties.The underlying sequences of amino acids (AAs) in a protein determines its unique there-dimensional structure, which, in turn, dictates its specific biological activity and associated outstanding properties.This inherent relationship has inspired scientists in the field of materials design and optimization to draw valuable insights from nature for creating novel protein-based materials.The diversity in protein design is immense, with over 20 100 possible AA sequences for just a relatively small 100-residue protein.However, the natural evolutionary process has sampled only a fraction of this vast sequence space.This leaves a substantial portion uncharted, presenting a significant opportunity for the de novo design of proteins with potentially remarkable properties.[1] Despite this potential, the extensive design space, coupled with the costs associated with experimental testing, poses formidable challenges in de novo protein design.Navigating this intricate landscape necessitates the development of a diverse set of effective tools enabling the targeted design of de novo proteins with specific structural features or properties.</p>
<p>Over the past years, in the field of de novo protein design, data-driven and machine learning methods have emerged as powerful tools, offering valuable insights and accelerating the discovery of novel proteins with desired properties [2,3,4,5,6,7,8,9,10,11,12,13,14,15].These methods have opened great avenues for predicting structure, properties, and functions of proteins solely based on their underlying AA sequence.For instance, the development of deep learning (DL)-based AlphaFold 2 marked a significant breakthrough in the field of 3D folding protein prediction with a level of accuracy that in some cases rivaled expensive and time-consuming experimental techniques.[16] Moreover, deep learning-based models have been developed to explore structure-property relationships in the analysis and design of proteins.These models encompass a broad spectrum of structural and mechanical properties, serving either as constraints or target values.For example, various DL-models developed predict the secondary structure of proteins from their primary sequences.Prediction of mechanical properties of spider silk protein sequences have been enabled by DL models [17,18,19,20,21,22].Moreover, DL-based models such as graph neural networks [23] and transformer-based language models [24] show enhanced accuracy in predicting the protein natural frequencies compared to physics-based all-atom molecular simulations.The development of such DL models significantly reduces the cost of screening the vast sequence space to target proteins with improved or optimized mechanical performance.In the field of de novo protein design, data-driven and machine learning methods have emerged as powerful tools, offering valuable insights and accelerating the discovery of novel proteins with desired properties [2,3,4,5,6,7,8,9,10,11,12,13,14,15].These methods have opened great avenues for predicting structure, properties, and functions of proteins solely based on their underlying AA sequence.For instance, the development of deep learning (DL)-based AlphaFold 2 marked a significant breakthrough in the field of 3D folding protein prediction with a level of accuracy that in some cases rivaled expensive and time-consuming experimental techniques.[16] Moreover, deep learning-based models have been developed to explore structure-property relationships in the analysis and design of proteins.These models encompass a broad spectrum of structural and mechanical properties, serving either as constraints or target values.For example, various DL-models developed predict the secondary structure of proteins from their primary sequences.Prediction of mechanical properties of spider silk protein sequences have been enabled by DL models [17,18,19,20,21,22].Moreover, DL-based models such as graph neural networks [23] and transformer-based language models [24] show enhanced accuracy in predicting the protein natural frequencies compared to physics-based all atomistic simulations.The development of such DL models significantly reduces the cost of screening the vast sequence space to target proteins with improved or optimized mechanical performance.</p>
<p>A frontier, however, that still exists is how we can create intelligent tools that can solve complex tasks and draw upon a diverse set of knowledge, tools and abilities.Another critical issue is that the combination of purely data-driven tools with physics-based modeling is important for accurate predictions.Moreover, such tools should ideally also be able to retrieve knowledge from, for instance, the literature or the internet.All these aspects must be combined in a nonlinear manner where multiple dependent steps in the iteration towards and answer are necessary to ultimately provide the solution to a task.As we will discuss in this study, such an integration of tools, methods, logic, reasoning and iterative solution can be implemented through the deployment of a multi-agent system driven by sophisticated Large Language Models (LLMs).</p>
<p>LLMs [25,26] have represented a paradigm shift in modeling problems across a spectrum of scientific and engineering domains [27,28,29,30,8,31,32,33,34,35,36,37,38,39,40,41].Such models, built upon attention mechanism and transformer architectures [42], have emerged as powerful tools recently in the field of materials science and related areas, contributing to various aspects ranging from knowledge retrieval to modeling, design, and analysis.For example, models such as ChatGPT and the underlying GPT-4 architecture [43], part of the Generative Pretrained Transformer (GPT) class, demonstrate exceptional proficiency in mastering human language, coding [44], logic and reasoning [45].Recent Figure 1: Multi-agent AI framework for automating protein discovery and analysis.a, A genetic agent structure in a multi-agent modeling environment that can communicate via language, has a focus defined by a profile, and has access to custom functions.b, A function is customized by a profile and a set of parameters.c, The structure of a team of agents, each with special expertise, that communicate to each other and allow for mutual correction and a division of labor.Given different profiles for each agent, agents are designed that are expert on describing the problem (user_proxy), plan making (planner), function executing (assistant), and result evaluation (critic).The whole process is automated via a dynamic group chat under the leading chat manager, offering a versatile approach in solving challenging tasks in the context of protein design and analysis without human intervention.studies highlight their ability to proficiently program numerical algorithms and troubleshoot code errors across several programming languages like Python, MATLAB, Julia, C, and C++ [46].The GPT class of LLMs has also represented a new paradigm in simulating and predicting the materials behavior under different conditions [28], a field of materials science often reserved for conventional deep learning frameworks [47] such as Convolutional Neural Networks [48,49], Generative Adversarial Networks [50,51,52], Recurrent Neural Networks22,54,55 [20,53,54],and Graph Neural Networks [23,55,56,57,58].Moreover, due to their proficiency in processing and comprehending vast amount of different types of multimodal data, LLMs show promising capabilities in materials analysis and prediction application including key knowledge retrieval [35], general language tasks, hypothesis generation [29], and structure-to-property mapping [28,59].</p>
<p>At the same time, LLMs are typically not best equipped to solve specific physics-based forward and inverse design tasks, and are often focused on leveraging their conversational capabilities.Here, LLMs have been instrumental in powering conversable AI agents, facilitating the transition from AI-human conversations to AI-AI or AI-tools interactions for increased autonomy.[31,35,60,61,62] This capability represents a significant advancement, enabling intelligent mediation, fostering interdisciplinary collaboration, and driving innovation across disparate domains, including materials analysis, design, and manufacturing.The overall process could be deemed as adapting a problem-solving strategy dictated and directed by the AI system comprised of different agents.Thereby, the entire process can be AI automated with reduced or little human intervention.Depending on the complexity of the problem, using the idea of labor division, the agents have the capability to break the overall task into subtasks for which different agents or tools are used consecutively to iteratively solve the problem until all subtasks have accomplished and the solution has achieved.There is no intrinsic limitation in defining the type of tools, making the multi-agent model a versatile approach in addressing problems across scales and disciplines.The tools could range from a simple linear mathematical function to sophisticated deep neural network architectures.The multi-agent strategy has been explored in materials and mechanics applications through in earlier work [29] and was further explored in the context of molecular modeling tasks [35].</p>
<p>In this paper, we propose a multi-agent strategy to the protein design problems by introducing ProtAgents, a multi-agent modeling framework to solve protein-related analysis and design problems by leveraging customized functions across domains and disciplines.The core underpinning concept of the multi-agent systems is the use state-of-the-art LLMs combined with a series of other tools.The LLM backbone demonstrate exceptional abilities in analysis, rational thinking, and strategic planning, essential for complex problem-solving.Leveraged by these capabilities, the proposed model aims to reduce the need for human intervention and intelligence at different stages of protein design.The agent model consists a suite of AI and physics based components such as:</p>
<p>• Physics simulators: obtain new physical data from simulations, specifically normal modes and vibrational properties by solving partial differential equations (PDEs)</p>
<p>• Generative AI model: conditional/unconditional de novo protein design, based on a denoising diffusion model Figure 3: Overview of the multi-agent work to solve the complex task posed in experiment II, Section 2.2.First the multi-agent uses Chroma to generate de novo protein sequences and then computes natural frequencies and secondary structures content for the generated structures.Next, from de novo AA sequences, the model finds the 3D folded structures using OmegaFold and finally computes the frequencies and secondary structure content for the protein structures.The results obtained from the Chroma and OmegaFold 3D protein structures are compared in Figure 5.</p>
<p>• Fine-tuned transformer model: predict mechanical properties of proteins from their sequence • Retrieval agent: retrieve new data from a knowledge database of scientific literature</p>
<p>The resulting model has the ability to go beyond the conventional DL models by integrating new physical data or information across disciplines, for instance via writing and executing code that solves differential equations or other physics-based numerical methods, or that conducts retrieval augmented generation (RAG) [63].A tool-baked agent has access to various tools and functions with different functionalities that may be called upon, for instance, to predict a specific protein property or to obtain new physical data such as natural frequency from physics-based simulations.The versatility of the approach in solving complex tasks is exhibited by providing a series of experiments in the context of proteins design, modeling, and data analysis.</p>
<p>The plan of this paper is as follows.In Section 2, we present an overview of the multi-agent framework developed to tackle multi-objective complex tasks.Subsequently, we delve into a series of experiments where each task is initially introduced, followed by a detailed examination of various aspects throughout the problem-solving process by the multi-agent teamwork.A comprehensive discussion regarding the multi-agent framework and future prospects is provided in Section 3.</p>
<p>Results and Discussion</p>
<p>We present a series of computational experiments aimed at evaluating the effectiveness and potential of a multi-agent modeling framework for various challenges within the domain of protein modeling, design, and analysis.The multiagent framework consists of a team of agents, each powered by a state-of-the-art general purpose large language model, GPT-4, [43] accessed via the OpenAI API [64] and characterized by a unique profile that details its role, and communication protocols, such as sharing information and engaging with humans via language as shown in Figure 1a.Furthermore, agents are given access to a set of tools with various functionalities across domains.As shown in Figure 1b each function is characterized by a descriptive profile and input parameters.The outline of the proposed multi-agent framework is shown in Figure 1c, illustrating the collaborative efforts of a team of agents with the following entities</p>
<p>• "User": human that poses the question • "Planner": develops a plan to solve the task.Also suggests the functions to be executed.</p>
<p>• "Assistant": who has access to all the customized functions, methods, and APIs and executes them to find or compute the relevant data necessary to solve the task • "Critic": Responsible for providing feedback about the plan developed by "planner" as well as analyzing the results and handling the possible mistakes and providing the output to the user.</p>
<p>The agents are organized into a team structure, overseen by a manager who coordinates overall communication among the agents.A generic structure showing the dynamic collaboration between the team of agents proposed in the current study is depicted in Figure 2.Moreover, Table 1 lists the full profile for the agents recruited in our multi-agent framework.Further details can be found in the Materials and Methods section 4. The plan may involve calling custom function for retrieving knowledge, designing proteins, and computing and analyzing protein properties.You include the function names in the plan and the necessary parameters.If the plan involves retrieving knowledge, retain all the key points of the query asked by the user for the input message.It is noteworthy that critical issues in the realm of protein design surpass the capabilities of mere Python code writing and execution.Instead, addressing these challenges necessitates the utilization of external tools specifically tailored for protein design and analysis, and the writing, adaptation, correction and execution of code depends nonlinearly on the progression of the solution strategy that is developed by the system.</p>
<p>The tools are incorporated into the model via the Assistant agent who oversees executing the tools.To assess the performance of the multi-agent framework in handling complex interdisciplinary tasks, we have defined a rich library of functions each with special powers in solving the protein problems.Each function has a distinct profile describing its functionally and takes one or more required entities as the input.The functions provide the ability to, for instance, retrieve knowledge, perform protein folding, analyze the secondary structure, and predict some parameters through a pre-trained autoregressive language model.Additionally, a function can carry out simulations to compute the protein natural frequencies, thus allowing the model to integrate the new physics-based data.A full list of functions implemented in the current study is provided in Table S1 in the supplementary material.</p>
<p>Given the complexities residing in the protein design problems, the primary contribution of our multi-agent framework lies in assessing whether the team of agents can discern the requisite tools for a given query and evaluating the framework's capability to initiate the execution of these tools, along with providing the necessary inputs.The designated tasks are intentionally designed to be sufficiently complex, involving multiple subtasks where, for some cases, the execution of each depends on the successful completion of the preceding ones.This design showcases the model's capacity for the automated handling of intricate tasks, eliminating or substantially reducing the need for human intervention.Although the multi-agent framework allows for the human intervention at different stages, we skip that to further examine the team's capability in handling different possible situations, for instance in case of a failure.</p>
<p>Experiment I: Knowledge retrieval, computations, and analysis</p>
<p>As the first example, we pose a task which involves a multi-step knowledge retrieval, analysis, and computations for a set of protein PDB identifiers.We start the first round of conversation by posing the following question: user_proxy (to chat_manager):</p>
<p>Can you provide examples of protein names whose mechanical properties have been studied through experiments?</p>
<p>The planer then correctly suggests the function "retrieve_content" to be executed with the argument "examples of protein names whose mechanical properties have been studied through experiments".Upon execution of the function, the assistant provides us with a list of protein names.Upon inspection, we find that the agent has successfully identified experimentally studied proteins, despite an abundance of information on proteins studied theoretically, for instance, through coarse-grain simulations.Since we are interested in the PDB ids, we continue the chat by a follow-up question "Can you provide me with the PDB ids for these proteins?"when "user_proxy" is being asked to provide feedback to chat manager.Again, the planner suggests "retrieve_content" function with the following message Planner (to chat_manager):</p>
<p><strong><em>*</em> Suggested function Call: retrieve_content </strong>*** Arguments: { "message": "PDB ids for I27 domain from titin, fibronectin type III domains, E2lip3, T4 lysozyme, ubiquitin, titin immunoglobulin domains, cell adhesion molecule, spectrin repeats, and tenascin" }</p>
<p>The "Assistant" agent then calls the function and gives the following output:</p>
<p>Assistant (to chat_manager):</p>
<p>Here are the PDB IDs for the proteins you mentioned: Upon careful examination of the results, we observe that, despite all the PDB ids exist in the source database, the PDB ids do not quite match with the protein names except for a few cases.Nevertheless, note that the error is caused by the poor performance of the "retrieve_content" function and the team of agents cannot mitigate that as they have no access to the knowledge database.As such, we continue to test the agent-team capability in more challenging queries centered around computational tasks and physics-based simulations by assigning the following task in the next round of conversation user_proxy (to chat_manager):</p>
<p>For the first 10 protein PDB ids in the list, if the AA sequence length of the protein is less than 128, do the following: (a) analyze the secondary structure of the protein, (b) compute its first 13 natural frequencies, (c) determine its structure classification.</p>
<p>The above is a complex multi-step analysis and computation task encompasses aspects such as secondary structures analysis, natural frequency calculations, and structure classification.Additionally, the task is subject to an initial condition that must be satisfied before proceeding through the next sequence of steps, adding an extra layer of complexity.In response, the planner comes up with a detailed plan which consists of all the actions that need to be taken to complete the task.Moreover, the plan mentions all the corresponding functions that need to be executed to accomplish the task.More importantly, the "planner" perfectly realizes to fetch the protein structures first before starting to analyze the secondary structure, although it was not explicitly mentioned in the task query.</p>
<p>The teamwork proceeds by a follow-up feedback provided by the "critic" agent about all the plan steps and functions which is concluded by the following statement critic (to chat_manager):</p>
<p>Overall, the plan rightly identifies all the necessary functions and respective parameters for each step.It illustrates a good understanding of what needs to be done to fulfill the user request.Good job!Therefore, the positive feedback from the "critic" further supports the good performance of the planner in addressing all the critical steps required to accomplish the tasks.</p>
<p>The "assistant" agent then follows the plan by calling and executing the corresponding functions, starting with AA length calculation, until all the steps have been undertaken.The results show that all the inputs to the functions are properly identified and provided and the functions are executed without any error.The conditional statement included in the tasks is also correctly satisfied for each protein, that is the computations are conducted only if the sequence length is less than 128 and are emitted otherwise.For instance, for the protein with PDB id "1hz6" the AA length is returned as 216 by the "assistant" which is then followed by the following message from the "critic" critic (to chat_manager):</p>
<p>The length of the amino-acid sequence for the PDB id '1hz6' is 216, which is greater than 128.Therefore, we will not perform the steps (a) analyze the secondary structure of the protein, (b) compute its first 13 natural frequencies, and (c) determine its structure classification for the '1hz6'.</p>
<p>Moving to the next PDB id in the list...After completion of all the tasks, the assistant returns a summary of all the results for each protein as representatively shown below for PDB id "1wit":</p>
<p>assistant (to chat_manager):</p>
<p>For PDB id '1wit':</p>
<p>-It has an amino acid sequence length of 93.</p>
<p>-The analysis of its secondary structure yielded the following composition: H: 0%, B: The results suggest that the framework effectively retains all outputs, demonstrating its strong memory even in the face of diverse and extended results.As the last round of conversation, we ask to save all the results which allows us to load them at later time for other purposes:</p>
<p>user_proxy (to chat_manager):</p>
<p>Save all the results into a csv file where each row corresponds to a protein and each column represent a protein property, utilizing proper identifiers.</p>
<p>In response, the planner suggests to call the python function "save_to_csv_file".The main task here is to generate the dictionary of results in JSON and with appropriate structure as instructed by the user.However, we see that upon generating the JSON data and inputting it into the function by the "assistant" agent, the following error occurs:</p>
<p><strong><em>*</em> Response from calling function "save_to_csv_file" </strong>*** Error: Expecting ',' delimiter: line 1 column 141 (char 140)</p>
<p>Without any human intervention, the agent team is able to resolve the issue by mutual correction.In particular, the "critic" identifies the cause of error by writing Critic (to chat_manager):</p>
<p>It seems like there is an error in the JSON format being used to feed data into the csv conversion function.The JSON string appears to be improperly formed leading the function to fail.The values for "Secondary Structure" and "First 13 Frequencies" are stringified JSON objects themselves, which maybe causing</p>
<p>The "assistant" then reconstructs the JSON file from the output results and is able to successfully execute the function and thus save the results in a csv file as shown in Table 2.The complete group chat records can be found in Table S2 of the Supplementary material.</p>
<p>Experiment II: De novo protein design using Chroma</p>
<p>An important characteristic of the multi-agent model is its capability in handling very complex tasks in the context of protein design and analysis.This partly stems from the possibility of incorporating customized functions in the model for various purposes from knowledge retrieval to performing physics-based simulations.In this example, we explore the model's performance in using the state-of-the art models in de novo protein design, Chroma [15], and protein folding, OmegaFold [4].Chroma offers a unified approach for protein design that can directly sample novel protein structures and sequences with the possibility to direct the generative design towards the desired functions and properties by appropriate conditioning.OmegaFold is a state-of-the-art folding method without the need of using multiple sequence alignments (MSA) to predict the three-dimensional folded protein structure given its AA sequence.</p>
<p>In this experiment, we formulate a complex multi-step task with the objective of comparing the two models based on various structural and physical features derived from the folded structures obtained through Chroma and OmegaFold2.</p>
<p>We pose the following task through the "user_proxy" agent:</p>
<p>user_proxy (to chat_manager):</p>
<p>Design three proteins with sequence length of 120 and analyze their secondary structure and first 10 frequencies.Then fold the proteins and repeat the analysis.Save all the results (including the AA sequences) into a csv file where each row corresponds to a protein and each column represents a protein property, utilizing proper identifiers.</p>
<p>The "planner" then suggests the following plan Planner (to chat_manager): Plan: 1. Use the 'design_protein_from_length' function three times to design three proteins each with a sequence length of 120.I will use 1, 2, 3 as names for the proteins.Save the amino acid sequences and PDB filenames.At the first glance, the plan seems to cover all the details necessary to accomplish the tasks included in the problem statement.However, the "critic" agent who is responsible in giving feedback about the plan spots a minuscule error in the saving part of the plan as follows:</p>
<p>Critic (to chat_manager):</p>
<p>Your plan is almost perfect.However, when a protein is folded, the protein sequence doesn't change, only the structure does, which then affects the secondary structure and possible frequencies you can calculate.So you need to only save Amino Acid Sequence once after designing the proteins.Each protein should have the following column properties:</p>
<p>-Protein Number -Amino Acid Sequence -Secondary Structure (Pre-Fold) -Frequencies (Pre-Fold)</p>
<p>-Secondary Structure (Post-Fold) -Frequencies (Post-Fold)</p>
<p>Other than that, your plan works perfectly.So steps will be as follows: ... The correction made by the "critic"concerning the sequence length underscores its notable proficiency in comprehending how diverse functions and parameters influence various aspects within the realm of protein design.</p>
<p>The "user_proxy" agent is then asked to confirm the plan.The "assistant" then takes the stage and starts following the plan by calling and executing the functions until all the steps have been undertaken.An overview of the work performed by the "assistant" is depicted in Figure 3.At the end of the computations, the results are formatted into a JSON dictionary to fed into the "save_to_csv_file" function.However, an error related to the JSON dictionary format occurs when executing the function as follows: The "critic" then steps in by making the following comment and suggesting a plan to fix the error as follows:
*
Critic (to chat_manager): I apologize for the mistake.It seems there was an issue with the formatting of the input JSON dictionary for saving to the CSV file.Let's try again with properly formatted JSON.</p>
<p>Plan to fix the error:</p>
<ol>
<li>Correctly format JSON with double quotes instead of single quotations.2. Call 'save_to_csv_file' function again with correctly formatted JSON dictionary.I'll directly proceed with this plan as the error was due to syntax not plan.</li>
</ol>
<p>The critic makes the necessary corrections and suggest the corrected JSON dictionary for the "assistant" to execute the "save_to_csv_file" function.This time, the function is successfully executed and the results are saved into a csv file as shown in Table 3.At last, the "critic" gives an evolution about the whole process:</p>
<p>Critic (to chat_manager):</p>
<p>The plan was executed successfully and all data including Amino Acid Sequence, Secondary Structure (Pre-Fold and Post-Fold), and Frequencies (Pre-Fold and Post-Fold) for three proteins have been saved into a CSV file named 'protein_data.csv'.Each row of the file corresponds to a protein and each column represents a protein property.</p>
<p>This experiment showcases a good example of multi-agent collaboration in solving complex tasks in the context of de novo protein design and analysis.It specially shows the great capability of the "critic" agent in providing valuable feedback to other working agents at different stages of the problem solving endeavor, further assisting the team of agents in handling possible errors without the need for human involvement.Figure 5 shows the plots of the generated results including the 3D folded structures.The full conversations can be found in Table S3 of the Supplementary material.</p>
<p>Experiment III: Protein design conditioned on the protein CATH class</p>
<p>CATH is a hierarchical classification system for protein structures that consists of four main levels.The highest level in this hierarchy is the "Class" which primarily characterizes the secondary structure content of the protein.For example, C 1 , C 2 , and C 3 correspond to proteins predominantly composed of α-helix, mainly β-sheet, and a combination of α and β secondary structures.Consequently, designing proteins based on the CATH class number, i.e.C 1 , C 2 , C 3 , can be understood as creating proteins with a specific fractional content of the secondary structure.Previous studies have demonstrated the importance of the protein secondary structures content, specially α-helix/β-sheet ratio, on the mechanical properties of the protein materials [65,66].For instance, α-helix-rich proteins tend to yield stretchy materials [67], while β-sheet-rich ones produce rigid materials.[68,69,70] Chroma has the potential to conditionally generate proteins with specified folds according to CATH class annotations at three levels.[15]   First the multi-agent uses Chroma to generate de novo protein sequences and structures conditioned on the input CATH class.Then using the generated protein structures, the natural frequencies and secondary structures content are computed.Next, the force (maximum force along the unfolding force-extension curve) and energy (the area under the force-extension curve) are computed from de novo AA sequences using ProteinForceGPT.</p>
<p>In this example, we task the multi-agent team with generating proteins based on their fractional content of the secondary structure and subsequently performing computational and structural analysis tasks.Specifically, in addition to secondary structure analysis and natural frequency calculations, as covered in previous examples, we instruct the team to compute the maximum unfolding force (maximum force in the unfolding force-separation curve) and unfolding energy (the area under the unfolding force-separation curve) for each generated protein.To accomplish the latter, we have equipped the multi-agent team with a custom function that utilizes a trained autoregressive transformer generative AI model, ForceGPT.In addition to maximum unfolding force and energy, the trained generative model is able to predict the entire unfolding force-separation curve based solely on the protein amino acid sequence.Furthermore, the model has the capability to perform inverse design tasks by generating protein AA sequences that yield desired unfolding behavior.Detailed information about the training of the model can be found in Materials and Methods section4.The task given is:</p>
<p>user_proxy (to chat_manager):</p>
<p>Design two proteins of length 100 for each family of mainly beta, mainly alpha, and alpha beta proteins.Then compute the maximum force and unfolding energy, analyze the secondary structure and compute the first 10 frequencies for these proteins.</p>
<p>ProtAgents: Protein discovery by combining physics and machine learning Note that, as before, we do not specify any particular function or offer hints for selecting the appropriate function to accomplish the tasks.Instead, we empower the agents to formulate a plan, wherein they decide which functions to select and determine the input parameters.The planner outlines the following plan for the given task:</p>
<p>Planner (to chat_manager):</p>
<ol>
<li>
<p>Use the "design_protein_from_CATH" function with inputs of length of 100, CATH_ANNOTATION of 1 (mainly alpha), and a protein name for the first mainly alpha protein.Repeat with a different name for the second protein.</p>
</li>
<li>
<p>Use the same function again with inputs of length 100, CATH_ANNOTATION of 2 (mainly beta), and a protein name for the first mainly beta protein.Repeat with a different name for the second protein.</p>
</li>
<li>
<p>Use the function once more with inputs of length 100, CATH_ANNOTATION of 3 (alpha beta), and a protein name for the first alpha beta protein.Repeat with a different name for the second protein.4. Use the "analyze_protein_structure" function on each of the protein structures obtained.5. Use the "calucalte_force_energy_from_seq" function to compute the maximum force and unfolding energy for each of the proteins.6. Use the "calc_protein_ANM" function to compute the first 10 frequencies for each protein.</p>
</li>
</ol>
<p>It can be seen that the planner demonstrates good performance in breaking the task into sub-tasks to be accomplished step by step.Moreover, it has identified and suggested the correct functions and corresponding input parameters for each sub-task.The plan is further supported by the "critic" who provides positive feedback as follows:</p>
<p>Critic (to chat_manager):</p>
<p>Yes, this plan sounds good.It covers each step required for the task.The parameter inputs for each function match with the required parameters as stated in their function definitions.Thus, this plan should successfully accomplish the task.</p>
<p>The multi-agent team then proceeds to execute the different steps outlined in the plan by calling and executing the functions.Specifically, the function 'design_protein_from_CATH' is executed with the appropriate 'CATH_ANNOTATION' for a specific protein structure design, as outlined in the plan.Following the generation of all proteins, the executions are followed by structural analysis and force and energy computations.It's noteworthy that the model exhibits good performance in restoring and memorizing the sequences of the generated proteins, which are essential for the force and energy calculations.Finally, the team successfully completes the task by computing the first 10 frequencies for each protein.An overview of the computations performed by the team of agents for this experiment is shown in Figure 4.</p>
<p>Given the complexity of the problem involving numerous computational tasks, a decent number of results have been generated in the first round of the conversation.In the next round, to evaluate the team's ability to memorize and restore the results, we present the following task: user_proxy (to chat_manager):</p>
<p>Could you save the results in a CSV file named "protein_analysis.csv,"where each row corresponds to a protein, and each column represents a specific property?Include the AA sequence in the results and use suitable identifiers for the columns.</p>
<p>In this task, we not only request the team to save the data but also require them to adhere to a customized format when storing the results.The model is proficient in creating a JSON dictionary that satisfies the specified format and saving the results to a CSV file, as illustrated in Table 4.</p>
<p>The plots of the obtained results are shown in Figure 4.The results indicate that Chroma has done a poor performance in creating β-rich protein named mainly_beta_protein_2 which its structure is dominant in α-helix.As an attempt to test the capability of the multi-agent model in analyzing the results, in the last round of the conversation, we ask the model to assess Chroma's performance in generating the proteins conditioned on the secondary structure by posing the following question:</p>
<p>user_proxy (to chat_manager):</p>
<p>Based on the results of this example, can you check if the protein generator (Chroma) has been successful in creating proteins with desired structure?Figure 5: The results generated by the multi-agent collaboration for the experiment II, Section 2.2.The first and second columns depict the 3D folded structures of proteins generated by Chroma and OmegaFold2, respectively, while the third and fourth columns represent the fractional content of secondary structures, and first ten natural frequencies for the generated proteins.</p>
<p>The "critic" agent conducts a thorough evaluation of Chroma's performance in generating proteins with targeted secondary structure content.Through a detailed analysis of each CATH structure, it reveals the inherent strengths and weaknesses in Chroma's capabilities.Specifically, addressing the limitations of Chroma's performance, the critic's evaluation provides the following observations for the mainly beta proteins:</p>
<p>-The mainly beta proteins showed higher percentages of extended strand/beta-sheet secondary structure ('E').Though, the percentages varied quite a bit (64% for mainly_beta_protein_1 and only 8% for mainly_beta_protein_2), which could be due to the complex nature of beta-structures.</p>
<p>This illustration not only highlights the multi-agent model's proficiency in computational tasks but also underscores its intelligence in handling intricate data analyses-an aspect traditionally reserved for human.The full conversations for this experiment can be found in Table S4 of the Supplementary material.</p>
<p>Conclusions</p>
<p>Large Language Models (LLMs) have made remarkable strides, revealing their immense potential to potentially replicate human-like intelligence across diverse domains and modalities, demonstrating proficiency in comprehending extensive collective knowledge and proving adept at effectively applying this information.However, to reach intelligent  problem-solving systems, these types of models are not yet sufficient and require integration with other methods.In this study we explored the capability of AI agents to solve protein design problems in an autonomous manner without the need for human intervention.The agents have been powered by a general purpose LLM model, GPT-4, which allows them to communicate via conversation.It should be noted that the general capabilities of the AI agents powered by the LLM plays an important role at different stages of the problem solving.In our case, GPT-4-powered agents showed excellent proficiency specially in problem understanding, strategy development, and criticizing the outcomes.Such an AI system is not limited to mere linguistic interactions between agents; they have the capacity to incorporate a variety of special-purpose modeling and simulation tools, human input, tools for knowledge retrieval, and even deep learning-based surrogate models to solve particular tasks.Furthermore, additional tools can be integrated into the multi-agent system with popular external APIs and up-to-date knowledge about special topics can be retrieved by searching and browsing the web through specialized API interfaces.By harnessing the collective abilities of agents, including reasoning, tool usage, criticism, mutual correction, adaptation to new observations, and communication this framework has proven highly effective in navigating intricate challenges including protein design.</p>
<p>To achieve this goal we constructed a group of agents, each assigned a unique profile through initial prompts, to dynamically interact in a group chat via conversations and make decisions and take actions based on their observations.The agents profile outlines their attributes, roles, and functionalities within the system and describe communication protocols to exchange information with other agents in the system.Our team of agents include a user proxy to pose the query, a planner to formulate a plan, an function-baked assistant to execute the functions, and a critic to evaluate the outcome and criticizing the performance.We also use a chat manager to lead the group chat by dynamically choosing the working agent based on the current outcome and the agents' roles.Through a series of experiments, we unleashed the power of agents in not only conducting the roles they were assigned to, but to autonomously collaborate by discussion powered by the all-purpose LLM.For example, the agent playing the role of a planner successfully identified all the tasks in the query and suggested a details plan including the necessary functions to accomplish them.Furthermore, the agent assigned the critic role, is able to give constructive feedback about the plan or provide suggestions in case of failure, to correct errors that may emerge.Our experiments have showcased the great potential of the multi-agent modeling framework in tackling complex tasks as well as integrating AI-agents into physics-based modeling.</p>
<p>Multi-agent modeling is a powerful technique that offers enhanced problem-solving capacity as shown here in various computational experiments in the realm of protein design, physics modeling, and analysis.Given a complex query comprising multi-objective tasks, using the idea of division of labor, the model excels at developing a strategy to break the task into sub-tasks and then, recruiting a set of agents to effectively engage in problem solving tasks in an autonomous fashion.Tool-backed agents have the capacity to execute tools via function execution.We equipped an agent with a rich library of tools that span a broad spectrum of functionalities including de novo protein design, protein folding, and protein secondary structure analysis among others.The fact that there is no intrinsic limitation in customizing the functions, allows us to integrate knowledge across different disciplines into our model and analysis, for instance by integrating knowledge retrieval systems or retrieving physical data via simulations.For instance, here we utilized coarse grained simulations to obtain natural frequencies of proteins but the model offers a high flexibility in defining functions that focus on other particular area simulation (e.g. an expert in performing Density Functional Theory, Molecular Dynamics, or even physics-inspired neural network solvers [71,72,37]).Multi-agent framework can also accelerate the discovery of de novo proteins with targeted mechanical properties by embracing the power of robust end-to-end deep models solving forward and inverse protein design problems [24,59,17,73,74,75,76] Developing these models that connect some structural protein features, such as secondary structure, to a material property, such as toughness or strength have gained a lot of attention recently.Here, we used a pre-trained autoregressive transformer model to predict the maximum force and energy of protein unfolding, but other end-to-end models could also be utilized.In the context of inverse protein design problems, a team of two agents, one expert in the forward tasks and the other in the inverse task, can be collaborated to assist the cycle check wherein the de novo proteins certainly meet the specified property criteria.Along the same line, one could benefit from the multi-agent collaboration in evaluating the accuracy of generative models in conditional designing of proteins or compare the created 3D structures with the state-of-the art folding tools [16,77,78].For example, through an automated process of protein generation and structure analysis, our ProtAgents framework revealed the shortcomings of Chroma in designing β-sheet-rich proteins.</p>
<p>In another example, the folded 3D structures of Chroma were compared with those obtained by OmegaFold2.All these examples, demonstrate the capacity of multi-agent framework in a wide range of applications in the context of protein design and analysis.Lastly, the model enables integrating various information across scales, whether new protein sequences or physics simulations output in form of rich data structures, for inclusion in easily readable file formats (like JSON) to be used by other agents or to be stored for future analysis.</p>
<p>Designing de novo proteins that meet special objectives in term of mechanical or structural properties present unique challenges calling for new strategies.The prevailing strategies often rely on developing data-driven end-to-end deep learning models to find the complex mapping from protein constitutive structure to property or vice versa.However, these models often focus on specific properties, limiting their functionality in multi-objective design purposes where several criteria needs to be met.To overcome these challenges and propel the field forward, future research endeavors could revolve around the development of an integrated system of agents designed to automate the entire lifecycle of training deep neural networks for protein design.Each agent within this system could be assigned specific responsibilities, such as data generation through simulations, data curation for ensuring quality and relevance, and the execution of the code required for model training.Additionally, a critic agent could monitor and critique the training process, making decisions like early stopping or tuning hyperparameters to enhance the model's accuracy.This collaborative and automated approach would not only streamline the design process but also contribute to achieving higher or desired levels of accuracy in the generated models.Furthermore, this agent-based strategy can extend to on-the-fly active learning, where agents dynamically adapt the model based on real-time feedback, improving its performance iteratively.By incorporating intelligent agents at every stage of the process, the proposed system aims to revolutionize the landscape of de novo protein design, making it more efficient, adaptive, and capable of meeting diverse and complex design objectives, offering a new paradigm in materials design workflows.</p>
<p>Materials and Methods</p>
<p>Agent design</p>
<p>As shown in Figure 1a, we design AI agents using the state-of-the-art all-purpose LLM GPT-4 and dynamic multi-agent collaboration is implemented in AutoGen framework [79], an open-source ecosystem for agent-based AI modeling.Additional agents are introduced as described below, including some based on generative AI as well as physics modeling.</p>
<p>In our multi-agent system, the human user_proxy agent is constructed using UserProxyAgent class from Autogen, and Assistant, Planner, Critic agents are created via AssistantAgent class from Autogen; and the group chat manager is created using GroupChatManager class.Each agent is assigned a role through a profile description listed in Table 1, included as system_message at their creation.</p>
<p>Function and tool design</p>
<p>All the tools implemented in this work are defined as python functions.Each function is characterized by a name, a description, and input properties with a description as tabulated in Table S1 of the Supplementary Material.The list of functions are incorporated into the multi-agent system, included as the function_map parameter in the Assistant agent at its creation.</p>
<p>Autoregressive transformer model to predict protein unfolding force-extension from sequences</p>
<p>We use a special-purpose GPT-style model denoted as ProteinForceGPT, similar as in , here trained to predict forceextension curves from sequences along with other mechanical properties, and vice versa (https://huggingface.co/ lamm-mit/ProteinForceGPT). The protein language model is based on the NeoGPT-X architecture and uses rotary positional embeddings (RoPE) [80].The model has 16 attention heads, 36 hidden layers and a hidden size of 1024, an intermediate size of 4096 and uses GeLU activation functions.</p>
<p>Pre-training was conducted based on a dataset of ∼ 800, 000 amino acid sequences, using next-token predictions using a "Sequence" task (https://huggingface.co/datasets/lamm-mit/GPTProteinPretrained):</p>
<p>Sequence<GEECDCGSPSNPCCDAATCKLRPGAQCADGLCCDQCRFKKKRTICRIARGDFPDDRCTGQSADCPRWN></p>
<p>The ProteinForceGPT model was then fine-tuned bidirectionally, to predict mechanical properties of proteins from their sequence, as well as sequence candidates that meet a required force-extension behavior and various other properties.Fine-tuning is conducted using a dataset derived from molecular dynamics (MD) simulations [81].Sample tasks for the model include: Sample results from validation of the model are shown in Figure S2.We only use forward predictions for use in the agent model reported here.</p>
<p>Software versions and hardware</p>
<p>We develop our multi-agent models using local workstations with NVIDIA GPUs.We use Python 3.10 and pyautogen-0.2.2 [82].Additional implementation details are included in the code.</p>
<p>Visualization</p>
<p>We use Py3DMol [83] for visualization of the protein structures.</p>
<p>Secondary Structure Analysis</p>
<p>We use the dictionary of protein secondary structure (DSSP) [84] module via BioPython [85] to analyze the secondary structure content of the proteins from its geometry.</p>
<p>Natural Vibrational Frequency Calculations</p>
<p>We perform Anisotropic Network Model (ANM) [86,87] calculations as implemented in ProDy [88] for normal mode analysis.The problem is solved by considering the protein as a network of interactions, defined within a cutoff distance for which spring-like potentials are assumed to define molecular interactions.</p>
<p>Retrieval Augmented Generation</p>
<p>We use Llama Index [89] as a tool to implement RAG where the full text of papers cited as references [65,66] are used as external sources from which information can be retrieved by the system in real-time.</p>
<p>Figure 2 :
2
Figure 2: A generic flowchart showing the dynamic interaction between the multi-agent team members organized by the group chat manager to solve protein design and analysis problems.The manager selects the working agents to collaborate in the team work based on the current context of the chat, thus forming close interactions and enabling mutual corrections.</p>
<p>have access to all the custom functions.You focus on executing the functions suggested by the planner or the critic.You also have the ability to prepare the required input parameters for the functions.4Critic user_proxy.You double-check the plan, especially the functions and function parameters.Check whether the plan included all the necessary parameters for the suggested function.You provide feedback.5Groupchat manager You repeat the following steps: Dynamically selecting a speaker, collecting responses, and broadcasting the message to the group.</p>
<p>2 .
2
Use the 'analyze_protein_structure' function to analyze the secondary structure of the proteins.Store the results.3. Use the 'calc_protein_ANM' function to calculate the first 10 frequencies of the proteins.Store the results.4. Use the 'fold_protein' function to fold the proteins.5. Repeat steps 2 and 3 with the folded proteins.6. Use the 'save_to_csv_file' function to save all results into a csv file where each row corresponds to a protein and each column represents a protein property.7. The identifiers will be named as follows: -Protein Name -Amino Acid Sequence (Pre-Fold) -Secondary Structure (Pre-Fold) -Frequencies (Pre-Fold) -Amino Acid Sequence (Post-Fold) -Secondary Structure (Post-Fold) -Frequencies (Post-Fold)</p>
<p><strong><em>* Response from calling function "save_to_csv_file" </em></strong><strong> Error: Expecting property name enclosed in double quotes: line 2 column 5 (char 6) </strong><strong><em>*</em></strong><strong><em>*</em></strong><strong><em>*</em></strong><strong><em>*</em></strong><strong><em>*</em></strong><strong><em>*</em></strong><strong><em>*</em></strong><strong><em>*</em></strong>**</p>
<p>Figure 4 :
4
Figure 4: Overview of the multi-agent work to solve the complex task posed in experiment III, Section 2.3.First the multi-agent uses Chroma to generate de novo protein sequences and structures conditioned on the input CATH class.Then using the generated protein structures, the natural frequencies and secondary structures content are computed.Next, the force (maximum force along the unfolding force-extension curve) and energy (the area under the force-extension curve) are computed from de novo AA sequences using ProteinForceGPT.</p>
<p>Figure 6 :
6
Figure 6: The results generated by the multi-agent collaboration for the experiment III, Section 2.3.The first and second columns depict the 3d folded structures and the last column represents the fractional content of secondary structures for the two proteins generated by Chroma conditioned on the CATH class of (a) 1: mainly alpha protein, (b) 2: mainly beta protein, and (c) 3: alpha beta protein.</p>
<p>Table 1 :
1
The profiles of the agents implemented in the current study to solve multi-objective tasks in the context of protein design and analysis.You develop a plan.Begin by explaining the plan.Revise the plan based on feedback from the critic and user_proxy, until user_proxy approval.
Agent #Agent roleAgent profile1user_proxyuser_proxy. Plan execution needs to be approved by user_proxy2PlannerPlanner.</p>
<p>Table 2 :
2
The results generated by the group chat in the form of a CSV file, without human intervention, for the experiment I, Section 2.1.
Protein ID #Amino Acid LengthSecondary StructureFirst 13 FrequenciesCATH Classification1wit93['H': 0.0, 'B': 3.23, 'E': 51.61, 'G': 3.23, 'I': 0.0, 'T':[4.3755, 5.0866, 5.5052, 6.7967, 7.908, 8.1947, 9.0166,2.60.40.1013.98, 'S': 5.38, 'P': 0.0, '-': 22.58]9.8528, 11.0632, 11.3968, 11.7355, 12.1279, 12.3498]1ubq76['H': 15.79, 'B': 2.63, 'E': 31.58, 'G': 7.89, 'I': 0.0, 'T':[0.7722, 1.0376, 1.5225, 1.6534, 2.5441, 2.9513, 3.2873,3.10.20.9015.79, 'S': 5.26, 'P': 5.26, '-': 15.79]3.7214, 4.1792, 4.3437, 4.3908, 4.6551, 5.1631]1nct106['H': 0.0, 'B': 4.08, 'E': 35.71, 'G': 0.0, 'I': 0.0, 'T': 2.04,[3.6644, 4.425, 6.5351, 6.7432, 7.1409, 7.1986, 9.0207,2.60.40.10'S': 21.43, 'P': 0.0, '-': 36.73]9.2223, 10.3163, 10.7313, 11.5299, 11.6373, 12.5606]1tit98['H': 0.0, 'B': 1.12, 'E': 35.96, 'G': 0.0, 'I': 0.0, 'T': 6.74,[5.5288, 5.9092, 8.2775, 8.6267, 9.3391, 9.8783, 10.1607,2.60.40.10'S': 17.98, 'P': 0.0, '-': 38.2]11.451, 11.5896, 11.7052, 12.1498, 12.6082, 13.8622]1qjo80['H': 0.0, 'B': 2.5, 'E': 40.0, 'G': 0.0, 'I': 0.0, 'T': 8.75,[3.8578, 4.4398, 5.4886, 5.7815, 6.6332, 6.9269, 7.2329,2.40.50.100'S': 13.75, 'P': 0.0, '-': 35.0]7.6453, 8.2545, 8.3076, 8.6118, 8.7135, 8.8546]2ptl78['H': 15.38, 'B': 1.28, 'E': 30.77, 'G': 0.0, 'I': 0.0, 'T':[0.0386, 0.1161, 0.2502, 0.5921, 1.1515, 1.5257, 2.0924,3.10.20.107.69, 'S': 19.23, 'P': 0.0, '-': 25.64]2.6793, 3.4292, 3.9289, 4.2172, 4.6878, 4.8022]</p>
<p>Table 3 :
3
The final results generated by the group chat in the form of a CSV file, without human intervention, for the second experiment II, Section 2.2.
Protein Number #Amino Acid SequenceSecondary Structure (Pre-Frequencies (Pre-Fold)Secondary Structure (Post-Frequencies (Post-Fold)Fold)Fold)1MIIINIKTENGLSITYNSD'H': 13.3333333333, 'B':[2.0337, 2.8678, 3.3843,'H': 15.8333333333, 'B': 0.0,[1.8739, 2.1563, 2.7611,EKKLELKYTPVKSPEDFK0.0, 'E': 46.6666666666,3.6263, 3.9904, 4.5381,'E': 46.666666666, 'G': 2.5,3.1086, 3.8712, 4.0481,FPEDAKATISEVEYKGKK'G': 0.0, 'I': 0.0, 'T':4.8373, 4.8956, 5.1492,'I': 0.0, 'T': 14.1666666666,4.3759, 4.6717, 4.8183,VIKIDAKLYVSPDLSKAK14.1666666666, 'S': 7.5, 'P':5.4416]'S': 4.1666666666, 'P': 0.0, '-4.9126]LTIEVNADISQEEADKIID0.0, '-': 18.33333333333': 16.666666666EFIKLLESLGNIKLKVTKDGNKYTIEVE2GSPLPRPPLSPEEQEALR'H': 61.666666666, 'B':[0.0207, 0.1058, 0.1782,'H': 62.5, 'B': 0.0, 'E':[0.0444, 0.1641, 0.3379,KKAQEKYNEFVSKIKEL0.0, 'E': 11.6666666666,0.4189, 0.49, 0.9015, 1.1832,11.6666666666, 'G': 0.0, 'I':0.5724,0.765,0.9568,LRRAADRVRRGEPVELIE'G': 0.0, 'I': 0.0, 'T': 7.5,1.8257, 2.1212, 2.8726]0.0, 'T': 6.6666666666,1.4306, 1.5344, 1.6834,KTIKIGDYEYKIVATSPEE'S': 3.33333333333, 'P':'S':1.66666666666,1.8099]AKELENLIKEMIDLGFKP3.33333333333, '-': 12.5'P': 4.1666666666,'-':SKEFSDKLVEAARLIREG13.3333333333RVDEALRLLDEM3APLDPDDLSAQLRAAIDE'H': 57.499999999, 'B': 0.0,[0.7546, 1.0836, 1.5026,'H': 61.666666666, 'B':[0.5256, 1.0278, 1.1566,LVRLGYEEEVSKPEFIEA'E': 13.3333333333, 'G':1.8874, 2.0844, 2.3192,0.0, 'E': 15.0, 'G': 0.0, 'I':1.2877, 1.5521, 1.9111,LRLYALDLGLKEVVLRR0.0,'I': 4.1666666666,2.7975, 3.0199, 3.0669,0.0, 'T': 8.3333333333,2.1887,2.4664,2.734,VTPAPASQPGVYTVEDV'T':8.3333333333,3.1382]'S': 3.33333333333, 'P':2.8731]TVDLEALRKQELSPEEQA'S':3.33333333333,1.66666666666, '-': 10.0RLEKIRAKYDEMLADPE'P': 6.6666666666,'-':FQALLDEVLARARAA6.6666666666</p>
<p>Table 4 :
4
The final results generated by the group chat in the form of a CSV file, without human intervention, for the third experiment III, Section 2.3.
Protein Name #AA SequenceSecondary StructureUnfolding EnergyMax ForceFirst 10 Frequenciesmainly_alpha_protein_1SMKKIEDYIREKLKALGLSDEEI'H': 89.0, 'B': 0.0, 'E': 0.0, 'G': 0.0,0.3810.444[0.2329,0.4901,0.9331,1.3741,1.734EERVKQLMEGIKNPKKFEKELQ'I': 0.0, 'T': 4.0, 'S': 1.0, 'P': 0.0, '-':7,2.1598,2.3686,2.6359,2.8555,3.03KRNDRESLLIFKEAYALYEASK6.064]DKEKGKKLINKVQSERDKWETEQAEAARAAAAAmainly_alpha_protein_2MSKKEIEELKKKLDEIVETLKEY'H': 89.0, 'B': 0.0, 'E': 0.0, 'G': 0.0,0.3760.536[1.6126,2.0783,2.3073,2.4565,3.399,ARQGDDACKKAADLIEEVKKA'I': 0.0, 'T': 5.0, 'S': 0.0, 'P': 0.0, '-':3.475,4.1377,4.7104,4.8864,5.2187]LEEGNPEKYSQLKKKLTDAINK6.0AIEEYRKRFEAEGKPEEAQKVIDKLKKILDEITNmainly_beta_protein_1TTVTVTPPVADADGNEHSTVTA'H': 0.0, 'B': 0.0, 'E': 64.0, 'G': 0.0,0.4620.533[1.2806,1.5057,1.9846,2.1025,2.472YGNKVTITITCPSNCTVTETVDG'I': 0.0, 'T': 10.0, 'S': 6.0, 'P': 0.0,3,2.702,2.9931,3.1498,3.4432,4.168VAKTLGTVSGNQTITETRTIAPD'-': 20.05]EVVTRTYTCTPNASATSSKTQTVTIKGSQPAPmainly_beta_protein_2SLKAKNLEEMIKEAEKLGYSRD'H': 57.99, 'B': 0.0, 'E': 8.0, 'G': 6.0,0.3710.548[2.8864, 4.3752, 4.5928, 4.8295,EVEKIINEIRDKFKKLGVKISEKT'I': 0.0, 'T': 8.0, 'S': 4.0, 'P': 3.0, '-':5.0854, 5.5618, 5.8646, 6.007,LAYIAYLRLLGVKIDWDKIKKV13.06.3847, 7.1246]KKATPADFRVSEEDLKKPEIQKILEKIKKEINalpha_beta_protein_1APTVKTFEDTINGQKVTVTVTA'H': 15.0, 'B': 0.0, 'E': 59.0, 'G': 3.0,0.4240.535[2.4383,2.5651,3.3175,3.8231,3.967SPGGKITIKTSPGYGDEVAKAFI'I': 0.0, 'T': 12.0, 'S': 1.0, 'P': 0.0,3,4.2655,4.6393,5.1509,5.6023,5.95EELKKQNVLESYKVESAPGKET'-': 10.055]TISDVKVKSGATVTFYVINNGKKGKEYSVTVDAalpha_beta_protein_2MELKVTEKKGKGDYKVKVIEL'H': 35.0,'B': 0.0,'E':0.3760.543[2.8756,3.8895,4.0594,4.2831,4.554NTPDKRYIIIESDASRESLIKAAE28.999999999999996, 'G': 0.0,2,5.171,5.3661,5.4312,6.1964,6.306ALLQGKEVEPTPVNEKNVVLFE'I': 0.0, 'T': 3.0, 'S': 12.0, 'P': 3.0,6]DEDVKTSIERSKKLFKSDNPEEN'-': 18.0IKKALEYLLK</p>
<p>,0.034,0.125,0.142,0.159,0.102,0.079,0.073,0.131,0.105,0.071,0.058,0.072,0.060,0.049,0.114,0.122,0.108,0.173,0.192,0.208,0.153,0.212,0.222,0.244] GenerateForceHistory&lt;0.004,0.034,0.125,0.142,0.159,0.102,0.079,0.073,0.131,0.105,0.071,0.058,0.072,
CalculateForce<GEECDCGSPSNPCCDAATCKLRPGAQCADGLCCDQCRFKKKRTICRIARGDFPDDRCTGQSADCPRWN> [0.262]CalculateEnergy<GEECDCGSPSNPCCDAATCKLRPGAQCADGLCCDQCRFKKKRTICRIARGDFPDDRCTGQSADCPRWN> [0.220]CalculateForceEnergy<GEECDCGSPSNPCCDAATCKLRPGAQCADGLCCDQCRFKKKRTICRIARGDFPDDRCTGQSADCPRWN> [0.262,0.220]CalculateForceHistory<GEECDCGSPSNPCCDAATCKLRPGAQCADGLCCDQCRFKKKRTICRIARGDFPDDRCTGQSADCPRWN>[0.004GenerateForce&lt;0.262&gt; [GEECDCGSPSNPCCDAATCKLRPGAQCADGLCCDQCRFKKKRTICRIARGDFPDDRCTGQSADCPRWN]GenerateForce&lt;0.220&gt; [GEECDCGSPSNPCCDAATCKL RPGAQCADGLCCDQCRFKKKRTICRIARGDFPDDRCTGQSADCPRWN]GenerateForceEnergy&lt;0.262,0.220&gt; [GEECDCGSPSNPCCDAATCKLRPGAQCADGLCCDQCRFKKKRTICRIARGDFPDDRCT GQSADCPRWN]0.060, 0.049,0.114,0.122,0.108,0.173,0.192,0.208, 0.153,0.212, 0.222,0.244&gt; [GEECDCGSPSNPCCDAATCKLRPGAQCADGLCCDQCRFKKKRTICRIARGDFPDDRCTGQSADCPRWN]
AcknowledgementsWe acknowledge support from USDA (2021-69012-35978), DOE-SERDP (WP22-S1-3475), ARO (79058LSCSB, W911NF-22-2-0213 and W911NF2120130) as well as the MIT-IBM Watson AI Lab, MIT's Generative AI Initiative, and Google.Additional support from NIH (U01EB014976 and R01AR077793) ONR (N00014-19-1-2375 and N00014-20-1-2189) is acknowledged.AG gratefully acknowledges the financial support from the Swiss National Science Foundation (#P500PT_214448).Data and code availabilityAll data and codes are available on GitHub at https://github.com/lamm-mit/ProtAgents.Alternatively, they will be provided by the corresponding author based on reasonable request.Conflict of interestThe author declares no conflict of interest.Author Contributions: MJB and AG conceived the study and developed the multi-agent models.AG performed the tests for various problems, analyzed the results and prepared the first draft of the paper.MJB supported the analysis, revised and finalized the paper with AG.Supplementary MaterialsThe full records of different conversation experiments along with additional materials are provided as Supplementary Materials.
The coming of age of de novo protein design. Ssu Po, Scott E Huang, David Boyken, Baker, Nature. 5372016. 2016</p>
<p>Tranception: Protein fitness prediction with autoregressive transformers and inference-time retrieval. Pascal Notin, Mafalda Dias, Jonathan Frazer, Javier Marchena Hurtado, Aidan N Gomez, Debora Marks, Yarin Gal, 62022</p>
<p>Generative models for graph-based protein design. John Ingraham, K Vikas, Regina Garg, Tommi Barzilay, Jaakkola, Advances in Neural Information Processing Systems. 201932</p>
<p>Protein structure generation via folding diffusion. Kevin E Wu, Kevin K Yang, Rianne Van Den, James Y Berg, Alex X Zou, Ava P Lu, Amini, 92022</p>
<p>Protein structure and sequence generation with equivariant denoising diffusion probabilistic models. Namrata Anand, Tudor Achim, 52022</p>
<p>Ig-vae: Generative modeling of protein structure by direct 3d coordinate generation. R Raphael, Christian A Eguchi, Po Ssu Choe, Huang, PLOS Computational Biology. 182022</p>
<p>Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences. Alexander Rives, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, Myle Ott, C Lawrence Zitnick, Jerry Ma, Rob Fergus, e2016239118, 4 2021118Proceedings of the National Academy of Sciences of the United States of America</p>
<p>Progen: Language modeling for protein generation. Ali Madani, Bryan Mccann, Nikhil Naik, Nitish Shirish Keskar, Namrata Anand, Raphael R Eguchi, Po-Ssu Huang, Richard Socher, 32020</p>
<p>Protein sequence design with a learned potential. Namrata Anand, Raphael Eguchi, Irimpan I Mathews, Carla P Perez, Alexander Derry, Russ B Altman, Po Ssu Huang, Nature Communications. 13120222022</p>
<p>Design of metalloproteins and novel protein folds using variational autoencoders. Joe G Greener, Lewis Moffat, David T Jones, Scientific Reports. 812018. 2018</p>
<p>Deep generative models of genetic variation capture the effects of mutations. Adam J Riesselman, John B Ingraham, Debora S Marks, Nature Methods. 15102018. 2018</p>
<p>Unified rational protein engineering with sequence-based deep representation learning. Ethan C Alley, Grigory Khimulya, Surojit Biswas, Mohammed Alquraishi, George M Church, Nature Methods. 1620192019</p>
<p>De novo design of protein structure and function with rfdiffusion. L Joseph, David Watson, Nathaniel R Juergens, Brian L Bennett, Jason Trippe, Helen E Yim, Woody Eisenach, Andrew J Ahern, Robert J Borst, Lukas F Ragotte, Milles, I M Basile, Nikita Wicky, Samuel J Hanikel, Alexis Pellock, William Courbet, Jue Sheffler, Preetham Wang, Isaac Venkatesh, Susana Vázquez Sappington, Anna Torres, Valentin De Lauko, Emile Bortoli, Sergey Mathieu, Regina Ovchinnikov, Tommi S Barzilay, Frank Jaakkola, Minkyung Dimaio, David Baek, Baker, Nature. 20236202023</p>
<p>De novo protein design by deep network hallucination. Ivan Anishchenko, Samuel J Pellock, M Tamuka, Theresa A Chidyausiku, Sergey Ramelot, Jingzhou Ovchinnikov, Khushboo Hao, Christoffer Bafna, Alex Norn, Asim K Kang, Frank Bera, Lauren Dimaio, Cameron M Carter, Chow, T Gaetano, David Montelione, Baker, Nature. 60020212021</p>
<p>Illuminating protein space with a programmable generative model. John B Ingraham, Max Baranov, Zak Costello, Karl W Barber, Wujie Wang, Ahmed Ismail, Vincent Frappier, Dana M Lord, Christopher Ng-Thow-Hing, Erik R Van Vlack, Shan Tie, Vincent Xue, Sarah C Cowles, Alan Leung, João V Rodrigues, Claudio L Morales-Perez, Alex M Ayoub, Robin Green, Katherine Puentes, Frank Oplinger, Nishant V Panwar, Fritz Obermeyer, Adam R Root, Andrew L Beam, Frank J Poelwijk, Gevorg Grigoryan, Nature. 62320232023</p>
<p>Highly accurate protein structure prediction with alphafold. John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Žídek, Anna Potapenko, Alex Bridgland, Clemens Meyer, A A Simon, Andrew J Kohl, Andrew Ballard, Bernardino Cowie, Stanislav Romera-Paredes, Rishub Nikolov, Jonas Jain, Trevor Adler, Stig Back, David Petersen, Ellen Reiman, Michal Clancy, Martin Zielinski, Michalina Steinegger, Tamas Pacholska, Sebastian Berghammer, David Bodenstein, Oriol Silver, Andrew W Vinyals, Koray Senior, Pushmeet Kavukcuoglu, Demis Kohli, Hassabis, Nature. 59620212021</p>
<p>End-to-end deep learning model to predict and design secondary structure content of structural proteins. Chi Hua, Yu , Wei Chen, Yu Hsuan Chiang, Kai Guo, Martin Zaira, David L Moldes, Markus J Kaplan, Buehler, ACS Biomaterials Science and Engineering. 82022</p>
<p>Prottrans: Toward understanding the language of life through self-supervised learning. Ahmed Elnaggar, Michael Heinzinger, Christian Dallago, Ghalia Rehawi, Yu Wang, Llion Jones, Tom Gibbs, Tamas Feher, Christoph Angerer, Martin Steinegger, Debsindhu Bhowmik, Burkhard Rost, IEEE Transactions on Pattern Analysis and Machine Intelligence. 442022</p>
<p>Porter, paleale 4.0: high-accuracy prediction of protein secondary structure and relative solvent accessibility. Claudio Mirabello, Gianluca Pollastri, Bioinformatics. 292013</p>
<p>Improving the prediction of protein secondary structure in three and eight classes using recurrent neural networks and profiles. Gianluca Pollastri, Darisz Przybylski, Burkhard Rost, Pierre Baldi, Proteins: Structure, Function, and Bioinformatics. 472002</p>
<p>Prediction of 8-state protein secondary structures by a novel deep learning architecture. Buzhong Zhang, Jinyan Li, Qiang Lü, BMC Bioinformatics. 192018</p>
<p>Porter: a new, accurate server for protein secondary structure prediction. Gianluca Pollastri, Aoife Mclysaght, Bioinformatics. 214 2005</p>
<p>Rapid prediction of protein natural frequencies using graph neural networks. Kai Guo, Markus J Buehler, Digital Discovery. 12022</p>
<p>End-to-end protein normal mode frequency predictions using language and graph models and application to sonification. Yiwen Hu, Markus J Buehler, ACS Nano. 162022</p>
<p>Emergent abilities of large language models. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, William Fedus, 20226</p>
<p>A survey on evaluation of large language models. Yupeng Chang, X U Wang, Yuan Wu, Hao Chen, Wei Ye, Yue Zhang, Y I Chang, Philip S Yu, Xiaoyuan Yi, Yi Chang, Qiang Yang, Hong Kong, Xu Wang, Jindong Wang, Linyi Yang, Kaijie Zhu, Cunxiang Wang, Yidong Wang, Xing Xie, J. ACM. 372023</p>
<p>14 examples of how LLMs can transform materials science and chemistry. Kevin Maik, Jablonka , Qianxiang Ai, Alexander Al-Feghali, Shruti Badhwar, Joshua D Bocarsly, Andres M Bran, Stefan Bringuier, L Catherine Brinson, Kamal Choudhary, Defne Circi, Sam Cox, Wibe A Jong, Matthew L Evans, Nicolas Gastellu, Jerome Genzling, Victoria María, Ankur K Gil, Zhi Gupta, Alishba Hong, Sabine Imran, Anne Kruschwitz, Jakub Labarre, Tao Lála, Steven Liu, Sauradeep Ma, Garrett W Majumdar, Nicolas Merz, Elias Moitessier, Beatriz Moubarak, Brenden Mouriño, Michael Pelkie, Mayk Pieler, Bojana Caldas Ramos, Ranković, G Samuel, Jacob N Rodriques, Philippe Sanders, Marcus Schwaller, Jiale Schwarting, Berend Shi, Ben E Smit, Joren Smith, Christoph Van Herck, Logan Völker, Sean Ward, Benjamin Warren, Sylvester Weiser, Xiaoqi Zhang, Ghezal Zhang, Aristana Ahmad Zia, K J Scourtas, Ian Schmidt, Andrew D Foster, Ben White, Blaiszik, 22023a reflection on a large language model hackathon. Digital Discovery</p>
<p>Melm, a generative pretrained language modeling framework that solves forward and inverse mechanics problems. Markus J Buehler, Journal of the Mechanics and Physics of Solids. 1812023</p>
<p>Mechgpt, a language-based strategy for mechanics and materials modeling that connects knowledge across scales, disciplines and modalities. Markus J Buehler, Applied Mechanics Reviews. 102023</p>
<p>Anastasia Visheratina, and Xin Xie. An interdisciplinary outlook on large language models for scientific research. James Boyko, Joseph Cohen, Nathan Fox, Maria Han Veiga, I-Hsiu Jennifer, Jing Li, Bernardo Liu, Andreas H Modenesi, Kenneth N Rauch, Soumi Reid, Tribedi, 112023</p>
<p>Mechagents: Large language model multi-agent collaborations can solve mechanics problems, generate new data, and integrate knowledge. Bo Ni, Markus J Buehler, 2023</p>
<p>Perspective: Large language models in applied mechanics. Neal R Brodnik, Caelin Samuel Carton, Satanu Muir, Doug Ghosh, Downey, P Mclean, Tresa M Echlin, Samantha Pollock, Daly, Journal of Applied Mechanics. 902023</p>
<p>Fine-tuning large neural language models for biomedical natural language processing highlights d systematic exploration of fine-tuning stability in biomedical nlp d domain-specific vocabulary and pretraining facilitate robust models for fine-tuning d pubmedbert-large and pubmedelectra models advance state-of-the-art in biomedical nlp in brief fine-tuning large neural language models for biomedical natural language processing. Robert Tinn, Hao Cheng, Yu Gu, Tristan Naumann, Jianfeng Gao, Hoifung Poon Correspondence, Naoto Usuyama, Xiaodong Liu, Hoifung Poon, 2023Patterns, 4:100729</p>
<p>Deep language models for interpretative and predictive materials science. Yiwen Hu, Markus J Buehler, APL Machine Learning. 12023</p>
<p>Generative retrieval-augmented ontologic graph and multi-agent strategies for interpretive large language model-based materials design. Markus J Buehler, ACS Engineering AU. 102023</p>
<p>Materials informatics tools in the context of bio-inspired material mechanics. Rachel K Luu, Markus J Buehler, Journal of Applied Mechanics. 902023</p>
<p>Multiscale modeling meets machine learning: What can we learn?. C Y Grace, Mark Peng, Adrian Alber, William R Buganza Tepole, Suvranu Cannon, Savador De, Krishna Dura-Bernal, George Garikipati, William W Karniadakis, Paris Lytton, Linda Perdikaris, Ellen Petzold, Kuhl, Archives of Computational Methods in Engineering. 282021</p>
<p>Language models are few-shot learners. Benjamin Tom B Brown, Nick Mann, Melanie Ryder, Jared Subbiah, Prafulla Kaplan, Arvind Dhariwal, Pranav Neelakantan, Girish Shyam, Amanda Sastry, Sandhini Askell, Ariel Agarwal, Gretchen Herbert-Voss, Tom Krueger, Rewon Henighan, Aditya Child, Ramesh, M Daniel, Jeffrey Ziegler, Clemens Wu, Christopher Winter, Mark Hesse, Eric Chen, Mateusz Sigler, Scott Litwin, Benjamin Gray, Jack Chess, Christopher Clark, Sam Berner, Alec Mccandlish, Ilya Radford, Dario Sutskever, Amodei, Advances in Neural Information Processing Systems. 332020</p>
<p>Models of natural language understanding. Madeleine Bates, Proceedings of the National Academy of Sciences. 921995</p>
<p>. Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze, Alicia Cheng, Taylor Jin, Leslie Bos, Yu Baker, Yaguang Du, Hongrae Li, Huaixiu Lee, Amin Steven Zheng, Marcelo Ghafouri, Yanping Menegali, Maxim Huang, Dmitry Krikun, James Lepikhin, Dehao Qin, Yuanzhong Chen, Zhifeng Xu, Adam Chen, Maarten Roberts, Vincent Bosma, Yanqi Zhao, Chung-Ching Zhou, Igor Chang, Will Krivokon, Marc Rusch, Pranesh Pickett, Laichee Srinivasan, Kathleen Man, Meier-Hellstern, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak20221Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben HutchinsonQuoc Le. Lamda: Language models for dialog applications</p>
<p>Palm: Scaling language modeling with pathways. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Emily Vinodkumar Prabhakaran, Nan Reif, Ben Du, Reiner Hutchinson, James Pope, Jacob Bradbury, Michael Austin, Guy Isard, Pengcheng Gur-Ari, Toju Yin, Anselm Duke, Sanjay Levskaya, Sunipa Ghemawat, Henryk Dev, Xavier Michalewski, Vedant Garcia, Kevin Misra, Liam Robinson, Denny Fedus, Daphne Zhou, David Ippolito, Hyeontaek Luan, Barret Lim, Alexander Zoph, Ryan Spiridonov, David Sepassi, Dohan, Journal of Machine Learning Research. Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel242023</p>
<p>Attention is all you need. Ashish Vaswani, Google Brain, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, Advances in Neural Information Processing Systems. 201730</p>
<p>. : Openai, Steven Josh Achiam, Sandhini Adler, Lama Agarwal, Ilge Ahmad, Florencia Akkaya, Diogo Leoni Aleman, Janko Almeida, Sam Altenschmidt, Shyamal Altman, Red Anadkat, Igor Avila, Suchir Babuschkin, Valerie Balaji, Paul Balcom, Haiming Baltescu, Mo Bao, Jeff Bavarian, Irwan Belgum, Jake Bello, Gabriel Berdine, Christopher Bernadett-Shapiro, Lenny Berner, Oleg Bogdonoff, Madelaine Boiko, Anna-Luisa Boyd, Greg Brakman, Tim Brockman, Miles Brooks, Kevin Brundage, Trevor Button, Rosie Cai, Andrew Campbell, Brittany Cann, Chelsea Carey, Rory Carlson, Brooke Carmichael, Che Chan, Fotis Chang, Derek Chantzis, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chen, Chester Chess, Casey Cho, Hyung Won Chu, Dave Chung, Jeremiah Cummings, Yunxing Currier, Cory Dai, Thomas Decareaux, Noah Degry, Damien Deutsch, Arka Deville, David Dhar, Steve Dohan, Sheila Dowling, Adrien Dunning, Atty Ecoffet, Tyna Eleti, David Eloundou, Liam Farhi, Niko Fedus, Simón Felix, Juston Posada Fishman, Isabella Forte, Leo Fulford, Elie Gao, Christian Georges, Vik Gibson, Tarun Goel, Gabriel Gogineni, Rapha Goh, Jonathan Gontijo-Lopes, Morgan Gordon, Scott Grafstein, Ryan Gray, Joshua Greene, Gross, Shane Shixiang, Yufei Gu, Chris Guo, Jesse Hallacy, Jeff Han, Yuchen Harris, Mike He, Johannes Heaton, Chris Heidecke, Alan Hesse, Wade Hickey, Peter Hickey, Brandon Hoeschele, Kenny Houghton, Shengli Hsu, Xin Hu, Joost Hu, Shantanu Huizinga, Shawn Jain, Joanne Jain, Angela Jang, Roger Jiang, Haozhun Jiang, Denny Jin, Shino Jin, Billie Jomoto, Heewoo Jonn, Tomer Jun, Łukasz Kaftan, Ali Kaiser, Ingmar Kamali, Nitish Kanitscheider, Tabarak Shirish Keskar, Logan Khan, Jong Wook Kilpatrick, Christina Kim, Yongjik Kim, Hendrik Kim, Jamie Kirchner, Matt Kiros, Daniel Knight, Łukasz Kokotajlo, Andrew Kondraciuk, Aris Kondrich, Kyle Konstantinidis, Gretchen Kosic, Vishal Krueger, Michael Kuo, Ikai Lampe, Teddy Lan, Jan Lee, Jade Leike, Daniel Leung, Levy, Ming Chak, Rachel Li, Molly Lim, Stephanie Lin, Mateusz Lin, Theresa Litwin, Ryan Lopez, Patricia Lowe, Anna Lue, Kim Makanju, Sam Malfacini, Todor Manning, Yaniv Markov, Bianca Markovski, Katie Martin, Andrew Mayer, Mayne ; Aalok, Jacob Mehta, Luke Menick, Andrey Metz, Pamela Mishchenko, Vinnie Mishkin, Evan Monaco, Daniel Morikawa, Tong Mossing, Mira Mu, Oleg Murati, David Murk, Ashvin Mély, Reiichiro Nair, Rajeev Nakano, Arvind Nayak, Richard Neelakantan, Hyeonwoo Ngo, Long Noh, Ouyang, O' Cullen, Jakub Keefe, Alex Pachocki, Joe Paino, Ashley Palermo, Giambattista Pantuliano, Joel Parascandolo, Emy Parish, Alex Parparita, Mikhail Passos, Andrew Pavlov, Adam Peng, Daniel Perelman ; John Schulman, Kyla Selsam, Toki Sheppard, Jessica Sherbakov, Sarah Shieh, Pranav Shoker, Szymon Shyam, Eric Sidor, Maddie Sigler, Jordan Simens, Katarina Sitkin, Ian Slama, Benjamin Sohl, Yang Sokolowsky, Natalie Song, Staudacher, Wei, Akila Cj Weinmann, Peter Welihinda, Jiayi Welinder, Lilian Weng, Matt Weng, Dave Wiethoff, Clemens Willner, Samuel Winter, Hannah Wolrich, Lauren Wong, Sherwin Workman, Jeff Wu, Michael Wu, Kai Wu, Tao Xiao, Sarah Xu, Kevin Yoo, Qiming Yu, Wojciech Yuan, Rowan Zaremba, Chong Zellers, Marvin Zhang, Shengjia Zhang, Tianhao Zhao, Juntang Zheng, William Zhuang, Barret Zhuk, Zoph, Felipe Petroski Such. Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr,32023Amin TootoonchianFilipe de Avila Belbute Peres ; Juan Felipe Cerón Uribe, Andrea Vallone, Arun VijayvergiyaGpt-4 technical report</p>
<p>Felipe Petroski Such. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet ; Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Josh Achiam, Vedant Misra, Jan Leike,. 20217Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang,; Andrew N. Carr; Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder; Bob McGrew, Dario Amodei, Sam McCandlishIlya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, Yi Zhang, Sparks of Artificial General Intelligence: Early experiments with GPT-4. 3 2023</p>
<p>Program synthesis with large language models. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, arXiv:2108.077322021arXiv preprint</p>
<p>Artificial intelligence and machine learning in design of mechanical materials. Kai Guo, Zhenze Yang, Chi Hua Yu, Markus J Buehler, Materials Horizons. 82021</p>
<p>A survey of convolutional neural networks: Analysis, applications, and prospects. Zewen Li, Fan Liu, Wenjie Yang, Shouheng Peng, Jun Zhou, IEEE Transactions on Neural Networks and Learning Systems. 332022</p>
<p>An introduction to convolutional neural networks. O' Keiron, Ryan Shea, Nash, International Journal for Research in Applied Science and Engineering Technology. 102015</p>
<p>End-to-end prediction of multimaterial stress fields and fracture patterns using cycle-consistent adversarial and transformer neural networks. Eric L Buehler, Markus J Buehler, Biomedical Engineering Advances. 42022</p>
<p>Deep learning model to predict complex stress and strain fields in hierarchical composites. Zhenze Yang, Chi Hua Yu, Markus J Buehler, Science Advances. 72021</p>
<p>Expanding functional protein sequence spaces using generative adversarial networks. Donatas Repecka, Vykintas Jauniskis, Laurynas Karpus, Elzbieta Rembeza, Irmantas Rokaitis, Jan Zrimec, Simona Poviloniene, Audrius Laurynenas, Sandra Viknander, Wissam Abuajwa, Otto Savolainen, Rolandas Meskys, K M Martin, Aleksej Engqvist, Zelezniak, Nature Machine Intelligence. 320212021</p>
<p>Prolango: Protein function prediction using neural machine translation based on a recurrent neural network. Renzhi Cao, Colton Freitas, Leong Chan, Miao Sun, Haiqing Jiang, Zhangxin Chen, Molecules. 2217322017. 10 2017</p>
<p>Using deep learning to predict fracture patterns in crystalline solids. Yu-Chuan Hsu, Chi-Hua Yu, Markus J Buehler, Matter. 32020</p>
<p>Rapid mechanical property prediction and de novo design of three-dimensional spider webs through graph and graphperceiver neural networks. Wei Lu, Zhenze Yang, Markus J Buehler, Journal of Applied Physics. 1322022</p>
<p>Fast and flexible protein design using deep graph neural networks. Alexey Strokach, David Becerra, Carles Corbi-Verge, Albert Perez-Riba, Philip M Kim, Cell Systems. 112020</p>
<p>Deepgraphgo: graph neural network for large-scale, multispecies protein function prediction. Ronghui You, Shuwei Yao, Hiroshi Mamitsuka, Shanfeng Zhu, Bioinformatics. 372021</p>
<p>Linking atomic structural defects to mesoscale properties in crystalline solids using graph neural networks. npj Computational Materials. Zhenze Yang, Markus J Buehler, 202282022</p>
<p>Generative modeling, design, and analysis of spider silk protein sequences for enhanced mechanical properties. Wei Lu, David L Kaplan, Markus J Buehler, Advanced Functional Materials. 122023</p>
<p>Building cooperative embodied agents modularly with large language models. Hongxin Zhang, Weihua Du, Jiaming Shan, Qinhong Zhou, Yilun Du, Joshua B Tenenbaum, Tianmin Shu, Chuang Gan, 20237</p>
<p>Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, Ji-Rong Wen, A survey on large language model based autonomous agents. 20238</p>
<p>Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, Rui Zheng, Xiaoran Fan, Xiao Wang, Limao Xiong, Yuhao Zhou, Weiran Wang, Changhao Jiang, Yicheng Zou, Xiangyang Liu, Zhangyue Yin, Shihan Dou, Rongxiang Weng, Wensen Cheng, Qi Zhang, Wenjuan Qin, Yongyan Zheng, Xipeng Qiu, Xuanjing Huang, and Tao Gui. The rise and potential of large language model based agents: A survey. 2023</p>
<p>Retrieval-augmented generation for knowledge-intensive nlp tasks. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen Tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela, Advances in Neural Information Processing Systems. 202033</p>
<p>. Openai Api, </p>
<p>Mechanical strength of 17 134 model proteins and cysteine slipknots. Mateusz Sikora, Joanna I Sułkowska, Marek Cieplak, PLOS Computational Biology. 5e10005472009</p>
<p>Mechanical stretching of proteins-a theoretical survey of the protein data bank. Arata Nakajo, Zacharie Wuillemin, Patrick Metzger, Yuxiu Liu, Michael W Murphy, Daniel R Baker, Ana Marija Damjanovi, Burak Koyutürk, Yan-Sheng Li, Joanna I Sułkowska, Marek Cieplak, Journal of Physics: Condensed Matter. 192832012007</p>
<p>Hierarchies, multiple energy barriers, and robustness govern the fracture mechanics of α-helical and β-sheet protein domains. Theodor Ackbarow, Xuefeng Chen, Sinan Keten, Markus J Buehler, 2007104Proceedings of the National Academy of Sciences of the United States of America</p>
<p>Nanomechanics of functional and pathological amyloid materials. P J Tuomas, Markus J Knowles, Buehler, Nature Nanotechnology. 682011. 2011</p>
<p>Mechanical energy transfer and dissipation in fibrous beta-sheet-rich proteins. Zhiping Xu, Markus J Buehler, Physical Review E -Statistical, Nonlinear, and Soft Matter Physics. 81619106 2010</p>
<p>Cooperative deformation of hydrogen bonds in beta-strands and beta-sheet nanocrystals. Zhao Qin, Markus J Buehler, Physical Review E -Statistical, Nonlinear, and Soft Matter Physics. 826190612 2010</p>
<p>Artificial neural networks for solving ordinary and partial differential equations. Isaac Elias Lagaris, Aristidis Likas, Dimitrios I Fotiadis, IEEE Transactions on Neural Networks. 91998</p>
<p>Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. M Raissi, P Perdikaris, G E Karniadakis, Journal of Computational Physics. 3782019</p>
<p>Presto: Rapid protein mechanical strength prediction with an end-to-end deep learning model. Y C Frank, Bo Liu, Markus J Ni, Buehler, Extreme Mechanics Letters. 552022</p>
<p>Sonification based de novo protein design using artificial intelligence, structure prediction, and analysis using molecular modeling. Chi Hua, Yu , Markus J Buehler, APL Bioengineering. 4161082020</p>
<p>Generative pretrained autoregressive transformer graph neural network applied to the analysis and discovery of novel proteins. Markus J Buehler, Journal of Applied Physics. 1342023</p>
<p>Generative design of de novo proteins based on secondary-structure constraints using an attention-based diffusion model. Bo Ni, David L Kaplan, Markus J Buehler, Chem. 92023</p>
<p>Machine learning in protein structure prediction. Mohammed Alquraishi, Current Opinion in Chemical Biology. 652021</p>
<p>Deep learning in protein structural modeling and design. Patterns, 1:100142. Wenhao Gao, Sai Pooja Mahajan, Jeremias Sulam, Jeffrey J Gray, 2020</p>
<p>Autogen: An automated dynamic model generation framework for recommender system. Chenxu Zhu, Bo Chen, Huifeng Guo, Hang Xu, Xiangyang Li, Xiangyu Zhao, Weinan Zhang, Yong Yu, Ruiming Tang, WSDM 2023 -Proceedings of the 16th ACM International Conference on Web Search and Data Mining. 22023</p>
<p>Roformer: Enhanced transformer with rotary position embedding. Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, Yunfeng Liu, Neurocomputing. 5681270632024</p>
<p>Forcegen: End-to-end de novo protein generation based on nonlinear mechanical unfolding responses using a protein language diffusion model. Bo Ni, David L Kaplan, Markus J Buehler, Science Advances. 2023</p>
<p>Autogen: Enabling next-gen LLM applications via multi-agent conversation. Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, Ahmed Awadallah, Ryen W White, Doug Burger, Chi Wang, 2023</p>
<p>Nicholas Rego, David Koes, 3dmol.js: molecular visualization with webgl. 201531</p>
<p>Dictionary of protein secondary structure: Pattern recognition of hydrogen-bonded and geometrical features. Wolfgang Kabsch, Christian Sander, Biopolymers. 22121983</p>
<p>J A Peter, Tiago Cock, Jeffrey T Antao, Brad A Chang, Cymon J Chapman, Andrew Cox, Iddo Dalke, Thomas Friedberg, Frank Hamelryck, Bartek Kauff, Wilczynski, J L Michiel, De Hoon, Biopython: freely available python tools for computational molecular biology and bioinformatics. 2009251422</p>
<p>Anisotropy of fluctuation dynamics of proteins with an elastic network model. S A R Atilgan, R L R Durell, M C Jernigan, Demirel, Keskin, Bahar, Biophys. J. 2001</p>
<p>Dynamics of proteins predicted by molecular dynamics simulations and analytical approaches: Application to-amylase inhibitor. Pemra Doruker, Ali Rana Atilgan, Ivet Bahar, PROTEINS: Structure, Function, and Bioinformatics. 2000</p>
<p>Prody: Protein dynamics inferred from theory and experiments. Ahmet Bakan, M Lidio, Ivet Meireles, Bahar, Bioinformatics. 27</p>
<p>LlamaIndex (formerly GPT index), a data framework for LLM applications. </p>            </div>
        </div>

    </div>
</body>
</html>
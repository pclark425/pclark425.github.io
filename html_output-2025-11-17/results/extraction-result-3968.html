<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3968 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3968</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3968</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-92.html">extraction-schema-92</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <p><strong>Paper ID:</strong> paper-39e0c341351f8f4a39ac890b96217c7f4bde5369</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/39e0c341351f8f4a39ac890b96217c7f4bde5369" target="_blank">A note on the evaluation of generative models</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> This article reviews mostly known but often underappreciated properties relating to the evaluation and interpretation of generative models with a focus on image models and shows that three of the currently most commonly used criteria---average log-likelihood, Parzen window estimates, and visual fidelity of samples---are largely independent of each other when the data is high-dimensional.</p>
                <p><strong>Paper Abstract:</strong> Probabilistic generative models can be used for compression, denoising, inpainting, texture synthesis, semi-supervised learning, unsupervised feature learning, and other tasks. Given this wide range of applications, it is not surprising that a lot of heterogeneity exists in the way these models are formulated, trained, and evaluated. As a consequence, direct comparison between models is often difficult. This article reviews mostly known but often underappreciated properties relating to the evaluation and interpretation of generative models with a focus on image models. In particular, we show that three of the currently most commonly used criteria---average log-likelihood, Parzen window estimates, and visual fidelity of samples---are largely independent of each other when the data is high-dimensional. Good performance with respect to one criterion therefore need not imply good performance with respect to the other criteria. Our results show that extrapolation from one criterion to another is not warranted and generative models need to be evaluated directly with respect to the application(s) they were intended for. In addition, we provide examples demonstrating that Parzen window estimates should generally be avoided.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3968.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3968.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Log-likelihood (KLD)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Average log-likelihood / Kullback-Leibler divergence</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The canonical density-estimation criterion: average log-probability assigned to held-out data (equivalently minimization of Kullback-Leibler divergence); closely tied to compression (bits per datum) and semi-supervised Bayesian inference.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Average log-likelihood on held-out data (often reported as nats or bits per dimension); equivalently minimizing Kullback-Leibler divergence from data to model.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Compute model log-density on dequantized test images (add uniform noise to discrete pixels to avoid infinite differential entropy); compare average log-likelihood across models; relate to discrete-data log-probabilities via integration over quantization bin.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Used/discussed on image datasets in paper — CIFAR-10 (32x32), MNIST (for Parzen comparisons), and small CIFAR patches (6x6) for Parzen illustration.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>Average log-likelihood (nats), bits per dimension (via base-2 conversion), and derived compression cost (average number of bits for lossless coding under model Q).</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Low — typically automatic numeric evaluation; human inspection can be used for qualitative diagnosis but not required for log-likelihood metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Computationally intractable for many models (unnormalized energies, complex latent-variable integrals); requires dequantization for discrete images; high likelihood does not imply good sample quality or good performance on other tasks; insensitive to small mixture components that dominate samples.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Paper argues average log-likelihood is a principled default for density tasks and semi-supervised learning, but is largely independent of sample visual fidelity and can be misleading if used outside its intended application.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A note on the evaluation of generative models', 'publication_date_yy_mm': '2015-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3968.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3968.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Parzen window estimate</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Parzen window (kernel density) estimate of sample-based likelihood</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proxy for intractable model likelihoods: fit a kernel density estimator (typically Gaussian) to samples from the generative model and evaluate test-data log-likelihood under that KDE.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Estimated test log-likelihood computed from KDE fitted to model-generated samples, often used when true model likelihood is intractable.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Generate N samples from model (commonly 10k), fit Gaussian-kernel Parzen estimator with bandwidth chosen (often by cross-validation), evaluate log-density of held-out test examples under Parzen model.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Applied to MNIST in paper (Table 1) and small CIFAR patches for demonstration (Figure 3); general practice cited across image-model literature.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>Parzen-estimated log-likelihood (nats) on test set; reported numeric scores used to rank models (see Table 1 in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>None for the Parzen computation itself; human inspection sometimes used in parallel but not part of Parzen metric.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Paper demonstrates Parzen estimates are unreliable in high dimensions (need infeasibly many samples), can produce rankings contradictory to true likelihood, can favor degenerate or overfitting models (e.g., k-means centroids beat true data), and therefore should generally be avoided unless the target application explicitly uses that loss.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Parzen estimates often fail to approximate true log-likelihood (Figure 3) and can rank implausible models above the true distribution (Table 1); authors recommend avoiding Parzen evaluation for generative models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A note on the evaluation of generative models', 'publication_date_yy_mm': '2015-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3968.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3968.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sample visual fidelity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Visual inspection / human judgement of generated samples</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Qualitative (and sometimes quantitative via human studies) assessment of how realistic or plausible generated samples look to humans; commonly used for image synthesis evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Perceptual plausibility / visual fidelity of model-generated samples as judged by humans (or by visual examples shown in papers).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Show generated samples to human observers for qualitative inspection or controlled psychophysical experiments; report percentage of samples judged realistic or human 'fooling' rates (when available).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Samples typically drawn from trained generative models on standard image datasets (CIFAR-10 patches, MNIST, etc.); Gerhard et al. (2013) reported a human study on small image patches referenced in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>Qualitative descriptions; when human experiments are used, possible metrics include human fooling rate or subjective ratings. The paper cites an empirical correlation (in Gerhard et al., 2013) between log-likelihood and fooling for small patches but warns this may not generalize.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>High — human observers are often required for visual fidelity judgments; can be expert panels or crowdworkers depending on study.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Visual fidelity can be misleading for high-dimensional data: models that memorize or store training examples yield convincing samples but poor generalization; good sample quality is neither necessary nor sufficient for good likelihood or downstream task performance; subjective and task-dependent.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Authors conclude visual inspection is appropriate only when image synthesis is the target application; otherwise it is an unreliable proxy for density estimation or other application performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A note on the evaluation of generative models', 'publication_date_yy_mm': '2015-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3968.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3968.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MMD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Maximum Mean Discrepancy (MMD)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A kernel-based two-sample criterion measuring distributional difference using expectations of kernel functions; used to train/evaluate generative models (e.g., GMMN).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>MMD value between model and data distributions (lower is better), computed with chosen kernel(s) and bandwidths.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Estimate empirical MMD via expectations over data and model samples using mixture of Gaussian kernels; optimize model parameters to minimize empirical MMD (as in GMMN) and compare fitted distributions using the MMD statistic.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Used in experiments/illustration in the paper (toy Gaussian-mixture example, references to Li et al. 2015 and Dziugaite et al. 2015); applied to image modeling contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>MMD statistic (scalar) computed from kernel expectations; no direct likelihood interpretation.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>None required for the MMD computation; purely statistical/automated.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Different training/evaluation objectives (MMD vs KLD) lead to different optima and trade-offs — e.g., MMD tends to fit single modes well and ignore others; performance on MMD does not imply good log-likelihood or good sample diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Paper shows MMD-optimized Gaussian in toy example captures a single mode, demonstrating that MMD optimization leads to different kinds of model behavior than likelihood-based criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A note on the evaluation of generative models', 'publication_date_yy_mm': '2015-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3968.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3968.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>JSD / GAN objective</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Jensen-Shannon divergence (and related GAN objectives)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A symmetric divergence measure (JSD) used as a conceptual objective for GANs; optimizing JSD or GAN-like adversarial objectives can lead to models that prioritize sample plausibility over covering full data support.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Jensen-Shannon divergence between model and data (lower is better), approximated in practice via adversarial training objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Direct optimization of JSD in toy experiments (not usually possible in practice where only samples are available) or approximate optimization via adversarial networks; evaluate sample quality or divergence proxies.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Demonstrated in toy Gaussian-mixture experiment (Figure 1) and referenced across image-generation literature (GAN papers on CIFAR/MNIST/etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>JSD value in toy analytic settings; in practice sample quality and discriminator loss are used as proxies.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>None required for computing JSD in analytic settings; human inspection often used to assess samples resulting from GANs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Optimization of JSD (or practical GAN objectives) can emphasize producing plausible samples (mode-seeking) and ignore parts of the data distribution, leading to poor coverage and low likelihood; GAN practical objective may differ from true JSD.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Paper demonstrates that JSD-optimized fits tend to capture modes (produce plausible samples) but can ignore other data regions, highlighting divergence in trade-offs compared to likelihood-based training.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A note on the evaluation of generative models', 'publication_date_yy_mm': '2015-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3968.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3968.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Nearest-neighbor sample comparison</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Nearest-neighbor inspection for overfitting detection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Qualitative check that compares generated samples to nearest training examples (often in Euclidean pixel space) to detect memorization/overfitting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Distance (typically Euclidean) between generated samples and nearest training images; visual side-by-side comparisons to see if samples are memorized/trivially transformed training data.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Compute nearest neighbors in pixel space and present them alongside generated samples; perform controlled perturbation experiments to assess sensitivity (paper shifted CIFAR image patches and measured nearest-neighbor assignment rates).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>CIFAR-10 (top-left 28x28 patches) used in nearest-neighbor experiments in the paper (Figure 2).</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>Fraction of query images assigned to the correct training image under shifts (empirical nearest-neighbor match rate); Euclidean distance distributions to multiple training images.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Moderate — humans often inspect pairs to decide whether memorization occurred; nearest-neighbor retrieval itself is automated.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Euclidean distance is not perceptually faithful; small perceptual changes (pixel shifts) yield different nearest neighbors; an overfitting model that stores transformed training images can pass nearest-neighbor tests; entropy-related overfitting (low diversity) may not be detectable via nearest neighbors.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Paper shows nearest-neighbor tests in pixel space are brittle: tiny shifts change nearest neighbors and overfitting can remain undetected; recommends perceptual metrics and showing multiple neighbors but notes deeper issues remain.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A note on the evaluation of generative models', 'publication_date_yy_mm': '2015-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3968.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e3968.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Surrogate tasks</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Evaluation via surrogate downstream tasks (classification, denoising, inpainting, compression)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Assess generative models by their utility in downstream tasks (e.g., semi-supervised classification, denoising, inpainting, compression) rather than by sample quality or density alone.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Task-specific performance measures (classification accuracy, denoising error, inpainting quality, compression rate), often reflecting practical utility of the model.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Use generative model as component in task (e.g., compute posteriors for classification, use likelihood for compression, use conditional generation for inpainting) and evaluate with standard task metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>General mention — semi-supervised learning contexts and tasks like inpainting/denoising on image datasets (references: Kingma et al., 2014; Hays & Efros, 2007).</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>Classification accuracy, denoising/inpainting reconstruction error or qualitative assessments, compression bits per image.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Depends on task — classification uses automated labels; inpainting/denoising sometimes evaluated with human perceptual judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Good performance on one task does not imply good performance on another; mixture models can have high likelihood yet produce poor unconditional samples but still give good posteriors for task-conditioned problems; evaluation must match the target application.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Paper argues that models should be evaluated directly on the application they are intended for: good sample quality is not necessary for good application performance and vice versa.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A note on the evaluation of generative models', 'publication_date_yy_mm': '2015-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Generative Adversarial Nets <em>(Rating: 2)</em></li>
                <li>Unlearning for better mixing <em>(Rating: 2)</em></li>
                <li>A kernel method for the two-sample-problem <em>(Rating: 2)</em></li>
                <li>Training generative neural networks via maximum mean discrepancy optimization <em>(Rating: 2)</em></li>
                <li>How sensitive is the human visual system to the local statistics of natural images? <em>(Rating: 2)</em></li>
                <li>Training Products of Experts by Minimizing Contrastive Divergence <em>(Rating: 1)</em></li>
                <li>Semi-supervised learning with deep generative models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3968",
    "paper_id": "paper-39e0c341351f8f4a39ac890b96217c7f4bde5369",
    "extraction_schema_id": "extraction-schema-92",
    "extracted_data": [
        {
            "name_short": "Log-likelihood (KLD)",
            "name_full": "Average log-likelihood / Kullback-Leibler divergence",
            "brief_description": "The canonical density-estimation criterion: average log-probability assigned to held-out data (equivalently minimization of Kullback-Leibler divergence); closely tied to compression (bits per datum) and semi-supervised Bayesian inference.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_criteria": "Average log-likelihood on held-out data (often reported as nats or bits per dimension); equivalently minimizing Kullback-Leibler divergence from data to model.",
            "evaluation_methods": "Compute model log-density on dequantized test images (add uniform noise to discrete pixels to avoid infinite differential entropy); compare average log-likelihood across models; relate to discrete-data log-probabilities via integration over quantization bin.",
            "benchmark_or_dataset": "Used/discussed on image datasets in paper — CIFAR-10 (32x32), MNIST (for Parzen comparisons), and small CIFAR patches (6x6) for Parzen illustration.",
            "metrics_reported": "Average log-likelihood (nats), bits per dimension (via base-2 conversion), and derived compression cost (average number of bits for lossless coding under model Q).",
            "human_involvement": "Low — typically automatic numeric evaluation; human inspection can be used for qualitative diagnosis but not required for log-likelihood metrics.",
            "limitations_or_challenges": "Computationally intractable for many models (unnormalized energies, complex latent-variable integrals); requires dequantization for discrete images; high likelihood does not imply good sample quality or good performance on other tasks; insensitive to small mixture components that dominate samples.",
            "llm_theory_example": null,
            "evaluation_results": "Paper argues average log-likelihood is a principled default for density tasks and semi-supervised learning, but is largely independent of sample visual fidelity and can be misleading if used outside its intended application.",
            "uuid": "e3968.0",
            "source_info": {
                "paper_title": "A note on the evaluation of generative models",
                "publication_date_yy_mm": "2015-11"
            }
        },
        {
            "name_short": "Parzen window estimate",
            "name_full": "Parzen window (kernel density) estimate of sample-based likelihood",
            "brief_description": "A proxy for intractable model likelihoods: fit a kernel density estimator (typically Gaussian) to samples from the generative model and evaluate test-data log-likelihood under that KDE.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_criteria": "Estimated test log-likelihood computed from KDE fitted to model-generated samples, often used when true model likelihood is intractable.",
            "evaluation_methods": "Generate N samples from model (commonly 10k), fit Gaussian-kernel Parzen estimator with bandwidth chosen (often by cross-validation), evaluate log-density of held-out test examples under Parzen model.",
            "benchmark_or_dataset": "Applied to MNIST in paper (Table 1) and small CIFAR patches for demonstration (Figure 3); general practice cited across image-model literature.",
            "metrics_reported": "Parzen-estimated log-likelihood (nats) on test set; reported numeric scores used to rank models (see Table 1 in paper).",
            "human_involvement": "None for the Parzen computation itself; human inspection sometimes used in parallel but not part of Parzen metric.",
            "limitations_or_challenges": "Paper demonstrates Parzen estimates are unreliable in high dimensions (need infeasibly many samples), can produce rankings contradictory to true likelihood, can favor degenerate or overfitting models (e.g., k-means centroids beat true data), and therefore should generally be avoided unless the target application explicitly uses that loss.",
            "llm_theory_example": null,
            "evaluation_results": "Parzen estimates often fail to approximate true log-likelihood (Figure 3) and can rank implausible models above the true distribution (Table 1); authors recommend avoiding Parzen evaluation for generative models.",
            "uuid": "e3968.1",
            "source_info": {
                "paper_title": "A note on the evaluation of generative models",
                "publication_date_yy_mm": "2015-11"
            }
        },
        {
            "name_short": "Sample visual fidelity",
            "name_full": "Visual inspection / human judgement of generated samples",
            "brief_description": "Qualitative (and sometimes quantitative via human studies) assessment of how realistic or plausible generated samples look to humans; commonly used for image synthesis evaluation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_criteria": "Perceptual plausibility / visual fidelity of model-generated samples as judged by humans (or by visual examples shown in papers).",
            "evaluation_methods": "Show generated samples to human observers for qualitative inspection or controlled psychophysical experiments; report percentage of samples judged realistic or human 'fooling' rates (when available).",
            "benchmark_or_dataset": "Samples typically drawn from trained generative models on standard image datasets (CIFAR-10 patches, MNIST, etc.); Gerhard et al. (2013) reported a human study on small image patches referenced in paper.",
            "metrics_reported": "Qualitative descriptions; when human experiments are used, possible metrics include human fooling rate or subjective ratings. The paper cites an empirical correlation (in Gerhard et al., 2013) between log-likelihood and fooling for small patches but warns this may not generalize.",
            "human_involvement": "High — human observers are often required for visual fidelity judgments; can be expert panels or crowdworkers depending on study.",
            "limitations_or_challenges": "Visual fidelity can be misleading for high-dimensional data: models that memorize or store training examples yield convincing samples but poor generalization; good sample quality is neither necessary nor sufficient for good likelihood or downstream task performance; subjective and task-dependent.",
            "llm_theory_example": null,
            "evaluation_results": "Authors conclude visual inspection is appropriate only when image synthesis is the target application; otherwise it is an unreliable proxy for density estimation or other application performance.",
            "uuid": "e3968.2",
            "source_info": {
                "paper_title": "A note on the evaluation of generative models",
                "publication_date_yy_mm": "2015-11"
            }
        },
        {
            "name_short": "MMD",
            "name_full": "Maximum Mean Discrepancy (MMD)",
            "brief_description": "A kernel-based two-sample criterion measuring distributional difference using expectations of kernel functions; used to train/evaluate generative models (e.g., GMMN).",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_criteria": "MMD value between model and data distributions (lower is better), computed with chosen kernel(s) and bandwidths.",
            "evaluation_methods": "Estimate empirical MMD via expectations over data and model samples using mixture of Gaussian kernels; optimize model parameters to minimize empirical MMD (as in GMMN) and compare fitted distributions using the MMD statistic.",
            "benchmark_or_dataset": "Used in experiments/illustration in the paper (toy Gaussian-mixture example, references to Li et al. 2015 and Dziugaite et al. 2015); applied to image modeling contexts.",
            "metrics_reported": "MMD statistic (scalar) computed from kernel expectations; no direct likelihood interpretation.",
            "human_involvement": "None required for the MMD computation; purely statistical/automated.",
            "limitations_or_challenges": "Different training/evaluation objectives (MMD vs KLD) lead to different optima and trade-offs — e.g., MMD tends to fit single modes well and ignore others; performance on MMD does not imply good log-likelihood or good sample diversity.",
            "llm_theory_example": null,
            "evaluation_results": "Paper shows MMD-optimized Gaussian in toy example captures a single mode, demonstrating that MMD optimization leads to different kinds of model behavior than likelihood-based criteria.",
            "uuid": "e3968.3",
            "source_info": {
                "paper_title": "A note on the evaluation of generative models",
                "publication_date_yy_mm": "2015-11"
            }
        },
        {
            "name_short": "JSD / GAN objective",
            "name_full": "Jensen-Shannon divergence (and related GAN objectives)",
            "brief_description": "A symmetric divergence measure (JSD) used as a conceptual objective for GANs; optimizing JSD or GAN-like adversarial objectives can lead to models that prioritize sample plausibility over covering full data support.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_criteria": "Jensen-Shannon divergence between model and data (lower is better), approximated in practice via adversarial training objectives.",
            "evaluation_methods": "Direct optimization of JSD in toy experiments (not usually possible in practice where only samples are available) or approximate optimization via adversarial networks; evaluate sample quality or divergence proxies.",
            "benchmark_or_dataset": "Demonstrated in toy Gaussian-mixture experiment (Figure 1) and referenced across image-generation literature (GAN papers on CIFAR/MNIST/etc.).",
            "metrics_reported": "JSD value in toy analytic settings; in practice sample quality and discriminator loss are used as proxies.",
            "human_involvement": "None required for computing JSD in analytic settings; human inspection often used to assess samples resulting from GANs.",
            "limitations_or_challenges": "Optimization of JSD (or practical GAN objectives) can emphasize producing plausible samples (mode-seeking) and ignore parts of the data distribution, leading to poor coverage and low likelihood; GAN practical objective may differ from true JSD.",
            "llm_theory_example": null,
            "evaluation_results": "Paper demonstrates that JSD-optimized fits tend to capture modes (produce plausible samples) but can ignore other data regions, highlighting divergence in trade-offs compared to likelihood-based training.",
            "uuid": "e3968.4",
            "source_info": {
                "paper_title": "A note on the evaluation of generative models",
                "publication_date_yy_mm": "2015-11"
            }
        },
        {
            "name_short": "Nearest-neighbor sample comparison",
            "name_full": "Nearest-neighbor inspection for overfitting detection",
            "brief_description": "Qualitative check that compares generated samples to nearest training examples (often in Euclidean pixel space) to detect memorization/overfitting.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_criteria": "Distance (typically Euclidean) between generated samples and nearest training images; visual side-by-side comparisons to see if samples are memorized/trivially transformed training data.",
            "evaluation_methods": "Compute nearest neighbors in pixel space and present them alongside generated samples; perform controlled perturbation experiments to assess sensitivity (paper shifted CIFAR image patches and measured nearest-neighbor assignment rates).",
            "benchmark_or_dataset": "CIFAR-10 (top-left 28x28 patches) used in nearest-neighbor experiments in the paper (Figure 2).",
            "metrics_reported": "Fraction of query images assigned to the correct training image under shifts (empirical nearest-neighbor match rate); Euclidean distance distributions to multiple training images.",
            "human_involvement": "Moderate — humans often inspect pairs to decide whether memorization occurred; nearest-neighbor retrieval itself is automated.",
            "limitations_or_challenges": "Euclidean distance is not perceptually faithful; small perceptual changes (pixel shifts) yield different nearest neighbors; an overfitting model that stores transformed training images can pass nearest-neighbor tests; entropy-related overfitting (low diversity) may not be detectable via nearest neighbors.",
            "llm_theory_example": null,
            "evaluation_results": "Paper shows nearest-neighbor tests in pixel space are brittle: tiny shifts change nearest neighbors and overfitting can remain undetected; recommends perceptual metrics and showing multiple neighbors but notes deeper issues remain.",
            "uuid": "e3968.5",
            "source_info": {
                "paper_title": "A note on the evaluation of generative models",
                "publication_date_yy_mm": "2015-11"
            }
        },
        {
            "name_short": "Surrogate tasks",
            "name_full": "Evaluation via surrogate downstream tasks (classification, denoising, inpainting, compression)",
            "brief_description": "Assess generative models by their utility in downstream tasks (e.g., semi-supervised classification, denoising, inpainting, compression) rather than by sample quality or density alone.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "evaluation_criteria": "Task-specific performance measures (classification accuracy, denoising error, inpainting quality, compression rate), often reflecting practical utility of the model.",
            "evaluation_methods": "Use generative model as component in task (e.g., compute posteriors for classification, use likelihood for compression, use conditional generation for inpainting) and evaluate with standard task metrics.",
            "benchmark_or_dataset": "General mention — semi-supervised learning contexts and tasks like inpainting/denoising on image datasets (references: Kingma et al., 2014; Hays & Efros, 2007).",
            "metrics_reported": "Classification accuracy, denoising/inpainting reconstruction error or qualitative assessments, compression bits per image.",
            "human_involvement": "Depends on task — classification uses automated labels; inpainting/denoising sometimes evaluated with human perceptual judgments.",
            "limitations_or_challenges": "Good performance on one task does not imply good performance on another; mixture models can have high likelihood yet produce poor unconditional samples but still give good posteriors for task-conditioned problems; evaluation must match the target application.",
            "llm_theory_example": null,
            "evaluation_results": "Paper argues that models should be evaluated directly on the application they are intended for: good sample quality is not necessary for good application performance and vice versa.",
            "uuid": "e3968.6",
            "source_info": {
                "paper_title": "A note on the evaluation of generative models",
                "publication_date_yy_mm": "2015-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Generative Adversarial Nets",
            "rating": 2
        },
        {
            "paper_title": "Unlearning for better mixing",
            "rating": 2
        },
        {
            "paper_title": "A kernel method for the two-sample-problem",
            "rating": 2
        },
        {
            "paper_title": "Training generative neural networks via maximum mean discrepancy optimization",
            "rating": 2
        },
        {
            "paper_title": "How sensitive is the human visual system to the local statistics of natural images?",
            "rating": 2
        },
        {
            "paper_title": "Training Products of Experts by Minimizing Contrastive Divergence",
            "rating": 1
        },
        {
            "paper_title": "Semi-supervised learning with deep generative models",
            "rating": 1
        }
    ],
    "cost": 0.01094125,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>A NOTE ON THE EVALUATION OF GENERATIVE MODELS</h1>
<p>Lucas Theis*<br>University of Tübingen<br>72072 Tübingen, Germany<br>lucas@bethgelab.org</p>
<p>Aäron van den Oord ${ }^{* \dagger}$<br>Ghent University<br>9000 Ghent, Belgium<br>aaron.vandenoord@ugent.be</p>
<h2>Matthias Bethge</h2>
<p>University of Tübingen
72072 Tübingen, Germany
matthias@bethgelab.org</p>
<h2>ABSTRACT</h2>
<p>Probabilistic generative models can be used for compression, denoising, inpainting, texture synthesis, semi-supervised learning, unsupervised feature learning, and other tasks. Given this wide range of applications, it is not surprising that a lot of heterogeneity exists in the way these models are formulated, trained, and evaluated. As a consequence, direct comparison between models is often difficult. This article reviews mostly known but often underappreciated properties relating to the evaluation and interpretation of generative models with a focus on image models. In particular, we show that three of the currently most commonly used criteria-average log-likelihood, Parzen window estimates, and visual fidelity of samples-are largely independent of each other when the data is high-dimensional. Good performance with respect to one criterion therefore need not imply good performance with respect to the other criteria. Our results show that extrapolation from one criterion to another is not warranted and generative models need to be evaluated directly with respect to the application(s) they were intended for. In addition, we provide examples demonstrating that Parzen window estimates should generally be avoided.</p>
<h2>1 INTRODUCTION</h2>
<p>Generative models have many applications and can be evaluated in many ways. For density estimation and related tasks, log-likelihood (or equivalently Kullback-Leibler divergence) has been the de-facto standard for training and evaluating generative models. However, the likelihood of many interesting models is computationally intractable. For example, the normalization constant of unnormalized energy-based models is generally difficult to compute, and latent-variable models often require us to solve complex integrals to compute the likelihood. These models may still be trained with respect to a different objective that is more or less related to log-likelihood, such as contrastive divergence (Hinton, 2002), score matching (Hyvärinen, 2005), lower bounds on the log-likelihood (Bishop, 2006), noise-contrastive estimation (Gutmann \&amp; Hyvärinen, 2010), probability flow (SohlDickstein et al., 2011), maximum mean discrepancy (MMD) (Gretton et al., 2007; Li et al., 2015), or approximations to the Jensen-Shannon divergence (JSD) (Goodfellow et al., 2014).</p>
<p>For computational reasons, generative models are also often compared in terms of properties more readily accessible than likelihood, even when the task is density estimation. Examples include visualizations of model samples, interpretations of model parameters (Hyvärinen et al., 2009), Parzen window estimates of the model's log-likelihood (Breuleux et al., 2009), and evaluations of model performance in surrogate tasks such as denoising or missing value imputation.</p>
<p>In this paper, we look at some of the implications of choosing certain training and evaluation criteria. We first show that training objectives such as JSD and MMD can result in very different optima than</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: An isotropic Gaussian distribution was fit to data drawn from a mixture of Gaussians by either minimizing Kullback-Leibler divergence (KLD), maximum mean discrepancy (MMD), or Jensen-Shannon divergence (JSD). The different fits demonstrate different tradeoffs made by the three measures of distance between distributions.
log-likelihood. We then discuss the relationship between log-likelihood, classification performance, visual fidelity of samples and Parzen window estimates. We show that good or bad performance with respect to one metric is no guarantee of good or bad performance with respect to the other metrics. In particular, we show that the quality of samples is generally uninformative about the likelihood and vice versa, and that Parzen window estimates seem to favor models with neither good likelihood nor samples of highest possible quality. Using Parzen window estimates as a criterion, a simple model based on $k$-means outperforms the true distribution of the data.</p>
<h1>2 TRAINING OF GENERATIVE MODELS</h1>
<p>Many objective functions and training procedures have been proposed for optimizing generative models. The motivation for introducing new training methods is typically the wish to fit probabilistic models with computationally intractable likelihoods, rendering direct maximum likelihood learning impractical. Most of the available training procedures are consistent in the sense that if the data is drawn from a model distribution, then this model distribution will be optimal under the training objective in the limit of an infinite number of training examples. That is, if the model is correct, and for extremely large amounts of data, all of these methods will produce the same result. However, when there is a mismatch between the data distribution and the model, different objective functions can lead to very different results.</p>
<p>Figure 1 illustrates this on a simple toy example where an isotropic Gaussian distribution has been fit to a mixture of Gaussians by minimizing various measures of distance. Maximum mean discrepancy (MMD) has been used with generative moment matching networks (Li et al., 2015; Dziugaite et al., 2015) and Jensen-Shannon divergence (JSD) has connections to the objective function optimized by generative adversarial networks (Goodfellow et al., 2014) (see box for a definition). Minimizing MMD or JSD yields a Gaussian which fits one mode well, but which ignores other parts of the data. On the other hand, maximizing average log-likelihood or equivalently minimizing Kullback-Leibler divergence (KLD) avoids assigning extremely small probability to any data point but assigns a lot of probability mass to non-data regions.</p>
<p>Understanding the trade-offs between different measures is important for several reasons. First, different applications require different trade-offs, and we want to choose the right metric for a given application. Assigning sufficient probability to all plausible images is important for compression, but it may be enough to generate a single plausible example in certain image reconstruction applications (e.g., Hays \&amp; Efros, 2007). Second, a better understanding of the trade-offs allows us to better interpret and relate empirical findings. Generative image models are often assessed based on the visual fidelity of generated samples (e.g., Goodfellow et al., 2014; Gregor et al., 2015; Denton et al., 2015; Li et al., 2015). Figure 1 suggests that a model optimized with respect to KLD is more likely to produce atypical samples than the same model optimized with respect to one of the other two measures. That is, plausible samples-in the sense of having large density under the target</p>
<p>MMD (Gretton et al., 2007) is defined as,</p>
<p>$$
\operatorname{MMD}[p, q]=\left(\mathrm{E}_{p, q}\left[k\left(\mathbf{x}, \mathbf{x}^{\prime}\right)-2 k(\mathbf{x}, \mathbf{y})+k\left(\mathbf{y}, \mathbf{y}^{\prime}\right)\right]\right)^{\frac{1}{2}}
$$</p>
<p>where $\mathbf{x}, \mathbf{x}^{\prime}$ are indepent and distributed according to the data distribution $p$, and $\mathbf{y}, \mathbf{y}^{\prime}$ are independently distributed according to the model distribution $q$. We followed the approach of Li et al. (2015), optimizing an empirical estimate of MMD and using a mixture of Gaussian kernels with various bandwidths for $k$.</p>
<p>JSD is defined as</p>
<p>$$
\operatorname{JSD}[p, q]=\frac{1}{2} \operatorname{KLD}[p | m]+\frac{1}{2} \operatorname{KLD}[q | m]
$$</p>
<p>where $m=(p+q) / 2$ is an equal mixture of distributions $p$ and $q$. We optimized JSD directly using the data density, which is generally not possible in practice where we only have access to samples from the data distribution. In this case, generative adversarial networks (GANs) may be used to approximately optimize JSD, although in practical applications the objective function optimized by GANs can be very different from JSD. Parameters were initialized at the maximum likelihood solution in all cases, but the same optimum was consistently found using random initializations.
distribution-are not necessarily an indication of a good density model as measured by KLD, but may be expected when optimizing JSD.</p>
<h1>3 EVALUATION OF GENERATIVE MODELS</h1>
<p>Just as choosing the right training method is important for achieving good performance in a given application, so is choosing the right evaluation metric for drawing the right conclusions. In the following, we first continue to discuss the relationship between average log-likelihood and the visual appearance of model samples.
Model samples can be a useful diagnostic tool, often allowing us to build an intuition for why a model might fail and how it could be improved. However, qualitative as well as quantitative analyses based on model samples can be misleading about a model's density estimation performance, as well as the probabilistic model's performance in applications other than image synthesis. Below we summarize a few examples demonstrating this.</p>
<h3>3.1 LOG-LIKELIHOOD</h3>
<p>Average log-likelihood is widely considered as the default measure for quantifying generative image modeling performance. However, care needs to be taken to ensure that the numbers measured are meaningful. While natural images are typically stored using 8-bit integers, they are often modeled using densities, i.e., an image is treated as an instance of a continuous random variable. Since the discrete data distribution has differential entropy of negative infinity, this can lead to arbitrary high likelihoods even on test data. To avoid this case, it is becoming best practice to add real-valued noise to the integer pixel values to dequantize the data (e.g., Uria et al., 2013; van den Oord \&amp; Schrauwen, 2014; Theis \&amp; Bethge, 2015).
If we add the right amount of uniform noise, the log-likelihood of the continuous model on the dequantized data is closely related to the log-likelihood of a discrete model on the discrete data. Maximizing the log-likelihood on the continuous data also optimizes the log-likelihood of the discrete model on the original data. This can be seen as follows.
Consider images $\mathbf{x} \in{0, \ldots, 255}^{D}$ with a discrete probability distribution $P(\mathbf{x})$, uniform noise $\mathbf{u} \in\left[0,1\left[^{D}\right.\right.$, and noisy data $\mathbf{y}=\mathbf{x}+\mathbf{u}$. If $p$ refers to the noisy data density and $q$ refers to the model density, then we have for the average log-likelihood:</p>
<p>$$
\begin{aligned}
\int p(\mathbf{y}) \log q(\mathbf{y}) d \mathbf{y} &amp; =\sum_{\mathbf{x}} P(\mathbf{x}) \int_{[0,1]^{D}} \log q(\mathbf{x}+\mathbf{u}) d \mathbf{u} \
&amp; \leq \sum_{\mathbf{x}} P(\mathbf{x}) \log \int_{[0,1]^{D}} q(\mathbf{x}+\mathbf{u}) d \mathbf{u} \
&amp; =\sum_{\mathbf{x}} P(\mathbf{x}) \log Q(\mathbf{x})
\end{aligned}
$$</p>
<p>where the second step follows from Jensen's inequality and we have defined</p>
<p>$$
Q(\mathbf{x})=\int_{[0,1]^{D}} q(\mathbf{x}+\mathbf{u}) d \mathbf{u}
$$</p>
<p>for $\mathbf{x} \in \mathbb{Z}^{D}$. The left-hand side in Equation 3 is the expected log-likelihood which would be estimated in a typical benchmark. The right-hand side is the log-likelihood of the probability mass function $Q$ on the original discrete-valued image data. The negative of this log-likelihood is equivalent to the average number of bits (assuming base-2 logarithm) required to losslessly compress the discrete data with an entropy coding scheme optimized for $Q$ (Shannon, 2001).</p>
<h1>SEMI-SUPERVISED LEARNING</h1>
<p>A second motivation for using log-likelihood comes from semi-supervised learning. Consider a dataset consisting of images $\mathcal{X}$ and corresponding labels $\mathcal{Y}$ for some but not necessarily all of the images. In classification, we are interested in the prediction of a class label $y$ for a previously unseen query image $\mathbf{x}$. For a given model relating $\mathbf{x}, y$, and parameters $\boldsymbol{\theta}$, the only correct way to infer the distribution over $y$-from a Bayesian point of view -is to integrate out the parameters (e.g., Lasserre et al., 2006),</p>
<p>$$
p(y \mid \mathbf{x}, \mathcal{X}, \mathcal{Y})=\int p(\boldsymbol{\theta} \mid \mathcal{X}, \mathcal{Y}) p(y \mid \mathbf{x}, \boldsymbol{\theta}) d \boldsymbol{\theta}
$$</p>
<p>With sufficient data and under certain assumptions, the above integral will be close to $p\left(y \mid \mathbf{x}, \hat{\boldsymbol{\theta}}_{\mathrm{MAP}}\right)$, where</p>
<p>$$
\begin{aligned}
\hat{\boldsymbol{\theta}}<em _boldsymbol_theta="\boldsymbol{\theta">{\mathrm{MAP}} &amp; =\operatorname{argmax}</em>) \
&amp; =\operatorname{argmax}_{\boldsymbol{\theta}}[\log p(\boldsymbol{\theta})+\log p(\mathcal{X} \mid \boldsymbol{\theta})+\log p(\mathcal{Y} \mid \mathcal{X}, \boldsymbol{\theta})]
\end{aligned}
$$}} p(\boldsymbol{\theta} \mid \mathcal{X}, \mathcal{Y</p>
<p>When no training labels are given, i.e., in the unsupervised setting, and for a uniform prior over parameters, it is therefore natural to try to optimize the log-likelihood, $\log p(\mathcal{X} \mid \boldsymbol{\theta})$.
In practice, this approach might fail because of a mismatch between the model and the data, because of an inability to solve Equation 9, or because of overfitting induced by the MAP approximation. These issues can be addressed by better image models (e.g., Kingma et al., 2014), better optimization and inference procedures, or a more Bayesian treatment of the parameters (e.g., Lacoste-Julien et al., 2011; Welling \&amp; Teh, 2011).</p>
<h3>3.2 SAMPLES AND LOG-LIKELIHOOD</h3>
<p>For many interesting models, average log-likelihood is difficult to evaluate or even approximate. For some of these models at least, generating samples is a lot easier. It would therefore be useful if we could use generated samples to infer something about a model's log-likelihood. This approach is also intuitive given that a model with zero KL divergence will produce perfect samples, and visual inspection can work well in low dimensions for assessing a model's fit to data. Unfortunately these intuitions can be misleading when the image dimensionality is high. A model can have poor loglikelihood and produce great samples, or have great log-likelihood and produce poor samples.</p>
<h2>POOR LOG-LIKELIHOOD AND GREAT SAMPLES</h2>
<p>A simple lookup table storing enough training images will generate convincing looking images but will have poor average log-likelihood on unseen test data. Somewhat more generally we might</p>
<p>consider a mixture of Gaussian distributions,</p>
<p>$$
q(\mathbf{x})=\frac{1}{N} \sum_{n} \mathcal{N}\left(\mathbf{x} ; \mathbf{x}_{n}, \varepsilon^{2} \mathbf{I}\right)
$$</p>
<p>where the means $\mathbf{x}_{n}$ are either training images or a number of plausible images derived from the training set (e.g., using a set of image transformations). If $\varepsilon$ is small enough such that the Gaussian noise becomes imperceptible, this model will generate great samples but will still have very poor log-likelihood. This shows that plausible samples are clearly not sufficient for a good log-likelihood.
Gerhard et al. (2013) empirically found a correlation between some models' log-likelihoods and their samples' ability to fool human observers into thinking they were extracted from real images. However, the image patches were small and all models used in the study were optimized to minimize KLD. The correlation between log-likelihood and sample quality may disappear, for example, when considering models optimized for different objective functions or already when considering a different set of models.</p>
<h1>GREAT LOG-LIKELIHOOD AND POOR SAMPLES</h1>
<p>Perhaps surprisingly, the ability to produce plausible samples is not only not sufficient, but also not necessary for high likelihood as a simple argument by van den Oord \&amp; Dambre (2015) shows: Assume $p$ is the density of a model for $d$ dimensional data $\mathbf{x}$ which performs arbitrarily well with respect to average log-likelihood and $q$ corresponds to some bad model (e.g., white noise). Then samples generated by the mixture model</p>
<p>$$
0.01 p(\mathbf{x})+0.99 q(\mathbf{x})
$$</p>
<p>will come from the poor model $99 \%$ of the time. Yet the log-likelihood per pixel will hardly change if $d$ is large:</p>
<p>$$
\log [0.01 p(\mathbf{x})+0.99 q(\mathbf{x})] \geq \log [0.01 p(\mathbf{x})]=\log p(\mathbf{x})-\log 100
$$</p>
<p>For high-dimensional data, $\log p(\mathbf{x})$ will be proportional to $d$ while $\log 100$ stays constant. For instance, already for the 32 by 32 images found in the CIFAR-10 dataset the difference between log-likelihoods of different models can be in the thousands, while $\log (100)$ is only about 4.61 nats (van den Oord \&amp; Dambre, 2015). This shows that a model can have large average log-likelihood but generate very poor samples.</p>
<h2>GOOD LOG-LIKELIHOOD AND GREAT SAMPLES</h2>
<p>Note that we could have also chosen $q$ (Equation 11) such that it reproduces training examples, e.g., by choosing $q$ as in Equation 10. In this case, the mixture model would generate samples indistinguishable from real images $99 \%$ of the time while the log-likelihood would again only change by at most 4.61 nats. This shows that any model can be turned into a model which produces realistic samples at little expense to its log-likelihood. Log-likelihood and visual appearance of samples are therefore largely independent.</p>
<h3>3.3 SAMPLES AND APPLICATIONS</h3>
<p>One might conclude that something must be wrong with log-likelihood if it does not care about a model's ability to generate plausible samples. However, note that the mixture model in Equation 11 might also still work very well in applications. While $q$ is much more likely a priori, $p$ is going to be much more likely a posteriori in tasks like inpainting, denoising, or classification. Consider prediction of a quantity $y$ representing, for example, a class label or missing pixels. A model with joint distribution</p>
<p>$$
0.01 p(\mathbf{x}) p(y \mid \mathbf{x})+0.99 q(\mathbf{x}) q(y \mid \mathbf{x})
$$</p>
<p>may again generate poor samples $99 \%$ of the time. For a given fixed $\mathbf{x}$, the posterior over $y$ will be a mixture</p>
<p>$$
\alpha p(y \mid \mathbf{x})+(1-\alpha) q(y \mid \mathbf{x})
$$</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: A: Two examples demonstrating that small changes of an image can lead to large changes in Euclidean distance affecting the choice of nearest neighbor. The images shown represent the query image shifted by between 1 and 4 pixels (left column, top to bottom), and the corresponding nearest neighbor from the training set (right column). The gray lines indicate Euclidean distance of the query image to 100 randomly picked images from the training set. B: Fraction of query images assigned to the correct training image. The average was estimated from 1,000 images. Dashed lines indicate a $90 \%$ confidence interval.
where a few simple calculations show that</p>
<p>$$
\alpha=\sigma(\ln p(\mathbf{x})-\ln q(\mathbf{x})-\ln 99)
$$</p>
<p>and $\sigma$ is the sigmoidal logistic function. Since we assume that $p$ is a good model, $q$ is a poor model, and $\mathbf{x}$ is high-dimensional, we have</p>
<p>$$
\ln p(\mathbf{x}) \gg \ln q(\mathbf{x})+\ln 99
$$</p>
<p>and therefore $\alpha \approx 1$. That is, mixing with $q$ has hardly changed the posterior over $y$. While the samples are dominated by $q$, the classification performance is dominated by $p$. This shows that high visual fidelity of samples is generally not necessary for achieving good performance in applications.</p>
<h1>3.4 EVALUATION BASED ON SAMPLES AND NEAREST NEIGHBORS</h1>
<p>A qualitative assessment based on samples can be biased towards models which overfit (Breuleux et al., 2009). To detect overfitting to the training data, it is common to show samples next to nearest neighbors from the training set. In the following, we highlight two limitations of this approach and argue that it is unfit to detect any but the starkest forms of overfitting.</p>
<p>Nearest neighbors are typically determined based on Euclidean distance. But already perceptually small changes can lead to large changes in Euclidean distance, as is well known in the psychophysics literature (e.g., Wang \&amp; Bovik, 2009). To illustrate this property, we used the top-left 28 by 28 pixels of each image from the 50,000 training images of the CIFAR-10 dataset. We then shifted this 28 by 28 window one pixel down and one pixel to the right and extracted another set of images. We repeated this 4 times, giving us 4 sets of images which are increasingly different from the training set. Figure 2A shows nearest neighbors of corresponding images from the query set. Although the images have hardly changed visually, a shift by only two pixels already caused a different nearest neighbor. The plot also shows Euclidean distances to 100 randomly picked images from the training set. Note that with a bigger dataset, a switch to a different nearest neighbor becomes more likely. Figure 2B shows the fraction of query images assigned to the correct training image in our example. A model which stores transformed training images can trivially pass the nearest-neighbor overfitting test. This problem can be alleviated by choosing nearest neighbors based on perceptual metrics, and by showing more than one nearest neighbor.
A second problem concerns the entropy of the model distribution and is harder to address. There are different ways a model can overfit. Even when overfitting, most models will not reproduce perfect or trivially transformed copies of the training data. In this case, no distance metric will find a close match in the training set. A model which overfits might still never generate a plausible image or might only be able to generate a small fraction of all plausible images (e.g., a model as in Equation 10 where instead of training images we store several transformed versions of the</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Parzen window estimates for a Gaussian evaluated on 6 by 6 pixel image patches from the CIFAR-10 dataset. Even for small patches and a very large number of samples, the Parzen window estimate is far from the true loglikelihood.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Parzen est. [nat]</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Stacked CAE</td>
<td style="text-align: center;">121</td>
</tr>
<tr>
<td style="text-align: center;">DBN</td>
<td style="text-align: center;">138</td>
</tr>
<tr>
<td style="text-align: center;">GMMN</td>
<td style="text-align: center;">147</td>
</tr>
<tr>
<td style="text-align: center;">Deep GSN</td>
<td style="text-align: center;">214</td>
</tr>
<tr>
<td style="text-align: center;">Diffusion</td>
<td style="text-align: center;">220</td>
</tr>
<tr>
<td style="text-align: center;">GAN</td>
<td style="text-align: center;">225</td>
</tr>
<tr>
<td style="text-align: center;">True distribution</td>
<td style="text-align: center;">243</td>
</tr>
<tr>
<td style="text-align: center;">GMMN + AE</td>
<td style="text-align: center;">282</td>
</tr>
<tr>
<td style="text-align: center;">$k$-means</td>
<td style="text-align: center;">313</td>
</tr>
</tbody>
</table>
<p>Table 1: Using Parzen window estimates to evaluate various models trained on MNIST, samples from the true distribution perform worse than samples from a simple model trained with $k$-means.
training images, or a model which only describes data in a lower-dimensional subspace). Because the number of images we can process is vanishingly small compared to the vast number of possible images, we would not be able to detect this by looking at samples from the model.</p>
<h1>3.5 Evaluation based on Parzen window estimates</h1>
<p>When log-likelihoods are unavailable, a common alternative is to use Parzen window estimates. Here, samples are generated from the model and used to construct a tractable model, typically a kernel density estimator with Gaussian kernel. A test log-likelihood is then evaluated under this model and used as a proxy for the true model's log-likelihood (Breuleux et al., 2009). Breuleux et al. (2009) suggested to fit the Parzen windows on both samples and training data, and to use at least as many samples as there are images in the training set. Following Bengio et al. (2013a), Parzen windows are in practice commonly fit to only 10,000 samples (e.g., Bengio et al., 2013b; Goodfellow et al., 2014; Li et al., 2015; Sohl-Dickstein et al., 2015). But even for a large number of samples Parzen window estimates generally do not come close to a model's true log-likelihood when the data dimensionality is high. In Figure 3 we plot Parzen window estimates for a multivariate Gaussian distribution fit to small CIFAR-10 image patches (of size 6 by 6). We added uniform noise to the data (as explained in Section 3.1) and rescaled between 0 and 1. As we can see, a completely infeasible number of samples would be needed to get close to the actual log-likelihood even for this small scale example. For higher dimensional data this effect would only be more pronounced.</p>
<p>While the Parzen window estimate may be far removed from a model's true log-likelihood, one could still hope that it produces a similar or otherwise useful ranking when applied to different models. Counter to this idea, Parzen window estimates of the likelihood have been observed to produce rankings different from other estimates (Bachman \&amp; Precup, 2015). More worryingly, a GMMN+AE (Li et al., 2015) is assigned a higher score than images from the training set (which are samples from the true distribution) when evaluated on MNIST (Table 1). Furthermore it is relatively easy to exploit the Parzen window loss function to achieve even better results. To illustrate this, we fitted 10,000 centroids to the training data using $k$-means. We then generated 10,000 independent samples by sampling centroids with replacement. Note that this corresponds to the model in Equation 10, where the standard deviation of the Gaussian noise is zero and instead of training examples we use the centroids. We find that samples from this $k$-means based model are assigned a higher score than any other model, while its actual log-likelihood would be $-\infty$.</p>
<h1>4 CONCLUSION</h1>
<p>We have discussed the optimization and evaluation of generative image models. Different metrics can lead to different trade-offs, and different evaluations favor different models. It is therefore important that training and evaluation match the target application. Furthermore, we should be cautious not to take good performance in one application as evidence of good performance in another application.
An evaluation based on samples is biased towards models which overfit and therefore a poor indicator of a good density model in a log-likelihood sense, which favors models with large entropy. Conversely, a high likelihood does not guarantee visually pleasing samples. Samples can take on arbitrary form only a few bits from the optimum. It is therefore unsurprising that other approaches than density estimation are much more effective for image synthesis (Portilla \&amp; Simoncelli, 2000; Dosovitskiy et al., 2015; Gatys et al., 2015). Samples are in general also an unreliable proxy for a model's performance in applications such as classification or inpainting, as discussed in Section 3.3.
A subjective evaluation based on visual fidelity of samples is still clearly appropriate when the goal is image synthesis. Such an analysis at least has the property that the data distribution will perform very well in this task. This cannot be said about Parzen window estimates, where the data distribution performs worse than much less desirable models ${ }^{1}$. We therefore argue Parzen window estimates should be avoided for evaluating generative models, unless the application specifically requires such a loss function. In this case, we have shown that a k-means based model can perform better than the true density. To summarize, our results demonstrate that for generative models there is no one-fits-all loss function but a proper assessment of model performance is only possible in the the context of an application.</p>
<h2>ACKNOWLEDGMENTS</h2>
<p>The authors would like to thank Jascha Sohl-Dickstein, Ivo Danihelka, Andriy Mnih, and Leon Gatys for their valuable input on this manuscript.</p>
<h2>REFERENCES</h2>
<p>Bachman, P. and Precup, D. Variational Generative Stochastic Networks with Collaborative Shaping. Proceedings of the 32nd International Conference on Machine Learning, pp. 1964-1972, 2015.</p>
<p>Bengio, Y., Mesnil, G., Dauphin, Y., and Rifai, S. Better mixing via deep representations. In Proceedings of the 30th International Conference on Machine Learning, 2013a.</p>
<p>Bengio, Y., Thibodeau-Laufer, E., Alain, G., and Yosinski, J. Deep generative stochastic networks trainable by backprop, 2013b. arXiv:1306.1091.</p>
<p>Bishop, C. M. Pattern Recognition and Machine Learning. Springer, 2006.
Breuleux, O., Bengio, Y., and Vincent, P. Unlearning for better mixing. Technical report, Universite de Montreal, 2009.</p>
<p>Denton, E., Chintala, S., Szlam, A., and Fergus, R. Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks. arXiv.org, 2015.</p>
<p>Dosovitskiy, A., Springenberg, J. T., and Brox, T. Learning to Generate Chairs with Convolutional Neural Networks. In IEEE International Conference on Computer Vision and Pattern Recognition, 2015.</p>
<p>Dziugaite, G. K., Roy, D. M., and Ghahramani, Z. Training generative neural networks via maximum mean discrepancy optimization, 2015. arXiv:1505.0390.</p>
<p>Gatys, L. A., Ecker, A. S., and Bethge, M. Texture synthesis and the controlled generation of natural stimuli using convolutional neural networks, 2015. arXiv:1505.07376.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Gerhard, H. E., Wichmann, F. A., and Bethge, M. How sensitive is the human visual system to the local statistics of natural images? PLoS Computational Biology, 9(1), 2013.</p>
<p>Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and Bengio, Y. Generative adversarial nets. In Advances in Neural Information Processing Systems 27, 2014 .</p>
<p>Gregor, K., Danihelka, I., Graves, A., and Wierstra, D. DRAW: A recurrent neural network for image generation. In Proceedings of the 32nd International Conference on Machine Learning, 2015 .</p>
<p>Gretton, A., Borgwardt, K. M., Rasch, M., Schölkopf, B., and Smola, A. J. A kernel method for the two-sample-problem. In Advances in Neural Information Processing Systems 20, 2007.</p>
<p>Gutmann, M. and Hyvärinen, A. Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. In Proceedings of the 13th International Conference on Artificial Intelligence and Statistics, 2010.</p>
<p>Hays, J. and Efros, A. A. Scene completion using millions of photographs. ACM Transactions on Graphics (SIGGRAPH), 26, 2007.</p>
<p>Hinton, G. E. Training Products of Experts by Minimizing Contrastive Divergence. Neural Computation, 14(8):1771-1800, 2002.</p>
<p>Hyvärinen, A., Hurri, J., and Hoyer, P. O. Natural Image Statistics: A Probabilistic Approach to Early Computational Vision. Springer, 2009.</p>
<p>Hyvärinen, A. Estimation of non-normalized statistical models using score matching. Journal of Machine Learning Research, pp. 695-709, 2005.</p>
<p>Kingma, D. P., Rezende, D. J., Mohamed, S., and Welling, M. Semi-supervised learning with deep generative models. In Advances in Neural Information Processing Systems 27, 2014.</p>
<p>Lacoste-Julien, S., Huszar, F., and Ghahramani, Z. Approximate inference for the loss-calibrated Bayesian. In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics, 2011.</p>
<p>Lasserre, J. A., Bishop, C. M., and Minka, T. P. Principled hybrids of generative and discriminative models. In Proceedings of the Computer Vision and Pattern Recognition Conference, 2006.</p>
<p>Li, Y., Swersky, K., and Zemel, R. Generative moment matching networks. In Proceedings of the 32nd International Conference on Machine Learning, 2015.</p>
<p>Portilla, J. and Simoncelli, E. P. A parametric texture model based on joint statistics of complex wavelet coefficients. International Journal of Computer Vision, 40:49-70, 2000.</p>
<p>Shannon, C. E. A mathematical theory of communication. ACM SIGMOBILE Mobile Computing and Communications Review, 5(1):3-55, 2001.</p>
<p>Sohl-Dickstein, J., Battaglino, P., and DeWeese, M. R. Minimum Probability Flow Learning. In Proceedings of the 28th International Conference on Machine Learning, 2011.</p>
<p>Sohl-Dickstein, J., Weiss, E. A., Maheswaranathan, N., and Ganguli, S. Deep unsupervised learning using nonequilibrium thermodynamics. In Proceedings of the 32nd International Conference on Machine Learning, 2015.</p>
<p>Theis, L. and Bethge, M. Generative Image Modeling Using Spatial LSTMs. In Advances in Neural Information Processing Systems 28, 2015.</p>
<p>Uria, B., Murray, I., and Larochelle, H. RNADE: The real-valued neural autoregressive densityestimator. In Advances in Neural Information Processing Systems 26, 2013.
van den Oord, A. and Dambre, J. Locally-connected transformations for deep GMMs, 2015. Deep Learning Workshop, ICML.</p>
<p>van den Oord, A. and Schrauwen, B. Factoring Variations in Natural Images with Deep Gaussian Mixture Models. In Advances in Neural Information Processing Systems 27, 2014.</p>
<p>Wang, Z. and Bovik, A. C. Mean squared error: Love it or leave it? IEEE Signal Processing Magazine, 2009.</p>
<p>Welling, M. and Teh, Y. W. Bayesian Learning via Stochastic Gradient Langevin Dynamics. In Proceedings of the 28th International Conference on Machine Learning, 2011.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ In decision theory, such a metric is called an improper scoring function.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
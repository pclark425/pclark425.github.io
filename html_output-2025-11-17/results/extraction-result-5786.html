<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5786 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5786</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5786</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-116.html">extraction-schema-116</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <p><strong>Paper ID:</strong> paper-b8cb51003582e47a233576c9fac63498d7d64fd5</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/b8cb51003582e47a233576c9fac63498d7d64fd5" target="_blank">SemEval-2010 Task 8: Multi-Way Classification of Semantic Relations between Pairs of Nominals</a></p>
                <p><strong>Paper Venue:</strong> International Workshop on Semantic Evaluation</p>
                <p><strong>Paper TL;DR:</strong> This paper defines the task, describes the training and test data and the process of their creation, lists the participating systems (10 teams, 28 runs), and discusses their results.</p>
                <p><strong>Paper Abstract:</strong> SemEval-2 Task 8 focuses on Multi-way classification of semantic relations between pairs of nominals. The task was designed to compare different approaches to semantic relation classification and to provide a standard testbed for future research. This paper defines the task, describes the training and test data and the process of their creation, lists the participating systems (10 teams, 28 runs), and discusses their results.</p>
                <p><strong>Cost:</strong> 0.004</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5786",
    "paper_id": "paper-b8cb51003582e47a233576c9fac63498d7d64fd5",
    "extraction_schema_id": "extraction-schema-116",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.003508,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>SemEval-2010 Task 8: Multi-Way Classification of Semantic Relations Between Pairs of Nominals</h1>
<p>Iris Hendrickx ${ }^{<em>}$, Su Nam Kim ${ }^{\dagger}$, Zornitsa Kozareva ${ }^{\ddagger}$, Preslav Nakov ${ }^{\S}$, Diarmuid Ó Séaghdha</em>, Sebastian Padó ${ }^{\S}$, Marco Pennacchiotti ${ }^{*}$; Lorenza Romano ${ }^{\dagger \dagger}$, Stan Szpakowicz ${ }^{\ddagger \ddagger}$</p>
<h4>Abstract</h4>
<p>SemEval-2 Task 8 focuses on Multi-way classification of semantic relations between pairs of nominals. The task was designed to compare different approaches to semantic relation classification and to provide a standard testbed for future research. This paper defines the task, describes the training and test data and the process of their creation, lists the participating systems (10 teams, 28 runs), and discusses their results.</p>
<h2>1 Introduction</h2>
<p>SemEval-2010 Task 8 focused on semantic relations between pairs of nominals. For example, tea and ginseng are in an ENTITY-ORIGIN relation in "The cup contained tea from dried ginseng.". The automatic recognition of semantic relations has many applications, such as information extraction, document summarization, machine translation, or construction of thesauri and semantic networks. It can also facilitate auxiliary tasks such as word sense disambiguation, language modeling, paraphrasing, and recognizing textual entailment.</p>
<p>Our goal was to create a testbed for automatic classification of semantic relations. In developing the task we met several challenges: selecting a suitable set of relations, specifying the annotation procedure, and deciding on the details of the task itself. They are discussed briefly in Section 2; see also Hendrickx et al. (2009), which includes a survey of related work. The direct predecessor of Task 8 was Classification of semantic relations between nominals, Task 4 at SemEval-1 (Girju et al., 2009),</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>which had a separate binary-labeled dataset for each of seven relations. We have defined SemEval2010 Task 8 as a multi-way classification task in which the label for each example must be chosen from the complete set of ten relations and the mapping from nouns to argument slots is not provided in advance. We also provide more data: 10,717 annotated examples, compared to 1,529 in SemEval-1 Task 4.</p>
<h2>2 Dataset Creation</h2>
<h3>2.1 The Inventory of Semantic Relations</h3>
<p>We first decided on an inventory of semantic relations. Ideally, it should be exhaustive (enable the description of relations between any pair of nominals) and mutually exclusive (each pair of nominals in context should map onto only one relation). The literature, however, suggests that no relation inventory satisfies both needs, and, in practice, some trade-off between them must be accepted.</p>
<p>As a pragmatic compromise, we selected nine relations with coverage sufficiently broad to be of general and practical interest. We aimed at avoiding semantic overlap as much as possible. We included, however, two groups of strongly related relations (Entity-Origin / Entity-Destination and Content-Container / ComponentWhole / Member-Collection) to assess models' ability to make such fine-grained distinctions. Our inventory is given below. The first four were also used in SemEval-1 Task 4, but the annotation guidelines have been revised, and thus no complete continuity should be assumed.</p>
<p>Cause-Effect (CE). An event or object leads to an effect. Example: those cancers were caused by radiation exposures</p>
<p>Instrument-Agency (IA). An agent uses an instrument. Example: phone operator</p>
<p>Product-Producer (PP). A producer causes a product to exist. Example: a factory manufactures suits</p>
<p>Content-Container (CC). An object is physically stored in a delineated area of space. Example: a bottle full of honey was weighed
Entity-Origin (EO). An entity is coming or is derived from an origin (e.g., position or material). Example: letters from foreign countries
Entity-Destination (ED). An entity is moving towards a destination. Example: the boy went to bed
Component-Whole (CW). An object is a component of a larger whole. Example: my apartment has a large kitchen</p>
<p>Member-Collection (MC). A member forms a nonfunctional part of a collection. Example: there are many trees in the forest
Message-Topic (MT). A message, written or spoken, is about a topic. Example: the lecture was about semantics</p>
<h3>2.2 Annotation Guidelines</h3>
<p>We defined a set of general annotation guidelines as well as detailed guidelines for each semantic relation. Here, we describe the general guidelines, which delineate the scope of the data to be collected and state general principles relevant to the annotation of all relations. ${ }^{1}$</p>
<p>Our objective is to annotate instances of semantic relations which are true in the sense of holding in the most plausible truth-conditional interpretation of the sentence. This is in the tradition of the Textual Entailment or Information Validation paradigm (Dagan et al., 2009), and in contrast to "aboutness" annotation such as semantic roles (Carreras and Márquez, 2004) or the BioNLP 2009 task (Kim et al., 2009) where negated relations are also labelled as positive. Similarly, we exclude instances of semantic relations which hold only in speculative or counterfactural scenarios. In practice, this means disallowing annotations within the scope of modals or negations, e.g., "Smoking may/may not have caused cancer in this case."</p>
<p>We accept as relation arguments only noun phrases with common-noun heads. This distinguishes our task from much work in Information Extraction, which tends to focus on specific classes of named entities and on considerably more finegrained relations than we do. Named entities are a specific category of nominal expressions best dealt</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>with using techniques which do not apply to common nouns. We only mark up the semantic heads of nominals, which usually span a single word, except for lexicalized terms such as science fiction.</p>
<p>We also impose a syntactic locality requirement on example candidates, thus excluding instances where the relation arguments occur in separate sentential clauses. Permissible syntactic patterns include simple and relative clauses, compounds, and pre- and post-nominal modification. In addition, we did not annotate examples whose interpretation relied on discourse knowledge, which led to the exclusion of pronouns as arguments. Please see the guidelines for details on other issues, including noun compounds, aspectual phenomena and temporal relations.</p>
<h3>2.3 The Annotation Process</h3>
<p>The annotation took place in three rounds. First, we manually collected around 1,200 sentences for each relation through pattern-based Web search. In order to ensure a wide variety of example sentences, we used a substantial number of patterns for each relation, typically between one hundred and several hundred. Importantly, in the first round, the relation itself was not annotated: the goal was merely to collect positive and near-miss candidate instances. A rough aim was to have $90 \%$ of candidates which instantiate the target relation ("positive instances").</p>
<p>In the second round, the collected candidates for each relation went to two independent annotators for labeling. Since we have a multi-way classification task, the annotators used the full inventory of nine relations plus OTHER. The annotation was made easier by the fact that the cases of overlap were largely systematic, arising from general phenomena like metaphorical use and situations where more than one relation holds. For example, there is a systematic potential overlap between CONTENTContainer and EntitY-DEStination depending on whether the situation described in the sentence is static or dynamic, e.g., "When I came, the $<e 1>$ apples $&lt;/ e 1&gt;$ were already put in the $<e 2>$ basket $&lt;/ e 2&gt;$." is CC(e1, e2), while "Then, the $<e 1>$ apples $&lt;/ e 1&gt;$ were quickly put in the $<e 2>$ basket $&lt;/ e 2&gt;$." is ED(e1, e2).</p>
<p>In the third round, the remaining disagreements were resolved, and, if no consensus could be achieved, the examples were removed. Finally, we merged all nine datasets to create a set of 10,717 instances. We released 8,000 for training and kept</p>
<p>the rest for testing. ${ }^{2}$
Table 1 shows some statistics about the dataset. The first column (Freq) shows the absolute and relative frequencies of each relation. The second column (Pos) shows that the average share of positive instances was closer to $75 \%$ than to $90 \%$, indicating that the patterns catch a substantial amount of "nearmiss" cases. However, this effect varies a lot across relations, causing the non-uniform relation distribution in the dataset (first column). ${ }^{3}$ After the second round, we also computed inter-annotator agreement (third column, IAA). Inter-annotator agreement was computed on the sentence level, as the percentage of sentences for which the two annotations were identical. That is, these figures can be interpreted as exact-match accuracies. We do not report Kappa, since chance agreement on preselected candidates is difficult to estimate. ${ }^{4}$ IAA is between $60 \%$ and $95 \%$, again with large relation-dependent variation. Some of the relations were particularly easy to annotate, notably CONTENT-CONTAINER, which can be resolved through relatively clear criteria, despite the systematic ambiguity mentioned above. Entity-ORIGIN was the hardest relation to annotate. We encountered ontological difficulties in defining both Entity (e.g., in contrast to Effect) and Origin (as opposed to Cause). Our numbers are on average around $10 \%$ higher than those reported by Girju et al. (2009). This may be a side effect of our data collection method. To gather 1,200 examples in realistic time, we had to seek productive search query patterns, which invited certain homogeneity. For example, many queries for CONTENT-CONTAINER centered on "usual suspect" such as box or suitcase. Many instances of Member-Collection were collected on the basis of from available lists of collective names.</p>
<h2>3 The Task</h2>
<p>The participating systems had to solve the following task: given a sentence and two tagged nominals, predict the relation between those nominals and the direction of the relation.</p>
<p>We released a detailed scorer which outputs (1) a confusion matrix, (2) accuracy and coverage, (3)</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 1: Annotation Statistics. Freq: Absolute and relative frequency in the dataset; Pos: percentage of "positive" relation instances in the candidate set; IAA: inter-annotator agreement
precision $(\mathrm{P})$, recall $(\mathrm{R})$, and $\mathrm{F}<em 1="1">{1}$-Score for each relation, (4) micro-averaged $\mathrm{P}, \mathrm{R}, \mathrm{F}</em>}$, (5) macroaveraged $\mathrm{P}, \mathrm{R}, \mathrm{F<em 1="1">{1}$. For (4) and (5), the calculations ignored the OTHER relation. Our official scoring metric is macro-averaged $\mathrm{F}</em>$-Score for ( $9+1$ )-way classification, taking directionality into account.</p>
<p>The teams were asked to submit test data predictions for varying fractions of the training data. Specifically, we requested results for the first 1000, 2000, 4000, and 8000 training instances, called TD1 through TD4. TD4 was the full training set.</p>
<h2>4 Participants and Results</h2>
<p>Table 2 lists the participants and provides a rough overview of the system features. Table 3 shows the results. Unless noted otherwise, all quoted numbers are $\mathrm{F}_{1}$-Scores.</p>
<p>Overall Ranking and Training Data. We rank the teams by the performance of their best system on TD4, since a per-system ranking would favor teams with many submitted runs. UTD submitted the best system, with a performance of over $82 \%$, more than $4 \%$ better than the second-best system. FBK_IRST places second, with $77.62 \%$, a tiny margin ahead of ISI ( $77.57 \%$ ). Notably, the ISI system outperforms the FBK_IRST system for TD1 to TD3, where it was second-best. The accuracy numbers for TD4 (Acc TD4) lead to the same overall ranking: micro- versus macro-averaging does not appear to make much difference either. A random baseline gives an uninteresting score of $6 \%$. Our competitive baseline system is a simple Naive Bayes classifier which relies on words in the sentential context only; two systems scored below this baseline.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">System</th>
<th style="text-align: center;">Institution</th>
<th style="text-align: center;">Team</th>
<th style="text-align: center;">Description</th>
<th style="text-align: center;">Res.</th>
<th style="text-align: center;">Class.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Baseline</td>
<td style="text-align: center;">Task organizers</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">local context of 2 words only</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">BN</td>
</tr>
<tr>
<td style="text-align: center;">ECNU-SR-1</td>
<td style="text-align: center;">East China Normal University</td>
<td style="text-align: center;">Man Lan, Yuan <br> Chen, Zhimin <br> Zhou, Yu Xu</td>
<td style="text-align: center;">stem, POS, syntactic patterns</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">$\begin{aligned} &amp; \hline \text { SVM } \ &amp; \text { (multi) } \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;">ECNU-SR-2,3</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">features like ECNU-SR-1, different prob. thresholds</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">SVM <br> (binary)</td>
</tr>
<tr>
<td style="text-align: center;">ECNU-SR-4</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">stem, POS, syntactic patterns, hyponymy and meronymy relations</td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { WN, } \ &amp; \text { S } \end{aligned}$</td>
<td style="text-align: center;">SVM <br> (multi)</td>
</tr>
<tr>
<td style="text-align: center;">ECNU-SR-5,6</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">features like ECNU-SR-4, different prob. thresholds</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">SVM <br> (binary)</td>
</tr>
<tr>
<td style="text-align: center;">ECNU-SR-7</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">majority vote of ECNU-1,2,4,5</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">FBK_IRST-6C32</td>
<td style="text-align: center;">Fondazione Bruno Kessler</td>
<td style="text-align: center;">Claudio Giuliano, Kateryna Tymoshenko</td>
<td style="text-align: center;">3-word window context features (word form, part of speech, orthography) + Cyc; parameter estimation by optimization on training set</td>
<td style="text-align: center;">Cyc</td>
<td style="text-align: center;">SVM</td>
</tr>
<tr>
<td style="text-align: center;">FBK_IRST-12C32</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">FBK_IRST-6C32 + distance features</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">FBK_IRST-12VBC32</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">FBK_IRST-12C32 + verbs</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">FBK_IRST-6CA, -12CA, -12VBCA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">features as above, parameter estimation by cross-validation</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">FBK_NK-RES1</td>
<td style="text-align: center;">Fondazione Bruno Kessler</td>
<td style="text-align: center;">Matteo Negri, <br> Milen Kouylekov</td>
<td style="text-align: center;">collocations, glosses, semantic relations of nominals + context features</td>
<td style="text-align: center;">WN</td>
<td style="text-align: center;">BN</td>
</tr>
<tr>
<td style="text-align: center;">FBK_NK-RES 2,3,4</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">like FBK_NK-RES1 with different context windows and collocation cutoffs</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">ISI</td>
<td style="text-align: center;">Information Sciences Institute, University of Southern California</td>
<td style="text-align: center;">Stephen Tratz</td>
<td style="text-align: center;">features from different resources, a noun compound relation system, and various feature related to capitalization, affixes, closed-class words</td>
<td style="text-align: center;">$\begin{aligned} &amp; \hline \text { WN, } \ &amp; \text { RT, G } \end{aligned}$</td>
<td style="text-align: center;">ME</td>
</tr>
<tr>
<td style="text-align: center;">ISTI-1,2</td>
<td style="text-align: center;">Istituto di sci- <br> enca e tecnologie <br> dell'informazione <br> "A. Faedo"</td>
<td style="text-align: center;">Andrea Esuli, Diego Marcheggiani, Fabrizio Sebastiani</td>
<td style="text-align: center;">Boosting-based classification. Runs differ in their initialization.</td>
<td style="text-align: center;">WN</td>
<td style="text-align: center;">2 S</td>
</tr>
<tr>
<td style="text-align: center;">JU</td>
<td style="text-align: center;">Jadavpur University</td>
<td style="text-align: center;">Santanu Pal, Partha <br> Pakray, Dipankar <br> Das, Sivaji Bandyopadhyay</td>
<td style="text-align: center;">Verbs, nouns, and prepositions; seed lists for semantic relations; parse features and NEs</td>
<td style="text-align: center;">$\begin{aligned} &amp; \hline \text { WN, } \ &amp; \text { S } \end{aligned}$</td>
<td style="text-align: center;">CRF</td>
</tr>
<tr>
<td style="text-align: center;">SEKA</td>
<td style="text-align: center;">Hungarian Academy of Sciences</td>
<td style="text-align: center;">Eszter Simon, Andras Kornai</td>
<td style="text-align: center;">Levin and Roget classes, ngrams; other grammatical and formal features</td>
<td style="text-align: center;">$\begin{aligned} &amp; \hline \text { RT, } \ &amp; \text { LC } \end{aligned}$</td>
<td style="text-align: center;">ME</td>
</tr>
<tr>
<td style="text-align: center;">TUD-base</td>
<td style="text-align: center;">Technische Universität Darmstadt</td>
<td style="text-align: center;">György Szarvas, <br> Iryna Gurevych</td>
<td style="text-align: center;">word, POS n-grams, dependency path, distance</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">ME</td>
</tr>
<tr>
<td style="text-align: center;">TUD-wp</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">TUD-base + ESA semantic relatedness scores</td>
<td style="text-align: center;">+WP</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">TUD-comb</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">TUD-base + own semantic relatedness scores</td>
<td style="text-align: center;">+WP,WN</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">TUD-comb-threshold</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">TUD-comb with higher threshold for OTHER</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">UNITN</td>
<td style="text-align: center;">University of <br> Trento</td>
<td style="text-align: center;">Fabio Celli</td>
<td style="text-align: center;">punctuation, context words, prepositional patterns, estimation of semantic relation</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">DR</td>
</tr>
<tr>
<td style="text-align: center;">UTD</td>
<td style="text-align: center;">University of Texas at Dallas</td>
<td style="text-align: center;">Bryan Rink, Sanda Harabagiu</td>
<td style="text-align: center;">context wods, hypernyms, POS, dependencies, distance, semantic roles, Levin classes, paraphrases</td>
<td style="text-align: center;">$\begin{aligned} &amp; \hline \text { WN, } \ &amp; \text { S, G, } \ &amp; \text { PB/NB, } \ &amp; \text { LC } \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; \hline \text { SVM, } \ &amp; 2 \mathrm{~S} \ &amp; \hline \end{aligned}$</td>
</tr>
</tbody>
</table>
<p>Table 2: Participants of SemEval-2010 Task 8. Res: Resources used (WN: WordNet data; WP: Wikipedia data; S: syntax; LC: Levin classes; G: Google n-grams, RT: Roget's Thesaurus, PB/NB: PropBank/NomBank). Class: Classification style (ME: Maximum Entropy; BN: Bayes Net; DR: Decision Rules/Trees; CRF: Conditional Random Fields; 2S: two-step classification)</p>
<table>
<thead>
<tr>
<th style="text-align: center;">System</th>
<th style="text-align: center;">TD1</th>
<th style="text-align: center;">TD2</th>
<th style="text-align: center;">TD3</th>
<th style="text-align: center;">TD4</th>
<th style="text-align: center;">Acc TD4</th>
<th style="text-align: center;">Rank</th>
<th style="text-align: center;">Best Cat</th>
<th style="text-align: center;">Worst Cat-9</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Baseline</td>
<td style="text-align: center;">33.04</td>
<td style="text-align: center;">42.41</td>
<td style="text-align: center;">50.89</td>
<td style="text-align: center;">57.52</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">MC (75.1)</td>
<td style="text-align: center;">IA (28.0)</td>
</tr>
<tr>
<td style="text-align: center;">ECNU-SR-1</td>
<td style="text-align: center;">52.13</td>
<td style="text-align: center;">56.58</td>
<td style="text-align: center;">58.16</td>
<td style="text-align: center;">60.08</td>
<td style="text-align: center;">57.1</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">CE (79.7)</td>
<td style="text-align: center;">IA (32.2)</td>
</tr>
<tr>
<td style="text-align: center;">ECNU-SR-2</td>
<td style="text-align: center;">46.24</td>
<td style="text-align: center;">47.99</td>
<td style="text-align: center;">69.83</td>
<td style="text-align: center;">72.59</td>
<td style="text-align: center;">67.1</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">CE (84.4)</td>
<td style="text-align: center;">IA (52.2)</td>
</tr>
<tr>
<td style="text-align: center;">ECNU-SR-3</td>
<td style="text-align: center;">39.89</td>
<td style="text-align: center;">42.29</td>
<td style="text-align: center;">65.47</td>
<td style="text-align: center;">68.50</td>
<td style="text-align: center;">62.0</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">CE (83.4)</td>
<td style="text-align: center;">IA (46.5)</td>
</tr>
<tr>
<td style="text-align: center;">ECNU-SR-4</td>
<td style="text-align: center;">67.95</td>
<td style="text-align: center;">70.58</td>
<td style="text-align: center;">72.99</td>
<td style="text-align: center;">74.82</td>
<td style="text-align: center;">70.5</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">CE (84.6)</td>
<td style="text-align: center;">IA (61.4)</td>
</tr>
<tr>
<td style="text-align: center;">ECNU-SR-5</td>
<td style="text-align: center;">49.32</td>
<td style="text-align: center;">50.70</td>
<td style="text-align: center;">72.63</td>
<td style="text-align: center;">75.43</td>
<td style="text-align: center;">70.2</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">CE (85.1)</td>
<td style="text-align: center;">IA (60.7)</td>
</tr>
<tr>
<td style="text-align: center;">ECNU-SR-6</td>
<td style="text-align: center;">42.88</td>
<td style="text-align: center;">45.54</td>
<td style="text-align: center;">68.87</td>
<td style="text-align: center;">72.19</td>
<td style="text-align: center;">65.8</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">CE (85.2)</td>
<td style="text-align: center;">IA (56.7)</td>
</tr>
<tr>
<td style="text-align: center;">ECNU-SR-7</td>
<td style="text-align: center;">58.67</td>
<td style="text-align: center;">58.87</td>
<td style="text-align: center;">72.79</td>
<td style="text-align: center;">75.21</td>
<td style="text-align: center;">70.2</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">CE (86.1)</td>
<td style="text-align: center;">IA (61.8)</td>
</tr>
<tr>
<td style="text-align: center;">FBK_IRST-6C32</td>
<td style="text-align: center;">60.19</td>
<td style="text-align: center;">67.31</td>
<td style="text-align: center;">71.78</td>
<td style="text-align: center;">76.81</td>
<td style="text-align: center;">72.4</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">ED (82.6)</td>
<td style="text-align: center;">IA (69.4)</td>
</tr>
<tr>
<td style="text-align: center;">FBK_IRST-12C32</td>
<td style="text-align: center;">60.66</td>
<td style="text-align: center;">67.91</td>
<td style="text-align: center;">72.04</td>
<td style="text-align: center;">76.91</td>
<td style="text-align: center;">72.4</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">MC (84.2)</td>
<td style="text-align: center;">IA (68.8)</td>
</tr>
<tr>
<td style="text-align: center;">FBK_IRST-12VBC32</td>
<td style="text-align: center;">62.64</td>
<td style="text-align: center;">69.86</td>
<td style="text-align: center;">73.19</td>
<td style="text-align: center;">77.11</td>
<td style="text-align: center;">72.3</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">ED (85.9)</td>
<td style="text-align: center;">PP (68.1)</td>
</tr>
<tr>
<td style="text-align: center;">FBK_IRST-6CA</td>
<td style="text-align: center;">60.58</td>
<td style="text-align: center;">67.14</td>
<td style="text-align: center;">71.63</td>
<td style="text-align: center;">76.28</td>
<td style="text-align: center;">71.4</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">CE (82.3)</td>
<td style="text-align: center;">IA (67.7)</td>
</tr>
<tr>
<td style="text-align: center;">FBK_IRST-12CA</td>
<td style="text-align: center;">61.33</td>
<td style="text-align: center;">67.80</td>
<td style="text-align: center;">71.65</td>
<td style="text-align: center;">76.39</td>
<td style="text-align: center;">71.4</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">ED (81.8)</td>
<td style="text-align: center;">IA (67.5)</td>
</tr>
<tr>
<td style="text-align: center;">FBK_IRST-12VBCA</td>
<td style="text-align: center;">63.61</td>
<td style="text-align: center;">70.20</td>
<td style="text-align: center;">73.40</td>
<td style="text-align: center;">77.62</td>
<td style="text-align: center;">72.8</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">ED (86.5)</td>
<td style="text-align: center;">IA (67.3)</td>
</tr>
<tr>
<td style="text-align: center;">FBK_NK-RES1</td>
<td style="text-align: center;">$55.71^{*}$</td>
<td style="text-align: center;">$64.06^{*}$</td>
<td style="text-align: center;">$67.80^{*}$</td>
<td style="text-align: center;">68.02</td>
<td style="text-align: center;">62.1</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">ED (77.6)</td>
<td style="text-align: center;">IA (52.9)</td>
</tr>
<tr>
<td style="text-align: center;">FBK_NK-RES2</td>
<td style="text-align: center;">$54.27^{*}$</td>
<td style="text-align: center;">$63.68^{*}$</td>
<td style="text-align: center;">$67.08^{*}$</td>
<td style="text-align: center;">67.48</td>
<td style="text-align: center;">61.4</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">ED (77.4)</td>
<td style="text-align: center;">PP (55.2)</td>
</tr>
<tr>
<td style="text-align: center;">FBK_NK-RES3</td>
<td style="text-align: center;">$54.25^{*}$</td>
<td style="text-align: center;">$62.73^{*}$</td>
<td style="text-align: center;">$66.11^{*}$</td>
<td style="text-align: center;">66.90</td>
<td style="text-align: center;">60.5</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">MC (76.7)</td>
<td style="text-align: center;">IA (56.3)</td>
</tr>
<tr>
<td style="text-align: center;">FBK_NK-RES4</td>
<td style="text-align: center;">$44.11^{*}$</td>
<td style="text-align: center;">$58.85^{*}$</td>
<td style="text-align: center;">$63.06^{*}$</td>
<td style="text-align: center;">65.84</td>
<td style="text-align: center;">59.4</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">MC (76.1)</td>
<td style="text-align: center;">IA/PP (58.0)</td>
</tr>
<tr>
<td style="text-align: center;">ISI</td>
<td style="text-align: center;">66.68</td>
<td style="text-align: center;">71.01</td>
<td style="text-align: center;">75.51</td>
<td style="text-align: center;">77.57</td>
<td style="text-align: center;">72.7</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">CE (87.6)</td>
<td style="text-align: center;">IA (61.5)</td>
</tr>
<tr>
<td style="text-align: center;">ISTI-1</td>
<td style="text-align: center;">$50.49^{*}$</td>
<td style="text-align: center;">$55.80^{*}$</td>
<td style="text-align: center;">$61.14^{*}$</td>
<td style="text-align: center;">68.42</td>
<td style="text-align: center;">63.2</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">ED (80.7)</td>
<td style="text-align: center;">PP (53.8)</td>
</tr>
<tr>
<td style="text-align: center;">ISTI-2</td>
<td style="text-align: center;">$50.69^{*}$</td>
<td style="text-align: center;">$54.29^{*}$</td>
<td style="text-align: center;">$59.77^{*}$</td>
<td style="text-align: center;">66.65</td>
<td style="text-align: center;">61.5</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">ED (80.2)</td>
<td style="text-align: center;">IA (48.9)</td>
</tr>
<tr>
<td style="text-align: center;">$J U$</td>
<td style="text-align: center;">$41.62^{*}$</td>
<td style="text-align: center;">$44.98^{*}$</td>
<td style="text-align: center;">$47.81^{*}$</td>
<td style="text-align: center;">52.16</td>
<td style="text-align: center;">50.2</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">CE (75.6)</td>
<td style="text-align: center;">IA (27.8)</td>
</tr>
<tr>
<td style="text-align: center;">SEKA</td>
<td style="text-align: center;">51.81</td>
<td style="text-align: center;">56.34</td>
<td style="text-align: center;">61.10</td>
<td style="text-align: center;">66.33</td>
<td style="text-align: center;">61.9</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">CE (84.0)</td>
<td style="text-align: center;">PP (43.7)</td>
</tr>
<tr>
<td style="text-align: center;">TUD-base</td>
<td style="text-align: center;">50.81</td>
<td style="text-align: center;">54.61</td>
<td style="text-align: center;">56.98</td>
<td style="text-align: center;">60.50</td>
<td style="text-align: center;">56.1</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">CE (80.7)</td>
<td style="text-align: center;">IA (31.1)</td>
</tr>
<tr>
<td style="text-align: center;">TUD-wp</td>
<td style="text-align: center;">55.34</td>
<td style="text-align: center;">60.90</td>
<td style="text-align: center;">63.78</td>
<td style="text-align: center;">68.00</td>
<td style="text-align: center;">63.5</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">ED (82.9)</td>
<td style="text-align: center;">IA (44.1)</td>
</tr>
<tr>
<td style="text-align: center;">TUD-comb</td>
<td style="text-align: center;">57.84</td>
<td style="text-align: center;">62.52</td>
<td style="text-align: center;">66.41</td>
<td style="text-align: center;">68.88</td>
<td style="text-align: center;">64.6</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">CE (83.8)</td>
<td style="text-align: center;">IA (46.8)</td>
</tr>
<tr>
<td style="text-align: center;">TUD-comb-0</td>
<td style="text-align: center;">58.35</td>
<td style="text-align: center;">62.45</td>
<td style="text-align: center;">66.86</td>
<td style="text-align: center;">69.23</td>
<td style="text-align: center;">65.4</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">CE (83.4)</td>
<td style="text-align: center;">IA (46.9)</td>
</tr>
<tr>
<td style="text-align: center;">UNITN</td>
<td style="text-align: center;">$16.57^{*}$</td>
<td style="text-align: center;">$18.56^{*}$</td>
<td style="text-align: center;">$22.45^{*}$</td>
<td style="text-align: center;">26.67</td>
<td style="text-align: center;">27.4</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">ED (46.4)</td>
<td style="text-align: center;">PP (0)</td>
</tr>
<tr>
<td style="text-align: center;">UTD</td>
<td style="text-align: center;">73.08</td>
<td style="text-align: center;">77.02</td>
<td style="text-align: center;">79.93</td>
<td style="text-align: center;">82.19</td>
<td style="text-align: center;">77.9</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">CE (89.6)</td>
<td style="text-align: center;">IA (68.5)</td>
</tr>
</tbody>
</table>
<p>Table 3: $\mathrm{F}_{1}$-Score of all submitted systems on the test dataset as a function of training data: TD1=1000, TD2=2000, TD3=4000, TD4=8000 training examples. Official results are calculated on TD4. The results marked with * were submitted after the deadline. The best-performing run for each participant is italicized.</p>
<p>As for the amount of training data, we see a substantial improvement for all systems between TD1 and TD4, with diminishing returns for the transition between TD3 and TD4 for many, but not all, systems. Overall, the differences between systems are smaller for TD4 than they are for TD1. The spread between the top three systems is around $10 \%$ at TD1, but below 5\% at TD4. Still, there are clear differences in the influence of training data size even among systems with the same overall architecture. Notably, ECNU-SR-4 is the second-best system at TD1 ( $67.95 \%$ ), but gains only $7 \%$ from the eightfold increase of the size of the training data. At the same time, ECNU-SR-3 improves from less than $40 \%$ to almost $69 \%$. The difference between the systems is that ECNU-SR-4 uses a multi-way classifier including the class OTHER, while ECNU-SR-3 uses binary classifiers and assigns OTHER if no other relation was assigned with $p&gt;0.5$. It appears that these probability estimates for classes are only reliable enough for TD3 and TD4.</p>
<p>The Influence of System Architecture. Almost all systems used either MaxEnt or SVM classifiers,
with no clear advantage for either. Similarly, two systems, UTD and ISTI (rank 1 and 6) split the task into two classification steps (relation and direction), but the 2nd- and 3rd-ranked systems do not. The use of a sequence model such as a CRF did not show a benefit either.</p>
<p>The systems use a variety of resources. Generally, richer feature sets lead to better performance (although the differences are often small - compare the different FBK_IRST systems). This improvement can be explained by the need for semantic generalization from training to test data. This need can be addressed using WordNet (contrast ECNU-1 to -3 with ECNU-4 to -6), the Google $n$-gram collection (see ISI and UTD), or a "deep" semantic resource (FBK_IRST uses Cyc). Yet, most of these resources are also included in the less successful systems, so beneficial integration of knowledge sources into semantic relation classification seems to be difficult.</p>
<p>System Combination. The differences between the systems suggest that it might be possible to achieve improvements by building an ensemble</p>
<p>system. When we combine the top three systems (UTD, FBK_IRST-12VBCA, and ISI) by predicting their majority vote, or OTHER if there was none, we obtain a small improvement over the UTD system with an $\mathrm{F}_{1}$-Score of $82.79 \%$. A combination of the top five systems using the same method shows a worse performance, however ( $80.42 \%$ ). This suggests that the best system outperforms the rest by a margin that cannot be compensated with system combination, at least not with a crude majority vote. We see a similar pattern among the ECNU systems, where the ECNU-SR-7 combination system is outperformed by ECNU-SR-5, presumably since it incorporates the inferior ECNU-SR-1 system.</p>
<p>Relation-specific Analysis. We also analyze the performance on individual relations, especially the extremes. There are very stable patterns across all systems. The best relation (presumably the easiest to classify) is CE, far ahead of ED and MC. Notably, the performance for the best relation is $75 \%$ or above for almost all systems, with comparatively small differences between the systems. The hardest relation is generally IA, followed by PP. ${ }^{5}$ Here, the spread among the systems is much larger: the highest-ranking systems outperform others on the difficult relations. Recall was the main problem for both IA and PP: many examples of these two relations are misclassified, most frequently as OTHER. Even at TD4, these datasets seem to be less homogeneous than the others. Intriguingly, PP shows a very high inter-annotator agreement (Table 1). Its difficulty may therefore be due not to questionable annotation, but to genuine variability, or at least the selection of difficult patterns by the dataset creator. Conversely, MC, among the easiest relations to model, shows only a modest IAA.</p>
<p>Difficult Instances. There were 152 examples that are classified incorrectly by all systems. We analyze them, looking for sources of errors. In addition to a handful of annotation errors and some borderline cases, they are made up of instances which illustrate the limits of current shallow modeling approaches in that they require more lexical knowledge and complex reasoning. A case in point: The bottle carrier converts your $<e 1>$ bottle $&lt;/ e 1&gt;$ into $a<e 2>$ canteen $&lt;/ e 2&gt;$. This instance of OTHER is misclassified either as CC (due to the</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>nominals) or as ED (because of the preposition into). Another example: $[\ldots]<e 1>$ Rudders $&lt;/ e 1&gt;$ are used by $<e 2>$ towboats $&lt;/ e 2&gt;$ and other vessels that require a high degree of manoeuvrability. This is an instance of CW misclassified as IA, probably on account of the verb use which is a frequent indicator of an agentive relation.</p>
<h2>5 Discussion and Conclusion</h2>
<p>There is little doubt that 19-way classification is a non-trivial challenge. It is even harder when the domain is lexical semantics, with its idiosyncrasies, and when the classes are not necessarily disjoint, despite our best intentions. It speaks to the success of the exercise that the participating systems' performance was generally high, well over an order of magnitude above random guessing. This may be due to the impressive array of tools and lexicalsemantic resources deployed by the participants.</p>
<p>Section 4 suggests a few ways of interpreting and analyzing the results. Long-term lessons will undoubtedly emerge from the workshop discussion. One optimistic-pessimistic conclusion concerns the size of the training data. The notable gain TD3 $\rightarrow$ TD4 suggests that even more data would be helpful, but that is so much easier said than done: it took the organizers well in excess of 1000 person-hours to pin down the problem, hone the guidelines and relation definitions, construct sufficient amounts of trustworthy training data, and run the task.</p>
<h2>References</h2>
<p>X. Carreras and L. Màrquez. 2004. Introduction to the CoNLL-2004 shared task: Semantic role labeling. In Proc. CoNLL-04, Boston, MA.
I. Dagan, B. Dolan, B. Magnini, and D. Roth. 2009. Recognizing textual entailment: Rational, evaluation and approaches. Natural Language Engineering, 15(4):i-xvii.
R. Girju, P. Nakov, V. Nastase, S. Szpakowicz, P. Turney, and D. Yuret. 2009. Classification of semantic relations between nominals. Language Resources and Evaluation, 43(2):105-121.
I. Hendrickx, S. Kim, Z. Kozareva, P. Nakov, D. Ó Séaghdha, S. Padó, M. Pennacchiotti, L. Romano, and S. Szpakowicz. 2009. SemEval-2010 Task 8: Multi-way classification of semantic relations between pairs of nominals. In Proc. NAACL Workshop on Semantic Evaluations, Boulder, CO.
J. Kim, T. Ohta, S. Pyysalo, Y. Kano, and J. Tsujii. 2009. Overview of BioNLP'09 shared task on event extraction. In Proc. BioNLP-09, Boulder, CO.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{5}$ The relation OTHER, which we ignore in the overall $F_{1}$ score, does even worse, often below $40 \%$. This is to be expected, since the OTHER examples in our datasets are near misses for other relations, thus making a very incoherent class.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
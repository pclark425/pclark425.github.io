<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-269 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-269</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-269</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-14.html">extraction-schema-14</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <p><strong>Paper ID:</strong> paper-252815431</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2210.05075v1.pdf" target="_blank">Reflection of Thought: Inversely Eliciting Numerical Reasoning in Language Models via Solving Linear Systems</a></p>
                <p><strong>Paper Abstract:</strong> Numerical reasoning over natural language has been a long-standing goal for the research community. However, cutting-edge language models have proven difficult to reliably generalize to a broad range of numbers, although they have shown proficiency in reasoning over common and simple numbers. In this paper, we propose a novel method to elicit and exploit the numerical reasoning knowledge hidden in pre-trained language models using simple anchor numbers. Concretely, we first leverage simple numbers as anchors to probe the implicitly inferred arithmetic expressions from language models, and then explicitly apply the expressions on complex numbers to get corresponding answers. To inversely elicit arithmetic expressions, we transform and formulate the task as an analytically solvable linear system. Experimental results on several numerical reasoning benchmarks demonstrate that our approach significantly improves numerical reasoning capabilities of existing LMs. More importantly, our approach is training-free and simply works in the inference phase, making it highly portable and achieving consistent performance benefits across a variety of language models (GPT-3, T5, BART, etc) in all zero-shot, few-shot, and fine-tuning scenarios.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e269.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e269.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SOLIS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Solving Linear Systems (SOLIS) for arithmetic expression inversion</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A training-free, inference-time method that elicits latent numerical reasoning in pretrained LMs by substituting complex operands with simple anchor numbers, collecting model outputs, and recovering the implicit arithmetic expression by formulating and solving a linear system over polynomial bases.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>method (applies to many LMs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>addition, subtraction, multiplication, division, multi-step arithmetic composed of up to 3 compositions (≤4 operands), plus constant terms (e.g., percent calculations)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>targets complex numbers (integers <10,000 and floating point precision <4 in MathExp experiments) but substitutes anchors as small integers in [1,20]; supports up to 4 operands in the formulation</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>anchor number substitution (1–20), operand proposal via perturbation sensitivity, derive expressions by transforming y=f(x) into polynomial-basis linear system Pa=0, then solve using analytical, exhaustive search, or heuristic (simulated annealing) solvers; inference-time only, no additional training</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Not a model — when applied to backbone LMs (e.g., BART, T5, POET-SQL, GPT-3) SOLIS yields consistent improvements; example: on DROP numeric subset BART total F1 improved from 66.4% → 75.2% (search-based, +8.8) and hard-case F1 from 30.4% → 64.8% (+30.4); T5 total F1 improved ~64.6% → 73.5% (+8.9); POET-SQL total F1 improved 78.4% → 81.4% (+3.0); GPT-3 on AddSub (zero-shot chain) improved 66.6% → 89.4% (+22.8) and MultiArith 63.8% → 80.0% (+16.2).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>SOLIS operationalizes the observation that pretrained LMs are far more reliable on simple/frequent numbers: by probing models with anchor numbers, one can elicit the implicit arithmetic mapping the model uses (represented as polynomial bases including x_i, products, and y terms) and then apply that inferred expression to the original complex numbers; the paper provides an analytic linearization (multiply denominators then form polynomial basis) rather than low-level neural mechanism claims.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>SOLIS improvements are consistent across model sizes and usage modes (zero-shot, few-shot, fine-tuning); analytic method theoretically complete but sensitive to noisy anchor answers, search-based is more robust but scales poorly with operand count; performance gain increases with size of anchor-group sampling and drops if anchor answers are noisy or number of operands grows beyond design (≥5 operands becomes impractical).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Cannot recover expressions outside the polynomial/compositional operator space (e.g., max/min), struggles when backbone LM cannot comprehend numeracy relations or when anchors produce noisy answers; search complexity explodes with many operands or wide/unbounded constants.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared backbone LMs' raw predictions vs. predictions with SOLIS; compared three solving algorithms (analytical, search-based, heuristic) and prompting regimes (zero-shot, few-shot, chain-of-thought); tested different anchor group sizes and anchor ranges.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>A simple inference-time procedure that leverages LMs' reliability on small/frequent numbers can extract (via linearization) the arithmetic expression a model implicitly applies, and applying that expression to the original complex numbers substantially improves numeric-answer accuracy across diverse LMs and prompting settings.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e269.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e269.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3 (Code-Davinci-002 in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large decoder-only language model used as a backbone to evaluate arithmetic reasoning; evaluated in few-shot and chain-of-thought prompting settings and combined with SOLIS to invert arithmetic expressions from anchor substitutions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>REFLECTION OF THOUGHT: INVERSELY ELICITING NUMERICAL REASONING IN LANGUAGE MODELS VIA SOLVING LINEAR SYSTEMS</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (Code-Davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>addition, subtraction, multiplication, division, multi-step arithmetic (composition up to 3 compositions / up to 4 operands) as tested in MathExp, AddSub, MultiArith</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>MathExp: integers <10,000 and floating precision <4; AddSub/MultiArith include typical word-problem numbers; anchors substituted from 1–20</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>few-shot prompting with 8–20 examples, chain-of-thought prompting, zero-shot chain-of-thought; combined with SOLIS (anchor substitution + linear-system inversion) and with single substitution for some API experiments</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Baseline few-shot/chain: e.g., Zero-shot Chain: AddSub 66.6%, MultiArith 63.8%; Chain few-shot: AddSub 88.4%, MultiArith 96.7%. With SOLIS: Zero-shot Chain improved to AddSub 89.4% (+22.8) and MultiArith 80.0% (+16.2); Chain improved to AddSub 90.9% (+2.5) and MultiArith 98.7% (+2.0). On DROP numeric subset (single-substitution experiment) baseline total F1 ~64.7% → w. SOLIS search 68.7% (+4.0); hard-case F1 improved from ~42.5% → 59.9% (+17.4) in their GPT-3 experiment.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Paper attributes GPT-3's relative success on anchors to memorization/frequent-number effects from pretraining; no low-level neural mechanism (e.g., attention head) is claimed — instead, GPT-3's implicit arithmetic mapping can be elicited by varying inputs and solved analytically as a polynomial linear system.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Performance degrades as integer range and floating-point precision increase and as number of operands increases; chain-of-thought prompting boosts baseline performance, and SOLIS provides larger relative gains in weaker prompting regimes (e.g., zero-shot) and for harder problems.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Unreliable end-to-end calculations on rare/large/complex numbers and on multi-operand expressions when anchors/LM outputs are noisy; specific error types observed across models (carry errors, missing high digits, extra digits, insufficient precision) also apply to GPT-3.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared standard prompting vs. chain-of-thought vs. zero-shot chain-of-thought, and with vs without SOLIS; reported gains largest in zero-shot/weak regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>GPT-3's arithmetic outputs are brittle for complex numbers but can be substantially improved (often dramatically in zero-shot settings) by SOLIS which recovers the implicit arithmetic relationship via anchors and applies it to original numbers.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e269.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e269.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BART</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BART (fine-tuned backbone in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An encoder-decoder transformer fine-tuned on DROP and used as a vanilla LM backbone; combined with SOLIS to improve numeric answer accuracy, especially on hard examples with large numeric answers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>REFLECTION OF THOUGHT: INVERSELY ELICITING NUMERICAL REASONING IN LANGUAGE MODELS VIA SOLVING LINEAR SYSTEMS</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BART (large; re-implemented)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>≈350M</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>encoder-decoder transformer</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>addition, subtraction, multiplication, division, multi-step arithmetic (up to 4 operands) as appearing in DROP numeric subset</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>DROP numeric subset, 'hard' cases defined as answers >1000; MathExp experiments across integers <10,000 and floats with precision <4</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>fine-tuning on DROP; SOLIS applied at inference with analytical, search-based, and heuristic solvers; number representation: character-level and reverse decoding used for numeric outputs</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Baseline on DROP numeric subset: total F1 66.4%, hard F1 30.4%. With SOLIS (search-based) total F1 75.2% (+8.8) and hard-case F1 64.8% (+30.4). Analytical and heuristic variants also improved performance, but search-based achieved best results in this setting.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>BART shows much higher accuracy on simple/frequent numbers; SOLIS exploits that by probing with anchors and inferring the arithmetic expression the model effectively applies; no claims about internal representational mechanisms (e.g., attention heads) are made.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Large relative gains on hard cases; search-based solver robust to noisy anchor outputs but analytic solver requires many correct anchor answers; scaling suffers when operand count increases or expressions include constants outside the searched set.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Severe degradation on complex/high-precision numbers and multi-operand expressions without SOLIS; common error patterns include carry errors and digit-level mistakes; search-based SOLIS can still fail when anchor answers are noisy or when expression type (e.g., max) is unsupported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared plain fine-tuned BART vs. BART + SOLIS (analytical/search/heuristic); also compared design choices like anchor group sizes and anchor ranges.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Applying SOLIS to a fine-tuned encoder-decoder LM like BART dramatically improves numeric reasoning on hard cases by recovering and reusing the model's implicit arithmetic expression elicited from simple anchors.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e269.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e269.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>T5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>T5 (LARGE re-implemented backbone)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An encoder-decoder transformer (T5 LARGE used) fine-tuned on DROP and evaluated with SOLIS; SOLIS yields substantial improvements in numeric-answer F1.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>REFLECTION OF THOUGHT: INVERSELY ELICITING NUMERICAL REASONING IN LANGUAGE MODELS VIA SOLVING LINEAR SYSTEMS</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5 (large, re-implemented)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>≈350M</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>encoder-decoder transformer</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>addition, subtraction, multiplication, division, multi-step arithmetic up to 4 operands (as in DROP numeric subset and MathExp)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>DROP numeric subset; MathExp constraints: integer range <10,000, floating precision <4; anchors in [1,20]</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>fine-tuning on DROP, character-level number representation and reverse decoding; SOLIS applied at inference with search-based solver as default</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Baseline T5 total F1 64.6%; with SOLIS total F1 improved to 73.5% (+8.9) and similar improvements in EM reported; SOLIS leads to consistent gains across answer types.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>T5 better handles simple anchor numbers due to pretraining frequency; SOLIS recovers the expression mapping on anchors and applies it to original numbers — the mechanism relies on analytic linearization rather than explicit internal model interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>SOLIS boosts T5 substantially, particularly on multi-operand/harder numeric cases; performance declines with increasing operand complexity and numeric magnitude if no SOLIS is used.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Same digit-level errors and instability on complex numbers as other models; search-based SOLIS helps but becomes less practical for expressions with many operands or varied constants.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared fine-tuned T5 baseline vs. T5 + SOLIS; tested different solving algorithms and anchor-group sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Even for a fine-tuned encoder-decoder LM, an inference-time anchor-probing + linear-system inversion substantially improves numeric reasoning without extra training.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e269.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e269.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>POET-SQL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>POET-SQL (reasoning-pretrained LM used as backbone)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reasoning-pretrained language model used as a stronger backbone for numeric reasoning; SOLIS still produces gains though smaller than for vanilla LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>REFLECTION OF THOUGHT: INVERSELY ELICITING NUMERICAL REASONING IN LANGUAGE MODELS VIA SOLVING LINEAR SYSTEMS</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>POET-SQL</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>≈350M</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>addition, subtraction, multiplication, division, multi-step arithmetic (same problem space as other backbones)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>DROP numeric subset including 'hard' cases (>1000); MathExp constraints used elsewhere</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>fine-tuned / reasoning-pretrained backbone; SOLIS with analytical/search/heuristic solvers; character-level number representation and reverse decoding used on DROP</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Baseline on DROP numeric subset: total F1 78.4%, hard F1 66.8%. With SOLIS (search) total F1 81.4% (+3.0) and hard F1 76.9% (+10.1). Analytical and heuristic variants gave smaller but positive gains.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Even a reasoning-pretrained LM retains more reliable behavior on simple numbers; SOLIS recovers its implicit arithmetic relation and corrects outputs on complex numbers — the paper does not claim internal mechanistic (neuron-level) explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Smaller absolute gains versus vanilla models (because POET-SQL is already stronger at numeracy), but SOLIS still meaningfully improves hard-case performance; robustness to noisy anchors improved by search-based solver.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Remains vulnerable to noisy anchor answers, unsupported expression types (e.g., max), and exponential search cost for many operands.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared POET-SQL baseline vs. POET-SQL + SOLIS (three solving algorithms); assessed robustness by reducing few-shot demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>SOLIS improves even reasoning-pretrained LMs, particularly on hard numeric cases, by revealing and reusing the model's implicit arithmetic mapping elicited via anchors.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e269.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e269.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TAPEX</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TAPEX (table-reasoning pretrained model used as backbone)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A table-pretrained, reasoning-capable LM used as a backbone on DROP; integrating SOLIS produces additional improvements in numeric reasoning metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>REFLECTION OF THOUGHT: INVERSELY ELICITING NUMERICAL REASONING IN LANGUAGE MODELS VIA SOLVING LINEAR SYSTEMS</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>TAPEX</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>≈350M</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>addition, subtraction, multiplication, division, multi-step arithmetic (as appearing in DROP)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>DROP numeric subset (including multi-span numeric answers and hard cases), MathExp constraints described in paper for synthesized tests</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>reasoning-pretrained backbone evaluated with SOLIS at inference using search/analytical/heuristic inversion; numeric design choices: character-level number representation and reverse decoding</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>TAPEX baseline total F1 reported ~79.3% → with SOLIS 81.6% (+2.3) in their experiments (numbers approximate as reported in results table summaries).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Same high-level insight: pretrained models memorize simple numeric facts and are more stable on small anchors; SOLIS exposes and reuses the model's implicit mapping via linearization rather than altering the model weights.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Smaller absolute gains than for vanilla models but consistent improvements across settings; analytic solver sensitive to noisy anchors while search-based more robust.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Limited by unsupported non-linear operations and by noisy anchor outputs; increased cost for many operands.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared TAPEX baseline vs. TAPEX + SOLIS (different solvers); evaluated design choices such as inclusion of constant 100 for percentage expressions.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Even specialized reasoning-pretrained models benefit modestly from SOLIS, confirming that elicit-and-infer expression approach is broadly applicable.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language models are few-shot learners <em>(Rating: 2)</em></li>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Impact of pretraining term frequencies on few-shot reasoning <em>(Rating: 1)</em></li>
                <li>Solving quantitative reasoning problems with language models <em>(Rating: 1)</em></li>
                <li>Palm: Scaling language modeling with pathways <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-269",
    "paper_id": "paper-252815431",
    "extraction_schema_id": "extraction-schema-14",
    "extracted_data": [
        {
            "name_short": "SOLIS",
            "name_full": "Solving Linear Systems (SOLIS) for arithmetic expression inversion",
            "brief_description": "A training-free, inference-time method that elicits latent numerical reasoning in pretrained LMs by substituting complex operands with simple anchor numbers, collecting model outputs, and recovering the implicit arithmetic expression by formulating and solving a linear system over polynomial bases.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "method (applies to many LMs)",
            "model_size": null,
            "model_architecture": null,
            "arithmetic_operation_type": "addition, subtraction, multiplication, division, multi-step arithmetic composed of up to 3 compositions (≤4 operands), plus constant terms (e.g., percent calculations)",
            "number_range_or_complexity": "targets complex numbers (integers &lt;10,000 and floating point precision &lt;4 in MathExp experiments) but substitutes anchors as small integers in [1,20]; supports up to 4 operands in the formulation",
            "method_or_intervention": "anchor number substitution (1–20), operand proposal via perturbation sensitivity, derive expressions by transforming y=f(x) into polynomial-basis linear system Pa=0, then solve using analytical, exhaustive search, or heuristic (simulated annealing) solvers; inference-time only, no additional training",
            "performance_result": "Not a model — when applied to backbone LMs (e.g., BART, T5, POET-SQL, GPT-3) SOLIS yields consistent improvements; example: on DROP numeric subset BART total F1 improved from 66.4% → 75.2% (search-based, +8.8) and hard-case F1 from 30.4% → 64.8% (+30.4); T5 total F1 improved ~64.6% → 73.5% (+8.9); POET-SQL total F1 improved 78.4% → 81.4% (+3.0); GPT-3 on AddSub (zero-shot chain) improved 66.6% → 89.4% (+22.8) and MultiArith 63.8% → 80.0% (+16.2).",
            "mechanistic_insight": "SOLIS operationalizes the observation that pretrained LMs are far more reliable on simple/frequent numbers: by probing models with anchor numbers, one can elicit the implicit arithmetic mapping the model uses (represented as polynomial bases including x_i, products, and y terms) and then apply that inferred expression to the original complex numbers; the paper provides an analytic linearization (multiply denominators then form polynomial basis) rather than low-level neural mechanism claims.",
            "performance_scaling": "SOLIS improvements are consistent across model sizes and usage modes (zero-shot, few-shot, fine-tuning); analytic method theoretically complete but sensitive to noisy anchor answers, search-based is more robust but scales poorly with operand count; performance gain increases with size of anchor-group sampling and drops if anchor answers are noisy or number of operands grows beyond design (≥5 operands becomes impractical).",
            "failure_modes": "Cannot recover expressions outside the polynomial/compositional operator space (e.g., max/min), struggles when backbone LM cannot comprehend numeracy relations or when anchors produce noisy answers; search complexity explodes with many operands or wide/unbounded constants.",
            "comparison_baseline": "Compared backbone LMs' raw predictions vs. predictions with SOLIS; compared three solving algorithms (analytical, search-based, heuristic) and prompting regimes (zero-shot, few-shot, chain-of-thought); tested different anchor group sizes and anchor ranges.",
            "key_finding": "A simple inference-time procedure that leverages LMs' reliability on small/frequent numbers can extract (via linearization) the arithmetic expression a model implicitly applies, and applying that expression to the original complex numbers substantially improves numeric-answer accuracy across diverse LMs and prompting settings.",
            "uuid": "e269.0"
        },
        {
            "name_short": "GPT-3",
            "name_full": "GPT-3 (Code-Davinci-002 in experiments)",
            "brief_description": "A large decoder-only language model used as a backbone to evaluate arithmetic reasoning; evaluated in few-shot and chain-of-thought prompting settings and combined with SOLIS to invert arithmetic expressions from anchor substitutions.",
            "citation_title": "REFLECTION OF THOUGHT: INVERSELY ELICITING NUMERICAL REASONING IN LANGUAGE MODELS VIA SOLVING LINEAR SYSTEMS",
            "mention_or_use": "use",
            "model_name": "GPT-3 (Code-Davinci-002)",
            "model_size": "175B",
            "model_architecture": "decoder-only transformer",
            "arithmetic_operation_type": "addition, subtraction, multiplication, division, multi-step arithmetic (composition up to 3 compositions / up to 4 operands) as tested in MathExp, AddSub, MultiArith",
            "number_range_or_complexity": "MathExp: integers &lt;10,000 and floating precision &lt;4; AddSub/MultiArith include typical word-problem numbers; anchors substituted from 1–20",
            "method_or_intervention": "few-shot prompting with 8–20 examples, chain-of-thought prompting, zero-shot chain-of-thought; combined with SOLIS (anchor substitution + linear-system inversion) and with single substitution for some API experiments",
            "performance_result": "Baseline few-shot/chain: e.g., Zero-shot Chain: AddSub 66.6%, MultiArith 63.8%; Chain few-shot: AddSub 88.4%, MultiArith 96.7%. With SOLIS: Zero-shot Chain improved to AddSub 89.4% (+22.8) and MultiArith 80.0% (+16.2); Chain improved to AddSub 90.9% (+2.5) and MultiArith 98.7% (+2.0). On DROP numeric subset (single-substitution experiment) baseline total F1 ~64.7% → w. SOLIS search 68.7% (+4.0); hard-case F1 improved from ~42.5% → 59.9% (+17.4) in their GPT-3 experiment.",
            "mechanistic_insight": "Paper attributes GPT-3's relative success on anchors to memorization/frequent-number effects from pretraining; no low-level neural mechanism (e.g., attention head) is claimed — instead, GPT-3's implicit arithmetic mapping can be elicited by varying inputs and solved analytically as a polynomial linear system.",
            "performance_scaling": "Performance degrades as integer range and floating-point precision increase and as number of operands increases; chain-of-thought prompting boosts baseline performance, and SOLIS provides larger relative gains in weaker prompting regimes (e.g., zero-shot) and for harder problems.",
            "failure_modes": "Unreliable end-to-end calculations on rare/large/complex numbers and on multi-operand expressions when anchors/LM outputs are noisy; specific error types observed across models (carry errors, missing high digits, extra digits, insufficient precision) also apply to GPT-3.",
            "comparison_baseline": "Compared standard prompting vs. chain-of-thought vs. zero-shot chain-of-thought, and with vs without SOLIS; reported gains largest in zero-shot/weak regimes.",
            "key_finding": "GPT-3's arithmetic outputs are brittle for complex numbers but can be substantially improved (often dramatically in zero-shot settings) by SOLIS which recovers the implicit arithmetic relationship via anchors and applies it to original numbers.",
            "uuid": "e269.1"
        },
        {
            "name_short": "BART",
            "name_full": "BART (fine-tuned backbone in experiments)",
            "brief_description": "An encoder-decoder transformer fine-tuned on DROP and used as a vanilla LM backbone; combined with SOLIS to improve numeric answer accuracy, especially on hard examples with large numeric answers.",
            "citation_title": "REFLECTION OF THOUGHT: INVERSELY ELICITING NUMERICAL REASONING IN LANGUAGE MODELS VIA SOLVING LINEAR SYSTEMS",
            "mention_or_use": "use",
            "model_name": "BART (large; re-implemented)",
            "model_size": "≈350M",
            "model_architecture": "encoder-decoder transformer",
            "arithmetic_operation_type": "addition, subtraction, multiplication, division, multi-step arithmetic (up to 4 operands) as appearing in DROP numeric subset",
            "number_range_or_complexity": "DROP numeric subset, 'hard' cases defined as answers &gt;1000; MathExp experiments across integers &lt;10,000 and floats with precision &lt;4",
            "method_or_intervention": "fine-tuning on DROP; SOLIS applied at inference with analytical, search-based, and heuristic solvers; number representation: character-level and reverse decoding used for numeric outputs",
            "performance_result": "Baseline on DROP numeric subset: total F1 66.4%, hard F1 30.4%. With SOLIS (search-based) total F1 75.2% (+8.8) and hard-case F1 64.8% (+30.4). Analytical and heuristic variants also improved performance, but search-based achieved best results in this setting.",
            "mechanistic_insight": "BART shows much higher accuracy on simple/frequent numbers; SOLIS exploits that by probing with anchors and inferring the arithmetic expression the model effectively applies; no claims about internal representational mechanisms (e.g., attention heads) are made.",
            "performance_scaling": "Large relative gains on hard cases; search-based solver robust to noisy anchor outputs but analytic solver requires many correct anchor answers; scaling suffers when operand count increases or expressions include constants outside the searched set.",
            "failure_modes": "Severe degradation on complex/high-precision numbers and multi-operand expressions without SOLIS; common error patterns include carry errors and digit-level mistakes; search-based SOLIS can still fail when anchor answers are noisy or when expression type (e.g., max) is unsupported.",
            "comparison_baseline": "Compared plain fine-tuned BART vs. BART + SOLIS (analytical/search/heuristic); also compared design choices like anchor group sizes and anchor ranges.",
            "key_finding": "Applying SOLIS to a fine-tuned encoder-decoder LM like BART dramatically improves numeric reasoning on hard cases by recovering and reusing the model's implicit arithmetic expression elicited from simple anchors.",
            "uuid": "e269.2"
        },
        {
            "name_short": "T5",
            "name_full": "T5 (LARGE re-implemented backbone)",
            "brief_description": "An encoder-decoder transformer (T5 LARGE used) fine-tuned on DROP and evaluated with SOLIS; SOLIS yields substantial improvements in numeric-answer F1.",
            "citation_title": "REFLECTION OF THOUGHT: INVERSELY ELICITING NUMERICAL REASONING IN LANGUAGE MODELS VIA SOLVING LINEAR SYSTEMS",
            "mention_or_use": "use",
            "model_name": "T5 (large, re-implemented)",
            "model_size": "≈350M",
            "model_architecture": "encoder-decoder transformer",
            "arithmetic_operation_type": "addition, subtraction, multiplication, division, multi-step arithmetic up to 4 operands (as in DROP numeric subset and MathExp)",
            "number_range_or_complexity": "DROP numeric subset; MathExp constraints: integer range &lt;10,000, floating precision &lt;4; anchors in [1,20]",
            "method_or_intervention": "fine-tuning on DROP, character-level number representation and reverse decoding; SOLIS applied at inference with search-based solver as default",
            "performance_result": "Baseline T5 total F1 64.6%; with SOLIS total F1 improved to 73.5% (+8.9) and similar improvements in EM reported; SOLIS leads to consistent gains across answer types.",
            "mechanistic_insight": "T5 better handles simple anchor numbers due to pretraining frequency; SOLIS recovers the expression mapping on anchors and applies it to original numbers — the mechanism relies on analytic linearization rather than explicit internal model interpretability.",
            "performance_scaling": "SOLIS boosts T5 substantially, particularly on multi-operand/harder numeric cases; performance declines with increasing operand complexity and numeric magnitude if no SOLIS is used.",
            "failure_modes": "Same digit-level errors and instability on complex numbers as other models; search-based SOLIS helps but becomes less practical for expressions with many operands or varied constants.",
            "comparison_baseline": "Compared fine-tuned T5 baseline vs. T5 + SOLIS; tested different solving algorithms and anchor-group sizes.",
            "key_finding": "Even for a fine-tuned encoder-decoder LM, an inference-time anchor-probing + linear-system inversion substantially improves numeric reasoning without extra training.",
            "uuid": "e269.3"
        },
        {
            "name_short": "POET-SQL",
            "name_full": "POET-SQL (reasoning-pretrained LM used as backbone)",
            "brief_description": "A reasoning-pretrained language model used as a stronger backbone for numeric reasoning; SOLIS still produces gains though smaller than for vanilla LMs.",
            "citation_title": "REFLECTION OF THOUGHT: INVERSELY ELICITING NUMERICAL REASONING IN LANGUAGE MODELS VIA SOLVING LINEAR SYSTEMS",
            "mention_or_use": "use",
            "model_name": "POET-SQL",
            "model_size": "≈350M",
            "model_architecture": null,
            "arithmetic_operation_type": "addition, subtraction, multiplication, division, multi-step arithmetic (same problem space as other backbones)",
            "number_range_or_complexity": "DROP numeric subset including 'hard' cases (&gt;1000); MathExp constraints used elsewhere",
            "method_or_intervention": "fine-tuned / reasoning-pretrained backbone; SOLIS with analytical/search/heuristic solvers; character-level number representation and reverse decoding used on DROP",
            "performance_result": "Baseline on DROP numeric subset: total F1 78.4%, hard F1 66.8%. With SOLIS (search) total F1 81.4% (+3.0) and hard F1 76.9% (+10.1). Analytical and heuristic variants gave smaller but positive gains.",
            "mechanistic_insight": "Even a reasoning-pretrained LM retains more reliable behavior on simple numbers; SOLIS recovers its implicit arithmetic relation and corrects outputs on complex numbers — the paper does not claim internal mechanistic (neuron-level) explanations.",
            "performance_scaling": "Smaller absolute gains versus vanilla models (because POET-SQL is already stronger at numeracy), but SOLIS still meaningfully improves hard-case performance; robustness to noisy anchors improved by search-based solver.",
            "failure_modes": "Remains vulnerable to noisy anchor answers, unsupported expression types (e.g., max), and exponential search cost for many operands.",
            "comparison_baseline": "Compared POET-SQL baseline vs. POET-SQL + SOLIS (three solving algorithms); assessed robustness by reducing few-shot demonstrations.",
            "key_finding": "SOLIS improves even reasoning-pretrained LMs, particularly on hard numeric cases, by revealing and reusing the model's implicit arithmetic mapping elicited via anchors.",
            "uuid": "e269.4"
        },
        {
            "name_short": "TAPEX",
            "name_full": "TAPEX (table-reasoning pretrained model used as backbone)",
            "brief_description": "A table-pretrained, reasoning-capable LM used as a backbone on DROP; integrating SOLIS produces additional improvements in numeric reasoning metrics.",
            "citation_title": "REFLECTION OF THOUGHT: INVERSELY ELICITING NUMERICAL REASONING IN LANGUAGE MODELS VIA SOLVING LINEAR SYSTEMS",
            "mention_or_use": "use",
            "model_name": "TAPEX",
            "model_size": "≈350M",
            "model_architecture": null,
            "arithmetic_operation_type": "addition, subtraction, multiplication, division, multi-step arithmetic (as appearing in DROP)",
            "number_range_or_complexity": "DROP numeric subset (including multi-span numeric answers and hard cases), MathExp constraints described in paper for synthesized tests",
            "method_or_intervention": "reasoning-pretrained backbone evaluated with SOLIS at inference using search/analytical/heuristic inversion; numeric design choices: character-level number representation and reverse decoding",
            "performance_result": "TAPEX baseline total F1 reported ~79.3% → with SOLIS 81.6% (+2.3) in their experiments (numbers approximate as reported in results table summaries).",
            "mechanistic_insight": "Same high-level insight: pretrained models memorize simple numeric facts and are more stable on small anchors; SOLIS exposes and reuses the model's implicit mapping via linearization rather than altering the model weights.",
            "performance_scaling": "Smaller absolute gains than for vanilla models but consistent improvements across settings; analytic solver sensitive to noisy anchors while search-based more robust.",
            "failure_modes": "Limited by unsupported non-linear operations and by noisy anchor outputs; increased cost for many operands.",
            "comparison_baseline": "Compared TAPEX baseline vs. TAPEX + SOLIS (different solvers); evaluated design choices such as inclusion of constant 100 for percentage expressions.",
            "key_finding": "Even specialized reasoning-pretrained models benefit modestly from SOLIS, confirming that elicit-and-infer expression approach is broadly applicable.",
            "uuid": "e269.5"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 2,
            "sanitized_title": "language_models_are_fewshot_learners"
        },
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chain_of_thought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Impact of pretraining term frequencies on few-shot reasoning",
            "rating": 1,
            "sanitized_title": "impact_of_pretraining_term_frequencies_on_fewshot_reasoning"
        },
        {
            "paper_title": "Solving quantitative reasoning problems with language models",
            "rating": 1,
            "sanitized_title": "solving_quantitative_reasoning_problems_with_language_models"
        },
        {
            "paper_title": "Palm: Scaling language modeling with pathways",
            "rating": 1,
            "sanitized_title": "palm_scaling_language_modeling_with_pathways"
        }
    ],
    "cost": 0.01681675,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>REFLECTION OF THOUGHT: INVERSELY ELICITING NUMERICAL REASONING IN LANGUAGE MODELS VIA SOLVING LINEAR SYSTEMS</p>
<p>Fan Zhou 
Shanghai Jiao Tong University</p>
<p>Haoyu Dong 
Microsoft Research Asia</p>
<p>Qian Liu liuqian@sea.com 
Sea AI Lab</p>
<p>Zhoujun Cheng 
Shanghai Jiao Tong University</p>
<p>Shi Han 
Microsoft Research Asia</p>
<p>Dongmei Zhang dongmeiz@microsoft.com 
Microsoft Research Asia</p>
<p>REFLECTION OF THOUGHT: INVERSELY ELICITING NUMERICAL REASONING IN LANGUAGE MODELS VIA SOLVING LINEAR SYSTEMS
Preprint
Numerical reasoning over natural language has been a long-standing goal for the research community. However, cutting-edge language models have proven difficult to reliably generalize to a broad range of numbers, although they have shown proficiency in reasoning over common and simple numbers. In this paper, we propose a novel method to elicit and exploit the numerical reasoning knowledge hidden in pre-trained language models using simple anchor numbers. Concretely, we first leverage simple numbers as anchors to probe the implicitly inferred arithmetic expressions from language models, and then explicitly apply the expressions on complex numbers to get corresponding answers. To inversely elicit arithmetic expressions, we transform and formulate the task as an analytically solvable linear system. Experimental results on several numerical reasoning benchmarks demonstrate that our approach significantly improves numerical reasoning capabilities of existing LMs. More importantly, our approach is training-free and simply works in the inference phase, making it highly portable and achieving consistent performance benefits across a variety of language models (GPT-3, T5, BART, etc) in all zero-shot, few-shot, and fine-tuning scenarios.Fortunately, by reverse thinking, we have a positive point of view: with the exact same context, LMs are significantly more accurate and stable on simple numbers -typically small integers that appear frequently in the pre-training corpora -than complex numbers, indicating that LMs have a strong aptitude for bearing arithmetic results of simple numbers in mind during pre-training. This motivates us to leverage simple numbers as "anchors" to probe the implicitly inferred arithmetic expressions from language models and then explicitly apply the expressions on complex numbers.</p>
<p>INTRODUCTION</p>
<p>Language Models (LMs) have demonstrated great success on a wide range of natural language tasks (Devlin et al., 2018;Brown et al., 2020b;Chowdhery et al., 2022), and recent works even explore to use LMs as a general-purpose interface for diverse modalities (Hao et al., 2022;He et al., 2022). But when it comes to reasoning about numbers, the crucial parts of text, tables, and knowledge bases, the performance of LMs slumps. Even rational numbers, a small subset of real numbers, readily constitute an infinite space that cannot be completely covered by pre-training corpora, hence posing a significant obstacle to LMs. Recent works have shown strong context understanding capabilities of LMs in numerical reasoning datasets (Dua et al., 2019;Cobbe et al., 2021), but LMs are still far from being robust on end-to-end numerical calculation: as numbers get larger and more complicated, the likelihood of failure for LMs increases, e.g., it is difficult for them to calculate the result of 8, 534.5 + 17.85; even for number additions between small numbers, e.g., 512 + 128 and 513 + 129, LMs are not stable enough to consistently produce the correct result. Similar observations are also reported by Razeghi et al. (2022), showing that LMs struggle to conduct end-to-end calculations on numbers that rarely appear in pre-training corpora. Figure 1: The illustration of our proposed framework, which elicits numerical reasoning in language models via Solving Linear Systems (SOLIS). challenging for LMs, to first replace them by anchor numbers (e.g., 10, 7) and use LMs to output answers (e.g., 3) that are more much accurate than complex numbers, then inversely elicit the hidden arithmetic relationship (e.g., x 1 − x 2 ) implicitly inferred by LMs through these anchor inputs and outputs, and finally explicitly applying the arithmetic relationship on the original complex numbers (10, 477 − 7, 459) to produce the answer (3,018). In this way, our method combines the advances of LMs on understanding complex context and memorizing simple numbers towards reliable numerical reasoning.</p>
<p>This paper introduces reflection of thought, a new idea of eliciting the numerical reasoning knowledge hidden in pre-trained LMs through probing with simple anchor numbers. Reflection of thought, in principle, allows models to unveil the underlying reasoning process by varying inputs at test time, so it does not need any extra training or labeled data. To inversely elicit arithmetic relationships in LMs through anchor numbers, we propose SOLIS, a novel method to transform and formulate this problem to a linear system that can be directly solved in an analytic way. To promote robustness to mistakes introduced by LMs, search-based and heuristic-based methods are further devised. Experimental results on several representative numerical reasoning datasets demonstrate that SOLIS significantly advances cutting-edge LMs. Importantly, our framework simply works in the inference phase without extra training or labeled data, making it highly portable to different kinds of LMs and achieving consistent gains over various LMs in zero-shot, few-shot and fine-tuning scenarios.</p>
<p>PRELIMINARY STUDY</p>
<p>In this section, we will first demonstrate the brittleness of language models' ability on arithmeticallyrelated tasks. Unlike arithmetic benchmarks such as AddSub or MultiArith (Roy &amp; Roth, 2015) which contain natural language context for each sample, we directly generate and feed the arithmetic expressions and test the performance on language models. This is done to reduce potential perturbing factors and highlight the models' calculating ability. We impose constraints on the complexity of the expressions: we only study the four fundamental operations, and demand no more than 4 operands, where each operand's integer range is less then 10, 000 and floating point precision is less than 4. To conduct a systematic investigation, we first produce F which represents the set of all the expressions satisfying our constraints. We randomly sample numbers within the limits of range and precision as the operands. For one expression f ∈ F with a specified range and precision, we randomly generate 50 samples. We evaluate the language model on these samples and denote this synthesized task as MathExp which stands for Math Expressions.</p>
<p>We sample a maximum of 50 expressions for each different settings of complexity, and test these samples using large scale language model GPT-3 (Brown et al., 2020a). We conduct the study on GPT-3 in a few-shot manner: to unleash its potential, we pre-pend 10 to 20 expressions (having the same f , integer range, and floating point precision as the tested sample) together with the answers as the prompt. We then call the OpenAI API 1 to get all the predictions, and evaluate the performance accordingly. Figure 2 indicate that even the latest powerful GPT-3(Code-Davinci-002) fails to achieve a satisfactory performance: (i) the prediction accuracy decreases largely as the number gets more complex, i.e., integer range or floating point precision of operands increases; (ii) the prediction accuracy also drops dramatically as the arithmetic relationship getting more complex, i.e., number of operands increases. In Appendix A, we also present the performance with our SOLIS framework, which is more robust to influence of floating point precision and integer range.</p>
<p>Results in</p>
<p>3 NUMERICAL REASONING VIA SOLVING LINEAR SYSTEMS The preliminary study demonstrates that the current language models are vulnerable to complex numbers. For example, they have no chance to guess the answer to the sum of two floating point numbers with three decimal places. However, the language model can perform reliably well when the operands are simple, i.e., relatively small integers. Such observations motivate us to simplify the numbers before feeding them into language models, thus enabling reliable neural-based numerical reasoning. In this section, we first provide an overview of our framework SOLIS, and then we elaborate on each part of our framework in detail.</p>
<p>METHOD OVERVIEW</p>
<p>As mentioned above, our method can be integrated into language models in a plug-and-play manner at test time. For the sake of clarification, in the following we refer to LMs that can steadily perform numerical reasoning as reasoning LMs. They can be either LMs obtained by fine-tuning on datasets involving numerical reasoning, or LMs that perform numerical reasoning via in-context learning.</p>
<p>As shown in Figure 1, our method generally involves three stages: (1) Operand Proposal: given a paragraph, we first identify the numbers which are necessary for the reasoning LM to perform numerical reasoning (e.g., 10, 477); (2) Number Substitution: these proposed operands 2 are generally complex for language models, and thus they need to be substituted with randomly chosen simple numbers (e.g., 10) to make the model input simpler. Using the reasoning LM, we can obtain a set of predicted answers with respect to each substituted paragraph after several substitutions.</p>
<p>(3) Arithmetic Relationship Inversion: using these paragraphs and their answers as observed data, we can inversely derive the internal reasoning flow from the reasoning LM, i.e. the arithmetic expression between the operands (e.g., y = x 1 − x 2 ). By applying the expression on the original numbers, the answer to the original paragraph can be obtained.</p>
<p>OPERAND PROPOSAL</p>
<p>There are often many numbers involved in a paragraph, and it is quite challenging to model the arithmetic relationships among all these numbers simultaneously. Consequently, it is important during the operand proposal step to trim the prospective operands to a manageable size. A straightforward strategy would be to select only the numbers pertinent to the answer as candidate operands, which is not trivial in practice since there is no intermediate supervision on the relevance between each number and the answer.</p>
<p>To address the issue, we provide a novel technique that employs number perturbation and the reasoning LM to measure the relevance systematically. It is largely inspired by prior works that leverage an image classifier to quantify the relevance of pixels with image categories (Samek et al., 2017) and its application on natural language tasks (Liu et al., 2021). In their works, relevance is assessed by the degradation of the classifier score after erasing each pixel, where a substantial degradation indicates a strong relevance. Similarly, we consider a number to be essential to the final answer if there is a difference between the model predictions before and after perturbing it. Regarding perturbations, we implement it by adding a small adjustment to each number in the paragraph (e.g., 98.5 → 98.6) and evaluate whether the model prediction changes correspondingly. Despite the fact that the reasoning LM hardly perform accurate calculations over numbers, we observe that LMs have strong context understanding capabilities about numbers and are sensitive to slight changes in the numbers used to forecast answer. More details about the operand proposal mechanism can be found in Appendix B.</p>
<p>NUMBER SUBSTITUTION</p>
<p>After the operand proposal stage, a random set of numbers is generated to substitute the proposed operands sequentially. These numbers are referred to as anchor numbers below. Each anchor number is an integer between 1 and 20, a range that we believe reasoning LMs can easily handle. Meanwhile, to minimize the effects of number substitution, we strive to maintain the order relationships among the numbers. Taking the example from Figure 1, we make the substitution number corresponding to 10, 477 larger than the one corresponding to 7, 459 since 10, 477 is larger than 7, 459.</p>
<p>Notably, the random number substitution must be repeated several times (e.g., three times in Figure 1) to obtain a group of anchor numbers. Along with the original question, each of these paragraphs is fed into the reasoning LM to predict the answer, which we call the anchor answer. Typically, the number of anchor answers must exceed the number of operands for the subsequent arithmetic relationship inversion stage to be feasible.</p>
<p>ARITHMETIC RELATIONSHIP INVERSION</p>
<p>Given a collection of anchor numbers and anchor answers, the arithmetic relationship inversion stage investigates the relationship between these numbers and induces an expression to reflect it. Taking the example from Figure 1, a typical expression can be y = x 1 − x 2 , where x 1 and x 2 are both anchor numbers while y is the anchor answer.</p>
<p>Although the example expression appears intuitive, deriving such an expression from data points is tremendously difficult because the solution space is theoretically infinite. To make it practicable, as a first step, we begin by limiting the problem-solving space to compositions of binary operators, where each operator can be addition, subtraction, multiplication or division, the four most prevalent operators in numerical reasoning (Dua et al., 2019). Meanwhile, there can be up to three compositions, which means the expression contains a maximum of four operands. With such priors, the insoluble expression induction problem can be turned into a linear system solving problem, where the anchor numbers, the anchor answer, and their compositions constitute a linear system. In this way, the problem of expression induction can be tackled by the solving algorithms for linear systems, which will be elaborated in Section 4. Finally, the answer can be reached in a trustworthy and interpretable manner by applying the derived expression to the original numbers.</p>
<p>SOLVING ALGORITHM</p>
<p>In this section, we introduce three algorithms that can derive expressions merely from anchor numbers and anchor answers, namely analytical-based, search-based and heuristic-based algorithm.</p>
<p>FORMULATION</p>
<p>Formally, given a paragraph and a question, we denote a group of anchor numbers as x = (x 1 , x 2 , . . . , x n ) and the arithmetic relationship as an expression f , which should produce the answer y by y = f (x). The goal is to recover f from different groups of anchor numbers X and corresponding anchor answers y. We propose to transform and formulate the arithmetic relationship inversion as solving a system of linear equations. Given expression f (x) with four fundamental arithmetic operations, we transform the equation y = f (x) by multiplying denominators on both sides when operator division exists, then we get:
a 0 · C + a 1 · x 1 + a 2 · x 2 + a 3 · y + a 4 · x 1 x 2 + . . . + a k · (x 1 x 2 . . . x n y) = 0(1)
For example, y = 1 − x 1 /x 2 can be transformed to x 2 − x 1 − x 2 y = 0. Then uncovering f (x) is equivalent to solving a = (a 0 , a 1 , . . . , a k ), which are coefficients of all possible polynomial basis combined by x 1 , , x n and y, denoted as p, where k = 2 n+1 − 1. Multiple groups of anchors X and y constitute multiple groups of values of polynomial basis, denoted as P, then Equation 1 can be denoted as Pa = 0, which is a typical set of linear equations.</p>
<p>ANALYTICAL-BASED ALGORITHM</p>
<p>To solve Pa = b, we can simply generate k+1 groups of anchor numbers as X and LMs' answers as y, compute P based on X and y, and finally get a = (P) −1 b when P is in full rank. But notice that y can be a linear weighted summation of x 0 , . . . , x n by itself, the coefficient matrix P may not be full-ranked. To address this, we generate k groups of anchor numbers and add an additional constraint by setting |a| = k i=0 a i = 1. So we augment P with an all-one vector to P * and finally get a = (P * ) −1 b, where b = (0, 0, . . . , 0, 1). In practice, randomly sampled groups of anchor numbers can form a full-ranked P * with a very high probability, and one can even add a buffer by sampling a bit more groups of anchor numbers than k to constitute different P * s for cross validation.</p>
<p>The analytic method is theoretically complete to deduce arithmetic expressions in our pre-defined problem space. But in practice, LMs may produce incorrect results even for anchor numbers, especially when given a complex expression, so as to violate the analytic method which needs purely correct anchor answers. To best tolerate them, we then propose search-based and heuristic-based methods to better solve a noisy linear system. Gladly, the analytic method theoretical supports other methods in aspects such as guiding the number of anchors to sample to ensure a unique expression.</p>
<p>SEARCH-BASED ALGORITHM</p>
<p>The search-based algorithm exhaustively explores the search space and finds out the most preferable arithmetic expression in the space. We constrain the search space of a in Equation 1 by: requiring a 1−n ∈ {−1, 0, 1} for all coefficients of the non-constant terms, and for coefficient a 0 of constant term C, one can restrict the search range to a pre-defined set, e.g., a 0 ∈ {−100, −1, 0, 1, 100} in our experiments for efficiency, and different from the analytic method that can easily solve constants in expressions. Constraints here mean that we only let this search algorithm cover f (x) with no more than one constant for efficiency. We then transform all searched polynomial-basis-based equations backwards into expressions because they have one-to-one mappings, e.g., from x 2 − x 1 − x 2 y = 0 to y = 1 − x 1 /x 2 . We denote the space of expressions as F, and for each f i ∈ F and each group of anchor numbers X j (using m to denote the number of groups), we get y ij by applying f i to X j .</p>
<p>Algorithm 1 SEARCH</p>
<p>Input: parameters X,ŷ, F, c threshold Output: Most preferable expression f 1: while j &lt; m do 2:
for f i ∈ F do 3: y * ij ← f i (X j ) 4: c i ← c i + 1(y * j ==ŷ ij ) 5: i ← i + |y * j −ŷ ij | 6: end for 7: j ← j + 1 8: end while 9: i * c ← arg max c, i * ← arg min 10: if c i * ≥ c threshold then f ← f i * c 11: else f ← f i * 12: end if
We define the prediction error between the target expressionf and f i as (f , f i ), which is calculated by (f , f i ) = j ij = j abs(ŷ j −y ij ), and the number of occurrence of exact matching as c i . We then find the most preferable expression with the minimum prediction error and the maximum number of exact matching. Specifically, when the number of exact matching exceeds a pre-defined c threshold , we pick the expression f i with the highest c i ; otherwise, we pick the expression f i with the lowest i . The search process is sketched in Algorithm 1.</p>
<p>This method is robust for probably incorrect predictions, i.e., when model does not have sufficient number of exact matching, it is still capable to return the most nearest expression by selecting the one with the minimum error. However, the search-based method can be challenged by exponentially explosive search space when the number of operands surges, and it's not efficient to search constant numbers that has a wide and even infinite range, neither.</p>
<p>HEURISTIC-BASED ALGORITHM</p>
<p>In this section, we introduce a heuristic-based algorithm, simulated annealing, which is efficient and does not need to search for the whole problem space, though it may produce sub-optimal results given a limited number of exploration steps. We follow the formulation introduced in Section 4.1 and proposed a optimization target L H to measure the L1 loss of P a. The pipeline includes: (1) randomly initialize a with values {-1, 0, 1} and calculate initial L H ; (2) randomly select i from 0 to k and perturb a i by adding or subtracting a constant number (we use 1 here); (3) calculate new L H , and adopt the perturbation with a large probability if L H decreases and with a low probability if it increases, balanced by a pre-defined temperature T , which decreases over steps; (4) return a if the number of steps is enough or L H equals to zero, otherwise repeat from step 1. Note that, we restrict coefficients in a to be integers for simplicity, so different from the analytical method restricting k i=0 a i = 1, we ensure only one of the coefficients of y-related polynomial basis {y, x 1 y, . . . ,</p>
<p>x 1 x 2 . . . x n y} to be non-zero (with a static value 1) and at least two coefficients in a are non-zero during the whole initialization and perturbation process to avoid some infeasible local optimal. In summary, Table 1 shows the strong and weak points of these algorithms. In the problem space introduced in Section 3.4 within at most four operands, the search-based method does not have scalability issues, so it achieves best performance in our experiments because it's robust to LMs' predictions and can retrieve optimal expression through exhaustive search except rare constants.</p>
<p>EXPERIMENTS</p>
<p>In this section, we integrate SOLIS with various language models as backbones and evaluate the effectiveness of SOLIS on two well-known numerical reasoning benchmarks.</p>
<p>EXPERIMENTAL SETUP</p>
<p>Datasets We perform experiments on DROP (Dua et al., 2019), AddSub and MultiArith, of which the latter two are widely used subsets from MAWPS (Roy &amp; Roth, 2015). DROP is a reading comprehension benchmark that focuses on numerical reasoning and has a variety of answer types, including span, number, and date. The experimental results of DROP are evaluated with the official evaluation metrics Exact Match (EM) and F1. As for MAWPS, it consists of math word problems which also require numerical reasoning ability. The subset AddSub features relatively easier numerical reasoning, whereas MultiArith necessitates multi-step numerical calculations. The EM metric is used to evaluate the results of AddSub and MultiArith. More details can be found in Appendix C.</p>
<p>Backbone and Baselines On DROP, we adopt two kinds of LMs as our backbones, including (i) Vanilla LMs: BART (Lewis et al., 2020) and T5 (Raffel et al., 2020), (ii) Reasoning LMs: TAPEX  and POET (Pi et al., 2022).</p>
<p>We also compare the performance of our method with previous specialized models designed for the DROP dataset, such as NumNet (Ran et al., 2019), NeRd (Chen et al., 2020b), MTMSN (Hu et al., 2019) and QDGAT . All models are fine-tuned on the DROP train set, and the best validation set performance is reported. If no explicit declarations are included, all LMs here are of large size and contain approximately 350M of parameters. On AddSub and MultiArith, we adopt GPT-3 Code-Davinci-002 (GPT-3) (Brown et al., 2020a) with different prompting techniques as our backbones: Chain-of-Thought Prompting (Chain) (Wei et al., 2022b) and the Zero-shot Chain-of-Thought Prompting (Zero-shot Chain) (Kojima et al., 2022). We compare our results to the PaLM model (Chowdhery et al., 2022). Unless otherwise specified, these models perform numerical reasoning by zero-shot/few-shot in-context learning, and the few-shot demonstrations are the 8 samples released by Wei et al. (2022b).</p>
<p>IMPLEMENTATION DETAILS</p>
<p>Hyperparameter Selection For fine-tuning, we apply Adam (Loshchilov &amp; Hutter, 2019) optimizer. The fine-tuning epochs are set as 50. For BART models (i.e., BART and POET-SQL), we follow previous works (Pi et al., 2022) to set the batch size as 128 and the learning rate as 3 × 10 −5 . For T5, we decrease the batch size to 32 due to the computational budget. The early stop technique is used to save training time. For GPT-3 API, we keep the temperature as default setting 0, and set the maximum output tokens to 128. As for anchor number groups: the group size is 6/8/10 corresponding to corresponding to 2/3/4 operands on DROP; the group size is 4 on AddSub, and   (Lewis et al., 2020) 67.4 70.6 w. SOLIS 72.9 (+5.5) 76.1 (+5.5) T5 (Raffel et al., 2020) 61.0 64.6 w. SOLIS 69.9 (+8.9) 73.5 (+8.9) Reasoning LMs TAPEX  76.3 79.3 w. SOLIS 78.5 (+2.2) 81.6 (+2.3) POET-SQL (Pi et al., 2022) 76 Design Choices on DROP Following previous work, we apply two general-purpose numerical designs on the DROP dataset. First, we employ the character-level rather than subword-level number representation, which proves to be more effective (Wallace et al., 2019;Pi et al., 2022). Second, we employ the reverse decoding technique, which proves to be a successful design to mimic arithmetic carry (Geva et al., 2020). Meanwhile, as mentioned above, the search-based algorithm has difficulties in covering expressions including constants. Considering the constant 100 is frequently used for percentage calculations (e.g., "How many percent of the national population does not live in Bangkok?"), we add it to be one candidate in DROP.</p>
<p>EXPERIMENTAL RESULTS</p>
<p>Since our work focuses on addressing arithmetic problems, we first evaluate suggested solving algorithms via their performance on the DROP subset whose answers are numbers (i.e., numeric subset). Meanwhile, we select cases in which the answer is greater than 1000, identify them as "hard" cases, and additionally report the average performance on them. As shown in Table 2, all of our proposed algorithms significantly improve the performance of LMs, especially in hard cases. For example, the search-based algorithm boosts BART with an absolute 30.4% improvement on hard cases. The full results of the performance comparison can be found in Appendix D. Notably, since the search-based algorithm is the most effective, we apply it as the default algorithm in SOLIS.   </p>
<p>54.25%</p>
<p>Composition How many more Albanian citizens were there compared to Bulgarian and Georgia citizens combined ? [y = x0 − (x1 + x2)]</p>
<p>0.34%</p>
<p>imental results on AddSub and MultiArith. The results indicate that our approach is surprisingly effective for giant LMs and can further boost the performance of chain-of-thought prompting.</p>
<p>MODEL ANALYSIS</p>
<p>Arithmetic Relationship Inversion In addition to performance improvement, SOLIS features the ability to derive an arithmetic expression for each question, whereas no such information is available during training. To better understand if these expressions align with question intentions, we collect all derived expressions and categorize them into four types in Table 5. As demonstrated, the majority of expressions contain addition and subtraction between variables and constants, which are largely consistent with the question intention, highlighting the superior interpretability of SOLIS.</p>
<p>Solving Algorithm Robustness The possibility that the anchor answers provided by reasoning LMs are inaccurate presents a challenge for the solving algorithms. To measure the robustness of our solving algorithms, we roughly decrease the probability that anchor answers are correct by decreasing the number of few-shot demonstrations in Figure 3. As shown, even though the backbone LM performance drops to 60.0%, the improvement of SOLIS is still as high as to 5.1%, suggesting its robustness.</p>
<p>Number Substitution To study the impact of different factors during the number substitution stage, we conduct experiments on MathExp in Figure 4. As demonstrated, expanding the range of anchor numbers results in a minor performance drop, showing that the reasoning LM is more familiar with small integers. Furthermore, increasing the size of anchor number groups gives a large improvement on the performance, especially when there are four operands.</p>
<p>Limitation Discussion The first limitation of our framework is that we cannot support expressions that cannot be solved with linear systems. For example, with respect to the question "How many yards was Donovan McNabb's longest rushing TD?", the expected expression [y = max i (x i )] is not supported by SOLIS. Second, the framework is less efficient when there are many operands. On the one hand, the group of anchor numbers would be quite huge, making the algorithm's runtime unacceptable. For example, when expanding to 5 operands, number substitution must be performed at least 50 times. On the other hand, for the search-based algorithm, the search space will increase exponentially, making the algorithm impracticable. Last, we assume a certain level of numeracy understanding of the reasoning LM. Therefore, if the reasoning LM is unable to comprehend the numeracy relationship, our method would not work well.</p>
<p>RELATED WORK</p>
<p>Numerical Reasoning via Specialized Models Previous works generally design trainable specialized modules and equip LMs with them to tackle different kinds of numerical reasoning problems (e.g., counting). While these methods work well on specific datasets (Dua et al., 2019;Andor et al., 2019;Hu et al., 2019;Ding et al., 2019), they are hardly suited across different datasets and backbone LMs (Chen et al., 2020b). Differently, since our method does not require additional model training, it is applicable to almost all models, even those that only provide an inference interface (e.g., GPT-3). As for methods that first generate programs or logic forms, it is quite laborious to define domain-specific language and collect corresponding training data (Berant et al., 2013). Unlike them, our methods does not require extra annotated programs. Instead, our method allows for the program discovery from examples via solving linear systems.</p>
<p>Numerical Reasoning via Pre-training This line of work always focuses on the pre-training of language models with corpus which involves reasoning. The corpus can be reasoning-oriented natural language texts from Internet (Deng et al., 2021;Lewkowycz et al., 2022), human-designed templates filled by different data sources (Geva et al., 2020;Yoran et al., 2022), or programs with rich reasoning semantics Pi et al., 2022). Although this kind of pre-training allows language models to perform better reasoning, they still require considerable computation budgets during pretraining and may still be challenged by complex numbers. In contrast, our method is efficient since it can be integrated into existing models without further training or pre-training.</p>
<p>Numerical Reasoning in Giant Language Models Recent works demonstrate that with proper prompting, giant language models (e.g., GPT-3) perform much better than smaller ones on several reasoning tasks (Wei et al., 2022b;a;Kojima et al., 2022;. For example, with the chain-of-thought prompting, the few-shot PaLM model (Chowdhery et al., 2022) can beat the previous best fine-tuned model on math word problems. However, their conclusions do not generalize to non-giant language models. Different from them, our method can be simultaneously applied to language models ranging from millions (e.g., BART) to billions (e.g., GPT-3). Moreover, our work is orthogonal to these giant LMs and can be complementary to each other. For example, Section 5 shows that our approach can further boost the numerical reasoning capability of GPT-3 with chain-of-thought prompting.</p>
<p>CONCLUSION</p>
<p>In this work, we present SOLIS, a framework which can elicit numerical reasoning in language models at test time. Motivated by the fact that language models usually excel at simple numbers, SOLIS uses simple numbers as anchors to inversely derive the implicitly inferred arithmetic expressions from language models, and subsequently apply these expressions to complex numbers to perform numerical reasoning. With modeling the expression derivation as solving linear systems, we propose three kinds of algorithms to achieve SOLIS with noisy signals. Experimental results on several numerical reasoning benchmarks demonstrate that SOLIS can be integrated to a variety of language models, and can greatly improve their performance under zero-shot, few-shot, and finetuning scenarios. Our work provides a new perspective towards tackling numerical reasoning, which can be potentially applied to more language models and numerical reasoning tasks.</p>
<p>A PRELIMINARY STUDY DETAILS</p>
<p>Here we present the model performance on MathExp of GPT-3 with different solving algorithms in Figure 5 and Figure 6. We can conclude that: (1) both algorithms are not sensitive with either the floating point precision or the integer range;</p>
<p>(2) the search-based algorithm is most robust than the analytical-based algorithm with respect to the number of operands. </p>
<p>B OPERAND PROPOSAL DETAILS</p>
<p>In Section 3.2, we mention that the textual context on a realistic dataset may be noisy, i.e., contains irrelevant numbers, thus we need to locate the operand number first. We substitute 10 times for each number appearing in the paragraph, if the output gives ≥ 3 different prediction numbers out of 10, we decide the current tested number is involved to the answer. Moreover, we substitute numbers following a template: suppose the original number x is with precision p, then the substituted numbers can be represented as x + k · 10 p , where k ∈ {−5, −4, −3, −2, −1, 1, 2, 3, 4, 5}.</p>
<p>C EXPERIMENTS</p>
<p>C.1 EXPERIMENTAL SETUP  For BART, we implement the fine-tuning methods using the Huggingface transformers library (Wolf et al., 2020) on 4 V100 16GB GPUs. We use BART LARGE (Lewis et al., 2020) as our backbone. We use same-scale reasoning-pretrained POET-SQL and TAPEX models in experiments. For T5, we implement its fine-tuning on the Huggingface transformers library on A100 GPUs. We use T5 LARGE (Raffel et al., 2020) as our backbone.</p>
<p>C.2 EXPERIMENTAL DETAILS ON DROP</p>
<p>Fine-tuning Details For all fine-tuning methods, we select the default max token length for each model. We set the max token length of generation as 96. To save training time, we set early stop mechanism: we evaluate the EM and F1 score per 500 or 1000 steps, if the performance does not increase in the latest 20 evaluations, we stop the training and save the best checkpoint.</p>
<p>On DROP, we pre-pend the question to the given paragraph. For multi-span answer, we insert ";" between each span and make up the final answer. For T5 LARGE , we also insert "</s>" token between the question and the given paragraph. Since most LMs' checkpoints on DROP is currently not off-the-shelf, we re-implement them and compare to the results reported in previous works. We present the comparison results in Table 8. </p>
<p>D MORE RESULTS ON DROP</p>
<p>We present the performance breakdown of F1 on dev set of DROP in Table 9 Apart from fine-tuning models on DROP dataset, we also use GPT-3 to conduct a study on few-shot learning. We pre-pend 10 random training samples in train set, and run all cases where answer type equals to "number". We also apply our search-based algorithm on GPT-3. To save API calling time, we only substitute the number for one time. Table 10 presents the F1 score comparison.</p>
<p>We also summarize common calculation error cases in our tested language models and present some of them for case study in Table 11, which again illustrates the unreliability of language models.   </p>
<p>Figure 2 :
2Performance with different floating point precision (left) and integer range (right).</p>
<p>MultiArith because MultiArith requires more compositional operations. More details can be found in Appendix C.</p>
<p>Figure 3 :Figure 4 :
34The experimental results of Chain and Chain w. SOLIS on AddSub as the number of few-shot examples decreases. The experimental results of SOLIS on Math-Exp with different choices of anchor number range (left) and anchor number groups (right).</p>
<p>Figure 5 :Figure 6 :
56Performance over different floating point precision (left) and integer range (right) on MathExp of GPT-3 w. search-based algorithm. Performance over different floating point precision (left) and integer range (right) on MathExp of GPT-3 w. analytical-based algorithm.</p>
<p>Table 1 :
1Comparison of solving algorithms.Optimum Robustness Scalability </p>
<p>Analytical </p>
<p>Search </p>
<p>Heuristic </p>
<p>Table 2 :
2Experimental results of SOLIS w. various solving algorithms on the DROP numeric subset.LM 
Algorithm F1(%) on Hard F1(%) on Total </p>
<p>BART </p>
<p>-
30.4 
66.4 
Analytical 46.4 (+16.0) 
69.3 (+2.9) 
Search 
64.8 (+30.4) 
75.2 (+8.8) 
Heuristic 
52.8 (+22.4) 
71.7 (+5.3) </p>
<p>POET-SQL </p>
<p>-
66.8 
78.4 
Analytical 73.3 (+6.5) 
80.0 (+1.6) 
Search 
76.9 (+10.1) 
81.4 (+3.0) 
Heuristic 
73.0 (+6.2) 
80.5 (+2.1) </p>
<p>Table 3 :
3Experimental results on the validation set of DROP dataset.Models 
EM(%) 
F1(%) </p>
<p>Specialized Models 
NumNet (Ran et al., 2019) 
64.9 
68.3 
MTMSN (Hu et al., 2019) 
76.7 
80.5 
NeRd (Chen et al., 2020b) 
78.6 
81.9 
QDGAT (Chen et al., 2020a) 84.1 
87.1 
Vanilla LMs 
BART </p>
<p>Table 3
3shows the experimental results of different models on DROP dataset. As shown, SOLIS can bring consistent and significant improvements over all backbone LMs, especially for the vanilla LMs. Taking the T5 model as an example, it could be boosted by a maximum of 8.9% with SOLIS. Even for POET-SQL which are already pre-trained for numerical reasoning, our method yields a 2.0% F1 improvement, pushing the best LM performance to 82.0% F1.Table 4presents the exper-</p>
<p>Table 4 :
4Experimental results of different methods on AddSub and MultiArith.Language Model Setting 
AddSub 
MultiArith </p>
<p>PaLM (540B) 
Standard (Chowdhery et al., 2022) 
− 
42.2 
Chain (Wei et al., 2022b) 
91.9 
94.7 </p>
<p>GPT-3 (175B) </p>
<p>Zero-shot Chain (Kojima et al., 2022) 
66.6 
63.8 
w. SOLIS 
89.4 (+22.8) 
80.0 (+16.2) 
Chain (Wei et al., 2022b) 
88.4 
96.7 
w. SOLIS 
90.9 (+2.5) 
98.7 (+2.0) </p>
<p>Table 5 :
5Case study on derived expressions using POET-SQL w. SOLIS on DROP. Listed are, the intention, the example question with intention trigger words (i.e., the colorful spans) and the derived expression, and the proportion of each intention.Question Intention Example Question with [Derived Expression] 
Proportion </p>
<p>Addition 
How many total yards of touchdown passes were there? 
[y = x1 + x2 + x3] </p>
<p>8.92% </p>
<p>Diff Constant 
How many in percent in the county from the census of 2000 
weren't English? [y = 100 − x] </p>
<p>36.49% </p>
<p>Subtraction 
How many more percentages of people were germans compared 
to irish? [y = x1 − x2] </p>
<p>Table 6 :
6Statistics of DROP datasetDataset </p>
<p>Train 
Dev </p>
<h1>Questions # Docs # Questions # Docs</h1>
<p>DROP 
77, 409 
5, 565 
9, 536 
582 </p>
<p>Table 7 :
7Statistics of MAWPS datasetSubset </p>
<h1>Questions</h1>
<p>AddSub 
395 
MultiArith 
600 </p>
<p>Table 8 :
8Performance Comparison on DROP between reported results in previous works and our re-implementation. Results marked with * represent our re-implementation results.Models </p>
<p>EM (%) F1 (%) </p>
<p>BART (Pi et al., 2022) 
66.2 
69.2 
BART  *<br />
67.4 
70.6 </p>
<h2>T5 (Yoran et al., 2022)</h2>
<p>64.6 
T5  *<br />
61.0 
64.6 </p>
<p>POET-SQL (Pi et al., 2022) 77.7 
80.6 
POET-SQL  *<br />
76.9 
80.0 </p>
<p>Table 9 :
9Breakdown of model F1 score by answer types on the dev set of DROP.Models 
Number Span Spans Date Total </p>
<p>Table 10 :
10Performance of GPT-3 w. SOLIS on the DROP numeric subset.Language Model Algorithm F1(%) on Hard F1(%) on Total </p>
<h2>GPT-3 (175B)</h2>
<p>42.5 
64.7 
Search 
59.9 (+17.4) 
68.7 (+4.9) </p>
<p>Table 11 :
11Common calculation error cases on DROP dataset. Carry Error . . . the size of the black-white IQ gap in the United States decreased from 16.33 to 9.94 IQ points. . . . Q: How many IQ points did the black-white IQ gap decrease in the United States in a 2013 analysis of the National Assessment of Educational Progress? Missing High Digit . . . The Department of Tourism recorded 26,861,095 Thai and 11,361,808 foreign visitors to Bangkok in 2010. . . . Q: How many more Thai visitors did Bangkok have in 2010 compared to other foreign visitors? Extra Integer digit . . . Rayner nailed a 23-yard field goal . . . Rayner got a 54yarder and a 46-yarder to end the half . . . Q: How many total yards of field goals did Dave Rayner have? Extra Float Number Digits . . . have estimated the IQ means of 17-year-old black, white, and Hispanic students to range respectively from 90.45-94.15 . . . Q: How many points difference is the IQ range in 17year-old black students? Insufficient Precision . . . The Diocese of Karelia has 22,000 church members in 12 parishes. . . . Q: How many church members approximately are in each one of the 12 parishes?Error Type 
Example 
Prediction 
Label </p>
<p>6.49 
6.39 </p>
<p>499287 
15499287 </p>
<p>111113 
123 </p>
<p>3.75 
3.7 </p>
<p>1833 
1833.33 </p>
<p>https://openai.com/api
We use the terms number and operand interchangeably.
ACKNOWLEDGEMENTSWe would like to thank Xuanyu Dong, who is working at Harvest Fund, for helping us with the linearization method based on polynomial basis from an analytical and mathematical perspective.
Giving bert a calculator: Finding operations and arguments with reading comprehension. Daniel Andor, Luheng He, Kenton Lee, Emily Pitler, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingDaniel Andor, Luheng He, Kenton Lee, and Emily Pitler. Giving bert a calculator: Finding oper- ations and arguments with reading comprehension. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 5947-5952, 2019.</p>
<p>Semantic parsing on Freebase from question-answer pairs. Jonathan Berant, Andrew Chou, Roy Frostig, Percy Liang, Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. the 2013 Conference on Empirical Methods in Natural Language ProcessingSeattle, Washington, USAAssociation for Computational LinguisticsJonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic parsing on Freebase from question-answer pairs. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pp. 1533-1544, Seattle, Washington, USA, October 2013. Association for Computational Linguistics.</p>
<p>Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Advances in Neural Information Processing Systems. H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. LinScott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec RadfordCurran Associates, Inc33Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 1877-1901. Curran Associates, Inc., 2020a.</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 33Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020b.</p>
<p>Question directed graph attention network for numerical reasoning over text. Kunlong Chen, Weidi Xu, Xingyi Cheng, Zou Xiaochuan, Yuyu Zhang, Le Song, Taifeng Wang, Yuan Qi, Wei Chu, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Kunlong Chen, Weidi Xu, Xingyi Cheng, Zou Xiaochuan, Yuyu Zhang, Le Song, Taifeng Wang, Yuan Qi, and Wei Chu. Question directed graph attention network for numerical reasoning over text. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Pro- cessing (EMNLP), pp. 6759-6768, 2020a.</p>
<p>Neural symbolic reader: Scalable integration of distributed and symbolic representations for reading comprehension. Xinyun Chen, Chen Liang, Adams Wei Yu, Denny Zhou, Dawn Song, Quoc V Le, ICLR. Xinyun Chen, Chen Liang, Adams Wei Yu, Denny Zhou, Dawn Song, and Quoc V Le. Neural symbolic reader: Scalable integration of distributed and symbolic representations for reading comprehension. In ICLR, 2020b.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won, Charles Chung, Sebastian Sutton, Gehrmann, arXiv:2204.02311Scaling language modeling with pathways. arXiv preprintAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, arXiv:2110.14168arXiv preprintKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.</p>
<p>ReasonBERT: Pre-trained to reason with distant supervision. Xiang Deng, Yu Su, Alyssa Lees, You Wu, Cong Yu, Huan Sun, 10.18653/v1/2021.emnlp-mainProceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican RepublicAssociation for Computational LinguisticsXiang Deng, Yu Su, Alyssa Lees, You Wu, Cong Yu, and Huan Sun. ReasonBERT: Pre-trained to reason with distant supervision. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 6112-6127, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova Bert, arXiv:1810.04805Pre-training of deep bidirectional transformers for language understanding. arXiv preprintJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.</p>
<p>Cognitive graph for multi-hop reading comprehension at scale. Ming Ding, Chang Zhou, Qibin Chen, Hongxia Yang, Jie Tang, 10.18653/v1/P19-1259Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational LinguisticsMing Ding, Chang Zhou, Qibin Chen, Hongxia Yang, and Jie Tang. Cognitive graph for multi-hop reading comprehension at scale. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 2694-2703, Florence, Italy, July 2019. Association for Com- putational Linguistics. doi: 10.18653/v1/P19-1259.</p>
<p>Drop: A reading comprehension benchmark requiring discrete reasoning over paragraphs. Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, Matt Gardner, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies1Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. Drop: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Com- putational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 2368-2378, 2019.</p>
<p>Injecting numerical reasoning skills into language models. Mor Geva, Ankit Gupta, Jonathan Berant, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsMor Geva, Ankit Gupta, and Jonathan Berant. Injecting numerical reasoning skills into language models. In Proceedings of the 58th Annual Meeting of the Association for Computational Lin- guistics, pp. 946-958, 2020.</p>
<p>Language models are general-purpose interfaces. Yaru Hao, Haoyu Song, Li Dong, Shaohan Huang, Zewen Chi, Wenhui Wang, Shuming Ma, Furu Wei, arXiv:2206.06336arXiv preprintYaru Hao, Haoyu Song, Li Dong, Shaohan Huang, Zewen Chi, Wenhui Wang, Shuming Ma, and Furu Wei. Language models are general-purpose interfaces. arXiv preprint arXiv:2206.06336, 2022.</p>
<p>Masked autoencoders are scalable vision learners. Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, Ross Girshick, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked au- toencoders are scalable vision learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 16000-16009, 2022.</p>
<p>A multi-type multi-span network for reading comprehension that requires discrete reasoning. Minghao Hu, Yuxing Peng, Zhen Huang, Dongsheng Li, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingMinghao Hu, Yuxing Peng, Zhen Huang, and Dongsheng Li. A multi-type multi-span network for reading comprehension that requires discrete reasoning. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Confer- ence on Natural Language Processing (EMNLP-IJCNLP), pp. 1596-1606, 2019.</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, arXiv:2205.11916arXiv preprintTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916, 2022.</p>
<p>Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, Luke Zettlemoyer, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsBartMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre- training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 7871-7880, 2020.</p>
<p>Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, arXiv:2206.14858Solving quantitative reasoning problems with language models. Theo Gutman-SoloarXiv preprintAitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ra- masesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language models. arXiv preprint arXiv:2206.14858, 2022.</p>
<p>On the advance of making language models better reasoners. Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, Weizhu Chen, arXiv:2206.02336arXiv preprintYifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. On the advance of making language models better reasoners. arXiv preprint arXiv:2206.02336, 2022.</p>
<p>Awakening latent grounding from pretrained language models for semantic parsing. Qian Liu, Dejian Yang, Jiahui Zhang, Jiaqi Guo, Bin Zhou, Jian-Guang Lou, 10.18653/v1/2021.findings-acl.100Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. Association for Computational LinguisticsQian Liu, Dejian Yang, Jiahui Zhang, Jiaqi Guo, Bin Zhou, and Jian-Guang Lou. Awakening latent grounding from pretrained language models for semantic parsing. In Findings of the Associa- tion for Computational Linguistics: ACL-IJCNLP 2021, pp. 1174-1189, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.findings-acl.100.</p>
<p>TAPEX: Table pre-training via learning a neural SQL executor. Qian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi Lin, Weizhu Chen, Jian-Guang Lou, International Conference on Learning Representations. Qian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi Lin, Weizhu Chen, and Jian-Guang Lou. TAPEX: Table pre-training via learning a neural SQL executor. In International Conference on Learning Representations, 2022.</p>
<p>Decoupled weight decay regularization. Ilya Loshchilov, Frank Hutter, 7th International Conference on Learning Representations. New Orleans, LA, USAIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https://openreview.net/forum?id=Bkg6RiCqY7.</p>
<p>. Xinyu Pi, Qian Liu, Bei Chen, Morteza Ziyadi, Zeqi Lin, Yan Gao, Qiang Fu, Jian-Guang Lou, Weizhu Chen, arXiv:2201.11473Reasoning like program executors. arXiv preprintXinyu Pi, Qian Liu, Bei Chen, Morteza Ziyadi, Zeqi Lin, Yan Gao, Qiang Fu, Jian-Guang Lou, and Weizhu Chen. Reasoning like program executors. arXiv preprint arXiv:2201.11473, 2022.</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, Journal of Machine Learning Research. 21Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21:1-67, 2020.</p>
<p>Numnet: Machine reading comprehension with numerical reasoning. Yankai Qiu Ran, Peng Lin, Jie Li, Zhiyuan Zhou, Liu, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingQiu Ran, Yankai Lin, Peng Li, Jie Zhou, and Zhiyuan Liu. Numnet: Machine reading comprehen- sion with numerical reasoning. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 2474-2484, 2019.</p>
<p>Yasaman Razeghi, I V Robert L Logan, Matt Gardner, Sameer Singh, arXiv:2202.07206Impact of pretraining term frequencies on few-shot reasoning. arXiv preprintYasaman Razeghi, Robert L Logan IV, Matt Gardner, and Sameer Singh. Impact of pretraining term frequencies on few-shot reasoning. arXiv preprint arXiv:2202.07206, 2022.</p>
<p>Solving general arithmetic word problems. Subhro Roy, Dan Roth, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language ProcessingSubhro Roy and Dan Roth. Solving general arithmetic word problems. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pp. 1743-1752, 2015.</p>
<p>Evaluating the visualization of what a deep neural network has learned. W Samek, A Binder, G Montavon, S Lapuschkin, K Müller, 10.1109/TNNLS.2016.2599820IEEE Transactions on Neural Networks and Learning Systems. 2811W. Samek, A. Binder, G. Montavon, S. Lapuschkin, and K. Müller. Evaluating the visualization of what a deep neural network has learned. IEEE Transactions on Neural Networks and Learning Systems, 28(11):2660-2673, 2017. doi: 10.1109/TNNLS.2016.2599820.</p>
<p>Do nlp models know numbers? probing numeracy in embeddings. Eric Wallace, Yizhong Wang, Sujian Li, Sameer Singh, Matt Gardner, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingEric Wallace, Yizhong Wang, Sujian Li, Sameer Singh, and Matt Gardner. Do nlp models know numbers? probing numeracy in embeddings. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 5307-5315, 2019.</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Denny Zhou, arXiv:2203.11171arXiv preprintXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022.</p>
<p>. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, William Fedus, Emergent abilities of large language models. ArXiv, abs/2206.07682Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yo- gatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language models. ArXiv, abs/2206.07682, 2022a.</p>
<p>Chain of thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, Denny Zhou, arXiv:2201.11903arXiv preprintJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022b.</p>
<p>Transformers: State-of-the-art natural language processing. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations. the 2020 conference on empirical methods in natural language processing: system demonstrationsThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations, pp. 38-45, 2020.</p>
<p>Unifiedskg: Unifying and multi-tasking structured knowledge grounding with text-to-text language models. Tianbao Xie, Chen Henry Wu, Peng Shi, Ruiqi Zhong, Torsten Scholak, Michihiro Yasunaga, Chien-Sheng Wu, Ming Zhong, Pengcheng Yin, I Sida, Wang, arXiv:2201.05966arXiv preprintTianbao Xie, Chen Henry Wu, Peng Shi, Ruiqi Zhong, Torsten Scholak, Michihiro Yasunaga, Chien-Sheng Wu, Ming Zhong, Pengcheng Yin, Sida I Wang, et al. Unifiedskg: Unifying and multi-tasking structured knowledge grounding with text-to-text language models. arXiv preprint arXiv:2201.05966, 2022.</p>
<p>Turning tables: Generating examples from semistructured tables for endowing language models with reasoning skills. Alon Ori Yoran, Jonathan Talmor, Berant, Proceedings of the 60th. the 60thOri Yoran, Alon Talmor, and Jonathan Berant. Turning tables: Generating examples from semi- structured tables for endowing language models with reasoning skills. In Proceedings of the 60th</p>
<p>Annual Meeting of the Association for Computational Linguistics. 1Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 6016-6031, 2022.</p>
<p>Least-to-most prompting enables complex reasoning in large language models. Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, Quoc Le, Ed Chi, arXiv:2205.10625arXiv preprintDenny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schu- urmans, Olivier Bousquet, Quoc Le, and Ed Chi. Least-to-most prompting enables complex reasoning in large language models. arXiv preprint arXiv:2205.10625, 2022.</p>            </div>
        </div>

    </div>
</body>
</html>
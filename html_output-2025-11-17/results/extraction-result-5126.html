<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5126 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5126</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5126</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-107.html">extraction-schema-107</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <p><strong>Paper ID:</strong> paper-391246ce9c59d61c94cca3f8bef56c95542a4708</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/391246ce9c59d61c94cca3f8bef56c95542a4708" target="_blank">ROSCOE: A Suite of Metrics for Scoring Step-by-Step Reasoning</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> ROSCOE is presented, a suite of interpretable, unsupervised automatic scores that improve and extend previous text generation evaluation metrics and can measure semantic consistency, logicality, informativeness, fluency, and factuality - among other traits - by leveraging properties of step-by-step rationales.</p>
                <p><strong>Paper Abstract:</strong> Large language models show improved downstream task performance when prompted to generate step-by-step reasoning to justify their final answers. These reasoning steps greatly improve model interpretability and verification, but objectively studying their correctness (independent of the final answer) is difficult without reliable methods for automatic evaluation. We simply do not know how often the stated reasoning steps actually support the final end task predictions. In this work, we present ROSCOE, a suite of interpretable, unsupervised automatic scores that improve and extend previous text generation evaluation metrics. To evaluate ROSCOE against baseline metrics, we design a typology of reasoning errors and collect synthetic and human evaluation scores on commonly used reasoning datasets. In contrast with existing metrics, ROSCOE can measure semantic consistency, logicality, informativeness, fluency, and factuality - among other traits - by leveraging properties of step-by-step rationales. We empirically verify the strength of our metrics on five human annotated and six programmatically perturbed diagnostics datasets - covering a diverse set of tasks that require reasoning skills and show that ROSCOE can consistently outperform baseline metrics.</p>
                <p><strong>Cost:</strong> 0.026</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5126.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5126.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ROSCOE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ROSCOE: A Suite of Metrics for Scoring Step-by-Step Reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An interpretable, unsupervised suite of 18 fine-grained automatic metrics (grouped into semantic alignment, semantic similarity, logical inference, and language coherence) designed to evaluate step-by-step rationales produced by language models when no gold rationale is available.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ROSCOE (evaluation suite)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Reference-free and reference-based scorers that (1) compute step- and token-level reasoning-alignments using sentence/token embeddings, (2) use an NLI model to score contradiction probabilities, and (3) use LM perplexity and grammar classifiers for language-coherence signals. ROSCOE uses a finetuned SimCSE sentence embedder, an off-the-shelf NLI classifier, and GPT-2 perplexity for components.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>Various reasoning benchmarks (diagnostic and human-judged): EntailmentBank, ProofWriter, MATH, ASDIV, AQUA, EQASC, StrategyQA (finetune), GSM8K, DROP, ESNLI, COSMOS-QA, SemEval.</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Evaluation of multi-step reasoning (deductive, logical/proof-style, arithmetic, commonsense, multi-hop) by scoring step-wise rationales for grounding, logical consistency, informativeness, repetition, hallucination, missing steps, and language quality.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Design and aggregate 18 metrics across four perspectives: (a) Semantic Alignment (reasoning alignment vectors between hypothesis steps and source; Faithfulness-Step/Token, Info-Step, Hallucination, Missing Step, Commonsense, etc.), (b) Semantic Similarity (chain-level embeddings e.g., Info-Chain, Semantic Coverage-Chain), (c) Logical Inference (use of an NLI classifier to compute contradiction probabilities for Self-Consistency and Source-Consistency), (d) Language Coherence (GPT-2 perplexity inversion, grammar acceptability). Embedding model choices and finetuning (SimCSE) are used to improve step-level similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>ROSCOE consistently outperforms prior reference-free baselines on both synthetic diagnostics and human-judged datasets measured by Somers' D. Representative diagnostics (maximum over perturbations): ROSCOE-SS: EntailmentBank 0.955, MATH 0.924, AQUA 0.982, EQASC 1.000, ASDIV 0.857; ROSCOE-SA: EntailmentBank 0.919, MATH 0.939, AQUA 0.971, EQASC 1.000, ASDIV 0.879. Baseline reference-free metrics (examples): BERTScore/BLEURT/BARTScore typically range ~0.1–0.5 on the same diagnostics. On human-judged datasets ROSCOE variants achieve the highest correlations among tested scorers (e.g., ROSCOE-SS Somers' D up to 0.638–0.799 on GSM8K/other datasets depending on embedding).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>ROSCOE is less effective on datasets where correct reasoning is expressed as an unordered set of facts/rules (e.g., some subsets of ProofWriter) because many ROSCOE scores assume ordered step sequences. Correlations with human judgements are moderate (hard cognitive task) and scores drift across datasets (need per-dataset calibration thresholds). Some fine-tuning choices degrade specific sub-metrics (e.g., Repetition-Step degraded slightly after embedding finetuning). Does not address bias/safety of rationales.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Directly compared with a broad set of generation-evaluation baselines (ROUGE, BLEURT, BERTScore, PRISM, BARTScore and its finetuned/ prompting variants, CTC relevance/consistency). ROSCOE outperforms these baselines across all diagnostic datasets and on the human-judged tasks in the reference-free setting; baselines often show much lower Somers' D (e.g., many baseline correlations <0.5 while ROSCOE metrics often >0.9 on diagnostics).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Ablations/analyses include: (1) embedding model sensitivity — comparing off-the-shelf all-mpnet-base-v2 and sup-simcse-roberta-base vs a SimCSE model finetuned on multi-step reasoning; finetuning SimCSE improved most ROSCOE correlations (notably InformativenessChain up to +0.556 in some cases) though some repetition metrics were unchanged or slightly degraded; (2) per-perspective analysis showing ROSCOE-SS often dominates on hallucination/repetition detection while ROSCOE-SA is strong for grounding and missing-step detection; (3) sensitivity experiments injecting 1–3 perturbations show ROSCOE-SA and ROSCOE-SS correlate monotonically with error level, whereas many baselines do not; (4) dataset drift analysis shows that thresholds calibrated on one dataset do not generalize, indicating need for calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ROSCOE: A Suite of Metrics for Scoring Step-by-Step Reasoning', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5126.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5126.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large autoregressive few-shot language model (Brown et al., 2020) used in this study to generate step-by-step reasoning chains via few-shot in-context prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language models are few-shot learners</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Used as an LLM to produce step-by-step explanations for multiple tasks by few-shot prompting (the paper used few-shot examples in prompts to elicit chains of thought for ESNLI, SemEval, CosmosQA, DROP and other human-judged datasets). The paper cites Brown et al. (2020) for GPT-3.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>Various human-judged reasoning benchmarks (ESNLI, SemEval, COSMOS-QA, DROP, and others) — used as the generator of rationales rather than as the target of fine-grained evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generation of multi-step natural-language rationales for classification and question-answering tasks requiring deductive, commonsense, and multi-step inference.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Few-shot in-context prompting with example chains (chain-of-thought-like prompts) to elicit step-by-step reasoning outputs from GPT-3; those outputs were then annotated by experts and scored by ROSCOE and baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>The paper does not report standalone quantitative accuracy/performance of GPT-3 on the benchmarks; instead, GPT-3-generated chains were used as inputs for human annotation and metric evaluation. ROSCOE correlations report how well automatic scorers align with human judgments on chains produced by GPT-3 (moderate correlations observed).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Generated rationales from GPT-3 contained diverse error types from the taxonomy (grammar, factuality, hallucination, repetition, missing steps, arithmetic, commonsense and coherency errors). No direct claims on improving GPT-3's logical reasoning were made in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>GPT-3 outputs were compared implicitly by evaluating the utility of scorers (ROSCOE vs baselines) on GPT-3-generated chains; not compared as a modeling target against other LMs in terms of accuracy in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>No ablation of GPT-3 itself presented; analysis focused on how well ROSCOE metrics correlate with human judgments of GPT-3 outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ROSCOE: A Suite of Metrics for Scoring Step-by-Step Reasoning', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5126.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5126.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>175b_verification</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>175b_verification model (chain-of-thought outputs referenced from Wei et al., 2022)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A very large LM chain-of-thought variant (referred to as '175b_verification' in Wei et al.) whose chain-of-thought outputs for GSM8K were used as data for human annotation and metric evaluation in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain of thought prompting elicits reasoning in large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>175b_verification</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LM checkpoint (from Wei et al., 2022) that produces chain-of-thought reasoning traces; the ROSCOE paper used chains produced by this 175b verification model for GSM8K annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B</td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>GSM8K (grade-school math word problems) — arithmetic multi-step reasoning benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Solve arithmetic word problems requiring multi-step quantitative calculations and to provide step-by-step chains.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Chain-of-thought style prompting (sampling/verification variant from Wei et al.) produced step-by-step rationales; these were annotated and used to evaluate ROSCOE metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>The ROSCOE paper does not report the verification model's accuracy; it used the model's chains as annotation material. ROSCOE correlations on GSM8K (human-judged) show moderate alignment (e.g., ROSCOE-SS variants Somers' D between ~0.47–0.64 depending on embedding choice).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Chains contained arithmetic and commonsense errors as per the taxonomy; paper did not evaluate or improve the 175b model itself.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>No direct comparative performance of the 175b_verification model vs other LMs is reported in this paper; the model's outputs were used to stress-test evaluation metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ROSCOE: A Suite of Metrics for Scoring Step-by-Step Reasoning', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5126.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5126.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SimCSE (finetuned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Finetuned SimCSE sentence embedding model (initialized from sup-simcse-roberta-base)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A supervised contrastive sentence embedding model (SimCSE) finetuned on multi-step reasoning datasets to produce more accurate step-level embeddings used by ROSCOE semantic-alignment and similarity metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Simcse: Simple contrastive learning of sentence embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Finetuned SimCSE (roscoe-512-roberta-base)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Initialized from sup-simcse-roberta-base and further trained for five epochs on synthetic reasoning data (positive pairs: context+reference chains; hard negatives: context+perturbed chains) to better capture stepwise reasoning similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>Used to compute step-level embeddings for scoring reasoning chains across diagnostic and human-judged datasets (listed above).</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Produce sentence/step embeddings that reflect fine-grained semantic and logical similarity for alignment between hypothesis steps, source sentences, and reference steps.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Contrastive finetuning on triplets (context, reference step positive; context, perturbed step negative) using in-batch negatives, to better separate correct vs perturbed reasoning steps.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Finetuned SimCSE improved ROSCOE metric correlations in experiments: on average, finetuning improved most correlations across datasets (e.g., notable improvement on ASDIV and up to +0.556 increase on Informativeness-Chain in some analyses). ROSCOE variants computed with the finetuned embeddings achieve the highest reported Somers' D values in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Finetuning improved many metrics but degraded Repetition-Step slightly and did not change Repetition-Token on average; embedding finetuning may bias certain token-level semantics versus chain-level semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Compared to using off-the-shelf all-mpnet-base-v2 and sup-simcse-roberta-base embeddings, the finetuned SimCSE consistently yielded higher ROSCOE correlations on several datasets, indicating sensitivity of the scoring pipeline to embedding model choice.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ROSCOE: A Suite of Metrics for Scoring Step-by-Step Reasoning', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5126.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5126.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BARTScore / BARTScore-P</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BARTScore (and finetuned variant BARTScore-P)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A probability-based text generation evaluation metric using BART to score the likelihood of generated text given a source; BARTScore-P is BART finetuned on reasoning datasets used as a baseline in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Bartscore: Evaluating generated text as text generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BARTScore / BARTScore-P</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>BARTScore computes log-probability of hypothesis given source via a seq2seq model; BARTScore-P is BART finetuned on the paper's reasoning datasets to better match the evaluation domain.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>Used as a baseline automatic metric for scoring step-by-step reasoning across diagnostics and human-judged tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Evaluate the relevance/factuality of generated rationales to source/context via sequence probability or finetuned scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Apply pre-trained BART scoring (and a version finetuned/prompted for reasoning) to compute faithfulness/relevance measures between source and hypothesis chains.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Baseline performance (reference-free Somers' D) in diagnostics (examples): BARTScore: EntailmentBank 0.358, MATH 0.185, AQUA 0.317, ProofWriter 0.081, EQASC 0.415. BARTScore-P (finetuned) generally performed worse on some diagnostics (e.g., EntBank 0.186, ProofWriter 0.011). On human-judged datasets BARTScore sometimes correlates negatively with human judgements (e.g., DROP -0.835 in Table 8), showing poor alignment for some reasoning quality judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>BARTScore (and finetuned variant) is designed for overall text probability/fidelity and is not specialized to step-level logical consistency; it often underperforms on fine-grained reasoning evaluation and can have negative correlations with human judgements on some datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>ROSCOE outperforms BARTScore variants across all diagnostic datasets and human-judged evaluations in the reference-free setting, often by large margins (e.g., ROSCOE-SA/SS correlations >> BARTScore).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ROSCOE: A Suite of Metrics for Scoring Step-by-Step Reasoning', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5126.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5126.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-2 Large (PPL scorer)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-2 Large</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An autoregressive transformer language model (Radford et al.) used without finetuning to compute perplexity-based language-coherence metrics for ROSCOE.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language models are unsupervised multitask learners</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 Large</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Used as-is to compute token-level perplexity for the Perplexity-Chain and Perplexity-Step ROSCOE-LC metrics (inverted to align orientation with other scores).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>Language-coherence scoring of generated step-by-step rationales across reasoning datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Detect fluency/coherence and local contextual plausibility of rationales using LM perplexity.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Compute average token perplexity for chain or step contexts and invert (1/PPL) to obtain a bounded score.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>ROSCOE-LC (which incorporates PPL) shows dataset-dependent utility; e.g., Perplexity-derived signals contributed to ROSCOE-LC achieving Somers' D = 0.788 on ProofWriter diagnostics but lower correlations on other diagnostics and human-judged datasets (sometimes negative), indicating limited generality.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Perplexity-based coherence metrics struggle to detect certain reasoning errors (e.g., arithmetic perturbations or subtle logical inconsistencies) and show inconsistent correlation with human judgments across datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Perplexity contributes complementary signals but is insufficient alone for accurate reasoning evaluation compared to ROSCOE's combined metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ROSCOE: A Suite of Metrics for Scoring Step-by-Step Reasoning', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5126.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5126.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NLI classifier (for contradiction)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NLI classifier (used to compute contradiction probabilities)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An off-the-shelf natural language inference (NLI) model (cited Laurer et al., 2022) used to estimate contradiction probabilities between reasoning steps and source sentences for ROSCOE-LI logical-inference metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Less annotating, more classifyingaddressing the data scarcity issue of supervised machine learning with deep transfer learning and bert-nli</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>NLI classifier (pretrained)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A transferred NLI model that classifies pairs as entailment/neutral/contradiction; used to compute p_contr(h_i, h_j) and p_contr(h_i, s_j) as inputs to Self-Consistency and Source-Consistency scores.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>Detecting contradictions within the reasoning chain (self-consistency) and between chain steps and source context (source-consistency) across datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Binary/ternary inference classification to flag contradicting step pairs or step-vs-source contradictions.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Apply pretrained NLI classifier to all step-pair and step-source sentence pairs and take maximum contradiction probability to penalize chains with any likely contradiction.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>ROSCOE-LI (using this NLI model) achieved strong correlations on some diagnostics (EntailmentBank 0.917) but weaker on others (MATH 0.331, ProofWriter 0.289). This indicates NLI-based contradiction detection is powerful for some logical tasks but limited on arithmetic and unordered-rule reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>NLI models trained on standard entailment corpora may not generalize to arithmetic errors or to reasoning expressed as unordered sets of facts; contradiction detection is sensitive to domain shifts.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>NLI-based logical-inference metrics are complementary to semantic-alignment metrics; ROSCOE combines both for better overall performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ROSCOE: A Suite of Metrics for Scoring Step-by-Step Reasoning', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5126.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e5126.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique that elicits multi-step natural-language reasoning traces from large LMs by providing exemplars of step-by-step solutions in prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain of thought prompting elicits reasoning in large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Chain-of-Thought prompting (method)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Used in related work and in this paper as the mechanism to obtain step-by-step rationales from LMs (few-shot in-context examples shown in Appendix B).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>Applied broadly to arithmetic (GSM8K, MATH), commonsense (COSMOS-QA), and deductive tasks to elicit reasoning traces.</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Elicit intermediate reasoning steps that can help models arrive at better final answers and provide interpretable rationales.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Provide one or more solved examples (step-by-step) in the prompt to guide model to produce chain-of-thought outputs for new problems.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>The paper references chain-of-thought as effective in prior work (Wei et al., 2022) and uses it to generate rationales; ROSCOE evaluates the quality of those chains but does not present end-to-end improvement numbers for chain-of-thought in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Chain-of-thought outputs still contain diverse errors (per the taxonomy); ROSCOE shows many such errors persist and need reliable automatic scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Mentioned alongside self-consistency and fine-tuning as prominent methods to improve LM reasoning in prior work; ROSCOE is positioned to evaluate the quality of CoT outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ROSCOE: A Suite of Metrics for Scoring Step-by-Step Reasoning', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5126.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e5126.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Consistency (ensemble decoding method)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ensemble decoding strategy that samples multiple diverse reasoning chains from an LM and selects the most consistent final answer across samples to improve reasoning accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-consistency improves chain of thought reasoning in language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Self-Consistency (method)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Samples many chain-of-thought outputs and aggregates answers (e.g., by majority) to improve final-answer robustness; discussed in related work as an approach to improve complex reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>Used in prior work to boost performance on complex reasoning tasks (e.g., arithmetic and multi-step reasoning benchmarks), cited here as related approach.</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Improve final-answer accuracy by leveraging diversity and redundancy across multiple sampled reasoning traces.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Sampling multiple chains-of-thought traces and combining predictions (self-consistency voting/verification).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>The ROSCOE paper cites prior work (Wang et al., 2022) that reports improvements; ROSCOE itself does not present new performance numbers for self-consistency, but notes such methods exist and are complementary to evaluation metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not evaluated within ROSCOE experiments directly; effectiveness depends on model's ability to produce diverse but correct chains and can still be undermined by systemic error modes across samples.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Presented as a complementary method to chain-of-thought prompting and finetuning; ROSCOE metrics can be used to analyze and detect error types across sampled chains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ROSCOE: A Suite of Metrics for Scoring Step-by-Step Reasoning', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5126.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e5126.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>STAR (Zelikman et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>STAR: Bootstrapping reasoning with reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A bootstrapping/finetuning approach (cited Zelikman et al., 2022) that uses generated reasoning to improve models; mentioned in related work as an approach to improve reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Star: Bootstrapping reasoning with reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>STAR (method)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prior work that bootstraps and finetunes models using their own generated reasoning traces to improve reasoning capabilities; cited as related to approaches that improve multi-step reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>General reasoning benchmarks (not evaluated directly in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Improve LM reasoning via iterative finetuning on generated rationales and verification.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Use model-generated reasoning as training signal to refine model reasoning abilities.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Not reported in this paper; cited as related work.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not discussed in detail here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Listed among other methods (prompting, self-consistency, fine-tuning) that aim to improve LM reasoning; ROSCOE could evaluate outputs from such methods.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ROSCOE: A Suite of Metrics for Scoring Step-by-Step Reasoning', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5126.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e5126.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Lampinen et al. (finetuning explanations)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tell me why! Explanations support learning relational and causal structure</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Work showing that training with explanations/finetuning can improve relational and causal learning in models; cited as evidence that fine-tuning on rationales helps reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Tell me why! Explanations support learning relational and causal structure</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Finetuning with explanations (method)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prior study demonstrating that exposure to explanations can support learning of relational and causal structure; cited in related work to motivate evaluation of explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>Relational and causal reasoning tasks (cited in related work), not directly evaluated in ROSCOE experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Improve models' reasoning by training on explanation-augmented data.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Supervised finetuning using explanation-labeled data to improve reasoning generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Not presented within ROSCOE; cited as prior evidence that finetuning can help reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not discussed in ROSCOE beyond citation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Cited alongside other improvement strategies; ROSCOE is designed to evaluate whether such finetuning actually produces higher-quality, consistent rationales.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ROSCOE: A Suite of Metrics for Scoring Step-by-Step Reasoning', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>Language models are few-shot learners <em>(Rating: 2)</em></li>
                <li>Simcse: Simple contrastive learning of sentence embeddings <em>(Rating: 2)</em></li>
                <li>Bartscore: Evaluating generated text as text generation <em>(Rating: 1)</em></li>
                <li>ProofWriter: Generating implications, proofs, and abductive statements over natural language <em>(Rating: 1)</em></li>
                <li>Explaining answers with entailment trees <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5126",
    "paper_id": "paper-391246ce9c59d61c94cca3f8bef56c95542a4708",
    "extraction_schema_id": "extraction-schema-107",
    "extracted_data": [
        {
            "name_short": "ROSCOE",
            "name_full": "ROSCOE: A Suite of Metrics for Scoring Step-by-Step Reasoning",
            "brief_description": "An interpretable, unsupervised suite of 18 fine-grained automatic metrics (grouped into semantic alignment, semantic similarity, logical inference, and language coherence) designed to evaluate step-by-step rationales produced by language models when no gold rationale is available.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ROSCOE (evaluation suite)",
            "model_description": "Reference-free and reference-based scorers that (1) compute step- and token-level reasoning-alignments using sentence/token embeddings, (2) use an NLI model to score contradiction probabilities, and (3) use LM perplexity and grammar classifiers for language-coherence signals. ROSCOE uses a finetuned SimCSE sentence embedder, an off-the-shelf NLI classifier, and GPT-2 perplexity for components.",
            "model_size": null,
            "logical_reasoning_task": "Various reasoning benchmarks (diagnostic and human-judged): EntailmentBank, ProofWriter, MATH, ASDIV, AQUA, EQASC, StrategyQA (finetune), GSM8K, DROP, ESNLI, COSMOS-QA, SemEval.",
            "task_description": "Evaluation of multi-step reasoning (deductive, logical/proof-style, arithmetic, commonsense, multi-hop) by scoring step-wise rationales for grounding, logical consistency, informativeness, repetition, hallucination, missing steps, and language quality.",
            "method_or_approach": "Design and aggregate 18 metrics across four perspectives: (a) Semantic Alignment (reasoning alignment vectors between hypothesis steps and source; Faithfulness-Step/Token, Info-Step, Hallucination, Missing Step, Commonsense, etc.), (b) Semantic Similarity (chain-level embeddings e.g., Info-Chain, Semantic Coverage-Chain), (c) Logical Inference (use of an NLI classifier to compute contradiction probabilities for Self-Consistency and Source-Consistency), (d) Language Coherence (GPT-2 perplexity inversion, grammar acceptability). Embedding model choices and finetuning (SimCSE) are used to improve step-level similarity.",
            "performance": "ROSCOE consistently outperforms prior reference-free baselines on both synthetic diagnostics and human-judged datasets measured by Somers' D. Representative diagnostics (maximum over perturbations): ROSCOE-SS: EntailmentBank 0.955, MATH 0.924, AQUA 0.982, EQASC 1.000, ASDIV 0.857; ROSCOE-SA: EntailmentBank 0.919, MATH 0.939, AQUA 0.971, EQASC 1.000, ASDIV 0.879. Baseline reference-free metrics (examples): BERTScore/BLEURT/BARTScore typically range ~0.1–0.5 on the same diagnostics. On human-judged datasets ROSCOE variants achieve the highest correlations among tested scorers (e.g., ROSCOE-SS Somers' D up to 0.638–0.799 on GSM8K/other datasets depending on embedding).",
            "limitations_or_failure_cases": "ROSCOE is less effective on datasets where correct reasoning is expressed as an unordered set of facts/rules (e.g., some subsets of ProofWriter) because many ROSCOE scores assume ordered step sequences. Correlations with human judgements are moderate (hard cognitive task) and scores drift across datasets (need per-dataset calibration thresholds). Some fine-tuning choices degrade specific sub-metrics (e.g., Repetition-Step degraded slightly after embedding finetuning). Does not address bias/safety of rationales.",
            "comparison": "Directly compared with a broad set of generation-evaluation baselines (ROUGE, BLEURT, BERTScore, PRISM, BARTScore and its finetuned/ prompting variants, CTC relevance/consistency). ROSCOE outperforms these baselines across all diagnostic datasets and on the human-judged tasks in the reference-free setting; baselines often show much lower Somers' D (e.g., many baseline correlations &lt;0.5 while ROSCOE metrics often &gt;0.9 on diagnostics).",
            "ablation_or_analysis_results": "Ablations/analyses include: (1) embedding model sensitivity — comparing off-the-shelf all-mpnet-base-v2 and sup-simcse-roberta-base vs a SimCSE model finetuned on multi-step reasoning; finetuning SimCSE improved most ROSCOE correlations (notably InformativenessChain up to +0.556 in some cases) though some repetition metrics were unchanged or slightly degraded; (2) per-perspective analysis showing ROSCOE-SS often dominates on hallucination/repetition detection while ROSCOE-SA is strong for grounding and missing-step detection; (3) sensitivity experiments injecting 1–3 perturbations show ROSCOE-SA and ROSCOE-SS correlate monotonically with error level, whereas many baselines do not; (4) dataset drift analysis shows that thresholds calibrated on one dataset do not generalize, indicating need for calibration.",
            "uuid": "e5126.0",
            "source_info": {
                "paper_title": "ROSCOE: A Suite of Metrics for Scoring Step-by-Step Reasoning",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "GPT-3",
            "name_full": "GPT-3",
            "brief_description": "A large autoregressive few-shot language model (Brown et al., 2020) used in this study to generate step-by-step reasoning chains via few-shot in-context prompts.",
            "citation_title": "Language models are few-shot learners",
            "mention_or_use": "use",
            "model_name": "GPT-3",
            "model_description": "Used as an LLM to produce step-by-step explanations for multiple tasks by few-shot prompting (the paper used few-shot examples in prompts to elicit chains of thought for ESNLI, SemEval, CosmosQA, DROP and other human-judged datasets). The paper cites Brown et al. (2020) for GPT-3.",
            "model_size": null,
            "logical_reasoning_task": "Various human-judged reasoning benchmarks (ESNLI, SemEval, COSMOS-QA, DROP, and others) — used as the generator of rationales rather than as the target of fine-grained evaluation.",
            "task_description": "Generation of multi-step natural-language rationales for classification and question-answering tasks requiring deductive, commonsense, and multi-step inference.",
            "method_or_approach": "Few-shot in-context prompting with example chains (chain-of-thought-like prompts) to elicit step-by-step reasoning outputs from GPT-3; those outputs were then annotated by experts and scored by ROSCOE and baselines.",
            "performance": "The paper does not report standalone quantitative accuracy/performance of GPT-3 on the benchmarks; instead, GPT-3-generated chains were used as inputs for human annotation and metric evaluation. ROSCOE correlations report how well automatic scorers align with human judgments on chains produced by GPT-3 (moderate correlations observed).",
            "limitations_or_failure_cases": "Generated rationales from GPT-3 contained diverse error types from the taxonomy (grammar, factuality, hallucination, repetition, missing steps, arithmetic, commonsense and coherency errors). No direct claims on improving GPT-3's logical reasoning were made in this work.",
            "comparison": "GPT-3 outputs were compared implicitly by evaluating the utility of scorers (ROSCOE vs baselines) on GPT-3-generated chains; not compared as a modeling target against other LMs in terms of accuracy in this paper.",
            "ablation_or_analysis_results": "No ablation of GPT-3 itself presented; analysis focused on how well ROSCOE metrics correlate with human judgments of GPT-3 outputs.",
            "uuid": "e5126.1",
            "source_info": {
                "paper_title": "ROSCOE: A Suite of Metrics for Scoring Step-by-Step Reasoning",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "175b_verification",
            "name_full": "175b_verification model (chain-of-thought outputs referenced from Wei et al., 2022)",
            "brief_description": "A very large LM chain-of-thought variant (referred to as '175b_verification' in Wei et al.) whose chain-of-thought outputs for GSM8K were used as data for human annotation and metric evaluation in this study.",
            "citation_title": "Chain of thought prompting elicits reasoning in large language models",
            "mention_or_use": "use",
            "model_name": "175b_verification",
            "model_description": "LM checkpoint (from Wei et al., 2022) that produces chain-of-thought reasoning traces; the ROSCOE paper used chains produced by this 175b verification model for GSM8K annotations.",
            "model_size": "175B",
            "logical_reasoning_task": "GSM8K (grade-school math word problems) — arithmetic multi-step reasoning benchmark.",
            "task_description": "Solve arithmetic word problems requiring multi-step quantitative calculations and to provide step-by-step chains.",
            "method_or_approach": "Chain-of-thought style prompting (sampling/verification variant from Wei et al.) produced step-by-step rationales; these were annotated and used to evaluate ROSCOE metrics.",
            "performance": "The ROSCOE paper does not report the verification model's accuracy; it used the model's chains as annotation material. ROSCOE correlations on GSM8K (human-judged) show moderate alignment (e.g., ROSCOE-SS variants Somers' D between ~0.47–0.64 depending on embedding choice).",
            "limitations_or_failure_cases": "Chains contained arithmetic and commonsense errors as per the taxonomy; paper did not evaluate or improve the 175b model itself.",
            "comparison": "No direct comparative performance of the 175b_verification model vs other LMs is reported in this paper; the model's outputs were used to stress-test evaluation metrics.",
            "uuid": "e5126.2",
            "source_info": {
                "paper_title": "ROSCOE: A Suite of Metrics for Scoring Step-by-Step Reasoning",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "SimCSE (finetuned)",
            "name_full": "Finetuned SimCSE sentence embedding model (initialized from sup-simcse-roberta-base)",
            "brief_description": "A supervised contrastive sentence embedding model (SimCSE) finetuned on multi-step reasoning datasets to produce more accurate step-level embeddings used by ROSCOE semantic-alignment and similarity metrics.",
            "citation_title": "Simcse: Simple contrastive learning of sentence embeddings",
            "mention_or_use": "use",
            "model_name": "Finetuned SimCSE (roscoe-512-roberta-base)",
            "model_description": "Initialized from sup-simcse-roberta-base and further trained for five epochs on synthetic reasoning data (positive pairs: context+reference chains; hard negatives: context+perturbed chains) to better capture stepwise reasoning similarity.",
            "model_size": null,
            "logical_reasoning_task": "Used to compute step-level embeddings for scoring reasoning chains across diagnostic and human-judged datasets (listed above).",
            "task_description": "Produce sentence/step embeddings that reflect fine-grained semantic and logical similarity for alignment between hypothesis steps, source sentences, and reference steps.",
            "method_or_approach": "Contrastive finetuning on triplets (context, reference step positive; context, perturbed step negative) using in-batch negatives, to better separate correct vs perturbed reasoning steps.",
            "performance": "Finetuned SimCSE improved ROSCOE metric correlations in experiments: on average, finetuning improved most correlations across datasets (e.g., notable improvement on ASDIV and up to +0.556 increase on Informativeness-Chain in some analyses). ROSCOE variants computed with the finetuned embeddings achieve the highest reported Somers' D values in the paper.",
            "limitations_or_failure_cases": "Finetuning improved many metrics but degraded Repetition-Step slightly and did not change Repetition-Token on average; embedding finetuning may bias certain token-level semantics versus chain-level semantics.",
            "comparison": "Compared to using off-the-shelf all-mpnet-base-v2 and sup-simcse-roberta-base embeddings, the finetuned SimCSE consistently yielded higher ROSCOE correlations on several datasets, indicating sensitivity of the scoring pipeline to embedding model choice.",
            "uuid": "e5126.3",
            "source_info": {
                "paper_title": "ROSCOE: A Suite of Metrics for Scoring Step-by-Step Reasoning",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "BARTScore / BARTScore-P",
            "name_full": "BARTScore (and finetuned variant BARTScore-P)",
            "brief_description": "A probability-based text generation evaluation metric using BART to score the likelihood of generated text given a source; BARTScore-P is BART finetuned on reasoning datasets used as a baseline in this study.",
            "citation_title": "Bartscore: Evaluating generated text as text generation",
            "mention_or_use": "use",
            "model_name": "BARTScore / BARTScore-P",
            "model_description": "BARTScore computes log-probability of hypothesis given source via a seq2seq model; BARTScore-P is BART finetuned on the paper's reasoning datasets to better match the evaluation domain.",
            "model_size": null,
            "logical_reasoning_task": "Used as a baseline automatic metric for scoring step-by-step reasoning across diagnostics and human-judged tasks.",
            "task_description": "Evaluate the relevance/factuality of generated rationales to source/context via sequence probability or finetuned scoring.",
            "method_or_approach": "Apply pre-trained BART scoring (and a version finetuned/prompted for reasoning) to compute faithfulness/relevance measures between source and hypothesis chains.",
            "performance": "Baseline performance (reference-free Somers' D) in diagnostics (examples): BARTScore: EntailmentBank 0.358, MATH 0.185, AQUA 0.317, ProofWriter 0.081, EQASC 0.415. BARTScore-P (finetuned) generally performed worse on some diagnostics (e.g., EntBank 0.186, ProofWriter 0.011). On human-judged datasets BARTScore sometimes correlates negatively with human judgements (e.g., DROP -0.835 in Table 8), showing poor alignment for some reasoning quality judgments.",
            "limitations_or_failure_cases": "BARTScore (and finetuned variant) is designed for overall text probability/fidelity and is not specialized to step-level logical consistency; it often underperforms on fine-grained reasoning evaluation and can have negative correlations with human judgements on some datasets.",
            "comparison": "ROSCOE outperforms BARTScore variants across all diagnostic datasets and human-judged evaluations in the reference-free setting, often by large margins (e.g., ROSCOE-SA/SS correlations &gt;&gt; BARTScore).",
            "uuid": "e5126.4",
            "source_info": {
                "paper_title": "ROSCOE: A Suite of Metrics for Scoring Step-by-Step Reasoning",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "GPT-2 Large (PPL scorer)",
            "name_full": "GPT-2 Large",
            "brief_description": "An autoregressive transformer language model (Radford et al.) used without finetuning to compute perplexity-based language-coherence metrics for ROSCOE.",
            "citation_title": "Language models are unsupervised multitask learners",
            "mention_or_use": "use",
            "model_name": "GPT-2 Large",
            "model_description": "Used as-is to compute token-level perplexity for the Perplexity-Chain and Perplexity-Step ROSCOE-LC metrics (inverted to align orientation with other scores).",
            "model_size": null,
            "logical_reasoning_task": "Language-coherence scoring of generated step-by-step rationales across reasoning datasets.",
            "task_description": "Detect fluency/coherence and local contextual plausibility of rationales using LM perplexity.",
            "method_or_approach": "Compute average token perplexity for chain or step contexts and invert (1/PPL) to obtain a bounded score.",
            "performance": "ROSCOE-LC (which incorporates PPL) shows dataset-dependent utility; e.g., Perplexity-derived signals contributed to ROSCOE-LC achieving Somers' D = 0.788 on ProofWriter diagnostics but lower correlations on other diagnostics and human-judged datasets (sometimes negative), indicating limited generality.",
            "limitations_or_failure_cases": "Perplexity-based coherence metrics struggle to detect certain reasoning errors (e.g., arithmetic perturbations or subtle logical inconsistencies) and show inconsistent correlation with human judgments across datasets.",
            "comparison": "Perplexity contributes complementary signals but is insufficient alone for accurate reasoning evaluation compared to ROSCOE's combined metrics.",
            "uuid": "e5126.5",
            "source_info": {
                "paper_title": "ROSCOE: A Suite of Metrics for Scoring Step-by-Step Reasoning",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "NLI classifier (for contradiction)",
            "name_full": "NLI classifier (used to compute contradiction probabilities)",
            "brief_description": "An off-the-shelf natural language inference (NLI) model (cited Laurer et al., 2022) used to estimate contradiction probabilities between reasoning steps and source sentences for ROSCOE-LI logical-inference metrics.",
            "citation_title": "Less annotating, more classifyingaddressing the data scarcity issue of supervised machine learning with deep transfer learning and bert-nli",
            "mention_or_use": "use",
            "model_name": "NLI classifier (pretrained)",
            "model_description": "A transferred NLI model that classifies pairs as entailment/neutral/contradiction; used to compute p_contr(h_i, h_j) and p_contr(h_i, s_j) as inputs to Self-Consistency and Source-Consistency scores.",
            "model_size": null,
            "logical_reasoning_task": "Detecting contradictions within the reasoning chain (self-consistency) and between chain steps and source context (source-consistency) across datasets.",
            "task_description": "Binary/ternary inference classification to flag contradicting step pairs or step-vs-source contradictions.",
            "method_or_approach": "Apply pretrained NLI classifier to all step-pair and step-source sentence pairs and take maximum contradiction probability to penalize chains with any likely contradiction.",
            "performance": "ROSCOE-LI (using this NLI model) achieved strong correlations on some diagnostics (EntailmentBank 0.917) but weaker on others (MATH 0.331, ProofWriter 0.289). This indicates NLI-based contradiction detection is powerful for some logical tasks but limited on arithmetic and unordered-rule reasoning.",
            "limitations_or_failure_cases": "NLI models trained on standard entailment corpora may not generalize to arithmetic errors or to reasoning expressed as unordered sets of facts; contradiction detection is sensitive to domain shifts.",
            "comparison": "NLI-based logical-inference metrics are complementary to semantic-alignment metrics; ROSCOE combines both for better overall performance.",
            "uuid": "e5126.6",
            "source_info": {
                "paper_title": "ROSCOE: A Suite of Metrics for Scoring Step-by-Step Reasoning",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "Chain-of-Thought prompting",
            "name_full": "Chain-of-Thought Prompting",
            "brief_description": "A prompting technique that elicits multi-step natural-language reasoning traces from large LMs by providing exemplars of step-by-step solutions in prompts.",
            "citation_title": "Chain of thought prompting elicits reasoning in large language models",
            "mention_or_use": "mention",
            "model_name": "Chain-of-Thought prompting (method)",
            "model_description": "Used in related work and in this paper as the mechanism to obtain step-by-step rationales from LMs (few-shot in-context examples shown in Appendix B).",
            "model_size": null,
            "logical_reasoning_task": "Applied broadly to arithmetic (GSM8K, MATH), commonsense (COSMOS-QA), and deductive tasks to elicit reasoning traces.",
            "task_description": "Elicit intermediate reasoning steps that can help models arrive at better final answers and provide interpretable rationales.",
            "method_or_approach": "Provide one or more solved examples (step-by-step) in the prompt to guide model to produce chain-of-thought outputs for new problems.",
            "performance": "The paper references chain-of-thought as effective in prior work (Wei et al., 2022) and uses it to generate rationales; ROSCOE evaluates the quality of those chains but does not present end-to-end improvement numbers for chain-of-thought in this study.",
            "limitations_or_failure_cases": "Chain-of-thought outputs still contain diverse errors (per the taxonomy); ROSCOE shows many such errors persist and need reliable automatic scoring.",
            "comparison": "Mentioned alongside self-consistency and fine-tuning as prominent methods to improve LM reasoning in prior work; ROSCOE is positioned to evaluate the quality of CoT outputs.",
            "uuid": "e5126.7",
            "source_info": {
                "paper_title": "ROSCOE: A Suite of Metrics for Scoring Step-by-Step Reasoning",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "Self-Consistency",
            "name_full": "Self-Consistency (ensemble decoding method)",
            "brief_description": "An ensemble decoding strategy that samples multiple diverse reasoning chains from an LM and selects the most consistent final answer across samples to improve reasoning accuracy.",
            "citation_title": "Self-consistency improves chain of thought reasoning in language models",
            "mention_or_use": "mention",
            "model_name": "Self-Consistency (method)",
            "model_description": "Samples many chain-of-thought outputs and aggregates answers (e.g., by majority) to improve final-answer robustness; discussed in related work as an approach to improve complex reasoning.",
            "model_size": null,
            "logical_reasoning_task": "Used in prior work to boost performance on complex reasoning tasks (e.g., arithmetic and multi-step reasoning benchmarks), cited here as related approach.",
            "task_description": "Improve final-answer accuracy by leveraging diversity and redundancy across multiple sampled reasoning traces.",
            "method_or_approach": "Sampling multiple chains-of-thought traces and combining predictions (self-consistency voting/verification).",
            "performance": "The ROSCOE paper cites prior work (Wang et al., 2022) that reports improvements; ROSCOE itself does not present new performance numbers for self-consistency, but notes such methods exist and are complementary to evaluation metrics.",
            "limitations_or_failure_cases": "Not evaluated within ROSCOE experiments directly; effectiveness depends on model's ability to produce diverse but correct chains and can still be undermined by systemic error modes across samples.",
            "comparison": "Presented as a complementary method to chain-of-thought prompting and finetuning; ROSCOE metrics can be used to analyze and detect error types across sampled chains.",
            "uuid": "e5126.8",
            "source_info": {
                "paper_title": "ROSCOE: A Suite of Metrics for Scoring Step-by-Step Reasoning",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "STAR (Zelikman et al.)",
            "name_full": "STAR: Bootstrapping reasoning with reasoning",
            "brief_description": "A bootstrapping/finetuning approach (cited Zelikman et al., 2022) that uses generated reasoning to improve models; mentioned in related work as an approach to improve reasoning.",
            "citation_title": "Star: Bootstrapping reasoning with reasoning",
            "mention_or_use": "mention",
            "model_name": "STAR (method)",
            "model_description": "Prior work that bootstraps and finetunes models using their own generated reasoning traces to improve reasoning capabilities; cited as related to approaches that improve multi-step reasoning.",
            "model_size": null,
            "logical_reasoning_task": "General reasoning benchmarks (not evaluated directly in this paper).",
            "task_description": "Improve LM reasoning via iterative finetuning on generated rationales and verification.",
            "method_or_approach": "Use model-generated reasoning as training signal to refine model reasoning abilities.",
            "performance": "Not reported in this paper; cited as related work.",
            "limitations_or_failure_cases": "Not discussed in detail here.",
            "comparison": "Listed among other methods (prompting, self-consistency, fine-tuning) that aim to improve LM reasoning; ROSCOE could evaluate outputs from such methods.",
            "uuid": "e5126.9",
            "source_info": {
                "paper_title": "ROSCOE: A Suite of Metrics for Scoring Step-by-Step Reasoning",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "Lampinen et al. (finetuning explanations)",
            "name_full": "Tell me why! Explanations support learning relational and causal structure",
            "brief_description": "Work showing that training with explanations/finetuning can improve relational and causal learning in models; cited as evidence that fine-tuning on rationales helps reasoning.",
            "citation_title": "Tell me why! Explanations support learning relational and causal structure",
            "mention_or_use": "mention",
            "model_name": "Finetuning with explanations (method)",
            "model_description": "Prior study demonstrating that exposure to explanations can support learning of relational and causal structure; cited in related work to motivate evaluation of explanations.",
            "model_size": null,
            "logical_reasoning_task": "Relational and causal reasoning tasks (cited in related work), not directly evaluated in ROSCOE experiments.",
            "task_description": "Improve models' reasoning by training on explanation-augmented data.",
            "method_or_approach": "Supervised finetuning using explanation-labeled data to improve reasoning generalization.",
            "performance": "Not presented within ROSCOE; cited as prior evidence that finetuning can help reasoning.",
            "limitations_or_failure_cases": "Not discussed in ROSCOE beyond citation.",
            "comparison": "Cited alongside other improvement strategies; ROSCOE is designed to evaluate whether such finetuning actually produces higher-quality, consistent rationales.",
            "uuid": "e5126.10",
            "source_info": {
                "paper_title": "ROSCOE: A Suite of Metrics for Scoring Step-by-Step Reasoning",
                "publication_date_yy_mm": "2022-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2
        },
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 2
        },
        {
            "paper_title": "Simcse: Simple contrastive learning of sentence embeddings",
            "rating": 2
        },
        {
            "paper_title": "Bartscore: Evaluating generated text as text generation",
            "rating": 1
        },
        {
            "paper_title": "ProofWriter: Generating implications, proofs, and abductive statements over natural language",
            "rating": 1
        },
        {
            "paper_title": "Explaining answers with entailment trees",
            "rating": 1
        }
    ],
    "cost": 0.026273499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>ROSCOE: A Suite of Metrics for Scoring STEP-bySTEP REASONING</h1>
<p>Olga Golovneva, Moya Chen, Spencer Poff, Martin Corredor, Luke Zettlemoyer, Maryam Fazel-Zarandi, Asli Celikyilmaz Meta AI Research<br>{olggol, mpchen, spoff, mcorredor, lsz, maryamfazel, aslic}@meta.com</p>
<h4>Abstract</h4>
<p>Large language models show improved downstream task performance when prompted to generate step-by-step reasoning to justify their final answers (Nye et al., 2021; Wei et al., 2022). These reasoning steps greatly improve model interpretability and verification, but objectively studying their correctness (independent of the final answer) is difficult without reliable methods for automatic evaluation. We simply do not know how often the stated reasoning steps actually support the final end task predictions. In this work, we present ROSCOE, a suite of interpretable, unsupervised automatic scores that improve and extend previous text generation evaluation metrics. To evaluate ROSCOE against baseline metrics, we design a typology of reasoning errors and collect synthetic and human evaluation scores on commonly used reasoning datasets. In contrast with existing metrics, ROSCOE can measure semantic consistency, logicality, informativeness, fluency, and factuality - among other traits - by leveraging properties of step-by-step rationales. We empirically verify the strength of our metrics on five human annotated and six programmatically perturbed diagnostics datasets - covering a diverse set of tasks that require reasoning skills and show that ROSCOE can consistently outperform baseline metrics. ${ }^{1}$</p>
<h2>1 INTRODUCTION</h2>
<p>Scaling language models has improved state-of-the-art performance on nearly every NLP benchmark (Brown et al., 2020), with large language models (LLMs) performing impressively as few-shot learners (Brown et al., 2020). Despite these achievements, even the largest of these models still struggle with tasks including math word problems (Hendrycks et al., 2021), symbolic manipulation (Rytting \&amp; Wingate, 2021), and commonsense reasoning (West et al., 2022). Recent work has shown that prompting (Wei et al., 2022; Wang et al., 2022) or fine-tuning (Lampinen et al., 2022) LLMs to generate step-by-step rationales can lead to improvements on reasoning tasks. Some of these include small-scale analysis of specific error types within step-by-step rationales (Lewkowycz et al., 2022; Chowdhery et al., 2022), as shown in Table 1. However, existing works primarily focus on end-task performance. Although text generation evaluation metrics sometimes offer fine-grained quality evaluations (e.g., adequacy, fluency) against human scores (Opitz \&amp; Frank, 2021; Leiter et al., 2022), these metrics generally treat the output as a whole, and many of these generative metrics operate on tasks such as summarization or machine-translation rather than reasoning.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>In this paper, we present ROSCOE, a suite of interpretable and fine-grained step-by-step generation evaluation metrics to address the above gaps. Rather than providing one score that only evaluates the generated text on the overall, ROSCOE encapsulates fine-grained metrics under four perspectives: (1) semantic alignment defines to what extend the generated reasoning is coherent, and grounded with the source context; (2) logical inference evaluates if the generated reasoning steps are consistent within itself and checks for logical fallacies; (3) semantic similarity quantifies the degree of similarity between the generated reasoning and the context or between intermediate steps to capture hallucinations or repetitions; and (4) language coherence evaluates if the whole chain flows naturally.</p>
<p>To evaluate ROSCOE against existing metrics, we devise a taxonomy of reasoning errors for multi-step generations and use it to create synthetic data and collect human evaluations on commonly used reasoning datasets. Our taxonomy and annotated datasets help us gain deeper insights into the causes of reasoning inconsistencies and weaknesses of LLMs. We evaluate ROSCOE with 18 fine-grained metrics under the above four perspectives. ROSCOE demonstrates performance gains against baseline evaluation metrics on all tasks that require reasoning over context. Additional sensitivity analysis shows that ROSCOE is more robust when dealing with tasks that require logical and arithmetic reasoning.</p>
<p>Contributions. (1) We propose a new taxonomy for reasoning errors, and use it for collecting human annotations and creating synthetic datasets. (2) Using our taxonomy, we propose a new suite of metrics that focus on sequence and step level analysis of step-by-step reasoning. (3) We present extensive comparative analysis on 11 datasets of varied complex reasoning problems demonstrating the strengths of each metric, especially in terms of interpretability relative to baselines, and considerations for use.</p>
<h1>2 RELATED WORK</h1>
<p>Evaluating Explanations. Free-form natural Language (NL) explanations of model decisions should enable accurate representation of the reasoning process and degree of plausibility (Danilevsky et al., 2020; Jacovi \&amp; Goldberg, 2021; Jacovi et al., 2021). A qualitative assessment of NL explanations with correctness labels collected from human judges was presented in (Camburu et al., 2018). Recent work has also investigated automatic metrics for natural language generation (NLG) evaluation including word overlap or embedding based similarly with human written explanations (Clinciu et al., 2021). Though fast and cost-effective, automatic metrics for NLG are not equipped to measure the logical inconsistencies or information gain with thinking steps (Reiter, 2019; Celikyilmaz et al., 2020). Explanations have also been evaluated by collecting datasets, and running correlation analysis to investigate the degree to which an automatic metric correlates with human judgements of clarity, relevance and informativeness (Leiter et al., 2022; Welleck et al., 2022). Although reliable, human evaluation is an expensive, domain specific, and time-consuming process. In comparison, ROSCOE provides generic automatic evaluation procedures that are domain and task specific.</p>
<p>Automatic Metrics. Many NLG evaluation metrics exist in the literature including ones based on: $n$-gram match (Lin, 2004), regression (Sellam et al., 2020), embedding proximity (Zhang et al., 2020), paraphrasing (Thompson \&amp; Post, 2020), generation as an evaluator (Yuan et al., 2021); information alignment (Deng et al., 2021); among others. Although these metrics are easy to use, they evaluate the alignment of two texts as a whole and are not designed to assess individual reasoning steps. The closest metrics to ours are CTC (Deng</p>
<p>Table 2: Taxonomy of Step-by-Step Reasoning Errors. Full list of the error types with examples is illustrated in Table 10.</p>
<table>
<thead>
<tr>
<th>Error Type</th>
<th>Definition</th>
</tr>
</thead>
<tbody>
<tr>
<td>Grammar</td>
<td>Faulty, unconventional, or controversial grammar usage</td>
</tr>
<tr>
<td>Factuality</td>
<td>Information about an object (i.e. quantity, characteristics) or a named entity doesn’t match with the input context.</td>
</tr>
<tr>
<td>Hallucination</td>
<td>Information is not provided in the problem statement and is irrelevant or wrong</td>
</tr>
<tr>
<td>Redundancy</td>
<td>Explanation contains redundant information, which even though might be factual, is not required to answer the question</td>
</tr>
<tr>
<td>Repetition</td>
<td>Step paraphrases information already mentioned in previous reasoning steps</td>
</tr>
<tr>
<td>Missing step</td>
<td>The content of the generated reasoning is incomplete and lacks required information to produce the correct answer.</td>
</tr>
<tr>
<td>Coherency</td>
<td>Steps contradict each other or do not follow a cohesive story</td>
</tr>
<tr>
<td>Commonsense</td>
<td>Model lacks relations that should be known from general world (e.g., "all ducks are birds")</td>
</tr>
<tr>
<td>Arithmetic</td>
<td>Error in math calculations</td>
</tr>
</tbody>
</table>
<p>et al., 2021) and BARTScore (Yuan et al., 2021), as both introduce a set of interpretable metrics to evaluate the similarity between two texts. However, ROSCOE is unique in providing fine-grained interpretations of reasoning steps, determining contradictions, and identifying ordering issues in the reasoning narrative.</p>
<p>Self-Consistency with LLMs. Recent work on improving LLMs performance on complex reasoning tasks uses an ensemble strategy called self-consistency (Wang et al., 2022). This method samples a diverse set of reasoning paths from a language model via reasoning traces prompting and returns the most consistent final answer in the set. Other work evaluates the diversity of a reasoning path (Li et al., 2022), or the consistency of an inference step (Creswell et al., 2022) or finetune LLMs (Zelikman et al., 2022) to improve on difficult NLP tasks. In contrast to these works, we present a suit of metrics that focus on determining the type of the error (e.g., commonsense or logical inconsistency) in a reasoning path, if one exists.</p>
<h1>3 REASONING ERROR TAXONOMY AND DATASETS CONSTRUCTION</h1>
<p>Problem Formulation. Our goal is to score step-by-step rationales generated by a language model. We assume that the model is given a source context $\boldsymbol{s}=\left{s_{1}, \cdots, s_{T}\right}$ of $T$-sentences indicating a problem statement followed by a question and is prompted to generate step-by-step reasoning (Nye et al., 2021). We refer to this as a hypothesis $\boldsymbol{h}=\left{h_{1}, \cdots, h_{N}\right}$ of $N$-steps, including a final answer as the last step. We do not assume availability of gold step-by-step reasoning references $\boldsymbol{r}=\left{r_{1}, \cdots, r_{K}\right}$ of $K$-steps.</p>
<p>Taxonomy. We propose a new taxonomy of generic reasoning errors for language problem solving. We first conduct manual preliminary analysis on different types of LLMs reasoning errors using five Human judged datasets described below. Based on our analysis, we identified nine error types centered on the overall reasoning chain (i.e., the quality of the step-by-step thinking, including consistency with the context and commonsense reasoning). Our taxonomy also includes fine-grained errors marking inconsistency of a reasoning step with the previous steps, whether each step contributes to the final decision, and overall logical inference or fluency issues. The definition of error types is in Table 2, and Table 10 provides examples.</p>
<p>Datasets and Annotations. To evaluate ROSCOE, we select datasets covering diverse set of tasks that require reasoning skills (e.g., logical, arithmetic, and commonsense reasoning tasks). We separate these datasets into two: (1) Diagnostics datasets that contain gold standard step-wise reasoning chains, where we synthetically perturb some of the reasoning steps to introduce different generation errors (e.g., missing step, mathematical error, etc.); (2) Human judged datasets with model generated step-by-step reasoning outputs where the reasoning error evaluations are solicited from expert judges. We investigate these in $\S 5$.</p>
<h2>4 REASONING SCORER: ROSCOE</h2>
<p>We present our fine-grained metrics under four perspectives: semantic alignment, semantic similarity, logical inference and language coherence. Each metric is bounded within $[0,1]$, where 1 indicates the perfect score</p>
<p>and 0 corresponds to failure. A metric is reference-free or unsupervised when it uses the source and hypothesis $(\boldsymbol{h} \rightarrow \boldsymbol{s})$, while reference-based or supervised when evaluated between hypothesis and reference $(\boldsymbol{h} \rightarrow \boldsymbol{r})$.</p>
<h1>4.1 Semantic Alignment Metrics (ROSCOE-SA)</h1>
<p>At the core of the ROSCOE semantic alignment ${ }^{2}$ metrics is the reasoning alignment vector from the $N$-step hypothesis $\boldsymbol{h}$ to the source $\boldsymbol{s}$ of length $T: r$-align $(\boldsymbol{h} \rightarrow \boldsymbol{s})=\left{\alpha_{1}, \alpha_{2}, \cdots, \alpha_{N}\right}$, where each alignment value $\alpha_{i}=r$-align $\left(h_{i} \rightarrow \boldsymbol{s}\right)=\left[1+\max <em i="i">{j=1}^{T}\left(\cos \left(h</em>$. Our proposed metrics are summarized in Table 3.}, s_{j}\right)\right] / 2 \in[0,1]\right.$ is the normalized cosine similarity between hypothesis step and most similar sentence in a context, and explicitly measures the grounding of the step-wise reasoning with respect to the source text (illustrated in App. D, Fig. 3). We estimate the alignment vector $r$-align $(\boldsymbol{h} \rightarrow \boldsymbol{s})$ by matching source text and the reasoning chains on the embeddings of tokens and individual reasoning steps. A similar information alignment score is introduced in CTC (Deng et al., 2021) to measure the confidence that the information of the $i$-th source document token $s_{j}$ is grounded by a hypothesis token $h_{i}$. Our reasoning alignment is different in that we measure if a hypothesized reasoning step $h_{i}$ supports the source context $\boldsymbol{s</p>
<p>Table 3: Semantic alignment metrics (ROSCOE-SA).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Score</th>
<th style="text-align: center;">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Faithfulness-Step $(\boldsymbol{h} \rightarrow \boldsymbol{s})$</td>
<td style="text-align: center;">This step-level score is based on the alignment from the hypothesis steps to the source sentences, and is calculated as the mean reasoning alignment score over the steps of reasoning (see illustration in Appendix D, Figure 3): $(1 / N) \sum_{i=1}^{N} r$-align $\left(h_{i} \rightarrow \boldsymbol{s}\right)$. Faithfulness measures if the model misinterpreted the problem statement, or the reasoning chain is too vague, irrelevant, or misuses information.</td>
</tr>
<tr>
<td style="text-align: center;">Faithfulness-Token $(\boldsymbol{h} \rightarrow \boldsymbol{s})$</td>
<td style="text-align: center;">We extend step-level embeddings of the Faithfulness-Step by measuring similarities between the token embeddings: $\left(1 /(N+M)\right) \sum_{i=1}^{N}[r$-align $\left(h_{i} \rightarrow \boldsymbol{s}\right)+\sum_{j=1}^{M} r$-align $^{\text {tokers }}\left(h_{i, j} \rightarrow \boldsymbol{s}\right]$, as shown in App. D, Fig. 3. $M_{i}$ is the number of tokens in step $h_{i}, M=\sum_{i=1}^{N} M_{i}$ is the total number of tokens in the reasoning chain, $h_{i, j}$ is the $j$ th token in $i$ th step, and $r$-align $^{\text {tokes }}$ is the alignment vector from tokens in step $h_{i}$ to all tokens in $\boldsymbol{s}$.</td>
</tr>
<tr>
<td style="text-align: center;">Informativeness-Step (Info-Step) $(\boldsymbol{h} \leftrightarrow \boldsymbol{s})$</td>
<td style="text-align: center;">Measures how well information present in the source is used in the reasoning steps: $\left[(1 / T) \sum_{t=1}^{T} r\right.$-align $\left(s_{t} \rightarrow \boldsymbol{h}\right)+$ $(1 / N) \sum_{t=1}^{N} r$-align $\left(h_{t} \rightarrow \boldsymbol{s}\right)] / 2$. Info-step gives a higher score to reasoning steps that are well-grounded with respect to the source, and identifies the degree of information from source that is covered by the generated hypothesis. A lower Info-Step score corresponds to the reasoning steps that are not related to the source sentences or have missed information provided in the context.</td>
</tr>
<tr>
<td style="text-align: center;">Repetition-Token $\left(h_{i} \rightarrow h_{j}\right)$</td>
<td style="text-align: center;">To identify repeated, or paraphrased steps, we look at the token alignment scores between all steps in the hypothesis chain: $1-\max <em _cdots="\cdots" i-1="i-1" j="1">{i=2, N} \max </em>\right)\right]$. For each pair of sentences, we look at the mean token alignment, and find those sentences that maximize this alignment score. In other words, Repetition-Token will punish chains where there are at least two steps with high overlap in token embeddings.}\left[\left(1 / M_{i}\right) \sum_{t=1}^{M_{i}} r\right.$-align $\left.^{\text {tokers }}\left(h_{i, t} \rightarrow h_{j</td>
</tr>
<tr>
<td style="text-align: center;">Hallucination $(\boldsymbol{h} \rightarrow(\boldsymbol{s}, \boldsymbol{r}))$</td>
<td style="text-align: center;">To find irrelevant reasoning steps, we use alignment score to identify steps that are both not related to the context and not in the reference chain (to avoid punishing for possibly relevant commonsense knowledge): $1-\max _{i=1 \ldots N}\left(</td>
</tr>
<tr>
<td style="text-align: center;">Redundancy $(\boldsymbol{h} \rightarrow \boldsymbol{r})$</td>
<td style="text-align: center;">To find chains that contain information that is not required to solve the problem (i.e., redundant steps), we identify those hypothesis steps that are least aligned with the the reference steps: $\min <em i="i">{t=1 \ldots N} r$-align $\left(h</em>\right)$. This score punishes chains with steps that are not required for the correct solution.} \rightarrow \boldsymbol{r</td>
</tr>
<tr>
<td style="text-align: center;">Semantic <br> Coverage-Step $\left((\boldsymbol{r}, \boldsymbol{h}) \rightarrow \boldsymbol{s}\right)$</td>
<td style="text-align: center;">This score can be viewed as a measure of how easily a gold reference could be generated by the hypothesis. It compares step level grounding of the hypothesis with respect to the source, and the gold reference grounding: $\left[(1 / T) \sum_{t=1}^{T} r\right.$-align $\left.\left(r_{t} \rightarrow\right.\right.$ $\boldsymbol{s})-\left((1 / N) \sum_{t=1}^{N} r\right.$-align $\left.\left(h_{i} \rightarrow \boldsymbol{s}\right)\right]$, where $\left</td>
</tr>
<tr>
<td style="text-align: center;">Reasoning Alignment $(\boldsymbol{h} \rightarrow \boldsymbol{r})$</td>
<td style="text-align: center;">The most straightforward way to evaluate the correctness of the hypothesis chain is to compare the degree of the overlap between the hypothesis and the reference. One way of doing that is to measure the reasoning alignment between them: $(1 / N) \sum_{t=1}^{T} r$-align $\left(h_{t} \rightarrow \boldsymbol{r}\right)$.</td>
</tr>
<tr>
<td style="text-align: center;">Commonsense $(\boldsymbol{r} \rightarrow(\boldsymbol{h}, \boldsymbol{s}))$</td>
<td style="text-align: center;">Measures if hypothesis lacks steps that are not stated in the source, but are required to solve the problem such as general world knowledge (e.g., "velocity is distance divided by time", "1 foot is 12 inches", "all ducks are birds", etc.). We detect such information by extracting steps in the reference reasoning that are not grounded by the source text: $1-\max _{i=1 \ldots K}\left(</td>
</tr>
<tr>
<td style="text-align: center;">Missing Step $(\boldsymbol{r} \rightarrow \boldsymbol{h})$</td>
<td style="text-align: center;">To identify steps that are missing from the hypothesis but could be required to solve the problem, we look at the alignment between reference and the hypothesis, similar to Redundancy. However, here we go through each step in the reference, and check if there is a similar step in the hypothesis: $\min <em i="i">{i=1 \ldots K}\left[r\right.$-align $\left.\left(r</em>\right)\right]$.} \rightarrow \boldsymbol{h</td>
</tr>
</tbody>
</table>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>4.2 Semantic Similarity Metrics (ROSCOE-SS)</h1>
<p>Semantic similarity metrics quantify the degree of semantic equivalence between pieces of text. As opposed to the ROSCOE-SA metrics, ROSCOE-SS considers text as a whole, rather than relying on text units comparisons. We propose the following metrics summarized in Table 4.</p>
<p>Table 4: Semantic similarity metrics (ROSCOE-SS).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Score</th>
<th style="text-align: left;">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Informativeness-Chain <br> (Info-Chain) $(\boldsymbol{h} \rightarrow \boldsymbol{s})$</td>
<td style="text-align: left;">Similar to Info-Step, this metric quantifies the degree of agreement between the hypothesis chain and the source and is <br> calculated as $[1+\cos (\boldsymbol{h}, \boldsymbol{s})] / 2$. We embed reasoning chain and source context as a whole, as opposed to using step-wise <br> embeddings in *-Step types of metrics introduced in Table 3.</td>
</tr>
<tr>
<td style="text-align: left;">Repetition-Step <br> $\left(h_{i} \mapsto h_{j}\right)$</td>
<td style="text-align: left;">Measures repetition-related errors on the step level by checking if it paraphrases information already mentioned in the <br> previous steps: $\left(1-\max <em _ldots="\ldots" i-1="i-1" j="1">{s=2..N} \max </em>\right)] / 2$. Unlike Repetition-Token, which is orderless and compares <br> individual tokens in pairs of steps, Repetition-Step considers step embeddings similarity and is more robust to changing <br> contexts.}\right) \cos \left(h_{i}, h_{j</td>
</tr>
<tr>
<td style="text-align: left;">Semantic Coverage- <br> Chain $(\boldsymbol{r} \leftrightarrow \boldsymbol{h})$</td>
<td style="text-align: left;">Reflects the overall degree of similarity between the reference and hypothesis chains, comparing reference and hypothesis <br> embeddings as a whole: $[1+\cos (\boldsymbol{r}, \boldsymbol{h})] / 2$.</td>
</tr>
</tbody>
</table>
<h3>4.3 Logical Inference Metrics (ROSCOE-LI)</h3>
<p>Logical inference metrics (Table 5) measure logical errors between pieces of text. We use an NLI model that was trained to classify hypothesis-context pairs into entailment, neutral, and contradiction classes (Laurer et al., 2022) to infer the contradiction probability $p_{\text {contr }}$.</p>
<p>Table 5: Logical inference metrics (ROSCOE-LI).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Score</th>
<th style="text-align: left;">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Self-Consistency <br> $\left(h_{i} \mapsto h_{j}\right)$</td>
<td style="text-align: left;">Measures logical entailment errors within the reasoning steps: $1-\max <em j_i="j&lt;i">{i=2..N} \max </em>\right)$. This metric will <br> punish chains where there is a pair of steps that are likely to contradict each other.} p_{\text {contr }}\left(h_{i}, h_{j</td>
</tr>
<tr>
<td style="text-align: left;">Source-Consistency <br> $\left(\boldsymbol{h} \leftrightarrow \boldsymbol{s}\right)$</td>
<td style="text-align: left;">Measures logical entailment errors between any generated reasoning $\boldsymbol{h}$ and the source context $\boldsymbol{s}$ : $1-$ <br> $\max <em T="T" class="" j="1">{i=1..N} \max </em>\right)$. Specifically, for each reasoning step we measure the probability that it contra- <br> dicts any sentence in the context. We take the maximum probability of contradiction over all steps, following the logic that <br> a contradiction anywhere in the reasoning chain signals a failure of the overall argument.} p_{\text {contr }}\left(h_{i}, s_{j</td>
</tr>
</tbody>
</table>
<h3>4.4 Language Coherence Metrics (ROSCOE-LC)</h3>
<p>To evaluate language coherence (Table 6), we use perplexity PPL as scored by the GPT2-Large model (Radford et al., 2019), and English grammatical acceptability $p_{\text {gram }}$ as scored by the classifier model from Krishna et al. (2020). Both models were used as-is with no finetuning.</p>
<p>Table 6: Language coherence metrics (ROSCOE-LC).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Score</th>
<th style="text-align: left;">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Perplexity-Chain $(\boldsymbol{h})$</td>
<td style="text-align: left;">Average perplexity of all tokens in the generated reasoning steps: $1 / \mathrm{PPL}(\boldsymbol{h})$. The context used to score each token is <br> the previous tokens in the current and all previous steps. Steps are joined with a space character. To keep the range and <br> orientation consistent with the other scores we invert the perplexity.</td>
</tr>
<tr>
<td style="text-align: left;">Perplexity-Step $\left(h_{i}\right)$</td>
<td style="text-align: left;">Average perplexity of all tokens in the generated reasoning steps, where the context used to score each token is only the <br> previous tokens within the current step: $1 /\left[(1 / N) \sum_{i=0}^{N} \operatorname{PPL}\left(h_{i}\right)\right]$. To keep the range and orientation consistent with the <br> other scores we invert the perplexity.</td>
</tr>
<tr>
<td style="text-align: left;">Grammar $\left(h_{i}\right)$</td>
<td style="text-align: left;">Probability of grammatical acceptability of each step, averaged over all steps: $(1 / N) \sum_{i=0}^{N} p_{\text {gram }}\left(h_{i}\right)$.</td>
</tr>
</tbody>
</table>
<h2>5 EXPERIMENTAL SETUP</h2>
<p>Diagnostics Datasets. We construct our first category of labeled datasets by generating perturbations - i.e., deterministic modifications - on half of the reference reasoning steps and assign binary labels based on whether or not a chain has been perturbed. We select seven language understanding and entailment datasets</p>
<p>that require complex problem solving skills, and have reference step-by-step explanations: Entailment-Bank (deductive reasoning) (Dalvi et al., 2021), ProofWriter (logical reasoning) (Tafjord et al., 2021); three arithmetic reasoning datasets MATH (Hendrycks et al., 2021), ASDIV (Miao et al., 2020) and AQUA (Liang et al., 2018); EQASC (explanations for commonsense question answering) (Aggarwal et al., 2021), and StrategyQA (question answering with implicit reasoning strategies) (Geva et al., 2021) (see dataset details in App. E.1). Using our taxonomy, we introduce 12 error perturbation rules and apply on these datasets to construct our diagnostics datasets (see details in App. E.3).</p>
<p>Human Judged Datasets. We select our second category of datasets from commonly used complex reasoning tasks: GSM8K (arithmetic reasoning) (Cobbe et al., 2021), DROP (discrete reasoning) (Dua et al., 2019), ESNLI (deductive and commonsense reasoning) (Camburu et al., 2018), COSMOS-QA (commonsense reasoning) (Huang et al., 2019) and SemEVAL (Ostermann et al., 2018) (commonsense reasoning). Wei et al. (2022) provide model generated chain of thought reasoning steps for GSM8K. We used chains produced by the 175b_verification model to annotate for reasoning errors. For other datasets, we prompt GPT-3 LLM (Brown et al., 2020) with few-shot in-context examples to obtain step-by-step reasoning sequences (see examples in App. E.2). We use the error types in our taxonomy in Table 2 as human evaluation perspectives of reasoning errors where we solicit five expert annotators ${ }^{3}$. The data collection interface provided judges with the source text (e.g., source and a question, or hypothesis, premise, and a question if they entail) and associated reasoning text clearly separated into individual steps. Judges were asked to rate the chain as a whole (e.g., on overall quality) as well as each individual step (e.g., commonsense errors, contradicts with the previous steps). App. Table 16 summarizes the distribution of error types annotated by the judges. See App. F for details.</p>
<p>ROSCOE Training. To obtain reasoning step embeddings, we finetune SimCSE (Gao et al., 2021), a supervised sentence similarity model extending the RoBERTa word embedding model (Liu et al., 2019) on multi-step reasoning datasets we listed in $\S 5$ (see details in Table 11) ${ }^{4}$. SimCSE is a contrastive learning model that is trained on triplets of reference reasoning steps, positive and hard-negative hypothesis reasoning steps to minimize the cross-entropy objective with in-batch negatives. For contrastive learning, we use the context and reference reasoning steps as a positive sample $(\boldsymbol{s}, \boldsymbol{r})$, and context and perturbed reference steps $(\boldsymbol{s}, \boldsymbol{h})$ as hard-negative pairs. For finetuning, we embed source context and hypothesis chain as a whole, without splitting it into steps. With the finetuned model we embed each individual step, as well as a reasoning chain as a whole. We use the pretrained checkpoint of supervised SimCSE model sup-simcse-roberta-base to initialize our model, and further train it for five epochs on our synthetic train data (details in App. G). We also compare ROSCOE scores calculated against sup-simcse-roberta-base SimCSE model, and all-mpnet-base-v2 sentence embedding model (Reimers \&amp; Gurevych, 2019) to understand metrics sensitivity to the embedding method.</p>
<p>Baseline Metrics. We use text generation evaluation metrics as baseline metrics and comprehensively examine the ones outlined in $\S 2$, which are: $n$-gram match based metrics including ROUGE-1, ROUGE-2, and ROUGE-L (Lin, 2004); pre-trained scores including BLEURT (Sellam et al., 2020), PRISM (Thompson \&amp; Post, 2020), BERTScore (Zhang et al., 2020), BARTScore using the Faithfulness ( $\boldsymbol{s} \rightarrow \boldsymbol{h}$ ) direction for factuality and relevance, and its finetuned variant BARTScore+CNN+Para BARTScore+ (Yuan et al., 2021); and information alignment metrics of CTC, CTC-Relevancy and CTC-Consistency. We also include BARTScore-P, which we obtain by finetuneing BART (Lewis et al., 2020) on the same reasoning datasets we use for finetuning our SimCSE embedding models. Most of our ROSCOE metrics are constructed referencefree. We also have metrics that use reference reasoning steps which we examine against human judgements. We use the official code for each metric.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Meta Evaluation. We use Somers' $D^{5}$ (Somers, 1962), which measures the ordinal association between two measured quantities, to meta-evaluate each scorer against synthetic and human scores. We prefer Somers' $D$ over more commonly used Kendall's $\tau$ or Kendall's $\tau-b$, because it is better in handling the ties of a biased random variable (Agresti, 2010, Section 7.1.5), which imposes an upper bound on the possible values Kendall's $\tau(-b)$ can take. For each score $Y$ considered, our correlations are built against the biased random variable $X \in[0,1]$, represented by the perturbation or error presence indicator and evaluated using $D(Y \mid X)=\tau(X, Y) / \tau(X, X)$.</p>
<h1>6 EXPERIMENTAL RESULTS</h1>
<p>Controlled Experiments with Diagnostics Datasets. Table 7 shows Somers' $D$ correlation for metrics measured reference-free on six different datasets and compares baselines to ROSCOE-* aggregated categories calculated with finetuned embeddings: ROSCOE-SA, ROSCOE-SS, ROSCOE-LI, ROSCOE-LC. Results also include ROSCOE metrics with all-mpnet-base-v2 (ROSCOE-SA ${ }^{1}$, ROSCOE-SS ${ }^{1}$ ) and sup-simcse-robertabase (ROSCOE-SA ${ }^{2}$, ROSCOE-SS ${ }^{2}$ ) sentence embedding models. Correlations for ProofWriter are taken on its depth-5 subset. We report highest correlation scores across perturbations within each dataset. The breakdown of all ROSCOE metrics is in App. Table 18.</p>
<p>We observe that: (1) ROSCOE can outperform all other reference-free methods on all six diagnostic datasets, (2) the gains for ROSCOE-SS are more pronounced in four out of six diagnostics datasets, which suggests that ROSCOE can capture hallucinations and repetitions in step-wise reasoning. On Proofwriter, our scorers show lower correlations, because as shown in Table E.1, the context is a list of facts and rules and the reasoning steps can include unordered fact and rule combinations, but still a correct answer can be deduced. This makes it challenging for ROSCOE to evaluate the steps in sequence. Overall, the correlations of the baseline metrics are much lower than ROSCOE, because the baseline metrics are designed to capture the semantic or lexical overlap between a reference and hypothesis and it is harder to detect logical consistency without a golden reference text. ROSCOE is specifically focused on reference-free settings, and can gauge each individual step against the source and other generated steps. In fact, our met-
Table 7: Somers' $D$ correlation of different metrics on six Diagnostics datasets. Metrics are measured reference-free on $(\boldsymbol{s}, \boldsymbol{h})$. We take the maximum score over different perturbations. The two highest correlations for each dataset are bolded and underlined, respectively. Correlations that are not significant ( $p$-value $\geq 0.05$ ) are omitted when aggregating, and "-" denotes an absence of any significant correlation. Breakdown of all baseline and ROSCOE metrics is shown in App. H.1, Table 18.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">EntBank</th>
<th style="text-align: center;">Math</th>
<th style="text-align: center;">AQUA</th>
<th style="text-align: center;">ProofWr.</th>
<th style="text-align: center;">EQASC</th>
<th style="text-align: center;">ASDIV</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ROUGE-L</td>
<td style="text-align: center;">0.365</td>
<td style="text-align: center;">0.156</td>
<td style="text-align: center;">0.264</td>
<td style="text-align: center;">0.106</td>
<td style="text-align: center;">0.315</td>
<td style="text-align: center;">0.269</td>
</tr>
<tr>
<td style="text-align: left;">BLEURT</td>
<td style="text-align: center;">0.257</td>
<td style="text-align: center;">0.148</td>
<td style="text-align: center;">0.252</td>
<td style="text-align: center;">0.024</td>
<td style="text-align: center;">0.447</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">BERTScore</td>
<td style="text-align: center;">0.380</td>
<td style="text-align: center;">0.124</td>
<td style="text-align: center;">0.220</td>
<td style="text-align: center;">0.117</td>
<td style="text-align: center;">0.462</td>
<td style="text-align: center;">0.322</td>
</tr>
<tr>
<td style="text-align: left;">BARTScore</td>
<td style="text-align: center;">0.358</td>
<td style="text-align: center;">0.185</td>
<td style="text-align: center;">0.317</td>
<td style="text-align: center;">0.081</td>
<td style="text-align: center;">0.415</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">BARTScore+</td>
<td style="text-align: center;">0.315</td>
<td style="text-align: center;">0.164</td>
<td style="text-align: center;">0.251</td>
<td style="text-align: center;">0.054</td>
<td style="text-align: center;">0.297</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">BARTScore-P</td>
<td style="text-align: center;">0.186</td>
<td style="text-align: center;">0.128</td>
<td style="text-align: center;">0.215</td>
<td style="text-align: center;">0.011</td>
<td style="text-align: center;">0.276</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">PRISM</td>
<td style="text-align: center;">0.453</td>
<td style="text-align: center;">0.208</td>
<td style="text-align: center;">0.191</td>
<td style="text-align: center;">0.235</td>
<td style="text-align: center;">0.436</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">CTC Relev.</td>
<td style="text-align: center;">0.258</td>
<td style="text-align: center;">0.188</td>
<td style="text-align: center;">0.217</td>
<td style="text-align: center;">0.394</td>
<td style="text-align: center;">0.485</td>
<td style="text-align: center;">0.382</td>
</tr>
<tr>
<td style="text-align: left;">CTC Consist.</td>
<td style="text-align: center;">0.310</td>
<td style="text-align: center;">0.282</td>
<td style="text-align: center;">0.157</td>
<td style="text-align: center;">0.513</td>
<td style="text-align: center;">0.270</td>
<td style="text-align: center;">0.396</td>
</tr>
<tr>
<td style="text-align: left;">ROSCOE-SA</td>
<td style="text-align: center;">$\underline{0.919}$</td>
<td style="text-align: center;">$\underline{0.939}$</td>
<td style="text-align: center;">0.971</td>
<td style="text-align: center;">0.763</td>
<td style="text-align: center;">$\mathbf{1 . 0 0 0}$</td>
<td style="text-align: center;">$\mathbf{0 . 8 7 9}$</td>
</tr>
<tr>
<td style="text-align: left;">ROSCOE-SA ${ }^{1}$</td>
<td style="text-align: center;">0.913</td>
<td style="text-align: center;">0.936</td>
<td style="text-align: center;">0.972</td>
<td style="text-align: center;">$\underline{0.771}$</td>
<td style="text-align: center;">$\mathbf{1 . 0 0 0}$</td>
<td style="text-align: center;">0.198</td>
</tr>
<tr>
<td style="text-align: left;">ROSCOE-SA ${ }^{2}$</td>
<td style="text-align: center;">$\underline{0.919}$</td>
<td style="text-align: center;">$\underline{0.939}$</td>
<td style="text-align: center;">0.971</td>
<td style="text-align: center;">0.732</td>
<td style="text-align: center;">$\mathbf{1 . 0 0 0}$</td>
<td style="text-align: center;">0.515</td>
</tr>
<tr>
<td style="text-align: left;">ROSCOE-SS</td>
<td style="text-align: center;">$\mathbf{0 . 9 5 5}$</td>
<td style="text-align: center;">0.924</td>
<td style="text-align: center;">$\underline{0.982}$</td>
<td style="text-align: center;">0.624</td>
<td style="text-align: center;">$\mathbf{1 . 0 0 0}$</td>
<td style="text-align: center;">$\underline{0.857}$</td>
</tr>
<tr>
<td style="text-align: left;">ROSCOE-SS ${ }^{1}$</td>
<td style="text-align: center;">0.909</td>
<td style="text-align: center;">0.932</td>
<td style="text-align: center;">$\underline{0.982}$</td>
<td style="text-align: center;">0.631</td>
<td style="text-align: center;">$\mathbf{1 . 0 0 0}$</td>
<td style="text-align: center;">0.280</td>
</tr>
<tr>
<td style="text-align: left;">ROSCOE-SS ${ }^{2}$</td>
<td style="text-align: center;">0.901</td>
<td style="text-align: center;">$\mathbf{0 . 9 4 9}$</td>
<td style="text-align: center;">$\mathbf{0 . 9 9 1}$</td>
<td style="text-align: center;">0.621</td>
<td style="text-align: center;">$\mathbf{1 . 0 0 0}$</td>
<td style="text-align: center;">0.289</td>
</tr>
<tr>
<td style="text-align: left;">ROSCOE-LI</td>
<td style="text-align: center;">0.917</td>
<td style="text-align: center;">0.331</td>
<td style="text-align: center;">0.424</td>
<td style="text-align: center;">0.289</td>
<td style="text-align: center;">0.793</td>
<td style="text-align: center;">0.771</td>
</tr>
<tr>
<td style="text-align: left;">ROSCOE-LC</td>
<td style="text-align: center;">0.604</td>
<td style="text-align: center;">0.392</td>
<td style="text-align: center;">0.359</td>
<td style="text-align: center;">$\mathbf{0 . 7 8 8}$</td>
<td style="text-align: center;">$\underline{0.859}$</td>
<td style="text-align: center;">0.485</td>
</tr>
</tbody>
</table>
<p>rics also work well against the baselines in the reference-based setting (comparing against reference reasoning steps). In App. Table 19 we present correlations when metrics are measured as reference-based. We also observe that finetuning SimCSE gives highest improvements on the ASDIV dataset. ASDIV is a 1-step reasoning dataset (see App. Table 12), where step is represented by an equation with one of the arithmetic perturbations added. We</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>hypothesize that including these patterns in finetuning helped the model to better learn relationships between context and equations, and resulted in higher scores. On EQASC dataset, Repetition<em> scores are able to catch all duplicated steps in a chain, i.e., we can separate perturbed and non-perturbed chains based on the given threshold value for the Repetition</em> scores, and achieve perfect correlation scores (App. Table 20). To understand if finetuning actually helps to improve scoring, we compare non-aggregated metrics (see details in App. Table 18). We observe, that finetuning indeed helps to improve ROSCOE: on average across datasets, all correlations except Repetition_* scores improve (up to 0.556 on InformativenessChain), with mean Repetition-Token not changing, and mean Repetition-Step degrading by 0.005 . We speculate that since we finetune the model using reasoning chains and context as a whole, it helps to better capture step-by-step rationales, while possibly degrading on word and sentence-level semantics.</p>
<p>Meta-Evaluations on Human Judgement Datasets. Table 8 reports a summary of meta-evaluation of ROSCOE metrics comparing against baselines on human judged datasets. The correlations are measured based on the presence of a particular error from Table 2 and we report the highest correlation across all error types within each dataset. We observe that: (1) on all tasks, ROSCOE metrics outperform all other baselines when evaluated as reference-free; (2) overall, ROSCOE yields considerably better correlations, which indicates that step-by-step reasoning generations can be more effectively evaluated with ROSCOE. In general, most correlations with human judgements are moderate when compared to the synthetic correlation scores, indicating that step-by-step reasoning evaluation is among the cognitively hard tasks for neural models (Deutsch et al., 2022). Interpretable metrics such as ROSCOE can
Table 8: Somers' $D$ correlations of metrics with human judgement. We report the maximum over the error types in Table 2. All metrics are measured reference-free on $\langle\boldsymbol{s}, \boldsymbol{h}\rangle$. The highest two correlations in each column are bolded and underlined, respectively. Correlations that are not significant ( $p$-value $\geq 0.05$ ) are omitted when aggregating, and "-" denotes an absence of any significant correlation. Breakdown of all baseline and ROSCOE metrics is shown in App. H.2.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">DROP</th>
<th style="text-align: center;">GSM8K</th>
<th style="text-align: center;">ESNLI</th>
<th style="text-align: center;">COSMOS</th>
<th style="text-align: center;">SemEVAL</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Rouge-L</td>
<td style="text-align: center;">0.278</td>
<td style="text-align: center;">0.252</td>
<td style="text-align: center;">0.557</td>
<td style="text-align: center;">-0.441</td>
<td style="text-align: center;">-0.478</td>
</tr>
<tr>
<td style="text-align: left;">BLEURT</td>
<td style="text-align: center;">0.328</td>
<td style="text-align: center;">0.256</td>
<td style="text-align: center;">0.541</td>
<td style="text-align: center;">0.218</td>
<td style="text-align: center;">-0.356</td>
</tr>
<tr>
<td style="text-align: left;">BERTScore</td>
<td style="text-align: center;">0.275</td>
<td style="text-align: center;">0.235</td>
<td style="text-align: center;">0.590</td>
<td style="text-align: center;">-0.420</td>
<td style="text-align: center;">-0.295</td>
</tr>
<tr>
<td style="text-align: left;">BARTScore</td>
<td style="text-align: center;">-0.835</td>
<td style="text-align: center;">-0.546</td>
<td style="text-align: center;">0.549</td>
<td style="text-align: center;">-0.544</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">BARTScore+</td>
<td style="text-align: center;">-0.665</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.482</td>
<td style="text-align: center;">-0.186</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">BARTScore-P</td>
<td style="text-align: center;">-0.642</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.255</td>
<td style="text-align: center;">-0.207</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">PRISM</td>
<td style="text-align: center;">-0.733</td>
<td style="text-align: center;">-0.455</td>
<td style="text-align: center;">0.580</td>
<td style="text-align: center;">-0.376</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">CTC-Relevance</td>
<td style="text-align: center;">0.333</td>
<td style="text-align: center;">-0.371</td>
<td style="text-align: center;">0.334</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-0.349</td>
</tr>
<tr>
<td style="text-align: left;">CTC-Consistency</td>
<td style="text-align: center;">0.462</td>
<td style="text-align: center;">-0.174</td>
<td style="text-align: center;">$\underline{0.647}$</td>
<td style="text-align: center;">0.275</td>
<td style="text-align: center;">-0.301</td>
</tr>
<tr>
<td style="text-align: left;">ROSCOE-SA</td>
<td style="text-align: center;">0.578</td>
<td style="text-align: center;">0.392</td>
<td style="text-align: center;">0.521</td>
<td style="text-align: center;">0.555</td>
<td style="text-align: center;">0.337</td>
</tr>
<tr>
<td style="text-align: left;">ROSCOE-SA ${ }^{1}$</td>
<td style="text-align: center;">0.790</td>
<td style="text-align: center;">0.500</td>
<td style="text-align: center;">$\mathbf{0 . 7 9 9}$</td>
<td style="text-align: center;">0.638</td>
<td style="text-align: center;">0.485</td>
</tr>
<tr>
<td style="text-align: left;">ROSCOE-SA ${ }^{2}$</td>
<td style="text-align: center;">0.578</td>
<td style="text-align: center;">0.392</td>
<td style="text-align: center;">0.599</td>
<td style="text-align: center;">0.555</td>
<td style="text-align: center;">0.337</td>
</tr>
<tr>
<td style="text-align: left;">ROSCOE-SS</td>
<td style="text-align: center;">$\underline{0.824}$</td>
<td style="text-align: center;">$\underline{0.514}$</td>
<td style="text-align: center;">0.530</td>
<td style="text-align: center;">0.593</td>
<td style="text-align: center;">0.411</td>
</tr>
<tr>
<td style="text-align: left;">ROSCOE-SS ${ }^{1}$</td>
<td style="text-align: center;">0.791</td>
<td style="text-align: center;">0.471</td>
<td style="text-align: center;">0.507</td>
<td style="text-align: center;">$\underline{0.642}$</td>
<td style="text-align: center;">0.508</td>
</tr>
<tr>
<td style="text-align: left;">ROSCOE-SS ${ }^{2}$</td>
<td style="text-align: center;">$\underline{0.799}$</td>
<td style="text-align: center;">$\mathbf{0 . 6 3 8}$</td>
<td style="text-align: center;">0.531</td>
<td style="text-align: center;">$\mathbf{0 . 6 5 8}$</td>
<td style="text-align: center;">$\mathbf{0 . 5 3 5}$</td>
</tr>
<tr>
<td style="text-align: left;">ROSCOE-L1</td>
<td style="text-align: center;">0.584</td>
<td style="text-align: center;">0.345</td>
<td style="text-align: center;">0.531</td>
<td style="text-align: center;">0.444</td>
<td style="text-align: center;">0.372</td>
</tr>
<tr>
<td style="text-align: left;">ROSCOE-LC</td>
<td style="text-align: center;">0.205</td>
<td style="text-align: center;">-0.184</td>
<td style="text-align: center;">0.447</td>
<td style="text-align: center;">-0.212</td>
<td style="text-align: center;">$\underline{0.517}$</td>
</tr>
</tbody>
</table>
<p>provide better information about a model's reasoning skills, thus future work should improve such metrics on aligning with human judgments. In App. H.2, we show fine-grained experimental analysis per each human labeled dataset. Specific examples showcasing ROSCOE scoring abilities are summarized in Table 40.</p>
<h1>7 ANALYSIS</h1>
<p>How sensitive are ROSCOE metrics against level of errors? To evaluate how well metric values match human assessment of reasoning, we measure sensitivity to the level of errors. We perturb sentences in the MATH (arithmetic) and EntailmentBank (deductive reasoning) diagnostic datasets (similar to $\S 5$ ) and inject different levels of errors into the reasoning text. Using randomly selected perturbation types, we construct up to a maximum of 3 perturbations per instance. We measure the correlation (Somers' $D$ ) between the reasoning inconsistency level 1, 2, 3 of the reasoning steps (i.e., the number of injected errors) and the metric score. Fig. 1 illustrates the results averaged over different perturbations.</p>
<p>We expect the metrics correlate with humans better when the level of errors is high. Both semantic alignment of the reasoning ROSCOE-SA, and the semantic similarity metrics ROSCOE-SS show consistent behavior</p>
<p>on both datasets, while baseline metrics fluctuate with low correlations. Baseline metrics perform better on EntailmentBank. On MATH, ROSCOE-LC and the baseline metrics show minimal impact, which can be that some of the perturbations applied on the MATH dataset (e.g., RandomOperation, or ShuffleNumbers) are harder to detect with language model based (BARTScore) and NLI model based (ROSCOE-LC) metrics.</p>
<p>What does ROSCOE illuminate about scores across errors and tasks? For an ideal scorer based on ease of use, it would be possible to pick a set of fixed thresholds that had error discrimination power across datasets. However, we show that this dataset-agnostic ideal is currently not possible and an issue endemic across scores, including baselines. We study which metrics correlate strongly with which perturbations, with a focus of consistency across datasets. From this, we plot the interquartile ranges for strongly correlated metric and perturbation pairs. We show a sample of these in Fig. 2, though find that the trends generally hold across metrics and perturbations (see Fig 6). We note that within a given dataset, scores are well separated: the perturbed version of a dataset for a given score and perturbation type shows little interquartile overlap with the original version. However, this does not hold across datasets - e.g., in (Score: InfoChain, Perturbation: Repetition), if one were to set a detective threshold for the Repetition perturbation based off EntBank (around $0.95$ ), it would mark almost all values of EQASC as perturbed, even non-perturbed samples. This shows the challenge of using metrics for classification without calibration for drifts in both mean and variance across datasets, even if a metric generally correlates well with detecting a given error.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Sensitivity of selected metrics on Somers' $D$ by injecting levels of error into reasoning steps.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Box-and-whisker plots of interquartile ranges of scores, for perturbations and reference-free metrics with strong Somers' $D$ values. Scores are split by dataset and perturbation use. While interquartile ranges separate well by perturbation use within a single dataset, there is overlap across datasets. This shows the drift of neural scores across datasets and applies to both ROSCOE (left, center) and strong baselines (right).</p>
<h1>8 CONCLUSION</h1>
<p>In this paper, we introduce ROSCOE, a new suite of interpretable, unsupervised metrics that enables evaluation of step-by-step reasoning generations of LMs when no golden reference generation exists. We present a taxonomy of reasoning errors used to generate and evaluate our metrics. Experimental results, from evaluating on both synthetic and human-labeled datasets exhibiting multiple types of reasoning (commonsense, arithmetic, and logical inference, etc.), demonstrate superior performance compared to prior semantic and lexical similarly based baseline metrics for text generation. Our analysis shows improved capability in evaluation of reasoning exhibiting nuances, such as factual and logical errors in step-wise decisions.</p>
<h1>ETHICS STATEMENT</h1>
<p>Explainability builds transparency and trust for users, eases bug-fixing and shortens improvement cycles for metric designers, and will be required by law/regulations for AI systems to be applied to large-scale, high-stakes domains. In this context, we hope our work will catalyze efforts on the topic of explainable evaluation metrics for language model rationale generations. We should mention that our evaluation metrics do not monitor the explanations from integrity or bias perspectives. Our work also uses five human expert annotators and in the annotation process, annotators need to rate the model generated candidate rationals. While the model-generated explanations can produce potentially unsafe content, the datasets for annotations include domains related to logical and arithmetic concepts and general commonsense knowledge. The anecdotal consensus was that the generations were safe and didn't include biased statements.</p>
<h2>REPRODUCIbility STATEMENT</h2>
<p>To ensure the reproducibility of our empirical results, we will open source our code to Github, which will contain: instructions for installing the virtual environment, data preprocessing, all score generation and correlation scripts (both for ROSCOE and baselines), and trained embedding models. Detailed explanation of all the finetuned models and metrics are given in the main paper as well as in the Appendices. We will also release all the diagnostic and human judgment datasets used in our experiments.</p>
<h2>REFERENCES</h2>
<p>Shourya Aggarwal, Divyanshu Mandowara, Vishwajeet Agrawal, Dinesh Khandelwal, Parag Singla, and Dinesh Garg. Explanations for CommonsenseQA: New Dataset and Models. 2021.</p>
<p>Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, and Weiwei Guo. <em>SEM 2013 shared task: Semantic textual similarity. In Second Joint Conference on Lexical and Computational Semantics (</em>SEM), Volume 1: Proceedings of the Main Conference and the Shared Task: Semantic Textual Similarity, pp. 32-43, Atlanta, Georgia, USA, June 2013. Association for Computational Linguistics. URL https: //aclanthology.org/S13-1004.</p>
<p>Alan Agresti. Analysis of ordinal categorical data, volume 656. John Wiley \&amp; Sons, 2010.
Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pp. 632-642, Lisbon, Portugal, September 2015. Association for Computational Linguistics. doi: 10.18653/v1/D15-1075. URL https://aclanthology.org/ D15-1075.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. 33:1877-1901, 2020. URL https://proceedings.neurips.cc/paper/ 2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.</p>
<p>Oana-Maria Camburu, Tim Rocktäschel, Thomas Lukasiewicz, and Phil Blunsom. e-snli: Natural language inference with natural language explanations. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems 31, pp. 95399549. Curran Associates, Inc., 2018. URL http://papers.nips.cc/paper/8163-e-snli-natural-language-inference-with-natural-language-explanations.pdf.</p>
<p>Asli Celikyilmaz, Elizabeth Clark, and Jianfeng Gao. Evaluation of text generation: A survey. CoRR, abs/2006.14799, 2020. URL https://arxiv.org/abs/2006.14799.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.</p>
<p>Miruna-Adriana Clinciu, Arash Eshghi, and Helen Hastie. A study of automatic metrics for the evaluation of natural language explanations. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pp. 2376-2387, Online, April 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.eacl-main.202. URL https:// aclanthology.org/2021.eacl-main. 202.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.</p>
<p>Antonia Creswell, Murray Shanahan, and Irina Higgins. Selection-inference: Exploiting large language models for interpretable logical reasoning. arXiv, 2022. URL https://arxiv.org/abs/2205.09712.</p>
<p>Bhavana Dalvi, Peter Jansen, Oyvind Tafjord, Zhengnan Xie, Hannah Smith, Leighanna Pipatanangkura, and Peter Clark. Explaining answers with entailment trees. EMNLP, 2021.</p>
<p>Marina Danilevsky, Kun Qian, Ranit Aharonov, Yannis Katsis, Ban Kawas, and Prithviraj Sen. A survey of the state of explainable AI for natural language processing. In Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, pp. 447-459, Suzhou, China, December 2020. Association for Computational Linguistics. URL https://aclanthology.org/2020.aacl-main. 46.</p>
<p>Mingkai Deng, Bowen Tan, Zhengzhong Liu, Eric P. Xing, and Zhiting Hu. Compression, transduction, and creation: A unified framework for evaluating natural language generation. In EMNLP, 2021. URL https://aclanthology.org/2021.emnlp-main.599.pdf.</p>
<p>Daniel Deutsch, Rotem Dror, and Dan Roth. Re-examining system-level correlations of automatic summarization evaluation metrics. arXiv preprint arXiv:2204.10216, 2022.</p>
<p>Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 2368-2378, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1246. URL https: //aclanthology.org/N19-1246.</p>
<p>Tianyu Gao, Xingcheng Yao, and Danqi Chen. Simcse: Simple contrastive learning of sentence embeddings. arXiv preprint arXiv:2104.08821, 2021.</p>
<p>Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies. Transactions of the Association for Computational Linguistics, 9:346-361, 2021.</p>
<p>Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. NeurIPS, 2021.</p>
<p>Lifu Huang, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Cosmos QA: Machine reading comprehension with contextual commonsense reasoning. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 2391-2401, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1243. URL https://aclanthology.org/D19-1243.</p>
<p>Alon Jacovi and Yoav Goldberg. Aligning faithful interpretations with their social attribution. volume 9, pp. 294-310, Cambridge, MA, 2021. MIT Press. doi: 10.1162/tacl_a_00367. URL https: //aclanthology.org/2021.tacl-1.18.</p>
<p>Alon Jacovi, Swabha Swayamdipta, Shauli Ravfogel, Yanai Elazar, Yejin Choi, and Yoav Goldberg. Contrastive explanations for model interpretability. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 1597-1611, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.120. URL https://aclanthology.org/2021.emnlp-main.120.</p>
<p>Kalpesh Krishna, John Wieting, and Mohit Iyyer. Reformulating unsupervised style transfer as paraphrase generation. In Empirical Methods in Natural Language Processing, 2020.</p>
<p>Andrew K Lampinen, Nicholas Roy, Ishita Dasgupta, Stephanie Cy Chan, Allison Tam, James Mcclelland, Chen Yan, Adam Santoro, Neil C Rabinowitz, Jane Wang, and Felix Hill. Tell me why! Explanations support learning relational and causal structure. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 11868-11890. PMLR, 17-23 Jul 2022. URL https://proceedings.mlr.press/v162/lampinen22a.html.</p>
<p>Moritz Laurer, Wouter van Atteveldt, Andreu Casas, and Kasper Welbers. Less annotating, more classifyingaddressing the data scarcity issue of supervised machine learning with deep transfer learning and bert-nli. 2022 .</p>
<p>Christoph Leiter, Piyawat Lertvittayakumjorn, Marina Fomicheva, Wei Zhao, Yang Gao, and Steffen Eger. Towards explainable evaluation metrics for natural language generation. CoRR, abs/2203.11131, 2022. URL https://doi.org/10.48550/arXiv.2203.11131.</p>
<p>Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 7871-7880, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.703. URL https://aclanthology.org/2020.aclmain. 703 .</p>
<p>Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language models. arXiv preprint arXiv:2206.14858, 2022.</p>
<p>Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. On the advance of making language models better reasoners. arXiv, 2022. URL https://arxiv.org/abs/ 2206.02336 .</p>
<p>Chao-Chun Liang, Yu-Shiang Wong, Yi-Chung Lin, and Keh-Yih Su. A meaning-based statistical English math word problem solver. pp. 652-662, June 2018. doi: 10.18653/v1/N18-1060. URL https: //aclanthology.org/N18-1060.</p>
<p>Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pp. 74-81, Barcelona, Spain, July 2004. Association for Computational Linguistics. URL https : //aclanthology.org/W04-1013.</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.</p>
<p>Shen-yun Miao, Chao-Chun Liang, and Keh-Yih Su. A diverse corpus for evaluating and developing english math word problem solvers. pp. 975-984, 2020.</p>
<p>Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114, 2021.</p>
<p>Juri Opitz and Anette Frank. Towards a decomposable metric for explainable evaluation of text generation from AMR. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pp. 1504-1518, Online, April 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.eacl-main.129. URL https://aclanthology.org/ 2021.eacl-main. 129 .</p>
<p>Simon Ostermann, Michael Roth, Ashutosh Modi, Stefan Thater, and Manfred Pinkal. Semeval-2018 task 11: Machine comprehension using commonsense knowledge. In *SEMEVAL, 2018. URL https : //aclanthology.org/S18-1119.pdf.</p>
<p>Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.</p>
<p>Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 11 2019. URL https://arxiv.org/abs/1908.10084.</p>
<p>Ehud Reiter. Natural language generation challenges for explainable AI. In Proceedings of the 1st Workshop on Interactive Natural Language Technology for Explainable Artificial Intelligence (NL4XAI 2019), pp. 3-7. Association for Computational Linguistics, 2019. doi: 10.18653/v1/W19-8402. URL https : //aclanthology.org/W19-8402.</p>
<p>Christopher Michael Rytting and David Wingate. Leveraging the inductive bias of large language models for abstract textual reasoning. 2021. URL https://openreview.net/forum?id=urueR03nkng.</p>
<p>Thibault Sellam, Dipanjan Das, and Ankur Parikh. BLEURT: Learning robust metrics for text generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 7881-7892, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.704. URL https://aclanthology.org/2020.acl-main. 704.</p>
<p>Robert H Somers. A new asymmetric measure of association for ordinal variables. American sociological review, pp. 799-811, 1962.</p>
<p>Oyvind Tafjord, Bhavana Dalvi, and Peter Clark. ProofWriter: Generating implications, proofs, and abductive statements over natural language. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pp. 3621-3634, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.findings-acl.317. URL https://aclanthology.org/2021.findingsacl. 317 .</p>
<p>Brian Thompson and Matt Post. Automatic machine translation evaluation in many languages via zeroshot paraphrasing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 90-121, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.8. URL https://aclanthology.org/2020.emnlp-main.8.</p>
<p>Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, Stéfan J. van der Walt, Matthew Brett, Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew R. J. Nelson, Eric Jones, Robert Kern, Eric Larson, C J Carey, Ilhan Polat, Yu Feng, Eric W. Moore, Jake VanderPlas, Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E. A. Quintero, Charles R. Harris, Anne M. Archibald, Antônio H. Ribeiro, Fabian Pedregosa, Paul van Mulbregt, and SciPy 1.0 Contributors. SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python. Nature Methods, 17:261-272, 2020. doi: 10.1038/s41592-019-0686-2.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022.</p>
<p>Sean Welleck, Jiacheng Liu, Ximing Lu, Hannaneh Hajishirzi, and Yejin Choi. Naturalprover: Grounded mathematical proof generation with language models. 2022.</p>
<p>Peter West, Chandra Bhagavatula, Jack Hessel, Jena Hwang, Liwei Jiang, Ronan Le Bras, Ximing Lu, Sean Welleck, and Yejin Choi. Symbolic knowledge distillation: from general language models to commonsense models. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 4602-4625, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.341. URL https://aclanthology.org/2022.naacl-main.341.</p>
<p>Weizhe Yuan, Graham Neubig, and Pengfei Liu. Bartscore: Evaluating generated text as text generation. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, volume 34, pp. 27263-27277. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper/2021/file/ e4d2b6e6fdeca3e60e0f1a62fee3d9dd-Paper.pdf.</p>
<p>Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D. Goodman. Star: Bootstrapping reasoning with reasoning. arXiv, 2022.</p>
<p>Tianyi Zhang, Varsha Kishore, Felix Wu*, Kilian Q. Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation with bert. In International Conference on Learning Representations, 2020. URL https: //openreview.net/forum?id=SkeHuCVFDr.</p>
<h1>APPENDIX</h1>
<h2>A Limitations</h2>
<p>Our study is the first initial step that investigates the evaluation of the step-by-step reasoning produced by large language models. Our taxonomy (in Table 2) covers several reasoning errors and we designed our metrics to evaluate a spectrum of criteria including the ones in the taxonomy. Even though we cannot say we cover all possible reasoning errors, our metrics are generic enough, work on natural language rationales, and consider the alignment with the input context and the generated explanation. Nevertheless, we believe our study can spur others to investigate different reasoning errors and use our code and datasets as templates to extend further.</p>
<p>Due to the extensive analysis needed to thoroughly test and communicate the ability of our proposed metrics to capture reasoning errors, we decided to leave some follow-up questions, such as the application of these metrics for improving downstream task performance, for future exploration.</p>
<h2>B Few-shot Prompting Examples (Cont. From § 1)</h2>
<p>Below is the 2-shot example we used to generate the explanations from GPT-3 as we show in the Fig. 1.</p>
<p>I will explain how to make logical inferences:
Context: John wakes to the sound of his children giggling outside the door. They bust in and say "Happy Father's Day!" with a plate full of food. 2022 has been a great year.
Question: Can we infer that John was born before the inauguration of Barack Obama?
Explanation: We can infer that John is a father because of "his children". We can infer that the current year is 2022. Barack Obama was inaugurated in 2009, so it has been about 13 years since Obama was inaugurated. John's children are old enough to speak, so if John were born after Obama's inauguration, he would have been younger than 10 when he had children. So, we can infer that John was born before Obama's inauguration. The answer is "yes".</p>
<p>Context: Sam sees a piano in the subway stations. Laughing, his friends prod him to play, saying "Remember how amazing your rendition of Chopsticks was at the Christmas party?"
Question: Can we infer that John majored in piano at Juilliard?
Explanation: Juilliard is a top school for music, so someone who majored in piano at Juilliard would be an expert pianist. "Chopsticks" is a very simple song to play on the piano. His friends were therefore likely being sarcastic when they told him to play, which would imply that Sam was not an expert piano player. So it is unlikely that Sam majored in piano at Juilliard. The answer is "no".</p>
<p>Table 9 illustrates sample if in-context examples we used as prompts to obtain step-wise explanations.</p>
<p>Table 9: Example prompts used to generate step-by-step explanations using GPT-3.
(Used for ESNLI dataset)
Explain if premise entails hypothesis.
Premise: Tomato sauce (also known as Neapolitan sauce, and referred to in Italy as Salsa di pomodoro) refers to any of a very large number of sauces made primarily from tomatoes, usually to be served as part of a dish (rather than as a condiment). Tomato sauces are common for meat and vegetables, but they are perhaps best known as sauces for pasta dishes.
Hypothesis: Tomato sauce is only rarely used to top pasta.
Explanation: Salsa di pomodoro is made of peppers. The answer is No.
Premise: Kirk Johnson (born 1960) is an American scientist, author, curator, and museum administrator, and is currently serving as Director of Smithsonian's National Museum of Natural History. Dr. Johnson is the host of the PBS Nova series, Making North America, which is a three-part series that describes the shaping of North America, which aired on November 4, 11 and 18, 2015.
Hypothesis: johnson hosted Making North America when he turned 50.
Explanation: Johnson was born in 1960 and hosted Making North America in 2015, so he was 55, not 50. The answer is No.</p>
<p>Premise: A person rides his bicycle in the sand beside the ocean.
Hypothesis: A person is on a beach.
Explanation: A person rides beside the ocean means is on a beach if there is sand beside the ocean. The answer is Yes.
(used for SEMEVAL dataset)
Explain if premise entails hypothesis.
Premise: I went into my bedroom and flipped the light switch. Oh, I see that the ceiling lamp is not turning on. It must be that the light bulb needs replacement. I go through my closet and find a new light bulb that will fit this lamp and I place it in my pocket. I also get my stepladder and I place it under the lamp. I make sure the light switch is in the off position. I climb up the ladder and unscrew the old light bulb. I place the old bulb in my pocket and take out the new one. I then screw in the new bulb. I climb down the stepladder and I place it back into the closet. I then throw out the old bulb into the recycling bin. I go back to my bedroom and turn on the light switch. I am happy to see that there is again light in my room. Which room did the light go out in?
Hypothesis: Bedroom
Label: Correct
Explanation: The answer is bedroom, because the light went out in the bedroom. The person replaced the light bulb in the bedroom.</p>
<p>Premise: The weather was so nice today that I decided to have a barbecue. I called up some of my friends to invite them over. I set up some chairs and tables outside for my friends to sit around. I then took out some hot dogs from my fridge and brought them over to my barbecue. My barbecue is a gas barbecue, so all I had to do was press the knob and turn it to ignite it. Once I got a little fire going I put my hot dogs on the grill over it to cook. I closed the lid and left my hot dogs to cook for about ten minutes. After ten minutes I decided to check on them to see if they were done. They looked like they were cooked, so I put them all on a plate to serve to my friends. I went to my kitchen and got hot dog buns, ketchup, mustard, and relish to serve on top of the hot dogs. I laid out all of the food and condiments on the table so my friends could help themselves. We ate the hot dogs and relaxed around the table. Why are we having the barbecue, to celebrate an occasion, or just because we want to?
Hypothesis: Just because they want to.
Label: Correct
Explanation: The answer is correct, because the weather was nice. They decided to have a barbecue. There is no mention of any special occasion.</p>
<p>Premise: Another Nurse can in to look at me, took one look at my rash and said OMG and ran out of the room. At that point I started crying, regretted coming to the hospital alone and started wheezing again. Then the doctor came in, her eyes were all bugged out. What will I do after the doctor examines me?
Hypothesis: I will talk with my friends .
Label: Incorrect
Explanation: The premise is about the nurse's reaction to the patient's rash, and how the patient feels. The hypothesis does not follow up from this premise correctly. Instead, it talks about talking with friends.</p>
<h1>C TAXONOMY OF REASONING ERRORS (CONT. FROM § 3)</h1>
<p>To gain deeper insights into the types of reasoning errors introduced by LLMs while explaining their decisions, we propose a new taxonomy of generic reasoning errors for language problem solving. Specifically, we sampled from the training portions of the logical inference and commonsense reasoning datasets, and prompted GPT-3 with reasoning explanations using prompts similar to App. B. We used task specific indomain examples for prompting. We also analyzed model generated explanations shared in Wei et al. (2022). We then manually looked into each explanation and identified potential errors that are inconsistent with the source, question or the prompt and within the reasoning chain. Some tasks require a model to classify the logical relationship between premise and a hypothesis, others are question and answering tasks. We adjusted our context and prompts according to the type of the task.</p>
<p>Our reasoning error taxonomy is summarized in Table 10. It contains types of errors concerning an overall chain or an individual step. Specifically, the chain-level coarse-grained evaluations of the overall reasoning chain deals with overall quality of the step-by-step thinking, coherence, consistency of the explanation within itself, and consistency with the context, etc. On the other hand the step-level fine-grained evaluations focus on the consistency of a reasoning step with the previous steps, if a step conveys new and supporting information over the previous steps, factuality or logical inference issues. We use these error categories to construct diagnostics datasets with perturbed errors as well as human judged datasets of reasoning errors. In the taxonomy, we indicate *-step level errors to differentiate from the chain level error types.</p>
<p>Table 10: Taxonomy of Step-by-Step Reasoning Errors. Errors used for perturbations in constructing the diagnostic datasets (Diag.) and for human annotation (Human) of the model generated reasoning chains are also marked. (Cont. from Table 2.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Error Type</th>
<th style="text-align: center;">Description</th>
<th style="text-align: center;">Example</th>
<th style="text-align: center;">Diag.</th>
<th style="text-align: center;">Human</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Grammar</td>
<td style="text-align: center;">Faulty, unconventional, or controversial grammar usage</td>
<td style="text-align: center;">Reference: He chews 4 pieces of gum a day so over 30 days he will chew $4 * 30=« 4 * 30=120» 120$ pieces of gum. <br> Model Expl: He eats 4 pieces of gum a day and lasts 30 days so he eats $4 * 30=« 4 * 30=120 » 120$ pieces of gum.</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">Factuality</td>
<td style="text-align: center;">Information about object (i.e. quantity, characteristics) or a personal named entity does not match information provided in the question</td>
<td style="text-align: center;">Context: Anakin caught 10 fish. <br> Model Expl: Locsin caught 10 fish.</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">Hallucination</td>
<td style="text-align: center;">Information is not provided in the problem statement and is irrelevant or wrong</td>
<td style="text-align: center;">Context: The basketball team went to the steakhouse to eat dinner. The first player ate a 6 -ounce steak. The second player ate beef tips, containing 8 beef tips, each an ounce in size. The third player ate a one-pound steak. And the fourth and fifth players ordered vegetarian meals. In total, how many ounces of meat were consumed by the team? <br> Model Expl: The fourth and fifth players ordered vegetarian meals, for a total of 2 ounces of meat.</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">Redundancy</td>
<td style="text-align: center;">Explanation contains redundant information, which even though might be factual, is not required to answer the question</td>
<td style="text-align: center;">Context: Bob is cold. If something is quiet then it is blue. All blue things are round. If something is round and smart then it is cold. Question: Bob is cold. <br> Model Expl: If something is round and not smart then it is blue. All blue things are round. Answer: true.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">Repetition</td>
<td style="text-align: center;">Step paraphrases information already mentioned in previous reasoning steps</td>
<td style="text-align: center;">Context: Parker chews 4 pieces of gum a day. A pack of gum has 15 pieces of chewing gum per pack. How many packs of gum will he need to last him 30 days? <br> Model Expl.: He chews 4 pieces of gum a day and there are 30 days in a month so he chews $4 * 30=120$ pieces of gum a month. Parker chews 120 pieces of gum a month. He needs 120 pieces of gum a month and a pack of gum has 15 pieces of gum so he will need $120 / 15=8$ packs of gum.</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">Missing step</td>
<td style="text-align: center;">The content of the generated reasoning is incomplete and lacks required information to produce the correct answer. If this missing steps are added, the model would produce the correct answer</td>
<td style="text-align: center;">Context: A farm has 102 -legged animals and 154 -legged animals. If all animals are healthy, how many pairs of animal legs in total are on the farm? Answer: 40 <br> Model Expl.: 10 2-legged animals have 10*2=20 legs. 15 4-legged animals have $15 * 4=60$ legs. In total on the farm, there are $20+60=80$ legs.Answer: 80 missing step: Total number of pairs is $80 / 2=40$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">Coherency</td>
<td style="text-align: center;">Steps contradict each other or do not follow a cohesive story</td>
<td style="text-align: center;">Context: If the reactor is intact, then it is safe. If the reactor is safe, then no graphite is outside it. <br> Model Expl.: The reactor is intact. All graphite is inside of it. There are some graphite outside of the the reactor.</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">Commonsense</td>
<td style="text-align: center;">Model lacks relations that should be known from general knowledge about the world (i.e. how to compute velocity, how many inches in one foot, all ducks are birds, etc.)</td>
<td style="text-align: center;">Question: Jordan has 2 children who wear diapers. Each child requires 5 diaper changes per day. Jordan's wife changes half of the diapers. How many diapers does Jordan change per day? <br> Model Expl.: Jordan has $2 * 5=10$ diapers to change per day. Jordan's wife changes $10 / 2=5.0$ diapers per day. For both children, Jordan changes $10+5=15$ diapers per day.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">Arithmetic</td>
<td style="text-align: center;">Error in math calculations</td>
<td style="text-align: center;">Reference: If a truck was carrying 20 tons of fertiliser packed in bags, the total number of bags in a truck is $20 * 20=400$ bags <br> Model Expl: If a truck was carrying 20 tons of fertiliser packed in bags, the total number of bags in a truck is $20 * 20$ $=40$ bags</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
</tbody>
</table>
<h1>D ROSCOE METRICS DETAILS (CONT. FROM §4)</h1>
<p>ROSCOE metrics are constructed under four categories: semantic alignment, semantic similarity, logical inference, and logical coherence. The details of each metric is explained in $\S 4$. At the core of ROSCOE semantic alignment metrics is the reasoning alignment score, which we designed to measure the grounding of step-by-step reasoning with respect to the source text. Fig. 3 illustrates the reasoning alignment.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Reasoning alignment illustrating the measurement of the Faithfulness-Step and Faithfulness-Token semantic alignment scores. $\boldsymbol{h}=\left{h_{1}, h_{2}\right}$ is a hypothesis chain with tokens $\left{h_{1,1}, h_{1,2}, h_{1,3}, h_{2,1}, h_{2,2}\right}$, and $\boldsymbol{s}=\left{s_{1}, s_{2}, s_{3}\right}$ is a context with tokens $\left{s_{1,1}, s_{2,1}, s_{2,2}, s_{2,3}, s_{3,1}, s_{3,2}, s_{3,3}\right}$. Alignment scores from hypothesis to context are highlighted, and alignment scores from context to hypothesis are underscored. The reasoning alignment combines token and step level similarities where each alignment value (cell) is the cosine similarity and explicitly measures the grounding of the token and step-wise reasoning with respect to the source text.
The variation of scorers of the ROSCOE shares some similarities, thus we explain them here:
BARTScore (Yuan et al., 2021) claims that more high level text can be generated using sequence to sequence model. It can support different evaluation perspectives such as factuality (by evaluating from source to hypothesis) or informativeness (by evaluating from both directions between reference and hypothesis). BARTScore is used to measure the probability of generated text from a source text $x$ to a target set $y$ :</p>
<p>$$
B A R T S c o r e=\sum_{t=1}^{m} w_{t} \log p\left(y_{t} \mid y_{&lt;t}, x, \theta\right)
$$</p>
<p>BARTScoreintroduce two variations: (1) finetuning, in which the BART model is finetuned on the task specific dataset to make the pre-training domain closer to the evaluation domain. (2) prompting, in which a task specific textual prompt is appended to the source $x$ to get the $y$. In our experiments we compare the the BARTScorebaseline and one with the prompting variant BARTScore+to compare in the experiments.</p>
<p>CTC (Compression, Transduction, and Creation) (Deng et al., 2021), is a suite of metrics that unifies different perspectives of different tasks (e.g, summarization, style transfer, or text rewriting) into information alignment, which measures weather the information in one generation component is grounded in another. The information alignment is defined as follows: let $x$ (e.g, dialog context) be the source input, $c$ (e.g., external world knowledge) be some additional context, and $y$ be the generated output text (e.g., generated response). The alignment is measured on token level and it is measured as the vector of scores:</p>
<p>$$
\operatorname{align}(a \rightarrow b)=\left\langle\alpha_{1}, \cdots, \alpha_{N}\right\rangle
$$</p>
<p>where each score $\alpha_{i}$ indicates confidence that the $n$-th token in $a$ aligns with the whole sentence $b$. Using the information alignment they define a list of metrics to evaluate text for different tasks. In our experiments we use two of these metrics that are closer to ROSCOE: the Relevance (CTC Relevance), which measures the consistency of the generated text with the source and its balanced between the reference, and the Consistency (CTC Consistency) which deals with the faithfullness of the generated text to the input context by the alignment between the two.</p>
<h1>E EXPERIMENTAL SETUP DETAILS (CONT. FROM § 5)</h1>
<h2>E. 1 DiAGNOSTIC DATASETS</h2>
<p>In the following we present details of each diagnostics dataset used in our work. Table 11 illustrates how each dataset is used in our experiments. StrategyQA dataset is only used to finetune the SimCSE embeddings model, because it contains reference reasoning chains in train and validation partitions, but not in the test partition. The rest of the six diagnostic datasets are used for sentence embedding model finetuning, and evaluating our models as presented in the experiments results. All datasets with examples are summarised in Table 12 .</p>
<p>Table 11: Summary of datasets used in our work. Reasoning Chain represent whether it contains human written golden step-wise reasoning explanation. Type indicates whether it is used for constructing Diagnostic or Human judged datasets. Train/Val./Test indicate whether the dataset is used for training, validation and/or testing. StrategyQA dataset is only used for finetuning SimCSE embedding model.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: center;">Reasoning <br> Chain</th>
<th style="text-align: center;">Type</th>
<th style="text-align: center;">Train</th>
<th style="text-align: center;">Val.</th>
<th style="text-align: center;">Test</th>
<th style="text-align: center;">Annotated <br> Instances</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">EntailmentBank (Dalvi et al., 2021)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Diagnostic, Finetuning</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">1,840</td>
</tr>
<tr>
<td style="text-align: left;">ProofWriter (Tafjord et al., 2021)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Diagnostic, Finetuning</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">272,430</td>
</tr>
<tr>
<td style="text-align: left;">MATH (Hendrycks et al., 2021)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Diagnostic, Finetuning</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">12,500</td>
</tr>
<tr>
<td style="text-align: left;">ASDIV (Miao et al., 2020)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Diagnostic, Finetuning</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">2,305</td>
</tr>
<tr>
<td style="text-align: left;">AQUA (Liang et al., 2018)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Diagnostic, Finetuning</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">97,975</td>
</tr>
<tr>
<td style="text-align: left;">EQASC (Aggarwal et al., 2021)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Diagnostic, Finetuning</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">9,060</td>
</tr>
<tr>
<td style="text-align: left;">StrategyQA (Geva et al., 2021)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Finetuning</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">2,290</td>
</tr>
<tr>
<td style="text-align: left;">DROP (Dua et al., 2019)</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">Human judged</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">210</td>
</tr>
<tr>
<td style="text-align: left;">GSM8K (Cobbe et al., 2021)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Human judged</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">200</td>
</tr>
<tr>
<td style="text-align: left;">ESNLI (Camburu et al., 2018)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Human judged</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">151</td>
</tr>
<tr>
<td style="text-align: left;">CosmosQA (Huang et al., 2019)</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">Human judged</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">195</td>
</tr>
<tr>
<td style="text-align: left;">SemEval (Ostermann et al., 2018)</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">Human judged</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">209</td>
</tr>
</tbody>
</table>
<p>EntailmentBank (EntBank) (Dalvi et al., 2021) is a complex question answering dataset which contains multi-step entailment trees, namely a tree of multi-premise entailment steps from facts that are known, through intermediate conclusions to hypothesis of interest (which in this case the question and answer).</p>
<p>ProofWriter (Tafjord et al., 2021) is a question answering dataset for logical reasoning. It contains 500k questions, answers and proofs over natural-language rulebases. This dataset is mostly used to emulate reasoning over rules expressed in language, including proof generation. The datasets proofs include intermediate conclusions. In our experiments, we used depth-0, depth-1, depth-2, depth-3, and depth-5 OWA sets.</p>
<p>MATH (Hendrycks et al., 2021) is a dataset of 12,500 problems from high school math competitions. Given a math problem such as in Table 12 models generate a sequence, such as $\frac{2}{3}$, that encodes the final answer.</p>
<p>ASDIV (Miao et al., 2020) (Academia Sinica Diverse MWP Dataset) is a dataset of 2,305 questions on diverse math word problem solving. It includes a diverse operations such as basic arithmetic or aggregative operations (e.g., comparisons, set-operations).</p>
<p>AQUA (Liang et al., 2018) is a dataset of 100,000 algebraic word problems with step-wise solutions as shown below. In the original dataset each question is decomposed in four parts, two inputs and two outputs: the description of the problem and a question, and the possible (multiple choice) answer options, one being the</p>
<p>Table 12: We show instances from seven of the Diagnostics Datasets here. (Continue from $\S 5$ ).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Instance</th>
<th style="text-align: center;">Reference Step-by-Step Solution</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">EntBank</td>
<td style="text-align: center;">Earth is a kind of celestial object. Stars appear to move relative to the horizon during the night. A star is a kind of celestial object celestial body. The earth rotating on its axis causes stars to appear to move across the sky at night. Apparent motion is when an object appears move relative to another object 's position. <br> Question: How does the appearance of a constellation change during the night? <br> Hypothesis: Solve the following entailment problem: "Earth is a kind of celestial object. During the night stars appear to move"</td>
<td style="text-align: center;">Step1: earth is a kind of celestial object Its position appears <br> Step2: a star is a kind of celestial object to shift relative / celestial body to the horizon. <br> Step3: apparent motion is when an object appears to move relative to another object 's position <br> Step4 Therefore apparent motion of stars is when stars appear to move relative to earth's position Step5: The earth rotating on its axis causes stars to appear to move across the sky at night <br> Step6: Therefore the earth rotating on its axis causes apparent motion of stars <br> Step7: Stars appear to move relative to the horizon during the night <br> Step8: Therefore the earth rotating on its axis causes stars to move relative to the horizon during the night.</td>
</tr>
<tr>
<td style="text-align: center;">ProofWriter</td>
<td style="text-align: center;">Facts: The cow is not big. The cow is not green. The lion eats the tiger. The lion sees the cow. The lion visits the cow. The lion does not visit the squirrel. the lion visits the tiger. The squirrel is big. The squirrel is round. The tiger is not green. The tiger does not see the cow. <br> Rules: if something sees the squirrel and the squirrel eats the cow then the cow is round. if something is green then it eats the tiger. if the squirrel is round then the squirrel visits the cow. if something eats the cow then it sees the squirrel. if something sees the tiger and the tiger visits the squirrel then it is nice. if something is round then it eats the cow. if something is kind then it eats the cow. if the tiger visits the cow then the cow sees the squirrel. if something sees the cow then the cow eats the tiger. <br> Question: The cow does not see the squirrel.</td>
<td style="text-align: center;">Step1: The squirrel is round. <br> Step2: something is round then it eats the cow. <br> Step3: The squirrel eats the cow. <br> Step4: If something sees the squirrel and the squirrel eats the cow then the cow is round. <br> Step5: The cow is round. <br> Step6: If something is round then it eats the cow. <br> Step7: The cow eats the cow. <br> Step8: if something eats the cow then it sees the squirrel. <br> Step9: the cow sees the squirrel. <br> Answer: True</td>
</tr>
<tr>
<td style="text-align: center;">MATH</td>
<td style="text-align: center;">Context: Tom has a red marble, a green marble, a blue marble, and three identical yellow marbles. <br> Question: How many different groups of two marbles can Tom choose?</td>
<td style="text-align: center;">Step1: There are two cases here: <br> Step2: either Tom chooses two yellow marbles (1 result), or he chooses two marbles of different colors ( $\left(\frac{4}{4}\right)=6$ results.). <br> Step3: The total number of distinct pairs of marbles Tom can choose is $1+6=7$. <br> Answer: 7</td>
</tr>
<tr>
<td style="text-align: center;">ASDIV</td>
<td style="text-align: center;">Context: A sandwich is priced at $\$ 0.75$. A cup of pudding is priced at $\$ 0.25$. Tim bought 2 sandwiches and 4 cups of pudding. <br> Question: How much money should Tim pay?</td>
<td style="text-align: center;">$0.75 \times 2+0.25 \times 4=2.5$ <br> Answer: 2.5</td>
</tr>
<tr>
<td style="text-align: center;">AQUA</td>
<td style="text-align: center;">Context: The entrance fee for a fair is $\$ 5$ for persons under the age of 18 and $20 \%$ more for persons older. Each ride at the fair costs $\$ 0.50$. If Joe goes with her 6 years old twin brothers, and they each took 3 rides in total. <br> Question: How much money does Joe end up spending at the fair?</td>
<td style="text-align: center;">Step1: Total entrance fee is $(2 * \$ 5)+(1.20 * 5)=\$ 16$ <br> Step2: Total rides fee is $(0.50 * 3) * 3=\$ 4.50$ <br> Step3: Total money spent is $\$ 20.50$ <br> Answer: 20.5</td>
</tr>
<tr>
<td style="text-align: center;">EQASC</td>
<td style="text-align: center;">Question: Where is water likely to form beads?</td>
<td style="text-align: center;">Step1: Beads of water are formed by water vapor condensing <br> Step2: Moisture builds up in condenses air and the wherever the surfaces are cold. <br> Answer: Water beads form on cold surfaces.</td>
</tr>
<tr>
<td style="text-align: center;">StrategyQA</td>
<td style="text-align: center;">Question: Are more people today related to Genghis Khan than Julius Caesar?</td>
<td style="text-align: center;">Step1: Julius Caesar had three children. <br> Step2: Genghis Khan had sixteen children. <br> Step3: Modern geneticists have determined that out of every 200 men today has DNA that can be traced to Genghis Khan. <br> Answer: True</td>
</tr>
</tbody>
</table>
<p>correct one. In this work we only used the context and question, the step-wise solution and the correct answer to construct our diagnostic dataset.</p>
<p>EQASC (Aggarwal et al., 2021) is a multi-hop question answering dataset with 98K explanation annotations for multi-step factual reasoning. Each instance in the dataset comes with a question, multiple answer choices,</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{5}$ We use SciPy (Virtanen et al., 2020) to calculate correlations and obtain $p$-values from a hypothesis test where the null hypothesis is an absence of association.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2767 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2767</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2767</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-70.html">extraction-schema-70</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <p><strong>Paper ID:</strong> paper-c52dddfbb4a3af4cc5e72849fe965c62801539e7</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/c52dddfbb4a3af4cc5e72849fe965c62801539e7" target="_blank">Counting to Explore and Generalize in Text-based Games</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> A recurrent RL agent with an episodic exploration mechanism that helps discovering good policies in text-based game environments and observes that the agent learns policies that generalize to unseen games of greater difficulty.</p>
                <p><strong>Paper Abstract:</strong> We propose a recurrent RL agent with an episodic exploration mechanism that helps discovering good policies in text-based game environments. We show promising results on a set of generated text-based games of varying difficulty where the goal is to collect a coin located at the end of a chain of rooms. In contrast to previous text-based RL approaches, we observe that our agent learns policies that generalize to unseen games of greater difficulty.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2767.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2767.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LSTM-DQN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LSTM-Deep Q-Network (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A text-game agent that encodes observations with an LSTM encoder and scores actions with MLPs; does not maintain recurrent state in the action scorer and therefore has limited capacity to remember long histories.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language understanding for text-based games using deep reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LSTM-DQN</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Representation generator: stacked embedding + encoder LSTM (hidden size 100) with mean pooling producing observation vectors; Action scorer: non-recurrent 1-layer MLP(s) predicting verb and object Q-values which are averaged to form action Q-values. The previous command is concatenated to the current observation to partially alleviate partial observability.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>TextWorld (Coin Collector / chain games)</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>Procedurally generated text-based 'coin collector' chain games in TextWorld: agent must traverse a chain of rooms (with varying numbers of distractor dead-end rooms) to reach and take a coin; observations are English room descriptions, actions are two-word (verb, object) commands from a small action set.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>working memory (short history via concatenated previous command)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>input concatenation of the previous command to current observation (no recurrent hidden state in action scorer)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>previous command (one-step history) appended to the current observation representation</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td>one previous command (1-step history); encoder LSTM hidden size 100 (for representation)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td>full inclusion of the appended previous command in the current input (no separate retrieval mechanism)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>the previous command is updated each step (concatenation at input time)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>to provide minimal short-term context to mitigate partial observability (helps avoid immediate backtracking in simplest tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Qualitative: With only the single-step concatenated history, LSTM-DQN still fails on many simple games; adding episodic discovery bonus (DQN++) greatly improves performance and generalization in many settings. Exact numeric metrics are reported in figures but not as tabulated numbers in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Vanilla DQN (no concatenated previous command) performs worse and often fails to learn simple chain games; adding count-based bonuses helps but less so than episodic bonuses combined with recurrence.</td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>Providing only the previous command is insufficient for harder games; recurrence or stronger memory mechanisms improve performance, especially in harder modes and when training on multiple games.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>The single-step concatenation is a limited memory signal and does not enable solving tasks requiring longer history; susceptible to failure on medium/hard modes where more history is needed.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td>While DQN++ (episodic bonus + single-step history) sometimes generalizes well by exploiting an environment-specific strategy, in general recurrence (DRQN variants) combined with episodic discovery bonus is more robust for harder tasks and multi-game training.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Counting to Explore and Generalize in Text-based Games', 'publication_date_yy_mm': '2018-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2767.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2767.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LSTM-DRQN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LSTM-Deep Recurrent Q-Network (LSTM-DRQN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recurrent RL agent that extends LSTM-DQN by replacing the shared MLP action scorer with an LSTM cell so the policy conditions on an internal recurrent hidden state (history), enabling it to remember past observations and actions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LSTM-DRQN</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Representation generator: same stacked embedding + encoder LSTM (hidden size 100) with mean pooling. Action scorer: an LSTM cell (hidden size 64) receives observation representations plus previous hidden state h_{t-1} to produce stateful inputs to verb/object MLPs and to pass forward as history (standard DRQN-style recurrence).</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>TextWorld (Coin Collector / chain games)</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>Same procedurally generated TextWorld chain games; deterministic transitions; three difficulty modes (easy/medium/hard) varying in distractor rooms and chain length.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>working memory implemented as recurrent hidden state (LSTM); complemented in training by an episodic count-based visitation memory (episodic discovery bonus)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>sequential recurrent buffer realized by an LSTM hidden state (64-dim); training uses replay sequences of length 8 with hidden states bootstrapped from first 4 steps</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>encoded past observations and implicit action-history summary stored in the LSTM hidden/cell state (summary of recent trajectory up to sequence length used in updates)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td>LSTM hidden size 64 (action-scorer recurrence) and encoder LSTM hidden size 100; training sequences sampled of length 8 (first 4 used to bootstrap hidden state), but no explicit upper bound on episodic memory beyond hidden-state capacity</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td>recurrence: hidden state passed forward each time step (recency-based sequential encoding); no explicit retrieval beyond using the current hidden state</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>hidden state updated at each time step by the LSTM cell; during training, sequences are sampled from replay and hidden state bootstrapped from initial sub-sequence</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>to condition the policy on past observations and actions to handle partial observability (e.g., avoid backtracking, remember which exits have been visited), and to leverage episodic exploration rewards to learn policies that use memory for within-episode discovery</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Qualitative: Recurrent models with episodic discovery bonus (DRQN++) learn to solve harder games and generalize better to unseen games than non-recurrent baselines; recurrence becomes crucial as mode difficulty increases and when training on multiple games. Specific numeric metrics are shown in figures in the paper but not provided as exact tables.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Non-recurrent models (LSTM-DQN / DQN variants) perform worse on medium/hard modes and fail to generalize as well when trained on fewer games; cumulative bonuses help somewhat but less than episodic bonuses combined with recurrence.</td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>Recurrence (LSTM hidden state) is especially helpful in harder game modes and when training on distributions of games; episodic discovery bonus further encourages learning to use within-episode memory. The paper hypothesizes and presents evidence that the episodic bonus teaches the agent to utilize its recurrent memory to avoid revisiting states within an episode.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Although recurrence + episodic bonus help, models can overfit in hard-mode training (test reward decreasing while training reward increases); certain environment strategies (wall-following) can obviate the need for deep memory, and the current recurrent memory may be insufficient for more complex topologies like cycles (planned future work).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td>Overall, recurrence combined with episodic discovery bonus (DRQN++) yields the most robust learning and generalization on harder and multi-game training setups; however, in some hard-mode zero-shot tests, DQN++ (non-recurrent with episodic bonus) performed near-perfectly by learning a general anti-clockwise exploration strategy that required little beyond short history.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Counting to Explore and Generalize in Text-based Games', 'publication_date_yy_mm': '2018-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2767.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2767.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Episodic discovery bonus</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Episodic discovery count-based exploration bonus</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An intrinsic reward that gives a positive bonus the first time a particular observation is seen within an episode (counts reset at episode start), intended to act as an episodic memory signal encouraging within-episode discovery and training agents to use memory.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Episodic discovery bonus (MODEL++)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A count-based intrinsic reward r^{++}(o_t) = beta if n(o_t)=1 else 0, where n(o_t) is a visitation count reset at the start of each episode; beta=1.0 in experiments. Used in combination with DQN and DRQN agents to shape exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>TextWorld (Coin Collector / chain games)</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>Same TextWorld coin-collector chain tasks; episodic bonus encourages visiting new rooms within each episode to find the coin.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>episodic memory implemented as per-episode visitation counts</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>table/hash of counts keyed by observations (o_t); the count table is reset to zero at episode start (per-game counts exist separately during multi-game training)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>binary/visit flag for each observed state/observation within the current episode (counts track whether observation seen already within the episode)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td>unbounded relative to the number of distinct observations in a game (practically limited by number of unique room observations); no explicit numeric capacity provided</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td>checked at each observation to decide whether to give the episodic bonus (simple existence/first-visit check), not used as an explicit retrieval during action selection</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>increment visitation count for observation at each time step during an episode; reset counts to zero at the beginning of each episode</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>provides intrinsic reward that encourages the agent to explore unvisited states within an episode and thereby incentivizes use of the agent's internal memory (recurrence) to avoid revisits and plan exploration</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Qualitative: Episodic discovery bonus substantially improves learning and generalization compared to cumulative counting bonuses, especially in medium/hard modes and when combined with recurrence; models with episodic bonus (MODEL++) solved and generalized to unseen harder games more reliably.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Without the episodic bonus (or with cumulative-only bonus), agents learned more slowly and generalized less well; cumulative bonus helped somewhat but was less effective as task difficulty increased.</td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>Episodic bonus outperforms cumulative count bonuses for encouraging within-episode discovery and for teaching recurrent agents to use memory; it is particularly effective for harder maps and multi-game training where avoiding revisits within an episode is important.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Episodic bonus is disabled at test time (counts are only used during training), so learned policies must internalize the behavior; may lead to overfitting in some hard-mode training regimes; effectiveness depends on environment structure (e.g., wall-following strategy can bypass need for episodic memory in acyclic maps).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td>Episodic discovery bonus combined with a recurrent policy (DRQN++) generally produced the best learning and generalization across harder and multi-game settings; cumulative (across-episode) counting bonuses were less helpful.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Counting to Explore and Generalize in Text-based Games', 'publication_date_yy_mm': '2018-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Reinforcement learning and episodic memory in humans and animals: an integrative framework <em>(Rating: 2)</em></li>
                <li>Deep recurrent q-learning for partially observable mdps <em>(Rating: 2)</em></li>
                <li>Count-based exploration with neural density models <em>(Rating: 2)</em></li>
                <li># exploration: A study of count-based exploration for deep reinforcement learning <em>(Rating: 2)</em></li>
                <li>Language understanding for text-based games using deep reinforcement learning <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2767",
    "paper_id": "paper-c52dddfbb4a3af4cc5e72849fe965c62801539e7",
    "extraction_schema_id": "extraction-schema-70",
    "extracted_data": [
        {
            "name_short": "LSTM-DQN",
            "name_full": "LSTM-Deep Q-Network (baseline)",
            "brief_description": "A text-game agent that encodes observations with an LSTM encoder and scores actions with MLPs; does not maintain recurrent state in the action scorer and therefore has limited capacity to remember long histories.",
            "citation_title": "Language understanding for text-based games using deep reinforcement learning",
            "mention_or_use": "use",
            "agent_name": "LSTM-DQN",
            "agent_description": "Representation generator: stacked embedding + encoder LSTM (hidden size 100) with mean pooling producing observation vectors; Action scorer: non-recurrent 1-layer MLP(s) predicting verb and object Q-values which are averaged to form action Q-values. The previous command is concatenated to the current observation to partially alleviate partial observability.",
            "base_model_size": null,
            "game_benchmark_name": "TextWorld (Coin Collector / chain games)",
            "game_description": "Procedurally generated text-based 'coin collector' chain games in TextWorld: agent must traverse a chain of rooms (with varying numbers of distractor dead-end rooms) to reach and take a coin; observations are English room descriptions, actions are two-word (verb, object) commands from a small action set.",
            "uses_memory": true,
            "memory_type": "working memory (short history via concatenated previous command)",
            "memory_structure": "input concatenation of the previous command to current observation (no recurrent hidden state in action scorer)",
            "memory_content": "previous command (one-step history) appended to the current observation representation",
            "memory_capacity": "one previous command (1-step history); encoder LSTM hidden size 100 (for representation)",
            "memory_retrieval_strategy": "full inclusion of the appended previous command in the current input (no separate retrieval mechanism)",
            "memory_update_strategy": "the previous command is updated each step (concatenation at input time)",
            "memory_usage_purpose": "to provide minimal short-term context to mitigate partial observability (helps avoid immediate backtracking in simplest tasks)",
            "performance_with_memory": "Qualitative: With only the single-step concatenated history, LSTM-DQN still fails on many simple games; adding episodic discovery bonus (DQN++) greatly improves performance and generalization in many settings. Exact numeric metrics are reported in figures but not as tabulated numbers in the paper.",
            "performance_without_memory": "Vanilla DQN (no concatenated previous command) performs worse and often fails to learn simple chain games; adding count-based bonuses helps but less so than episodic bonuses combined with recurrence.",
            "has_memory_ablation": true,
            "memory_effectiveness_findings": "Providing only the previous command is insufficient for harder games; recurrence or stronger memory mechanisms improve performance, especially in harder modes and when training on multiple games.",
            "memory_limitations": "The single-step concatenation is a limited memory signal and does not enable solving tasks requiring longer history; susceptible to failure on medium/hard modes where more history is needed.",
            "comparison_with_other_memory_types": true,
            "best_memory_configuration": "While DQN++ (episodic bonus + single-step history) sometimes generalizes well by exploiting an environment-specific strategy, in general recurrence (DRQN variants) combined with episodic discovery bonus is more robust for harder tasks and multi-game training.",
            "uuid": "e2767.0",
            "source_info": {
                "paper_title": "Counting to Explore and Generalize in Text-based Games",
                "publication_date_yy_mm": "2018-06"
            }
        },
        {
            "name_short": "LSTM-DRQN",
            "name_full": "LSTM-Deep Recurrent Q-Network (LSTM-DRQN)",
            "brief_description": "A recurrent RL agent that extends LSTM-DQN by replacing the shared MLP action scorer with an LSTM cell so the policy conditions on an internal recurrent hidden state (history), enabling it to remember past observations and actions.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "LSTM-DRQN",
            "agent_description": "Representation generator: same stacked embedding + encoder LSTM (hidden size 100) with mean pooling. Action scorer: an LSTM cell (hidden size 64) receives observation representations plus previous hidden state h_{t-1} to produce stateful inputs to verb/object MLPs and to pass forward as history (standard DRQN-style recurrence).",
            "base_model_size": null,
            "game_benchmark_name": "TextWorld (Coin Collector / chain games)",
            "game_description": "Same procedurally generated TextWorld chain games; deterministic transitions; three difficulty modes (easy/medium/hard) varying in distractor rooms and chain length.",
            "uses_memory": true,
            "memory_type": "working memory implemented as recurrent hidden state (LSTM); complemented in training by an episodic count-based visitation memory (episodic discovery bonus)",
            "memory_structure": "sequential recurrent buffer realized by an LSTM hidden state (64-dim); training uses replay sequences of length 8 with hidden states bootstrapped from first 4 steps",
            "memory_content": "encoded past observations and implicit action-history summary stored in the LSTM hidden/cell state (summary of recent trajectory up to sequence length used in updates)",
            "memory_capacity": "LSTM hidden size 64 (action-scorer recurrence) and encoder LSTM hidden size 100; training sequences sampled of length 8 (first 4 used to bootstrap hidden state), but no explicit upper bound on episodic memory beyond hidden-state capacity",
            "memory_retrieval_strategy": "recurrence: hidden state passed forward each time step (recency-based sequential encoding); no explicit retrieval beyond using the current hidden state",
            "memory_update_strategy": "hidden state updated at each time step by the LSTM cell; during training, sequences are sampled from replay and hidden state bootstrapped from initial sub-sequence",
            "memory_usage_purpose": "to condition the policy on past observations and actions to handle partial observability (e.g., avoid backtracking, remember which exits have been visited), and to leverage episodic exploration rewards to learn policies that use memory for within-episode discovery",
            "performance_with_memory": "Qualitative: Recurrent models with episodic discovery bonus (DRQN++) learn to solve harder games and generalize better to unseen games than non-recurrent baselines; recurrence becomes crucial as mode difficulty increases and when training on multiple games. Specific numeric metrics are shown in figures in the paper but not provided as exact tables.",
            "performance_without_memory": "Non-recurrent models (LSTM-DQN / DQN variants) perform worse on medium/hard modes and fail to generalize as well when trained on fewer games; cumulative bonuses help somewhat but less than episodic bonuses combined with recurrence.",
            "has_memory_ablation": true,
            "memory_effectiveness_findings": "Recurrence (LSTM hidden state) is especially helpful in harder game modes and when training on distributions of games; episodic discovery bonus further encourages learning to use within-episode memory. The paper hypothesizes and presents evidence that the episodic bonus teaches the agent to utilize its recurrent memory to avoid revisiting states within an episode.",
            "memory_limitations": "Although recurrence + episodic bonus help, models can overfit in hard-mode training (test reward decreasing while training reward increases); certain environment strategies (wall-following) can obviate the need for deep memory, and the current recurrent memory may be insufficient for more complex topologies like cycles (planned future work).",
            "comparison_with_other_memory_types": true,
            "best_memory_configuration": "Overall, recurrence combined with episodic discovery bonus (DRQN++) yields the most robust learning and generalization on harder and multi-game training setups; however, in some hard-mode zero-shot tests, DQN++ (non-recurrent with episodic bonus) performed near-perfectly by learning a general anti-clockwise exploration strategy that required little beyond short history.",
            "uuid": "e2767.1",
            "source_info": {
                "paper_title": "Counting to Explore and Generalize in Text-based Games",
                "publication_date_yy_mm": "2018-06"
            }
        },
        {
            "name_short": "Episodic discovery bonus",
            "name_full": "Episodic discovery count-based exploration bonus",
            "brief_description": "An intrinsic reward that gives a positive bonus the first time a particular observation is seen within an episode (counts reset at episode start), intended to act as an episodic memory signal encouraging within-episode discovery and training agents to use memory.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Episodic discovery bonus (MODEL++)",
            "agent_description": "A count-based intrinsic reward r^{++}(o_t) = beta if n(o_t)=1 else 0, where n(o_t) is a visitation count reset at the start of each episode; beta=1.0 in experiments. Used in combination with DQN and DRQN agents to shape exploration.",
            "base_model_size": null,
            "game_benchmark_name": "TextWorld (Coin Collector / chain games)",
            "game_description": "Same TextWorld coin-collector chain tasks; episodic bonus encourages visiting new rooms within each episode to find the coin.",
            "uses_memory": true,
            "memory_type": "episodic memory implemented as per-episode visitation counts",
            "memory_structure": "table/hash of counts keyed by observations (o_t); the count table is reset to zero at episode start (per-game counts exist separately during multi-game training)",
            "memory_content": "binary/visit flag for each observed state/observation within the current episode (counts track whether observation seen already within the episode)",
            "memory_capacity": "unbounded relative to the number of distinct observations in a game (practically limited by number of unique room observations); no explicit numeric capacity provided",
            "memory_retrieval_strategy": "checked at each observation to decide whether to give the episodic bonus (simple existence/first-visit check), not used as an explicit retrieval during action selection",
            "memory_update_strategy": "increment visitation count for observation at each time step during an episode; reset counts to zero at the beginning of each episode",
            "memory_usage_purpose": "provides intrinsic reward that encourages the agent to explore unvisited states within an episode and thereby incentivizes use of the agent's internal memory (recurrence) to avoid revisits and plan exploration",
            "performance_with_memory": "Qualitative: Episodic discovery bonus substantially improves learning and generalization compared to cumulative counting bonuses, especially in medium/hard modes and when combined with recurrence; models with episodic bonus (MODEL++) solved and generalized to unseen harder games more reliably.",
            "performance_without_memory": "Without the episodic bonus (or with cumulative-only bonus), agents learned more slowly and generalized less well; cumulative bonus helped somewhat but was less effective as task difficulty increased.",
            "has_memory_ablation": true,
            "memory_effectiveness_findings": "Episodic bonus outperforms cumulative count bonuses for encouraging within-episode discovery and for teaching recurrent agents to use memory; it is particularly effective for harder maps and multi-game training where avoiding revisits within an episode is important.",
            "memory_limitations": "Episodic bonus is disabled at test time (counts are only used during training), so learned policies must internalize the behavior; may lead to overfitting in some hard-mode training regimes; effectiveness depends on environment structure (e.g., wall-following strategy can bypass need for episodic memory in acyclic maps).",
            "comparison_with_other_memory_types": true,
            "best_memory_configuration": "Episodic discovery bonus combined with a recurrent policy (DRQN++) generally produced the best learning and generalization across harder and multi-game settings; cumulative (across-episode) counting bonuses were less helpful.",
            "uuid": "e2767.2",
            "source_info": {
                "paper_title": "Counting to Explore and Generalize in Text-based Games",
                "publication_date_yy_mm": "2018-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Reinforcement learning and episodic memory in humans and animals: an integrative framework",
            "rating": 2
        },
        {
            "paper_title": "Deep recurrent q-learning for partially observable mdps",
            "rating": 2
        },
        {
            "paper_title": "Count-based exploration with neural density models",
            "rating": 2
        },
        {
            "paper_title": "# exploration: A study of count-based exploration for deep reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Language understanding for text-based games using deep reinforcement learning",
            "rating": 2
        }
    ],
    "cost": 0.01078125,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Counting to Explore and Generalize in Text-based Games</h1>
<p>Xingdi Yuan ${ }^{<em> 1}$ Marc-Alexandre Côté ${ }^{</em> 1}$ Alessandro Sordoni ${ }^{1}$ Romain Laroche ${ }^{1}$ Remi Tachet des Combes ${ }^{1}$<br>Matthew Hausknecht ${ }^{1}$ Adam Trischler ${ }^{1}$</p>
<h4>Abstract</h4>
<p>We propose a recurrent RL agent with an episodic exploration mechanism that helps discovering good policies in text-based game environments. We show promising results on a set of generated text-based games of varying difficulty where the goal is to collect a coin located at the end of a chain of rooms. In contrast to previous text-based RL approaches, we observe that our agent learns policies that generalize to unseen games of greater difficulty.</p>
<h2>1. Introduction</h2>
<p>Text-based games like Zork (Infocom, 1980) are complex, interactive simulations. They use natural language to describe the state of the world, to accept actions from the player, and to report subsequent changes in the environment. The player works toward goals which are seldom specified explicitly and must be discovered through exploration. The observation and action spaces in text games are both combinatorial and compositional, and players must contend with partial observability, since descriptive text does not communicate complete, unambiguous information about the underlying game state.</p>
<p>In this paper, we study several methods of exploration in text-based games. Our basic task is a deterministic textbased version of the chain experiment (Osband et al., 2016; Plappert et al., 2017) with distractor nodes that are off-chain: the agent must navigate a path composed of discrete locations (rooms) to the goal, ideally without revisiting dead ends. We propose a DQN-based recurrent model for solving text-based games, where the recurrence gives the model the capacity to condition its policy on historical state information. To encourage exploration, we extend count-based exploration approaches (Ostrovski et al., 2017; Tang et al.,</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>2017), which assign an intrinsic reward derived from the count of state visitations during learning, across episodes. Specifically, we propose an episodic count-based exploration scheme, where state counts are reset at the beginning of each episode. This reward plays the role of an episodic memory (Gershman \&amp; Daw, 2017) that pushes the agent to visit states not previously encountered within an episode. Although the recurrent policy architecture has the capacity to solve the task by remembering and avoiding previously visited locations, we hypothesize that exploration rewards will help the agent learn to utilize its memory.</p>
<p>We generate a set of games of varying difficulty (measured with respect to the path length and the number of off-chain rooms) with a text-based game generator (Côté et al., 2018). We observe that, in contrast to a baseline model and standard count-based exploration methods, the recurrent model with episodic bonus learns policies that not only complete multiple training games at same time successfully but also generalize to unseen games of greater difficulty.</p>
<h2>2. Text-based Games as POMDPs</h2>
<p>Text-based games are sequential decision-making problems that can be described naturally by the Reinforcement Learning (RL) setting. Fundamentally, text-based games are partially observable Markov decision processes (POMDP) (Kaelbling et al., 1998) where the environment state is never observed directly. To act optimally, an agent must keep track of all observations. Formally, a text-based game is a discrete-time POMDP defined by $(S, T, A, \Omega, O, R, \gamma)$, where $\gamma \in[0,1]$ is the discount factor.</p>
<p>Environment States $(S)$ : The environment state at turn $t$ in the game is $s_{t} \in S$. It contains the complete internal information of the game, much of which is hidden from the agent. When an agent issues a command $\boldsymbol{c}<em t_1="t+1">{t}$ (defined next), the environment transitions to state $s</em>}$ with probability $T\left(s_{t+1} \mid s_{t}, \boldsymbol{c<em t="t">{t}\right)$.
Actions $(A)$ : At each turn $t$, the agent issues a text command $\boldsymbol{c}</em>$. The interpreter can accept any sequence of characters but will only recognize a tiny subset thereof. Furthermore, only a fraction of recognized commands will actually change the state of the world. The resulting action space</p>
<p>is enormous and intractable for existing RL algorithms. In this work, we make the following two simplifying assumptions. (1) Word-level Each command is a two-word sequence where the words are taken from a fixed vocabulary $V$. (2) Command syntax Each command is a (verb, object) pair (direction words are considered objects).</p>
<p>Observations ( $\Omega$ ): The text information perceived by the agent at a given turn $t$ in the game is the agent's observation, $o_{t} \in \Omega$, which depends on the environment state and the previous command with probability $O\left(o_{t} \mid s_{t}, \boldsymbol{c}_{t-1}\right)$. Thus, the function $O$ selects from the environment state what information to show to the agent given the last command.</p>
<p>Reward Function $(R)$ : Based on its actions, the agent receives reward signals $r_{t}=R\left(s_{t}, a_{t}\right)$. The goal is to maximize the expected discounted sum of rewards $E\left[\sum_{t} \gamma^{t} r_{t}\right]$.</p>
<h2>3. Method</h2>
<h3>3.1. Model Architecture</h3>
<p>In this work, we adopt the LSTM-DQN (Narasimhan et al., 2015) model as baseline. It has two modules: a representation generator $\Phi_{R}$, and an action scorer $\Phi_{A} . \Phi_{R}$ takes observation strings $o$ as input, after a stacked embedding layer and LSTM (Hochreiter \&amp; Schmidhuber, 1997) encoder, a mean-pooling layer produces a vector representation of the observation. This feeds into $\Phi_{A}$, in which two MLPs, sharing a lower layer, predict the Q-values over all verbs $w_{v}$ and object words $w_{o}$ independently. The average of the two resulting scores gives the Q-values for the composed actions. The LSTM-DQN does not condition on previous actions or observations, so it cannot deal with partial observability. We concatenate the previous command $\boldsymbol{c}<em t="t">{t-1}$ to the current observation $o</em>$ to lessen this limitation.</p>
<p>To further enhance the agent's capacity to remember previous states, we replace the shared MLP in $\Phi_{A}$ by an LSTM cell. This model is inspired by (Hausknecht \&amp; Stone, 2015; Lample \&amp; Chaplot, 2016) and we call it LSTM-DRQN. The LSTM cell in $\Phi_{A}$ takes the representation generated by $\Phi_{R}$ together with history information $h_{t-1}$ from the previous game step as input. It generates the state information at the current game step, which is then fed into the two MLPs as well as passed forward to next game step. Figure 1 shows the LSTM-DRQN architecture.</p>
<h3>3.2. Discovery Bonus</h3>
<p>To promote exploration we use an intrinsic reward by counting state visits (Kolter \&amp; Ng, 2009; Tang et al., 2017; Martin et al., 2017; Ostrovski et al., 2017). We investigate two approaches to counting rewards. The first is inspired by (Kolter $\&amp; \mathrm{Ng}, 2009$ ), where we define the cumulative counting bonus as $r^{+}\left(o_{t}\right)=\beta \cdot n\left(o_{t}\right)^{-1 / 3}$, where $n\left(o_{t}\right)$ is the num-
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. LSTM-DRQN processes textual observations word-byword to generate a fixed-length vector representation. This representation is used by the recurrent policy to estimate Q-values for all verbs $Q(s, v)$ and objects $Q(s, o)$.
ber of times the agent has observed $o_{t}$ since the beginning of training (across episodes), and $\beta$ is the bonus coefficient. During training, as the agent observes new states more and more, the cumulative counting bonus gradually converges to 0 .</p>
<p>The second approach is the episodic discovery bonus, which encourages the agent to discover unseen states by assigning a positive reward whenever it sees a new state. It is defined as: $r^{++}\left(o_{t}\right)=\left{\begin{array}{ll}\beta &amp; \text { if } n\left(o_{t}\right)=1 \ 0.0 &amp; \text { otherwise }\end{array}\right.$, where $n(\cdot)$ is reset to zero at the beginning of each episode. Taking inspiration from (Gershman \&amp; Daw, 2017), we hope this behavior pushes the agent to visit states not previously encountered in the current episode and teaches the agent how to use its memory for this purpose so it may generalize to unseen environments.</p>
<h2>4. Related Work</h2>
<p>RL Applied to Text-based Games: Narasimhan et al. (2015) test their LSTM-DQN in two text-based environments: Home World and Fantasy World. They report the quest completion ratio over multiple runs but not how many steps it takes to complete them. He et al. (2015) introduce the Deep Reinforcement Relevance Network (DRRN) for tackling choice-based (as opposed to parser-based) text games, evaluating the DRRN on one deterministic game and one larger-scale stochastic game. The DRRN model converges on both games; however, this model must know in advance the valid commands at each state. Fulda et al. (2017) propose a method to reduce the action space for parserbased games by training word embeddings to be aware of verb-noun affordances. One drawback of this approach is it requires pre-trained embeddings.</p>
<p>Count-based Exploration: The Model Based Interval Estimation-Exploration Bonus (MBIE-EB) (Strehl \&amp; Littman, 2008) derives an intrinsic reward by counting stateaction pairs with a table $n(s, a)$. Their exploration bonus has the form $\beta / \sqrt{n(s, a)}$ to encourage exploring less-visited pairs. In this work, we use $n(s)$ rather than $n(s, a)$, since the majority of actions leave the agent in the same state</p>
<p>(i.e., unrecognized commands). Using the latter would reward the agent for trying invalid commands, which is not sensible in our setting.</p>
<p>Tang et al. (2017) propose a hashing function for countbased exploration in order to discretize high-dimensional, continuous state spaces. Their exploration bonus $r^{+}=$ $\beta / \sqrt{n(\phi(s))}$, where $\phi(\cdot)$ is a hashing function that can either be static or learned. This is similar to the cumulative counting bonus defined above.</p>
<p>Deep Recurrent Q-Learning: Hausknecht \&amp; Stone (2015) propose the Deep Recurrent Q-Networks (DRQN), adding a recurrent neural network (such as an LSTM (Hochreiter \&amp; Schmidhuber, 1997)) on top of the standard DQN model. DRQN estimates $Q\left(o_{t}, h_{t-1}, a_{t}\right)$ instead of $Q\left(o_{t}, a_{t}\right)$, so it has the capacity to memorize the state history. Lample \&amp; Chaplot (2016) use a model built on the DRQN architecture to learn to play FPS games.</p>
<p>A major difference between the work presented in this paper and the related work is that we test on unseen games and train on a set of similar (but not identical) games rather than training and testing on the same game.</p>
<h2>5. Experiments</h2>
<h3>5.1. Coin Collector Game Setup</h3>
<p>To evaluate the two models described above and the proposed discovery bonus, we designed a set of simple textbased games inspired by the chain experiment (Osband et al., 2016; Plappert et al., 2017). Each game contains a given number of rooms that are randomly connected to each other to form a chain (see figures in Appendix C). The goal is to find and collect a "coin" placed in one of the rooms. The player's initial position is at one end of the chain and the coin is at the other. These games have deterministic state transitions.</p>
<p>Games stop after a set number of steps or after the player has collected the coin. The game interpreter understands only five commands (go north, go east, go south, go west and take coin), while the action space is twice as large: ${\mathrm{go}$, take $} \times{$ north, south, east, west, coin $}$. See Figure 12, Appendix C for an example of what the agent observes in-game.</p>
<p>Our games have 3 modes: easy (mode 0 ), there are no distractor rooms (dead ends) along the path; medium (mode 1), each room along the optimal trajectory has one distractor room randomly connected to it; hard (mode 2), each room on the path has two distractor rooms, i.e., within a room on the optimal trajectory, all 4 directions lead to a connected room. We use difficulty levels to indicate the optimal trajectory's length of a game.</p>
<p>To solve easy games, the agent must learn to recall its previous directional action and to issue the command that does not reverse it (e.g., if the agent entered the current room by going east, do not now go west). Conversely, to solve medium and hard games, the agent must reverse its previous action when it enters distractor rooms to return to the chain, and also recall farther into the past to track which exits it has already passed through. Alternatively, since there are no cycles, it can learn a less memory intensive "wall-following" strategy by, e.g., taking exits in a clockwise order from where it enters a room.</p>
<p>We refer to models with the cumulative counting bonus as MODEL+, and models with episodic discovery bonus as MODEL++, where MODEL $\in{\mathrm{DQN}, \mathrm{DRQN}}^{1}$ (implementation details in Appendix A). In this section we cover part of the experiment results, the full extent of our experiment results are provided in Appendix B.</p>
<h3>5.2. Solving Training Games</h3>
<p>We first investigate whether the variant models can learn to solve single games with different difficulty modes (easy, medium, hard) and levels ${L 5, L 10, L 15, L 20, L 25, L 30}^{2}$. As shown in Figure 2 (top row), when the games are simple, vanilla DQN and DRQN already fail to learn. Adding the cumulative bonus helps somewhat and models perform similarly with and without recurrence. When the games become harder, the cumulative bonus helps less, while episodic bonus remains very helpful and recurrence in the model becomes very helpful.</p>
<p>Next, we are interested to see whether models can learn to solve a distribution of games. Note that each game has its own counting memory, i.e., the states visited in one game do not affect the counters for other games. Here, we fix the game difficulty level to 10 , and randomly generate training sets that contain ${2,5,10,30,50,100}$ games in each mode. As shown in Figure 2 (bottom row), when the game mode becomes harder, the episodic bonus has an advantage over the cumulative bonus, and recurrence becomes more crucial for memorizing the game distribution. It is also clear that the episodic bonus and recurrence help significantly when more training games are provided.</p>
<h3>5.3. Zero-shot Evaluation</h3>
<p>Finally, we want to see if a pre-trained model can generalize to unseen games. The generated training set contains ${1,2,5,10,30,50,100,500} \mathrm{L} 10$ games for each mode. Then, for each corresponding mode the test set contains 10 unseen ${L 5, L 10, L 15, L 20, L 30}$ games. There is no</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. Model performance on single games (top row) and multiple games (bottom row).
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3. Zero-shot evaluation: Average rewards of DQN++ (left) and DRQN++ (right) as a function of the number of games in the training set.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4. Average rewards and steps used corresponding to best validation performance in hard games.
overlap between training and test games in either text descriptions or optimal trajectories. At test time, the counting modules are disabled, the agent is not updated, and its generates verb and noun actions based on the $\operatorname{argmax}$ of their Q-values.</p>
<p>As shown in Figure 3, when the game mode is easy, both models with and without recurrence can generalize well on unseen games by training on a large training set. It is worth noting that by training on 500 L10 easy games, both models can almost perfectly solve level 30 unseen easy games. We also observe that models with recurrence are able to generalize better when trained on fewer games.</p>
<p>When testing on hard mode games, we observe that both models suffer from overfitting (after a certain number of episodes, average test reward starts to decrease while training reward increases). Therefore, we further generated a validation set that contains 10 L10 hard games, and report test results corresponding to best validation performance. In
addition, we investigated what happens when concatenating the previous 4 steps' history observation into the input. In Figure 4, we add $H$ to model names to indicate this variant.</p>
<p>As shown in Figure 4, all models can memorize the 500 training games, while DQN++ and DRQN++H are able to generalize better on unseen games. In particular, the former performs near perfectly on test games. To investigate this, we looked into all the bi-grams of generated commands (i.e., two commands from adjacent game steps) from DQN++ model. Surprisingly, except for moving back from dead end rooms, the agent always explores exits in anti-clockwise order. This means the agent has learned a general strategy that does not require history information beyond the previous command. This strategy generalizes perfectly to all possible hard games because there are no cycles in the maps.</p>
<h2>6. Final Remarks</h2>
<p>We propose an RL model with a recurrent component, together with an episodic count-based exploration scheme that promotes the agent's discovery of the game environment. We show promising results on a set of generated text-based games of varying difficulty. In contrast to baselines, our approach learns policies that generalize to unseen games of greater difficulty.</p>
<p>In future work, we plan to experiment on games with more complex topology, such as cycles (where the "wallfollowing" strategy will not work). We would like to explore games that require multi-word commands (e.g., unlock red door with red key), necessitating a model that generates sequences of words. Other interesting directions include agents that learn to map or to deal with stochastic transitions in text-based games.</p>
<h2>References</h2>
<p>Côté, Marc-Alexandre, Kádár, Ákos, Yuan, Xingdi, Kybartas, Ben, Barnes, Tavian, Fine, Emery, Moore, James, Hausknecht, Matthew, Asri, Layla El, Adada, Mahmoud, Tay, Wendy, and Trischler, Adam. Textworld: A learning environment for text-based games. Computer Games Workshop at IJCAI 2018, Stockholm, 2018.</p>
<p>Fulda, Nancy, Ricks, Daniel, Murdoch, Ben, and Wingate, David. What can you do with a rock? affordance extraction via word embeddings. arXiv preprint arXiv:1703.03429, 2017.</p>
<p>Gershman, Samuel J and Daw, Nathaniel D. Reinforcement learning and episodic memory in humans and animals: an integrative framework. Annual review of psychology, 68:101-128, 2017.</p>
<p>Hausknecht, Matthew J. and Stone, Peter. Deep recurrent q-learning for partially observable mdps. CoRR, abs/1507.06527, 2015. URL http://arxiv.org/ abs/1507.06527.</p>
<p>He, Ji, Chen, Jianshu, He, Xiaodong, Gao, Jianfeng, Li, Lihong, Deng, Li, and Ostendorf, Mari. Deep reinforcement learning with a natural language action space. arXiv preprint arXiv:1511.04636, 2015.</p>
<p>Hochreiter, Sepp and Schmidhuber, Jürgen. Long shortterm memory. Neural Comput., 9(8):1735-1780, November 1997. ISSN 0899-7667. doi: 10.1162/neco.1997. 9.8.1735. URL http://dx.doi.org/10.1162/ neco.1997.9.8.1735.</p>
<p>Infocom. Zork I, 1980. URL http://ifdb.tads. org/viewgame?id=0dbnusxunq7fw5ro.</p>
<p>Kaelbling, Leslie Pack, Littman, Michael L, and Cassandra, Anthony R. Planning and acting in partially observable stochastic domains. Artificial intelligence, 101(1-2):99134, 1998.</p>
<p>Kingma, Diederik and Ba, Jimmy. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.</p>
<p>Kolter, J Zico and Ng, Andrew Y. Near-bayesian exploration in polynomial time. In Proceedings of the 26th Annual International Conference on Machine Learning, pp. 513520. ACM, 2009.</p>
<p>Lample, Guillaume and Chaplot, Devendra Singh. Playing FPS games with deep reinforcement learning. CoRR, abs/1609.05521, 2016. URL http://arxiv.org/ abs/1609.05521.</p>
<p>Martin, Jarryd, Sasikumar, Suraj Narayanan, Everitt, Tom, and Hutter, Marcus. Count-based exploration in feature space for reinforcement learning. arXiv preprint arXiv:1706.08090, 2017.</p>
<p>Narasimhan, Karthik, Kulkarni, Tejas, and Barzilay, Regina. Language understanding for text-based games using deep reinforcement learning. arXiv preprint arXiv:1506.08941, 2015.</p>
<p>Osband, Ian, Blundell, Charles, Pritzel, Alexander, and Van Roy, Benjamin. Deep exploration via bootstrapped dqn. In Lee, D. D., Sugiyama, M., Luxburg, U. V., Guyon, I., and Garnett, R. (eds.), Advances in Neural Information Processing Systems 29, pp. 4026-4034. Curran Associates, Inc., 2016. URL http://papers.nips.cc/paper/ 6501-deep-exploration-via-bootstrapped-dqn. pdf.</p>
<p>Ostrovski, Georg, Bellemare, Marc G, Oord, Aaron van den, and Munos, Rémi. Count-based exploration with neural density models. arXiv preprint arXiv:1703.01310, 2017.</p>
<p>Paszke, Adam, Gross, Sam, Chintala, Soumith, Chanan, Gregory, Yang, Edward, DeVito, Zachary, Lin, Zeming, Desmaison, Alban, Antiga, Luca, and Lerer, Adam. Automatic differentiation in pytorch. In NIPS-W, 2017.</p>
<p>Plappert, Matthias, Houthooft, Rein, Dhariwal, Prafulla, Sidor, Szymon, Chen, Richard Y, Chen, Xi, Asfour, Tamim, Abbeel, Pieter, and Andrychowicz, Marcin. Parameter space noise for exploration. arXiv preprint arXiv:1706.01905, 2017.</p>
<p>Strehl, Alexander L and Littman, Michael L. An analysis of model-based interval estimation for markov decision processes. Journal of Computer and System Sciences, 74 (8):1309-1331, 2008.</p>
<p>Tang, Haoran, Houthooft, Rein, Foote, Davis, Stooke, Adam, Chen, Xi, Duan, Yan, Schulman, John, DeTurck, Filip, and Abbeel, Pieter. # exploration: A study of countbased exploration for deep reinforcement learning. In Advances in Neural Information Processing Systems, pp. 2750-2759, 2017.</p>
<h1>A. Implementation Details</h1>
<p>Implementation details of our neural baseline agent are as follows ${ }^{3}$. In all experiments, the word embeddings are initialized with 20-dimensional random matrices; the number of hidden units of the encoder LSTM is 100. In the nonrecurrent action scorer we use a 1-layer MLP which has 64 hidden units, with $\operatorname{ReLU}$ as non-linear activation function, in the recurrent action scorer, we use an LSTM cell which hidden size is 64 .</p>
<p>In replay memory, we used a memory with capacity of 500000 , a mini-batch gradient update is performed every 4 steps in the gameplay, the mini-batch size is 32 . We apply prioritized sampling in all experiments, in which, we used $\rho=0.25$. In LSTM-DQN and LSTM-DRQN model, we used discount factor $\gamma=0.9$, in all models with discovery bonus, we used $\gamma=0.5$.</p>
<p>When updating models with recurrent components, we follow the update strategy in (Lample \&amp; Chaplot, 2016), i.e., we randomly sample sequences of length 8 from the replay memory, zero initialize hidden state and cell state, use the first 4 states to bootstrap a reliable hidden state and cell state, and then update on rest of the sequence.</p>
<p>We anneal the $\epsilon$ for $\epsilon$-greedy from 1 to 0.2 over 1000 epochs, it remains at 0.2 afterwards. In both cumulative and episodic discovery bonus, we use coefficient $\beta$ of 1.0.</p>
<p>When zero-shot evaluating hard games, we use max_train_step $=100$, in all other experiments we use max_train_step $=50$; during test, we always use max_test_step $=200$.</p>
<p>We use adam (Kingma \&amp; Ba, 2014) as the step rule for optimization. The learning rate is $1 e^{-3}$. The model is implemented using PyTorch (Paszke et al., 2017).</p>
<p>All games are generated using TextWorld framework (Côté et al., 2018), we used the house grammar.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>B. More Results</h1>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5. Model performance on single games.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6. Model performance on multiple games.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7. Model performance on unseen easy test games when pre-trained on easy games.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8. Model performance on unseen medium test games when pre-trained on medium games.</p>
<h1>C. Text-based Chain Experiment</h1>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9. Examples of the games used in the experiments: level 10, easy
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10. Examples of the games used in the experiments: level 10, medium</p>
<p><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 11. Examples of the games used in the experiments: level 10, hard</p>
<p>I hope your ready to go into rooms and interact with objects, because you've just entered TextWorld! take the coin that's in the launderette.
-= Cookhouse =-
Fancy seeing you here. Here, by the way, being the cookhouse.</p>
<p>There is an unguarded exit to the south.
$&gt;$ go south
-= Study =-
You've entered a study. The room seems oddly familiar, as though it were only superficially different from the other rooms in the building.</p>
<p>You need an unguarded exit? You should try going west. You need an unguarded exit? You should try going north.
$&gt;$</p>
<p>Figure 12. Text the agent gets to observe for one of the level 10 easy games.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ Our implementation is publicly available at https://github.com/xingdi-eric-yuan/ TextWorld-Coin-Collector.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
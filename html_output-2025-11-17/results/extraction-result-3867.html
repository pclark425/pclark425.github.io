<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3867 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3867</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3867</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-91.html">extraction-schema-91</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-a6fdb277d0a4b09899f802bda3359f5c2021a156</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/a6fdb277d0a4b09899f802bda3359f5c2021a156" target="_blank">Recursively Summarizing Books with Human Feedback</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This method combines learning from human feedback with recursive task decomposition: it uses models trained on smaller parts of the task to assist humans in giving feedback on the broader task, and generates sensible summaries of entire books.</p>
                <p><strong>Paper Abstract:</strong> A major challenge for scaling machine learning is training models to perform tasks that are very difficult or time-consuming for humans to evaluate. We present progress on this problem on the task of abstractive summarization of entire fiction novels. Our method combines learning from human feedback with recursive task decomposition: we use models trained on smaller parts of the task to assist humans in giving feedback on the broader task. We collect a large volume of demonstrations and comparisons from human labelers, and fine-tune GPT-3 using behavioral cloning and reward modeling to do summarization recursively. At inference time, the model first summarizes small sections of the book and then recursively summarizes these summaries to produce a summary of the entire book. Our human labelers are able to supervise and evaluate the models quickly, despite not having read the entire books themselves. Our resulting model generates sensible summaries of entire books, even matching the quality of human-written summaries in a few cases ($\sim5\%$ of books). We achieve state-of-the-art results on the recent BookSum dataset for book-length summarization. A zero-shot question-answering model using these summaries achieves state-of-the-art results on the challenging NarrativeQA benchmark for answering questions about books and movie scripts. We release datasets of samples from our model.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3867.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3867.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Recursive Summarizer (RLHF)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Recursively Summarizing Books with Human Feedback</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based system that combines recursive task decomposition (algorithmic chunking and composition) with human feedback (behavioral cloning, reward modeling, and RL) to produce abstractive summaries of entire books by training on leaf-level summaries and composing them up a tree.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>Recursive summarization with human feedback (task decomposition + RLHF)</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>Algorithmically chunk the long source (book) into leaf passages, collect human demonstrations on leaf summarization, train a single pretrained transformer (GPT-3 family) with behavioral cloning, learn a reward model from human pairwise comparisons, and fine-tune the policy with reinforcement learning; at inference recursively summarize child summaries to produce higher-level summaries, including 'previous context' of prior summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>A subset of books from GPT-3 pretraining data (primarily narrative fiction; books average >100K words); leaf inputs ~600 tokens; trees typically split a book into ~200 leaf nodes and ~20 height-1 nodes; trees reach height 3 on average.</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Natural-language summarization tasks provided as per-node prompts: for leaf nodes 'summarize this passage' with length constraints; for composition nodes 'summarize the concatenation of child summaries'; decomposition is algorithmic (chunking) rather than learned.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Not a distillation-to-single-model approach in this work — uses recursive composition of summaries produced by the same policy; training uses behavioral cloning on human demonstrations and reward modeling + RL on human comparisons; no retrieval augmentation beyond the chunking; curriculum sampling to mitigate auto-induced distributional shift.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Abstractive summaries at multiple hierarchical 'heights' (length limits 128–384 tokens depending on height); final output is a full-book abstractive summary (single text) plus intermediate summaries at each node.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Human labeler Likert ratings (1–7) of full-book summaries and of subtasks; automated evaluation on BookSum (ROUGE, BERTScore) and NarrativeQA (zero-shot QA using summaries as input to a QA model); ablations comparing BC vs RL and model sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>A 175B-parameter model trained with RL on this procedure substantially outperformed BC baselines, achieved state-of-the-art results on BookSum (beat non-oracle baselines on ROUGE and BERTScore), produced some summaries approaching human level (~5% rated 6/7), and produced competitive zero-shot NarrativeQA results when summaries were fed into a large QA model.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Summaries can lack global coherence and sometimes omit or mischaracterize important details due to limited local context; task decomposition can be fundamentally limiting for phenomena that require nonlocal context; training higher-level tasks is difficult due to error accumulation and auto-induced distributional shift (ADS); RL at higher heights was challenging in practice; approach uses a fixed decomposition (chunking) which may miss cross-chunk signals.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>Outperforms supervised behavior-cloning baselines (same model family) and non-oracle baselines on BookSum; still on average significantly worse than human-written full-book summaries, though a small fraction of model summaries approach human quality.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Recursively Summarizing Books with Human Feedback', 'publication_date_yy_mm': '2021-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3867.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3867.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Stiennon 2020 (RLHF Summ.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Learning to summarize from human feedback</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method/paper that fine-tunes large pretrained language models using human preference comparisons (reward modeling + RL) to improve abstractive summarization quality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning to summarize from human feedback</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>Fine-tuning language models from human preferences for summarization (RLHF)</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>Collect human demonstrations and human pairwise comparisons of model outputs, fit a reward model to predict preferences, and use RL (with KL penalty) to optimize the policy against the learned reward.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>In prior work (as noted in this paper) reference summaries were scraped from Reddit TL;DRs; exact corpora vary by experiment (not detailed here).</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Summarization prompts/tasks (e.g., summarize an article) with human comparisons used to specify preferences.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Reward modeling from pairwise human comparisons followed by policy optimization via RL; also supervised fine-tuning (behavioral cloning) on demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Abstractive summaries; model outputs trained to match human preference signal.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Human preference evaluations and comparisons; automatic metrics reported in original work (not detailed here).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Reported in this paper as prior evidence that RL from human preferences can greatly improve summarization quality and in Stiennon et al. reportedly even outperformed certain human-written summaries (though dataset details and reference quality were caveats).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Quality of reference summaries used in prior work (e.g., scraped TL;DRs) can be variable; collecting demonstrations is more time-consuming than collecting comparisons; potential for reward misspecification.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>In prior work RL outperformed BC baselines and sometimes exceeded human reference summaries (with caveats about reference quality).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Recursively Summarizing Books with Human Feedback', 'publication_date_yy_mm': '2021-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3867.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3867.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Ziegler 2019</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fine-tuning language models from human preferences</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An earlier application of reward modeling and fine-tuning LMs from human preference data; foundational for later RLHF summarization work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Fine-tuning language models from human preferences</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>Fine-tuning language models from human preferences (reward modeling)</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>Train a reward model on human comparisons of model outputs and optimize a pretrained language model with RL to maximize predicted reward.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Reward modeling + RL (policy optimization with KL regularization).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Improved LM outputs aligned to human preferences (e.g., summaries, dialog responses).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Recursively Summarizing Books with Human Feedback', 'publication_date_yy_mm': '2021-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3867.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3867.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Christiano 2017</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deep reinforcement learning from human preferences</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Introduced a method for learning complex behaviors by training reward models from human comparisons and optimizing policies via RL; foundational for RLHF approaches used here.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deep reinforcement learning from human preferences</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>Reward learning from human preferences</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>Collect human pairwise preferences over short trajectory snippets, train a reward model to predict preferences, then use RL to optimize behavior according to the learned reward.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Human comparison-based reward modeling + RL.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Recursively Summarizing Books with Human Feedback', 'publication_date_yy_mm': '2021-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3867.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3867.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Leike 2018</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Scalable agent alignment via reward modeling: a research direction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A conceptual/research-direction paper proposing recursive reward modeling and decompositional approaches (including iterated amplification) for scaling human feedback to complex tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Scalable agent alignment via reward modeling: a research direction</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>Recursive reward modeling / iterated amplification (conceptual)</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>Proposes using decompositions and models' assistance to scale human oversight: train models on leaf tasks and use model outputs to help humans provide feedback on higher-level tasks (recursive reward modeling / iterated amplification).</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Recursive decomposition and reward modeling; theoretical proposal rather than a concrete experimental system within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Recursively Summarizing Books with Human Feedback', 'publication_date_yy_mm': '2021-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3867.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3867.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Cohan 2018</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>A discourse-aware attention model for abstractive summarization of long documents</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model for abstractive summarization of long documents (e.g., scientific papers) that uses discourse- and hierarchical-aware attention to handle long source texts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A discourse-aware attention model for abstractive summarization of long documents</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>Discourse-aware hierarchical summarization model</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>Hierarchical/discourse-aware encoder-decoder architecture designed to capture document structure for abstractive summarization of long documents (cited here as an approach for scientific-paper summarization).</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Applied to long documents (example domain: scientific papers) as discussed in related work.</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Document-level summarization (not described in detail here).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Hierarchical encoder and attention mechanisms (as an architectural solution rather than human-feedback-based distillation).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Abstractive long-document summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Recursively Summarizing Books with Human Feedback', 'publication_date_yy_mm': '2021-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3867.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e3867.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Abu-Jbara & Radev 2011</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Coherent citation-based summarization of scientific papers</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach for summarizing scientific papers that leverages citation contexts to produce coherent summaries of scientific articles (cited here among prior work on scientific-paper summarization).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Coherent citation-based summarization of scientific papers</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>Citation-based summarization of scientific papers</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>Uses citation contexts and coherence modeling to build summaries of scientific publications (extractive-oriented research cited in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Scientific papers and their citation contexts (not detailed here).</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Scientific-paper summaries (typically extractive or citation-synthesis outputs).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Recursively Summarizing Books with Human Feedback', 'publication_date_yy_mm': '2021-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3867.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e3867.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Izacard & Grave 2020</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Distilling knowledge from reader to retriever for question answering</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A study showing how to distill knowledge from a strong reader model into a retriever to improve open-domain question answering; cited here in the context of using summarized texts for downstream QA.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Distilling knowledge from reader to retriever for question answering</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>Reader-to-retriever distillation for QA</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>Train a retriever using signals from a powerful reader model (distillation) to improve retrieval for QA; discussed as related work in QA over long texts.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Model distillation (reader -> retriever) for improved retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Improved retriever components and end-to-end QA performance.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Recursively Summarizing Books with Human Feedback', 'publication_date_yy_mm': '2021-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3867.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e3867.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Perez et al. 2020</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Unsupervised question decomposition for question answering</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that decomposes complex questions into simpler subquestions to improve QA; cited as prior work that uses decomposition for QA tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Unsupervised question decomposition for question answering</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>Unsupervised question decomposition</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>Algorithmically decomposes complex questions into simpler ones (one-step decomposition used in prior QA work), used here as conceptual precedent for task decomposition.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Complex question given in natural language, decomposed into subquestions.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Task decomposition (unsupervised methods) rather than reward-model-based distillation.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Sets of subquestions/answers to support final QA.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Recursively Summarizing Books with Human Feedback', 'publication_date_yy_mm': '2021-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Learning to summarize from human feedback <em>(Rating: 2)</em></li>
                <li>Fine-tuning language models from human preferences <em>(Rating: 2)</em></li>
                <li>Deep reinforcement learning from human preferences <em>(Rating: 2)</em></li>
                <li>Scalable agent alignment via reward modeling: a research direction <em>(Rating: 2)</em></li>
                <li>A discourse-aware attention model for abstractive summarization of long documents <em>(Rating: 2)</em></li>
                <li>Coherent citation-based summarization of scientific papers <em>(Rating: 2)</em></li>
                <li>Distilling knowledge from reader to retriever for question answering <em>(Rating: 2)</em></li>
                <li>Unsupervised question decomposition for question answering <em>(Rating: 1)</em></li>
                <li>Booksum: A collection of datasets for long-form narrative summarization <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3867",
    "paper_id": "paper-a6fdb277d0a4b09899f802bda3359f5c2021a156",
    "extraction_schema_id": "extraction-schema-91",
    "extracted_data": [
        {
            "name_short": "Recursive Summarizer (RLHF)",
            "name_full": "Recursively Summarizing Books with Human Feedback",
            "brief_description": "An LLM-based system that combines recursive task decomposition (algorithmic chunking and composition) with human feedback (behavioral cloning, reward modeling, and RL) to produce abstractive summaries of entire books by training on leaf-level summaries and composing them up a tree.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_or_method_name": "Recursive summarization with human feedback (task decomposition + RLHF)",
            "system_or_method_description": "Algorithmically chunk the long source (book) into leaf passages, collect human demonstrations on leaf summarization, train a single pretrained transformer (GPT-3 family) with behavioral cloning, learn a reward model from human pairwise comparisons, and fine-tune the policy with reinforcement learning; at inference recursively summarize child summaries to produce higher-level summaries, including 'previous context' of prior summaries.",
            "input_corpus_description": "A subset of books from GPT-3 pretraining data (primarily narrative fiction; books average &gt;100K words); leaf inputs ~600 tokens; trees typically split a book into ~200 leaf nodes and ~20 height-1 nodes; trees reach height 3 on average.",
            "topic_or_query_specification": "Natural-language summarization tasks provided as per-node prompts: for leaf nodes 'summarize this passage' with length constraints; for composition nodes 'summarize the concatenation of child summaries'; decomposition is algorithmic (chunking) rather than learned.",
            "distillation_method": "Not a distillation-to-single-model approach in this work — uses recursive composition of summaries produced by the same policy; training uses behavioral cloning on human demonstrations and reward modeling + RL on human comparisons; no retrieval augmentation beyond the chunking; curriculum sampling to mitigate auto-induced distributional shift.",
            "output_type_and_format": "Abstractive summaries at multiple hierarchical 'heights' (length limits 128–384 tokens depending on height); final output is a full-book abstractive summary (single text) plus intermediate summaries at each node.",
            "evaluation_or_validation_method": "Human labeler Likert ratings (1–7) of full-book summaries and of subtasks; automated evaluation on BookSum (ROUGE, BERTScore) and NarrativeQA (zero-shot QA using summaries as input to a QA model); ablations comparing BC vs RL and model sizes.",
            "results_summary": "A 175B-parameter model trained with RL on this procedure substantially outperformed BC baselines, achieved state-of-the-art results on BookSum (beat non-oracle baselines on ROUGE and BERTScore), produced some summaries approaching human level (~5% rated 6/7), and produced competitive zero-shot NarrativeQA results when summaries were fed into a large QA model.",
            "limitations_or_challenges": "Summaries can lack global coherence and sometimes omit or mischaracterize important details due to limited local context; task decomposition can be fundamentally limiting for phenomena that require nonlocal context; training higher-level tasks is difficult due to error accumulation and auto-induced distributional shift (ADS); RL at higher heights was challenging in practice; approach uses a fixed decomposition (chunking) which may miss cross-chunk signals.",
            "comparison_to_baselines_or_humans": "Outperforms supervised behavior-cloning baselines (same model family) and non-oracle baselines on BookSum; still on average significantly worse than human-written full-book summaries, though a small fraction of model summaries approach human quality.",
            "uuid": "e3867.0",
            "source_info": {
                "paper_title": "Recursively Summarizing Books with Human Feedback",
                "publication_date_yy_mm": "2021-09"
            }
        },
        {
            "name_short": "Stiennon 2020 (RLHF Summ.)",
            "name_full": "Learning to summarize from human feedback",
            "brief_description": "A method/paper that fine-tunes large pretrained language models using human preference comparisons (reward modeling + RL) to improve abstractive summarization quality.",
            "citation_title": "Learning to summarize from human feedback",
            "mention_or_use": "mention",
            "system_or_method_name": "Fine-tuning language models from human preferences for summarization (RLHF)",
            "system_or_method_description": "Collect human demonstrations and human pairwise comparisons of model outputs, fit a reward model to predict preferences, and use RL (with KL penalty) to optimize the policy against the learned reward.",
            "input_corpus_description": "In prior work (as noted in this paper) reference summaries were scraped from Reddit TL;DRs; exact corpora vary by experiment (not detailed here).",
            "topic_or_query_specification": "Summarization prompts/tasks (e.g., summarize an article) with human comparisons used to specify preferences.",
            "distillation_method": "Reward modeling from pairwise human comparisons followed by policy optimization via RL; also supervised fine-tuning (behavioral cloning) on demonstrations.",
            "output_type_and_format": "Abstractive summaries; model outputs trained to match human preference signal.",
            "evaluation_or_validation_method": "Human preference evaluations and comparisons; automatic metrics reported in original work (not detailed here).",
            "results_summary": "Reported in this paper as prior evidence that RL from human preferences can greatly improve summarization quality and in Stiennon et al. reportedly even outperformed certain human-written summaries (though dataset details and reference quality were caveats).",
            "limitations_or_challenges": "Quality of reference summaries used in prior work (e.g., scraped TL;DRs) can be variable; collecting demonstrations is more time-consuming than collecting comparisons; potential for reward misspecification.",
            "comparison_to_baselines_or_humans": "In prior work RL outperformed BC baselines and sometimes exceeded human reference summaries (with caveats about reference quality).",
            "uuid": "e3867.1",
            "source_info": {
                "paper_title": "Recursively Summarizing Books with Human Feedback",
                "publication_date_yy_mm": "2021-09"
            }
        },
        {
            "name_short": "Ziegler 2019",
            "name_full": "Fine-tuning language models from human preferences",
            "brief_description": "An earlier application of reward modeling and fine-tuning LMs from human preference data; foundational for later RLHF summarization work.",
            "citation_title": "Fine-tuning language models from human preferences",
            "mention_or_use": "mention",
            "system_or_method_name": "Fine-tuning language models from human preferences (reward modeling)",
            "system_or_method_description": "Train a reward model on human comparisons of model outputs and optimize a pretrained language model with RL to maximize predicted reward.",
            "input_corpus_description": "",
            "topic_or_query_specification": "",
            "distillation_method": "Reward modeling + RL (policy optimization with KL regularization).",
            "output_type_and_format": "Improved LM outputs aligned to human preferences (e.g., summaries, dialog responses).",
            "evaluation_or_validation_method": "",
            "results_summary": "",
            "limitations_or_challenges": "",
            "comparison_to_baselines_or_humans": "",
            "uuid": "e3867.2",
            "source_info": {
                "paper_title": "Recursively Summarizing Books with Human Feedback",
                "publication_date_yy_mm": "2021-09"
            }
        },
        {
            "name_short": "Christiano 2017",
            "name_full": "Deep reinforcement learning from human preferences",
            "brief_description": "Introduced a method for learning complex behaviors by training reward models from human comparisons and optimizing policies via RL; foundational for RLHF approaches used here.",
            "citation_title": "Deep reinforcement learning from human preferences",
            "mention_or_use": "mention",
            "system_or_method_name": "Reward learning from human preferences",
            "system_or_method_description": "Collect human pairwise preferences over short trajectory snippets, train a reward model to predict preferences, then use RL to optimize behavior according to the learned reward.",
            "input_corpus_description": "",
            "topic_or_query_specification": "",
            "distillation_method": "Human comparison-based reward modeling + RL.",
            "output_type_and_format": "",
            "evaluation_or_validation_method": "",
            "results_summary": "",
            "limitations_or_challenges": "",
            "comparison_to_baselines_or_humans": "",
            "uuid": "e3867.3",
            "source_info": {
                "paper_title": "Recursively Summarizing Books with Human Feedback",
                "publication_date_yy_mm": "2021-09"
            }
        },
        {
            "name_short": "Leike 2018",
            "name_full": "Scalable agent alignment via reward modeling: a research direction",
            "brief_description": "A conceptual/research-direction paper proposing recursive reward modeling and decompositional approaches (including iterated amplification) for scaling human feedback to complex tasks.",
            "citation_title": "Scalable agent alignment via reward modeling: a research direction",
            "mention_or_use": "mention",
            "system_or_method_name": "Recursive reward modeling / iterated amplification (conceptual)",
            "system_or_method_description": "Proposes using decompositions and models' assistance to scale human oversight: train models on leaf tasks and use model outputs to help humans provide feedback on higher-level tasks (recursive reward modeling / iterated amplification).",
            "input_corpus_description": "",
            "topic_or_query_specification": "",
            "distillation_method": "Recursive decomposition and reward modeling; theoretical proposal rather than a concrete experimental system within this paper.",
            "output_type_and_format": "",
            "evaluation_or_validation_method": "",
            "results_summary": "",
            "limitations_or_challenges": "",
            "comparison_to_baselines_or_humans": "",
            "uuid": "e3867.4",
            "source_info": {
                "paper_title": "Recursively Summarizing Books with Human Feedback",
                "publication_date_yy_mm": "2021-09"
            }
        },
        {
            "name_short": "Cohan 2018",
            "name_full": "A discourse-aware attention model for abstractive summarization of long documents",
            "brief_description": "A model for abstractive summarization of long documents (e.g., scientific papers) that uses discourse- and hierarchical-aware attention to handle long source texts.",
            "citation_title": "A discourse-aware attention model for abstractive summarization of long documents",
            "mention_or_use": "mention",
            "system_or_method_name": "Discourse-aware hierarchical summarization model",
            "system_or_method_description": "Hierarchical/discourse-aware encoder-decoder architecture designed to capture document structure for abstractive summarization of long documents (cited here as an approach for scientific-paper summarization).",
            "input_corpus_description": "Applied to long documents (example domain: scientific papers) as discussed in related work.",
            "topic_or_query_specification": "Document-level summarization (not described in detail here).",
            "distillation_method": "Hierarchical encoder and attention mechanisms (as an architectural solution rather than human-feedback-based distillation).",
            "output_type_and_format": "Abstractive long-document summaries.",
            "evaluation_or_validation_method": "",
            "results_summary": "",
            "limitations_or_challenges": "",
            "comparison_to_baselines_or_humans": "",
            "uuid": "e3867.5",
            "source_info": {
                "paper_title": "Recursively Summarizing Books with Human Feedback",
                "publication_date_yy_mm": "2021-09"
            }
        },
        {
            "name_short": "Abu-Jbara & Radev 2011",
            "name_full": "Coherent citation-based summarization of scientific papers",
            "brief_description": "An approach for summarizing scientific papers that leverages citation contexts to produce coherent summaries of scientific articles (cited here among prior work on scientific-paper summarization).",
            "citation_title": "Coherent citation-based summarization of scientific papers",
            "mention_or_use": "mention",
            "system_or_method_name": "Citation-based summarization of scientific papers",
            "system_or_method_description": "Uses citation contexts and coherence modeling to build summaries of scientific publications (extractive-oriented research cited in related work).",
            "input_corpus_description": "Scientific papers and their citation contexts (not detailed here).",
            "topic_or_query_specification": "",
            "distillation_method": "",
            "output_type_and_format": "Scientific-paper summaries (typically extractive or citation-synthesis outputs).",
            "evaluation_or_validation_method": "",
            "results_summary": "",
            "limitations_or_challenges": "",
            "comparison_to_baselines_or_humans": "",
            "uuid": "e3867.6",
            "source_info": {
                "paper_title": "Recursively Summarizing Books with Human Feedback",
                "publication_date_yy_mm": "2021-09"
            }
        },
        {
            "name_short": "Izacard & Grave 2020",
            "name_full": "Distilling knowledge from reader to retriever for question answering",
            "brief_description": "A study showing how to distill knowledge from a strong reader model into a retriever to improve open-domain question answering; cited here in the context of using summarized texts for downstream QA.",
            "citation_title": "Distilling knowledge from reader to retriever for question answering",
            "mention_or_use": "mention",
            "system_or_method_name": "Reader-to-retriever distillation for QA",
            "system_or_method_description": "Train a retriever using signals from a powerful reader model (distillation) to improve retrieval for QA; discussed as related work in QA over long texts.",
            "input_corpus_description": "",
            "topic_or_query_specification": "",
            "distillation_method": "Model distillation (reader -&gt; retriever) for improved retrieval.",
            "output_type_and_format": "Improved retriever components and end-to-end QA performance.",
            "evaluation_or_validation_method": "",
            "results_summary": "",
            "limitations_or_challenges": "",
            "comparison_to_baselines_or_humans": "",
            "uuid": "e3867.7",
            "source_info": {
                "paper_title": "Recursively Summarizing Books with Human Feedback",
                "publication_date_yy_mm": "2021-09"
            }
        },
        {
            "name_short": "Perez et al. 2020",
            "name_full": "Unsupervised question decomposition for question answering",
            "brief_description": "A method that decomposes complex questions into simpler subquestions to improve QA; cited as prior work that uses decomposition for QA tasks.",
            "citation_title": "Unsupervised question decomposition for question answering",
            "mention_or_use": "mention",
            "system_or_method_name": "Unsupervised question decomposition",
            "system_or_method_description": "Algorithmically decomposes complex questions into simpler ones (one-step decomposition used in prior QA work), used here as conceptual precedent for task decomposition.",
            "input_corpus_description": "",
            "topic_or_query_specification": "Complex question given in natural language, decomposed into subquestions.",
            "distillation_method": "Task decomposition (unsupervised methods) rather than reward-model-based distillation.",
            "output_type_and_format": "Sets of subquestions/answers to support final QA.",
            "evaluation_or_validation_method": "",
            "results_summary": "",
            "limitations_or_challenges": "",
            "comparison_to_baselines_or_humans": "",
            "uuid": "e3867.8",
            "source_info": {
                "paper_title": "Recursively Summarizing Books with Human Feedback",
                "publication_date_yy_mm": "2021-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Learning to summarize from human feedback",
            "rating": 2
        },
        {
            "paper_title": "Fine-tuning language models from human preferences",
            "rating": 2
        },
        {
            "paper_title": "Deep reinforcement learning from human preferences",
            "rating": 2
        },
        {
            "paper_title": "Scalable agent alignment via reward modeling: a research direction",
            "rating": 2
        },
        {
            "paper_title": "A discourse-aware attention model for abstractive summarization of long documents",
            "rating": 2
        },
        {
            "paper_title": "Coherent citation-based summarization of scientific papers",
            "rating": 2
        },
        {
            "paper_title": "Distilling knowledge from reader to retriever for question answering",
            "rating": 2
        },
        {
            "paper_title": "Unsupervised question decomposition for question answering",
            "rating": 1
        },
        {
            "paper_title": "Booksum: A collection of datasets for long-form narrative summarization",
            "rating": 1
        }
    ],
    "cost": 0.01800125,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Recursively Summarizing Books with Human Feedback</h1>
<p>Jeff Wu<em> Long Ouyang</em> Daniel M. Ziegler<em> Nisan Stiennon</em> Ryan Lowe<em><br>Jan Leike</em><br>Paul Christiano*<br>OpenAI</p>
<h4>Abstract</h4>
<p>A major challenge for scaling machine learning is training models to perform tasks that are very difficult or time-consuming for humans to evaluate. We present progress on this problem on the task of abstractive summarization of entire fiction novels. Our method combines learning from human feedback with recursive task decomposition: we use models trained on smaller parts of the task to assist humans in giving feedback on the broader task. We collect a large volume of demonstrations and comparisons from human labelers, and fine-tune GPT-3 using behavioral cloning and reward modeling to do summarization recursively. At inference time, the model first summarizes small sections of the book and then recursively summarizes these summaries to produce a summary of the entire book. Our human labelers are able to supervise and evaluate the models quickly, despite not having read the entire books themselves. Our resulting model generates sensible summaries of entire books, even matching the quality of human-written summaries in a few cases ( $\sim 5 \%$ of books). We achieve state-of-the-art results on the recent BookSum dataset for book-length summarization. A zero-shot question-answering model using these summaries achieves competitive results on the challenging NarrativeQA benchmark for answering questions about books and movie scripts. We release datasets of samples from our model. ${ }^{2}$</p>
<h2>1 Introduction</h2>
<p>To train an ML model on a new task, we need a training signal that tells the model which behaviors are better and which are worse. For some tasks, like playing a video game, this training signal can be calculated automatically. However, for many useful tasks an accurate training signal can only be provided via a human in the loop. For example, humans can provide demonstrations of the correct behavior (Bain and Sammut, 1995) or compare two outputs from the model being trained (Christiano et al., 2017), and this data is used to train the model.</p>
<p>In this paper we focus on tasks that are difficult for humans to supervise or evaluate, either because the tasks take a lot of time or because they require specialized knowledge and expertise to evaluate. For example, imagine training a model to summarize an entire sub-field of scientific research. For a human to provide a demonstration or evaluate the quality of a model-generated summary, they would likely need a huge amount of time and expertise. One could circumvent this difficulty by using easier-to-measure proxy objectives (e.g. how often words in the summary relate to the topic, and how accurate individual sentences in the summary are), but these proxies are usually less aligned with</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>Summaries of "A Promised Land" by Barack Obama</h1>
<h2>Best human-written summary</h2>
<p>Barack Obama details his journey from a flippant Hawaiian high schooler raised by a traveling, unconventional mother to the first African American President of the United States. His political career is influenced by the tales of his mother, grandmother Toot, his childhood international travels, and civil rights leaders. While he's in Harvard Law, he meets and marries Michelle Robinson. They have two daughters, Malia in 1998 and Sasha in 2001, while he is a Democratic Illinois Senator. Barack juggles marriage and fatherhood with a political life that keeps him from home. This often causes strain with Michelle. In 2004, he wins a seat in the U.S. Senate. By 2006, several Democratic senators urge him to run for President in 2008. He feels a good campaign could shake loose the traces of America's racial past. He wins the primary after a contentious battle with Hillary Clinton. After a groundbreaking grassroots campaign, Barack wins the general election against Republican John McCain. [...]</p>
<h2>Best 175B RL summary</h2>
<p>Barack Obama wins the presidency in 2008, and his first term is marked by economic turmoil and political polarization. His domestic policy includes the Affordable Care Act, which reformed healthcare in the United States.
Obama's presidency includes terrorism, the Arab-Israeli conflict, and the Deepwater Horizon oil spill. His foreign policy includes finding and eliminating Osama bin Laden. After the successful Abbottabad raid, Obama reflects on the unity of effort displayed by all the individuals involved in the operation. He wonders if that same unity of effort could be applied to other areas of government.</p>
<h2>Best 175B BC summary</h2>
<p>Barack Obama writes a book about his experiences as president. He explains that he started questioning the U.S. government when he was at Occidental College. He didn't believe in politics, but was inspired by social movements. He ran Project VOTE! in advance of the 1992 presidential race. Obama thinks that the politics are going to be brutal. He introduces Laura. She has been diagnosed with aggressive breast cancer that had spread to her bones. The U.S. policy in the Middle East focuses narrowly on maintaining stability, preventing disruptions to our oil supplies, and keeping adversarial powers from expanding their influence.
Table 1: Summaries of "A Promised Land" written by our models (175B RL policy and 175B behavior cloning policy) and a human. Since this book was written in 2020, it was not seen by our model during either pretraining or fine-tuning. For each model, we select the summary with the highest Likert rating according to human evaluations (out of 6 summaries for RL, 9 for BC, and 2 for the human-written summary). See our website for all human and model summaries.
our actual goals, and optimizing them can have unintended consequences (Clark and Amodei, 2016; Krakovna et al., 2020; Amodei et al., 2016).
Successfully training ML systems on such tasks will require more scalable means of producing an effective training signal - this problem is known as scalable oversight (Amodei et al., 2016).
Our approach to scalable oversight is directly inspired by Christiano et al. (2018) and Leike et al. (2018), who make use of task decomposition (Singh, 1992; Dayan and Hinton, 1993) and learning from human feedback. At a high level, these methods take a top-level task and decompose it into several smaller subtasks whose answers would help a human solve or evaluate the top-level task. These subtasks can in turn be decomposed into smaller tasks until it is feasible for humans to provide a training signal for a leaf task. ML models can be trained to solve the leaf tasks, to solve higher-level tasks given answers to the lower-level tasks, and to decompose the harder tasks into subtasks. While Dayan and Hinton (1993) and Christiano et al. (2018) only tried this on simple algorithmic tasks, Perez et al. (2020) and Rajani et al. (2019) use similar ideas for question-answering tasks using a single step of decomposition.
We take a step further in this direction by scaling task decomposition to abstractive book summarization. Abstractive book summarization is a difficult task, where dataset collection is challenging (Mihalcea and Ceylan, 2007; Ladhak et al., 2020; Kryściński et al., 2021) and existing methods are typically either extractive (Radev et al., 2004; Mihalcea and Tarau, 2004; Bamman and Smith, 2013; Ladhak et al., 2020) or focused on shorter stories (Kazantseva, 2006; Zhang et al., 2019b).
We implement a natural task decomposition for long-form summarization: first, we train models to summarize small parts of the book, and then use these models to help humans summarize larger sections of the book, and continue with this strategy recursively. We train a single model to perform these tasks using standard cross-entropy behavioral cloning (BC) and reinforcement learning (RL) from human preferences (Christiano et al., 2017).</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Our procedure for summarizing books that combines task decomposition with learning from human feedback. We first decompose the book text into multiple chunks using a fixed (not learned) chunking algorithm (height 0). We then collect demonstrations from humans summarizing these chunks, and train an ML model on this data using behavior cloning (each node with a pencil symbol corresponds to a summarization task carried out by either a human or model). We can then collect human data comparing different model outputs, and use this data to further train the summarization policy using reward modeling (Christiano et al., 2017). We then concatenate several height 0 summaries, collect data for summarizing these summaries, and fine-tune our model on this summarization task (height 1). We repeat this procedure recursively until we've summarized the entire book. We use the same policy to summarize text at all levels. Summarization tasks later in the book are conditioned on previous summaries at the same height; we show this for the blue task at height 1 using dotted arrows, but this happens at all levels of the tree.</p>
<p>Our main result is a model that can be applied recursively to generate plausible summaries of entire books. Our approach lets us summarize books of arbitrary length – we achieve believable summaries on books with hundreds of thousands of words by recursing to depth 3. With a non-recursive approach, generating or evaluating a book summary requires a human reading the entire book, so naively collecting such a dataset is over 50x more expensive per data point (see Appendix E.2).</p>
<p>Qualitatively, these summaries contain important events from the book, and sometimes synthesize these details abstractively; however, they often leave out important details or fail to grasp the broader context. When evaluated quantitatively, our model significantly outperforms our behavioral cloning baseline, and a small number of summaries approach human-level quality. Separately, we perform an ablation comparing RL to BC on summarizing smaller sections of a book, and find that RL has better scaling properties. We also evaluate our summaries with the NarrativeQA question-answering dataset (Kočiskỳ et al., 2018) and find that a zero-shot model taking our summaries as input achieves competitive results at answering questions about books and movie scripts. We also achieve state-of-the-art results on the recent BookSum dataset (Kryściński et al., 2021) for book-length summarization.</p>
<p>Overall, our results show that combining recursive task decomposition with learning from human feedback can be a practical approach to scalable oversight for difficult long-document NLP tasks. We hope that our work encourages more research in using models trained on simpler tasks to aid humans in providing training signals on more difficult tasks.</p>
<h1>2 Approach</h1>
<h3>2.1 Task decomposition</h3>
<p>Consider a task for which it is very expensive for a human to provide a training signal. Christiano et al. (2018), Irving et al. (2018), and Leike et al. (2018) all propose in some way reducing the task into simpler parts which humans can supervise.
In task decomposition, a human decomposes this parent task into several subtasks, such that each subtask is simpler than the parent task, and having the responses to the subtasks would help a human provide a training signal for the parent task. This task decomposition process can be applied recursively to obtain a tree of tasks, such that the leaf tasks are simple enough for a human to demonstrate or evaluate. For example, the parent task "Write a research report on climate change interventions" might decompose into a subtask like: "Give me a list of the most promising climate change interventions", which then further decomposes into simpler tasks like "How effective is reducing food waste?" and "What are ways to make nations coordinate in avoiding tragedy of the commons scenarios?".</p>
<p>If we repeat this process many times, we obtain a dataset that we can use to train an ML model. Specifically, given a (sub)task we want to train a model that can perform two fundamental operations:</p>
<ol>
<li>Decompose: Ask for responses to a set of simpler tasks.</li>
<li>Respond: Given responses to some number (possibly none) of simpler tasks, respond to the original task. When there are simpler tasks used, we sometimes refer to the operation as Compose, since it composes the sub-responses into an overall response.</li>
</ol>
<p>Then any task can be performed via a recursive procedure if it is amenable to decomposition; we show a pseudocode implementation in Appendix A. It remains an open question to what extent natural tasks are actually amenable to decomposition (Ought, 2020).
While the framework above is fully general, it can be further simplified if the task lends itself to a simple recursive structure where the decomposition operation can be performed algorithmically, and the ML model only needs to be trained on the Respond operation.</p>
<h3>2.2 Decomposition for book summarization</h3>
<p>We use a simple procedure to algorithmically decompose a summarization task for a piece of text: If the text is short enough, summarize it directly. If it is longer, chunk the text into smaller pieces, and recursively ask to summarize each one. This results in a tree of summarization tasks (see Figure 1), where only the leaf tasks operate on passages of the original book text.
Each task, corresponding to nodes with pencil symbols in Figure 1, has a height and depth, which correspond to the standard terminology used for trees. The height of a node is the length of the longest downward path to a leaf from that node. A height 0 task is a leaf task, where the goal is to summarize the original book text. We sometimes refer to tasks that are height $&gt;0$ as composition tasks, since the input is a concatenation of summaries, and the goal is to produce another summary. The depth of a node is the length of the path from the node to the root. A depth 0 task is the final summarization task, where the goal is to produce a summary of an entire book (given summaries produced from the depth 1 tasks).
An evident issue with the above approach is that tasks corresponding to passages further into a book may lack the necessary context for a successful summary. We remedy this by additionally putting prior summaries in context, from the same depth, concatenated together in order. ${ }^{3}$ We call these summaries the previous context. In Figure 1, the previous summaries inputs for the blue task are indicated using dotted lines. We include as many prior summaries as can fit in the model's context length. We would like each summary to flow naturally from the previous context, since it may get concatenated with it at a higher height or in the previous context for a later task.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>A convenient property of this decomposition is that all of the tasks in the tree are extremely similar to one another. Every task for the model is a summarization task that can be formatted the same way. The input text is either the original book text or a concatenation of summaries, and we optionally have additional previous context in the form of summaries.</p>
<p>Pseudocode and detailed parameters of tree construction can be found in Appendix A.5.</p>
<h1>2.3 Training</h1>
<p>For training the model, we most closely follow the procedure of Stiennon et al. (2020). We start with a pretrained language model and a pool of trained human labelers (see Appendix B for details). We collect demonstrations from labelers and train a model via behavioral cloning. We then repeat many iterations of reward learning and reinforcement learning. To learn the reward function, we collect comparisons from labelers on outputs from the current best policy and train a reward model to predict log odds that a response is better. Reinforcement learning directly optimizes the reward with an additional KL term to prevent too much drift from the initial policy, typically our best supervised policy. More details in Appendix D.</p>
<p>To collect a label for a given task, we need to generate its inputs: if a node is not a leaf, we run an existing model (typically the best available) recursively to generate summaries for each of its children.</p>
<p>In summary, we use the following algorithm:</p>
<ol>
<li>Recursively decompose books (and compose child summaries) into tasks using the procedure described in 2.2, using the best models we have ${ }^{4}$ and the best sampling parameters we have ${ }^{5}$. While this could be done with humans, it would be prohibitively expensive.</li>
<li>Sample a node from the tree, corresponding to a summarization task which we'd like to train on. ${ }^{6}$ Details below in 2.3.2.</li>
<li>Obtain training data, given the inputs to that node
(a) For demonstrations, we then have human labelers write a desired output
(b) For comparisons, we run the model we wish to train to obtain two outputs, typically at temperature 1 . We then ask human labelers to choose which output is better.</li>
<li>We then finetune the model using the training data
(a) For demonstrations, we use behavior cloning (BC). We do a supervised finetune using the standard cross entropy loss function.
(b) For comparisons, we use reinforcement learning (RL) against a reward model trained to predict human preferences.</li>
</ol>
<p>We can iterate this entire process with newer models, different node sampling strategies, and different choice of training data type (demonstration versus comparison).</p>
<h3>2.3.1 Auto-induced distributional shift</h3>
<p>Since each model is trained on inputs produced by a different model, inputs produced by itself are outside of the training distribution, thus causing auto-induced distributional shift (ADS) (Krueger et al., 2020). This effect is more severe at later parts in the tree computation (later in the book, and especially higher in the tree). This means that after each round of training, running the full procedure always results in inputs out of the prior training distributions, for tasks at non-zero height. While we did not systematically measure the severity of this effect, in practice we generally found that additional rounds of training at height 0 resulted in better-rated summaries at height 1 .</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>2.3.2 Training curriculum</h1>
<p>Because of the ADS mentioned in Section 2.3.1, it is advantageous to prioritize training on nodes earlier/lower in the tree computation, before moving to nodes later in the computation.
We define the following terms:</p>
<ul>
<li>First subtree. The first subtree refers to the first height 1 task, and its height 0 child tasks (of which there are typically 10-13). See the yellow nodes in Figure 1 for an example. In Section 4.1, we find that by training on merely the first subtree, the model can generalize to the entire tree.</li>
<li>First leaves. The first leaves refers to the height 0 tasks in the first subtree, i.e. those which are children of the first height 1 task.</li>
</ul>
<p>For early rounds, we initially train only on the first leaves, since inputs to later nodes depend on having plausible summaries from earlier nodes, and we do not want to use excessive human time. We then move to the entire first subtree (additionally training on a single height 1 task), once the summaries for the first leaves look reasonable. At this point, our model is already capable of generalizing to the full tree, and we switch to training on all nodes. Curriculum changes were made in an ad hoc manner, moving on when we deemed the models "good enough" at earlier tasks.</p>
<h3>2.3.3 Fine-tuning details</h3>
<p>We use pretrained transformer language models (Vaswani et al., 2017) from the GPT-3 family (Brown et al., 2020), which take 2048 tokens of context. Input tokens are produced by the byte pair encoding introduced in Radford et al. (2019). Other architecture and hyperparameters choices follow those of Stiennon et al. (2020). More details in Appendix D.</p>
<p>Behavioral cloning and reward modeling In the first leaves phase of the project, we collect data for all first leaves together. When moving to first subtree, we independently collect data for the height 1 tasks, letting us vary the ratio of training data at the different heights. Finally, for the full tree phase, we follow a strategy of first randomly sampling a depth, and then randomly selecting a task amongst tasks at that depth. Inputs are typically generated using the best model available and best guess sampling parameters (see Appendix D.2).
In all cases, we train on all past data (individual demonstrations and comparisons for tasks from various parts of the tree). We then shuffle and sample tasks randomly.</p>
<p>Reinforcement learning We ran three variants of sampling tasks for reinforcement learning episodes, corresponding to our changes in the training curriculum.</p>
<ol>
<li>The first leaves: Each episode is a single first leaf task. The algorithm trains on consecutive leaf tasks in succession; the sampled summaries are used as previous context for later leaves.</li>
<li>The first subtree: Each episode consists of a first leaf task or the height 1 composition task for the first subtree. The algorithm trains on the leaf tasks in succession, followed by the composition task using their sampled outputs.</li>
<li>Full tree: We choose a random depth $d$ and then a random node at that depth. The algorithm trains on N successive depth $d+1$ tasks followed by a single depth $d$ composition task using those N outputs. Input trees are generated ahead of time from the initial model with best-effort sampling settings (in practice, we sometimes use some trees from older models as well).</li>
</ol>
<p>Since our demonstration and comparison data is at the level of individual nodes, we train the RL policy at the same granularity: each task is its own episode, and no rewards propagate to other nodes of the tree.</p>
<h3>2.4 Advantages of decomposition</h3>
<p>Compared to end-to-end training, decomposition makes it much easier to collect human feedback for a given task. Correspondingly, it makes the task much easier for the ML model. But it also offers other benefits:</p>
<ol>
<li>It empowers a human to do or evaluate parts of the task themself. For example, a human with access to lower-level summaries can quickly summarize themselves.</li>
<li>It makes it easier to trace what the model is thinking, and debug errors in the model. If a model summary contains a relatively isolated fact, a human with access to the tree can trace it back to the original text.</li>
<li>Our procedure generalizes gracefully to longer books. It can be used at test time on books of unbounded length, regardless of the length of books in the training dataset.</li>
</ol>
<h1>3 Task details</h1>
<h3>3.1 Training dataset</h3>
<p>For training, we use a subset of the books used in GPT-3's training data (Brown et al., 2020). The books are primarily fiction, and contain over 100K words on average. We further constrain our dataset by asking labelers to skip non-narrative books.</p>
<p>We chose narrative fiction books due to our belief that they were the most difficult to summarize, which is supported by our later qualitative findings (Appendix J). Summarizing narrative texts is particularly challenging for extractive methods since any given sentence tends to be a very low-level description. We find additional evidence for this in Section 4.2, where our models outperform an extractive oracle on the BERTScore metric.</p>
<h3>3.2 Summarization task</h3>
<p>We aim to summarize abstractively, tracing out narrative arcs and larger themes rather than listing series of events. Our primary metric is labeler judgments of overall summary quality on a 1-7 Likert scale, on held-out books that were neither in the GPT-3 pretraining dataset nor in our book dataset. We also ask labelers to evaluate summary accuracy, coverage of the source text, coherence, and amount of abstraction; see more details on our instructions to labelers in Appendix C.1.
For each summarization subtask, we generally aim to compress the text by a factor of 5-10x, with length upper limits of 128 to 384 tokens, depending on the task height. We ask labelers to evaluate summary quality conditioned on its length; that is, labelers are answering the question "how good is this summary, given that it is X words long?" This is in part to avoid the scenario where, if longer summaries are preferred by labelers, models will generate the longest summaries allowed by the length constraints (Stiennon et al., 2020).
We emphasize that for each subtask, labelers only consider the quality of the summary with respect to the direct input to the model, rather than the subset of the book representing the true summarization target. See Appendix A. 3 for more discussion.</p>
<h2>4 Results</h2>
<h3>4.1 Full book human evaluations</h3>
<h3>4.1.1 Methodology</h3>
<p>We first evaluate our models' ability to summarize full books that were unseen during pretraining or fine-tuning. To do this, we use the 40 most popular books published in 2020 according to Goodreads at the time we looked. The resulting books span a variety of genres (see Table 5).
We then assigned two labelers to read each book (purchased with reimbursement) and to write a summary of the book. Finally, we ask the labelers to rate summaries from various models and from the other labeler. Labeler agreement for relative quality of model-written summaries was nearly $80 \%$.
We evaluate two model sizes, 175B parameters and 6B parameters. For each size, we also evaluate three different modes of training: RL on the whole tree, RL on the first subtree, and BC on the whole tree. For each policy, we generate 3 summaries each, in order to reduce error bars. Even for temperature 0 policies, we can vary the summaries by changing the seed used to randomly choose chunking boundaries - we found this to produce significant variation in the summaries.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Results on full book evaluations, (a) as a function of model size (measured in billions of parameters), and (b) as a function of number of labels. Error bars are obtained by averaging ratings for each book, then computing the standard error of the mean across books. At larger model sizes, our RL models significantly outperform our BC models ('supervised on full tree'). Our models are still far from human performance.</p>
<p>We evaluated all BC policies at temperatures $\mathrm{T}=0.0,0.3$, and 0.6 on this test set. The results in Figures 2 and 3 use the best temperatures for these policies. ${ }^{7}$ This is because it was too expensive to ablate temperature on the full book summarization task on our validation set (though we we show temperature sweeps on the validation set for leaf summarization tasks in Appendix D.2, these temperatures are not a priori the best for full book summarization). In the end, we empirically found that the best temperatures for the leaf task were also the best for full book summarization: $\mathrm{T}=0.6$ was best for our 6B BC baseline, and all temperatures performed about equally for our 175B BC baseline.</p>
<h1>4.1.2 Findings</h1>
<p>Our best models can generate realistic summaries of books unseen during training. Some of these summaries approach human-level quality: over $5 \%$ of summaries from the best 175B model were given a score of 6 out of 7 , and over $15 \%$ were given a 5 out of 7 , scores which were also sometimes assigned to human-written summaries (Figure 3). However, on average our model summaries are still significantly worse than human-written summaries (Figure 2a), See our website ${ }^{8}$ for our model summaries and ratings.
We find that training on the first subtree does comparably to training on the full tree (Figure 2b). Our models trained on just the first subtree generalize quite well to the full book summarization task. However, we also found the full tree models disappointing; the final 175B full tree model we trained was noticeably worse than the previous one. ${ }^{9}$ We discuss possible reasons for this in Appendix G. We also find that our 175B RL policies significantly outperform our 175B BC baseline, though the improvement is smaller for the 6B models.
Likert scores for the full book summaries were significantly lower than Likert scores of any of the individual decomposed tasks. This is unsurprising, since the errors accumulated at each depth are all reflected in the full book summary score. See Appendix A. 3 for more discussion.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Likert distribution for summaries from our supervised baselines (SL, same as BC), our best full tree RL models, our first subtree RL 175B model, and humans.</p>
<h1>4.2 BookSum results</h1>
<p>We also evaluate our models on the recently proposed BookSum dataset for book-length summarization (Kryściński et al., 2021) We compare to the best extractive (BertExt; Liu and Lapata, 2019b) and abstractive (T5; Raffel et al., 2019) models, as well as an extractive oracle (which uses the reference summary to find the sentences in the source text that lead to the highest score). Kryściński et al. (2021) evaluate book summaries using ROUGE (Lin and Och, 2004), BERTScore (Zhang et al., 2019a), and SummaQA (Scialom et al., 2019). SummaQA requires paragraph-aligned summaries, which we do not have, and so we report results on ROUGE and BERTScore. Our depth 0 summaries are substantially shorter than the reference summaries, so we use the concatenation of depth 1 summaries.
Our 175B models beat all non-oracle baselines on ROUGE by 3-4 points and approach the performance of an extractive oracle. They also significantly outperform all baselines on BERTScore, including the extractive oracle. The 6B models are comparable to baselines on ROUGE while also significantly outperforming all baselines on BERTScore, including an 11B T5 model (Raffel et al., 2019) fine-tuned on the BookSum dataset.</p>
<p>Kryściński et al. (2021) report length being a confounder for BERTScore, with longer summaries having lower scores. We also find a slight negative correlation between length and BERTScore, but controlling for it does not significantly affect our conclusions (see Appendix I).
Note that we cannot rule out overlap of the BookSum dataset with our pretraining dataset. Nevertheless, from manual inspection of the trees, we believe that the summarization procedure largely reflects the structure of the book, rather than being a result of memorization from pretraining.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Abstractive</th>
<th style="text-align: center;">ROUGE-1</th>
<th style="text-align: center;">ROUGE-2</th>
<th style="text-align: center;">ROUGE-L</th>
<th style="text-align: center;">BERTScore</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Extractive Oracle</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">46.62</td>
<td style="text-align: center;">9.17</td>
<td style="text-align: center;">18.31</td>
<td style="text-align: center;">0.082</td>
</tr>
<tr>
<td style="text-align: center;">BertExt</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">36.71</td>
<td style="text-align: center;">6.16</td>
<td style="text-align: center;">13.40</td>
<td style="text-align: center;">0.028</td>
</tr>
<tr>
<td style="text-align: center;">T5 zero-shot</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">35.43</td>
<td style="text-align: center;">5.62</td>
<td style="text-align: center;">12.02</td>
<td style="text-align: center;">0.011</td>
</tr>
<tr>
<td style="text-align: center;">T5 fine-tuned</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">39.46</td>
<td style="text-align: center;">7.69</td>
<td style="text-align: center;">13.77</td>
<td style="text-align: center;">0.060</td>
</tr>
<tr>
<td style="text-align: center;">175b full tree RL</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">41.51</td>
<td style="text-align: center;">10.46</td>
<td style="text-align: center;">16.88</td>
<td style="text-align: center;">0.1821</td>
</tr>
<tr>
<td style="text-align: center;">175b first subtree RL</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">43.19</td>
<td style="text-align: center;">10.63</td>
<td style="text-align: center;">17.10</td>
<td style="text-align: center;">0.1778</td>
</tr>
<tr>
<td style="text-align: center;">6b full tree RL</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">36.79</td>
<td style="text-align: center;">7.22</td>
<td style="text-align: center;">14.84</td>
<td style="text-align: center;">0.1246</td>
</tr>
</tbody>
</table>
<p>Table 2: Results on the test set of full book version of the BookSum dataset. Baselines (top two sections) are from Kryściński et al. (2021). Our 175 RL models significantly outperform the nonoracle baselines.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: (a) Performance on the first leaves, as a function of amount of human labels. We see that there are diminishing returns to behavioral cloning, such that RL becomes substantially more efficient on the margin. A policy trained with RL on 5 K demonstrations +5 K comparisons is comparable to one trained with BC on 10K demonstrations. However, one trained with RL on 10K demonstrations + 10K comparisons significantly outperforms one trained with BC on 20K demonstrations. Standard error of the mean is estimated via bootstrapping at the label level. (b) Performance on the first leaves, as a function of amount of estimated human time. Adjusting for human hours gives RL a greater advantage since comparisons are 3x faster to collect than demonstrations (see Appendix E).</p>
<h1>4.3 Human label efficiency of RL vs. BC</h1>
<p>In Section 4.1.2 we found that our RL models outperformed our BC models. However, our RL models were trained on significantly more data. A significant open question is whether doing RL on summary comparisons is actually better than simple behavior cloning on an equal number of high-quality human demonstrations. Previous results from Stiennon et al. (2020) showed that doing RL greatly improved summary quality over their BC baseline, and even outperformed human-written summaries. However, their reference summaries were scraped from Reddit TL;DRs, which are often not good summaries of the original text, and they do not compare to collecting a similar number of high-quality demonstrations.
In this work, we use the same trained labelers to create demonstrations and comparisons, and directly compare RL to BC by plotting model performance versus the amount of human time required to produce each dataset. We study this on the first leaf summarization task rather than the full book summarization task to save human time.
We trained 3 versions of a 6B parameter BC baseline, with $1 / 4,1 / 2$, and all the demonstrations. Then, we trained RL policies starting from each of the $1 / 4$ and $1 / 2$ BC policies, ${ }^{10}$ with approximately the same</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">ROUGE-L</th>
<th style="text-align: center;">BLEU-1</th>
<th style="text-align: center;">BLEU-4</th>
<th style="text-align: center;">METEOR</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">BiDAF (Kočiskỳ et al., 2018)</td>
<td style="text-align: center;">6.2</td>
<td style="text-align: center;">5.7</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">3.7</td>
</tr>
<tr>
<td style="text-align: center;">BM25 + BERT (Mou et al., 2020)</td>
<td style="text-align: center;">15.5</td>
<td style="text-align: center;">14.5</td>
<td style="text-align: center;">1.4</td>
<td style="text-align: center;">5.0</td>
</tr>
<tr>
<td style="text-align: center;">RoBERTa (Zemlyanskiy et al., 2021)</td>
<td style="text-align: center;">18.0</td>
<td style="text-align: center;">18.0</td>
<td style="text-align: center;">2.6</td>
<td style="text-align: center;">5.4</td>
</tr>
<tr>
<td style="text-align: center;">ETC (Zemlyanskiy et al., 2021)</td>
<td style="text-align: center;">18.8</td>
<td style="text-align: center;">17.2</td>
<td style="text-align: center;">2.7</td>
<td style="text-align: center;">5.4</td>
</tr>
<tr>
<td style="text-align: center;">ReadTwice (Zemlyanskiy et al., 2021)</td>
<td style="text-align: center;">23.3</td>
<td style="text-align: center;">21.1</td>
<td style="text-align: center;">4.0</td>
<td style="text-align: center;">7.0</td>
</tr>
<tr>
<td style="text-align: center;">Retriever + Reader (Izacard and Grave, 2020)</td>
<td style="text-align: center;">32.0</td>
<td style="text-align: center;">35.3</td>
<td style="text-align: center;">7.5</td>
<td style="text-align: center;">11.1</td>
</tr>
<tr>
<td style="text-align: center;">175b full tree, depth 1</td>
<td style="text-align: center;">21.03</td>
<td style="text-align: center;">21.82</td>
<td style="text-align: center;">3.87</td>
<td style="text-align: center;">10.52</td>
</tr>
<tr>
<td style="text-align: center;">6b full tree, depth 1</td>
<td style="text-align: center;">17.01</td>
<td style="text-align: center;">19.09</td>
<td style="text-align: center;">2.75</td>
<td style="text-align: center;">8.53</td>
</tr>
<tr>
<td style="text-align: center;">175b first subtree, depth 1</td>
<td style="text-align: center;">21.55</td>
<td style="text-align: center;">22.27</td>
<td style="text-align: center;">4.24</td>
<td style="text-align: center;">10.58</td>
</tr>
<tr>
<td style="text-align: center;">175b full tree, depth 0</td>
<td style="text-align: center;">18.47</td>
<td style="text-align: center;">20.29</td>
<td style="text-align: center;">3.16</td>
<td style="text-align: center;">9.04</td>
</tr>
</tbody>
</table>
<p>Table 3: Results on the test set for the full stories version of the NarrativeQA dataset. We ran a 3B UnifiedQA model against summaries from our best guess model ( 175 b full tree with depth 1 ). We also run ablations; notably, our first subtree model outperformed the full tree model, consistent with results from Section 4.1
number of comparisons as there were demonstrations. For these BC policies, we used temperature $\mathrm{T}=0.6$, while for RL policies, we use $\mathrm{T}=0$ (see Appendix D. 2 for justification).
We found that while RL on comparisons was about as effective as BC on demonstrations after 5k-10k demonstrations, comparisons were far more efficient on the margin after 10k-20k demonstrations (Figure 4). Furthermore, comparisons used to produce this figure were 3 x as fast for us to collect as demonstrations (see Appendix E).</p>
<h1>4.4 NarrativeQA: using book summaries for question answering</h1>
<p>Another way to evaluate summaries is to test whether they can be used to answer questions about the original text (Scialom et al., 2019; Wang et al., 2020).
We applied our summarization model to the NarrativeQA question answering dataset (Kočiskỳ et al., 2018), a dataset consisting of question/answer pairs about full book texts and movie transcripts. The question/answer pairs come from Wikipedia summaries, matched by title to the full text. In the full stories version of NarrativeQA, the model must use the original text.
We test whether our summaries can be used as input (instead of the full book or movie text) to a question answering (QA) model. For the QA model, we simply use a trained UnifiedQA model (Khashabi et al., 2020) in a zero-shot manner with temperature 0 . We can give it either the depth 0 summary, or a concatenation of the depth 1 summaries (the concatenation of depth 2 summaries can be quite long). We found that depth 1 summaries work better.
As shown in Table 3, we achieve competitive results, despite our summarization model not being trained explicitly for question answering. However, we use far more parameters than Izacard and Grave (2020), the previous SOTA. When using smaller UnifiedQA models for question answering, results are substantially worse, suggesting that the quality of the QA model is a primary bottleneck (Figure 7). All our samples are available on our website.</p>
<h2>5 Related work</h2>
<p>Our work is directly inspired by previous papers that lay the groundwork for applying human feedback to reinforcement learning (Christiano et al., 2017), especially to large-scale tasks. Our task decomposition approach can be thought of as a specific instantiation of iterated amplification (Christiano et al., 2018), except we assume a fixed decomposition and start training from the leaf tasks, rather than using the entire tree. Similarly, our approach can be considered a form of recursive reward modeling (Leike et al., 2018) if we understand the purpose of model-generated lower-level summaries to be to help the human evaluate the model's performance on higher-level summaries. Our contribution over these works is showing that this approach can be realistically applied to a difficult, large-scale task. We also build on the growing body of work that fine-tunes models with human feedback. This has been applied in many domains including summarization (Böhm et al., 2019;</p>
<p>Ziegler et al., 2019; Stiennon et al., 2020), dialogue (Jaques et al., 2019; Yi et al., 2019; Hancock et al., 2019), translation (Kreutzer et al., 2018; Bahdanau et al., 2016), semantic parsing (Lawrence and Riezler, 2018), story generation (Zhou and Xu, 2020), review generation (Cho et al., 2018), and evidence extraction (Perez et al., 2019), and agents in simulated environments (Christiano et al., 2017; Ibarz et al., 2018).</p>
<p>There has been relatively little work on summarizing novels and other long-form fiction writing. Early work (Gorinski and Lapata, 2015) used graph-based methods to summarize movie scripts. Mihalcea and Ceylan (2007) introduced a dataset of book summaries scraped from CliffsNotes and tested an unsupervised extractive system based on MEAD (Radev et al., 2004) and Textrank (Mihalcea and Tarau, 2004). More recently, Ladhak et al. (2020) propose a method for extractive summarization of chapters of novels. There has been work on generating partial summaries of fictional stories: Zhang et al. (2019b) investigate generating character descriptions written by the story author, and Kazantseva (2006) investigate extractive methods for generating information about the story setting and characters, but not the plot. Relatedly, Bamman and Smith (2013) proposes an unsupervised method for aligning books with human-written summaries. There has also been some work on question answering using full books (Mou et al., 2020; Izacard and Grave, 2020; Zemlyanskiy et al., 2021). Concurrent with our work, Kryściński et al. (2021) extended the datasets of Mihalcea and Ceylan (2007) and evaluated neural baselines.</p>
<p>While work on summarizing novels is sparse, there has been plenty of work on summarizing other kinds of long documents, such as scientific papers (Abu-Jbara and Radev, 2011; Collins et al., 2017; Subramanian et al., 2019; Cohan et al., 2018; Xiao and Carenini, 2019; Zhao et al., 2020; Sotudeh et al., 2020), and patents (Sharma et al., 2019), as well as multi-document summarization (Liu et al., 2018; Ma et al., 2020; Gharebagh et al., 2020; Chandrasekaran et al., 2020; Liu and Lapata, 2019a; Gao et al., 2020). Many of these techniques use a hierarchical approach to generating final summaries, either by having a hierarchical encoder (Cohan et al., 2018; Zhang et al., 2019c; Liu and Lapata, 2019a), or by first running an extractive summarization model followed by an abstractive model (Subramanian et al., 2019; Liu et al., 2018; Zhao et al., 2020; Gharebagh et al., 2020). The latter can be seen as a form of task decomposition, where the leaf task is documentlevel extractive summarization and the parent task is abstractive summarization conditioned on the extracted summaries.</p>
<p>The idea of decomposing hard tasks into multiple smaller sub-tasks has been used extensively in NLP. For example, Fan et al. (2018) generate fictional stories by first training models to generate a story prompt, and then training another model to generate the story conditioned on this prompt. The idea of saving human time by using models trained at lower levels of the hierarchy to help humans label data for higher-level tasks has also been explored. In Fan et al. (2020), models are used to search for evidence of facts, to help humans fact check faster and more accurately.</p>
<h1>6 Discussion</h1>
<p>Our main interest in this work is scaling human feedback to hard problems; we want to empower humans to give feedback to models on tasks that are very difficult to evaluate. We expect this to be a critical part of the alignment problem because we need to make sure humans can communicate their values to AI systems as they take on more societally-relevant tasks (Leike et al., 2018). If we develop techniques to optimize AI systems on what we actually care about, then we make optimization of convenient but misspecified proxy objectives obsolete.</p>
<p>In this paper, we showed that it is feasible to train models using human feedback on the difficult task of abstractive book summarization, by leveraging task decomposition and learning from human feedback. We also showed that doing RL on summary comparisons is more efficient than supervised learning on summary demonstrations, once the summarization policy has passed a quality threshold. Though we used a fixed decomposition strategy that applies only to summarization, the general techniques could be applied to any task. In this sense we have made progress towards optimizing what we actually care about: good summarization performance as judged by humans.</p>
<p>Something we do not address in this paper is training a single model to perform the entire top-level task, e.g. a single model that maps a book to a summary. This could be done via distillation as suggested in Christiano et al. (2018), however in our case that would require training a single model</p>
<p>with a very large context window, which introduces additional complexity. Furthermore, since the majority of our compute is at the leaf tasks, this would not save us much compute at test-time.</p>
<h1>6.1 Limitations</h1>
<p>Our model's book summaries lack coherence. While our models successfully generate booklevel summaries that contain much of the important information, they often read more as a list of events from the book, rather than a coherent summary that a human would write. In theory, this could be remedied with more rounds of RL at the top-level summarization task, however in practice we found RL at higher levels of the tree to be challenging (see below).</p>
<p>Task decomposition could be fundamentally limiting. Task decomposition assumes that separate parts of the task can be completed independently. However, this may not be true for summarizing books. For example, it may be hard to catch cases where earlier details in the book are only later revealed to be important (e.g. in mystery books). Our summarization models also sometimes generate inaccurate statements due to a lack of context; for example, there is a passage of Pride and Prejudice in which the main character gets asked for "their hand". In the broader context of the chapter, it is clear that the character is being asked for a dance. However, this is not clear from only the local context of the leaf task, and thus the model summarizes it as asking for "her hand in marriage". This is a general weakness of our training setup because we require each summary to be produced from only this local context, with a model that has not read the rest of the book.</p>
<p>Some of these issues may be alleviated by learning a decomposition procedure rather than using a fixed algorithm (see Appendix A. 3 for some discussion). However, this may not resolve all of the problems with decomposition. Consider a case where important information is sprinkled lightly across many parts of the book, e.g. small details implying a buildup of love or resentment, where each detail is too minor to be included in a chapter summary despite being a prominent overall theme. Determining the kinds of tasks that are amenable to decomposition remains an open problem.</p>
<p>Training on higher height tasks may be difficult. In general, policy errors at lower levels compound at each composition task, ultimately leading to large errors on the top-level task. Auto-induced distributional shift (ADS, see Section 2.3.1) may also be making training significantly more difficult, and curriculum choice may matter a lot as a result. Our curriculum and node sampling strategies were chosen in an ad hoc way.</p>
<p>As shown in Section 4.1, training on the full tree of tasks did not lead to improved performance. We discuss some possible reasons in Appendix G but leave thorough investigations to future work.</p>
<h3>6.2 Open questions</h3>
<p>Though our approach produced plausible book summaries, the limitations above suggest some open questions for future research. First, are there better and more principled curricula? Could one obtain improved performance by doing RL more on-policy, by generating the summary trees on the fly, or by training the reward model online as in Ziegler et al. (2019)? Is it better to have longer or shorter episodes, encompassing more or less of the tree? While having longer episodes means the policy has more in-distribution inputs at test time, it also means training on fewer trees for a given amount of compute and makes the reward model less on-distribution.</p>
<p>There are also many ways to improve the fundamental techniques for fine-tuning models using human feedback. For example, are there more efficient ways to collect data from humans instead of binary comparisons? Could other methods for optimizing against human feedback, such as expert iteration (Anthony et al., 2017), be more efficient?</p>
<p>Finally, there are questions for how this procedure extends to other tasks. Is learning a task decomposition model, rather than using a fixed decomposition, feasible for hard real-world tasks? For what kinds of tasks is task decomposition fundamentally limiting? How else can we use ML models to assist humans in specifying their preferences for high-level tasks? We hope to address some of these in future work.</p>
<h1>6.3 Broader impacts</h1>
<p>This work expands on the reward modeling technique proposed in Ziegler et al. (2019) and Stiennon et al. (2020). Thus, the broader impacts are similar to the ones described in those papers. On the positive side, our research is motivated by the benefits of aligning ML systems with human intentions. We believe alignment techniques are an increasingly important tool to improve the safety of ML systems, particularly as these systems become more capable. Conversely, improved alignment could also enable malicious actors to more easily train models that cause harm, and could also lead to increased automation of some jobs, leading to job loss. See the broader impacts discussion of Stiennon et al. (2020) for more discussion of these points. The difference in this paper compared to previous work on reward modeling is that we combine the technique with task decomposition, which allows us to use human feedback to train ML models to perform more difficult tasks. This amplifies both the potential benefits and the risks listed above.
One point we reiterate from Stiennon et al. (2020) is to be careful when defining the 'good' model behavior that labelers will reinforce. In other words, what or who should we align our models to? Deciding what makes a good summary is relatively straightforward, but defining good behavior becomes more difficult as we move beyond summarization to more complex tasks where humans might disagree on the correct model behavior.
When solely considering the impacts of automatic book summarization, our models still make many mistakes while summarizing, and thus should not be deployed in a setting where high summarization accuracy is necessary. Our model summaries also seek to preserve the intent of the book, whose contents may be harmful or biased.</p>
<h2>Acknowledgements</h2>
<p>We thank Wojciech Kryściński for discussion of book evaluation methods, and for help with BookSum; Alec Radford for discussions about baselines and NarrativeQA; Ben Mann, for help with our initial dataset; Michael Petrov, Alethea Power, Chris Hesse, and the entire OpenAI Supercomputing team for help with infrastructure; and Alex Ray, Mark Chen, Tom Brown, Nick Ryder, and others for help with and work on pretrained models.
We also thank Jonathan Uesato, Ethan Perez, Sam Bowman, Wojciech Kryściński, and Diogo Moitinho de Almeida for detailed feedback and suggestions on the paper; Pamela Mishkin for book suggestions and feedback on broader impacts; Kelly Clancy for discovering the Pride and Prejudice example; Natalie Summers for suggestions on books/scripts to use; Geoffrey Irving, Beth Barnes, William Saunders, and Dario Amodei for their support and thinking about our research agenda; Justin Wang for creating the graphics for the blog post; and Jeff Clune for the idea to modify books to check prior knowledge.
Last but not least, we'd like to thank all of our labelers, without whom this research would be impossible: Russell Bernandez, Gabriel Ricafrente, Laura Cowley-Martinson, Kelly Guerrero, Megan Niffenegger, Rachelle Froyalde, Ethan Myers, Stephen Ogunniyi, Jack Kausch, Jenny Fletcher, Charles Boone, Justin Dill, Celina Georgette T. Paglinawan, Bryce Vogel, Gabriel Perez, Cody St. Clair, Jelena Ostojic, Erol Can Akbaba, Maria Orzek, Alfred Lee, Ollie Horsfall, Eli Kapsack, Tasmai Dave, Cyra Mayell Denura, Sarah Mulligan, Emill Jayson Caypuno, Morris Stuttard, Ife Riamah, Sebastian Gonzalez, Vladan Djordjevic, Sarah Kirsten, Conor Agnew, William Brewer, Medeea Bunea, Joe Kwon, Chait Singh, Jennifer Brillo, Bashir Harrell, Leo Yung, Bekah Guess, Atresha Singh, and Jacob Bryan.</p>
<h1>References</h1>
<p>Abu-Jbara, A. and Radev, D. (2011). Coherent citation-based summarization of scientific papers. In Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies, pages 500-509.
Amodei, D., Olah, C., Steinhardt, J., Christiano, P., Schulman, J., and Mané, D. (2016). Concrete problems in ai safety. arXiv preprint arXiv:1606.06565.
Anthony, T., Tian, Z., and Barber, D. (2017). Thinking fast and slow with deep learning and tree search. arXiv preprint arXiv:1705.08439.
Bahdanau, D., Brakel, P., Xu, K., Goyal, A., Lowe, R., Pineau, J., Courville, A., and Bengio, Y. (2016). An actor-critic algorithm for sequence prediction. arXiv preprint arXiv:1607.07086.</p>
<p>Bain, M. and Sammut, C. (1995). A framework for behavioural cloning. In Machine Intelligence 15, pages 103-129.
Bamman, D. and Smith, N. A. (2013). New alignment methods for discriminative book summarization. arXiv preprint arXiv:1305.1319.
Böhm, F., Gao, Y., Meyer, C. M., Shapira, O., Dagan, I., and Gurevych, I. (2019). Better rewards yield better summaries: Learning to summarise without references. arXiv preprint arXiv:1909.01214.
Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. (2020). Language models are few-shot learners. arXiv preprint arXiv:2005.14165.
Chandrasekaran, M. K., Feigenblat, G., Hovy, E., Ravichander, A., Shmueli-Scheuer, M., and de Waard, A. (2020). Overview and insights from the shared tasks at scholarly document processing 2020: Cl-scisumm, laysumm and longsumm. In Proceedings of the First Workshop on Scholarly Document Processing, pages 214-224.
Cho, W. S., Zhang, P., Zhang, Y., Li, X., Galley, M., Brockett, C., Wang, M., and Gao, J. (2018). Towards coherent and cohesive long-form text generation. arXiv preprint arXiv:1811.00511.
Christiano, P., Shlegeris, B., and Amodei, D. (2018). Supervising strong learners by amplifying weak experts. arXiv preprint arXiv:1810.08575.
Christiano, P. F., Leike, J., Brown, T., Martic, M., Legg, S., and Amodei, D. (2017). Deep reinforcement learning from human preferences. In Advances in Neural Information Processing Systems, pages 4299-4307.
Clark, J. and Amodei, D. (2016). Faulty reward functions in the wild. Internet: https://blog. openai. com/faulty-reward-functions.
Cohan, A., Dernoncourt, F., Kim, D. S., Bui, T., Kim, S., Chang, W., and Goharian, N. (2018). A discourse-aware attention model for abstractive summarization of long documents. arXiv preprint arXiv:1804.05685.
Collins, E., Augenstein, I., and Riedel, S. (2017). A supervised approach to extractive summarisation of scientific papers. arXiv preprint arXiv:1706.03946.
Dayan, P. and Hinton, G. (1993). Feudal reinforcement learning. nips'93 (pp. 271-278).
Fan, A., Lewis, M., and Dauphin, Y. (2018). Hierarchical neural story generation. arXiv preprint arXiv:1805.04833.
Fan, A., Piktus, A., Petroni, F., Wenzek, G., Saeidi, M., Vlachos, A., Bordes, A., and Riedel, S. (2020). Generating fact checking briefs. arXiv preprint arXiv:2011.05448.</p>
<p>Gao, Y., Zhao, W., and Eger, S. (2020). Supert: Towards new frontiers in unsupervised evaluation metrics for multi-document summarization. arXiv preprint arXiv:2005.03724.
Gharebagh, S. S., Cohan, A., and Goharian, N. (2020). Guir@ longsumm 2020: Learning to generate long summaries from scientific documents. In Proceedings of the First Workshop on Scholarly Document Processing, pages 356-361.
Gorinski, P. and Lapata, M. (2015). Movie script summarization as graph-based scene extraction. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1066-1076.</p>
<p>Hancock, B., Bordes, A., Mazare, P.-E., and Weston, J. (2019). Learning from dialogue after deployment: Feed yourself, chatbot! arXiv preprint arXiv:1901.05415.
Ibarz, B., Leike, J., Pohlen, T., Irving, G., Legg, S., and Amodei, D. (2018). Reward learning from human preferences and demonstrations in atari. In Advances in neural information processing systems, pages 8011-8023.
Irving, G., Christiano, P., and Amodei, D. (2018). Ai safety via debate. arXiv preprint arXiv:1805.00899.
Izacard, G. and Grave, E. (2020). Distilling knowledge from reader to retriever for question answering. arXiv preprint arXiv:2012.04584.
Jaques, N., Ghandeharioun, A., Shen, J. H., Ferguson, C., Lapedriza, A., Jones, N., Gu, S., and Picard, R. (2019). Way off-policy batch deep reinforcement learning of implicit human preferences in dialog. arXiv preprint arXiv:1907.00456.
Kazantseva, A. (2006). An approach to summarizing short stories. In Student Research Workshop.
Khashabi, D., Min, S., Khot, T., Sabharwal, A., Tafjord, O., Clark, P., and Hajishirzi, H. (2020). Unifiedqa: Crossing format boundaries with a single qa system. arXiv preprint arXiv:2005.00700.
Kočiskỳ, T., Schwarz, J., Blunsom, P., Dyer, C., Hermann, K. M., Melis, G., and Grefenstette, E. (2018). The narrativeqa reading comprehension challenge. Transactions of the Association for Computational Linguistics, 6:317-328.
Krakovna, V., Uesato, J., Mikulik, V., Rahtz, M., Everitt, T., Kumar, R., Kenton, Z., Leike, J., and Legg, S. (2020). Specification gaming: the flip side of ai ingenuity. DeepMind Blog.
Kreutzer, J., Khadivi, S., Matusov, E., and Riezler, S. (2018). Can neural machine translation be improved with user feedback? arXiv preprint arXiv:1804.05958.
Krueger, D., Maharaj, T., and Leike, J. (2020). Hidden incentives for auto-induced distributional shift. arXiv preprint arXiv:2009.09153.
Kryściński, W., Rajani, N., Agarwal, D., Xiong, C., and Radev, D. (2021). Booksum: A collection of datasets for long-form narrative summarization. arXiv preprint arXiv:2105.08209.
Ladhak, F., Li, B., Al-Onaizan, Y., and McKeown, K. (2020). Exploring content selection in summarization of novel chapters. arXiv preprint arXiv:2005.01840.
Lawrence, C. and Riezler, S. (2018). Improving a neural semantic parser by counterfactual learning from human bandit feedback. arXiv preprint arXiv:1805.01252.
Leike, J., Krueger, D., Everitt, T., Martic, M., Maini, V., and Legg, S. (2018). Scalable agent alignment via reward modeling: a research direction. arXiv preprint arXiv:1811.07871.
Lin, C.-Y. and Och, F. J. (2004). Automatic evaluation of machine translation quality using longest common subsequence and skip-bigram statistics. In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, page 605. Association for Computational Linguistics.
Liu, P. J., Saleh, M., Pot, E., Goodrich, B., Sepassi, R., Kaiser, L., and Shazeer, N. (2018). Generating wikipedia by summarizing long sequences. arXiv preprint arXiv:1801.10198.
Liu, Y. and Lapata, M. (2019a). Hierarchical transformers for multi-document summarization. arXiv preprint arXiv:1905.13164.
Liu, Y. and Lapata, M. (2019b). Text summarization with pretrained encoders. arXiv preprint arXiv:1908.08345.
Ma, C., Zhang, W. E., Guo, M., Wang, H., and Sheng, Q. Z. (2020). Multi-document summarization via deep learning techniques: A survey. arXiv preprint arXiv:2011.04843.
Mihalcea, R. and Ceylan, H. (2007). Explorations in automatic book summarization. In Proceedings of the 2007 joint conference on empirical methods in natural language processing and computational natural language learning (EMNLP-CoNLL), pages 380-389.
Mihalcea, R. and Tarau, P. (2004). Textrank: Bringing order into text. In Proceedings of the 2004 conference on empirical methods in natural language processing, pages 404-411.
Mou, X., Yu, M., Yao, B., Yang, C., Guo, X., Potdar, S., and Su, H. (2020). Frustratingly hard evidence retrieval for qa over books. arXiv preprint arXiv:2007.09878.
Ought (2020). Evaluating arguments one step at a time.</p>
<p>Perez, E., Karamcheti, S., Fergus, R., Weston, J., Kiela, D., and Cho, K. (2019). Finding generalizable evidence by learning to convince q\&amp;a models. arXiv preprint arXiv:1909.05863.
Perez, E., Lewis, P., Yih, W.-t., Cho, K., and Kiela, D. (2020). Unsupervised question decomposition for question answering. arXiv preprint arXiv:2002.09758.
Radev, D., Allison, T., Blair-Goldensohn, S., Blitzer, J., Çelebi, A., Dimitrov, S., Drabek, E. F., Hakim, A., Lam, W., Liu, D., Otterbacher, J., Qi, H., Saggion, H., Teufel, S., Topper, M., Winkel, A., and Zhang, Z. (2004). Mead-a platform for multidocument multilingual text summarization. In Proceedings of the Fourth International Conference on Language Resources and Evaluation.
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. (2019). Language models are unsupervised multitask learners. OpenAI Blog, 1(8):9.
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. (2019). Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv preprint arXiv:1910.10683.
Rajani, N. F., McCann, B., Xiong, C., and Socher, R. (2019). Explain yourself! leveraging language models for commonsense reasoning. arXiv preprint arXiv:1906.02361.
Scialom, T., Lamprier, S., Piwowarski, B., and Staiano, J. (2019). Answers unite! unsupervised metrics for reinforced summarization models. arXiv preprint arXiv:1909.01610.
Sharma, E., Li, C., and Wang, L. (2019). Bigpatent: A large-scale dataset for abstractive and coherent summarization. arXiv preprint arXiv:1906.03741.
Singh, S. P. (1992). Transfer of learning by composing solutions of elemental sequential tasks. Machine Learning, 8(3):323-339.
Sotudeh, S., Cohan, A., and Goharian, N. (2020). On generating extended summaries of long documents. arXiv preprint arXiv:2012.14136.
Stiennon, N., Ouyang, L., Wu, J., Ziegler, D. M., Lowe, R., Voss, C., Radford, A., Amodei, D., and Christiano, P. (2020). Learning to summarize from human feedback. arXiv preprint arXiv:2009.01325.
Subramanian, S., Li, R., Pilault, J., and Pal, C. (2019). On extractive and abstractive neural document summarization with transformer language models. arXiv preprint arXiv:1909.03186.
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. (2017). Attention is all you need. In Advances in neural information processing systems, pages 5998-6008.
Wang, A., Cho, K., and Lewis, M. (2020). Asking and answering questions to evaluate the factual consistency of summaries. arXiv preprint arXiv:2004.04228.
Xiao, W. and Carenini, G. (2019). Extractive summarization of long documents by combining global and local context. arXiv preprint arXiv:1909.08089.
Yi, S., Goel, R., Khatri, C., Cervone, A., Chung, T., Hedayatnia, B., Venkatesh, A., Gabriel, R., and Hakkani-Tur, D. (2019). Towards coherent and engaging spoken dialog response generation using automatic conversation evaluators. arXiv preprint arXiv:1904.13015.
Zemlyanskiy, Y., Ainslie, J., de Jong, M., Pham, P., Eckstein, I., and Sha, F. (2021). Readtwice: Reading very large documents with memories. arXiv preprint arXiv:2105.04241.
Zhang, T., Kishore, V., Wu, F., Weinberger, K. Q., and Artzi, Y. (2019a). Bertscore: Evaluating text generation with bert. arXiv preprint arXiv:1904.09675.
Zhang, W., Cheung, J. C. K., and Oren, J. (2019b). Generating character descriptions for automatic summarization of fiction. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 7476-7483.
Zhang, X., Wei, F., and Zhou, M. (2019c). Hibert: Document level pre-training of hierarchical bidirectional transformers for document summarization. arXiv preprint arXiv:1905.06566.
Zhao, Y., Saleh, M., and Liu, P. J. (2020). Seal: Segment-wise extractive-abstractive long-form text summarization. arXiv preprint arXiv:2006.10213.
Zhou, W. and Xu, K. (2020). Learning to compare for better training and evaluation of open domain natural language generation models. arXiv preprint arXiv:2002.05058.</p>
<p>Ziegler, D. M., Stiennon, N., Wu, J., Brown, T. B., Radford, A., Amodei, D., Christiano, P., and Irving, G. (2019). Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593.</p>
<h1>Part I</h1>
<h2>Appendix</h2>
<h2>Table of Contents</h2>
<p>A Decomposition details and pseudocode ..... 20
A. 1 Sectioning ..... 20
A. 2 Structure ..... 20
A. 3 Using input model summaries as ground truth ..... 20
A. 4 General task decomposition pseudocode ..... 20
A. 5 Book decomposition pseudocode ..... 21
B Labeler interaction details ..... 22
B. 1 Selection and training ..... 22
B. 2 Quality control ..... 22
B. 3 Task interface ..... 22
C Labeling task details ..... 23
C. 1 Guidelines ..... 23
C. 2 Differences between human and model tasks ..... 24
D Additional training details and hyperparameters ..... 25
D. 1 Fine-tuning details ..... 25
D. 2 Temperature ..... 25
D. 3 Input format ..... 25
E Human timing ..... 26
E. 1 First leaves ..... 26
E. 2 End-to-end baseline estimates ..... 26
F Mistakes and miscellaneous learnings ..... 26
F. 1 Mistakes ..... 26
F. 2 Miscellaneous Learnings ..... 27
G Difficulty and mysteries of full tree training ..... 27
H NarrativeQA: additional findings ..... 28
H. 1 Ablations ..... 28
H. 2 GPT-3 memorization ..... 28
H. 3 Zero-shot recursive question answering ..... 28
H. 4 Comparison to prior work ..... 29
I BookSum: BertSCORE length control ..... 29
J Book summary qualitative findings ..... 29
J. 1 Limitations observed by labelers/researchers ..... 29
J. 2 Preexisting knowledge ..... 30
J. 3 Difficulty of summarizing narrative fiction ..... 31
K Book summary samples ..... 31
K. 1 Books used for full book human evaluation ..... 31
K. 2 Book samples ..... 31</p>
<h1>A Decomposition details and pseudocode</h1>
<h2>A. 1 Sectioning</h2>
<p>We generally aim for a text compression rate of 5-10x at each step, although the compression rate at top of the tree is typically lower, depending on the number of children of the root.
We also generally aim to chunk text at white-space boundaries such as repeated newlines, chapter boundaries, etc., though we do not guarantee this and it is done heuristically.
We filter out preamble and postamble using manually devised heuristics, though our labelers are instructed to output empty summaries upon such inputs if our heuristics do not catch everything.
Finally, the chunking code also consumes a random seed, allowing us to vary sectioning while chunking the above desiderata.</p>
<h2>A. 2 Structure</h2>
<p>Inputs to leaf nodes are typically around 600 tokens. Then, for height 1 tasks, we concatenate 10-13 summaries (each up to 128 tokens). For higher height tasks, we target concatenating up to 8 summaries (each up to 192 tokens at height 2 , or 384 tokens at higher heights), though it can be as low as 2 if there is not enough text, which is common at higher heights.
When applying our tree procedure, each book is split into about 200 leaf nodes on average, and about 20 height 1 nodes. Trees typically reach height 3 (meaning there are additionally height 2 composition tasks, and a final composition task), but on rare occasions reach height 4 or greater.</p>
<h2>A. 3 Using input model summaries as ground truth</h2>
<p>For each task, we ask labelers to consider only the quality of the summary with respect to the direct input to the model, rather than the subset of the book representing the true summarization target. Ideally, we would consider the ultimate task of the labeler or model to be to summarize or evaluate summaries of the full range of the book corresponding to the input in our decomposition. The role of the existing best model would be as a "helper model" to aid in that task (by producing summaries of parts of the book), but the labeler/model would potentially still refer to the original text when needed. Then the reward model at depth 0 would correspond to the "true" reward, rather than corresponding to only part of the trajectory.
Had we defined the tasks this way, it may have helped address issues the error accumulation problem discussed in Section 4.1.2. When inputs were contradictory or confusing, labelers could consult the original source. This would be particularly compelling if the model was also capable of questionanswering.
Unfortunately, while we find this framing appealing, the pretrained models we had access to had limited context length. Furthermore, this would have complicated our infrastructure and made the task for labelers somewhat more difficult. Thus we start with the simpler version and leave such investigations to future work.</p>
<h2>A. 4 General task decomposition pseudocode</h2>
<p>In this implementation of decomposition, the input at each step is simply a task which we wish to do, and a list of (subtask, response) pairs. The subtasks are assumed to have come from a previous invocation of the function, and the subtask responses should help in answering the primary task.</p>
<div class="codehilite"><pre><span></span><code><span class="nx">def</span><span class="w"> </span><span class="nx">do_task</span><span class="p">(</span><span class="nx">task</span><span class="p">,</span><span class="w"> </span><span class="nx">subtask_pairs</span><span class="p">=[]):</span>
<span class="w">    </span><span class="nx">result</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">decompose_if_needed</span><span class="p">(</span><span class="nx">task</span><span class="p">,</span><span class="w"> </span><span class="nx">subtask_pairs</span><span class="p">)</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="k">type</span><span class="p">(</span><span class="nx">result</span><span class="p">)</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="nx">Decompose</span><span class="p">:</span>
<span class="w">        </span><span class="err">#</span><span class="w"> </span><span class="nx">recursively</span><span class="w"> </span><span class="nx">get</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">response</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">subtask</span>
<span class="w">        </span><span class="nx">subresponse</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">do_task</span><span class="p">(</span><span class="nx">result</span><span class="p">.</span><span class="nx">subtask</span><span class="p">)</span>
<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="nx">do_task</span><span class="p">(</span>
<span class="w">            </span><span class="nx">task</span><span class="p">,</span>
<span class="w">            </span><span class="nx">subtask_pairs</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="p">[(</span><span class="nx">result</span><span class="p">.</span><span class="nx">subtask</span><span class="p">,</span><span class="w"> </span><span class="nx">subresponse</span><span class="p">)]</span>
<span class="w">        </span><span class="p">)</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="k">if</span><span class="w"> </span><span class="k">type</span><span class="p">(</span><span class="nx">result</span><span class="p">)</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="nx">Respond</span><span class="p">:</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="nx">answer_directly</span><span class="p">(</span><span class="nx">task</span><span class="p">,</span><span class="w"> </span><span class="nx">subtask_pairs</span><span class="p">)</span>
</code></pre></div>

<p>We have assumed existence of two functions:</p>
<ol>
<li>decompose_if_needed, which returns either a Respond() indicating the subtasks can be synthesized and answered by the model directly, or a Decompose (subtask) if the model requires help to solve the task. This subtask can be decomposed even further if necessary.</li>
<li>answer_directly, which returns an actual answer to the task, synthesizing the answers to subtasks</li>
</ol>
<p>In general, both decompose_if_needed and answer_directly could be learned and implemented by an ML model. In the fixed decomposition case, decompose_if_needed is implemented programmatically instead.
Note also that Decompose only returns a single subtask, rather than a list of them. This way, other child subtasks can depend on the result of the prior ones.</p>
<h1>A. 5 Book decomposition pseudocode</h1>
<p>A basic implementation of our tree decomposition for books described in Section 2 might look like this:</p>
<div class="codehilite"><pre><span></span><code><span class="n">def</span><span class="w"> </span><span class="n">decompose_if_needed</span><span class="p">(</span><span class="n">task</span><span class="p">,</span><span class="w"> </span><span class="n">child_summaries</span><span class="p">)</span><span class="err">:</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="nf">len</span><span class="p">(</span><span class="n">task</span><span class="p">.</span><span class="nc">text</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="nl">MAX_LENGTH</span><span class="p">:</span>
<span class="w">        </span><span class="err">#</span><span class="w"> </span><span class="n">just</span><span class="w"> </span><span class="n">summarize</span><span class="w"> </span><span class="n">actual</span><span class="w"> </span><span class="n">book</span><span class="w"> </span><span class="nc">text</span>
<span class="w">        </span><span class="n">assert</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="nf">len</span><span class="p">(</span><span class="n">child_summaries</span><span class="p">)</span>
<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">Respond</span><span class="p">()</span>
<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="n">split</span><span class="w"> </span><span class="nc">text</span><span class="w"> </span><span class="k">into</span><span class="w"> </span><span class="n">parts</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="k">similar</span><span class="w"> </span><span class="n">length</span>
<span class="w">    </span><span class="nl">chunks</span><span class="p">:</span><span class="w"> </span><span class="n">List</span><span class="o">[</span><span class="n">str</span><span class="o">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">chunkify_text</span><span class="p">(</span><span class="n">task</span><span class="p">.</span><span class="nc">text</span><span class="p">)</span>
<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="n">assume</span><span class="w"> </span><span class="ow">any</span><span class="w"> </span><span class="n">existing</span><span class="w"> </span><span class="n">N</span><span class="w"> </span><span class="n">answers</span><span class="w"> </span><span class="k">are</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="k">first</span><span class="w"> </span><span class="n">N</span><span class="w"> </span><span class="n">chunks</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="nf">len</span><span class="p">(</span><span class="n">child_summaries</span><span class="p">)</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="nf">len</span><span class="p">(</span><span class="n">chunks</span><span class="p">)</span><span class="err">:</span>
<span class="w">        </span><span class="err">#</span><span class="w"> </span><span class="n">we</span><span class="w"> </span><span class="n">have</span><span class="w"> </span><span class="ow">all</span><span class="w"> </span><span class="n">answers</span><span class="w"> </span><span class="n">necessary</span><span class="p">,</span><span class="w"> </span><span class="n">summarize</span><span class="w"> </span><span class="n">concatenation</span>
<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">Respond</span><span class="p">()</span>
<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="n">We</span><span class="w"> </span><span class="n">still</span><span class="w"> </span><span class="n">need</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">summary</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">one</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">our</span><span class="w"> </span><span class="n">children</span><span class="p">,</span><span class="w"> </span><span class="n">recurse</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">it</span><span class="p">.</span>
<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="k">outer</span><span class="w"> </span><span class="n">loop</span><span class="w"> </span><span class="n">will</span><span class="w"> </span><span class="k">call</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">model</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">summarize</span><span class="w"> </span><span class="n">this</span><span class="p">,</span>
<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">append</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">child_summaries</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">Decompose</span><span class="p">(</span><span class="n">Task</span><span class="p">(</span><span class="nc">text</span><span class="o">=</span><span class="n">chunks</span><span class="o">[</span><span class="n">len(child_summaries)</span><span class="o">]</span><span class="p">))</span>
<span class="n">def</span><span class="w"> </span><span class="n">answer_directly</span><span class="p">(</span><span class="n">task</span><span class="p">,</span><span class="w"> </span><span class="n">child_summaries</span><span class="p">)</span><span class="err">:</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="nf">len</span><span class="p">(</span><span class="n">child_summaries</span><span class="p">)</span><span class="err">:</span>
<span class="w">        </span><span class="err">#</span><span class="w"> </span><span class="n">actual</span><span class="w"> </span><span class="n">book</span><span class="w"> </span><span class="nc">text</span>
<span class="w">        </span><span class="n">to_summarize</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">task</span><span class="p">.</span><span class="nc">text</span>
<span class="w">    </span><span class="k">else</span><span class="err">:</span>
<span class="w">        </span><span class="n">to_summarize</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="ss">&quot;\n\n&quot;</span><span class="p">.</span><span class="k">join</span><span class="p">(</span><span class="n">child_summaries</span><span class="p">)</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">model</span><span class="p">(</span><span class="n">to_summarize</span><span class="p">)</span>
</code></pre></div>

<p>A version which correctly uses "previous context" is a bit more involved to implement. We keep an info field which tracks a mapping from depth to all summaries written at that depth so far. Note that the "previous context" summaries are from the same task depth (not necessarily the same task height). For example, at height 0 , if summarizing page 5-6, in addition to receiving the original text for pages 5-6, a model/human would also read the tail end of summaries for pages 1-4.</p>
<div class="codehilite"><pre><span></span><code>def decompose_if_needed(task, child_summaries):
    if len(task.text) &lt; MAX_LENGTH:
        # just summarize actual book text
        assert not len(child_summaries)
        return Respond()
    # split text into parts of similar length
    chunks = chunkify_text(task.text)
    # assume any existing N answers are for the first N chunks
    if len(child_summaries) == len(chunks):
</code></pre></div>

<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{10}$ We collected comparisons of the initial BC policies at temperature $\mathrm{T}=1$, trained a reward model, and then ran a single round of RL with the initial BC policy at initialization.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1785 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1785</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1785</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-32.html">extraction-schema-32</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <p><strong>Paper ID:</strong> paper-45d2d80e95fa81c7f8d2813b36e6bbffde782e92</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/45d2d80e95fa81c7f8d2813b36e6bbffde782e92" target="_blank">BayesSim: adaptive domain randomization via probabilistic inference for robotics simulators</a></p>
                <p><strong>Paper Venue:</strong> Robotics: Science and Systems</p>
                <p><strong>Paper TL;DR:</strong> Results show that the posterior computed from BayesSim can be used for domain randomization outperforming alternative methods that randomize based on uniform priors.</p>
                <p><strong>Paper Abstract:</strong> We introduce BayesSim, a framework for robotics simulations allowing a full Bayesian treatment for the parameters of the simulator. As simulators become more sophisticated and able to represent the dynamics more accurately, fundamental problems in robotics such as motion planning and perception can be solved in simulation and solutions transferred to the physical robot. However, even the most complex simulator might still not be able to represent reality in all its details either due to inaccurate parametrization or simplistic assumptions in the dynamic models. BayesSim provides a principled framework to reason about the uncertainty of simulation parameters. Given a black box simulator (or generative model) that outputs trajectories of state and action pairs from unknown simulation parameters, followed by trajectories obtained with a physical robot, we develop a likelihood-free inference method that computes the posterior distribution of simulation parameters. This posterior can then be used in problems where Sim2Real is critical, for example in policy search. We compare the performance of BayesSim in obtaining accurate posteriors in a number of classical control and robotics problems. Results show that the posterior computed from BayesSim can be used for domain randomization outperforming alternative methods that randomize based on uniform priors.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1785.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1785.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BayesSim</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BayesSim: adaptive domain randomization via probabilistic inference for robotics simulators</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A likelihood-free Bayesian inference framework that learns full posterior distributions over simulator parameters from (real or simulated) trajectories using a conditional mixture-density estimator (mixture density random Fourier / NN features) and uses that posterior to drive domain-randomized policy training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>general robotic agents (multiple tasks: CartPole, Fetch, Hopper, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>Controllers/policies for a variety of simulated robotic systems used as testbeds (classic control CartPole and MuJoCo/PyBullet robots like Fetch and Hopper); policies map observations to continuous actions for control and manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general robotics manipulation and control</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>OpenAI Gym, PyBullet, MuJoCo</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Physics-based simulators that generate state-action trajectories from simulator parameters (dynamics simulators modeling rigid-body physics, contact and friction; MuJoCo used for manipulation tasks, PyBullet for locomotion, Gym for classic control benchmarks).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>physics-based dynamics simulators of moderate-to-high fidelity (rigid-body dynamics and contact modeling), not necessarily photorealistic; treated as black-box generative models</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>rigid-body dynamics, contact dynamics, friction, masses, link lengths, simulation time-step (dt), action/force application; sufficient trajectory statistics derived from state-action sequences</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>sensor noise and detailed sensory rendering (visual photorealism) largely not modeled; some simulator simplifications and numeric solver approximations acknowledged; no explicit modeling of actuator latency/noisy sensors in experiments unless part of randomized parameters</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Conceptually intended to be physical robot rollouts providing observed trajectories; in the experiments of this paper, 'real' observations were approximated by running the simulator with the true parameter settings and averaging trajectories (i.e., no hardware robot experiments reported in this work).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Robust control policies for balancing (CartPole) and manipulation skills for pushing/sliding objects (Fetch push and slide) and locomotion-like tasks (Hopper); learning policies in sim with aim of transferring to corresponding real-world controllers.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Reinforcement learning (PPO for CartPole experiments; DDPG + HER for Fetch tasks; PPO/DDPG as appropriate), with domain randomization driven by posterior over simulator parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Policy robustness measured via accumulated reward across a range of simulator parameter values and log-probability of true parameters under inferred posterior; no direct hardware transfer metrics (e.g., real-world success rate) reported.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td>Numerical performance reported as accumulated rewards across parameter sweeps (plots) and log-probabilities of true parameters under inferred posteriors (Table I). Specific examples: BayesSim RFF produced higher log-probabilities for many tasks (e.g., Fetch Push friction log-prob ~2.423±0.07) but no absolute task success percentages on hardware were reported.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>Randomization sampled simulator parameters from the inferred posterior (mixture of Gaussians) rather than a uniform prior; parameters randomized across experiments included pole length and mass (CartPole), dt (Pendulum), motor power (MountainCar), link masses and lengths (Acrobot), lateral friction (Hopper), and friction coefficients (Fetch Push/Slide).</td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Uncertain/incorrect simulator parameters, oversimplified dynamics models, numerical solver precision, unmodeled friction/contact behaviors and multi-modality/parameter degeneracy (multiple parameter combinations produce similar observations) — all noted as contributors to the reality gap.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Estimating a full posterior over simulator parameters (instead of point-estimates or uniform priors) and using that posterior for domain randomization; using informative sufficient statistics from state-action trajectories; stable conditional density estimation (mixture density with RFF features) to capture multimodality; sampling components from posterior mixture during policy training.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>Paper emphasizes that accurate modeling of dynamics-relevant aspects (e.g., friction/contact for sliding tasks) is critical; specifically, slide (open-loop) tasks are sensitive to friction modeling while closed-loop push tasks are more tolerant. No numeric fidelity thresholds (e.g., percent error) are given.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>BayesSim can recover multi-modal posteriors over simulator parameters from trajectory statistics and using the posterior to guide domain randomization produces policies that are more robust across parameter variations (in simulation) than policies trained with uniform prior randomization; posterior-guided randomization is especially beneficial when unmodeled dynamics (e.g., friction in open-loop sliding) make uniform randomization ineffective. The paper does not present direct hardware sim-to-real evaluation, but positions BayesSim as a principled method to reduce sim-to-real gap by narrowing randomization to simulator parameter regions consistent with real observations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BayesSim: adaptive domain randomization via probabilistic inference for robotics simulators', 'publication_date_yy_mm': '2019-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1785.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1785.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Fetch Push/Slide experiments</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fetch robot push and slide manipulation experiments (simulated)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Simulated experiments with a Fetch robotic arm (MuJoCo) performing push (closed-loop) and slide (open-loop) tasks used to evaluate how posterior-based domain randomization (BayesSim) affects policy robustness with respect to object-surface friction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Fetch robotic arm (simulated in MuJoCo)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>A 7-DoF robotic manipulator with an end-effector used to push a puck across a tabletop toward targets; used in both closed-loop (push) and open-loop (slide) task variants.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>robotics manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>MuJoCo (via OpenAI Gym Fetch environments)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>MuJoCo physics engine simulating rigid-body dynamics, contacts, and friction for the Fetch arm interacting with objects on a table.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>Moderate-to-high fidelity rigid-body/contact dynamics (MuJoCo), focusing on friction and contact forces relevant to object motion under pushes.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>Contact dynamics, surface-object friction, applied forces/torques, rigid-body dynamics of robot and object.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>Sensor noise, actuator delays, and unmodeled micro-scale surface effects (e.g., complex frictional regimes) not explicitly modeled; in experiments, real-world data was not used — 'real' observations were simulated from true friction value.</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Intended real environment would be a physical Fetch robot pushing/sliding an object on a tabletop; in this paper, no physical runs were reported — the 'real' statistics were generated by running the simulator with the true friction parameter multiple times and averaging.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Pushing/sliding an object to a target position on a table; skill requires choosing forces and trajectories that account for object friction and contact dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>DDPG with Hindsight Experience Replay (HER) to learn a base policy (for data generation) and then RL with domain randomization using the posterior to train robust policies.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Average episodic accumulated reward across a sweep of friction parameter values (plots) and log-probability of true friction under inferred posterior (table).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td>BayesSim posterior-guided policies achieved higher rewards near the true friction value for the slide task and improved log-probabilities for friction posterior (e.g., Fetch Slide friction: BayesSim RFF log-prob ~2.391±0.06 compared with uniform prior ~1.014±0.38).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>Friction coefficient of object-surface randomized according to inferred posterior (mixture of Gaussians) vs uniform prior baseline; training policies sampled new friction each episode.</td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>High sensitivity to friction/contact modeling in open-loop (slide) tasks; if simulator friction does not match real friction, open-loop policies fail because they cannot correct trajectories online.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Accurate posterior concentration around true friction enables training policies that perform well for the true physical friction; closed-loop tasks are more forgiving, so wide uniform randomization can suffice for push but not for slide.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>Contact and friction modeling must be accurate (or posterior must concentrate around true friction) for successful transfer in open-loop sliding tasks; closed-loop push is less demanding.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Posterior-guided domain randomization produced a distribution concentrated near the true friction and yielded higher simulated rewards for slide tasks compared to uniform prior training; uniform prior randomization performed adequately for closed-loop push tasks but poorly for open-loop slide tasks, demonstrating that task structure (open vs closed loop) interacts with fidelity requirements for sim-to-real transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BayesSim: adaptive domain randomization via probabilistic inference for robotics simulators', 'publication_date_yy_mm': '2019-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1785.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1785.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CartPole experiment</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CartPole posterior estimation and posterior-guided domain randomization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A canonical control experiment where BayesSim recovered a multi-modal posterior over pole length and mass and used posterior-guided randomization to train more robust PPO controllers across parameter variations in simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>CartPole (classic control simulated environment)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>A simulated cart with an inverted pendulum (pole) where the agent must apply left/right forces to balance the pole; used to study posterior estimation over pole mass and length and policy robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>control / reinforcement learning (benchmark robotics-like control)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>OpenAI Gym (CartPole)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Classic control simulator modeling continuous dynamics of cart and pole; simulates rigid-body dynamics and control forces but is a simplified dynamical system.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>Simplified dynamics (analytical ordinary differential equations with numerical integration), appropriate for algorithmic benchmarking rather than direct robot fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>Pole/cart dynamics including mass and length parameters and simulation timestep; deterministic dynamics with integration.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>No contact dynamics, no sensor noise modeling, simplified frictionless pivot assumptions (typical CartPole simplifications); not representative of full robot complexities.</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>No physical CartPole experiments reported; 'real' observations simulated by running environment with the true parameter values to emulate real trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Pole balancing control policies robust to variations in pole length and mass.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>PPO reinforcement learning; BayesSim used to estimate posterior over pole mass and length from trajectories, then posterior sampled to train policies.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Accumulated reward across parameter sweeps; posterior log-probability of true parameter pair used to assess posterior accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td>Posterior-guided policy showed significantly improved robustness around the actual parameter values compared to policy trained with uniform prior; plots show lower variance and better reward at true parameters (no absolute numeric success rate reported for hardware).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>Randomized pole length and pole mass during training; compared uniform prior randomization vs posterior-guided randomization (BayesSim RFF).</td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Parameter uncertainty and multimodality (different parameter combinations produce similar trajectories) can cause domain randomization with improper priors to produce suboptimal robustness at the true parameter configuration.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Estimating a multimodal posterior that captures dependencies between mass and length and using posterior-based sampling for RL yields policies that are more robust at the true parameter settings.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>Accurate representation of the key dynamical parameters (mass, length, timestep) is necessary to concentrate posterior and train robust controllers; no quantitative fidelity thresholds provided.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>BayesSim recovered multimodal posterior distributions over CartPole parameters and posterior-guided domain randomization produced policies with higher robustness and lower variance across runs in simulation than uniform prior randomization; highlights importance of targeted randomization based on inferred simulator-parameter posterior to improve expected real-world performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BayesSim: adaptive domain randomization via probabilistic inference for robotics simulators', 'publication_date_yy_mm': '2019-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Sim-to-real transfer of robotic control with dynamics randomization <em>(Rating: 2)</em></li>
                <li>Domain randomization for transferring deep neural networks from simulation to the real world <em>(Rating: 2)</em></li>
                <li>Closing the sim-to-real loop: Adapting simulation randomization with real world experience <em>(Rating: 2)</em></li>
                <li>Sim-to-real: Learning agile locomotion for quadruped robots <em>(Rating: 2)</em></li>
                <li>Fast ε-free inference of simulation models with bayesian conditional density estimation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1785",
    "paper_id": "paper-45d2d80e95fa81c7f8d2813b36e6bbffde782e92",
    "extraction_schema_id": "extraction-schema-32",
    "extracted_data": [
        {
            "name_short": "BayesSim",
            "name_full": "BayesSim: adaptive domain randomization via probabilistic inference for robotics simulators",
            "brief_description": "A likelihood-free Bayesian inference framework that learns full posterior distributions over simulator parameters from (real or simulated) trajectories using a conditional mixture-density estimator (mixture density random Fourier / NN features) and uses that posterior to drive domain-randomized policy training.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "general robotic agents (multiple tasks: CartPole, Fetch, Hopper, etc.)",
            "agent_system_description": "Controllers/policies for a variety of simulated robotic systems used as testbeds (classic control CartPole and MuJoCo/PyBullet robots like Fetch and Hopper); policies map observations to continuous actions for control and manipulation.",
            "domain": "general robotics manipulation and control",
            "virtual_environment_name": "OpenAI Gym, PyBullet, MuJoCo",
            "virtual_environment_description": "Physics-based simulators that generate state-action trajectories from simulator parameters (dynamics simulators modeling rigid-body physics, contact and friction; MuJoCo used for manipulation tasks, PyBullet for locomotion, Gym for classic control benchmarks).",
            "simulation_fidelity_level": "physics-based dynamics simulators of moderate-to-high fidelity (rigid-body dynamics and contact modeling), not necessarily photorealistic; treated as black-box generative models",
            "fidelity_aspects_modeled": "rigid-body dynamics, contact dynamics, friction, masses, link lengths, simulation time-step (dt), action/force application; sufficient trajectory statistics derived from state-action sequences",
            "fidelity_aspects_simplified": "sensor noise and detailed sensory rendering (visual photorealism) largely not modeled; some simulator simplifications and numeric solver approximations acknowledged; no explicit modeling of actuator latency/noisy sensors in experiments unless part of randomized parameters",
            "real_environment_description": "Conceptually intended to be physical robot rollouts providing observed trajectories; in the experiments of this paper, 'real' observations were approximated by running the simulator with the true parameter settings and averaging trajectories (i.e., no hardware robot experiments reported in this work).",
            "task_or_skill_transferred": "Robust control policies for balancing (CartPole) and manipulation skills for pushing/sliding objects (Fetch push and slide) and locomotion-like tasks (Hopper); learning policies in sim with aim of transferring to corresponding real-world controllers.",
            "training_method": "Reinforcement learning (PPO for CartPole experiments; DDPG + HER for Fetch tasks; PPO/DDPG as appropriate), with domain randomization driven by posterior over simulator parameters.",
            "transfer_success_metric": "Policy robustness measured via accumulated reward across a range of simulator parameter values and log-probability of true parameters under inferred posterior; no direct hardware transfer metrics (e.g., real-world success rate) reported.",
            "transfer_performance_sim": "Numerical performance reported as accumulated rewards across parameter sweeps (plots) and log-probabilities of true parameters under inferred posteriors (Table I). Specific examples: BayesSim RFF produced higher log-probabilities for many tasks (e.g., Fetch Push friction log-prob ~2.423±0.07) but no absolute task success percentages on hardware were reported.",
            "transfer_performance_real": null,
            "transfer_success": null,
            "domain_randomization_used": true,
            "domain_randomization_details": "Randomization sampled simulator parameters from the inferred posterior (mixture of Gaussians) rather than a uniform prior; parameters randomized across experiments included pole length and mass (CartPole), dt (Pendulum), motor power (MountainCar), link masses and lengths (Acrobot), lateral friction (Hopper), and friction coefficients (Fetch Push/Slide).",
            "sim_to_real_gap_factors": "Uncertain/incorrect simulator parameters, oversimplified dynamics models, numerical solver precision, unmodeled friction/contact behaviors and multi-modality/parameter degeneracy (multiple parameter combinations produce similar observations) — all noted as contributors to the reality gap.",
            "transfer_enabling_conditions": "Estimating a full posterior over simulator parameters (instead of point-estimates or uniform priors) and using that posterior for domain randomization; using informative sufficient statistics from state-action trajectories; stable conditional density estimation (mixture density with RFF features) to capture multimodality; sampling components from posterior mixture during policy training.",
            "fidelity_requirements_identified": "Paper emphasizes that accurate modeling of dynamics-relevant aspects (e.g., friction/contact for sliding tasks) is critical; specifically, slide (open-loop) tasks are sensitive to friction modeling while closed-loop push tasks are more tolerant. No numeric fidelity thresholds (e.g., percent error) are given.",
            "fine_tuning_in_real_world": false,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": false,
            "fidelity_comparison_results": null,
            "key_findings": "BayesSim can recover multi-modal posteriors over simulator parameters from trajectory statistics and using the posterior to guide domain randomization produces policies that are more robust across parameter variations (in simulation) than policies trained with uniform prior randomization; posterior-guided randomization is especially beneficial when unmodeled dynamics (e.g., friction in open-loop sliding) make uniform randomization ineffective. The paper does not present direct hardware sim-to-real evaluation, but positions BayesSim as a principled method to reduce sim-to-real gap by narrowing randomization to simulator parameter regions consistent with real observations.",
            "uuid": "e1785.0",
            "source_info": {
                "paper_title": "BayesSim: adaptive domain randomization via probabilistic inference for robotics simulators",
                "publication_date_yy_mm": "2019-06"
            }
        },
        {
            "name_short": "Fetch Push/Slide experiments",
            "name_full": "Fetch robot push and slide manipulation experiments (simulated)",
            "brief_description": "Simulated experiments with a Fetch robotic arm (MuJoCo) performing push (closed-loop) and slide (open-loop) tasks used to evaluate how posterior-based domain randomization (BayesSim) affects policy robustness with respect to object-surface friction.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "Fetch robotic arm (simulated in MuJoCo)",
            "agent_system_description": "A 7-DoF robotic manipulator with an end-effector used to push a puck across a tabletop toward targets; used in both closed-loop (push) and open-loop (slide) task variants.",
            "domain": "robotics manipulation",
            "virtual_environment_name": "MuJoCo (via OpenAI Gym Fetch environments)",
            "virtual_environment_description": "MuJoCo physics engine simulating rigid-body dynamics, contacts, and friction for the Fetch arm interacting with objects on a table.",
            "simulation_fidelity_level": "Moderate-to-high fidelity rigid-body/contact dynamics (MuJoCo), focusing on friction and contact forces relevant to object motion under pushes.",
            "fidelity_aspects_modeled": "Contact dynamics, surface-object friction, applied forces/torques, rigid-body dynamics of robot and object.",
            "fidelity_aspects_simplified": "Sensor noise, actuator delays, and unmodeled micro-scale surface effects (e.g., complex frictional regimes) not explicitly modeled; in experiments, real-world data was not used — 'real' observations were simulated from true friction value.",
            "real_environment_description": "Intended real environment would be a physical Fetch robot pushing/sliding an object on a tabletop; in this paper, no physical runs were reported — the 'real' statistics were generated by running the simulator with the true friction parameter multiple times and averaging.",
            "task_or_skill_transferred": "Pushing/sliding an object to a target position on a table; skill requires choosing forces and trajectories that account for object friction and contact dynamics.",
            "training_method": "DDPG with Hindsight Experience Replay (HER) to learn a base policy (for data generation) and then RL with domain randomization using the posterior to train robust policies.",
            "transfer_success_metric": "Average episodic accumulated reward across a sweep of friction parameter values (plots) and log-probability of true friction under inferred posterior (table).",
            "transfer_performance_sim": "BayesSim posterior-guided policies achieved higher rewards near the true friction value for the slide task and improved log-probabilities for friction posterior (e.g., Fetch Slide friction: BayesSim RFF log-prob ~2.391±0.06 compared with uniform prior ~1.014±0.38).",
            "transfer_performance_real": null,
            "transfer_success": null,
            "domain_randomization_used": true,
            "domain_randomization_details": "Friction coefficient of object-surface randomized according to inferred posterior (mixture of Gaussians) vs uniform prior baseline; training policies sampled new friction each episode.",
            "sim_to_real_gap_factors": "High sensitivity to friction/contact modeling in open-loop (slide) tasks; if simulator friction does not match real friction, open-loop policies fail because they cannot correct trajectories online.",
            "transfer_enabling_conditions": "Accurate posterior concentration around true friction enables training policies that perform well for the true physical friction; closed-loop tasks are more forgiving, so wide uniform randomization can suffice for push but not for slide.",
            "fidelity_requirements_identified": "Contact and friction modeling must be accurate (or posterior must concentrate around true friction) for successful transfer in open-loop sliding tasks; closed-loop push is less demanding.",
            "fine_tuning_in_real_world": false,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": false,
            "fidelity_comparison_results": null,
            "key_findings": "Posterior-guided domain randomization produced a distribution concentrated near the true friction and yielded higher simulated rewards for slide tasks compared to uniform prior training; uniform prior randomization performed adequately for closed-loop push tasks but poorly for open-loop slide tasks, demonstrating that task structure (open vs closed loop) interacts with fidelity requirements for sim-to-real transfer.",
            "uuid": "e1785.1",
            "source_info": {
                "paper_title": "BayesSim: adaptive domain randomization via probabilistic inference for robotics simulators",
                "publication_date_yy_mm": "2019-06"
            }
        },
        {
            "name_short": "CartPole experiment",
            "name_full": "CartPole posterior estimation and posterior-guided domain randomization",
            "brief_description": "A canonical control experiment where BayesSim recovered a multi-modal posterior over pole length and mass and used posterior-guided randomization to train more robust PPO controllers across parameter variations in simulation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "CartPole (classic control simulated environment)",
            "agent_system_description": "A simulated cart with an inverted pendulum (pole) where the agent must apply left/right forces to balance the pole; used to study posterior estimation over pole mass and length and policy robustness.",
            "domain": "control / reinforcement learning (benchmark robotics-like control)",
            "virtual_environment_name": "OpenAI Gym (CartPole)",
            "virtual_environment_description": "Classic control simulator modeling continuous dynamics of cart and pole; simulates rigid-body dynamics and control forces but is a simplified dynamical system.",
            "simulation_fidelity_level": "Simplified dynamics (analytical ordinary differential equations with numerical integration), appropriate for algorithmic benchmarking rather than direct robot fidelity.",
            "fidelity_aspects_modeled": "Pole/cart dynamics including mass and length parameters and simulation timestep; deterministic dynamics with integration.",
            "fidelity_aspects_simplified": "No contact dynamics, no sensor noise modeling, simplified frictionless pivot assumptions (typical CartPole simplifications); not representative of full robot complexities.",
            "real_environment_description": "No physical CartPole experiments reported; 'real' observations simulated by running environment with the true parameter values to emulate real trajectories.",
            "task_or_skill_transferred": "Pole balancing control policies robust to variations in pole length and mass.",
            "training_method": "PPO reinforcement learning; BayesSim used to estimate posterior over pole mass and length from trajectories, then posterior sampled to train policies.",
            "transfer_success_metric": "Accumulated reward across parameter sweeps; posterior log-probability of true parameter pair used to assess posterior accuracy.",
            "transfer_performance_sim": "Posterior-guided policy showed significantly improved robustness around the actual parameter values compared to policy trained with uniform prior; plots show lower variance and better reward at true parameters (no absolute numeric success rate reported for hardware).",
            "transfer_performance_real": null,
            "transfer_success": null,
            "domain_randomization_used": true,
            "domain_randomization_details": "Randomized pole length and pole mass during training; compared uniform prior randomization vs posterior-guided randomization (BayesSim RFF).",
            "sim_to_real_gap_factors": "Parameter uncertainty and multimodality (different parameter combinations produce similar trajectories) can cause domain randomization with improper priors to produce suboptimal robustness at the true parameter configuration.",
            "transfer_enabling_conditions": "Estimating a multimodal posterior that captures dependencies between mass and length and using posterior-based sampling for RL yields policies that are more robust at the true parameter settings.",
            "fidelity_requirements_identified": "Accurate representation of the key dynamical parameters (mass, length, timestep) is necessary to concentrate posterior and train robust controllers; no quantitative fidelity thresholds provided.",
            "fine_tuning_in_real_world": false,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": false,
            "fidelity_comparison_results": null,
            "key_findings": "BayesSim recovered multimodal posterior distributions over CartPole parameters and posterior-guided domain randomization produced policies with higher robustness and lower variance across runs in simulation than uniform prior randomization; highlights importance of targeted randomization based on inferred simulator-parameter posterior to improve expected real-world performance.",
            "uuid": "e1785.2",
            "source_info": {
                "paper_title": "BayesSim: adaptive domain randomization via probabilistic inference for robotics simulators",
                "publication_date_yy_mm": "2019-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Sim-to-real transfer of robotic control with dynamics randomization",
            "rating": 2
        },
        {
            "paper_title": "Domain randomization for transferring deep neural networks from simulation to the real world",
            "rating": 2
        },
        {
            "paper_title": "Closing the sim-to-real loop: Adapting simulation randomization with real world experience",
            "rating": 2
        },
        {
            "paper_title": "Sim-to-real: Learning agile locomotion for quadruped robots",
            "rating": 2
        },
        {
            "paper_title": "Fast ε-free inference of simulation models with bayesian conditional density estimation",
            "rating": 1
        }
    ],
    "cost": 0.01280925,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>BayesSim: adaptive domain randomization via probabilistic inference for robotics simulators</h1>
<p>Fabio Ramos<em> ${ }^{</em> \dagger}$ Rafael Carvalhaes Possas<em> ${ }^{</em> \dagger}$ Dieter Fox<em> ${ }^{</em>}$<br>*NVIDIA ${ }^{\dagger}$ University of Sydney ${ }^{\ddagger}$ University of Washington</p>
<h4>Abstract</h4>
<p>We introduce BayesSim ${ }^{1}$, a framework for robotics simulations allowing a full Bayesian treatment for the parameters of the simulator. As simulators become more sophisticated and able to represent the dynamics more accurately, fundamental problems in robotics such as motion planning and perception can be solved in simulation and solutions transferred to the physical robot. However, even the most complex simulator might still not be able to represent reality in all its details either due to inaccurate parametrization or simplistic assumptions in the dynamic models. BayesSim provides a principled framework to reason about the uncertainty of simulation parameters. Given a black box simulator (or generative model) that outputs trajectories of state and action pairs from unknown simulation parameters, followed by trajectories obtained with a physical robot, we develop a likelihood-free inference method that computes the posterior distribution of simulation parameters. This posterior can then be used in problems where Sim2Real is critical, for example in policy search. We compare the performance of BayesSim in obtaining accurate posteriors in a number of classical control and robotics problems. Results show that the posterior computed from BayesSim can be used for domain randomization outperforming alternative methods that randomize based on uniform priors.</p>
<h2>I. INTRODUCTION</h2>
<p>Simulators are emerging as one of the most important tools for efficient learning in robotics. With physically accurate and photo-realistic simulation, perception models and control policies can be trained more easily before being transferred to real robots, saving both time and costs of running complex experiments. Unfortunately, in many cases, models and policies trained in simulation are not seamlessly transferable to the real systems. Lack of knowledge about the correct simulation parameters, oversimplified simulation models, or insufficient numerical precision for differential equation solvers can all play a significant role in this problem. To ameliorate this problem, a popular approach is to sample different simulation parameters during training and thereby learn models that are robust to simulation perturbations. This approach, often referred to as domain randomization (DR), has been shown to perform surprisingly well in areas such as learning to control a humanoid robot [23], manipulate table top objects [38], estimating 6D object poses from images [17], or dexterous in-hand manipulation [2].</p>
<p>A crucial question regarding domain randomization is which simulation parameters to randomize over and from which distributions to sample their values from. Typically, these parameters and their distributions are determined in a manual</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>process by iteratively testing whether a model learned in randomized simulation works well on the real system. If the model does not work on the real robot, the randomization parameters are changed so that they better cover the conditions observed in the real world. To overcome this manual tuning process, [10] recently showed how policy executions on a real robot can be used to automatically update a Gaussian distribution over the sampling parameters such that the simulator better matches reality. However, by restricting sampling distributions to Gaussians, this approach cannot model more complex uncertainties and dependencies among parameters. Alternatively, one could perform system identification to better estimate simulation parameters from the real data. Since most of these techniques assume that the simulation equations are known and only provide point estimates for the parameters, they do not account for the uncertainties associated with the measurement process, numerical precision of differential equation solvers, or simplistic models [14].</p>
<p>In this paper we provide a principled Bayesian method to compute full posteriors over simulator parameters, thereby overcoming the limitations of previous approaches. Our technique, called BayesSim, leverages recent advances in likelihood-free inference for Bayesian analysis to update posteriors over simulation parameters based on small sets of observations obtained on the real system. The main difficulty in computing such posteriors relates to the evaluation of the likelihood function, which models the relationship between simulation parameters and corresponding system behavior, or observations in the real world. While a simulator implicitly defines this relationship, the likelihood function requires the inverse of the simulator model, i.e., how observed system behavior can be used to derive corresponding simulation parameters. Importantly, BayesSim does not assume access to the internal differential equations underlying the simulator and treats the simulator as a black box.</p>
<p>We make the following contributions: First, we introduce BayesSim as a generic framework for probabilistic inference with robotics simulators and show that it can provide a full space of simulation parameters that best fit observed data. This is in contrast to traditional system identification methods that only provide the best fitting solution. Second, we propose a novel mixture density random Fourier network to approximate the conditional distribution $p(\boldsymbol{\theta} \mid \mathbf{x}^{r})$ directly by learning from pairs $\left{\boldsymbol{\theta}<em i="i">{i}, \mathbf{x}</em>$ generated from the proposal prior and the simulator. Finally, we show that learning policies with domain randomization where the simulator parameters are randomized}^{s}\right}_{i=1}^{N</p>
<p>according to the posterior provided by BayesSim generates policies that are significantly more robust and easier to train than randomization directly from the prior.</p>
<h2>II. Related Work</h2>
<p>Simulators accelerate machine learning impact by allowing faster, highly-scalable and low cost data collection. Many other scientific domains such as economics [15], evolutionary biology [4] and cosmology [29] also rely on simulator-based modelling to provide further advancements in research. In robotics, "reality gap" is not only seen in control, robotics vision is also affected by this problem [38]. Algorithms trained on images from a simulation can frequently fail on different environments as the appearance of the world can differ greatly from one system to the other.</p>
<p>Randomizing the dynamics of a simulator while training a control policy has proven to mitigate the reality gap problem [25]. Simulation parameters could vary from physical settings like damping, friction and object masses [25] to visual parameters like objects textures, shapes and etc [38]. Another similar approach is that of adding noise to the system parameters [37] instead of sampling new parameters from a uniform prior distribution. Perturbation can also be seen on robot locomotion [22] where planning is done through an ensemble of perturbed models. Lastly, interleaving policy roll outs between simulation and reality has also proven to work well on swing-peg-in-hole and opening a cabinet drawer tasks [11].</p>
<p>Learning models from simulations of data can leverage one’s understanding of the physical world potentially helping to solve the aforementioned problem. Until recently, Approximate Bayesian Computation [4] has been one of the main methods used to tackle this type of problem. Rejection ABC [26] is the most basic method where parameter settings are accepted/rejected if they are within a certain specified range. The set of accepted parameters approximates the posterior for the real parameters. Markov Chain Monte Carlo ABC (MCMCABC) [20] improves over its precedent by perturbing accepted parameters instead of independently proposing new parameters. Lastly, Sequential Monte Carlo ABC (SMC-ABC) [6] leverages sequential importance sampling to simulate slowly-change distributions where the last one is an approximation of the true parameter posterior. In this work, we use a $\epsilon$-free approach [24] for likelihood-free inference, where a Mixture of Density Random Fourier Network estimates the parameters of the true posterior through a Gaussian mixture.</p>
<p>A wide range of complex robotics control problems have been recently solved using Deep Reinforcement Learning (Deep RL) techniques [2, 25, 37]. Classic control problems like Pendulum, Mountain Car, Acrobot and Cartpole have been successfully tackled using policy search with algorithms like Trust Region Policy Optimization (TRPO) [32] and Proximal Policy Optimization (PPO) [33]. More complex tasks in robotics such as the ones in manipulation are still difficult to solve using traditional policy search. Both Push and Slide tasks (Figure 1) on the fetch robot [8] were only solved recently using the combination of Deep Deterministic Policy Gradients (DDPG) [18] and Hindsight Experience Replay (HER) [1].</p>
<h2>III. PRELIMINARIES</h2>
<p>In this section we provide background on likelihood free inference and reinforcement learning. As we shall see, policy search via domain randomization is one of the applications in which BayesSim proved to be valuable.</p>
<h3>III-A. Likelihood-free inference</h3>
<p>BayesSim takes a prior $p(\boldsymbol{\theta})$ over simulation parameters $\boldsymbol{\theta}$, a black box generative model or simulator $\mathbf{x}^{s}=g(\boldsymbol{\theta})$ that generates simulated observations $\mathbf{x}^{s}$ from these parameters, and observations from the physical world $\mathbf{x}^{r}$ to compute the posterior $p(\boldsymbol{\theta}|\mathbf{x}^{s},\mathbf{x}^{r})$. The main difficulty in computing this posterior relates to the evaluation of the likelihood function $p(\mathbf{x}|\boldsymbol{\theta})$ which is defined implicitly from the simulator [12]. Here we assume that the simulator is a set of dynamical differential equations associated with a numerical or analytical solver which are typically intractable and expensive to evaluate. Furthermore, we do not assume these equations are known and treat the simulator as a black box. This allows our method to be utilized with many robotics simulators (even closed source ones) but requires a method where the likelihood cannot be evaluated directly but instead only sampled from, by performing forward simulations. This is referred to in statistics as likelihood-free inference of which the most popular family of algorithms to address it are known as approximate Bayesian computation (ABC) [4, 20, 34].</p>
<p>In ABC, the simulator is used to generate synthetic observations from samples following the parameters prior. These samples are accepted when features or sufficient statistics computed from the synthetic data are similar to those from real observations obtained from physical experiments. As a sampling-based technique, ABC can be notoriously slow to converge, particularly when the dimensionality of the parameter space is large. Formally, ABC approximates the posterior $p(\boldsymbol{\theta}|\mathbf{x}=\mathbf{x}^{r}) \propto p\left(\mathbf{x}=\mathbf{x}^{r} \mid \boldsymbol{\theta}\right) p(\boldsymbol{\theta})$ using the Bayes' rule. However as the likelihood function $p\left(\mathbf{x}=\mathbf{x}^{r} \mid \boldsymbol{\theta}\right)$ is not available, conventional methods for Bayesian inference cannot be applied. ABC sidesteps this problem by approximating $p\left(\mathbf{x}=\mathbf{x}^{r} \mid \boldsymbol{\theta}\right)$ by $p\left(\left|\mathbf{x}-\mathbf{x}^{r}\right|&lt;\epsilon \mid \boldsymbol{\theta}\right)$, where $\epsilon$ is a small value defining a sphere around real observations $\mathbf{x}^{r}$, and using Monte Carlo to estimate its value. The quality of the approximation increases as $\epsilon$ decreases however, the computational cost can become prohibitive as most simulations will not fall within the acceptable region.</p>
<h3>III-B. Reinforcement learning and policy search in robotics</h3>
<p>We consider the default RL scenario where an agent interacts in discrete timesteps with an environment $\mathbf{E}$. At each step $t$ the agent receives an observation $\mathbf{o}<em t="t">{t}$, takes an action $\mathbf{a}</em>}$ and receives a real number reward $r_{t}$. In general, actions in robotics are real valued $\mathbf{a<em t="t">{t} \in \mathbb{R}^{D}$ and environments are usually partially observed so that the entire history of observation, action pairs $\boldsymbol{\eta}=\left{\mathbf{s}</em>}, \mathbf{a<em t="t">{t}, \mathbf{o}</em>$. The goal is to maximize}\right}_{t=0}^{T-1</p>
<p>the expected sum of discounted future rewards by following a policy $\pi(\mathbf{a}<em t="t">{t}|\mathbf{s}</em>$,};\boldsymbol{\beta})$, parametrized by $\boldsymbol{\beta</p>
<p>$J(\boldsymbol{\beta})=\mathbb{E}<em t="0">{\boldsymbol{\eta}}\left[\sum</em>}^{T-1}\gamma^{(t)}r(\mathbf{s<em t="t">{t},\mathbf{a}</em>\right].$ (1)})|\boldsymbol{\beta</p>
<p>Many approaches in reinforcement learning make use of the recursive relationship known as the Bellman equation where $Q^{\pi}$ is the action-value function describing the expected return after taking an action $\mathbf{a}<em t="t">{t}$, in state $\mathbf{s}</em>$ and thereafter following policy $\pi$.</p>
<p>$Q^{\pi}(\mathbf{s}<em t="t">{t},\mathbf{a}</em>})=\mathbb{E<em t="t">{r</em>},s_{t+1}}[r(\mathbf{s<em t="t">{t},\mathbf{a}</em>})+\gamma\mathbb{E<em t_1="t+1">{a</em>}}[Q^{\pi}(\mathbf{s<em t_1="t+1">{t+1},\mathbf{a}</em>)]]$ (2)</p>
<p>In recent years, the advancements in traditional RL methods have allowed their application to control tasks with continuous action spaces. Inheriting ideas from DQN [21], Deep Deterministic Policy Gradients have been relatively successful in a wide range of control problems. The main caveat of DDPG algorithms is that they rely on efficient experience sampling to perform well. Improving the way how experience is collected is one of most important topics in today’s RL community. Experience Replay [19] and Prioritized Experience replay [31] still performs poorly in a repertoire of robotics tasks where the reward signal is sparse. Hindsight Experience replay (HER) [1], on the other hand, performs well in this scenario as it breaks down single trajectories/goals into smaller ones and, thus, provides the policy optimization algorithm with better reward signals. HER has been mostly based in a recent RL concept: Multi-Goal learning with Universal Function Approximators [30].</p>
<p>Another set of successful policy search algorithms is based on optimization through trust regions. They are less sensitive to the experience sampling problem mentioned above. The maximum step size for exploration is determined by its trust region and the optimal point is then evaluated progressively until convergence has been reached. The main idea is that updates are always limited by their own trust region, and, therefore, learning speed is better controlled. Proximal Policy Optimization [33] and Trust Region Policy optimization [32] have applied these ideas providing state of the art performance in a wide range of control problems.</p>
<p>Both techniques differ on the way they sample experiences. While the first is an off-policy algorithm - experiences are generated by a behaviour policy, the second is an on-policy algorithm where the policy used to generated experience is the same used to perform the control task. These algorithms will have comparable performance on different robotics control scenarios therefore should be considered the current state of the art on such problems.</p>
<h2>IV. BayesSim</h2>
<h3>A. Problem setup</h3>
<p>Following [24], BayesSim approximates the intractable posterior $p(\boldsymbol{\theta}|\mathbf{x}=\mathbf{x}^{r})$ by directly learning a conditional density $q_{\phi}(\boldsymbol{\theta}|\mathbf{x})$ parameterised by parameters $\phi$. As we shall see, $q_{\phi}(\boldsymbol{\theta}|\mathbf{x})$ takes the form of a mixture density random feature network. To learn the parameters $\phi$ we first generate a dataset</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1: Fetch Push and Sliding tasks: the robot has full access to the entire table and multiple iterations with the object (pushing) or one shot at pushing the object to its target (sliding).</p>
<p>with $N$ pairs $(\boldsymbol{\theta}<em n="n">{n}, \mathbf{x}</em>})$ where $\boldsymbol{\theta<em n="n">{n}$ is drawn independently from a distribution $\tilde{p}(\boldsymbol{\theta})$ referred to as the proposal prior. $\mathbf{x}</em>}$ is obtained by running the simulator with parameter $\boldsymbol{\theta<em n="n">{n}$ such that $\mathbf{x}</em>}=g\left(\boldsymbol{\theta<em _phi="\phi">{n}\right)$. In [24] the authors show that $q</em>}(\boldsymbol{\theta}|\mathbf{x})$ is proportional to $\frac{\tilde{p}(\boldsymbol{\theta})}{\tilde{p}(\boldsymbol{\theta})} p(\boldsymbol{\theta}|\mathbf{x})$ when the likelihood $\prod_{n} q_{\phi}\left(\boldsymbol{\theta<em n="n">{n} \mid \mathbf{x}</em>\right)$ is maximised w.r.t. $\phi$. We follow a similar procedure and maximise the log likelihood,</p>
<p>$$
\mathcal{L}(\phi)=\frac{1}{N} \sum_{n} \log q_{\phi}\left(\boldsymbol{\theta}<em n="n">{\boldsymbol{n}} \mid \mathbf{x}</em>\right)
$$</p>
<p>to determine $\phi$. After this is done, an estimate of the posterior is obtained by</p>
<p>$$
\hat{p}(\boldsymbol{\theta} \mid \mathbf{x}=\mathbf{x}^{r}) \propto \frac{p(\boldsymbol{\theta})}{\hat{p}(\boldsymbol{\theta})} q_{\phi}\left(\boldsymbol{\theta} \mid \mathbf{x}=\mathbf{x}^{r}\right)
$$</p>
<p>where $p(\boldsymbol{\theta})$ is the desirable prior that might be different than the proposal prior. In the case when $\hat{p}(\boldsymbol{\theta})=p(\boldsymbol{\theta})$, it follows that $\hat{p}(\boldsymbol{\theta} \mid \mathbf{x}=\mathbf{x}^{r}) \propto q_{\phi}(\boldsymbol{\theta} \mid \mathbf{x}=\mathbf{x}^{r})$. When $\hat{p}(\boldsymbol{\theta}) \neq p(\boldsymbol{\theta})$ we need to adjust the posterior as detailed in Section IV-E.</p>
<h3>B. Mixture density random feature networks</h3>
<p>We model the conditional density $q_{\phi}(\boldsymbol{\theta} \mid \mathbf{x})$ as a mixture of $K$ Gaussians,</p>
<p>$$
q_{\phi}(\boldsymbol{\theta} \mid \mathbf{x})=\sum_{k} \alpha_{k} \mathcal{N}\left(\boldsymbol{\theta} \mid \boldsymbol{\mu}<em k="k">{k}, \boldsymbol{\Sigma}</em>\right)
$$</p>
<p>where $\boldsymbol{\alpha}=\left(\alpha_{1}, \ldots, \alpha_{K}\right)$ are mixing coefficients, $\left{\mu_{k}\right}$ are means and $\left{\Sigma_{k}\right}$ are covariance matrices. This is analogous to mixture density networks [5] except that we replace the feedforward neural network with Quasi Monte Carlo (QMC) random Fourier features when computing $\boldsymbol{\alpha}, \boldsymbol{\mu}$ and $\boldsymbol{\Sigma}$. We justify and describe these features in the next section.</p>
<p>Denoting $\Phi(\boldsymbol{x})$ as the feature vector, the mixing coeficients are calculated as</p>
<p>$$
\boldsymbol{\alpha}=\operatorname{softmax}\left(\mathbf{W}<em _boldsymbol_alpha="\boldsymbol{\alpha">{\boldsymbol{\alpha}} \Phi(\mathbf{x})+\mathbf{b}</em>\right)
$$}</p>
<p>Note that the operator $\operatorname{softmax}\left(\mathbf{z}<em i="i">{i}\right)=\frac{\exp\left(z</em>$ for $i=$ $1, \ldots, K$ enforces that the sum of coeficients equals to 1 and each coefficient is between 0 and 1 .}\right)}{\sum_{k=1}^{K}\exp z_{k}</p>
<p>The means are defined as linear combinations of feature vectors. For each component of the mixture,</p>
<p>$$
\boldsymbol{\mu}<em _boldsymbol_mu="\boldsymbol{\mu">{k}=\mathbf{W}</em><em _boldsymbol_mu="\boldsymbol{\mu">{k}} \Phi(\mathbf{x})+\mathbf{b}</em>
$$}_{k}</p>
<p>Finally we parametrize the covariance matrices as diagonals matrices with</p>
<p>$$
\operatorname{diag}\left(\boldsymbol{\Sigma}<em _boldsymbol_Sigma="\boldsymbol{\Sigma">{k}\right)=\operatorname{mELU}\left(\mathbf{W}</em><em _boldsymbol_Sigma="\boldsymbol{\Sigma">{k}} \Phi(\mathbf{x})+\mathbf{b}</em>\right)
$$}_{k}</p>
<p>where mELU is a modified exponential linear unit defined as</p>
<p>$$
m E L U(z)= \begin{cases}\alpha\left(e^{z}-1\right)+1 &amp; \text { for } z \leq 0 \ z+1 &amp; \text { for } z&gt;0\end{cases}
$$</p>
<p>to enforce positive values. Experimentally this parametrization provided slightly better results than with the exponential function. The diagonal parametrization assumes independence between the dimensions of the simulator parameters $\boldsymbol{\theta}$. This turns out to be not too restrictive if the number of components in the mixture is large enough.</p>
<p>The full set of parameters for the mixture density network is then,</p>
<p>$$
\phi=\left(\mathbf{W}<em _boldsymbol_alpha="\boldsymbol{\alpha">{\boldsymbol{\alpha}}, \mathbf{b}</em>}},\left{\mathbf{W<em k="k">{\boldsymbol{\mu}</em>}}, \mathbf{b<em k="k">{\boldsymbol{\mu}</em>}}, \mathbf{W<em k="k">{\boldsymbol{\Sigma}</em>}}, \mathbf{b<em k="k">{\boldsymbol{\Sigma}</em>\right)
$$}}\right}_{k=1}^{K</p>
<h2>C. Neural Network features</h2>
<p>BayesSim can use neural network features creating a model similar to the mixture density network in [5]. For a feedforward neural network with two fully connected layers, the features take the form</p>
<p>$$
\Phi(\mathbf{x})=\sigma\left(\mathbf{W}<em 1="1">{2}\left(\sigma\left(\mathbf{W}</em>} \mathbf{x}+\mathbf{b<em 2="2">{1}\right)\right)+\mathbf{b}</em>\right)
$$</p>
<p>where $\sigma(\cdot)$ is a sigmoid function; we use $\sigma(\cdot)=\tanh (\cdot)$ in our experiments. This network structure was used in the experiments and compared to the Quasi Monte Carlo random features described below.</p>
<h2>D. Quasi Monte Carlo random features</h2>
<p>BayesSim can use random Fourier features [27] instead of neural nets to parameterise the mixture density. There are several reasons why this can be good choice. Notably, 1) random Fourier features - of which QMC features are a particular type - approximate possibly infinite Hilbert spaces with properties defined by the choice of the associated kernel. In this way prior information about properties of the function space can be readily incorporated by selecting a suitable positive semidefinite kernel; 2) the approximation converges to the original Hilbert space with order $\mathcal{O}(1 / \sqrt{s})$, where $s$ is the number of features, therefore independent of the input dimensionality; 3) experimentally, we verified that mixture densities with random Fourier features are more stable to different initialisations and converge to the same local maximum in most cases.</p>
<p>Random Fourier features approximate a shift invariant kernel $k(\boldsymbol{\tau})$, where $\boldsymbol{\tau}=\left|\mathbf{x}-\mathbf{x}^{\prime}\right|$, by a dot product $k(\boldsymbol{\tau}) \approx \Phi(\mathbf{x})^{T} \Phi(\mathbf{x})$ of finite dimensional features $\Phi(\mathbf{x})$. This is possible by first applying the Bochner's theorem [36] stated below:</p>
<p>Theorem 1 (Bochner's Theorem) A shift invariant kernel $k(\boldsymbol{\tau})$, $\boldsymbol{\tau} \in \mathbb{R}^{D}$, associated with a positive finite measure $d \mu(\boldsymbol{\omega})$ can be represented in terms of its Fourier transform as,</p>
<p>$$
k(\boldsymbol{\tau})=\int_{\mathbb{R}^{D}} e^{-i \boldsymbol{\omega} \cdot \boldsymbol{\tau}} d \mu(\boldsymbol{\omega})
$$</p>
<p>The proof can be found in [13]. When $\mu$ has density $\mathcal{K}(\boldsymbol{\omega})$ then $\mathcal{K}$ represents the spectral distribution for a positive semidefinite $k$. In this case $k(\boldsymbol{\tau})$ and $\mathcal{K}(\boldsymbol{\omega})$ are Fourier duals:</p>
<p>$$
k(\boldsymbol{\tau})=\int \mathcal{K}(\boldsymbol{\omega}) e^{-i \boldsymbol{\omega} \cdot \boldsymbol{\tau}} d \boldsymbol{\omega}
$$</p>
<p>Approximating Equation 13 with a Monte Carlo estimate with $N$ samples, yields</p>
<p>$$
k(\boldsymbol{\tau}) \approx \frac{1}{N} \sum_{n=1}^{N}\left(e^{-i \boldsymbol{\omega}<em n="n">{n} \mathbf{x}}\right)\left(e^{-i \boldsymbol{\omega}</em>\right)
$$} \mathbf{x}^{\prime}</p>
<p>where $\boldsymbol{\omega}$ is sampled from the density $\mathcal{K}(\boldsymbol{\omega})$.
Finally, using Euler's formula $\left(e^{-i x}=\cos (x)-i \sin (x)\right)$ we recover the features:</p>
<p>$$
\begin{aligned}
\Phi(\mathbf{x}) &amp; =\frac{1}{\sqrt{N}}\left[\cos \left(\boldsymbol{\omega}<em 1="1">{1} \mathbf{x}+b</em>}\right), \ldots, \cos \left(\boldsymbol{\omega<em n="n">{n} \mathbf{x}+b</em>\right)\right. \
&amp; \left.-i \cdot \sin \left(\boldsymbol{\omega}<em 1="1">{1} \mathbf{x}+b</em>}\right), \ldots,-i \cdot \sin \left(\boldsymbol{\omega<em n="n">{n} \mathbf{x}+b</em>\right)\right]
\end{aligned}
$$</p>
<p>where bias terms $\mathbf{b}_{i}$ are introduced with the goal of rotating the projection and allowing for more flexibility in capturing the correct frequencies.</p>
<p>This approximation can be used with all shift invariant kernels proving flexibility in introducing prior knowledge by selecting a suitable kernel for the problem. For example, the RBF kernel can be approximated using the features above with $\boldsymbol{\omega} \sim \mathcal{N}\left(0,2 \sigma^{-2} I\right)$ and $b \sim \mathcal{U}[-\pi, \pi] . \sigma$ is a hyperparameter that corresponds to the kernel length scale and is usually set up with cross validation.</p>
<p>We further adopt a quasi Monte Carlo strategy for sampling the frequencies. In particular we use Halton sequences [7] which has been shown in [3] to have better convergence rate and lower approximation error than standard Monte Carlo.</p>
<h2>E. Posterior recovery</h2>
<p>From Equation 4 we note that when the proposal prior is different than the desirable prior, we need to adjust the posterior by weighting it with the ratio $p(\boldsymbol{\theta}) / \hat{p}(\boldsymbol{\theta})$.</p>
<p>In this paper we assume the prior to be uniform, either with finite support - defined within a range and zero elsewhere or improper, constant value everywhere. Therefore,</p>
<p>$$
\hat{p}\left(\boldsymbol{\theta} \mid \mathbf{x}=\mathbf{x}^{r}\right) \propto \frac{q_{\phi}\left(\boldsymbol{\theta} \mid \mathbf{x}^{r}\right)}{\hat{p}(\boldsymbol{\theta})}
$$</p>
<p>When the proposal prior is Gaussian, we can compute the division between a mixture and a single Gaussian analytically.</p>
<p>In this case, since $q_{\phi}(\boldsymbol{\theta}|\mathbf{x})$ is a mixture of Gaussians and $\tilde{p}(\boldsymbol{\theta}) \sim \mathcal{N}\left(\boldsymbol{\theta} \mid \boldsymbol{\mu}<em 0="0">{0}, \boldsymbol{\Sigma}</em>\right)$, the solution is given by</p>
<p>$$
\tilde{p}\left(\boldsymbol{\theta} \mid \mathbf{x}=\mathbf{x}^{r}\right)=\sum_{k} \alpha_{k}^{\prime} \mathcal{N}\left(\boldsymbol{\theta} \mid \boldsymbol{\mu}<em k="k">{k}^{\prime}, \boldsymbol{\Sigma}</em>\right)
$$}^{\prime</p>
<p>where,</p>
<p>$$
\begin{aligned}
\boldsymbol{\Sigma}<em k="k">{k}^{\prime} &amp; =\left(\boldsymbol{\Sigma}</em>}^{-1}-\boldsymbol{\Sigma<em k="k">{0}^{-1}\right)^{-1} \
\boldsymbol{\mu}</em>}^{\prime} &amp; =\boldsymbol{\Sigma<em k="k">{k}^{-1}\left(\boldsymbol{\Sigma}</em>}^{-1} \boldsymbol{\mu<em 0="0">{k}-\boldsymbol{\Sigma}</em>}^{-1} \boldsymbol{\mu<em k="k">{0}\right) \
\alpha</em>
\end{aligned}
$$}^{\prime} &amp; =\frac{\alpha_{k} \exp \left(-\frac{1}{2} \lambda_{k}\right)}{\sum_{k^{\prime}} \alpha_{k^{\prime}} \exp \left(-\frac{1}{2} \lambda_{k^{\prime}}\right)</p>
<p>and the coefficients $\lambda_{k}$ are given by</p>
<p>$$
\begin{aligned}
\lambda_{k}=\log \operatorname{det} \boldsymbol{\Sigma}<em 0="0">{k}-\log &amp; \operatorname{det} \boldsymbol{\Sigma}</em>}-\log \operatorname{det} \boldsymbol{\Sigma<em k="k">{k}^{\prime}+\boldsymbol{\mu}</em>}^{T} \boldsymbol{\Sigma<em k="k">{k}^{-1} \boldsymbol{\mu}</em> \
&amp; -\boldsymbol{\mu}<em 0="0">{0}^{T} \boldsymbol{\Sigma}</em>}^{-1} \boldsymbol{\mu<em k_prime="k^{\prime">{0}-\boldsymbol{\mu}</em>}}^{\prime T} \boldsymbol{\Sigma<em k="k">{k}^{\prime-1} \boldsymbol{\mu}</em>
\end{aligned}
$$}^{\prime</p>
<h2>F. Sufficient statistics for state-action trajectories</h2>
<p>Trajectories of state and action pairs in typical problems can be long sequences making the input dimensionality to the model prohibitive large and computationally expensive. We adopt a strategy commonly used in ABC ; instead of inputting raw state and action sequences to the model, we first compute some sufficient statistics. Formally, $\mathbf{x}=\psi(\mathbf{S}, \mathbf{A})$ where $\mathbf{S}=\left{\mathbf{s}^{t}\right}<em t="1">{t=1}^{T}$ and $\mathbf{A}=\left{\mathbf{a}^{t}\right}</em>$ are sequences of states and actions from $t=1$ to $T$. There are many options in the literature for sufficient statistics for time series or trajectory data. For example, the mean, log variance and autocorrelation for each time series as well as cross-correlation between two time series. Another possibility is to learn these from data, for example with an unsupervised encoder-decoder recurrent neural network [35]. However, such a representation would need to be trained with simulated trajectories and might not be able to capture complexities in the real trajectories. This will be investigated in future work. Here we adopt a simpler strategy and use statistics commonly applied to stochastic dynamic systems such as the Lotka-Volterra model [40].}^{T</p>
<p>Defining $\boldsymbol{\tau}=\left{\mathbf{s}^{t}-\mathbf{s}^{t-1}\right}_{t=1}^{T}$ as the difference between immediate future states and current states, the statistics</p>
<p>$$
\psi(\mathbf{S}, \mathbf{A})=\left(\left{\left\langle\boldsymbol{\tau}<em j="j">{i}, \mathbf{A}</em>\right\rangle\right}<em s="s">{i=1, j=1}^{D</em>]\right)
$$}, D_{a}}, \mathrm{E}[\boldsymbol{\tau}], \operatorname{Var}[\boldsymbol{\tau</p>
<p>where $D_{s}$ is the dimensionality of the state space, $D_{a}$ is the dimensionality of the action space, $\langle\cdot, \cdot\rangle$ denotes the dot product, $\mathrm{E}[\cdot]$ is the expectation, and $\operatorname{Var}[\cdot]$ the variance.</p>
<h2>G. Example: CartPole posterior</h2>
<p>We provide a simple example to demonstrate the algorithm in estimating unknown simulation parameters for the famous CartPole problem. In this problem a pole installed on a cart needs to be balanced by applying forces to the left or to the right of the cart. For this example we assume that both the mass and the length of the pole are not available and we use BayesSim to obtain the posterior for these parameters. We assume uniform priors for both parameters and collect 1000 simulations following a rl-zoo policy ${ }^{2}$ to train BayesSim.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>With the model trained, we collected 10 trajectories with the correct parameters to simulate the real observations. Figure 2 shows the posteriors for both problems. As with many problems involving two related variables, masspole and pole length exhibit statistical dependencies that generate multiple explanations for their values. For example, the pole might have lower mass and longer length, or vice versa. BayesSim is able to recover the multi-modality nature of the posterior providing densities that represent the uncertainty of the problem accurately.</p>
<h2>H. Domain randomization with BayesSim</h2>
<p>Here we describe the domain randomization strategy to take full advantage of the posterior obtained by the inference method. Given the posterior obtained from the simulation parameters $\tilde{p}\left(\boldsymbol{\theta} \mid \mathbf{x}=\mathbf{x}^{r}\right)$ we maximize the objective,</p>
<p>$$
J(\boldsymbol{\beta})=\mathbb{E}<em _boldsymbol_eta="\boldsymbol{\eta">{\boldsymbol{\theta}}\left[\mathbb{E}</em>}}\left[\sum_{t=0}^{T-1} \gamma^{(t)} r\left(\mathbf{s<em t="t">{t}, \mathbf{a}</em>\right]\right]
$$}\right) \mid \boldsymbol{\beta</p>
<p>where $\boldsymbol{\theta} \sim \tilde{p}\left(\boldsymbol{\theta} \mid \mathbf{x}=\mathbf{x}^{r}\right)$ with respect to the policy parameters $\boldsymbol{\beta}$. Since the posterior is a mixture of Gaussians, the first expectation can be approximated by sampling a mixture component following the distribution over $\boldsymbol{\alpha}$ to obtain a component $k$, followed by sampling the corresponding Gaussian $\mathcal{N}\left(\boldsymbol{\theta} \mid \boldsymbol{\mu}<em k="k">{k}, \boldsymbol{\Sigma}</em>\right)$.</p>
<h2>V. EXPERIMENTS</h2>
<p>Experiments are presented in two different cases to demonstrate and assess the performance of BayesSim. In Section V-A we verify and compare the accuracy of the posterior recovered. In Section V-B we compare the robustness of policies trained by randomizing following the prior versus posterior distribution over simulation parameters.</p>
<h2>A. Posterior recovery</h2>
<p>The first analysis we carry out is the quality of the posteriors obtained for different problems and methods. We use the log probability of the target under the mixture model as the measure, defined as $\log p\left(\boldsymbol{\theta}_{<em>} \mid \mathbf{x}=\mathbf{x}^{r}\right)$, where $\boldsymbol{\theta}_{</em>}$ is the actual value for the parameter. We compare RejectionABC [26] as the baseline, the recent $\epsilon$-Free [24] which also provides a mixture model as the posterior, and BayesSim using either a two layer neural network with 24 units in each layer, and BayesSim with quasi random Fourier Features. For the later we use the Matern 5/2 kernel [28] and set up the the sampling precision $\sigma$ by cross validation. Three different simulators were used for different problems; OpenAI Gym [9], PyBullet ${ }^{3}$, and MuJoCo [39]. Finally, the following problems were considered; CartPole (Gym), Pendulum (Gym), Mountain Car (Gym), Acrobot (Gym), Hopper (PyBullet), Fetch Push (MuJoCo) and Fetch Slide (MuJoCo). For all configurations of methods and parameters, training and testing were performed 5 times with the log probabilities averaged and standard deviation computed. To extract the real observations,</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 2: Example of joint posteriors obtained for the CartPole problem with different parametrizations for length and masspole. The true value is indicated by a star. Note that the joint posteriors capture the multimodality of the problem when two or more explanations seem likely, for example, a longer pole length with a lighter masspole or vice versa.</p>
<p>we simulate the environments with the actual parameters 10 times and average the sufficient statistics to obtain x<sup>r</sup>. In all cases we collect sufficient statistics by performing rollouts for either a maximum of 200 time steps or until the end of the episode.</p>
<p>Table I shows the results (means and standard deviations) for the log probabilities. BayesSim with either RFF or Neural Network features provides generally higher log-probabilities and lower standard deviation than Rejection ABC. This indicates that the posteriors provided by BayesSim are more peaked and centered around the correct values for the parameters. Compared to ε-Free, the results are equivalent in terms of the means but BayesSim generally provides lower standard deviation across multiple runs of the method, indicating it is more stable than ε-Free. Comparing BayesSim with RFF and NN, the RFF features lead to higher log probabilities in most cases but BayesSim with neural networks have lower standard deviation.</p>
<p>These results suggest that BayesSim with either RFF or NN is comparable to the state-of-art, and in many cases superior when estimating the posterior distribution over the simulation parameters. For the robotics problems analyzed in the next section, however, BayesSim with RFF provide significant superior results than the other methods and slightly better than BayesSim with NN. This can be better observed when we plot the posteriors in Figure 3. BayesSim RFF is significantly more peaked and centered around the true friction value.</p>
<h3><em>B. Robustness of policies</em></h3>
<p>We evaluate robustness of policies by comparing their performance on the uniform prior and the learned posterior provided by BayesSim. Evaluation is done over a pre-defined range of simulator settings and the average reward is shown for each parameter value.</p>
<p>In the first set of experiments we use the CartPole problem as a simple example to illustrate the benefits of posterior randomization. We trained two policies, the first randomizing with a uniform prior for <em>length</em> and <em>masspole</em> as indicated in Table I. The second, randomized based on the posterior provided by BayesSim with RFF. In both cases we use PPO to train the policies with 100 samples from the prior and posterior, for 2M timesteps. The results are presented in Figure 4, averaged over several runs with the corresponding standard deviations. It can be observed that randomization over the posterior yields a significantly more robust policy, in particular at the actual parameter value. Also noticeable is the reduction in performance for lower <em>length</em> values and higher <em>masspole</em> values. This is expected as it is more difficult to control the pole position when the length is short due to the increased dynamics of the system. Similarly, when the mass increases too much, beyond the value it was actually trained on, the controller struggles to maintain the pole balanced. Importantly, the policy learned with the posterior seems much</p>
<table>
<thead>
<tr>
<th>Problem</th>
<th>Parameter</th>
<th>Uniform prior</th>
<th>Rejection ABC</th>
<th>$\epsilon$-Free</th>
<th>BayesSim RFF</th>
<th>BayesSim NN</th>
</tr>
</thead>
<tbody>
<tr>
<td>CartPole</td>
<td>pole length</td>
<td>$[0.1,2.0]$</td>
<td>-0.342$\pm$0.15</td>
<td>-0.211$\pm$0.07</td>
<td>-0.609$\pm$0.39</td>
<td>-0.657$\pm$0.25</td>
</tr>
<tr>
<td></td>
<td>pole mass</td>
<td>$[0.1,2.0]$</td>
<td>0.032$\pm$0.21</td>
<td>0.056$\pm$0.14</td>
<td>0.973 $\pm$ 0.26</td>
<td>0.633$\pm$ 0.52</td>
</tr>
<tr>
<td>Pendulum</td>
<td>dt</td>
<td>$[0.01,0.3]$</td>
<td>2.101$\pm$1.04</td>
<td>2.307$\pm$0.84</td>
<td>3.192$\pm$0.30</td>
<td>3.199$\pm$0.17</td>
</tr>
<tr>
<td>Mountain Car</td>
<td>power</td>
<td>$[0.0005,0.1]$</td>
<td>3.69$\pm$1.21</td>
<td>3.800$\pm$1.06</td>
<td>3.863$\pm$0.52</td>
<td>3.901$\pm$0.2</td>
</tr>
<tr>
<td>Acrobot</td>
<td>link mass 1</td>
<td>$[0.5,2.0]$</td>
<td>1.704$\pm$0.82</td>
<td>1.883$\pm$0.79</td>
<td>2.046$\pm$0.37</td>
<td>1.331$\pm$0.22</td>
</tr>
<tr>
<td></td>
<td>link mass 2</td>
<td>$[0.5,2.0]$</td>
<td>1.832$\pm$0.93</td>
<td>2.237$\pm$0.76</td>
<td>0.321$\pm$1.85</td>
<td>1.513$\pm$0.39</td>
</tr>
<tr>
<td></td>
<td>link length 1</td>
<td>$[0.1,1.5]$</td>
<td>2.421$\pm$0.75</td>
<td>2.135$\pm$0.50</td>
<td>2.072$\pm$0.76</td>
<td>1.856$\pm$0.18</td>
</tr>
<tr>
<td></td>
<td>link length 2</td>
<td>$[0.5,1.5]$</td>
<td>-0.521$\pm$0.36</td>
<td>-0.703$\pm$0.16</td>
<td>-0.148$\pm$0.19</td>
<td>-0.672$\pm$0.09</td>
</tr>
<tr>
<td>Hopper</td>
<td>lateral friction</td>
<td>$[0.3,0.5]$</td>
<td>3.032$\pm$0.43</td>
<td>3.154$\pm$0.81</td>
<td>2.622$\pm$0.64</td>
<td>3.391$\pm$0.08</td>
</tr>
<tr>
<td>Fetch Push</td>
<td>friction</td>
<td>$[0.1,1.0]$</td>
<td>1.332$\pm$0.54</td>
<td>2.013$\pm$0.09</td>
<td>2.423$\pm$0.07</td>
<td>2.404$\pm$0.05</td>
</tr>
<tr>
<td>Fetch Slide</td>
<td>friction</td>
<td>$[0.1,1.0]$</td>
<td>1.014$\pm$0.38</td>
<td>1.614$\pm$0.12</td>
<td>2.391$\pm$0.06</td>
<td>2.111$\pm$0.03</td>
</tr>
</tbody>
</table>
<p>TABLE I: Mean and standard deviation of log predicted probabilities for several likelihood-free methods, applied to seven different problems and parameters.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 4: Accumulated rewards for CartPole policies trained with PPO by randomizing over prior and posterior joint densities. Top left: Performance of the policy trained with the prior, over parameter length. masspole is set to actual. Top right: Similar to top left, but over multiple masspole values. Bottom left: Performance of policy trained with the posterior, over parameter length. Bottom right: Similar to bottom left, but over multiple masspole values.</p>
<p>more stable across multiple runs as indicated by the lower variance in the plots.</p>
<p>In the second set of experiments we use a Fetch robot available in OpenAI Gym [8] to perform both push and slide tasks. The first is a closed loop scenario, where the arm is always in range of the entire table and, hence, it can correct its trajectories according to the input it receives from the environment. The second is a more difficult open loop scenario, where the robot has usually only one shot at pushing the puck to its desired target. For both tasks, the friction coefficient of the object and the surface plays a major role in the final result as they are strictly related to how far the object goes after each force is applied. A very low friction coefficient means that the object is harder to control as it slides more easily and a very high one means that more force needs to be applied in order to make the object to move.</p>
<p>Our goal is to recover a good approximation of the posterior over friction coefficients using BayesSim. Initially, we need to learn a policy with a fixed friction coefficient that will be used for data generation purposes. We train this policy using</p>
<p>DDPG with experiences being sampled using HER for 200 epochs with 100 episodes/rollouts per epoch. Gradient updates are done using Adam with step size of 0.001. We then run this policy multiple times with different friction coefficients in order to approximate the likelihood function and recover the full posterior over simulation parameters. With the dynamics model in hand, we can finally recover the desired posterior using some data sampled from the environment we want to learn the dynamics from. Training is carried out using the same aforementioned settings but instead of using a fixed friction coefficient, we sample a new one from its respective distribution every time a new episode starts.</p>
<p>The results from both tasks are presented on Figure 5. As it has been shown in previously work [25], the uniform prior works remarkably well on the push task. This happens as the robot has the opportunity to correct its trajectory whether something goes wrong. As it has been exposed to a wide range of scenarios involving different dynamics, it can then use the input of the environment to perform corrective actions and still be able to achieve its goal. However, the results for slide task differ significantly since using a wide uniform prior has led the robot to achieve a very poor performance. This happens as not only the actions for different coefficients in most times are completely different but also because the robot has no option of correcting its trajectory. This is where methods like BayesSim are useful as it recovers a distribution with very high density around the true parameter and, hence, leads to a better overall control policy. Our results shows that higher rewards are achieved around the true friction value while the uniform prior results are mostly flat throughout all values.</p>
<h2>VI. CONCLUSIONS</h2>
<p>This paper represents the first step towards a Bayesian treatment of robotics simulation parameters, combined with domain randomization for policy search. Our approach is connected to system identification in that both attempt to estimate dynamic models, but ours uses a black-box generative model, or simulator, totally integrated into the framework. Prior distributions can also be provided and incorporated into the model to compute a full, potentially multi-modal posterior over the parameters. The method proposed here, BayesSim, performs comparably to other state-of-the-art likelihood-free approaches for Bayesian inference but appears more stable to different initializations, and across multiple runs when recovering the true posterior. Finally, we show that domain randomization with the posterior leads to more robust policies over multiple parameter values compared to policies trained on uniform prior randomization.</p>
<p>The two applications described in the paper for likelihoodfree inference are two instances of a large range of problems where simulators can make use of a full set of parametrizations to best represent reality. In this manner, our framework can be integrated in many other problems involving simulators. An interesting line of research for future work is to use BayesSim to help simulators synthesize images by randomizing over background properties. This can potentially help in making many computer vision problems more robust to environment variability in many tasks including object recognition, 3D pose estimation, or motion tracking.</p>
<p>As typical in the likelihood-free inference literature, BayesSim relies on the definition of meaningful sufficient statistics for the trajectories of states and actions. Alternatively, a lower dimensional representation for the trajectories could be created using recent encoder-decoder methods and recurrent neural networks known to perform well for time series prediction such as LSTMs [16]. Hence, the entire framework can be learnt end to end. This is an interesting area for future development but careful consideration should be given to potential overfitting to simulation data. LSTMs usually require a lot of data for training therefore most of the training trajectories will be generated from simulated trajectories. This can introduce undesirable specific characteristics of the simulator in the low dimensional representation that are not observed in real trajectories, making the representation less sensitive to variations in the simulator parameters to be estimated. Despite this, automating the process of generating robust statistics from trajectories remains a valuable direction for future research.</p>
<h2>REFERENCES</h2>
<p>[1] Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay. In Advances in Neural Information Processing Systems, pages 5048-5058, 2017.
[2] Marcin Andrychowicz, Bowen Baker, Maciek Chociej, Rafal Jozefowicz, Bob McGrew, Jakub Pachocki, Arthur Petron, Matthias Plappert, Glenn Powell, Alex Ray, et al. Learning dexterous in-hand manipulation. arXiv preprint arXiv:1808.00177, 2018.
[3] Haim Avron, Vikas Sindhwani, Jiyan Yang, and Michael W. Mahoney. Quasi-Monte Carlo feature maps for shift-invariant kernels. Journal of Machine Learning Research, 17(120):1-38, 2016.
[4] Mark A Beaumont, Wenyang Zhang, and David J Balding. Approximate bayesian computation in population genetics. Genetics, 4(162):2025-2035, 2002.
[5] Christopher M Bishop. Mixture density networks. Technical report, Citeseer, 1994.
[6] Fernando V Bonassi, Mike West, et al. Sequential Monte Carlo with adaptive weights for approximate bayesian computation. Bayesian Analysis, 10(1):171-187, 2015.
[7] E. Braaten and G. Weller. An improved low-discrepancy sequence for multidimensional quasi-Monte Carlo integration. Journal of Computational Physics, 33:249-258, November 1979.
[8] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.
[9] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym, 2016.
[10] Yevgen Chebotar, Ankur Handa, Viktor Makoviychuk, Miles Macklin, Jan Issac, Nathan Ratliff, and Dieter Fox. Closing the sim-to-real loop: Adapting simulation randomization with real world experience. In IEEE International Conference on Robotics and Automation (ICRA), 2018.
[11] Yevgen Chebotar, Ankur Handa, Viktor Makoviychuk, Miles Macklin, Jan Issac, Nathan Ratliff, and Dieter Fox. Closing the sim-to-real loop: Adapting simulation randomization with real world experience. arXiv preprint arXiv:1810.05687, 2018.
[12] P.J. Diggle and R.J. Gratton. Monte Carlo methods of inference for implicit statistical models. Journal of the Royal Statistical Society. Series B (Methodological), 2 (46):193-227, 1984.
[13] I. I. Gihman and A. V. Skorohod. The Theory of Stochastic Processes, volume 1. Springer Verlag, Berlin, 1974.
[14] Graham C. Goodwin and Robert L. Payne. Dynamic System Identification: Experiment Design and Data Analysis. Academic Press, 1977.
[15] Christian Gourieroux, Alain Monfort, and Eric Renault. Indirect inference. Journal of applied econometrics, 8 (S1):S85-S118, 1993.
[16] Sepp Hochreiter and Jürgen Schmidhuber. Long shortterm memory. Neural Comput., 9(8):1735-1780, November 1997. ISSN 0899-7667. doi: 10.1162/neco.1997. 9.8.1735. URL http://dx.doi.org/10.1162/neco.1997.9.8. 1735.
[17] Balakumar Sundaralingam Yu Xiang Dieter Fox Jonathan Tremblay, Thang To and Stan Birchfield. Deep object pose estimation for semantic robotic grasping of household objects. In Conference on Robot Learning (CoRL), 2018.
[18] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.
[19] Long-Ji Lin. Self-improving reactive agents based on reinforcement learning, planning and teaching. Machine learning, 8(3-4):293-321, 1992.
[20] Paul Marjoram, John Molitor, Vincent Plagnol, and Simon Tavaré. Markov chain Monte Carlo without likelihoods. Proceedings of the National Academy of Sciences, 100(26):15324-15328, 2003.
[21] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529, 2015.
[22] Igor Mordatch, Kendall Lowrey, and Emanuel Todorov. Ensemble-cio: Full-body dynamic motion planning that transfers to physical humanoids. In Intelligent Robots and Systems (IROS), 2015 IEEE/RSJ International Conference on, pages 5307-5314. IEEE, 2015.
[23] Lowrey K. Mordatch, I. and E. Todorov. EnsembleCIO: Full-body dynamic motion planning that transfers to physical humanoids. In Intelligent Robots and Systems (IROS), IEEE/RSJ International Conference on. IEEE, 2015.
[24] George Papamakarios and Iain Murray. Fast $\varepsilon$-free inference of simulation models with bayesian conditional density estimation. In Advances in Neural Information Processing Systems, pages 1028-1036, 2016.
[25] Xue Bin Peng, Marcin Andrychowicz, Wojciech Zaremba, and Pieter Abbeel. Sim-to-real transfer of robotic control with dynamics randomization. In 2018 IEEE International Conference on Robotics and Automation (ICRA), pages 1-8. IEEE, 2018.
[26] Jonathan K Pritchard, Mark T Seielstad, Anna PerezLezaun, and Marcus W Feldman. Population growth of human y chromosomes: a study of y chromosome microsatellites. Molecular biology and evolution, 16(12): 1791-1798, 1999.
[27] Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In J. C. Platt, D. Koller, Y. Singer, and S. T. Roweis, editors, Advances in Neural</p>
<p>Information Processing Systems 20, pages 1177-1184. Curran Associates, Inc., 2008.
[28] Carl Edward Rasmussen and Christopher K. I. Williams. Gaussian Processes for Machine Learning (Adaptive Computation and Machine Learning). The MIT Press, 2005.
[29] Chad M Schafer and Peter E Freeman. Likelihood-free inference in cosmology: Potential for the estimation of luminosity functions. In Statistical Challenges in Modern Astronomy V, pages 3-19. Springer, 2012.
[30] Tom Schaul, Daniel Horgan, Karol Gregor, and David Silver. Universal value function approximators. In International Conference on Machine Learning, pages 1312-1320, 2015.
[31] Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. arXiv preprint arXiv:1511.05952, 2015.
[32] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In International Conference on Machine Learning, pages 1889-1897, 2015.
[33] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
[34] S.A. Sisson, Y. Fan, and M.M. Tanaka. Sequential Monte Carlo without likelihoods. Proceedings of the National Academy of Sciences, 104(6):1760-1765, 2007.
[35] Nitish Srivastava, Elman Mansimov, and Ruslan Salakhudinov. Unsupervised learning of video representations using lstms. In Francis Bach and David Blei, editors, Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pages 843-852, Lille, France, 07-09 Jul 2015. PMLR.
[36] M. L. Stein. Interpolation of Spatial Data. SpringerVerlag, New york, 1999.
[37] Jie Tan, Tingnan Zhang, Erwin Coumans, Atil Iscen, Yunfei Bai, Danijar Hafner, Steven Bohez, and Vincent Vanhoucke. Sim-to-real: Learning agile locomotion for quadruped robots. arXiv preprint arXiv:1804.10332, 2018.
[38] Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and Pieter Abbeel. Domain randomization for transferring deep neural networks from simulation to the real world. In Intelligent Robots and Systems (IROS), 2017 IEEE/RSJ International Conference on, pages 23-30. IEEE, 2017.
[39] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 5026-5033. IEEE, 2012.
[40] Darren J. Wilkinson. Stochastic Modelling for Systems Biology, Second Edition. Chapman \&amp; Hall/CRC Mathematical and Computational Biology. Taylor \&amp; Francis, 2011.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ https://github.com/araffin/rl-baselines-zoo&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{3}$ https://pypi.org/project/pybullet/&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
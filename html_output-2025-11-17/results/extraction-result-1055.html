<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1055 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1055</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1055</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-24.html">extraction-schema-24</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <p><strong>Paper ID:</strong> paper-10470466</p>
                <p><strong>Paper Title:</strong> <a href="https://web.archive.org/web/20170829151719/http:/eprints.uwe.ac.uk/21571/1/Erbas_etal_ImitationEnhancedLearning.pdf" target="_blank">Embodied imitation-enhanced reinforcement learning in multi-agent systems</a></p>
                <p><strong>Paper Abstract:</strong> Imitation is an example of social learning in which an individual observes and copies another’s actions. This paper presents a new method for using imitation as a way of enhancing the learning speed of individual agents that employ a well-known reinforcement learning algorithm, namely Q-learning. Compared with other research that uses imitation with reinforcement learning, our method uses imitation of purely observed behaviours to enhance learning, with no internal state access or sharing of experiences between agents. The paper evaluates our imitation-enhanced reinforcement learning approach in both simulation and with real robots in continuous space. Both simulation and real robot experimental results show that the learning speed of the group is improved.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1055.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1055.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Q-learner (no-imitation)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Epsilon-greedy Q-learning agent (no imitation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A single-agent epsilon-greedy Q-learning baseline used in simulation: discrete-state, discrete-action Q-learning with α=0.2, γ=0.7 and 8 compass-direction actions; used as control against imitation-enhanced variants.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Simulated Q-learner (no-imitation)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Standard tabular Q-learning with ε-greedy action selection (α=0.2, γ=0.7). The agent updates Q(s,a) after each state transition; no imitation or external action copying is used.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>10x10 grid-world (discrete)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>A 10×10 discrete grid arena with agent starting at top-left and a single goal at bottom-right. Agent can move to any of 8 neighbouring cells per time step. Episodes end when agent reaches goal.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Grid size: 10×10 = 100 states; 8 discrete actions per state → 100×8 = 800 state-action pairs (inferred from setup); episode length measured in time steps until goal.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>low</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>None / single fixed environment instance (same arena layout, same goal position), no procedural variation.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>low</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Mean best path length (shortest-path length inferred from current Q-values), measured periodically (every 10 time units in simulation); Q-value convergence checked after 100,000 goal reaches.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Baseline converges to optimal Q-values only after long training (Q-values run for 100,000 goal reaches per experiment); learns slower than imitation-enhanced agents in same setting (exact numeric best-path lengths not reported in text figures).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Not explicitly discussed for this baseline alone; used as a control to demonstrate that imitation (when beneficial) accelerates learning in same low-complexity environment.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td>This baseline performance represents the low-complexity/low-variation case in the paper and is slower to discover the shortest path than imitation-enhanced agents (no precise numeric values provided).</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Single-environment training; long single-task episodic training (100,000 goal reaches in simulation).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>This baseline was later compared across larger/more complex arenas and continuous robot experiments to test whether imitation variants generalize benefits; baseline serves as comparator rather than a generalization target.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Training runs used 100,000 goal-reaching episodes (simulation) to allow Q-values to converge; used as reference for sample-efficiency comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Provides reference learning speed: pure Q-learning is slower to reach optimal shortest paths than imitation-enhanced variants when beneficial demonstrations are available; it converges given sufficient visits (100k goal reaches) but is outperformed in sample efficiency by agents that exploit beneficial observed behaviours.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1055.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1055.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Imitation-Q (avg‑Q policy)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Imitation-enhanced Q-learning agent (average-Q temporal test selection)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Q-learning agent augmented with an imitation mechanism that probabilistically executes observed action sequences and adapts imitation probability using temporal change in average Q-values (average-Q test); this selection method minimized harm from imitating non-beneficial paths.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Imitation-enhanced Q-learner (average-Q test)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Tabular Q-learning (α=0.2, γ=0.7, ε-greedy) augmented with memory of observed action sequences (paths). For each observed path i the agent tracks how often the average Q increased when the path was replicated and computes R_i to choose and set p_i_imitate = βR_i. Three selection strategies were compared; the average-Q temporal-change test performed best at avoiding negative imitation.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>10x10 grid-world (initial tests) and 100x100 grid with central obstacle (further tests)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Initially tested in 10×10 grid with fixed start/goal; later tested in a 100×100 grid arena containing a central obstacle (more complex navigation). Observed paths (demonstrations) could be beneficial or non-beneficial; multiple distinct observed paths are stored (n distinct paths).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>For 10×10: 100 states × 8 actions = 800 state-action pairs; for 100×100: 10,000 states × 8 actions = 80,000 state-action pairs. Complexity also measured qualitatively by obstacle presence (100×100 case) and required shortest-path length (e.g., shortest path in 100×100 example consisted of 120 directional moves).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>low (10×10) → high (100×100 with obstacle)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Variation arises from (a) multiple distinct observed paths (e.g., set of 8 predefined 20-move paths), (b) whether paths are beneficial/non-beneficial, and (c) embodiment-produced copy errors in robot experiments; path-selection memory size = up to 10 paths; β controls initial imitation probability (values tested: 0.1, 0.3, 0.5).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium (simulated multiple path sets) to high (when embodied copying errors occur in robot experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Mean best path length (average shortest path length inferred from the current Q-table), tracked over time; statistical comparisons used pair-wise t-tests to compare learning curves (significance windows reported in time-step units).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Imitation with average-Q test: clear improvement in learning speed when observed paths are beneficial. When copying non-beneficial paths, average-Q approach produced minimal performance loss compared to fixed p_im or path-completion tests; specifically, with β=0.1 difference vs no-imitation was not statistically significant, while β=0.3/0.5 produced small but significant losses (no precise numeric path-lengths provided in text).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Yes — explicitly discussed: imitation helps accelerate learning in both simple and more complex environments when observed behaviours are useful; however, imitating non-beneficial actions can slow learning unless the agent adaptively filters them (average-Q test). Increased environment complexity increases the value of selecting beneficial imitations, while high variation (many non-beneficial demonstrations or mutual copying) can reduce benefits.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td>In the 100×100 arena with obstacle and a single high-quality expert demonstration, imitation-enhanced agent (with path selection) found the shortest path much earlier than baseline (statistically significant advantage lasting until ~15.5m time steps). Exact path-length numbers are not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td>In 10×10 with many differing/non-beneficial paths, naive imitation (fixed p_im) caused statistically significant performance loss; average-Q test reduced this loss substantially (with small or non-significant loss at β=0.1).</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td>When two agents copied each other in the 100×100 arena (mutual imitation/no expert), initial improvement occurred but benefit vanished after ~5m time steps due to mutual influence and lack of consistently beneficial demonstrations; overall performance similar to no-imitation after convergence.</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td>When a single beneficial path was provided in 10×10 and p_im appropriately set, imitation-enhanced agent discovered the shortest path much earlier than baseline (significantly improved sample efficiency).</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Single-environment training augmented by sampling observed paths (demonstrations) periodically (e.g., copy a new fragment every 100 simulation runs); path memory and selection operate online. No explicit curriculum or domain-randomization used.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Algorithm was shown to generalize from the 10×10 simulation tests to the more complex 100×100 arena and to continuous-space robot experiments: the same imitation-selection mechanisms (average-Q test, R_i) allowed agents/robots to exploit beneficial observed behaviours across environment complexities.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Simulation experiments used long training: 100,000 goal-reaching episodes per run for discrete simulations; imitation-enhanced variants reached near-optimal solutions substantially earlier (relative time windows reported, e.g., significant advantage until 15.5m time steps in some setups).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Adaptive imitation probability based on temporal increases in average Q (average-Q test) enables agents to exploit beneficial observed behaviour while minimizing harm from non-beneficial demonstrations; in more complex environments, selecting useful paths is more critical and imitation yields larger sample-efficiency gains; mutual imitation without a good source (expert) can limit long-term benefit due to circular reinforcement.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1055.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1055.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multi-agent copying (100×100)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Imitation-enhanced agents in large arena (given path set / copying expert / copying experienced / mutual copying)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Set of multi-agent experiments in a 100×100 grid with a central obstacle comparing several social-learning configurations: (a) agent given 8 directional path primitives, (b) agent copying an expert, (c) agent copying an experienced (but not expert) agent, and (d) two agents copying each other from the start.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Imitation-enhanced agents (multi-agent scenarios)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agents run imitation-enhanced Q-learning with stored path lists (memory up to 10 paths). Demonstrations are injected periodically (e.g., fragment every 100 simulation runs). Path selection uses R_i ratios (average-Q increase counts) and stochastic selection P_i_choose = R_i / sum R_k.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated multi-agent system</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>100×100 grid-world with central obstacle</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Large discrete arena (100×100 cells) with an obstacle placed centrally; start is top-left, goal bottom-right. Long shortest paths require combinations of many directional moves (example shortest path: 80 SE, 20 S, 20 E). This environment increases navigation complexity and provides more opportunities for demonstrations to be useful or misleading.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>100×100 grid = 10,000 states; 8 actions per state → 80,000 state-action pairs (inferred). Increased episode length (thousands to millions of time steps) to reach goal; presence of obstacle increases path planning complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Different experimental social configurations (different demonstration sources) and a set of 8 predefined path primitives (each 20 moves) provide structured variation; mutual copying introduces distributional shift as agents influence each other. Number of paths in memory limited to 10; new path fragment copied every 100 simulation runs (1000 fragments across 100k runs).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium to high (depends on scenario; mutual copying introduces high social variation)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Mean best path length averaged over 100 runs; statistical comparisons via pair-wise t-tests; significance windows reported in time-step units (m notation: e.g. 5m = 5,000,000 time steps).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Agent copying an expert outperformed all other scenarios and found the shortest path much earlier (statistically significant until ≈15.5m time steps). Agent given 8 path primitives achieved similar performance by ≈5m time steps; agent copying an experienced (non-expert) source matched performance of the expert/given-path scenarios by ≈8m time steps. Two agents copying each other improved early but converged to performance similar to no-imitation after ≈5m time steps.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Explicitly discussed: in the high-complexity arena, high-quality demonstrations (expert) strongly accelerate learning; structured variation (a curated set of path primitives) also helps. However, high social variation without a reliable source (mutual copying) reduces long-term benefit because agents reinforce suboptimal behaviours. Thus, complexity increases the value of good demonstrations but also increases the cost of copying poor/variable demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td>Copying a single expert in the 100×100 arena (high complexity, low external variation) produced the best performance: shortest path found much earlier than baseline and other scenarios; advantage statistically significant until ≈15.5m time steps.</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td>Not directly reported as a separate controlled condition; when many non-beneficial paths were available in simpler 10×10 tests, imitation could harm learning unless filtered (see average-Q test results).</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td>Two agents copying each other (both learning) in the 100×100 arena: initial benefit but long-term performance similar to no-imitation, with improvements disappearing after ≈5m time steps; indicates mutual high variation reduces sustained benefit in complex environments.</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td>Agent given a single beneficial path in 10×10 (low complexity, low variation) quickly exploited it and found the shortest path much earlier than baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Single environment with repeated episodic training; demonstrations injected periodically (every 100 simulation runs); path memory/replacement policy used (evict lowest R_i when full).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Agents and mechanisms that successfully exploited demonstrations in 10×10 generalized to the more complex 100×100 environment, but performance gains depended strongly on quality of demonstrations and on social configuration (expert vs mutual copying).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Simulation length 100,000 runs per experiment; imitation accelerated time to discover shortest paths by orders of magnitude in favorable conditions (qualitative/time-window descriptions reported; exact numeric path-length/time-to-optimum not tabulated in text).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>In a high-complexity spatial task, high-quality demonstrations (expert) and structured primitives substantially improve sample efficiency; social scenarios without a reliable model (mutual copying) deliver only transient gains. Imitation-selection mechanisms (R_i, average-Q tests) are critical to exploit demonstrations without being harmed by non-beneficial ones.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1055.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1055.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Q-share (internal-state access)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Internal-state-access imitation (copying mentor Q-values / Q-table sharing)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A variant where the observer has full access to the mentor's Q-table (internal state) and selects actions using the mentor's highest-Q suggestion for the observer's current state; yields near-instant learning from an expert but little benefit when observed agent lacks state-specific knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Internal-state-access agent (Q-table sharing)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Observer reads mentor's Q-values for its current state and repeatedly selects the mentor's argmax action for each state; falls back to its own Q-values when mentor-suggested action conflicts with its own high-Q action. This is equivalent to direct state-specific advice / policy suggestion.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent (with privileged communication)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>100×100 grid and other simulated arenas</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same grid arenas as other simulations; here the difference is that agents can request mentor Q(s,·) values for the observer's current state (telepathy-like internal access).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>As above: state-space size matches grid size (100×100 → 10,000 states). Complexity arises from requiring mentor to have visited same observer states for suggestions to be useful.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high (when applied to 100×100) or low for 10×10</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Variation arises from mentor experience level: expert mentor (high-quality Q-table) vs inexperienced mentor (sparse/noisy Q-values). Degree of variation depends on mentor coverage of observer states.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>dependent on mentor; can be low (expert) or effectively high (inexperienced/noisy mentor).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Mean best path length over time; convergence time to optimal policy.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>When copying from an expert, the observer became optimal after a very short time interval (near-instant convergence). When two inexperienced agents shared Q-tables from the start, performance was almost identical to standard ε-greedy Q-learning (no statistical difference reported).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Yes: this method is powerful in high-complexity environments only if the mentor has state-specific knowledge (visited states). If the mentor has not visited the observer's states, the suggested actions are essentially random and no benefit accrues; in contrast observed-action imitation allows testing applicability of actions across states and can provide more exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td>High performance (near-instant optimal) when copying an expert (low variation in advice because mentor is optimal).</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td>Not directly tested, but sharing noisy Q-tables from inexperienced mentors produced no advantage over baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td>When two inexperienced agents share internal states, performance similar to baseline: internal-state access did not improve learning and lacks the exploration benefits of observed imitation.</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Direct policy/Q-value transfer (state-specific advice) in addition to own Q-learning updates; no demonstration filtering required because actions are suggested per state.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Shows limited generalization advantage: works extremely well if mentor is expert (generalizes optimal policy to observer states that mentor has visited), but provides little benefit otherwise. Observed imitation (action sequences) offers different generalization properties by enabling the learner to evaluate the utility of action sequences in multiple states.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Very high sample efficiency when mentor is expert (near-instant), otherwise similar to baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Direct access to another agent's internal Q-values can produce extremely fast learning when the mentor is expert, but offers little or no benefit (and less scope for exploration) when mentors are inexperienced; observed action imitation provides broader exploratory potential because actions are tested in multiple contexts.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1055.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1055.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>E-puck embodied imitative robots</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>E-puck robots with embodied imitation-enhanced Q-learning (continuous-space SMDP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Physical e-puck robots running an embodied movement-imitation pipeline (vision-based frame processing → line-seg regression → motor command sequence) combined with SMDP-adapted Q-learning over a 10×10 discretization of each compartment; experiments show embodied imitation increases group learning speed and that copying errors (embodiment noise) can produce beneficial variation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>E-puck robots (embodied imitation-enhanced Q-learning)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Physical robots (e-puck with Linux extension) use a vision-based embodied imitation algorithm (up to 5 fps frame processing, regression to line segments, conversion to motor commands) and an SMDP version of Q-learning (continuous time update per Bradtke & Duff / Crites & Barto style with e^{-βΔt} discounting). Agents discretize their compartment into a 10×10 grid and use imitation-selection mechanisms (R_i / average-Q temporal checks) analogous to simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>physical robot (embodied, continuous-space)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>3 m × 3 m arena divided into 120 cm × 120 cm compartments; each compartment discretized to 10×10 grid</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Continuous 2D physical arena with high-precision external tracking; each robot resides in a separate compartment (120×120 cm) discretized into 10×10 cells for SMDP Q-learning. Robots can observe other robots in adjacent compartments (moved by human intervention to observe) and copy movement patterns for ~30 seconds; copying is purely vision-based (no internal state sharing).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Continuous-space SMDP approximated by 10×10 discretization per compartment: 100 states × 8 actions = 800 state-action pairs per robot (inferred). Complexity arises from sensor noise, physical embodiment, non-perfect imitation, and continuous-time dynamics; each robot run involved up to 15,000 moves (a full experimental run).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>medium-high (continuous embodiment + discretization approximation)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Embodiment-induced variation (copying errors) in perceived demonstrations; number of stored paths limited to 10; demonstrations injected periodically (every 10 tracks up to a cap). Variation measured qualitatively by differences between original and copied paths (example figures show imperfect matches); these variations act like mutations.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>high (due to embodied sensor/motor noise producing diverse imperfect copies)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Mean best path length (shortest path length inferred from current Q-values) averaged over 4 experimental runs; result points spaced every 10 state transitions; confidence intervals reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Imitation-enhanced robots (copying an expert) found the shortest path much earlier than robots without imitation (results averaged over 4 runs; exact numeric path-lengths omitted in text). Two robots copying each other showed higher early learning speed than no-imitation on average, but effect size was smaller and more variable; copying errors sometimes produced novel beneficial movement patterns selected by R_i.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Explicitly discussed: embodiment produces variation (copy errors) that can be beneficial (analogy to mutation) because the imitation-selection mechanism can select beneficial variants; thus in embodied continuous environments, variation arising from physical instantiation can improve exploration and solution diversity. However, the benefit depends on the ability to filter/recognize beneficial variants (average-Q temporal checks).</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td>In the embodied continuous setting, high variation (copying errors) sometimes produced novel beneficial patterns which, when selected by the path-filtering mechanism, improved learning; no single aggregate numeric performance reported, but qualitative result: embodied variation can increase emergence of useful behaviours.</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Single-task SMDP learning in a discretized compartment; demonstrations observed episodically (human-assisted repositioning to observe demonstrator for ~30s) and stored; path-selection and imitation probability adapted via R_i and average-Q temporal tests.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Mechanisms developed in simulation generalized to real robots: imitation-selection via temporal Q change allowed robots to exploit expert/experienced demonstrations and to avoid useless ones; embodied copying errors introduced new variation exploited by selection, demonstrating that real-world embodiment changes dynamics relative to pure simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Physical experiment runs consisted of 15,000 moves per run and required ~10 hours per run; imitation accelerated discovery of good solutions substantially compared to no-imitation in these time-limited runs (quantitative run-averaged curves shown but raw numeric values not enumerated in text).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Embodied imitation in real robots increases learning speed when beneficial demonstrations are available; sensorimotor copying errors introduce variation analogous to mutation, which can produce novel beneficial movement patterns that are then selected by the imitation-selection mechanism; practical trade-offs include experiment time cost (long physical runs) and the need for demonstration-filtering to avoid copying harmful behaviors.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Apprenticeship learning via inverse reinforcement learning <em>(Rating: 2)</em></li>
                <li>Accelerating reinforcement learning through implicit imitation <em>(Rating: 2)</em></li>
                <li>Imitative reinforcement learning for soccer playing robots <em>(Rating: 2)</em></li>
                <li>Learning from observation and practice using behavioral primitives: marble maze <em>(Rating: 2)</em></li>
                <li>Improving elevator performance using reinforcement learning <em>(Rating: 2)</em></li>
                <li>On the emergence of structure in behaviours evolved through embodied imitation in group of robots <em>(Rating: 2)</em></li>
                <li>Robot programming by demonstration <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1055",
    "paper_id": "paper-10470466",
    "extraction_schema_id": "extraction-schema-24",
    "extracted_data": [
        {
            "name_short": "Q-learner (no-imitation)",
            "name_full": "Epsilon-greedy Q-learning agent (no imitation)",
            "brief_description": "A single-agent epsilon-greedy Q-learning baseline used in simulation: discrete-state, discrete-action Q-learning with α=0.2, γ=0.7 and 8 compass-direction actions; used as control against imitation-enhanced variants.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Simulated Q-learner (no-imitation)",
            "agent_description": "Standard tabular Q-learning with ε-greedy action selection (α=0.2, γ=0.7). The agent updates Q(s,a) after each state transition; no imitation or external action copying is used.",
            "agent_type": "simulated agent",
            "environment_name": "10x10 grid-world (discrete)",
            "environment_description": "A 10×10 discrete grid arena with agent starting at top-left and a single goal at bottom-right. Agent can move to any of 8 neighbouring cells per time step. Episodes end when agent reaches goal.",
            "complexity_measure": "Grid size: 10×10 = 100 states; 8 discrete actions per state → 100×8 = 800 state-action pairs (inferred from setup); episode length measured in time steps until goal.",
            "complexity_level": "low",
            "variation_measure": "None / single fixed environment instance (same arena layout, same goal position), no procedural variation.",
            "variation_level": "low",
            "performance_metric": "Mean best path length (shortest-path length inferred from current Q-values), measured periodically (every 10 time units in simulation); Q-value convergence checked after 100,000 goal reaches.",
            "performance_value": "Baseline converges to optimal Q-values only after long training (Q-values run for 100,000 goal reaches per experiment); learns slower than imitation-enhanced agents in same setting (exact numeric best-path lengths not reported in text figures).",
            "complexity_variation_relationship": "Not explicitly discussed for this baseline alone; used as a control to demonstrate that imitation (when beneficial) accelerates learning in same low-complexity environment.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": "This baseline performance represents the low-complexity/low-variation case in the paper and is slower to discover the shortest path than imitation-enhanced agents (no precise numeric values provided).",
            "training_strategy": "Single-environment training; long single-task episodic training (100,000 goal reaches in simulation).",
            "generalization_tested": true,
            "generalization_results": "This baseline was later compared across larger/more complex arenas and continuous robot experiments to test whether imitation variants generalize benefits; baseline serves as comparator rather than a generalization target.",
            "sample_efficiency": "Training runs used 100,000 goal-reaching episodes (simulation) to allow Q-values to converge; used as reference for sample-efficiency comparisons.",
            "key_findings": "Provides reference learning speed: pure Q-learning is slower to reach optimal shortest paths than imitation-enhanced variants when beneficial demonstrations are available; it converges given sufficient visits (100k goal reaches) but is outperformed in sample efficiency by agents that exploit beneficial observed behaviours.",
            "uuid": "e1055.0"
        },
        {
            "name_short": "Imitation-Q (avg‑Q policy)",
            "name_full": "Imitation-enhanced Q-learning agent (average-Q temporal test selection)",
            "brief_description": "A Q-learning agent augmented with an imitation mechanism that probabilistically executes observed action sequences and adapts imitation probability using temporal change in average Q-values (average-Q test); this selection method minimized harm from imitating non-beneficial paths.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Imitation-enhanced Q-learner (average-Q test)",
            "agent_description": "Tabular Q-learning (α=0.2, γ=0.7, ε-greedy) augmented with memory of observed action sequences (paths). For each observed path i the agent tracks how often the average Q increased when the path was replicated and computes R_i to choose and set p_i_imitate = βR_i. Three selection strategies were compared; the average-Q temporal-change test performed best at avoiding negative imitation.",
            "agent_type": "simulated agent",
            "environment_name": "10x10 grid-world (initial tests) and 100x100 grid with central obstacle (further tests)",
            "environment_description": "Initially tested in 10×10 grid with fixed start/goal; later tested in a 100×100 grid arena containing a central obstacle (more complex navigation). Observed paths (demonstrations) could be beneficial or non-beneficial; multiple distinct observed paths are stored (n distinct paths).",
            "complexity_measure": "For 10×10: 100 states × 8 actions = 800 state-action pairs; for 100×100: 10,000 states × 8 actions = 80,000 state-action pairs. Complexity also measured qualitatively by obstacle presence (100×100 case) and required shortest-path length (e.g., shortest path in 100×100 example consisted of 120 directional moves).",
            "complexity_level": "low (10×10) → high (100×100 with obstacle)",
            "variation_measure": "Variation arises from (a) multiple distinct observed paths (e.g., set of 8 predefined 20-move paths), (b) whether paths are beneficial/non-beneficial, and (c) embodiment-produced copy errors in robot experiments; path-selection memory size = up to 10 paths; β controls initial imitation probability (values tested: 0.1, 0.3, 0.5).",
            "variation_level": "medium (simulated multiple path sets) to high (when embodied copying errors occur in robot experiments)",
            "performance_metric": "Mean best path length (average shortest path length inferred from the current Q-table), tracked over time; statistical comparisons used pair-wise t-tests to compare learning curves (significance windows reported in time-step units).",
            "performance_value": "Imitation with average-Q test: clear improvement in learning speed when observed paths are beneficial. When copying non-beneficial paths, average-Q approach produced minimal performance loss compared to fixed p_im or path-completion tests; specifically, with β=0.1 difference vs no-imitation was not statistically significant, while β=0.3/0.5 produced small but significant losses (no precise numeric path-lengths provided in text).",
            "complexity_variation_relationship": "Yes — explicitly discussed: imitation helps accelerate learning in both simple and more complex environments when observed behaviours are useful; however, imitating non-beneficial actions can slow learning unless the agent adaptively filters them (average-Q test). Increased environment complexity increases the value of selecting beneficial imitations, while high variation (many non-beneficial demonstrations or mutual copying) can reduce benefits.",
            "high_complexity_low_variation_performance": "In the 100×100 arena with obstacle and a single high-quality expert demonstration, imitation-enhanced agent (with path selection) found the shortest path much earlier than baseline (statistically significant advantage lasting until ~15.5m time steps). Exact path-length numbers are not provided.",
            "low_complexity_high_variation_performance": "In 10×10 with many differing/non-beneficial paths, naive imitation (fixed p_im) caused statistically significant performance loss; average-Q test reduced this loss substantially (with small or non-significant loss at β=0.1).",
            "high_complexity_high_variation_performance": "When two agents copied each other in the 100×100 arena (mutual imitation/no expert), initial improvement occurred but benefit vanished after ~5m time steps due to mutual influence and lack of consistently beneficial demonstrations; overall performance similar to no-imitation after convergence.",
            "low_complexity_low_variation_performance": "When a single beneficial path was provided in 10×10 and p_im appropriately set, imitation-enhanced agent discovered the shortest path much earlier than baseline (significantly improved sample efficiency).",
            "training_strategy": "Single-environment training augmented by sampling observed paths (demonstrations) periodically (e.g., copy a new fragment every 100 simulation runs); path memory and selection operate online. No explicit curriculum or domain-randomization used.",
            "generalization_tested": true,
            "generalization_results": "Algorithm was shown to generalize from the 10×10 simulation tests to the more complex 100×100 arena and to continuous-space robot experiments: the same imitation-selection mechanisms (average-Q test, R_i) allowed agents/robots to exploit beneficial observed behaviours across environment complexities.",
            "sample_efficiency": "Simulation experiments used long training: 100,000 goal-reaching episodes per run for discrete simulations; imitation-enhanced variants reached near-optimal solutions substantially earlier (relative time windows reported, e.g., significant advantage until 15.5m time steps in some setups).",
            "key_findings": "Adaptive imitation probability based on temporal increases in average Q (average-Q test) enables agents to exploit beneficial observed behaviour while minimizing harm from non-beneficial demonstrations; in more complex environments, selecting useful paths is more critical and imitation yields larger sample-efficiency gains; mutual imitation without a good source (expert) can limit long-term benefit due to circular reinforcement.",
            "uuid": "e1055.1"
        },
        {
            "name_short": "Multi-agent copying (100×100)",
            "name_full": "Imitation-enhanced agents in large arena (given path set / copying expert / copying experienced / mutual copying)",
            "brief_description": "Set of multi-agent experiments in a 100×100 grid with a central obstacle comparing several social-learning configurations: (a) agent given 8 directional path primitives, (b) agent copying an expert, (c) agent copying an experienced (but not expert) agent, and (d) two agents copying each other from the start.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Imitation-enhanced agents (multi-agent scenarios)",
            "agent_description": "Agents run imitation-enhanced Q-learning with stored path lists (memory up to 10 paths). Demonstrations are injected periodically (e.g., fragment every 100 simulation runs). Path selection uses R_i ratios (average-Q increase counts) and stochastic selection P_i_choose = R_i / sum R_k.",
            "agent_type": "simulated multi-agent system",
            "environment_name": "100×100 grid-world with central obstacle",
            "environment_description": "Large discrete arena (100×100 cells) with an obstacle placed centrally; start is top-left, goal bottom-right. Long shortest paths require combinations of many directional moves (example shortest path: 80 SE, 20 S, 20 E). This environment increases navigation complexity and provides more opportunities for demonstrations to be useful or misleading.",
            "complexity_measure": "100×100 grid = 10,000 states; 8 actions per state → 80,000 state-action pairs (inferred). Increased episode length (thousands to millions of time steps) to reach goal; presence of obstacle increases path planning complexity.",
            "complexity_level": "high",
            "variation_measure": "Different experimental social configurations (different demonstration sources) and a set of 8 predefined path primitives (each 20 moves) provide structured variation; mutual copying introduces distributional shift as agents influence each other. Number of paths in memory limited to 10; new path fragment copied every 100 simulation runs (1000 fragments across 100k runs).",
            "variation_level": "medium to high (depends on scenario; mutual copying introduces high social variation)",
            "performance_metric": "Mean best path length averaged over 100 runs; statistical comparisons via pair-wise t-tests; significance windows reported in time-step units (m notation: e.g. 5m = 5,000,000 time steps).",
            "performance_value": "Agent copying an expert outperformed all other scenarios and found the shortest path much earlier (statistically significant until ≈15.5m time steps). Agent given 8 path primitives achieved similar performance by ≈5m time steps; agent copying an experienced (non-expert) source matched performance of the expert/given-path scenarios by ≈8m time steps. Two agents copying each other improved early but converged to performance similar to no-imitation after ≈5m time steps.",
            "complexity_variation_relationship": "Explicitly discussed: in the high-complexity arena, high-quality demonstrations (expert) strongly accelerate learning; structured variation (a curated set of path primitives) also helps. However, high social variation without a reliable source (mutual copying) reduces long-term benefit because agents reinforce suboptimal behaviours. Thus, complexity increases the value of good demonstrations but also increases the cost of copying poor/variable demonstrations.",
            "high_complexity_low_variation_performance": "Copying a single expert in the 100×100 arena (high complexity, low external variation) produced the best performance: shortest path found much earlier than baseline and other scenarios; advantage statistically significant until ≈15.5m time steps.",
            "low_complexity_high_variation_performance": "Not directly reported as a separate controlled condition; when many non-beneficial paths were available in simpler 10×10 tests, imitation could harm learning unless filtered (see average-Q test results).",
            "high_complexity_high_variation_performance": "Two agents copying each other (both learning) in the 100×100 arena: initial benefit but long-term performance similar to no-imitation, with improvements disappearing after ≈5m time steps; indicates mutual high variation reduces sustained benefit in complex environments.",
            "low_complexity_low_variation_performance": "Agent given a single beneficial path in 10×10 (low complexity, low variation) quickly exploited it and found the shortest path much earlier than baseline.",
            "training_strategy": "Single environment with repeated episodic training; demonstrations injected periodically (every 100 simulation runs); path memory/replacement policy used (evict lowest R_i when full).",
            "generalization_tested": true,
            "generalization_results": "Agents and mechanisms that successfully exploited demonstrations in 10×10 generalized to the more complex 100×100 environment, but performance gains depended strongly on quality of demonstrations and on social configuration (expert vs mutual copying).",
            "sample_efficiency": "Simulation length 100,000 runs per experiment; imitation accelerated time to discover shortest paths by orders of magnitude in favorable conditions (qualitative/time-window descriptions reported; exact numeric path-length/time-to-optimum not tabulated in text).",
            "key_findings": "In a high-complexity spatial task, high-quality demonstrations (expert) and structured primitives substantially improve sample efficiency; social scenarios without a reliable model (mutual copying) deliver only transient gains. Imitation-selection mechanisms (R_i, average-Q tests) are critical to exploit demonstrations without being harmed by non-beneficial ones.",
            "uuid": "e1055.2"
        },
        {
            "name_short": "Q-share (internal-state access)",
            "name_full": "Internal-state-access imitation (copying mentor Q-values / Q-table sharing)",
            "brief_description": "A variant where the observer has full access to the mentor's Q-table (internal state) and selects actions using the mentor's highest-Q suggestion for the observer's current state; yields near-instant learning from an expert but little benefit when observed agent lacks state-specific knowledge.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Internal-state-access agent (Q-table sharing)",
            "agent_description": "Observer reads mentor's Q-values for its current state and repeatedly selects the mentor's argmax action for each state; falls back to its own Q-values when mentor-suggested action conflicts with its own high-Q action. This is equivalent to direct state-specific advice / policy suggestion.",
            "agent_type": "simulated agent (with privileged communication)",
            "environment_name": "100×100 grid and other simulated arenas",
            "environment_description": "Same grid arenas as other simulations; here the difference is that agents can request mentor Q(s,·) values for the observer's current state (telepathy-like internal access).",
            "complexity_measure": "As above: state-space size matches grid size (100×100 → 10,000 states). Complexity arises from requiring mentor to have visited same observer states for suggestions to be useful.",
            "complexity_level": "high (when applied to 100×100) or low for 10×10",
            "variation_measure": "Variation arises from mentor experience level: expert mentor (high-quality Q-table) vs inexperienced mentor (sparse/noisy Q-values). Degree of variation depends on mentor coverage of observer states.",
            "variation_level": "dependent on mentor; can be low (expert) or effectively high (inexperienced/noisy mentor).",
            "performance_metric": "Mean best path length over time; convergence time to optimal policy.",
            "performance_value": "When copying from an expert, the observer became optimal after a very short time interval (near-instant convergence). When two inexperienced agents shared Q-tables from the start, performance was almost identical to standard ε-greedy Q-learning (no statistical difference reported).",
            "complexity_variation_relationship": "Yes: this method is powerful in high-complexity environments only if the mentor has state-specific knowledge (visited states). If the mentor has not visited the observer's states, the suggested actions are essentially random and no benefit accrues; in contrast observed-action imitation allows testing applicability of actions across states and can provide more exploration.",
            "high_complexity_low_variation_performance": "High performance (near-instant optimal) when copying an expert (low variation in advice because mentor is optimal).",
            "low_complexity_high_variation_performance": "Not directly tested, but sharing noisy Q-tables from inexperienced mentors produced no advantage over baseline.",
            "high_complexity_high_variation_performance": "When two inexperienced agents share internal states, performance similar to baseline: internal-state access did not improve learning and lacks the exploration benefits of observed imitation.",
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Direct policy/Q-value transfer (state-specific advice) in addition to own Q-learning updates; no demonstration filtering required because actions are suggested per state.",
            "generalization_tested": true,
            "generalization_results": "Shows limited generalization advantage: works extremely well if mentor is expert (generalizes optimal policy to observer states that mentor has visited), but provides little benefit otherwise. Observed imitation (action sequences) offers different generalization properties by enabling the learner to evaluate the utility of action sequences in multiple states.",
            "sample_efficiency": "Very high sample efficiency when mentor is expert (near-instant), otherwise similar to baseline.",
            "key_findings": "Direct access to another agent's internal Q-values can produce extremely fast learning when the mentor is expert, but offers little or no benefit (and less scope for exploration) when mentors are inexperienced; observed action imitation provides broader exploratory potential because actions are tested in multiple contexts.",
            "uuid": "e1055.3"
        },
        {
            "name_short": "E-puck embodied imitative robots",
            "name_full": "E-puck robots with embodied imitation-enhanced Q-learning (continuous-space SMDP)",
            "brief_description": "Physical e-puck robots running an embodied movement-imitation pipeline (vision-based frame processing → line-seg regression → motor command sequence) combined with SMDP-adapted Q-learning over a 10×10 discretization of each compartment; experiments show embodied imitation increases group learning speed and that copying errors (embodiment noise) can produce beneficial variation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "E-puck robots (embodied imitation-enhanced Q-learning)",
            "agent_description": "Physical robots (e-puck with Linux extension) use a vision-based embodied imitation algorithm (up to 5 fps frame processing, regression to line segments, conversion to motor commands) and an SMDP version of Q-learning (continuous time update per Bradtke & Duff / Crites & Barto style with e^{-βΔt} discounting). Agents discretize their compartment into a 10×10 grid and use imitation-selection mechanisms (R_i / average-Q temporal checks) analogous to simulation.",
            "agent_type": "physical robot (embodied, continuous-space)",
            "environment_name": "3 m × 3 m arena divided into 120 cm × 120 cm compartments; each compartment discretized to 10×10 grid",
            "environment_description": "Continuous 2D physical arena with high-precision external tracking; each robot resides in a separate compartment (120×120 cm) discretized into 10×10 cells for SMDP Q-learning. Robots can observe other robots in adjacent compartments (moved by human intervention to observe) and copy movement patterns for ~30 seconds; copying is purely vision-based (no internal state sharing).",
            "complexity_measure": "Continuous-space SMDP approximated by 10×10 discretization per compartment: 100 states × 8 actions = 800 state-action pairs per robot (inferred). Complexity arises from sensor noise, physical embodiment, non-perfect imitation, and continuous-time dynamics; each robot run involved up to 15,000 moves (a full experimental run).",
            "complexity_level": "medium-high (continuous embodiment + discretization approximation)",
            "variation_measure": "Embodiment-induced variation (copying errors) in perceived demonstrations; number of stored paths limited to 10; demonstrations injected periodically (every 10 tracks up to a cap). Variation measured qualitatively by differences between original and copied paths (example figures show imperfect matches); these variations act like mutations.",
            "variation_level": "high (due to embodied sensor/motor noise producing diverse imperfect copies)",
            "performance_metric": "Mean best path length (shortest path length inferred from current Q-values) averaged over 4 experimental runs; result points spaced every 10 state transitions; confidence intervals reported.",
            "performance_value": "Imitation-enhanced robots (copying an expert) found the shortest path much earlier than robots without imitation (results averaged over 4 runs; exact numeric path-lengths omitted in text). Two robots copying each other showed higher early learning speed than no-imitation on average, but effect size was smaller and more variable; copying errors sometimes produced novel beneficial movement patterns selected by R_i.",
            "complexity_variation_relationship": "Explicitly discussed: embodiment produces variation (copy errors) that can be beneficial (analogy to mutation) because the imitation-selection mechanism can select beneficial variants; thus in embodied continuous environments, variation arising from physical instantiation can improve exploration and solution diversity. However, the benefit depends on the ability to filter/recognize beneficial variants (average-Q temporal checks).",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": "In the embodied continuous setting, high variation (copying errors) sometimes produced novel beneficial patterns which, when selected by the path-filtering mechanism, improved learning; no single aggregate numeric performance reported, but qualitative result: embodied variation can increase emergence of useful behaviours.",
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Single-task SMDP learning in a discretized compartment; demonstrations observed episodically (human-assisted repositioning to observe demonstrator for ~30s) and stored; path-selection and imitation probability adapted via R_i and average-Q temporal tests.",
            "generalization_tested": true,
            "generalization_results": "Mechanisms developed in simulation generalized to real robots: imitation-selection via temporal Q change allowed robots to exploit expert/experienced demonstrations and to avoid useless ones; embodied copying errors introduced new variation exploited by selection, demonstrating that real-world embodiment changes dynamics relative to pure simulation.",
            "sample_efficiency": "Physical experiment runs consisted of 15,000 moves per run and required ~10 hours per run; imitation accelerated discovery of good solutions substantially compared to no-imitation in these time-limited runs (quantitative run-averaged curves shown but raw numeric values not enumerated in text).",
            "key_findings": "Embodied imitation in real robots increases learning speed when beneficial demonstrations are available; sensorimotor copying errors introduce variation analogous to mutation, which can produce novel beneficial movement patterns that are then selected by the imitation-selection mechanism; practical trade-offs include experiment time cost (long physical runs) and the need for demonstration-filtering to avoid copying harmful behaviors.",
            "uuid": "e1055.4"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Apprenticeship learning via inverse reinforcement learning",
            "rating": 2,
            "sanitized_title": "apprenticeship_learning_via_inverse_reinforcement_learning"
        },
        {
            "paper_title": "Accelerating reinforcement learning through implicit imitation",
            "rating": 2,
            "sanitized_title": "accelerating_reinforcement_learning_through_implicit_imitation"
        },
        {
            "paper_title": "Imitative reinforcement learning for soccer playing robots",
            "rating": 2,
            "sanitized_title": "imitative_reinforcement_learning_for_soccer_playing_robots"
        },
        {
            "paper_title": "Learning from observation and practice using behavioral primitives: marble maze",
            "rating": 2,
            "sanitized_title": "learning_from_observation_and_practice_using_behavioral_primitives_marble_maze"
        },
        {
            "paper_title": "Improving elevator performance using reinforcement learning",
            "rating": 2,
            "sanitized_title": "improving_elevator_performance_using_reinforcement_learning"
        },
        {
            "paper_title": "On the emergence of structure in behaviours evolved through embodied imitation in group of robots",
            "rating": 2,
            "sanitized_title": "on_the_emergence_of_structure_in_behaviours_evolved_through_embodied_imitation_in_group_of_robots"
        },
        {
            "paper_title": "Robot programming by demonstration",
            "rating": 1,
            "sanitized_title": "robot_programming_by_demonstration"
        }
    ],
    "cost": 0.020456250000000002,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Embodied Imitation-Enhanced Reinforcement Learning in Multi-Agent Systems</p>
<p>Mehmet D Erbas mehmet.erbas@kemerburgaz.edu.tr 
Istanbul Kemerburgaz University</p>
<p>Alan Ft 
Faculty of Environment and Technology
University of the West of England</p>
<p>Larry Bull 
Faculty of Environment and Technology
University of the West of England</p>
<p>Embodied Imitation-Enhanced Reinforcement Learning in Multi-Agent Systems
C3018D9FD297EFD9B079BCA15034ED9EEmbodied imitationreinforcement Q-learningsocial learningmulti-agent systems
Imitation is an example of social learning in which an individual observes and copies another's actions.This paper presents a new method for using imitation as a way of enhancing the learning speed of individual agents that employ a well-known reinforcement learning algorithm, namely Q-learning.Compared to other research that uses imitation with reinforcement learning, our method uses imitation of purely observed behaviours to enhance learning, with no internal state access or sharing of experiences between agents.The paper evaluates our imitation-enhanced reinforcement learning approach in both simulation and with real robots in continuous space.Both simulation and real robot experimental results show that the learning speed of the group is improved.</p>
<p>Introduction</p>
<p>Social learning, which enables individuals to learn from others in a community, is an important mechanism for social animals.One of the most important types of social learning is imitation as it allows certain skills and behaviours to be transferred between individuals without language or other complex symbolic communication (Nehaniv and Dautenhahn, 2007).Imitation learning differs from other adaptive learning algorithms that have been used in robotic research, including reinforcement learning (Barto et al., 2004), evolutionary algorithms (Nolfi and Floreano, 2000) and supervised learning (Rumelhart et al., 1986), as learning by imitation is based upon social interactions.Another important aspect of imitation is that the only information transferred between agents is the set of observed actions.An agent imitating another may not know anything about the internal state and structure of the other agent.Therefore imitation is different from other types of learning in that supervision can not be used to directly influence internal processes.This paper presents a simple method for linking reinforcement learning with imitation.</p>
<p>The problem of matching the actuators of the observed robot to the robot's own actuators is presented as the correspondence problem (Nehaniv and Dautenhahn, 2002).A solution to the correspondence problem for robot to robot imitation, for our experimental context, is presented in this paper in the following way: The problem can be divided into two separate issues, (1) how to replicate the observed actions and (2) how to find an appropriate context (or states) for which those actions are meaningful.The first problem is solved by programming intrinsically, that is to say if an agent observes that the other agent turns to its left then goes forward for some time, by using the embodied imitation algorithm developed in this paper, it algorithmically determines that in order to replicate those actions, it has to turn to its left and go forward.The second issue of finding the appropriate state (or context) for which the observed actions are meaningful is solved by the learning process of the imitating agent so that it will infer in what state the observed actions are useful through, for instance, a trial and error mechanism.Thus imitated actions that cause an increase in the performance in a state will be more likely to be learned.Similarly, any negative outcome will make the observed actions unlikely to be associated with that state.</p>
<p>Programming by Demonstration (PbD) (Billard et al., 2008), in which a robot is taught new behaviours by humans or other robots, has been widely studied in Robotics research.With the development of humanoid robots, recent PbD research has an increasingly interdisciplinary approach.For example, it has benefited from examining the neural mechanism for imitation in primates (Billard et al., 2008).As humanoid robots are expected to adapt to changing environments, the flexibility of their control system becomes crucial.As a result, the notion of PbD has been progressively replaced by a more biologically inspired label: imitation learning.Some example research that used imitation learning in order to train robots are (Gaussier et al., 1998), (Dillmann, 2004), (Breazeal et al., 2005), (Nicolescu and Mataric, 2007), (Calinon and Billard, 2007) and (Guenter &amp; Billard, 2007).There is some research that attempts to use imitation in conjunction with individual learning.Abbel and Ng (Abbeel and Ng, 2004) used an expert in order to learn to perform a task in which the reward function is not known.They tried to obtain the unknown reward function which is supposed to be implicitly followed in the expert's behaviour.A policy is defined as a mapping from states to probability distributions for actions that are possible in a state so that an agent acts according to its policy.The value of a policy is defined as a linear function of the features of the environment and the learning agent has an estimate of the expert's feature expectations.Their algorithm is proven to attain performance close to that of the expert agent.Latzke et al. (Latzke et al., 2006) utilized Q-learning which uses the experience of an expert agent as training data.The imitating agent has full access to the experience of the expert and these experiences are provided as sequences of states and actions along with their rewards.Price and Boutilier (Price and Boutilier, 2003) devised implicit imitation to accelerate reinforcement learning.In their method, the observer agent obtains an experience tuple from a mentor agent with each mentor transition.The experience tuple is used to train the observer robot.Bentivegna et al. (Bentivegna et al., 2004) used a modified version of Q-learning in order to improve the performance of an agent through practice.Their algorithm stores the actions of a teacher agent in a number of states.During the practice period, the learner agent selects one of the data entries based on the distance between the state recorded in that entry and the current state.The value of selecting that entry in the current state is then updated, based on the reward observed after the action stored in the selected data entry is performed.</p>
<p>The approach presented in this paper is different from those outlined above for two reasons.Firstly, there is no assumption about the learner agent having access to the internal state, experience or expectations of the imitated agent.The only information transferred to the learner agent are the imitated agent's executed actions.The learner agent does not know anything about in what state these actions were executed or what were the outcomes of these actions.Secondly, the case in which the observed actions are not useful at all is also considered; in this case the learner agent must discriminate between useful and useless actions.</p>
<p>This paper presents and adaptive learning algorithm in which imitation is used as a method for enhancing learning of agents that employ a well-known reinforcement learning algorithm, namely Q-learning.Compared to other research that uses imitation with reinforcement learning, our method uses imitation of purely observed behaviours to enhance learning, with no internal state access or sharing of experiences between agents.As in nature, imitation provides agents with model behaviours that can influence their individual learning.Finding in what state (or context) these model behaviours are useful is determined by the learning process of the imitating agent.As developing the algorithm purely on real robots would take a long time, it is firstly tested in simulation, with simulated agents.Once the algorithm is shown to be effective, it is then tested on real robots.Both simulation and real robot experiment results show that the learning speed of the agents is improved.</p>
<p>2 Imitation-Enhanced Reinforcement Learning</p>
<p>Simulation Setup</p>
<p>To examine the effects of imitation on learning strategy, an agent that employs Q-learning in order to reach a target location is simulated.For the first set of experiments, the arena in which the agent operates is a 10 by 10 grid world (figure 1).The agent starts from the top-left corner of the arena and, in each time unit, it moves to one of the eight neighbouring cells.One simulation run ends when the agent reaches the goal item, which is placed in the bottom-right corner of the arena.The learning agent uses an − greedy algorithm in which at each time step it finds the action that has the highest Q value estimate for its current position.With probability 1 − , it chooses that action and with a probability of , it chooses a random action.In this way, it updates the Q value for its current state and chosen action:</p>
<p>(1)
Q(s t , a c ) = Q(s t , a c ) + α[r t+1 + γargmax a Q(s t+1 , a) − Q(s t , a c )]
in which a c is the chosen action, α is the learning ratio, γ is the discount factor and r t+1 is the reward for getting to state s t+1 (α and γ values are set to 0.2 and 0.7 respectively in the experiments).In one experiment run, the agent gets to the goal state 100,000 times and so the Q values for each state-action pair have adequate time to converge to their final values.Every 10 time units, the shortest path to the goal item that is learned by the agent is determined by using a greedy action selection method on the current Q values.</p>
<p>Imitation-Enhanced Reinforcement Learning Algorithm</p>
<p>Only the executed actions are observable and hence copied when an agent imitates another.To simulate this process, for the first experiments a set of actions is given to the learning agent.At each time step, the agent may start to replicate the given set of actions with a probability equal to p imitate .When the agent starts to imitate the path, all actions that constitute the path are executed one by one by the agent.If these actions contradict the Q values of the agent, i.e. have a low value, then the agent stops following the imitated path and acts based on its original − greedy algorithm.Thus the controller of the agent with imitation is updated as shown in Algorithm 1.But how do we select, p imitate , the starting imitation probability?</p>
<p>The rest of the section compares 3 different approaches for selecting p imitate .Approach 1 -Fixed value: With the first approach, p imitate is set to a fixed value.In the first experiment run, the agent is given a path that moves it towards the goal item (figure 2) which is a beneficial path, so that the agent has this path in memory and is able to enact it according to its controller.</p>
<p>Figure 3 shows the performance of imitation enhanced learning agent with different p imitate values.As can be expected, there is a performance increase in the learning speed of the agent compared to a no-imitation agent.The imitation-enhanced agent finds the shortest path much earlier than the agent with Q-learning only.But what would happen if the path given is not beneficial?To test this scenario, the path in figure 4 is given to the agent in the second experiment run. Figure 5 shows the performance of the agent when this path is given, which is clearly not a beneficial path.A pair-wise t-test 1 reveals that there is a statistically significant performance loss for all β values when the path given is not directing the agent towards the goal item.It is clear that there should be an adaptive p imitate calculation which can determine if an observed path is beneficial or not.</p>
<p>Approach 2 -Path completion test: The agent can also check if an imitated path is typically completed.If the imitated path is being abandoned continuously (because it is rejected by the Q values or because of the physical boundaries of the arena) whenever it is started, it may be a sign of a non-beneficial path.So for the second try, the p imitate value is calculated by: n completed = number of times the observed path is completed n replicated = number of times the observed path is replicated 1 A pair-wise t-test checks the hypothesis that, for two data sets S 1 and S 2 , the data in S 1 − S 2 comes from a normal distribution with mean equal to 0 and unknown variance.The same statistical test is applied to experiment results for comparison.
p imitate = β n completed + 1 n replicated + 1 (2)
in which β is a constant that regulates initial imitation probability.Therefore by using this formula, the agent will tend to imitate paths more if they can be completed.Figure 6 shows the performance of the imitation-enhanced learning agent for different β values when the beneficial path in figure 2 is given.Once again there is a clear performance increase in the learning speed of the agent when the imitated path is beneficial.Figure 7 shows the performance of the imitationenhanced agent when the path in figure 4 is given.Although it is slightly better than the case with a fixed p imitate , there is still a statistically significant performance loss for all β values when the path given is not beneficial.Approach 1: Fixed value, performance of the agent when a beneficial path is given.The results are mean best path length from 100 experiment runs along with 95% confidence intervals (note these are shown only at 10 point intervals).The best path for each agent is calculated every 10 time units by using a greedy algorithm on the current Q values.Time is given in time units.
s = GoalState do if actionIndex = 0 then actionIndex ← actionIndex + 1 a ← actionList[actionIndex] if ListCompleted(actionList) = true then actionIndex ← 0 end if else if p imitate &gt; random() then actionIndex ← 1 a ← actionList[actionIndex] else a ← argmax a Q(s, a ) if &gt; random() then a ← SelectRandomAction() end if actionIndex ← 0 end if end if if actionIndex &gt; 0 then if ∃ a : Q(s, a ) &gt; Q(s, a) then a ← argmax a Q(s, a ) if &gt; random() then a ← SelectRandomAction() end if actionIndex ← 0 end if end if Q(s, a) ← Q(s, a) + α[r + argmax a Q(s , a ) − Q(s, a)] s ← s end while
Approach 3 -Average Q value test: Another approach is to examine the effects of imitation on the Q values of the agent.For this purpose, we check if the observed actions cause any change in Q-value of the related state-action pairs.If they do not, this means that the observed actions are causing the agent to explore some parts of the state-action space which have already converged to their final values or the agent is exploring some parts of the state action-space which do not contribute to its overall performance.If the observed actions do cause a change in Q-values then it is possible that imitation .Approach 1: Fixed value, performance of the agent when a nonbeneficial path is given.The results are mean best path length from 100 experiment runs along with 95% confidence intervals (note these are shown only at 10 point intervals).The best path for each agent is calculated every 10 time units by using a greedy algorithm on the current Q values.Time is given in time units.may be beneficial.This is determined as follows: the average Q value is recorded at the start of each imitation sequence and is compared to the average Q value when the imitated path is completed or abandoned.So p imitate is calculated by: n Q+ = number of times the average Q value increased when the observed path is replicated n replicated = number of times the observed path is replicated .Approach 2: Path completion test, performance of the agent when a non-beneficial path is given.The results are mean best path length from 100 experiment runs along with 95% confidence intervals (note these are shown only at point intervals).The best path for each agent is calculated every 10 time units by using a greedy algorithm on the current Q values.Time is given in time units.
p imitate = β n Q+ + 1 n replicated + 1 (3)
Figure 8 shows the performance of the imitation-enhanced agent for different β values when the beneficial path in figure 2 is given.Once again there is a clear performance increase in the learning speed of the agent when the imitated path is beneficial.Figure 9 shows the performance of the imitation-enhanced agent when the path in figure 4 is given.As can be seen, the performance loss due to imitating a non-beneficial path is minimal compared to the previous two tries.A pair-wise t-test reveals that when the β value is set to 0.1, the difference between an imitation-enhanced agent and a no-imitation agent is not statistically significant.For higher β values (0.3 and 0.5) there is a statistically significant performance loss but the difference is much smaller than the case with a fixed p imitate and with the path completion test.Therefore, by checking temporal changes in Q values, it is possible to determine if imitating an observed path is beneficial or not.As the imitation-enhanced agent is able to determine if a path is beneficial or not by checking temporal changes in Q values, when it observes multiple paths, it should be able to select the ones that are useful and avoid others.For this purpose, the ratio of the times the average Q value increased is calculated for each path i as:</p>
<p>n i Q+ = number of times the average Q value changed when path i is replicated n i replicated = number of times path i is replicated
R i = n i Q+ + 1 n i replicated + 1 (4)
R i is used for both regulating the imitation probability of each path observed and choosing which path to imitate.If the agent has n distinct paths, the probability of choosing path i is calculated by: .Approach 3: Average Q value test, performance of the agent when a non-beneficial path is given.The results are mean best path length from 100 experiment runs along with 95% confidence intervals (note these are shown only at 10 point intervals).The best path for each agent is calculated every 10 time units by using a greedy algorithm on the current Q values.Time is given in time units.
P i choose = R i n k=1 R k (5)
The probability of imitating path i once it is chosen is:
p i imitate = βR i (6)
in which β is a constant that regulates the initial imitation probability.The controller of the agent with multiple paths is updated as shown in Algorithm 2. In appendix A, it is shown that the imitation-enhanced Q-learning algorithm does not violate the conditions for the convergence of Q-learning algorithm.</p>
<p>Experiments</p>
<p>In order to test the generalization of the algorithm and to make the problem more complex, the next set of experiments are performed in a 100 by 100 grid with an obstacle in the middle (figure 10).Once again the learning agents start from the top-left corner and try to reach the goal item which is placed in the bottom-right corner.</p>
<p>Imitating Predefined Paths</p>
<p>To check if the algorithm is able to select useful paths and improve the learning speed by exploiting them, the learning agent is given 8 paths.These are chosen to each represent 20 consecutive moves in the eight different directions of the compass, as shown in figure 11.The agent has these paths in its memory and it is able to enact them according to its controller.During the experiment, the agent has the following action sets in its memory:
• P ath 1 : E-E-E-E-E-E....-E (20</p>
<p>-NE</p>
<p>If the corresponding R i values for each path are R 1 , R 2 ,....,R 8 , the probability of selecting each path would be equal to R i /(R 1 +R 2 +R 3 +R 4 +R 5 +R 6 +R 7 +R 8 ).Assume the agent is in state s t and it selects P ath 1 and starts it.If there exists Q(s t , a) &gt; Q(s t , E ), the path is rejected and the agent moves according to its -greedy Q-learning.Otherwise the agent starts to follow P ath 1 , making the same check at each step.Whenever a path is rejected or completed, its R i value is updated.</p>
<p>Figure 12 shows the performance of the imitating agent averaged over 100 experimental runs in comparison with a no-imitation agent.The imitation-enhanced learning agent was able to choose the beneficial paths and improve its performance.A pair-wise t-test reveals that the difference between the two is statistically significant until around time step 15.5m2 .Figure 13 shows an example shortest path in our arena.Any shortest path consists of a combination of 80 moves south-east, 20 moves east and 20 moves south.Figure 14 shows the R i values for each of the 8 paths averaged over 100 runs.As can be seen, the path to the SE has the highest R i value which makes it more likely to be chosen.The paths E and S have relatively higher R i values compared to the other 5 paths.This confirms that the agent was able to select the 3 paths that are more likely to improve its performance.</p>
<p>Agents Imitating Each Other</p>
<p>a) Agent Copying an Expert Agent: An important aspect of imitation is the possibility of a teacher or expert whose experience can be transferred, by imitation, to other individuals.To simulate an expert in the arena, an agent is programmed to follow the shortest path in Figure 13 indefinitely.Then a second agent is presented in the arena that starts learning and is able to watch and copy the expert agent, following the imitation-enhanced Q-learning algorithm as shown in Algorithm 2. The second agent starts with an empty memory and once every 100 simulation runs, it copies a random consecutive portion of the expert's trajectory.Note that although this is a part of an ideal (i.e.shortest) path, the imitating agent does not know anything about in which state (or location in the arena) these actions are meaningful.The imitating agent can store up to 10 paths.After its memory is full, when a new path is copied, of the paths in its memory the one with the lowest R i is erased and the new path is saved.Since each experiment consists of 100,000 simulation runs, the agent will imitate and test 1000 portions in each experiment run. Figure 15 shows the performance of the agent imitating an expert.As can be seen, the imitation-enhanced learning agent finds the shortest path much earlier than the agent with Q-learning only.A pair-wise t-test reveals that the difference between the two is statistically significant until around time step 15.5m.</p>
<p>Algorithm 2 Pseudocode for the controller of the agent with multiple paths</p>
<p>Input: Set Q(s, a) ← 0 for all state-action pairs A set of observed paths pathList ← {P ath 1 , ..., P ath n } pathChosen ← 0, actionIndex ← 0 Repeat 100.000 times Place the agent in the top-left corner
while s = GoalState do if pathChosen = 0 then actionIndex ← actionIndex + 1 a ← pathChosen[actionIndex] if P athCompleted(pathChosen) = true then update R pathChosen pathChosen ← 0 end if else pathChosen ← SelectP robabilistically(P athList) if p pathChosen imitate &gt; random() then actionIndex ← 1 a ← pathChosen[actionIndex] else a ← argmax a Q(s, a ) if &gt; random() then a ← SelectRandomAction() end if pathChosen ← 0 end if end if if pathChosen &gt; 0 then if ∃ a : Q(s, a ) &gt; Q(s, a) then a ← argmax a Q(s, a ) if &gt; random() then a ← SelectRandomAction() end if update R pathChosen pathChosen ← 0 end if end if Q(s, a) ← Q(s, a) + α[r + argmax a Q(s , a ) − Q(s, a)] s ← s end while
b) An Agent Copying an Experienced Agent: In this case, the effect of the existence of a slightly experienced agent instead of an expert is tested.A single agent is trained in the arena for some time (90,000 simulation runs), hence with no-imitation, and a second agent is presented later.The second agent is able to watch and copy the trained agent, following the imitation-enhanced Q-learning algorithm.Note that, even if the experienced agent discovers the best path, it may still make random moves because of its −greedy algorithm.Once again the imitating agent copies a new path every 100 simulation runs and has a memory of 10 action lists.Figure 16 shows the results for this experiment.The agent that is watching an experienced (but not expert) agent has a faster learning rate compared to a no-imitation agent.A pair-wise t-test reveals that the difference between the two is statistically significant until around time step 15.5m.</p>
<p>c) Agents Copying Each Other :</p>
<p>But what would happen if there is no expert agent to share its experience?In order to simulate this scenario, two agents are present in the arena, both starting to learn at the same time and able to watch each other.Once again they store 10 action lists and they copy (i.e.imitate) a new path from each other every 100 simulation runs.Figure 17 shows results for this experiment.As can be seen, the imitating agents have a better performance than a no-imitation agent initially.But the difference between them vanishes after 5m time steps.This is due to the fact that the two imitating agents influence each other throughout the experiment.At the start, the one that has slightly better performance is able to improve the other but as time passes both start to converge to the same performance.After some point, their continuous effect on each other causes less improvement which explains why they get similar performance to a noimitation agent.Pair-wise t-test shows that the imitating agent has a statistically better performance between the time steps 4m and 9.5m.Note also that some runs of the no-imitation The results are mean best path length from 100 experiment runs along with 95% confidence intervals (note these are shown only at 10 point intervals).The best path for each agent is calculated every 1000 time units by using a greedy algorithm on the current Q values.The β value that regulates the initial imitation probability is set to 0.5 for these experiments.Time is given in 100 time units.</p>
<p>agent converge to a shortest path earlier than the imitating agent which makes the no-imitation agent statistically better between the time steps 12.5m and 19.5m, although the difference between the two is very small.Figure 18 compares all scenarios: an agent given 8 paths (figure 12), an agent watching an expert (figure 15), two agents watching each other from the beginning (figure 17) and an agent watching an experienced agent (figure 16).As expected the agent copying an expert outperforms all others although the agent that is given 8 paths achieves the same performance as around 5m time steps.The agent that is watching an experienced (but not expert) agent achieves the same performance with these two after around 8m time steps.All three have significantly better performance than the worst case of two agents copying each other.</p>
<p>Beyond Imitation; Agents Having Access to Each Other's Internal State</p>
<p>Based on the related work discussed in section 1, the perfor- Best path length for imitating (copying an expert) and noimitation agents.The results are mean best path length from 100 experiment runs along with 95% confidence intervals.The best path for each agent is calculated every 1000 time units by using a greedy algorithm on the current Q values.The β value that regulates the initial imitation probability is set to 0.5 for these experiments.Time is given in 100 time units.and no-imitation agents.The results are mean best path length from 100 experiment runs along with 95% confidence intervals.The best path for each agent is calculated every 1000 time units by using a greedy algorithm on the current Q values.The β value that regulates the initial imitation probability is set to 0.5 for these experiments.Time is given in 100 time units.</p>
<p>mance of the agents when they are able to access the internal state of the other agent is tested.In this case the agents have complete access to the Q-table of the other agent, i.e.Q values for all state-action pairs.This essentially means that the agent being observed makes available its estimate of the best action for the current state of the observing agent.Therefore the observing agent, instead of copying actions executed by the other agent from potentially any state, determines a path by and no-imitation agents.The results are mean best path length from 100 experiment runs along with 95% confidence intervals.The best path for each agent is calculated every 1000 time units by using a greedy algorithm on the current Q values.The β value that regulates the initial imitation probability is set to 0.5 for these experiments.Time is given in time units.repeatedly selecting the action with the highest Q value of the other agent in the current state.Based on the previous mechanism, if the suggested action contradicts its own Q values, i.e. it has a lower value, the observing agent stops imitating.</p>
<p>Figure 19 shows the results of this scheme when copying from an expert.As can be seen, the learning agent becomes optimal after a very short time interval.This is perhaps not unexpected, since the scenario is similar to telepathy.But what if there is no expert?Figure 20 shows the results when two agents, starting to learn at the same time, are able to access each other's internal state.It almost has the same performance as the − greedy Q-learning algorithm as a pair-wise t-test reveals that there is no statistical difference between two.</p>
<p>Figure 21 compares two agents copying each other (figure 17) and two agents copying each other with access to each other's internal state (figure 20).As can be seen, the agent with the imitation-enhanced Q-learning algorithm learns more quickly and a pair-wise t-test reveals that the agent utilising imitation is statistically better than the other scenario between time steps 4m to 9.5m.The reason for the difference can be explained as follows: The agent with the state access receives state specific information from the other agent.For this information to be useful, it is required that the observed agent has visited that specific state and have some Q-values assigned to the state-action pairs in that state.If not, the observed agent essentially suggests a random action to the imitating agent.But with observed imitation, the only information that is transferred to the imitating agent is the set of performed actions.Further, the imitating agent is able to test the utility of these sets of actions in different states and determine if they are useful.Thus, it can be deduced that there is more scope for exploration using the observed action sequences.</p>
<p>Imitation-Enhanced Reinforcement Learning in Continuous Space</p>
<p>The most important simplification in the previous section is that the algorithm was tested in simulation.The agents do not possess any physical structure and they interact based on an abstract model of perfect imitation by copying each other's trajectory.In this section, the imitation-enhanced Q-learning algorithm is tested on real robots.The robots use an extended The results are mean best path length from 100 experiment runs along with 95% confidence intervals.The best path for each agent is calculated every 1000 time units by using a greedy algorithm on the current Q values.Time is given in 100 time units.The best path for each agent is calculated every 1000 time units by using a greedy algorithm on the current Q values.Time is given in 100 time units.</p>
<p>version of Q-learning that is adapted for continuous time.The effects of embodied imitation on the learning speed of robots is evaluated in experiments as the robots learn a simple task.</p>
<p>There are a number of mechanisms proposed in order to use reinforcement learning methods in continuous time and space (Santamaria et al., 1998), (Smart and Kaelbling, 2000), (Doya, 2000), (Strosslin and Gerstner, 2006), (Peters and Schaal, 2006).Although we have a continuous-time problem, the learning robot has to make choices only infrequently.Such systems are called semi-Markov Decision Processes (SMDP) (Sutton and Barto, 1998).They can be treated as a Markov process by taking the reward on each discrete transition as an integral of the cumulative reward on the continuous time interval passed before that transition.Bradtke and Duff (Bradtke and Duff, 1995) introduced algorithms, based on reinforcement learning, adopted to the solution of SMDPs.They applied the SMDP version of Q-learning to the problem of determining the optimal control for a queuing system.Using a similar approach, Crites and Barto (Crites and Barto, 1996) applied the SMDP version of Q-learning, with some additional constraints that provide prior knowledge to the system, to an elevator dispatching task.They used a neural network for function approximation to represent the action-value function.Since we wanted to apply the imitation-enhanced Q-learning algorithm developed in previous section to continuous time and space, and our system has the property of SMDP, we used the method that is developed by Crites and Barto, by dividing our arena into 10x10 grids.Assume that the robot is in state s and it selects action a at time t 1 .If the next state transition will happen at time t 2 and the next state will be s , then the update for the Q value for the semi-Markov decision process is calculated by (Crites and Barto, 1996):
(7) Q(s, a) = Q(s, a) + α[ t2 t1 e −β(τ −t1) r τ dτ + e −β(t2−t1) argmax a Q(s , a ) − Q(s, a)]
in which e −β(t2−t1) acts as a variable discount factor, similar to the γ parameter in the discrete time formula, and it depends on the time between state transitions.Each time the robot makes a decision and moves to its next state, the Q value of its state-action pair is updated based on this formula.</p>
<p>Hardware Setup</p>
<p>The robots that are used in the experiments are e-puck miniature robots (Mondada et al., 2009), 7 cm in diameter and 5 cm in height.They are equipped with 2 stepper motors, two wheels of 41 mm diameter, 8 proximity sensors, a CMOS image sensor, an accelerometer, a microphone, a speaker and a ring of coloured LEDs.Their on-board battery provides 3 hours of autonomy.The original e-puck robot lacks the computational power needed for the image processing required for imitation.To overcome this limitation, the robots are enhanced with a Linux extension board (Liu and Winfield, 2011) based on the 32-bit ARM9 micro-controller with the Debian/Linux system installed.The board has a USB extension port, used to connect a wireless network card, and is equipped with a MicroSD card slot.These additions to the standard e-puck robot offer some benefits, including increased processing power and increased memory.The robots are also fitted with coloured 'skirts' to enable them to see each other using their built-in image sensors.The experiments are performed in an arena measuring 3 m x 3 m.A vision-tracking system provides high-precision position tracking and a dedicated swarm server combines the data from the tracking system and the internal data from robots for later analysis.Each robot is also fitted with a tracking 'hat' which provides a matrix of pins holding unique patterns of reflective markers that allow the tracking system to uniquely identify and track each robot (Figure 22).An e-puck with Linux board fitted in between the e-puck motherboard (lower) and the e-puck speaker board(upper).Note both the red skirt, and the yellow hat that provides a matrix of pins for the reflective spheres which allow the tracking system to identify and track each robot.</p>
<p>Movement Imitation Algorithm</p>
<p>In this section, an embodied robot-to-robot movement imitation algorithm is implemented on the Linux extended e-puck robots.Each robot is able to track and copy the other robot's movement patterns.The algorithm completely depends on the visual data coming from the image sensor of the robots; no other type of communication is allowed between the robots.</p>
<p>There are 3 main stages in the imitation algorithm:</p>
<p>• Frame processing: While observing captured visual frames, the observing robot tracks the movement of the demonstrator robot.As stated above, the robots are fitted with coloured skirts; by determining the size and location of the skirt on the demonstrator robot, the observing robot estimates the relative position of the demonstrator and stores this information in a linked list of positions.In this way, up to 5 frames per second are processed.• Data processing: After the demonstrator's movement pattern is completed, the observer robot processes the linked list of positions using a regression line-fitting method to convert the estimated positions into straight line segments.• Pattern replication: The straight line segments and their intersections are converted into a sequence of motor commands (moves and turns).In this way, the observing robot replicates the pattern demonstrated by the demonstrator robot.Further information on the embodied imitation algorithm can be found at (Winfield and Erbas, 2011).</p>
<p>Experimental Setup</p>
<p>To examine the effects of embodied imitation on learning speed, robots that employ imitation-enhanced Q-learning to reach a target location are used.The robot arena is divided into multiple compartments, of size 120 cm x 120 cm and a robot resides in each of them (figure 23). Figure 24 shows two robots in their separate compartments performing individual learning.Similar to the previous section's settings, each compartment is divided into 10x10 grids and a look-up table method is used to store Q values for each state-action pair.The tracking system acts as a GPS server and so it broadcasts the location of each robot over the network.Upon receiving this information, the robots determine their current state and decide on their next action.So the robots use the position information to determine where they themselves are (not to determine the movement pattern of an observed robot).They start from the top-left corner of their compartment and they can move to eight different directions of the compass to get to their next state.An experiment track ends when the robot reaches its target location which is placed in the bottom-right corner of the compartment.One experiment run ends when 15000 moves are enacted by the robot.Compared to a simulation with the same settings, the real robot experiments need a much longer time period as an experimental run requires around 10 hours.The learning robot uses an -greedy algorithm in which, at every state transition, it finds the action that has the highest Q value estimate for its current state.With probability of 1 − , it chooses the action with the highest Q value and, with probability of , it chooses a random action.In this way, it updates the Q value for its current state and selected action using the update formula explained in the previous section.After every 10 moves, the shortest path that is learned by the robot is determined by using a greedy action selection method on the current Q values.The robots are able to watch and copy the actions that are enacted by other robots, using the embodied imitation algorithm presented in the previous section.In accordance with the experimental scenario in the previous section, each time a robot needs to observe another robot, by human intervention, it is physically moved to the bottom-right corner of the compartment of the robot that it plans to imitate.The imitating robot then watches the actions enacted by the observed robot for approximately 30 seconds.Once its observation finishes, the observing robot is returned to its compartment and continues its individual learning.</p>
<p>Experiments</p>
<p>Learner Robot Copying an Expert Robot</p>
<p>In the first set of experiments, the learning speed of a robot, that is able to watch and copy the actions that are executed by an expert robot, using the embodied imitation algorithm presented in section 3.2, is examined.In order to do that, a robot (expert) is programmed to follow the shortest path to the target location indefinitely in its compartment.A second robot (learner) is then placed in another compartment and starts learning using the imitation-enhanced Q-learning method.The pseudo-code of the controller of the learning robot is given in algorithm 3.At the beginning of the experiment, the learning robot moves to the bottom-right corner of the compartment of the expert robot and randomly copies 5 consecutive moves of the expert.It then moves back to its own compartment and starts its individual learning.At the end of every 10 experiment tracks, the learning robot copies a new path of the expert robot3 .The learning robot has a memory of 10 paths and once filled, no more paths are copied.</p>
<p>Figure 25 shows the performance of robots imitating an expert, in comparison with robots without imitation.As can Figure 25.Best path length for Q-learning with imitation (copying an expert) and Q-learning with no-imitation robots.The results are mean best path length from 4 experiment runs along with 95% confidence intervals.The best path for each robot is calculated every 10 state transitions by using a greedy action selection algorithm on the current Q values.The β value that regulates the initial imitation probability is set to 0.5 for these experiments.The distance between two subsequent data points in the plot corresponds to the time needed for 10 state transitions of the robots.</p>
<p>Two Robots Copying Each Other</p>
<p>In the second set of experiments, two robots are placed in separate compartments, both starting to learn at the same time and able to watch each other.At the beginning of the experiment, robots, in turn, are moved to the bottom-right corner of the compartment of the other robot and copy 5 consecutive moves of the other robot.Then, they are moved back to their own compartment and start their individual learning.Once again, at the end of every 10 experiment tracks, the robots copy a new path of the other robot.They have a memory of size 10 paths and once it is filled, no more paths are copied.Figure 26 shows results for this experiment.In accordance with the results from simulation with the same settings (figure 17), the imitating robots have a higher learning speed, on average, as they are able to achieve better solutions earlier than the robots with Q-learning only.However, as expected, the difference between the two cases is smaller compared to the previous set of experiments.A close examination of the results reveals that if one of the robots makes a better start in its individual learning, by chance, then it is able to achieve better solutions sooner than the other robot.As that robot improves its performance, the other robot is able to learn from it by imitation.Therefore, imitation has a positive effect on the collective success of the robots in this scenario.On the other hand, if none of the robots make a good start, imitation does not improve the learning speed of robots as there is nothing to be gained by copying the other robot.The chance effect explains the larger variations in the experimental results.Best path length for Q-learning with imitation (two robots imitating each other) and Q-learning with no-imitation robots.The results are mean best length from 4 experiment runs along with 95% confidence intervals.The best path for each robot is calculated every 10 state transitions by using a greedy action selection algorithm in the current Q values.The β value that regulates the initial imitation probability is set to 0.5 for these experiments.The distance between two subsequent data points in the plot corresponds to the time needed for 10 state transitions of the robots.</p>
<p>During these experiments, the robots are able to choose the paths that are more beneficial to their individual learning, by using the selection method that checks temporal changes in the Q values.Figure 27 shows two example paths that are copied and then enacted the highest number of times during an experimental run.As imitating these paths improves the performance of the robot, they are more likely to be selected and they have the highest R i values.Figure 28 shows two example paths that are copied and then enacted the smallest number of times for the same experimental run.These two paths have the lowest R i values which make them less likely to be selected.</p>
<p>On the Effects of Embodied Imitation</p>
<p>As stated above, the robots use embodied imitation in which they utilise their on-board sensors to watch and copy each other's actions.Copying errors that arise from embodiment provide variation in the imitation which cause copied paths to differ from their originals.Figure 29 shows two example paths that were copied and then enacted the largest number of times during an experimental run, together with the original paths.As can be seen, they are not perfect copies.As the most enacted movement patterns were the result of imperfect imitations (in some experimental runs), it can be deduced that these errors may have a positive effect on the learning of robots.Variations that arise from embodiment may have a functionality somewhat similar to the that of mutations in a genetic algorithm in that they allow different movement patterns to emerge.If these variations are by chance beneficial, they can be selected by the path selection method of the imitation-enhanced Q-learning algorithm, as in fact observed in some experimental runs.The effects of embodied imitation on the emergence of shared behaviours in a group of robots is further explored in (Erbas and Winfield, 2011).</p>
<p>Conclusion and Discussion</p>
<p>This paper has presented a method for using imitation as an enhancing factor to increase the speed of learning for the well known reinforcement learning algorithm, Q-learning.It was shown that information gained by imitation could be used within the Q-learning algorithm.By checking the temporal changes in Q values, it is possible to determine if imitating some observed paths is beneficial or not.The algorithm was first tested in a simple abstract model in simulation and it was shown that the imitation-enhanced Q-learning agent learned faster than the same Q-learner without imitation.It is important to note that, compared to other research that uses imitation with reinforcement learning, our method uses imitation of purely observed behaviours to enhance learning with no internal state access or sharing of experiences between agents.The only information transferred between agents is the imitated agent's executed actions.Finding in what state or context these actions are meaningful is determined by the learning process of the imitating agent.</p>
<p>The algorithm was tested on real robots and it was shown that information gained by imitation could be used within the Q-learning algorithm in a continuous time learning task.Robots were able to exploit imitated actions of an expert or experienced robot to improve their individual learning speed.In the case of two robots copying each other, imitation improved the collective learning speed of the robots given that, generally, one of the robots made a better start than the other.The effects of embodied imitation on learning speed was also examined.It was shown that variations that result from copying errors may allow novel solutions to emerge.If these variations lead to a beneficial movement pattern, it could be selected by the path selection method of the imitation-enhanced learning algorithm.The variations that result from embodied imitation can only be examined if real robots are used instead of simulated agents.Therefore, there is a clear value in examining embodied imitation in real robots rather than relying entirely on simulation.</p>
<p>As it was explained in section 3, applying reinforcement learning methods to tasks in continuous time and space is an open research area and there are many mechanisms for different applications.Since the task our robots solved could be formalised as an SMDP, we used the method that was developed by Crites and Barto by dividing the compartment of each robot into 10x10 grids.This simple solution allowed us to test the imitation-enhanced Q-learning algorithm on real robots.The algorithm can be further extended so that it could be used for tasks in continuous time without dividing the robot arena into grids and so allowing any type of move and turn in continuous time and space.</p>
<p>Figure 1 .
1
Figure 1.Simulated arena.The arena is a 10 by 10 grid world.The agent is placed in the top-left corner and the goal item is placed in the bottom right corner.</p>
<p>Figure 2 .
2
Figure 2. Path given to the agent.It consists of 5 consecutive moves South East.Since by following this path, the agent gets close to the goal item, this path is beneficial for the agent.These moves are shown at relative positions.</p>
<p>Figure 4 .
4
Figure 4. Path given to the agent.It consists of 5 consecutive moves East.Since by following this path, the agent does not get close to the goal item, this path is not beneficial for the agent.These moves are shown at relative positions.</p>
<p>Figure 6 .
6
Figure6.Approach 2: Path completion test, performance of the agent when a beneficial path is given.The results are mean best path length from 100 experiment runs along with 95% confidence intervals (note these are shown only at point intervals).The best path for each agent is calculated every 10 time units by using a greedy algorithm on the current Q values.Time is given in time units.</p>
<p>Figure 8 .
8
Figure8.Approach 3: Average Q value test, performance of the agent when a beneficial path is given.The results are mean best path length from 100 experiment runs along with 95% confidence intervals (note these are shown only at 10 point intervals).The best path for each agent is calculated every 10 time units by using a greedy algorithm on the current Q values.Time is given in time units.</p>
<p>Figure9.Approach 3: Average Q value test, performance of the agent when a non-beneficial path is given.The results are mean best path length from 100 experiment runs along with 95% confidence intervals (note these are shown only at 10 point intervals).The best path for each agent is calculated every 10 time units by using a greedy algorithm on the current Q values.Time is given in time units.</p>
<p>moves) • P ath 2 : SE-SE-SE-SE-SE-SE....-SE • P ath 3 : S-S-S-S-S-S....-S • P ath 4 : SW-SW-SW-SW-SW-SW....-SW • P ath 5 : W-W-W-W-W-W....-W • P ath 6 : NW-NW-NW-NW-NW-NW....-NW • P ath 7 : N-N-N-N-N-N....-N • P ath 8 : NE-NE-NE-NE-NE-NE....</p>
<p>Figure 10 .
10
Figure 10.Arena with obstacle: The arena is a 100 by 100 grid world.The agents are placed in the top-left corner and the goal item is placed in the bottom-right corner.There is an obstacle in the middle of the arena.</p>
<p>Figure 11 .Figure 12 .
1112
Figure 11.8 Paths given to the agent.Each list of actions consist of 20 consecutive moves in one direction: E, SE, S, SW, W, NW, N and NE</p>
<p>Figure 13 .Figure 14 .
1314
Figure 13.An example shortest path to the goal state.Any shortest path consists of a combination of 80 moves to SE, 20 moves to S and 20 moves to E directions.</p>
<p>Figure 15.Best path length for imitating (copying an expert) and noimitation agents.The results are mean best path length from 100 experiment runs along with 95% confidence intervals.The best path for each agent is calculated every 1000 time units by using a greedy algorithm on the current Q values.The β value that regulates the initial imitation probability is set to 0.5 for these experiments.Time is given in 100 time units.</p>
<p>Figure 16 .
16
Figure16.Best path length for imitating (copying an experienced agent) and no-imitation agents.The results are mean best path length from 100 experiment runs along with 95% confidence intervals.The best path for each agent is calculated every 1000 time units by using a greedy algorithm on the current Q values.The β value that regulates the initial imitation probability is set to 0.5 for these experiments.Time is given in 100 time units.</p>
<p>Figure 17 .
17
Figure17.Best path length for imitating (two agents copying each other) and no-imitation agents.The results are mean best path length from 100 experiment runs along with 95% confidence intervals.The best path for each agent is calculated every 1000 time units by using a greedy algorithm on the current Q values.The β value that regulates the initial imitation probability is set to 0.5 for these experiments.Time is given in time units.</p>
<p>Figure 18 .
18
Figure18.Best path length for 4 scenarios.The results are mean best path length from 100 experiment runs along with 95% confidence intervals.The best path for each agent is calculated every 1000 time units by using a greedy algorithm on the current Q values.Time is given in time units.</p>
<p>Figure 19 .
19
Figure19.Best path length for an internal-state-access (accessing an expert agent's Q values) agent and no-imitation/no-internal-state-access agents.The results are mean best path length from 100 experiment runs along with 95% confidence intervals.The best path for each agent is calculated every 1000 time units by using a greedy algorithm on the current Q values.Time is given in 100 time units.</p>
<p>Figure 20 .
20
Figure20.Best path length for an internal-state-access (two agents using each other's Q values) agent and no-imitation/no-internal-state-access agents.The results are mean best path length from 100 experiment runs along with 95% confidence intervals.The best path for each agent is calculated every 1000 time units by using a greedy algorithm on the current Q values.Time is given in 100 time units.</p>
<p>Figure 21 .
21
Figure21.Best path length for an agent accessing another agent's Q values and imitating (two agents copying each other) agent.The results are mean best path length from 100 experiment runs along with 95% confidence intervals.The best path for each agent is calculated every 1000 time units by using a greedy algorithm on the current Q values.Time is given in 100 time units.</p>
<p>Figure22.An e-puck with Linux board fitted in between the e-puck motherboard (lower) and the e-puck speaker board(upper).Note both the red skirt, and the yellow hat that provides a matrix of pins for the reflective spheres which allow the tracking system to identify and track each robot.</p>
<p>Figure 23 .
23
Figure 23.The robot arena is divided into multiple compartments and a robot resides in each of them.The figure on the left shows two robots, robot A and robot B, in their own compartments, performing their individual learning.In the figure on the right, robot B is watching and copying actions performed by robot A.</p>
<p>Figure 24 .
24
Figure 24.Two robots performing individual learning in their separate compartments.</p>
<p>Figure 26.Best path length for Q-learning with imitation (two robots imitating each other) and Q-learning with no-imitation robots.The results are mean best length from 4 experiment runs along with 95% confidence intervals.The best path for each robot is calculated every 10 state transitions by using a greedy action selection algorithm in the current Q values.The β value that regulates the initial imitation probability is set to 0.5 for these experiments.The distance between two subsequent data points in the plot corresponds to the time needed for 10 state transitions of the robots.</p>
<p>Figure 27 .
27
Figure 27.Two paths that are copied and then enacted highest number of times during an experimental run.</p>
<p>Figure 28 .
28
Figure 28.Two paths that are copied and then enacted lowest number of times during an experimental run.</p>
<p>Figure 29 .
29
Figure 29.Two example movement patterns that were copied and then enacted the largest number of times.The shape on the left shows the original movement pattern of the observed robot and the figure on the right shows its perceived copy by the imitating robot.</p>
<p>15.5m means 15.500.000 time steps
The number of consecutive moves to be copied and how often a new path will be copied is chosen to be proportional to the simulation settings in section 2.3.
AcknowledgementsThis work was supported by EPSRC research grant: EP/E062083/1.The authors are grateful for the very helpful comments of the anonymous reviewers.Algorithm 3 Pseudocode for the controller of the robot Input: Set Q(s, a) ← 0 for all state-action pairs pathList ← ∅, pathChosen ← 0, actionIndex ← 0, trackCounter ← 0 moveCounter ← 0, s ← ∅ while moveCounter &lt; 15000 do if trackCounter%10 = 0 &amp;&amp; trackCounter &lt; 100 then newP ath ← CopyN ewP ath(); P athList ← AddN ewP ath(newP ath);s ← GetState() moveCounter ← moveCounter + 1 end while end while be seen, the imitation-enhanced robot finds the shortest path much earlier than the robot with Q-learning only.During experiments, copying errors occasionally occur but the learning robot is able to exploit the information coming from the expert as the robots are able to discriminate between useful and useless observed behaviours.The effect of copying errors is further discussed in section 3.5.Assume that we have a Markov decision process that is denoted by the tuple (S,A,P,R) in which S is the finite set of possible states, A is the finite set of possible actions, P is the transition probabilities, and R is the reward function.Q-learning converges to optimal Q function ?iffor all s ∈ S and for all a ∈ A, in which α is the learning rate.Since 0 ≤ α t (s, a) &lt; 1, all state-action pairs should be visited infinitely often.We will prove that this prerequisite is not validated in imitation-enhanced Q-learning algorithm.Assume that the observed path i constitutes policy π i and π i (0) = a 0 i (the first action given by the policy π i ).For each s ∈ S,which gives that if p start i &lt; 1, all state-action pairs in this state will be visited.if argmax a Q(s, a) &gt; 0,then if ∃a such that Q(s, a ) &gt; Q(s, a 0 i ), then the a 0 i is rejected and the agent acts in its -greedy algorithm.if) which gives that if p start i &lt; 1, all state-action pairs will be visited.Therefore, the actions that are imitated are reinforced by the algorithm but it does not cause any of the stateaction pairs not to be visited if the initial imitation starting probability, p imitate , is smaller than 1.
Apprenticeship learning via inverse reinforcement learning. P Abbeel, A Y Ng, Proceedings of ICML 2004. ICML 20042004</p>
<p>Learning to act using real-time dynacim programming. A G Barto, S J Bradtke, S P Singh, Artificial Intelligence. 62004</p>
<p>D C Bentivegna, C G Atkeson, G Cheng, Learning from observation and practice using behavioral primitives: marble maze. Robotics. 2004</p>
<p>Robot programming by demonstration. A Billard, S Calinon, R Dillmann, S Schaal, 2008Springer Handbook of Robotics</p>
<p>Reinforcement learning methods for continuous-time markov decision problems. S J Bradtke, M O Duff, Advances in Neural Information Processing Systems: Proceedings of the 1994 Conference. G Tesauro, D Touretzky, T Leen, Cambridge, MAMIT Press1995</p>
<p>Learning from and about others: Towards using imitation to bootstrap the social understanding of others by robots. C Breazeal, D Buchsbaum, J Gray, D Gatenby, B Blumberg, Artificial Life. 111-22005</p>
<p>Incremental learning of gestures by imitation in a humanoid robot. S Calinon, A Billard, Proceedings of ACM/IEEE International Conference on Human-Robot Interaction. ACM/IEEE International Conference on Human-Robot Interaction2007</p>
<p>Improving elevator performance using reinforcement learning. R H Crites, A G Barto, Proceedings of the Neural Information Processing Systems. D Touretzky, M Mozer, M Hasselmo, the Neural Information Processing Systems19968</p>
<p>Teaching and learning of robot tasks via observation of human performance. R Dillmann, Journal of Robotics and Autonomous Systems. 472-32004</p>
<p>Reinforcement learning in continuous time and space. K Doya, Neural Computation. 122000</p>
<p>On the emergence of structure in behaviours evolved through embodied imitation in group of robots. M D Erbas, A F T Winfield, Proceedings of the Eleventh European Conference on the Synthesis and Simulation of Living Systems (ECAL 2011). the Eleventh European Conference on the Synthesis and Simulation of Living Systems (ECAL 2011)2011</p>
<p>From perception-action loop to imitation process: a bottomup approach of learning by imitation. P Gaussier, S Moga, J P Banquet, M Quoy, Applied Artificial Intelligence. 711998</p>
<p>Imitative reinforcement learning for soccer playin robots. T Latzke, S Behnke, M Benewitz, Proceedings of 10th International Robocup Symposium. 10th International Robocup Symposium2006</p>
<p>Open-hardware epuck linux extension board for experimental swarm robotics research. W Liu, A F T Winfield, Microprocessors and Microsystems. 3512011</p>
<p>The e-puck, a robot designed for education in engineering. F Mondada, M Bonani, X Raemy, J Pugh, C Cianci, A Klaptocz, S Magnenat, J C Zufferey, D Floreano, A Martinoli, 9th Conference on Autonomous Robot Systems and Competitions. 200965</p>
<p>The correspondence problem. C L Nehaniv, K Dautenhahn, Imitation in Animals amd Artifacts. C L Nehaniv, K Dautenhahn, MIT Press2002</p>
<p>Imitation and Social Learning in Robots, Humans and Animals. C L Nehaniv, K Dautenhahn, 2007Cambridge University Press</p>
<p>Task learning through imitation and human-robot interaction. M Nicolescu, M J Mataric, Imitation and Social Learning in Robots, Humans and Animals. C L Nehaniv, K Dautenhahn, Cambridge University Press2007</p>
<p>Evolutionary Robotics. S Nolfi, D Floreano, 2000MIT Press</p>
<p>Policy gradient methods for robotics. J Peters, S Schaal, Proceedings of the International Conference on Artificial Neural Networks. the International Conference on Artificial Neural Networks2006</p>
<p>Accelerating reinforcement learning through implicit imitation. B Price, C Boutilier, Artificial Intelligence Research. 192003</p>
<p>Learning representation by back-propagation. D E Rumelhart, G E Hinton, R J William, Nature. 3231986</p>
<p>Experiments with reinforcement learning in problems with continuous state and action spaces. J C Santamaria, R S Sutton, A Ram, Adaptive Behaviour. 621998</p>
<p>Practical reinforcement learning in continuous spaces. W D Smart, L P Kaelbling, Proceedings of the 17th International Conference on Machine Learning. the 17th International Conference on Machine Learning2000</p>
<p>Reinforcement learning in continuous state and action space. T Strosslin, W Gerstner, Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems. the IEEE/RSJ International Conference on Intelligent Robots and Systems2006</p>
<p>Reinforcement Learning. R S Sutton, G B Barto, 1998MIT Press</p>
<p>On embodied memetic evolution and the emergence of behavioural traditions. A F T Winfield, M D Erbas, Memetic Computing. 342011</p>            </div>
        </div>

    </div>
</body>
</html>
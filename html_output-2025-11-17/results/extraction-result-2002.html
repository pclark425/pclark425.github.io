<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2002 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2002</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2002</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-46.html">extraction-schema-46</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that compare learned operators (LLM-based, neural, or data-driven) with traditional genetic programming operators, including performance comparisons, training data effects, generalization, computational costs, and hybrid approaches.</div>
                <p><strong>Paper ID:</strong> paper-281525825</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2509.20529v1.pdf" target="_blank">MDBench: Benchmarking Data-Driven Methods for Model Discovery</a></p>
                <p><strong>Paper Abstract:</strong> Model discovery aims to uncover governing differential equations of dynamical systems directly from experimental data. Benchmarking such methods is essential for tracking progress and understanding trade-offs in the field. While prior efforts have focused mostly on identifying single equations, typically framed as symbolic regression, there remains a lack of comprehensive benchmarks for discovering dynamical models. To address this, we introduce MDBench, an open-source benchmarking framework for evaluating model discovery methods on dynamical systems. MDBench assesses 12 algorithms on 14 partial differential equations (PDEs) and 63 ordinary differential equations (ODEs) under varying levels of noise. Evaluation metrics include derivative prediction accuracy, model complexity, and equation fidelity. We also introduce seven challenging PDE systems from fluid dynamics and thermodynamics, revealing key limitations in current methods. Our findings illustrate that linear methods and genetic programming methods achieve the lowest prediction error for PDEs and ODEs, respectively. Moreover, linear models are in general more robust against noise. MDBench accelerates the advancement of model discovery methods by offering a rigorous, extensible benchmarking framework and a rich, diverse collection of dynamical system datasets, enabling systematic evaluation, comparison, and improvement of equation accuracy and robustness.</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2002.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2002.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that compare learned operators (LLM-based, neural, or data-driven) with traditional genetic programming operators, including performance comparisons, training data effects, generalization, computational costs, and hybrid approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ODEFormer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ODEFormer: Symbolic Regression of Dynamical Systems with Transformers</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer-based large-scale pretrained (LSPT) model that generates full differential equations (including constants) from state trajectories without relying on explicit derivative estimates; designed to operate on dynamical-system data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ODEFormer: Symbolic Regression of Dynamical Systems with Transformers</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ODEFormer</td>
                        </tr>
                        <tr>
                            <td><strong>operator_type</strong></td>
                            <td>LSPT (Transformer) / neural network</td>
                        </tr>
                        <tr>
                            <td><strong>operator_description</strong></td>
                            <td>A Transformer encoder-decoder model pretrained in the LSPT paradigm on synthetic symbolic regression problems and extended to dynamical systems; it consumes state trajectories and timestamps and autoregressively generates symbolic differential equations (including constants). Does not require precomputed derivatives.</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Pretrained on a large corpus of synthetic symbolic-regression problems (text reports 'LSPT methods pretrain on large corpus of symbolic regression problems'); ODEFormer specifically was extended/trained for dynamical systems (single observed trajectory examples) per d'Ascoli et al. 2024. Exact dataset sizes and diversity are not provided in MDBench.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_benchmark</strong></td>
                            <td>Model discovery / symbolic regression for dynamical systems (evaluated on MDBench ODEBench and selected PDE datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against GP methods (PySR, Operon), linear models (SINDy, ESINDy, WSINDy), hybrid methods (uDSR), and other neural methods (EQL, DeepMoD, End2end).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_learned_operator</strong></td>
                            <td>Qualitative: achieves good predictive accuracy on time derivatives for ODEs in low-noise settings (SNR ≥ 30 dB) and clean data; generalization degrades rapidly as noise increases (noted drop at SNR ≤ 20 dB). (No exact NMSE numeric provided in main text for ODEFormer.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_traditional_operator</strong></td>
                            <td>GP methods (e.g., PySR) generally achieved lower NMSE than many other methods in low-noise ODE settings (SNR ≥ 30 dB); linear methods (SINDy/ESINDy) produced simpler but sometimes less accurate models. (Paper reports statistical tests: PySR significantly lower NMSE than ESINDy: Wilcoxon W=9774, p≤0.0001.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_hybrid_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validity_or_executability_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_or_diversity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Discussed: ODEFormer generalizes well on low-noise, but performance degrades with noise—suggested overfitting due to many parameters; no formal OOD test other than noise/clean splits.</td>
                        </tr>
                        <tr>
                            <td><strong>training_bias_evidence</strong></td>
                            <td>Yes — evidence of sensitivity to training distribution and noise: ODEFormer performs well when data are clean/low-noise but overfits and degrades under higher noise (SNR ≤ 20 dB), likely due to large parameter count and training regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_comparison</strong></td>
                            <td>Qualitative: Transformer-based methods like ODEFormer have many trainable parameters and long training times compared to some linear/G P baselines; specific timing per method shown in supplementary figures but no single numeric provided for ODEFormer in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td>Not reported beyond the LSPT pretraining paradigm; authors note potential to extend pretrained models jointly on synthetic ODE and PDE systems as future work.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_vs_general_pretraining</strong></td>
                            <td>Discussed qualitatively: LSPT models are fast at inference after pretraining but may perform poorly on real-world systems whose structure differs from synthetic pretraining data. ODEFormer was trained/extended for dynamical systems and performs better than End2end which lacked dynamical examples in pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_space_characterization</strong></td>
                            <td>Not quantified for ODEFormer specifically; paper contrasts LSPT learned generation (structured by pretraining) vs GP's unstructured search space and LM constrained linear form.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_during_evolution</strong></td>
                            <td>Not applicable (no online adaptation reported); ODEFormer is a pretrained inference model rather than an evolving operator within GP loops.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Overfitting and rapid performance degradation under moderate-to-high measurement noise (SNR ≤ 20 dB); increases in discovered-equation complexity with noise indicating memorization/poor denoising.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_for_theory</strong></td>
                            <td>Transformer LSPT models (like ODEFormer) can produce accurate symbolic differential equations quickly at inference time for clean/low-noise trajectories, but they are sensitive to noise and can overfit due to large parameter counts; their generalization depends strongly on whether pretraining included representative dynamical-system examples.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2002.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2002.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that compare learned operators (LLM-based, neural, or data-driven) with traditional genetic programming operators, including performance comparisons, training data effects, generalization, computational costs, and hybrid approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>End2end</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>End-to-end symbolic regression with transformers</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Transformer sequence-to-sequence LSPT model that maps input data to symbolic expressions, pretrained on large synthetic equation corpora; used as an off-the-shelf symbolic regression model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>End-to-end symbolic regression with transformers</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>End2end</td>
                        </tr>
                        <tr>
                            <td><strong>operator_type</strong></td>
                            <td>LSPT (Transformer) / neural network</td>
                        </tr>
                        <tr>
                            <td><strong>operator_description</strong></td>
                            <td>Sequence-to-sequence Transformer pretrained on synthetic symbolic equations to directly generate symbolic expressions (constants included); two-stage or end-to-end variants exist in the literature.</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Pretrained on large synthetic equation datasets; MDBench notes End2end's pretraining did not include dynamical-system examples (no ODE/PDE examples), restricting generalization to model discovery tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_benchmark</strong></td>
                            <td>General-purpose symbolic regression; evaluated on MDBench ODE and PDE tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared qualitatively against GP methods (PySR, Operon), linear methods (SINDy family), transformer LSPT extensions (ODEFormer), and neural methods (EQL, DeepMoD).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_learned_operator</strong></td>
                            <td>Qualitative: exhibited the highest predictive errors among evaluated methods on ODE datasets; performed poorly on dynamical systems tasks in MDBench (Wilcoxon test with EQL: W=12795, p<10^-10 reported vs EQL).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_traditional_operator</strong></td>
                            <td>GP methods and some linear methods outperformed End2end on dynamical-system discovery tasks; GP methods achieved lower NMSE in low-noise ODE settings.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_hybrid_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validity_or_executability_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_or_diversity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Poor: End2end struggled to generalize to dynamical systems because its pretraining lacked such examples.</td>
                        </tr>
                        <tr>
                            <td><strong>training_bias_evidence</strong></td>
                            <td>Explicit: failure attributed to absence of dynamical-system examples in pretraining corpus, indicating bias toward the synthetic equation patterns it saw during training.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_comparison</strong></td>
                            <td>Requires GPU for training/inference; MDBench excluded End2end from many PDE evaluations due to failures and limitations in handling high-dimensional inputs; runtime comparisons not numerically broken down beyond 'long' for DL methods.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td>Not evaluated in MDBench; authors suggest including dynamical systems in pretraining would improve generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_vs_general_pretraining</strong></td>
                            <td>Clear example: general-purpose pretraining (no dynamical systems) resulted in poor performance on ODE/PDE model discovery tasks compared to a domain-specialized LSPT (ODEFormer).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_space_characterization</strong></td>
                            <td>Not explicitly characterized; failure modes suggest the model's learned distribution of expressions did not cover dynamical-system equation structures.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_during_evolution</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>High predictive error on dynamical systems; inability to handle high-dimensional PDE inputs; poor OOD generalization from synthetic training corpus lacking dynamical examples.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_for_theory</strong></td>
                            <td>Large pretrained transformer SR models can be fast at inference but fail to generalize to domains not represented in pretraining (dynamical systems), leading to worse performance than GP and linear baselines on MDBench tasks.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2002.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2002.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that compare learned operators (LLM-based, neural, or data-driven) with traditional genetic programming operators, including performance comparisons, training data effects, generalization, computational costs, and hybrid approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>uDSR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>uDSR / unified Deep Symbolic Regression</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A unified deep symbolic regression framework that integrates multiple approaches (GP, linear models, optimization) as inner loops to improve training signal and discovery performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A unified framework for deep symbolic regression</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>uDSR</td>
                        </tr>
                        <tr>
                            <td><strong>operator_type</strong></td>
                            <td>hybrid (neural + GP + LM + optimization)</td>
                        </tr>
                        <tr>
                            <td><strong>operator_description</strong></td>
                            <td>A deep RL / neural approach that incorporates GP seeding, linear-model fitting, and post-hoc optimization as inner loops to strengthen learning signals and improve discovery; effectively a hybrid pipeline combining learned and traditional operators.</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Trained using synthetic SR benchmarks (e.g., SRBench) and integrated components; in MDBench uDSR was run but it required subsampling on large PDE datasets (limited to 10,000 points) due to memory/runtime limits.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_benchmark</strong></td>
                            <td>Symbolic regression / model discovery evaluated on SRBench historically and MDBench ODE/PDE datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against standalone GP (PySR, Operon), linear methods (SINDy family), LSPT (End2end, ODEFormer), and DL methods (EQL, DeepMoD).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_learned_operator</strong></td>
                            <td>Mixed: uDSR achieved state-of-the-art on SRBench historically (as claimed in literature), but in MDBench uDSR produced inaccurate equations in some noise-free PDE/ODE settings and was among slower DL methods; on PDE datasets uDSR often runs fast but accuracy varied (paper notes uDSR tends to generate most inaccurate equations in noise-free PDE setting).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_traditional_operator</strong></td>
                            <td>GP and LM methods often outperformed uDSR on many MDBench PDE/ODE tasks; specific statistical comparisons: LM methods (WSINDy) obtained significantly lower error than PySR and Operon in PDE low-noise settings (Wilcoxon p<10^-4).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_hybrid_operator</strong></td>
                            <td>uDSR itself is a hybrid; MDBench does not report a separate hybrid-vs-traditional numeric comparison beyond qualitative performance notes and runtime behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>validity_or_executability_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_or_diversity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not specifically quantified; uDSR's reliance on multiple inner loops can help with exploration but MDBench observed inaccuracies on some PDEs indicating limited generalization to complex real-world PDEs.</td>
                        </tr>
                        <tr>
                            <td><strong>training_bias_evidence</strong></td>
                            <td>Implicit: requires subsampling for large datasets, which may bias learned components toward subsampled patterns; paper notes uDSR can be slow and inaccurate under some conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_comparison</strong></td>
                            <td>Computationally costly: uDSR is among the slowest DL methods on ODE datasets (orders of magnitude slower than SINDy); on PDEs it is reported among faster runtimes for some datasets but accuracy trade-offs exist. MDBench notes uDSR required subsampling (10k points) due to memory/runtime.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td>Not evaluated in MDBench beyond claim of SRBench performance; no cross-domain transfer experiments reported.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_vs_general_pretraining</strong></td>
                            <td>Not directly compared; uDSR leverages RL and hybrid components rather than large-scale pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_space_characterization</strong></td>
                            <td>Not explicitly characterized; MDBench notes uDSR and GP expand search space and can struggle on very large function libraries (e.g., complex PDEs with forcing).</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_during_evolution</strong></td>
                            <td>Yes — uDSR integrates optimization/GP/LM inner loops which can be considered as adaptive components during training; MDBench notes such hybrid integration is intended to improve learning signals.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>In MDBench uDSR produced particularly inaccurate equations on some noise-free PDE settings and needed aggressive subsampling on large datasets; overall sensitivity to dataset size and noise, and long runtimes, are limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_for_theory</strong></td>
                            <td>Hybrid approaches that combine learned neural controllers with GP and LM inner loops can achieve strong performance on synthetic SR benchmarks but may struggle on complex, high-dimensional, or noisy dynamical-system discovery tasks; computational costs and data subsampling are practical bottlenecks limiting their empirical advantage on MDBench.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2002.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2002.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that compare learned operators (LLM-based, neural, or data-driven) with traditional genetic programming operators, including performance comparisons, training data effects, generalization, computational costs, and hybrid approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PySR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PySR (Parallel Genetic Programming for Symbolic Regression)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A high-performance genetic programming (GP) system implemented in Julia using parallel population-based evolve-simplify-optimize loops for symbolic regression and model discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Interpretable machine learning for science with PySR and SymbolicRegression</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>PySR</td>
                        </tr>
                        <tr>
                            <td><strong>operator_type</strong></td>
                            <td>traditional GP (population-based evolutionary operators)</td>
                        </tr>
                        <tr>
                            <td><strong>operator_description</strong></td>
                            <td>Genetic programming that evolves expression trees using mutation, crossover, and simplification, with parallelized evaluation implemented in Julia; does not learn operators in a data-driven way by default (search heuristics standard, though can be hybridized externally).</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_benchmark</strong></td>
                            <td>Symbolic regression / model discovery for ODEs and PDEs (evaluated on MDBench ODEBench and various PDE datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against linear methods (SINDy family), other GP (Operon), LSPT models (End2end, ODEFormer), neural methods (EQL, DeepMoD), and hybrids (uDSR).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_learned_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_traditional_operator</strong></td>
                            <td>GP performance: On ODEs in low-noise/clean settings (SNR ≥ 30 dB), GP methods (including PySR) generally achieved the lowest NMSE for predicted time derivatives; however, GP methods overfit in noisy settings producing highly complex and inaccurate expressions. Statistical result: PySR has significantly lower NMSE than ESINDy (Wilcoxon W=9774, p≤0.0001).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_hybrid_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validity_or_executability_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_or_diversity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not explicitly quantified; GP shows strong recovery on clean/synthetic systems but is sensitive to noise and can produce spurious complex terms under noise.</td>
                        </tr>
                        <tr>
                            <td><strong>training_bias_evidence</strong></td>
                            <td>Not applicable (traditional GP not pretrained), but GP's behavior reflects sensitivity to noise and search-space assumptions rather than pretraining bias.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_comparison</strong></td>
                            <td>PySR is generally slower than linear methods on 1D PDE problems but scales more favorably to higher dimensions; paper reports PySR slows by ~33× in 2D and ~12× in 3D (relative to its 1D runtime), whereas PDEFind slows by ~40× and ~465× for 2D and 3D respectively. On ODEs, PySR is slower than SINDy but competitive.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_vs_general_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_space_characterization</strong></td>
                            <td>GP methods operate over a large, flexible, unstructured search space (expression trees) which enables accurate recovery in clean settings but makes them sensitive to noise and prone to overfitting.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_during_evolution</strong></td>
                            <td>Standard evolutionary adaptation (mutation, crossover) but no learned operator adaptation unless extended with learned heuristics (see neural-guided GP / LLM crossover mentions).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Overfitting under noisy data (producing complex inaccurate expressions) and long runtimes on some low-dimensional PDEs; implementation and runtime limits can cause timeouts on very large datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_for_theory</strong></td>
                            <td>Traditional GP excels at recovering accurate symbolic models on clean, low-noise ODEs and scales comparatively better than some linear methods to higher spatial dimensions in PDEs, but lacks explicit denoising and regularization making it sensitive to noise and prone to complex overfitted expressions.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2002.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2002.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that compare learned operators (LLM-based, neural, or data-driven) with traditional genetic programming operators, including performance comparisons, training data effects, generalization, computational costs, and hybrid approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Operon</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Operon (C++ Genetic Programming Framework)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An efficient C++ genetic programming implementation using vectorized operations and compact linear-tree representations for symbolic regression.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Operon C++ an efficient genetic programming framework for symbolic regression</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Operon</td>
                        </tr>
                        <tr>
                            <td><strong>operator_type</strong></td>
                            <td>traditional GP (population-based evolutionary operators)</td>
                        </tr>
                        <tr>
                            <td><strong>operator_description</strong></td>
                            <td>Genetic programming system implemented in C++ emphasizing vectorized evaluation and compact tree encodings to accelerate symbolic regression search; uses standard mutation and crossover operators.</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_benchmark</strong></td>
                            <td>Symbolic regression / model discovery on MDBench ODE and PDE datasets</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared with PySR (GP), SINDy family (LM), WSINDy, DeepMoD, uDSR, ODEFormer, End2end, EQL, Bayesian methods.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_learned_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_traditional_operator</strong></td>
                            <td>Operon (GP) performed competitively on clean/low-noise ODEs and PDEs; like PySR, it shows reduced predictive fidelity as noise increases and can produce complex overfit equations under noise. Statistical tests in paper reference PySR and Operon performing worse than WSINDy on PDEs in low-noise settings (WSINDy significantly lower NMSE than PySR and Operon, p<10^-4).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_hybrid_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validity_or_executability_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_or_diversity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not quantified; similar behavior to other GP methods: good in clean data, sensitive to noise.</td>
                        </tr>
                        <tr>
                            <td><strong>training_bias_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_comparison</strong></td>
                            <td>Operon is computationally efficient among GP methods; on ODEs SINDy and Operon are the most computationally efficient overall. However GP methods can be slower than LM on low-dimensional PDEs, but scale better to higher dimensions compared to LM (Operon follows this trend).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_vs_general_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_space_characterization</strong></td>
                            <td>Operon explores large expression-tree spaces; MDBench notes such unstructured spaces can hurt convergence and make methods sensitive to noise.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_during_evolution</strong></td>
                            <td>Standard GP evolutionary adaptation; no learned adaptation reported in MDBench for Operon.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Overfitting in noisy regimes, producing complex inaccurate expressions; failures/timeouts on some high-dimensional PDEs depending on implementation constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_for_theory</strong></td>
                            <td>Efficient GP implementations (Operon) can be competitive in accuracy and runtime across datasets, but GP's susceptibility to noise and lack of built-in denoising regularization remain key limitations compared to linear methods like WSINDy.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2002.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2002.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that compare learned operators (LLM-based, neural, or data-driven) with traditional genetic programming operators, including performance comparisons, training data effects, generalization, computational costs, and hybrid approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-as-GP-operator</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based crossover/mutation operators for GP (LLM-SR / language-model crossover)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of large language models (LLMs) to implement GP variation operators (crossover, mutation) via few-shot prompting or learned code-generation to accelerate GP convergence and introduce learned variation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>LLM-SR: Scientific Equation Discovery via Programming with Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLM-based GP operators (LLM-SR, Language-model crossover)</td>
                        </tr>
                        <tr>
                            <td><strong>operator_type</strong></td>
                            <td>LLM-based learned operators (few-shot / prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>operator_description</strong></td>
                            <td>LLMs are used as black-box operators to propose offspring expressions (crossover/mutation) via few-shot prompting or program synthesis; these learned operators replace or augment hand-designed subtree crossover/mutation to produce higher-quality variation and faster convergence.</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>LLMs are pretrained on very large corpora (code, text, symbolic math) external to these experiments; specific fine-tuning for SR is described in referenced works (Shojaee et al. 2025, Meyerson et al. 2024) rather than MDBench itself.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_benchmark</strong></td>
                            <td>Symbolic regression and genetic programming (discussed in related work); not directly evaluated as an operator inside MDBench experiments but cited as recent advances.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared conceptually against traditional GP operators (subtree crossover, subtree mutation) and neural-guided search heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_learned_operator</strong></td>
                            <td>Reported in referenced works to accelerate GP convergence and improve variation quality; MDBench only cites these works and does not provide new numeric evaluations in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_traditional_operator</strong></td>
                            <td>Traditional GP uses hand-designed subtree crossover/mutation; MDBench notes GP performance variability and convergence issues in large, unstructured spaces without learned heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_hybrid_operator</strong></td>
                            <td>Referenced works (Shojaee et al., Meyerson et al.) propose hybrids where LLM operators are embedded in GP pipelines and show empirical acceleration of GP search in their own studies (no MDBench numeric replication).</td>
                        </tr>
                        <tr>
                            <td><strong>validity_or_executability_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_or_diversity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not discussed in MDBench specifically; implied risk that LLM operators may bias towards patterns present in LLM pretraining data.</td>
                        </tr>
                        <tr>
                            <td><strong>training_bias_evidence</strong></td>
                            <td>MDBench references these methods as recent innovations; it highlights general LSPT/Learned-operator risk: learned operators can perform poorly on systems whose structure differs from the pretraining distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_comparison</strong></td>
                            <td>Not quantified in MDBench; LLM-based operators add inference cost per operator proposal but can reduce overall GP iterations by better proposals—tradeoff depends on LLM size and prompt/inference costs (not specified here).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td>Not evaluated in MDBench.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_vs_general_pretraining</strong></td>
                            <td>Implicit: LLMs pretrained on general corpora may need domain-specific prompting/fine-tuning to be effective for scientific SR/G P tasks; MDBench cites concerns about mismatch between pretraining and domain.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_space_characterization</strong></td>
                            <td>Not formally measured in MDBench; LLM operators change the effective exploration distribution by biasing proposals towards distributions seen during LLM pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_during_evolution</strong></td>
                            <td>Yes in referenced work: LLM operators can be used as adaptive variation operators guided by few-shot prompts; MDBench mentions learned operators accelerate GP convergence.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Potential bias toward patterns in LLM pretraining data leading to poor exploration of novel equation forms; added per-proposal compute cost; not empirically evaluated in MDBench.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_for_theory</strong></td>
                            <td>LLM-based variation operators are promising for accelerating GP search and providing learned inductive biases, but they inherit pretraining distribution biases and add inference cost; MDBench recommends caution because learned operators may fail when target systems differ from pretraining examples.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2002.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2002.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that compare learned operators (LLM-based, neural, or data-driven) with traditional genetic programming operators, including performance comparisons, training data effects, generalization, computational costs, and hybrid approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Neural-guided GP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neural-guided genetic programming (deep-seeded GP / learned search heuristics)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Methods that incorporate learned search heuristics (neural networks or reinforcement learning) to guide GP search (seeding populations, learned mutation/crossover probabilities) aiming to mitigate convergence issues.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Symbolic regression via deep reinforcement learning enhanced genetic programming seeding</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Neural-guided GP (Mundhenk et al. style)</td>
                        </tr>
                        <tr>
                            <td><strong>operator_type</strong></td>
                            <td>neural-network-guided / hybrid</td>
                        </tr>
                        <tr>
                            <td><strong>operator_description</strong></td>
                            <td>Neural networks (RNNs, RL agents, or other learned models) are used to propose candidate subtrees, bias mutation/crossover selection, or seed GP populations; these learned components are trained to improve search efficiency and recovery rates.</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Typically trained on synthetic SR tasks or past GP runs to predict promising building blocks; MDBench references these approaches but does not retrain them within its experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_benchmark</strong></td>
                            <td>Symbolic regression and model discovery; cited as prior work to mitigate GP convergence.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared conceptually to vanilla GP (unbiased evolutionary operators) and to pure neural SR methods.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_learned_operator</strong></td>
                            <td>Referenced prior work indicates improved GP convergence and recovery rates; MDBench states neural-guided approaches mitigate GP convergence issues, but provides no MDBench-specific numerical results.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_traditional_operator</strong></td>
                            <td>Traditional GP often suffers from poor convergence in large unstructured search spaces; neural-guided variants aim to improve on this.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_hybrid_operator</strong></td>
                            <td>By design hybrid; MDBench cites them as effective mitigators of GP convergence problems.</td>
                        </tr>
                        <tr>
                            <td><strong>validity_or_executability_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_or_diversity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not reported in MDBench; general concern that learned guidance may overfit to training distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>training_bias_evidence</strong></td>
                            <td>MDBench notes learned heuristics can help but also implies the risk that they encode biases from training data, possibly reducing exploration of novel expressions.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_comparison</strong></td>
                            <td>Not quantified in MDBench; adding learned components increases compute per-generation but can reduce total generations needed.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_vs_general_pretraining</strong></td>
                            <td>Not specifically compared here; underlying idea is that better domain-representative training improves neural-guided GP performance.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_space_characterization</strong></td>
                            <td>MDBench notes GP's large unstructured search space cause convergence issues; neural guidance effectively reshapes exploration but is not characterized numerically in the benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_during_evolution</strong></td>
                            <td>Yes — learned heuristics adapt the selection/proposal distribution during evolutionary runs (in prior work); MDBench cites these methods qualitatively.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>May bias search toward patterns in the training data, reducing discovery of novel operators; still sensitive to noise and complexity of PDE libraries.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_for_theory</strong></td>
                            <td>Incorporating learned search heuristics into GP can reduce convergence problems and speed discovery on synthetic benchmarks, but the benefit depends on representativeness of training data and may reduce exploration if the guidance is overly biased.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2002.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2002.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that compare learned operators (LLM-based, neural, or data-driven) with traditional genetic programming operators, including performance comparisons, training data effects, generalization, computational costs, and hybrid approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SINDy / WSINDy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SINDy (Sparse Identification of Nonlinear Dynamics) and WSINDy (Weak SINDy)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Linear-model-based sparse-regression approaches for model discovery that represent dynamics as sparse linear combinations of candidate basis functions; WSINDy uses a weak-form formulation to improve noise robustness for PDEs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Discovering governing equations from data by sparse identification of nonlinear dynamical systems</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SINDy (and WSINDy)</td>
                        </tr>
                        <tr>
                            <td><strong>operator_type</strong></td>
                            <td>linear-model / hand-designed sparse regression operator</td>
                        </tr>
                        <tr>
                            <td><strong>operator_description</strong></td>
                            <td>Constructs a predefined library of candidate basis functions evaluated on data and uses L1 (LASSO) or weak-form identification to select a sparse linear combination; WSINDy avoids pointwise derivative estimation using integral (weak) formulations.</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_benchmark</strong></td>
                            <td>Model discovery for ODEs and PDEs (used widely as baseline in MDBench).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against GP methods (PySR, Operon), LSPT/Transformer methods (End2end, ODEFormer), neural (EQL, DeepMoD), Bayesian, and hybrid uDSR.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_learned_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_traditional_operator</strong></td>
                            <td>Linear models achieved lowest prediction error on many PDE datasets in low-noise/clean settings (WSINDy performed particularly well). On ODEs, SINDy/ESINDy generated simpler equations under noisy conditions; LM methods are generally more robust to noise than GP and LSPT in PDE tasks. Statistical comparisons: WSINDy had significantly lower error compared to PySR and Operon in PDE low-noise settings (Wilcoxon W values, p<10^-4).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_hybrid_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validity_or_executability_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_or_diversity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>LMs are constrained by the chosen library and can miss relevant nonlinear dynamics when library is insufficient; generalization depends on library expressiveness rather than learned pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>training_bias_evidence</strong></td>
                            <td>Not applicable in pretraining sense; bias arises from library construction and the linearity assumption w.r.t. basis functions.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_comparison</strong></td>
                            <td>SINDy and Operon reported as most computationally efficient on ODE datasets; linear models are good choices in time-constrained settings for low-dimensional data, but scale poorly with very large function libraries in high-dimensional PDEs.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_vs_general_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_space_characterization</strong></td>
                            <td>LM methods restrict hypothesis space to sparse linear combinations of a predefined library; this yields robustness to noise but limits expressivity and scalability as library size grows combinatorially with dimension/order.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_during_evolution</strong></td>
                            <td>Not applicable (no evolutionary adaptation); methods rely on optimization (sparse regression) not population evolution.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Fail when true dynamics are not well represented by the chosen library, suffer from explosive library growth in high-dimensional PDEs, and can miss nonlinear interactions; sensitive to derivative estimation in pointwise variants (WSINDy mitigates this).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_for_theory</strong></td>
                            <td>Linear sparse-regression approaches provide strong noise robustness and computational efficiency on low-dimensional problems, but their constrained hypothesis space and exploding library sizes limit applicability to complex, high-dimensional PDE systems where GP or learned operators may be more flexible.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>LLM-SR: Scientific Equation Discovery via Programming with Large Language Models <em>(Rating: 2)</em></li>
                <li>Language model crossover: Variation through few-shot prompting <em>(Rating: 2)</em></li>
                <li>Symbolic regression via deep reinforcement learning enhanced genetic programming seeding <em>(Rating: 2)</em></li>
                <li>A unified framework for deep symbolic regression <em>(Rating: 2)</em></li>
                <li>End-to-end symbolic regression with transformers <em>(Rating: 2)</em></li>
                <li>ODEFormer: Symbolic Regression of Dynamical Systems with Transformers <em>(Rating: 2)</em></li>
                <li>Interpretable machine learning for science with PySR and SymbolicRegression <em>(Rating: 1)</em></li>
                <li>Operon C++ an efficient genetic programming framework for symbolic regression <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2002",
    "paper_id": "paper-281525825",
    "extraction_schema_id": "extraction-schema-46",
    "extracted_data": [
        {
            "name_short": "ODEFormer",
            "name_full": "ODEFormer: Symbolic Regression of Dynamical Systems with Transformers",
            "brief_description": "A transformer-based large-scale pretrained (LSPT) model that generates full differential equations (including constants) from state trajectories without relying on explicit derivative estimates; designed to operate on dynamical-system data.",
            "citation_title": "ODEFormer: Symbolic Regression of Dynamical Systems with Transformers",
            "mention_or_use": "use",
            "system_name": "ODEFormer",
            "operator_type": "LSPT (Transformer) / neural network",
            "operator_description": "A Transformer encoder-decoder model pretrained in the LSPT paradigm on synthetic symbolic regression problems and extended to dynamical systems; it consumes state trajectories and timestamps and autoregressively generates symbolic differential equations (including constants). Does not require precomputed derivatives.",
            "training_data_description": "Pretrained on a large corpus of synthetic symbolic-regression problems (text reports 'LSPT methods pretrain on large corpus of symbolic regression problems'); ODEFormer specifically was extended/trained for dynamical systems (single observed trajectory examples) per d'Ascoli et al. 2024. Exact dataset sizes and diversity are not provided in MDBench.",
            "domain_or_benchmark": "Model discovery / symbolic regression for dynamical systems (evaluated on MDBench ODEBench and selected PDE datasets)",
            "comparison_baseline": "Compared against GP methods (PySR, Operon), linear models (SINDy, ESINDy, WSINDy), hybrid methods (uDSR), and other neural methods (EQL, DeepMoD, End2end).",
            "performance_learned_operator": "Qualitative: achieves good predictive accuracy on time derivatives for ODEs in low-noise settings (SNR ≥ 30 dB) and clean data; generalization degrades rapidly as noise increases (noted drop at SNR ≤ 20 dB). (No exact NMSE numeric provided in main text for ODEFormer.)",
            "performance_traditional_operator": "GP methods (e.g., PySR) generally achieved lower NMSE than many other methods in low-noise ODE settings (SNR ≥ 30 dB); linear methods (SINDy/ESINDy) produced simpler but sometimes less accurate models. (Paper reports statistical tests: PySR significantly lower NMSE than ESINDy: Wilcoxon W=9774, p≤0.0001.)",
            "performance_hybrid_operator": null,
            "validity_or_executability_rate": null,
            "novelty_or_diversity_metric": null,
            "out_of_distribution_performance": "Discussed: ODEFormer generalizes well on low-noise, but performance degrades with noise—suggested overfitting due to many parameters; no formal OOD test other than noise/clean splits.",
            "training_bias_evidence": "Yes — evidence of sensitivity to training distribution and noise: ODEFormer performs well when data are clean/low-noise but overfits and degrades under higher noise (SNR ≤ 20 dB), likely due to large parameter count and training regimes.",
            "computational_cost_comparison": "Qualitative: Transformer-based methods like ODEFormer have many trainable parameters and long training times compared to some linear/G P baselines; specific timing per method shown in supplementary figures but no single numeric provided for ODEFormer in main text.",
            "transfer_learning_results": "Not reported beyond the LSPT pretraining paradigm; authors note potential to extend pretrained models jointly on synthetic ODE and PDE systems as future work.",
            "domain_specific_vs_general_pretraining": "Discussed qualitatively: LSPT models are fast at inference after pretraining but may perform poorly on real-world systems whose structure differs from synthetic pretraining data. ODEFormer was trained/extended for dynamical systems and performs better than End2end which lacked dynamical examples in pretraining.",
            "ablation_study_results": null,
            "hypothesis_space_characterization": "Not quantified for ODEFormer specifically; paper contrasts LSPT learned generation (structured by pretraining) vs GP's unstructured search space and LM constrained linear form.",
            "adaptation_during_evolution": "Not applicable (no online adaptation reported); ODEFormer is a pretrained inference model rather than an evolving operator within GP loops.",
            "failure_modes": "Overfitting and rapid performance degradation under moderate-to-high measurement noise (SNR ≤ 20 dB); increases in discovered-equation complexity with noise indicating memorization/poor denoising.",
            "key_findings_for_theory": "Transformer LSPT models (like ODEFormer) can produce accurate symbolic differential equations quickly at inference time for clean/low-noise trajectories, but they are sensitive to noise and can overfit due to large parameter counts; their generalization depends strongly on whether pretraining included representative dynamical-system examples.",
            "uuid": "e2002.0"
        },
        {
            "name_short": "End2end",
            "name_full": "End-to-end symbolic regression with transformers",
            "brief_description": "A Transformer sequence-to-sequence LSPT model that maps input data to symbolic expressions, pretrained on large synthetic equation corpora; used as an off-the-shelf symbolic regression model.",
            "citation_title": "End-to-end symbolic regression with transformers",
            "mention_or_use": "use",
            "system_name": "End2end",
            "operator_type": "LSPT (Transformer) / neural network",
            "operator_description": "Sequence-to-sequence Transformer pretrained on synthetic symbolic equations to directly generate symbolic expressions (constants included); two-stage or end-to-end variants exist in the literature.",
            "training_data_description": "Pretrained on large synthetic equation datasets; MDBench notes End2end's pretraining did not include dynamical-system examples (no ODE/PDE examples), restricting generalization to model discovery tasks.",
            "domain_or_benchmark": "General-purpose symbolic regression; evaluated on MDBench ODE and PDE tasks.",
            "comparison_baseline": "Compared qualitatively against GP methods (PySR, Operon), linear methods (SINDy family), transformer LSPT extensions (ODEFormer), and neural methods (EQL, DeepMoD).",
            "performance_learned_operator": "Qualitative: exhibited the highest predictive errors among evaluated methods on ODE datasets; performed poorly on dynamical systems tasks in MDBench (Wilcoxon test with EQL: W=12795, p&lt;10^-10 reported vs EQL).",
            "performance_traditional_operator": "GP methods and some linear methods outperformed End2end on dynamical-system discovery tasks; GP methods achieved lower NMSE in low-noise ODE settings.",
            "performance_hybrid_operator": null,
            "validity_or_executability_rate": null,
            "novelty_or_diversity_metric": null,
            "out_of_distribution_performance": "Poor: End2end struggled to generalize to dynamical systems because its pretraining lacked such examples.",
            "training_bias_evidence": "Explicit: failure attributed to absence of dynamical-system examples in pretraining corpus, indicating bias toward the synthetic equation patterns it saw during training.",
            "computational_cost_comparison": "Requires GPU for training/inference; MDBench excluded End2end from many PDE evaluations due to failures and limitations in handling high-dimensional inputs; runtime comparisons not numerically broken down beyond 'long' for DL methods.",
            "transfer_learning_results": "Not evaluated in MDBench; authors suggest including dynamical systems in pretraining would improve generalization.",
            "domain_specific_vs_general_pretraining": "Clear example: general-purpose pretraining (no dynamical systems) resulted in poor performance on ODE/PDE model discovery tasks compared to a domain-specialized LSPT (ODEFormer).",
            "ablation_study_results": null,
            "hypothesis_space_characterization": "Not explicitly characterized; failure modes suggest the model's learned distribution of expressions did not cover dynamical-system equation structures.",
            "adaptation_during_evolution": null,
            "failure_modes": "High predictive error on dynamical systems; inability to handle high-dimensional PDE inputs; poor OOD generalization from synthetic training corpus lacking dynamical examples.",
            "key_findings_for_theory": "Large pretrained transformer SR models can be fast at inference but fail to generalize to domains not represented in pretraining (dynamical systems), leading to worse performance than GP and linear baselines on MDBench tasks.",
            "uuid": "e2002.1"
        },
        {
            "name_short": "uDSR",
            "name_full": "uDSR / unified Deep Symbolic Regression",
            "brief_description": "A unified deep symbolic regression framework that integrates multiple approaches (GP, linear models, optimization) as inner loops to improve training signal and discovery performance.",
            "citation_title": "A unified framework for deep symbolic regression",
            "mention_or_use": "use",
            "system_name": "uDSR",
            "operator_type": "hybrid (neural + GP + LM + optimization)",
            "operator_description": "A deep RL / neural approach that incorporates GP seeding, linear-model fitting, and post-hoc optimization as inner loops to strengthen learning signals and improve discovery; effectively a hybrid pipeline combining learned and traditional operators.",
            "training_data_description": "Trained using synthetic SR benchmarks (e.g., SRBench) and integrated components; in MDBench uDSR was run but it required subsampling on large PDE datasets (limited to 10,000 points) due to memory/runtime limits.",
            "domain_or_benchmark": "Symbolic regression / model discovery evaluated on SRBench historically and MDBench ODE/PDE datasets.",
            "comparison_baseline": "Compared against standalone GP (PySR, Operon), linear methods (SINDy family), LSPT (End2end, ODEFormer), and DL methods (EQL, DeepMoD).",
            "performance_learned_operator": "Mixed: uDSR achieved state-of-the-art on SRBench historically (as claimed in literature), but in MDBench uDSR produced inaccurate equations in some noise-free PDE/ODE settings and was among slower DL methods; on PDE datasets uDSR often runs fast but accuracy varied (paper notes uDSR tends to generate most inaccurate equations in noise-free PDE setting).",
            "performance_traditional_operator": "GP and LM methods often outperformed uDSR on many MDBench PDE/ODE tasks; specific statistical comparisons: LM methods (WSINDy) obtained significantly lower error than PySR and Operon in PDE low-noise settings (Wilcoxon p&lt;10^-4).",
            "performance_hybrid_operator": "uDSR itself is a hybrid; MDBench does not report a separate hybrid-vs-traditional numeric comparison beyond qualitative performance notes and runtime behavior.",
            "validity_or_executability_rate": null,
            "novelty_or_diversity_metric": null,
            "out_of_distribution_performance": "Not specifically quantified; uDSR's reliance on multiple inner loops can help with exploration but MDBench observed inaccuracies on some PDEs indicating limited generalization to complex real-world PDEs.",
            "training_bias_evidence": "Implicit: requires subsampling for large datasets, which may bias learned components toward subsampled patterns; paper notes uDSR can be slow and inaccurate under some conditions.",
            "computational_cost_comparison": "Computationally costly: uDSR is among the slowest DL methods on ODE datasets (orders of magnitude slower than SINDy); on PDEs it is reported among faster runtimes for some datasets but accuracy trade-offs exist. MDBench notes uDSR required subsampling (10k points) due to memory/runtime.",
            "transfer_learning_results": "Not evaluated in MDBench beyond claim of SRBench performance; no cross-domain transfer experiments reported.",
            "domain_specific_vs_general_pretraining": "Not directly compared; uDSR leverages RL and hybrid components rather than large-scale pretraining.",
            "ablation_study_results": null,
            "hypothesis_space_characterization": "Not explicitly characterized; MDBench notes uDSR and GP expand search space and can struggle on very large function libraries (e.g., complex PDEs with forcing).",
            "adaptation_during_evolution": "Yes — uDSR integrates optimization/GP/LM inner loops which can be considered as adaptive components during training; MDBench notes such hybrid integration is intended to improve learning signals.",
            "failure_modes": "In MDBench uDSR produced particularly inaccurate equations on some noise-free PDE settings and needed aggressive subsampling on large datasets; overall sensitivity to dataset size and noise, and long runtimes, are limitations.",
            "key_findings_for_theory": "Hybrid approaches that combine learned neural controllers with GP and LM inner loops can achieve strong performance on synthetic SR benchmarks but may struggle on complex, high-dimensional, or noisy dynamical-system discovery tasks; computational costs and data subsampling are practical bottlenecks limiting their empirical advantage on MDBench.",
            "uuid": "e2002.2"
        },
        {
            "name_short": "PySR",
            "name_full": "PySR (Parallel Genetic Programming for Symbolic Regression)",
            "brief_description": "A high-performance genetic programming (GP) system implemented in Julia using parallel population-based evolve-simplify-optimize loops for symbolic regression and model discovery.",
            "citation_title": "Interpretable machine learning for science with PySR and SymbolicRegression",
            "mention_or_use": "use",
            "system_name": "PySR",
            "operator_type": "traditional GP (population-based evolutionary operators)",
            "operator_description": "Genetic programming that evolves expression trees using mutation, crossover, and simplification, with parallelized evaluation implemented in Julia; does not learn operators in a data-driven way by default (search heuristics standard, though can be hybridized externally).",
            "training_data_description": null,
            "domain_or_benchmark": "Symbolic regression / model discovery for ODEs and PDEs (evaluated on MDBench ODEBench and various PDE datasets)",
            "comparison_baseline": "Compared against linear methods (SINDy family), other GP (Operon), LSPT models (End2end, ODEFormer), neural methods (EQL, DeepMoD), and hybrids (uDSR).",
            "performance_learned_operator": null,
            "performance_traditional_operator": "GP performance: On ODEs in low-noise/clean settings (SNR ≥ 30 dB), GP methods (including PySR) generally achieved the lowest NMSE for predicted time derivatives; however, GP methods overfit in noisy settings producing highly complex and inaccurate expressions. Statistical result: PySR has significantly lower NMSE than ESINDy (Wilcoxon W=9774, p≤0.0001).",
            "performance_hybrid_operator": null,
            "validity_or_executability_rate": null,
            "novelty_or_diversity_metric": null,
            "out_of_distribution_performance": "Not explicitly quantified; GP shows strong recovery on clean/synthetic systems but is sensitive to noise and can produce spurious complex terms under noise.",
            "training_bias_evidence": "Not applicable (traditional GP not pretrained), but GP's behavior reflects sensitivity to noise and search-space assumptions rather than pretraining bias.",
            "computational_cost_comparison": "PySR is generally slower than linear methods on 1D PDE problems but scales more favorably to higher dimensions; paper reports PySR slows by ~33× in 2D and ~12× in 3D (relative to its 1D runtime), whereas PDEFind slows by ~40× and ~465× for 2D and 3D respectively. On ODEs, PySR is slower than SINDy but competitive.",
            "transfer_learning_results": null,
            "domain_specific_vs_general_pretraining": null,
            "ablation_study_results": null,
            "hypothesis_space_characterization": "GP methods operate over a large, flexible, unstructured search space (expression trees) which enables accurate recovery in clean settings but makes them sensitive to noise and prone to overfitting.",
            "adaptation_during_evolution": "Standard evolutionary adaptation (mutation, crossover) but no learned operator adaptation unless extended with learned heuristics (see neural-guided GP / LLM crossover mentions).",
            "failure_modes": "Overfitting under noisy data (producing complex inaccurate expressions) and long runtimes on some low-dimensional PDEs; implementation and runtime limits can cause timeouts on very large datasets.",
            "key_findings_for_theory": "Traditional GP excels at recovering accurate symbolic models on clean, low-noise ODEs and scales comparatively better than some linear methods to higher spatial dimensions in PDEs, but lacks explicit denoising and regularization making it sensitive to noise and prone to complex overfitted expressions.",
            "uuid": "e2002.3"
        },
        {
            "name_short": "Operon",
            "name_full": "Operon (C++ Genetic Programming Framework)",
            "brief_description": "An efficient C++ genetic programming implementation using vectorized operations and compact linear-tree representations for symbolic regression.",
            "citation_title": "Operon C++ an efficient genetic programming framework for symbolic regression",
            "mention_or_use": "use",
            "system_name": "Operon",
            "operator_type": "traditional GP (population-based evolutionary operators)",
            "operator_description": "Genetic programming system implemented in C++ emphasizing vectorized evaluation and compact tree encodings to accelerate symbolic regression search; uses standard mutation and crossover operators.",
            "training_data_description": null,
            "domain_or_benchmark": "Symbolic regression / model discovery on MDBench ODE and PDE datasets",
            "comparison_baseline": "Compared with PySR (GP), SINDy family (LM), WSINDy, DeepMoD, uDSR, ODEFormer, End2end, EQL, Bayesian methods.",
            "performance_learned_operator": null,
            "performance_traditional_operator": "Operon (GP) performed competitively on clean/low-noise ODEs and PDEs; like PySR, it shows reduced predictive fidelity as noise increases and can produce complex overfit equations under noise. Statistical tests in paper reference PySR and Operon performing worse than WSINDy on PDEs in low-noise settings (WSINDy significantly lower NMSE than PySR and Operon, p&lt;10^-4).",
            "performance_hybrid_operator": null,
            "validity_or_executability_rate": null,
            "novelty_or_diversity_metric": null,
            "out_of_distribution_performance": "Not quantified; similar behavior to other GP methods: good in clean data, sensitive to noise.",
            "training_bias_evidence": null,
            "computational_cost_comparison": "Operon is computationally efficient among GP methods; on ODEs SINDy and Operon are the most computationally efficient overall. However GP methods can be slower than LM on low-dimensional PDEs, but scale better to higher dimensions compared to LM (Operon follows this trend).",
            "transfer_learning_results": null,
            "domain_specific_vs_general_pretraining": null,
            "ablation_study_results": null,
            "hypothesis_space_characterization": "Operon explores large expression-tree spaces; MDBench notes such unstructured spaces can hurt convergence and make methods sensitive to noise.",
            "adaptation_during_evolution": "Standard GP evolutionary adaptation; no learned adaptation reported in MDBench for Operon.",
            "failure_modes": "Overfitting in noisy regimes, producing complex inaccurate expressions; failures/timeouts on some high-dimensional PDEs depending on implementation constraints.",
            "key_findings_for_theory": "Efficient GP implementations (Operon) can be competitive in accuracy and runtime across datasets, but GP's susceptibility to noise and lack of built-in denoising regularization remain key limitations compared to linear methods like WSINDy.",
            "uuid": "e2002.4"
        },
        {
            "name_short": "LLM-as-GP-operator",
            "name_full": "LLM-based crossover/mutation operators for GP (LLM-SR / language-model crossover)",
            "brief_description": "Use of large language models (LLMs) to implement GP variation operators (crossover, mutation) via few-shot prompting or learned code-generation to accelerate GP convergence and introduce learned variation.",
            "citation_title": "LLM-SR: Scientific Equation Discovery via Programming with Large Language Models",
            "mention_or_use": "mention",
            "system_name": "LLM-based GP operators (LLM-SR, Language-model crossover)",
            "operator_type": "LLM-based learned operators (few-shot / prompting)",
            "operator_description": "LLMs are used as black-box operators to propose offspring expressions (crossover/mutation) via few-shot prompting or program synthesis; these learned operators replace or augment hand-designed subtree crossover/mutation to produce higher-quality variation and faster convergence.",
            "training_data_description": "LLMs are pretrained on very large corpora (code, text, symbolic math) external to these experiments; specific fine-tuning for SR is described in referenced works (Shojaee et al. 2025, Meyerson et al. 2024) rather than MDBench itself.",
            "domain_or_benchmark": "Symbolic regression and genetic programming (discussed in related work); not directly evaluated as an operator inside MDBench experiments but cited as recent advances.",
            "comparison_baseline": "Compared conceptually against traditional GP operators (subtree crossover, subtree mutation) and neural-guided search heuristics.",
            "performance_learned_operator": "Reported in referenced works to accelerate GP convergence and improve variation quality; MDBench only cites these works and does not provide new numeric evaluations in this paper.",
            "performance_traditional_operator": "Traditional GP uses hand-designed subtree crossover/mutation; MDBench notes GP performance variability and convergence issues in large, unstructured spaces without learned heuristics.",
            "performance_hybrid_operator": "Referenced works (Shojaee et al., Meyerson et al.) propose hybrids where LLM operators are embedded in GP pipelines and show empirical acceleration of GP search in their own studies (no MDBench numeric replication).",
            "validity_or_executability_rate": null,
            "novelty_or_diversity_metric": null,
            "out_of_distribution_performance": "Not discussed in MDBench specifically; implied risk that LLM operators may bias towards patterns present in LLM pretraining data.",
            "training_bias_evidence": "MDBench references these methods as recent innovations; it highlights general LSPT/Learned-operator risk: learned operators can perform poorly on systems whose structure differs from the pretraining distribution.",
            "computational_cost_comparison": "Not quantified in MDBench; LLM-based operators add inference cost per operator proposal but can reduce overall GP iterations by better proposals—tradeoff depends on LLM size and prompt/inference costs (not specified here).",
            "transfer_learning_results": "Not evaluated in MDBench.",
            "domain_specific_vs_general_pretraining": "Implicit: LLMs pretrained on general corpora may need domain-specific prompting/fine-tuning to be effective for scientific SR/G P tasks; MDBench cites concerns about mismatch between pretraining and domain.",
            "ablation_study_results": null,
            "hypothesis_space_characterization": "Not formally measured in MDBench; LLM operators change the effective exploration distribution by biasing proposals towards distributions seen during LLM pretraining.",
            "adaptation_during_evolution": "Yes in referenced work: LLM operators can be used as adaptive variation operators guided by few-shot prompts; MDBench mentions learned operators accelerate GP convergence.",
            "failure_modes": "Potential bias toward patterns in LLM pretraining data leading to poor exploration of novel equation forms; added per-proposal compute cost; not empirically evaluated in MDBench.",
            "key_findings_for_theory": "LLM-based variation operators are promising for accelerating GP search and providing learned inductive biases, but they inherit pretraining distribution biases and add inference cost; MDBench recommends caution because learned operators may fail when target systems differ from pretraining examples.",
            "uuid": "e2002.5"
        },
        {
            "name_short": "Neural-guided GP",
            "name_full": "Neural-guided genetic programming (deep-seeded GP / learned search heuristics)",
            "brief_description": "Methods that incorporate learned search heuristics (neural networks or reinforcement learning) to guide GP search (seeding populations, learned mutation/crossover probabilities) aiming to mitigate convergence issues.",
            "citation_title": "Symbolic regression via deep reinforcement learning enhanced genetic programming seeding",
            "mention_or_use": "mention",
            "system_name": "Neural-guided GP (Mundhenk et al. style)",
            "operator_type": "neural-network-guided / hybrid",
            "operator_description": "Neural networks (RNNs, RL agents, or other learned models) are used to propose candidate subtrees, bias mutation/crossover selection, or seed GP populations; these learned components are trained to improve search efficiency and recovery rates.",
            "training_data_description": "Typically trained on synthetic SR tasks or past GP runs to predict promising building blocks; MDBench references these approaches but does not retrain them within its experiments.",
            "domain_or_benchmark": "Symbolic regression and model discovery; cited as prior work to mitigate GP convergence.",
            "comparison_baseline": "Compared conceptually to vanilla GP (unbiased evolutionary operators) and to pure neural SR methods.",
            "performance_learned_operator": "Referenced prior work indicates improved GP convergence and recovery rates; MDBench states neural-guided approaches mitigate GP convergence issues, but provides no MDBench-specific numerical results.",
            "performance_traditional_operator": "Traditional GP often suffers from poor convergence in large unstructured search spaces; neural-guided variants aim to improve on this.",
            "performance_hybrid_operator": "By design hybrid; MDBench cites them as effective mitigators of GP convergence problems.",
            "validity_or_executability_rate": null,
            "novelty_or_diversity_metric": null,
            "out_of_distribution_performance": "Not reported in MDBench; general concern that learned guidance may overfit to training distributions.",
            "training_bias_evidence": "MDBench notes learned heuristics can help but also implies the risk that they encode biases from training data, possibly reducing exploration of novel expressions.",
            "computational_cost_comparison": "Not quantified in MDBench; adding learned components increases compute per-generation but can reduce total generations needed.",
            "transfer_learning_results": null,
            "domain_specific_vs_general_pretraining": "Not specifically compared here; underlying idea is that better domain-representative training improves neural-guided GP performance.",
            "ablation_study_results": null,
            "hypothesis_space_characterization": "MDBench notes GP's large unstructured search space cause convergence issues; neural guidance effectively reshapes exploration but is not characterized numerically in the benchmark.",
            "adaptation_during_evolution": "Yes — learned heuristics adapt the selection/proposal distribution during evolutionary runs (in prior work); MDBench cites these methods qualitatively.",
            "failure_modes": "May bias search toward patterns in the training data, reducing discovery of novel operators; still sensitive to noise and complexity of PDE libraries.",
            "key_findings_for_theory": "Incorporating learned search heuristics into GP can reduce convergence problems and speed discovery on synthetic benchmarks, but the benefit depends on representativeness of training data and may reduce exploration if the guidance is overly biased.",
            "uuid": "e2002.6"
        },
        {
            "name_short": "SINDy / WSINDy",
            "name_full": "SINDy (Sparse Identification of Nonlinear Dynamics) and WSINDy (Weak SINDy)",
            "brief_description": "Linear-model-based sparse-regression approaches for model discovery that represent dynamics as sparse linear combinations of candidate basis functions; WSINDy uses a weak-form formulation to improve noise robustness for PDEs.",
            "citation_title": "Discovering governing equations from data by sparse identification of nonlinear dynamical systems",
            "mention_or_use": "use",
            "system_name": "SINDy (and WSINDy)",
            "operator_type": "linear-model / hand-designed sparse regression operator",
            "operator_description": "Constructs a predefined library of candidate basis functions evaluated on data and uses L1 (LASSO) or weak-form identification to select a sparse linear combination; WSINDy avoids pointwise derivative estimation using integral (weak) formulations.",
            "training_data_description": null,
            "domain_or_benchmark": "Model discovery for ODEs and PDEs (used widely as baseline in MDBench).",
            "comparison_baseline": "Compared against GP methods (PySR, Operon), LSPT/Transformer methods (End2end, ODEFormer), neural (EQL, DeepMoD), Bayesian, and hybrid uDSR.",
            "performance_learned_operator": null,
            "performance_traditional_operator": "Linear models achieved lowest prediction error on many PDE datasets in low-noise/clean settings (WSINDy performed particularly well). On ODEs, SINDy/ESINDy generated simpler equations under noisy conditions; LM methods are generally more robust to noise than GP and LSPT in PDE tasks. Statistical comparisons: WSINDy had significantly lower error compared to PySR and Operon in PDE low-noise settings (Wilcoxon W values, p&lt;10^-4).",
            "performance_hybrid_operator": null,
            "validity_or_executability_rate": null,
            "novelty_or_diversity_metric": null,
            "out_of_distribution_performance": "LMs are constrained by the chosen library and can miss relevant nonlinear dynamics when library is insufficient; generalization depends on library expressiveness rather than learned pretraining.",
            "training_bias_evidence": "Not applicable in pretraining sense; bias arises from library construction and the linearity assumption w.r.t. basis functions.",
            "computational_cost_comparison": "SINDy and Operon reported as most computationally efficient on ODE datasets; linear models are good choices in time-constrained settings for low-dimensional data, but scale poorly with very large function libraries in high-dimensional PDEs.",
            "transfer_learning_results": null,
            "domain_specific_vs_general_pretraining": null,
            "ablation_study_results": null,
            "hypothesis_space_characterization": "LM methods restrict hypothesis space to sparse linear combinations of a predefined library; this yields robustness to noise but limits expressivity and scalability as library size grows combinatorially with dimension/order.",
            "adaptation_during_evolution": "Not applicable (no evolutionary adaptation); methods rely on optimization (sparse regression) not population evolution.",
            "failure_modes": "Fail when true dynamics are not well represented by the chosen library, suffer from explosive library growth in high-dimensional PDEs, and can miss nonlinear interactions; sensitive to derivative estimation in pointwise variants (WSINDy mitigates this).",
            "key_findings_for_theory": "Linear sparse-regression approaches provide strong noise robustness and computational efficiency on low-dimensional problems, but their constrained hypothesis space and exploding library sizes limit applicability to complex, high-dimensional PDE systems where GP or learned operators may be more flexible.",
            "uuid": "e2002.7"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "LLM-SR: Scientific Equation Discovery via Programming with Large Language Models",
            "rating": 2
        },
        {
            "paper_title": "Language model crossover: Variation through few-shot prompting",
            "rating": 2
        },
        {
            "paper_title": "Symbolic regression via deep reinforcement learning enhanced genetic programming seeding",
            "rating": 2
        },
        {
            "paper_title": "A unified framework for deep symbolic regression",
            "rating": 2
        },
        {
            "paper_title": "End-to-end symbolic regression with transformers",
            "rating": 2
        },
        {
            "paper_title": "ODEFormer: Symbolic Regression of Dynamical Systems with Transformers",
            "rating": 2
        },
        {
            "paper_title": "Interpretable machine learning for science with PySR and SymbolicRegression",
            "rating": 1
        },
        {
            "paper_title": "Operon C++ an efficient genetic programming framework for symbolic regression",
            "rating": 1
        }
    ],
    "cost": 0.02308,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>MDBench: Benchmarking Data-Driven Methods for Model Discovery
24 Sep 2025</p>
<p>Amirmohammad Ziaei Bideh 
Department of Computer Science
Graduate Center
CUNY, New YorkNYUSA</p>
<p>Aleksandra Georgievska 
Department of Computer Science
Queens College
CUNY
New YorkNYUSA</p>
<p>Jonathan Gryak 
Department of Computer Science
Graduate Center
CUNY, New YorkNYUSA</p>
<p>Department of Computer Science
Queens College
CUNY
New YorkNYUSA</p>
<p>MDBench: Benchmarking Data-Driven Methods for Model Discovery
24 Sep 202567317C7947F40A924A23FB00CABEC7CAarXiv:2509.20529v1[cs.LG]
Model discovery aims to uncover governing differential equations of dynamical systems directly from experimental data.Benchmarking such methods is essential for tracking progress and understanding trade-offs in the field.While prior efforts have focused mostly on identifying single equations, typically framed as symbolic regression, there remains a lack of comprehensive benchmarks for discovering dynamical models.To address this, we introduce MDBench, an opensource benchmarking framework for evaluating model discovery methods on dynamical systems.MDBench assesses 12 algorithms on 14 partial differential equations (PDEs) and 63 ordinary differential equations (ODEs) under varying levels of noise.Evaluation metrics include derivative prediction accuracy, model complexity, and equation fidelity.We also introduce seven challenging PDE systems from fluid dynamics and thermodynamics, revealing key limitations in current methods.Our findings illustrate that linear methods and genetic programming methods achieve the lowest prediction error for PDEs and ODEs, respectively.Moreover, linear models are in general more robust against noise.MDBench accelerates the advancement of model discovery methods by offering a rigorous, extensible benchmarking framework and a rich, diverse collection of dynamical system datasets, enabling systematic evaluation, comparison, and improvement of equation accuracy and robustness.</p>
<p>Introduction</p>
<p>Data-driven equation discovery-rather than relying solely on conservation laws or physical principles-has historically played a pivotal role in scientific breakthroughs.Johannes Kepler discovered the third law of planetary motion by leveraging geometrical intuition and searching for patterns in empirically gathered data (Cranmer 2023).Similarly, Edwin Hubble identified the relationship between redshift and distance through empirical observations that, at the time, lacked theoretical explanation (Kragh 2021).</p>
<p>A dynamical system is a mathematical framework that describes how the state of a system evolves over time through differential equations.Many complex real-world systems arising in finance, medicine, and engineering can be modeled as nonlinear dynamical systems, e.g., in physics, they model fluid dynamics (Kleinstreuer 2018); in biology, they</p>
<p>Copyright © 2026, Association for the Advancement of Artificial Intelligence (www.aaai.org).All rights reserved.describe neural activity (Breakspear 2017); and in engineering, they underpin control theory (Walker 2013).Relying solely on expert domain knowledge to derive governing equations is increasingly impractical, especially in the era of high-dimensional and large-scale experimental data.This has motivated the development of automated tools for scientific discovery.While machine learning (ML) models have demonstrated remarkable predictive performance, their black-box nature often impedes interpretability and insight into the underlying system dynamics.Data-driven model discovery (MD) aims to bridge this gap by using measurement data and ML algorithms to infer interpretable dynamical system models.</p>
<p>Unlike traditional black-box ML methods, MD involves optimizing both the parameters and the structure of the model, resulting in models that are inherently interpretable.However, generic MD techniques are not directly applicable to the discovery of differential equations.In this work, we present an extension of MD tailored specifically to the discovery of dynamical system models.In the literature, symbolic regression (SR) typically refers to the discovery of a single equation rather than a system.Here, we adopt the term model discovery to reflect the broader scope of our work.</p>
<p>Despite recent progress, the MD field lacks a standardized benchmark for evaluating algorithms on dynamical systems.To address this, we introduce MDBench, an open-source and extensible benchmarking framework, that includes a suite of ODE and PDE datasets (Figure 1).Establishing such a benchmark is essential for characterizing algorithmic PDEBench (Takamoto et al. 2022) is a more recent benchmark focused on PDE systems that evaluated 11 physical systems using four machine learning surrogates: U-Net (Ronneberger, Fischer, and Brox 2015), Fourier Neural Operator (Li et al. 2021), and Physics-Informed Neural Networks (PINNs) (Raissi, Perdikaris, and Karniadakis 2019).While these methods are effective for solving PDEs, they are black-box models and cannot generate symbolic equations, limiting their utility for interpretable model discovery.</p>
<p>Gilpin et al. (Gilpin 2021) benchmarked 131 chaotic ODE systems, framing them as time series forecasting tasks.Although their study includes symbolic regression methods, only four MD methods are tested, and no evaluation is provided on the complexity or interpretability of the discovered models.</p>
<p>Other benchmarks have narrower or domain-specific scopes.cp3-bench (Thing and Koksbang 2025) evaluated 12 MD algorithms on 28 datasets from cosmology and astroparticle physics.CFDBench (Luo, Chen, and Zhang 2023) focuses on computational fluid dynamics (CFD) simulations and evaluated neural operators under varying conditions, without considering symbolic recovery of governing equations.The benchmark by Otness et al. (Otness et al. 2021) includes four PDE systems, but evaluated only black-box predictive models such as convolutional neural networks and k-nearest neighbors.</p>
<p>Table 1 summarizes key characteristics of these benchmarks, highlighting MDBench's broader coverage across ODEs, PDEs, and model discovery algorithms.</p>
<p>Benchmark</p>
<p>Overview of Model Discovery Methods</p>
<p>Since Koza's early work on genetic programming (GP) for symbolic regression (Koza 1994), numerous methods have emerged for data-driven model discovery.We categorize them into four main classes.</p>
<p>Genetic Programming (GP) GP-based methods evolve expression trees using evolutionary operators to search for equations that best fit the data.Early implementations such as gplearn2 are still widely used.More modern alternatives, including PySR (La Cava et al. 2021) and Operon (Burlacu, Kronberger, and Kommenda 2020), offer improved search efficiency and parallelism.GP methods often struggle with convergence due to their large, unstructured search spaces.They also do not naturally learn parameters from data.Neural-guided approaches like those in (Mundhenk et al. 2021) mitigate this by incorporating learned search heuristics.Recent work further accelerates GP convergence by using large language models (LLMs) as crossover and mutation operators (Shojaee et al. 2025;Meyerson et al. 2024).</p>
<p>Linear Models (LM) Linear model-based methods represent equations as sparse linear combinations of candidate basis functions ("library").A typical form is f = k i=1 β i ϕ i , where ϕ i are predefined basis terms and β i are scalar coefficients.SINDy (Brunton, Proctor, and Kutz 2016) pioneered this approach using LASSO for sparsity.PDEFIND (Rudy et al. 2017) extended it to PDEs, and WSINDy (Messenger and Bortz 2021) further improved robustness by formulating equations in weak form, avoiding direct differentiation.ESINDy (Fasel et al. 2022) introduced ensemble learning for robustness in low-data or high-noise regimes.DeepMoD (Both et al. 2021) combines neural networks with sparse regression by learning spatial features via automatic differentiation.</p>
<p>Bayesian variants (Yuan et al. 2023;North, Wikle, and Schliep 2025;More et al. 2023) introduce uncertainty modeling and noise-aware inference.However, all LM methods are constrained by the assumption of linearity with respect to basis functions and often lack principled ways to construct these function libraries for unknown systems.</p>
<p>Large-Scale Pretraining (LSPT) LSPT methods pretrain a single model-often a Transformer architecture (Vaswani et al. 2017)-on a large corpus of symbolic regression problems.These models learn to map input-output pairs to symbolic equations, enabling rapid inference on new data once training is complete.</p>
<p>Neural Symbolic Regression that Scales (NeSymReS) (Biggio et al. 2021) exemplifies this approach.It uses a Set Transformer (Lee et al. 2019) as an encoder to learn latent representations of input-output mappings from synthetic equations.A decoder then generates symbolic expressions autoregressively using beam search.After generating symbolic forms, constant placeholders are refined using post-hoc optimization with methods such as BFGS (Fletcher 2000).</p>
<p>To overcome limitations of this two-stage design, Kamienny et al. (Kamienny et al. 2022) introduced an end-to-end approach where the Transformer directly generates complete mathematical expressions, including constants.ODEFormer (d'Ascoli et al. 2024) further extends this framework to dynamical systems, generating full differential equations from a single observed trajectory.</p>
<p>The main advantage of LSPT methods is their inference speed.Once trained, they can quickly generate symbolic equations for new datasets without the need for retraining or optimization, which makes them suitable for real-time applications.However, they may perform poorly on real-world systems whose structure or dynamics differ from the synthetic equations seen during pretraining (Kamienny et al. 2022).</p>
<p>Deep Learning (DL) DL-based methods use neural networks to learn symbolic equations from data.One example is Equation Learner Networks (EQL) (Martius and Lampert 2016;Sahoo, Lampert, and Martius 2018), which are fully differentiable feed-forward networks that incorporate symbolic operators (e.g., sine, cosine, multiplication) as activation functions.This design enables the integration of symbolic structure into neural models and allows EQL to be combined with other architectures for scientific discovery tasks (Kim et al. 2020).</p>
<p>Another class of DL methods uses recurrent neural networks (RNNs) to learn a probability distribution over symbolic expressions conditioned on data.Deep Symbolic Regression (DSR) (Petersen et al. 2021) follows this approach.It trains an RNN using a risk-seeking policy gradient algorithm, where the reward is based on the predictive accuracy of sampled equations.However, the learning signal in this setup comes only from the scalar reward based on the (X, y) fit, which can be weak and indirect.</p>
<p>To overcome this limitation, uDSR (Landajuela et al. 2022) integrates optimization techniques from genetic programming (GP), linear models (LM), and large-scale pretraining (LSPT) as inner loops within the DSR framework.This hybrid approach achieves state-of-the-art performance on the SRBench benchmark (La Cava et al. 2021).</p>
<p>MDBench</p>
<p>MDBench is a unified benchmarking framework for evaluating data-driven MD algorithms on both ODE and PDE systems (Figure 1).It includes a diverse suite of 63 ODEs and 14 PDEs, ranging from simple linear dynamics to highdimensional physical systems.MDBench standardizes data formats, provides symbolic preprocessing for PDEs, incorporates realistic noise modeling, and supports automated hyperparameter tuning across methods.</p>
<p>Datasets</p>
<p>Dynamical systems are commonly modeled using either ODEs or PDEs, depending on whether the variables of interest evolve only over time or over both time and space.ODEbased systems involve temporal dynamics, while PDE-based systems also incorporate spatial variation.ODE Systems.ODEs describe the evolution of a system's state variables over time.Formally, a dynamical system with d state variables u ∈ R d is governed by equations of the form:
du i dt = f i (t, u 1 , u 2 , . . . , u d ), i = 1, . . . , d,
where f i : R d → R. Each dataset consists of observed trajectories U ∈ R Nt×d .We adopt the ODEBench dataset (d'Ascoli et al. 2024), which includes 63 systems from a textbook by Strogatz (Strogatz 2024) and several sourced from Wikipedia.These systems span one to four state variables and cover a range of real-world phenomena.For all methods, the input consists of state variables u, and the targets are their time derivatives u.PDE Systems.PDEs describe spatiotemporal systems where each state variable depends on both space and time.A PDE system with d state variables u ∈ R d is represented on a D-dimensional spatial grid X ∈ R Nx 1 ×•••×Nx D .The spatial grid is constructed from uniformly-spaced intervals along each spatial coordinate, with spacing that may differ between them (i.e., ∆x
1 ̸ = • • • ̸ = ∆x D ). The time domain t ∈ [0, T ] is uniformly spaced.
We focus on PDEs of the form:
∂u i ∂t = f i (u, ∂u ∂x 1 , ∂ 2 u ∂x 2 1 , ..., ∂u ∂x 2 , ∂ 2 u ∂x 2 2 , ...); i = 1, ..., d,(1)
where f i depends nonlinearly on the state variables and their spatial derivatives.</p>
<p>Our benchmark includes both widely-studied PDE systems in the MD literature and seven systems simulating complex physical processes in fluid dynamics.Previously established datasets include Advection and Burgers (Takamoto et al. 2022); Korteweg-de Vries (KdV), Kuramoto Sivishinky (KS), Nonlinear Schrödinger (NLS) , and Reaction Diffusion (RD) (Brunton, Proctor, and Kutz 2016); Advection Diffusion (AD) (Both et al. 2021).</p>
<p>We observe that commonly used PDE systems in the MD literature often fail to capture the complexity of realworld phenomena, which may involve space-dependent or piecewise forcing functions, or exhibit high dimensionality.Therefore, we include the following PDE systems to our testbed: Heat (Laser) (Abali 2016); Heat (Solar), Navier-Stokes (Channel), Navier-Stokes (Cylinder), and Reaction-Diffusion (Cylinder) (Langtangen and Logg 2017).</p>
<p>A summary of the dataset properties (Table A1) along with full equations and implementation details are included in the supplementary material.</p>
<p>Experimental Setup</p>
<p>Noise Setup.To assess robustness, we simulate measurement noise by corrupting clean state variables with Gaussian noise at signal-to-noise ratios (SNRs) of 40 dB, 30 dB, 20 dB, and 10 dB.We apply multiplicative noise as:
u → (1 + ξ)u, ξ ∼ N (0, σ), σ = 10 −SNRdB/20 .
This impacts both the right-hand side and the estimated time derivatives.We compute noisy derivatives using finite differences and evaluate prediction error against the clean derivatives.</p>
<p>Metrics.In order to assess the performance of the discovered equations, we investigate two metrics: 1) NMSE refers to the accuracy of the predicted time derivatives defined as the Normalized Mean Square Error of the deriva-
tives, NMSE(u t , ût ) = N i=1 u (i) t −û (i) t 2 N i=1 u (i) t 2 +ϵ
, where ϵ is a small regularization constant set to 10 −10 .NMSE measures trajectory-level predictive fidelity.A lower NMSE indicates better agreement between the model and data, and serves as a proxy for how well the discovered dynamics reproduce observed behavior.2) Complexity refers to the total number of nodes, constant terms, and operations in the expression tree of the equations.This metric encourages parsimony and penalizes unnecessary long or redundant terms.The SymPy package is used to parse the discovered equations and compute their complexities (Meurer et al. 2017).</p>
<p>Hyperparameter Tuning.Since MD methods are sensitive to hyperparameters, we adopt a unified and automated tuning protocol.For each method-dataset pair, we evaluate all combinations from a predefined hyperparameter grid and select the best configuration using a composite fitness score that balances between the complexity and accuracy of the equations.Similar to (Merler et al. 2024;Shojaee et al. 2023), the fitness function is defined as
s(f |u) = 1 1 + NMSE(u t , f (u)) + λ exp − l(f ) L ,(2)
where the hyperparameter λ weights the relative importance of accuracy and complexity, l(.) computes the complexity of the equations, and L denotes the maximum equation length.</p>
<p>The set of defined hyperparameters for each algorithm is provided in the supplementary materials.Throughout the experiments, we set λ = 1 and L = 200.</p>
<p>Extending Generic Methods to PDEs.Most generic MD methods were originally designed for standard supervised learning tasks, where both input features and target outputs are explicitly available in the datasets.However, in the context of PDE-based dynamical systems, the right-hand side of the equations-typically involving spatial derivativesmust be constructed from the observed data.To enable the application of these methods to PDE discovery, we preprocess each dataset to compute the required symbolic features.Specifically, for each PDE system with d state variables, we generate a symbol set that includes the state variables themselves along with their spatial derivatives up to fourth order.These are computed using second-order accurate finite difference approximations via the findiff package (Baer 2018).The resulting symbolic feature set is {u 1 , u 2 , ..., u d } ∪
∂ j ui ∂x j k | i = 1, . . . , d; j = 1, . . . , 4; k = 1, . . . , D .
The target variables, i.e., the time derivatives ∂u i /∂t, are also computed using the same finite difference scheme.</p>
<p>Training and Inference.We split each dataset along the time dimension into three parts: the first 60% is used for training, the next 20% for validation (used in hyperparameter tuning), and the final 20% for testing.After selecting the optimal hyperparameters based on the validation set, each model is retrained on the combined training and validation sets.The final evaluation is performed on the test set using the metrics described earlier.</p>
<p>For GP-based methods, we follow the authors' recommended default settings and train the models on the combined training and validation data.From the set of candidate expressions generated during evolution, we select the best equation using the fitness function defined in Equation 2.</p>
<p>To ensure fairness and feasibility, we impose a time limit of 12 hours per experiment, covering both training and hyperparameter tuning.All time-bounded experiments are conducted on an Ubuntu 22.04.3 server equipped with a 24-core Intel Core Ultra 9 285K @ 1.44 GHz CPU and 96 GB of RAM.For methods that require GPU acceleration (EQL, uDSR, ODEFormer, End2end, and DeepMoD), we use a single NVIDIA RTX 4000 Ada Generation GPU with 20 GB of memory.</p>
<p>Methods</p>
<p>From the algorithm categories presented in Section 2, we selected a representative set of model discovery methods to benchmark.Selection criteria included strong reported performance in prior work and the availability of well-documented, easily adoptable implementations.The studied method include PDEFIND (Rudy et al. 2017), WSINDy (Messenger and Bortz 2021), ESINDy/EWSINDy (Fasel et al. 2022), Bayesian (More et al. 2023), DeepMoD (Both et al. 2021), SINDy (Brunton, Proctor, and Kutz 2016), EQL (Sahoo, Lampert, and Martius 2018), uDSR (Landajuela et al. 2022), PySR (Cranmer 2023), Operon (Burlacu, Kronberger, andKommenda 2020), ODEFormer (d'Ascoli et al. 2024), and End2end (Kamienny et al. 2022).Table 2 provides an overview of the selected algorithms, including their methodological type, the systems they are applied to, and a brief description.</p>
<p>Due to memory or runtime limitations, the methods Deep-MoD, uDSR, and End2end are unable to handle large datasets efficiently.For these methods, we perform training on a subsampled version of the data, limited to 10,000 points.</p>
<p>Results and Discussion</p>
<p>Performance on ODE Data</p>
<p>Figure 2a compares the performance of a representative subset of model discovery methods on the ODEBench dataset (see Figure A10a for additional methods).Most methods were able to recover valid equations under noiseless conditions, but their robustness to increasing noise varied considerably.Tables A5 and A6 in supplementary materials provide a more detailed account of the performance metrics on ODE datasets.</p>
<p>Equations generated by ODEFormer have good predictive accuracy on time derivatives in low-noise settings and clean data (SNR ≥ 30 dB), indicating effective generalization.However, its performance degrades rapidly under higher noise levels (SNR ≤ 20 dB), suggesting overfitting likely due to its large number of trainable parameters.This is also reflected in the increasing complexity of the discovered equations as noise increases.</p>
<p>In contrast, linear models SINDy and ESINDy generated simpler equations under noisy conditions.The Wilcoxon signed-rank test showed a significant difference between the complexity of SINDy equations and that of EQL (next simplest equations) for noisy data W = 6951.5,p &lt; 10 −8 .The included sparsity-promoting regularization acts as an  implicit denoising method, suppressing uncertain or spurious terms introduced by noise.This behavior, while leading to less complex equations, may also result in the omission of relevant nonlinear dynamics, especially at low SNR.GP-based methods generally achieved lower error in predicted time derivatives compared to other methods in lownoise settings and clean data (SNR ≥ 30 dB).For instance, PySR has a significantly lower NMSE than ESINDy based on Wilcoxon signed-rank test (log-scaled, W = 9774, p ≤ 0.0001).GP algorithms showed a tendency to overfit in noisy settings, producing highly complex and inaccurate expressions.This might suggest that GP-based methods lacking explicit regularization or those imposing minimal assumptions on the equation skeleton in the search space are more sensitive to noisy inputs.</p>
<p>End2end exhibited the highest predictive errors among the evaluated methods, indicating that it struggled to capture the underlying system dynamics (Wilcoxon signed-rank test with EQL, log scale NMSE, W = 12795, p &lt; 10 −10 ).This may be due to the fact that its pretrained model was not exposed to any dynamical systems during training, limiting its ability to generalize to such tasks.</p>
<p>As system dimensionality increases, most methods exhibit a corresponding rise in training time.Among the evaluated approaches, SINDy and Operon are the most computationally efficient, whereas EQL and uDSR are significantly slower, differing by several orders of magnitude.Figure A11 in supplementary materials shows the training times of various MD methods on ODE datasets.</p>
<p>Performance on PDE Data</p>
<p>Compared to ODEs, PDE systems present a more challenging testbed due to the presence of spatial derivatives and higher-order terms, which tend to amplify noise.Fig-  The primary reasons for a method's failure to discover an underlying model include: a lack of support for higherdimensional data in the published software, failing to finish training before the timeout threshold was reached, and runtime errors within the published codebases.EQL and End2end are excluded from the PDE evaluations due to repeated failures, mainly caused by implementation issues or limitations in handling high-dimensional inputs.In particular, the pretrained transformer model for the End2end method was trained on synthetic equations with up to 10 features (Kamienny et al. 2022), which prohibits its usage on high-dimensional systems or systems with more than two state variables.Figure 2b shows that there is a sharp error gap between the NMSE for clean and low-noise systems (SNR = 40 dB) among the selected PDE datasets.The logarithm of the error ratio between the noisy and clean settings for PDEs is 3.13± 2.09, compared to 0.88 ± 2.12 for the ODE datasets.This illustrates that the accuracy of the predicted time derivatives estimated by model discovery methods for PDE systems is more sensitive to noise than for ODE systems.One reason is that while ODE systems involve only a single time derivative per state variable, PDE systems include higher-order and spatial derivatives.These high-order derivatives amplify noise in measured data, leading to greater accumulation of the errors.</p>
<p>Most LM methods, except for DeepMoD, achieved lower NMSE and better robustness in low-noise settings (SNR ≥ 30 dB) and noise-free data compared to GP methods.For instance, WSINDy obtained significantly lower error compared to PySR (W = 455.0,p &lt; 10 −4 ) and Operon (W = 466.0,p &lt; 10 −4 ) in the mentioned noise settings.PySR and Operon, while producing competitive results in clean settings, suffer from reduced predictive fidelity as noise increases.Despite simpler equations, uDSR tends to generate the most inaccurate equations in the noise-free setting.</p>
<p>Table 3 summarizes the NMSE and expression complex-ity for each method across all PDE datasets in the clean (noise-free) setting.Almost all methods achieve low prediction error on the Heat (Solar) 1D dataset.However, as the spatial dimension of the Heat (Solar) system increases (from 1D to 3D), the NMSE of predicted derivatives decreases.This is because higher dimensions roughly double the size of the function library for sparse-regression methods and the number of symbols for GP-based methods, thereby enlarging the search space and making model discovery more challenging.These results highlight the need for developing scalable algorithms capable of handling high-dimensional data.</p>
<p>The datasets Heat (Laser) and RD (Cylinder) are more challenging than the rest of the PDEs in the benchmark due to their use of custom forcing functions.The Heat (Laser) system contains a spatially-dependent forcing function, while the RD (Cylinder) system not only includes six state variables but also has piecewise linear forcing functions that vary across the spatial domain.Most of the LM methods fail at generating equations for these datasets likely because of the huge library size and sparse regression problem.The results of PySR, Operon, and uDSR methods on these datasets illustrate that existing model discovery methods perform poorly on such datasets, suggesting the need for more generalizable and robust algorithms for discovering real-world PDE systems.</p>
<p>Runtimes vary widely by method and PDE dataset, ranging from a few seconds to several hours (Table 3).Overall, PDEFind, WSINDy, PySR, and uDSR demonstrate the fastest runtimes.While GP methods, particularly PySR, tend to be slower than linear methods on 1D problems, they scale more favorably in higher dimensions.For example, on the Heat (Solar) dataset, PDEFind is approximately 40× and 465× slower in the 2D and 3D cases, respectively, compared to the 1D case.In contrast, PySR slows down by about 33× in 2D and only 12× in 3D.This disparity likely stems from the growing size of sparse regression systems encountered by linear model-based methods.This suggests that in general, in time-constrained environments, LM methods are a better choice for low-dimensional datasets, while GP methods and uDSR scale more robustly to higher-dimensional data.More granular runtime data can be found in Table A7.
Dataset PDEFIND Bayesian WSINDy EWSINDy DeepMoD PySR Operon uDSR Advection ✓ 10 −6 (21) ✓ ✓ 10 1 (12) ✓ ✓ 10 −6 (8) Burgers ✓ 10 −6 (9) ✓ ✓ ✓ ✓ ✓ 10 −1 (8) KdV ✓ 10 −3 (9) ✓ ✓ ✓ ✓ 10 −1 (7) 10 −1 (7) KS ✓ 10 −3 (12) ✓ ✓ 10 1 (3) ✓ 10 −3 (23) 10 0 (4) AD 10 −5 (17) 10 −5 (12) ✓ ✓ 10 1 (3) ✓ ✓ 10 0 (15) Heat (Solar) 1D ✓ 10 −3 (8) ✓ ✓ 10 6 (3) ✓ ✓ 10 −2 (13) Heat (Solar) 2D 10 −2 (30) - 10 −3 (17) 10 −3 (9) - 10 −3 (3) 10 −3 (7) 10 −2 (12) Heat (Solar) 3D 10 1 (23) - - - -10</p>
<p>Limitations</p>
<p>Despite recent progress in data-driven model discovery, we highlight several key limitations.First, many algorithms implicitly assume uniform physical parameters (e.g., diffusivity, conductivity) across space and time.This assumption simplifies inference but fails to reflect heterogeneities in real-world systems such as Heat (Laser) and Reaction-diffusion (Cylinder) systems (Table 3).Models built under this assumption may generalize poorly to such heterogeneous environments.</p>
<p>Second, current methods show degraded performance on systems with many state variables or high-dimensional spaces.As shown in Table 3, systems with one state variable have a much higher successful discovery rate and lower errors on average compared to systems with more than one state variable.In the case of Heat (Solar) systems, the error increases for all methods as the spatial dimension grows.LM approaches suffer from the exponential growth of function libraries, while GP-based methods face intractable search spaces.This scalability bottleneck limits applicability to realistic systems such as multi-component dynamical systems or high-resolution simulations.</p>
<p>Third, most discovery methods operate on numerical approximations of derivatives, making them highly sensitive to noise.In PDE systems, the amplification of noise through higher-order derivatives further compromises model fidelity as can be seen from Figure A10b in supplementary materials.Without robust denoising mechanisms or noise-aware formulations, the discovered equations may be structurally incorrect despite yielding a low NMSE on predicted time derivatives.</p>
<p>Fourth, there is no established metric for quantifying equation fidelity.Standard metrics such as NMSE and symbolic complexity do not always reflect the symbolic correctness of discovered equations.For example, while GP methods achieved a relatively low NMSE on NS (Channel) dataset, the discovered equations do not capture the true dynamics.Without a robust metric that captures functional equivalence, e.g., using a discretized version of the Sobolev semi-norm for PDE systems (Schaback 2015), benchmarking results may obscure deeper algorithmic failures.</p>
<p>Finally, several methods fail on specific datasets due to implementation limitations (e.g., lack of support for multidimensional systems, unstable solvers, or uninformative error messages).This undermines reproducibility and suggests the need for compatible implementations that can be used by other researchers to apply the methods on their datasets.</p>
<p>Conclusion</p>
<p>In this paper, we introduced MDBench, a comprehensive and extensible benchmark for evaluating 12 model discovery methods across 77 dynamical systems.We evaluated the quality of the discovered dynamics across four methodological classes, assessing predictive accuracy of time derivatives, equation complexity, and fidelity to ground-truth dynamics.We release MDBench as an open-source, extensible benchmarking framework as well as a representative collection of datasets, and believe it provides a foundation for the evaluation and development of novel model discovery algorithms for dynamical systems.We hope MDBench serves as a foundation for future research in interpretable, robust model discovery across complex dynamical systems.</p>
<p>A MDBench Pipeline</p>
<p>We release the MDBench pipeline as an open-source tool to support reproducible, extensible research in model discovery.The framework is modular by design -drawing inspiration from SRBench (La Cava et al. 2021) -and is structured to simplify the integration of new algorithms and datasets.All datasets are provided in a unified format to ensure cross-method compatibility and ease of adoption by the broader research community.</p>
<p>To promote reproducibility, we provide isolated environments for each method, with exact package versions and dependencies specified.The datasets and the complete pipeline, including environment setup, algorithm installation, noise configuration, and experiment execution, are publicly available.</p>
<p>The MDBench repository is organized to easily support integration of new methods.Implementations are located in mdbench/algorithms/.Methods under sr are general-purpose model discovery algorithms applicable to both ODE and PDE systems.The pde and ode directories contain methods specific to PDE and ODE datasets, respectively.See the main README.mdfor guidance on adding new methods to the benchmark.</p>
<p>The command ./install.shcreates a separate Conda environment for each algorithm and installs all required dependencies.The script run.shserves as the main entry point for training.For example, ./run.sh --algorithm operon --data type ode --dataset path data/ --n jobs 20 launches 20 parallel processes, trains the Operon method on all ODE datasets in the data directory, and stores results in [method name]-[data type].jsonlfiles.Use ./run.sh -h to see all available options.</p>
<p>Datasets are stored in compressed NumPy format (.npz).Filenames indicate the dataset name and the SNR level for noisy variants.Clean derivatives, computed from the ground-truth differential equations for ODEs, and via finite differences for PDEs, are included to enable evaluation of predictive accuracy.</p>
<p>B Datasets</p>
<p>Table A1 provides an overview of PDE systems in the benchmark.The newly generated datasets require the legacy FEniCs package (Alnaes et al. 2015), which can be readily installed on Ubuntu-based systems.Please refer to the README in the /scripts subfolder of the MDBench codebase for additional guidance on using FEniCS to generate the datasets, as well as the dataset-specific instructions below.</p>
<p>B.1 Advection</p>
<p>The advection system, adopted from (Takamoto et al. 2022), is a simple linear system in 1D space with the equation
∂u(t, x) ∂t + β ∂(t, x) ∂x = 0,
where β = 0.1 denotes constant advection speed.The boundary condition is periodic and the initial condition is a superposition of sinusoidal waves
u(0, x) = ki=k1,k2 A i sin(k i x + ϕ i ),
where k i = 2πn i are wave numbers with n i being random integer numbers between [1, 8], and A i and ϕ i are floating point numbers uniformly chosen between [0, 1] and (0, 2ϕ), respectively.</p>
<p>B.2 Burgers</p>
<p>The Burgers equation, from (Takamoto et al. 2022), models the diffusion process in fluid dynamics as
∂u(t, x) ∂t + u(t, x) ∂(t, x) ∂x − ν ∂ 2 u(t, x) ∂x 2 = 0,
where ν = 0.1 is the constant diffusion coefficient.</p>
<p>The initial and boundary conditions are similar to the Advection system.</p>
<p>B.3 Korteweg-de Vries (KdV)</p>
<p>The KdV system, taken from (Brunton, Proctor, and Kutz 2016), models unidirectional propagation of smallamplitude waves in shallow water.The governing equation is given by
∂u(t, x) ∂t + 6u(t, x) ∂u(t, x) ∂x + ∂ 3 u(t, x) ∂x 3 = 0,
where the initial condition is constructed from two KdV solutions of different amplitudes in order to distinguish the solution from the standard advection equation.The initial condition is a superposition of squared hyperbolic secant equations
u(0, x) = ci=1,5 c i 2 sech 2 √ 2 2 (x − x 0 ) ,
where c i are the speeds at which the waves travel.</p>
<p>B.4 Kuramoto Sivishinky (KS)</p>
<p>The KS equation is known for modeling the dynamics of various physical systems such as fluctuations in fluid films on inclines (Brunton, Proctor, and Kutz 2016).This equation includes a fourth-order diffusion term as a diffusive regularization of non-linear dynamics.The system is governed by the equation
∂u(t, x) ∂t = −u(t, x) ∂u(t, x) ∂x − ∂ 2 u(t, x) ∂x 2 − ∂ 4 u(t, x) ∂x 4 .</p>
<p>B.5 Nonlinear Schrödinger (NLS)</p>
<p>The NLS system appears in optics such as studying nonlinear wave propagation optical fibers.The complex-valued equation follows Since many of the model discovery implementations do not support complex numbers, the complex-valued system is decomposed into a real-valued, coupled PDE system
∂u(t, x) ∂t = 0.5i ∂ 2 (t, x) ∂ 2 x + i|u(t, x)| 2 u(t, x∂a(t, x) ∂t = −0.5 ∂ 2 b(t, x) ∂x 2 − a 2 (t, x)b(t, x) − b 3 (t, x), ∂b(t, x) ∂t = 0.5 ∂ 2 a(t, x) ∂x 2 + a(t, x)b 2 (t, x) + a 3 (t, x).</p>
<p>B.6 Reaction Diffusion</p>
<p>Diffusion reaction systems are used extensively to study pattern forming systems in physics (Brunton, Proctor, and Kutz 2016).One particular class of DR systems with nonlinear coupling terms are governed by the coupled PDE
∂u ∂t = 0.1∇ 2 u + λ(A)u − ω(A)v ∂v ∂t = 0.1∇ 2 v + ω(A)u + λ(A)v A = u 2 + v 2 , ω = −βA 2 , λ = 1 − A 2 ,
where β = 1 is a coefficient that controls the strength of the nonlinear coupling between the u and v fields.</p>
<p>B.7 Advection Diffusion</p>
<p>The Advection Diffusion system, adopted from (Both et al. 2021), appears in heat transfer, pollution transport, and chemical physics, to name a few.We study the 2D Advection Diffusion system
∂u ∂t = −∇ • (−D∇u + v.u),
where v = (0.25, 0.5) is the constant velocity vector describing the advection, and D = 0.5 denotes the diffusion coefficient.</p>
<p>B.8 Heat (Solar)</p>
<p>The Heat (Solar) dataset from (Logg, Mardal, and Wells 2012) simulates the heating of the earth's surface by solar radiation.For a given dimension d ∈ {1, 2, 3}, a rectangular domain ω is defined with x d−1 = 0 representing the earth's surface.Using default settings, the domain will be 1.5m in depth and 0.75m in width (for 2D and 3D problems).In the 3D case, we sample the first 20 time points.Formally, the Heat (Solar) system is the solution to the following initial-boundary value problem:
ρc ∂u ∂t = κ∇ 2 u u(x 0 , . . . , 0) = T (t) ∂u ∂x i = 0 for i ̸ = d − 1 T = u 0
The function T (t) = T R + T A sin(ωt) represents surface temperature boundary condition.The parameters of the model were chosen to be representative of the earth's surface:</p>
<p>• T R = 10 • C, the reference temperature;</p>
<p>• T A = 10 • C, the amplitude of temperature variation;</p>
<p>• ω = 7.27 × 10 −5 Hz, the frequency of temperature variation, ≈ 2π/86, 400, the number of seconds in one day; • ρ = 1500 kg/m 3 , soil density; • c = 1600 N•m/kg, heat capacity; and • κ = 2.3 N/K•s, thermal conductivity.The model captures the temperature variations in the soil at five minute intervals over two days, resulting in 576 timesteps.The mesh is uniformly spaced with 50 subdivisions in 1D, of size 50 × 50 grid in 2D, and of size 50×50×10 in 3D.The dataset can be reproduced by running the script fenics heat soil uniform.py.</p>
<p>B.9 Heat (Laser)</p>
<p>The Heat (Laser) dataset from (Abali 2016) simulates the dynamics of heat transfer through a rigid body with a custom heat source.In this dataset, the heat supply comes from a moving laser beam that is used to heat a 3D plate in a concentrated manner.The power of the laser follows
L(t, x, y) = P e −k (x− l 2 (1+ 1 2 sin(2πτ )) 2 +(y−v L t) 2
, where P = 3, 000 W/Kg represents the power of the laser beam and k = 5 × 10 4 denotes a large coefficient used to make the beam effect local.The laser moves sinusoidally along x with a time parameter τ = t/t end and linearly along y with a constant speed v L = 0.02 m/s.The temperature dynamics follows the equation
ρc ∂T ∂t − κ∇ 2 T − ρL = 0,
where T (t, x, t) is the temperature of the plate, c = 624J/kg denotes heat capacity, κ = 30.1W/mrefers to thermal conductivity, and ρ = 7860kg/m 3 is mass density of steel.The dataset can be reproduced from the script fenics heat3d laser.py.The original dataset simulates the dynamics for t ∈ [0, 50], ∆t = 0.1.We sample the first 20 time points in our analysis.</p>
<p>B.10 Navier-Stokes Channel (NS Channel)</p>
<p>The incompressible Navier-Stokes equations model the motion of viscous fluids.The NS Channel dataset, adopted from (Langtangen and Logg 2017), simulates a liquid traveling through a 2D channel along the x direction.The dynamics follow the system of equations
ρ ∂u ∂t + (u • ∇)u = ∇ • σ(u, p) ∇ • u = 0,</p>
<p>B.11 Navier-Stokes Cylinder (NS Cylinder)</p>
<p>The NS Cylinder dataset is another example of a Navier-Stokes system as published in the FEATFLOW Benchmark Suite (Turek and Becker 1998).The system records the velocity and pressure generated by flow through a 2D rectan-gular channel 2.2 m in length and 0.41 m in width, with a 5 cm wide cylinder obstruction centered at xy-coordinates (0.2, 0.2).In this system, the fluid density ρ = 1 and the kinematic velocity µ = 0.001.With a maximum velocity of 1.5 m/s, the system has a Reynolds number Re = 100, which results in turbulent flow, with vortex shedding beyond the cylinder.The unstructured mesh for the NS Cylinder dataset was generated using the mshr subpackage of FEniCS (Alnaes et al. 2015).The dataset simulates the system over 5 seconds, saving the data at 1,000 Hz for a total of 5,000 timesteps.The dataset can be reproduced from the script fenics navier stokes cylinder.py.Please note that the dataset must be converted into a structured mesh on a uniform grid prior to use for model discovery.The structured data can be generated from the unstructured version using the ns cylinder structured.pyscript.In our analysis, we subsample a rectangular grid in front of the cylinder (Figure A6) in space and t ∈ [1, 2], ∆t = 0.02 in time.</p>
<p>B.12 Diffusion-Reaction Cylinder</p>
<p>The Diffusion-Reaction Cylinder dataset utilizes the velocity component v of the solution to the NS Cylinder system to simulation an advection-diffusion-reaction system as detailed in (Langtangen and Logg 2017).Two chemical reagents A and B are carried by the fluid flow, i.e., advected, around the the cylinder and react to product chemical C, which is then disbursed downstream.The dynamical system of the chemical reactions is:
∂u A ∂t + v • ∇u A − ∇ • (ϵ∇u A ) = f A − Ku a u b ∂u B ∂t + v • ∇u B − ∇ • (ϵ∇u B ) = f B − Ku a u b ∂u C ∂t + v • ∇u C − ∇ • (ϵ∇u C ) = f C + Ku a u b − Ku C
The functions u A , u B , and u C represent the concentration of chemicals A, B, and C respectively at a given time and xycoordinate.f A and f B represent the source terms for chemicals A and B, which enter the domain at opposite sides (see Figures A7 and A8).The parameter ϵ = 0.01 corresponds to the domain diffusivity, while K = 10 is the reaction rate.The Diffusion-Reaction Cylinder is simulated for 5.0 seconds (matching the NS Cylinder velocity solution), saving the data at 100 Hz for a total of 500 timesteps.The unstructured solution can be regenerated using the script fenics diffusion reaction cylinder.py,from which the structured version can be generated using the diffusion reaction cylinder structured.pyscript.We subsample the data both in time and space the same way as we did in the NS Cylinder system.</p>
<p>C Methods and Hyperparameter Optimization</p>
<p>This section provides a brief overview of the algorithms along with their corresponding hyperparameter sets.Tables A2 and A3 list the hyperparameters used for each method.SINDy (Brunton, Proctor, and Kutz 2016) constructs a library of candidate functions (e.g., polynomials, trigonometric functions), evaluated on the state variables over time, and applies LASSO (ℓ 1 )-regularized regression to identify a sparse set of terms that best represent an ODE system.PDEFind (Rudy et al. 2017)</p>
<p>D Additional Experimental Results</p>
<p>This section provides more detailed results on the runtime and performance of the the evaluated methods on ODE and PDE systems.</p>
<p>D.1 ODE Systems</p>
<p>Figure A10a presents aggregated performance of model discovery methods on all ODE systems, as measured by NMSE of predicted derivatives on the test set.While LM tend to discover less complex equations as the noise level increases, GP methods and ODEFormer exhibit the opposite trend.Figures A12 and A13 provide more detailed metrics on how different noise levels affect the quality of equations for each dataset.The dataset names are described in Table A4.For more details about the equations and initial values, refer to (d'Ascoli et al. 2024) or scripts/strogatz ode.py in the MD-Bench project repository.</p>
<p>As we discuss in the Section 4.3, no established equation similarity metric exists to enable automatic assessment of equation fidelity.Therefore, in order to have a closer look at the fidelity of generated equations to the ground truth dynamics, we manually compare the discovered and true equations for ODE datasets in noise-free settings.Tables A5 and  A6 show the fidelity of the discovered equations along with their NMSE and complexity on ODE systems.</p>
<p>LM and GP methods have a higher successful discovery rate compared to other evaluated methods.It can be observed that EQL is the most unstable method with the highest rate of failure.Both EQL and End2end were not able to accurately discover any of the ODE systems in the clean settings.The inaccuracy of End2end method likely stems from the lack of dynamical system examples in the datasets that the underlying model was pre-trained on.On the other hand, ODEFormer, while having a similar architecture to End2end, achieves a higher successful rediscovery rate.This suggests the necessity of including representative systems in the pre-training datasets for LSPT methods.A promising direction for improving the LSPT methods is to extend End2end and ODEFormer architectures to PDE systems, where a single transformer model is jointly trained on synthetic ODE and PDE systems.</p>
<p>Figure A11 shows the average and 95% confidence interval for training time of each method on ODE datasets across different noise levels.The DL based methods -EQL and uDSR -possess the longest training time, whereas SINDy has the fastest execution time, taking less than a second for each ODE dataset.</p>
<p>D.2 PDE Systems</p>
<p>Figures A14 and A15 presents the NMSE and complexity of discovered equations on each PDE dataset across varying noise levels.Generally, LM methods such as WSINDy achieved a lower prediction error compared to GP methods.The training runtimes on the aforementioned datasets are provided in Table A7.</p>
<p>Figure 1 :
1
Figure 1: A schematic overview of MDBench pipeline.</p>
<p>Figure 2 :
2
Figure 2: Performance box plot for representative MD methods on ODE and PDE datasets.</p>
<p>Figure A1 :
A1
Figure A1: The state of the Heat (Solar) 2D system at t = 360.</p>
<p>Figure A2 :
A2
Figure A2: The state of the Heat (Laser) system at t = 40.</p>
<p>Figure A3 :
A3
Figure A3: The state of the NS Channel system at t = 10.</p>
<p>Figure A4 :
A4
Figure A4: Velocity of the NS Cylinder system at t = 999.</p>
<p>Figure A5 :
A5
Figure A5: Velocity of the NS Cylinder system at t = 4, 999.</p>
<p>Figure A6 :
A6
Figure A6: Geometry for the NS Cylinder and RD Cylinder systems along with the subsampling grid.</p>
<p>Figure A7 :
A7
Figure A7: Concentration of A in the Diffusion-Reaction Cylinder system at t = 199.</p>
<p>Figure A8 :
A8
Figure A8: Concentration of B in the Diffusion-Reaction Cylinder system at t = 199.</p>
<p>Figure A9 :
A9
Figure A9: Concentration of C in the Diffusion-Reaction Cylinder system at t = 199.</p>
<p>Figure A12 :
A12
Figure A12: Performance of SINDy, ESINDy, PySR, and Operon on ODE datasets: complexity of the discovered equations and the NMSE of time derivatives on the test set.</p>
<p>Table 1 :
1
Comparison of existing benchmarks for scientific MD.#ODE refers to the number of ODE problems, #PDE refers to the number of PDE problems, #MD refers to the number of model discovery algorithms evaluated, and Noise indicates whether robustness to noise is studied.</p>
<h1>ODE #PDE #MD NoiseSRBench 1.07014✓SRBench 2.00025✓PDEBench0110(Gilpin 2021)13104cp3-bench7012✓CFDBench040(Otness et al. 2021)040MDBench (ours)631412✓</h1>
<p>Table 2 :
2
Overview of model discovery methods benchmarked in MDBench.
ure 2b shows the aggregate NMSE and complexity of rep-resentative methods over a selected subset of PDE datasetsfor which all methods completed successfully (refer to Fig-
ure A10b for all methods).The selected datasets include Advection, Burgers, KdV, KS, Heat (Solar) 1D, and AD.</p>
<p>Table 3 :
3
Performance of MD methods on PDE datasets, measured by the NMSE of the time derivatives on the test set.Equation complexity is given in parenthesis.Reported NMSEs are rounded to the nearest power of 10.A check mark (✓) indicates successful identification of the full equation, with all coefficients being in ±5% of ground truth.Underlined entries denote partially correct equations, where only one term is missing or extra, and the remaining terms have coefficients that are within ±5% of ground truth.Empty entries (-) show method failure or timeout.The table includes the average and standard deviation of method runtimes on clean data (in minutes).
0 (10)10 0 (15)10 0 (3)</p>
<p>Table A1 :
A1
).An overview of PDE datasets properties, where N d denotes number of state variables, N t represents number of time points, ∆t shows time steps, N s is spatial resolution, and Ω represents the spatial domain.Note that spacing between time points and spatial grid along each coordinate is uniformly spaced.
DatasetN d N t∆tN sΩAdvection12010.011024x ∈ [0.00049, 0.9995]Burgers11010.1256x ∈ [−8, 8)KdV12010.1512x ∈ [−30, 30)KS12510.41024x ∈ [0.098, 100.53]AD1610.151 × 51x ∈ [−5, 5], y ∈ [−5, 5]Heat (Solar) 1D157630051x ∈ [−1.5, 0]Heat (Solar) 2D157630051 × 51x ∈ [−0.375, 0.375], y ∈ [−1.5, 0]Heat (Solar) 3D12030051 × 51 × 11 x ∈ [−0.375, 0.375], y ∈ [−0.375, 0.375], z ∈ [−1.5, 0]Heat (Laser)1200.1201 × 201 × 3x ∈ [0, 0.1], y ∈ [0, 0.1], z ∈ [0, 0.001]NLS2251 0.0126256x ∈ [−5, 5)RD2100 0.10132 × 32x ∈ [−10, 10), y ∈ [−10, 10)NS (Channel)2500.029 × 9x ∈ [0.25, 0.75], y ∈ [0.25, 0.75]NS (Cylinder)3510.02100 × 30x ∈ [0.441, 1.316], y ∈ [0.084, 0.326]RD (Cylinder)6510.02100 × 30x ∈ [0.441, 1.316], y ∈ [0.084, 0.326]</p>
<p>extends SINDy to PDEs by evaluating the candidate library on a spatiotemporal grid.WSINDy (Messenger and Bortz 2021) is another linear method for PDE discovery that avoids explicit pointwise derivative estimation by leveraging the weak form of the governing equations.ESINDy and EWSINDy (Fasel et al. 2022) extend SINDy and WSINDy, respectively, using bootstrap aggregation for improved robustness on ODE and PDE systems.All SINDy-based methods are implemented using the PySINDy library https://pysindy.readthedocs.io/en/latest/.DeepMoD (Both et al. 2021) is a linear approach that integrates a neural network to simultaneously learn the underlying function, its derivatives, and the coefficients of a predefined candidate library.Bayesian (More et al. 2023) builds on PDEFind by incorporating variational Bayes and a spike-and-slab prior for sparse model selection.Its implementation is available at https://github.com/TapasTripura/Bayesian-Discovery-of-PDEs.Ascoli et al. 2024) extends End2end to ODE systems.Unlike most generic model discovery methods, ODEFormer does not rely on derivative estimates and instead takes state trajectories and timestamps as input.Its implementation is available at https://github.com/sdascoli/odeformer.
PySR (Cranmer 2023) is a genetic programming methodwith an optimized Julia backend, enabling high-performancemodel discovery through a parallel population-based evolve-simplify-optimize loop. See the official repository at https://github.com/MilesCranmer/PySR. Operon (Burlacu, Kro-nberger, and Kommenda 2020) is another efficient geneticprogramming method, implemented in C++ with vectorizedoperations and a compact linear tree representation. Its code-base is available at https://github.com/heal-research/operon.EQL (Martius and Lampert 2016) is a deep learning-based method that embeds symbolic operations (e.g., addi-tion, cosine) into the activation functions of a shallow neu-ral network. After training, the network weights correspondto the coefficients of the discovered equation. We use theimplementation available at https://github.com/cavalab/eql.uDSR (Landajuela et al. 2022) provides a unified frame-work that integrates mutiple approaches, from linear mod-els to genetic programming, for model discovery. Its codeis available at https://github.com/dso-org/deep-symbolic-optimization.End2end (Kamienny et al. 2022) is a general-purposemodel discovery approach based on a sequence-to-sequenceTransformer architecture, pre-trained on large datasets ofsynthetic equations. The implementation can be foundat https://github.com/facebookresearch/symbolicregression.ODEFormer (d'
https://cavalab.org/srbench/
https://github.com/trevorstephens/gplearn
10 1 10 1 10 3 10 5 10 7 10 9 NMSE#models[10,20,50] (In addition to SINDy subset ratio [0.5, 0.7, 0.9] and WSINDy hyperparamters) inclusion prob.threshold [0.2, 0.3, 0.4, 0.5] Bayesian threshold np.logspace(-7, 0, 16) basis functions[polynomial] derivative order 1, 2, 3, 4 polynomial order 1, 2, 3, 4 alpha 10 −5 , 10 −4 tolerance 10 −4 , 10 −3 , 10 −2 pip 0.5, 0.7, 0.9 DeepMoD derivative order 1, 2, 3, 4 polynomial order 1, 2, 3, 4 hidden layers[[50, 50, 50, 50]
Computational reality. B E Abali, Springer, G S I Aldeia, H Zhang, G Bomarito, M Cranmer, A Fonseca, B Burlacu, La Cava, arXiv:2505.03977Call for Action: towards the next generation of symbolic regression benchmark. 2016arXiv preprint</p>
<p>M Alnaes, J Blechta, J Hake, A Johansson, B Kehlet, A Logg, C Richardson, J Ring, M E Rognes, G N Wells, The FEniCS project version 1.5. Archive of numerical software. 20153</p>
<p>M Baer, findiff Software Package. 2018</p>
<p>DeepMoD: Deep learning for model discovery in noisy data. L Biggio, T Bendinelli, A Neitz, A Lucchi, G Parascandolo, G.-J Both, S Choudhury, P Sens, R Kusters, International Conference on Machine Learning. 2021. 2021428109985Neural symbolic regression that scales</p>
<p>Dynamic models of large-scale brain activity. M Breakspear, Nature neuroscience. 2032017</p>
<p>Discovering governing equations from data by sparse identification of nonlinear dynamical systems. S L Brunton, J L Proctor, J N Kutz, Proceedings of the national academy of sciences. the national academy of sciences2016113</p>
<p>. B Burlacu, G Kronberger, M Kommenda, 2020</p>
<p>Operon C++ an efficient genetic programming framework for symbolic regression. Proceedings of the 2020 Genetic and Evolutionary Computation Conference Companion. the 2020 Genetic and Evolutionary Computation Conference Companion</p>
<p>Interpretable machine learning for science with PySR and SymbolicRegression. M Cranmer, S Ascoli, S Becker, P Schwaller, A Mathis, N Kilbertus, arXiv:2305.01582The Twelfth International Conference on Learning Representations. 2023. 2024jl. arXiv preprintODEFormer: Symbolic Regression of Dynamical Systems with Transformers</p>
<p>Ensemble-SINDy: Robust sparse model discovery in the low-data, high-noise limit, with active learning and control. U Fasel, J N Kutz, B W Brunton, S L Brunton, Proceedings of the Royal Society A. 478202109042022. 2260</p>
<p>Practical methods of optimization. R Fletcher, 2000John Wiley &amp; Sons</p>
<p>Chaos as an interpretable benchmark for forecasting and data-driven modelling. W Gilpin, Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2021</p>
<p>End-to-end symbolic regression with transformers. P Kamienny, S -A.; D'ascoli, G Lample, F Charton, Advances in Neural Information Processing Systems. 202235</p>
<p>Integration of neural network-based symbolic regression in deep learning for scientific discovery. S Kim, P Y Lu, S Mukherjee, M Gilbert, L Jing, V Čeperić, M Soljačić, IEEE transactions on neural networks and learning systems. 202032</p>
<p>Genetic programming as a means for programming computers by natural selection. C Kleinstreuer, Springer, J R Koza, Statistics and computing. 42018. 1994Modern fluid dynamics</p>
<p>Contemporary symbolic regression methods and their relative performance. Advances in neural information processing systems. H Kragh, W La Cava, B Burlacu, M Virgolin, M Kommenda, P Orzechowski, F O De Franc ¸a, Y Jin, J H Moore, 2021. 2021Princeton University Press1New JerseyCosmology and Controversy : The Historical Development of Two Theories of the Universe</p>
<p>A unified framework for deep symbolic regression. M Landajuela, C S Lee, J Yang, R Glatt, C P Santiago, I Aravena, T Mundhenk, G Mulcahy, B K Petersen, Advances in Neural Information Processing Systems. 202235</p>
<p>Solving PDEs in Python: The FEniCS Tutorial I. H P Langtangen, A Logg, 2017Springer Nature</p>
<p>Set Transformer: A Framework for Attentionbased Permutation-Invariant Neural Networks. J Lee, Y Lee, J Kim, A Kosiorek, S Choi, Y W Teh, Proceedings of the 36th International Conference on Machine Learning. K Chaudhuri, R Salakhutdinov, the 36th International Conference on Machine LearningPMLR2019Proceedings of Machine Learning Research</p>
<p>Fourier Neural Operator for Parametric Partial Differential Equations. Z Li, N B Kovachki, K Azizzadenesheli, B Liu, K Bhattacharya, A Stuart, A Anandkumar, International Conference on Learning Representations. 2021</p>
<p>Automated solution of differential equations by the finite element method: The FEniCS book. A Logg, K.-A Mardal, G Wells, Y Luo, Y Chen, Z Zhang, G Martius, C H Lampert, M Merler, K Haitsiukevich, N Dainese, P Marttinen, arXiv:2310.05963Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. X Fu, E Fleisig, the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational Linguistics2012. 2023. 2016. 202484arXiv preprintCFDBench: A Large-Scale Benchmark for Machine Learning Methods in Fluid Dynamics</p>
<p>Weak SINDy for partial differential equations. D A Messenger, D M Bortz, Journal of Computational Physics. 4431105252021</p>
<p>SymPy: symbolic computing in Python. A Meurer, C P Smith, M Paprocki, O Čertík, S B Kirpichev, M Rocklin, A Kumar, S Ivanov, J K Moore, S Singh, T Rathnayake, S Vig, B E Granger, R P Muller, F Bonazzi, H Gupta, S Vats, F Johansson, F Pedregosa, M J Curry, A R Terrel, V Roučka, A Saboo, I Fernando, S Kulal, R Cimrman, A Scopatz, PeerJ Computer Science. 3e1032017</p>
<p>Language model crossover: Variation through few-shot prompting. E Meyerson, M J Nelson, H Bradley, A Gaier, A Moradi, A K Hoover, J Lehman, ACM Transactions on Evolutionary Learning. 442024</p>
<p>. K S More, T Tripura, R Nayek, S Chakraborty, </p>
<p>A Bayesian framework for learning governing partial differential equation from data. Physica D: Nonlinear Phenomena. 456133927</p>
<p>Symbolic regression via deep reinforcement learning enhanced genetic programming seeding. T Mundhenk, M Landajuela, R Glatt, C P Santiago, B K Petersen, Advances in Neural Information Processing Systems. 202134</p>
<p>A bayesian approach for spatio-temporal data-driven dynamic equation discovery. J S North, C K Wikle, E M Schliep, Bayesian Analysis. 2022025</p>
<p>An Extensible Benchmark Suite for Learning to Simulate Physical Systems. K Otness, A Gjoka, J Bruna, D Panozzo, B Peherstorfer, T Schneider, D Zorin, Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2021</p>
<p>Deep symbolic regression: Recovering mathematical expressions from data via risk-seeking policy gradients. B K Petersen, M L Larma, T N Mundhenk, C P Santiago, S K Kim, J T Kim, International Conference on Learning Representations. 2021</p>
<p>Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. M Raissi, P Perdikaris, G E Karniadakis, Journal of Computational physics. 3782019</p>
<p>U-net: Convolutional networks for biomedical image segmentation. O Ronneberger, P Fischer, T Brox, Medical image computing and computer-assisted intervention-MICCAI 2015: 18th international conference. Munich, GermanySpringer2015. October 5-9, 2015proceedings, part III 18</p>
<p>. S H Rudy, S L Brunton, J L Proctor, J Kutz, </p>
<p>Data-driven discovery of partial differential equations. Science advances. 34e1602614</p>
<p>Learning equations for extrapolation and control. S Sahoo, C Lampert, G Martius, International Conference on Machine Learning. Pmlr2018</p>
<p>A computational tool for comparing all linear PDE solvers: Error-optimal methods are meshless. R Schaback, Advances in Computational Mathematics. 4122015</p>
<p>Transformer-based planning for symbolic regression. P Shojaee, K Meidani, A Barati Farimani, C Reddy, Advances in Neural Information Processing Systems. 202336</p>
<p>LLM-SR: Scientific Equation Discovery via Programming with Large Language Models. P Shojaee, K Meidani, S Gupta, A B Farimani, C K Reddy, The Thirteenth International Conference on Learning Representations. 2025</p>
<p>S H Strogatz, Nonlinear dynamics and chaos: with applications to physics, biology, chemistry, and engineering. Chapman and Hall/CRC2024</p>
<p>Pdebench: An extensive benchmark for scientific machine learning. M Takamoto, T Praditia, R Leiteritz, D Mackinlay, F Alesiani, D Pflüger, M Niepert, Advances in Neural Information Processing Systems. 202235</p>
<p>cp3-bench: a tool for benchmarking symbolic regression algorithms demonstrated with cosmology. M Thing, S Koksbang, Journal of Cosmology and Astroparticle Physics. 01402025</p>
<p>S Turek, C Becker, FEATFLOW DFG Benchmark 2D-2. 1998</p>
<p>Attention is all you need. Advances in neural information processing systems. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, Ł Kaiser, I Polosukhin, 201730</p>
<p>Dynamical systems and evolution equations: theory and applications. J A Walker, 2013Springer Science &amp; Business Media20</p>
<p>Machine discovery of partial differential equations from spatiotemporal data: A sparse Bayesian learning framework. Y Yuan, X Li, L Li, F J Jiang, X Tang, F Zhang, J Goncalves, H U Voss, H Ding, J Kurths, Chaos: An Interdisciplinary Journal of Nonlinear Science. 11332023</p>            </div>
        </div>

    </div>
</body>
</html>
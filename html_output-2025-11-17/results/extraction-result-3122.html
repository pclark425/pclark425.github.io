<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3122 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3122</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3122</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-73.html">extraction-schema-73</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <p><strong>Paper ID:</strong> paper-af46b5ee6d0c1aada1c482d53018a50909aa4c90</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/af46b5ee6d0c1aada1c482d53018a50909aa4c90" target="_blank">Impact of Pretraining Term Frequencies on Few-Shot Reasoning</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> Overall, although LMs exhibit strong performance at few-shot numerical reasoning tasks, the results raise the question of how much models actually generalize beyond pretraining data, and researchers are encouraged to take thepretraining data into account when interpreting evaluation results.</p>
                <p><strong>Paper Abstract:</strong> Pretrained Language Models (LMs) have demonstrated ability to perform numerical reasoning by extrapolating from a few examples in few-shot settings. However, the extent to which this extrapolation relies on robust reasoning is unclear. In this paper, we investigate how well these models reason with terms that are less frequent in the pretraining data. In particular, we examine the correlations between the model performance on test instances and the frequency of terms from those instances in the pretraining data. We measure the strength of this correlation for a number of GPT-based language models (pretrained on the Pile dataset) on various numerical deduction tasks (e.g., arithmetic and unit conversion). Our results consistently demonstrate that models are more accurate on instances whose terms are more prevalent, in some cases above $70\%$ (absolute) more accurate on the top 10\% frequent terms in comparison to the bottom 10\%. Overall, although LMs exhibit strong performance at few-shot numerical reasoning tasks, our results raise the question of how much models actually generalize beyond pretraining data, and we encourage researchers to take the pretraining data into account when interpreting evaluation results.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3122.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3122.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-J-6B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-J-6B (EleutherAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 6-billion-parameter autoregressive Transformer language model from EleutherAI, pretrained on the Pile (800GB) and evaluated in this paper for few-shot numerical reasoning (addition, multiplication, operation-inference, and time-unit conversion).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-J-6B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive Transformer (EleutherAI/GPT family), ~6B parameters, pretrained on the Pile dataset (800GB). Evaluated via HuggingFace integration; numerical token counts for integers (<7 digits) extracted from the Pile with whitespace tokenization. Co-occurrence counted within a 5-word window.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>2-digit addition and multiplication (first operand <100, second operand 1-50), operation-inference (operation replaced with '#'), and time-unit conversions (e.g., hours→minutes). Test sets: 5000 instances for addition/multiplication/operation-inference; 79–100 instances for various time-unit conversions.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Model performance on arithmetic appears driven substantially by memorization / pattern-matching to pretraining co-occurrence statistics (unigram and low-order co-occurrence) rather than robust algorithmic arithmetic reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Strong positive correlation between pretraining term frequency (ω) and model accuracy (plots and binned averages); large performance gaps Δ between top-10% and bottom-10% frequency terms (e.g., Δ up to ~90 percentage points for some definitions); performance improvements with more few-shot examples concentrate on frequent instances; co-occurrence with the true answer (ω_{x1,y}) shows growing gap as shots increase.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>A few simple tasks (e.g., decade→year where answer is appending '0') show near-perfect generalization with small performance gap at higher shot counts, indicating that when the mapping is trivial the model can generalize; authors do not make causal claims and acknowledge possible confounders.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Prompting / few-shot in-context examples (k = 0, 2, 4, 8, 16); random sampling of demonstration examples averaged over 5 seeds. No fine-tuning, calculator/tool use, chain-of-thought, or targeted representational interventions applied.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Increasing shot count raises overall accuracy on tasks but typically increases the performance gap between high-frequency and low-frequency terms (i.e., improvements concentrate on frequent/pretrained instances). Example: multiplication accuracy increases from 5.4% (k=0) to 35.9% (k=2) and 42.9% (k=8) while Δ_{1,y} grows from 30.8 to 89.9 then 86.0 for respective k.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported (GPT-J-6B, selected values from Table 2 and Table 3): Multiplication acc: k=0 5.4%, k=2 35.9%, k=4 39.2%, k=8 42.9%, k=16 40.9%. Multiplication performance gaps (ω_{x1} Δ1, ω_{x1,x2} Δ1,2, ω_{x1,y} Δ1,y): k=2 Δ1=77.6, Δ1,2=79.3, Δ1,y=89.9 (percentage-point differences between top10% and bottom10%). Addition acc: k=2 88.2%, k=4 91.4%, k=8 89.6%, k=16 88.6%; addition Δ1 values are much smaller (e.g., k=2 Δ1=16.8). Operation-inference (multiplication '#') acc low (k=2 3.1%, k=16 11.0%) with sizable Δs (e.g., k=16 Δ1=39.6). Time-unit conversions: varied; e.g., Year→Month acc: k=2 21.8% (Δ1,2=58.0), k=8 45.4% (Δ1,2=55.0), Decade→Year acc: k=8 99.6% with Δ approx 0. Metrics averaged over multiple sampled prompts and seeds; co-occurrence window size = 5 words; integer frequencies counted via whitespace tokenizer.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Large dependence on pretraining frequency: rare (but still relatively frequent in corpus) operands yield much lower accuracy; poor generalization on operation-inference tasks; smaller models and low-shot setups succeed primarily on frequent operands; improvements with more shots often amplify bias toward frequent instances; inability to attribute causal mechanism (confounders possible).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>No direct empirical comparison to humans or exact symbolic calculators; authors argue that a model that has learned arithmetic should be invariant to pretraining term frequency (i.e., match algorithmic/symbolic generalization), which is not observed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Impact of Pretraining Term Frequencies on Few-Shot Reasoning', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3122.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3122.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-Neo (1.3B/2.7B)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-Neo-1.3B and GPT-Neo-2.7B (EleutherAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Smaller EleutherAI autoregressive Transformer models (≈1.3B and 2.7B parameters) pretrained on the Pile and evaluated for the same arithmetic and conversion tasks; used to probe the effect of model size on frequency dependence.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-Neo-1.3B, GPT-Neo-2.7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive Transformer models (EleutherAI GPT-Neo family) with ~1.3B and ~2.7B parameters respectively, pretrained on the Pile. Evaluated identically to GPT-J-6B using same prompts, dataset construction, and pretraining-frequency statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Same set as GPT-J-6B: 2-digit+ multiplication/addition, operation-inference, time-unit conversion. Evaluated under few-shot prompting (k=2 and k=8 assessed for size comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Same overall mechanism observed: reliance on pretraining frequency / memorization and pattern-matching; smaller models show even stronger concentration of success on frequent terms.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Figures (6a, 6b) and experiments show smaller models are less accurate overall but still perform relatively well on frequent operands and poorly on rare ones; the binned accuracy vs frequency trend persists across sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>No countervailing evidence presented; smaller models' lower absolute performance could reflect capacity rather than qualitatively different mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Few-shot prompting (k varied, comparisons done for k=2 and k=8 for size study).</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Larger models benefit more from added shots in absolute accuracy, but all models show performance concentrated on frequent terms; smaller models improve less and remain reliant on frequent operands.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Aggregated/qualitative: smaller models have lower overall accuracy than GPT-J-6B on arithmetic tasks (plots show lower curves), but preserve positive correlation between operand frequency and accuracy. Exact numeric tables for these sizes are not detailed in the main tables but trends are shown in Figures 6a/6b.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Can only reliably solve frequent instances; capacity-limited and frequency-dependent generalization; same systematic bias toward pretraining frequency as larger model.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>No empirical human or symbolic baseline comparisons; authors note smaller models' failure to generalize as further evidence against robust algorithmic reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Impact of Pretraining Term Frequencies on Few-Shot Reasoning', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3122.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3122.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Memorization / Pattern-matching mechanism</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Memorization and high-order co-occurrence pattern matching from pretraining data</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper attributes much of the observed arithmetic performance to memorization and exploitation of low-order co-occurrence statistics (unigram and small-window co-occurrence) in the pretraining corpus rather than learned algorithmic arithmetic procedures.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>applies to evaluated EleutherAI GPT-family models (GPT-J-6B, GPT-Neo-1.3B, GPT-Neo-2.7B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Not a model but a proposed mechanism: models rely on token frequencies and co-occurrences in the Pile (counts ω_{X} for sets X of 1–3 terms within a 5-word window) to favor frequent formulae/answers.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Explains behavior across the arithmetic (add/mul), operation-inference, and time-unit conversion tasks examined.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Memorization/pattern-matching: LMs recall frequent tuples or phrase patterns from pretraining and probabilistically reproduce likely outputs (including direct memorized equation-answer co-occurrences), causing frequency-dependent accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Quantitative: large Δ (performance gaps) computed across frequency quantiles (Δ(Ω) = Acc(top10%) − Acc(bottom10%)); co-occurrence with answer (ω_{x1,y}) yields especially large Δs that grow with shots; trends hold across tasks and model sizes. Related citations: memorization literature (Feldman 2020, Zhang et al. 2021) discussed as consistent explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Authors acknowledge they do not prove causality and that simple algorithmic mappings (e.g., appending 0 for decade→year) can be learned and generalized, so not all arithmetic behavior is attributable solely to memorization. Confounders possible.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Measured effect under few-shot prompting; no targeted de-memorization or causal interventions performed in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Few-shot prompting increases accuracy but tends to amplify the model's reliance on memorized/frequent instances (increased performance gap). Authors recommend future causal interventions during training as further study.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Mechanism supported by numeric Δs (examples: multiplication Δ1,y up to ~89.9 percentage points at k=2 for GPT-J-6B; many Δs in 40–80+ range across tasks and shot settings) and by accuracy-vs-frequency plots (Figures 1,3,4,5).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Explains systematic failures on rare operands, operation-inference confusion, and shot-driven amplification of bias to frequent cases.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Authors contrast expected invariance of algorithmic/symbolic arithmetic (humans or calculators apply rules irrespective of operand frequency) with observed frequency dependence, arguing the latter is inconsistent with algorithmic arithmetic reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Impact of Pretraining Term Frequencies on Few-Shot Reasoning', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language models are few-shot learners <em>(Rating: 2)</em></li>
                <li>Analysing mathematical reasoning abilities of neural models <em>(Rating: 2)</em></li>
                <li>Injecting numerical reasoning skills into language models <em>(Rating: 2)</em></li>
                <li>Do NLP models know numbers? probing numeracy in embeddings <em>(Rating: 2)</em></li>
                <li>Are nlp models really able to solve simple math word problems? <em>(Rating: 1)</em></li>
                <li>Counterfactual memorization in neural language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3122",
    "paper_id": "paper-af46b5ee6d0c1aada1c482d53018a50909aa4c90",
    "extraction_schema_id": "extraction-schema-73",
    "extracted_data": [
        {
            "name_short": "GPT-J-6B",
            "name_full": "GPT-J-6B (EleutherAI)",
            "brief_description": "A 6-billion-parameter autoregressive Transformer language model from EleutherAI, pretrained on the Pile (800GB) and evaluated in this paper for few-shot numerical reasoning (addition, multiplication, operation-inference, and time-unit conversion).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-J-6B",
            "model_description": "Autoregressive Transformer (EleutherAI/GPT family), ~6B parameters, pretrained on the Pile dataset (800GB). Evaluated via HuggingFace integration; numerical token counts for integers (&lt;7 digits) extracted from the Pile with whitespace tokenization. Co-occurrence counted within a 5-word window.",
            "arithmetic_task_type": "2-digit addition and multiplication (first operand &lt;100, second operand 1-50), operation-inference (operation replaced with '#'), and time-unit conversions (e.g., hours→minutes). Test sets: 5000 instances for addition/multiplication/operation-inference; 79–100 instances for various time-unit conversions.",
            "reported_mechanism": "Model performance on arithmetic appears driven substantially by memorization / pattern-matching to pretraining co-occurrence statistics (unigram and low-order co-occurrence) rather than robust algorithmic arithmetic reasoning.",
            "evidence_for_mechanism": "Strong positive correlation between pretraining term frequency (ω) and model accuracy (plots and binned averages); large performance gaps Δ between top-10% and bottom-10% frequency terms (e.g., Δ up to ~90 percentage points for some definitions); performance improvements with more few-shot examples concentrate on frequent instances; co-occurrence with the true answer (ω_{x1,y}) shows growing gap as shots increase.",
            "evidence_against_mechanism": "A few simple tasks (e.g., decade→year where answer is appending '0') show near-perfect generalization with small performance gap at higher shot counts, indicating that when the mapping is trivial the model can generalize; authors do not make causal claims and acknowledge possible confounders.",
            "intervention_type": "Prompting / few-shot in-context examples (k = 0, 2, 4, 8, 16); random sampling of demonstration examples averaged over 5 seeds. No fine-tuning, calculator/tool use, chain-of-thought, or targeted representational interventions applied.",
            "effect_of_intervention": "Increasing shot count raises overall accuracy on tasks but typically increases the performance gap between high-frequency and low-frequency terms (i.e., improvements concentrate on frequent/pretrained instances). Example: multiplication accuracy increases from 5.4% (k=0) to 35.9% (k=2) and 42.9% (k=8) while Δ_{1,y} grows from 30.8 to 89.9 then 86.0 for respective k.",
            "performance_metrics": "Reported (GPT-J-6B, selected values from Table 2 and Table 3): Multiplication acc: k=0 5.4%, k=2 35.9%, k=4 39.2%, k=8 42.9%, k=16 40.9%. Multiplication performance gaps (ω_{x1} Δ1, ω_{x1,x2} Δ1,2, ω_{x1,y} Δ1,y): k=2 Δ1=77.6, Δ1,2=79.3, Δ1,y=89.9 (percentage-point differences between top10% and bottom10%). Addition acc: k=2 88.2%, k=4 91.4%, k=8 89.6%, k=16 88.6%; addition Δ1 values are much smaller (e.g., k=2 Δ1=16.8). Operation-inference (multiplication '#') acc low (k=2 3.1%, k=16 11.0%) with sizable Δs (e.g., k=16 Δ1=39.6). Time-unit conversions: varied; e.g., Year→Month acc: k=2 21.8% (Δ1,2=58.0), k=8 45.4% (Δ1,2=55.0), Decade→Year acc: k=8 99.6% with Δ approx 0. Metrics averaged over multiple sampled prompts and seeds; co-occurrence window size = 5 words; integer frequencies counted via whitespace tokenizer.",
            "notable_failure_modes": "Large dependence on pretraining frequency: rare (but still relatively frequent in corpus) operands yield much lower accuracy; poor generalization on operation-inference tasks; smaller models and low-shot setups succeed primarily on frequent operands; improvements with more shots often amplify bias toward frequent instances; inability to attribute causal mechanism (confounders possible).",
            "comparison_to_humans_or_symbolic": "No direct empirical comparison to humans or exact symbolic calculators; authors argue that a model that has learned arithmetic should be invariant to pretraining term frequency (i.e., match algorithmic/symbolic generalization), which is not observed.",
            "uuid": "e3122.0",
            "source_info": {
                "paper_title": "Impact of Pretraining Term Frequencies on Few-Shot Reasoning",
                "publication_date_yy_mm": "2022-02"
            }
        },
        {
            "name_short": "GPT-Neo (1.3B/2.7B)",
            "name_full": "GPT-Neo-1.3B and GPT-Neo-2.7B (EleutherAI)",
            "brief_description": "Smaller EleutherAI autoregressive Transformer models (≈1.3B and 2.7B parameters) pretrained on the Pile and evaluated for the same arithmetic and conversion tasks; used to probe the effect of model size on frequency dependence.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-Neo-1.3B, GPT-Neo-2.7B",
            "model_description": "Autoregressive Transformer models (EleutherAI GPT-Neo family) with ~1.3B and ~2.7B parameters respectively, pretrained on the Pile. Evaluated identically to GPT-J-6B using same prompts, dataset construction, and pretraining-frequency statistics.",
            "arithmetic_task_type": "Same set as GPT-J-6B: 2-digit+ multiplication/addition, operation-inference, time-unit conversion. Evaluated under few-shot prompting (k=2 and k=8 assessed for size comparisons).",
            "reported_mechanism": "Same overall mechanism observed: reliance on pretraining frequency / memorization and pattern-matching; smaller models show even stronger concentration of success on frequent terms.",
            "evidence_for_mechanism": "Figures (6a, 6b) and experiments show smaller models are less accurate overall but still perform relatively well on frequent operands and poorly on rare ones; the binned accuracy vs frequency trend persists across sizes.",
            "evidence_against_mechanism": "No countervailing evidence presented; smaller models' lower absolute performance could reflect capacity rather than qualitatively different mechanisms.",
            "intervention_type": "Few-shot prompting (k varied, comparisons done for k=2 and k=8 for size study).",
            "effect_of_intervention": "Larger models benefit more from added shots in absolute accuracy, but all models show performance concentrated on frequent terms; smaller models improve less and remain reliant on frequent operands.",
            "performance_metrics": "Aggregated/qualitative: smaller models have lower overall accuracy than GPT-J-6B on arithmetic tasks (plots show lower curves), but preserve positive correlation between operand frequency and accuracy. Exact numeric tables for these sizes are not detailed in the main tables but trends are shown in Figures 6a/6b.",
            "notable_failure_modes": "Can only reliably solve frequent instances; capacity-limited and frequency-dependent generalization; same systematic bias toward pretraining frequency as larger model.",
            "comparison_to_humans_or_symbolic": "No empirical human or symbolic baseline comparisons; authors note smaller models' failure to generalize as further evidence against robust algorithmic reasoning.",
            "uuid": "e3122.1",
            "source_info": {
                "paper_title": "Impact of Pretraining Term Frequencies on Few-Shot Reasoning",
                "publication_date_yy_mm": "2022-02"
            }
        },
        {
            "name_short": "Memorization / Pattern-matching mechanism",
            "name_full": "Memorization and high-order co-occurrence pattern matching from pretraining data",
            "brief_description": "The paper attributes much of the observed arithmetic performance to memorization and exploitation of low-order co-occurrence statistics (unigram and small-window co-occurrence) in the pretraining corpus rather than learned algorithmic arithmetic procedures.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "applies to evaluated EleutherAI GPT-family models (GPT-J-6B, GPT-Neo-1.3B, GPT-Neo-2.7B)",
            "model_description": "Not a model but a proposed mechanism: models rely on token frequencies and co-occurrences in the Pile (counts ω_{X} for sets X of 1–3 terms within a 5-word window) to favor frequent formulae/answers.",
            "arithmetic_task_type": "Explains behavior across the arithmetic (add/mul), operation-inference, and time-unit conversion tasks examined.",
            "reported_mechanism": "Memorization/pattern-matching: LMs recall frequent tuples or phrase patterns from pretraining and probabilistically reproduce likely outputs (including direct memorized equation-answer co-occurrences), causing frequency-dependent accuracy.",
            "evidence_for_mechanism": "Quantitative: large Δ (performance gaps) computed across frequency quantiles (Δ(Ω) = Acc(top10%) − Acc(bottom10%)); co-occurrence with answer (ω_{x1,y}) yields especially large Δs that grow with shots; trends hold across tasks and model sizes. Related citations: memorization literature (Feldman 2020, Zhang et al. 2021) discussed as consistent explanations.",
            "evidence_against_mechanism": "Authors acknowledge they do not prove causality and that simple algorithmic mappings (e.g., appending 0 for decade→year) can be learned and generalized, so not all arithmetic behavior is attributable solely to memorization. Confounders possible.",
            "intervention_type": "Measured effect under few-shot prompting; no targeted de-memorization or causal interventions performed in this study.",
            "effect_of_intervention": "Few-shot prompting increases accuracy but tends to amplify the model's reliance on memorized/frequent instances (increased performance gap). Authors recommend future causal interventions during training as further study.",
            "performance_metrics": "Mechanism supported by numeric Δs (examples: multiplication Δ1,y up to ~89.9 percentage points at k=2 for GPT-J-6B; many Δs in 40–80+ range across tasks and shot settings) and by accuracy-vs-frequency plots (Figures 1,3,4,5).",
            "notable_failure_modes": "Explains systematic failures on rare operands, operation-inference confusion, and shot-driven amplification of bias to frequent cases.",
            "comparison_to_humans_or_symbolic": "Authors contrast expected invariance of algorithmic/symbolic arithmetic (humans or calculators apply rules irrespective of operand frequency) with observed frequency dependence, arguing the latter is inconsistent with algorithmic arithmetic reasoning.",
            "uuid": "e3122.2",
            "source_info": {
                "paper_title": "Impact of Pretraining Term Frequencies on Few-Shot Reasoning",
                "publication_date_yy_mm": "2022-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 2
        },
        {
            "paper_title": "Analysing mathematical reasoning abilities of neural models",
            "rating": 2
        },
        {
            "paper_title": "Injecting numerical reasoning skills into language models",
            "rating": 2
        },
        {
            "paper_title": "Do NLP models know numbers? probing numeracy in embeddings",
            "rating": 2
        },
        {
            "paper_title": "Are nlp models really able to solve simple math word problems?",
            "rating": 1
        },
        {
            "paper_title": "Counterfactual memorization in neural language models",
            "rating": 1
        }
    ],
    "cost": 0.012211,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Impact of Pretraining Term Frequencies on Few-Shot Reasoning</h1>
<p>Yasaman Razeghi ${ }^{1}$ Robert L. Logan IV ${ }^{1}$ Matt Gardner ${ }^{2}$ Sameer Singh ${ }^{13}$</p>
<h4>Abstract</h4>
<p>Pretrained Language Models (LMs) have demonstrated ability to perform numerical reasoning by extrapolating from a few examples in few-shot settings. However, the extent to which this extrapolation relies on robust reasoning is unclear. In this paper, we investigate how well these models reason with terms that are less frequent in the pretraining data. In particular, we examine the correlations between the model performance on test instances and the frequency of terms from those instances in the pretraining data. We measure the strength of this correlation for a number of GPT-based language models (pretrained on the Pile dataset) on various numerical deduction tasks (e.g., arithmetic and unit conversion). Our results consistently demonstrate that models are more accurate on instances whose terms are more prevalent, in some cases above $70 \%$ (absolute) more accurate on the top $10 \%$ frequent terms in comparison to the bottom $10 \%$. Overall, although LMs exhibit strong performance at few-shot numerical reasoning tasks, our results raise the question of how much models actually generalize beyond pretraining data, and we encourage researchers to take the pretraining data into account when interpreting evaluation results.</p>
<h2>1. Introduction</h2>
<p>Large language models have demonstrated outstanding performance in zero- and few-shot learning settings on numerous tasks, from simple classifications such as sentiment analysis to complex reasoning-related task like natural language inference and arithmetic (Brown et al., 2020; Radford et al., 2019). These results suggest that models may have gained the ability to perform simple inductive reasoning through a combination of pretraining and model size.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. Multiplication Performance: Plot of GPT-J-6B's 2shot accuracy on multiplication (averaged over multiple multiplicands and training instances) against the frequency of the equation's first term in the pretraining corpus. Each point represents the average performance for that term (e.g., 24) multiplied by numbers 1-50 and 5 choices of random seeds. As in the example, the performance difference for the numbers 24 and 23 is more than $20 \%$. We find a strong correlation between accuracy and frequency.</p>
<p>However, current evaluation schemes for the reasoning of large language models, often neglect or underestimate the impact of data leakage from pretraining data when assessing their reasoning ability. Although the overlap between the training and evaluation splits of public datasets and their effect on the generalization of the language models have been studied (Elangovan et al., 2021; Lewis et al., 2020), the effect of the pretraining data has gotten less attention. Traditionally, a model that has learned to reason in the training phase should be able to generalize outside of the narrow context that it was trained in. Specifically, if the model has learned to reason, its performance on instances with less frequent terms (based on pretraining data) should not be significantly lower than its performance on the instances with more common terms.</p>
<p>As an illustration, consider the arithmetic task of multiplying two integers (shown in Figure 1). A model that has learned proper arithmetic skills should be able to answer the queries irrespective of the frequencies of the operands in the pretraining data. Therefore, it should have roughly equiv-</p>
<p>alent performance when answering the queries $Q$ : what is 24 times $X$ ? and $Q$ : what is 23 times $X$ ?, when aggregated over various values of $X$. This is not the case with current LMs and we will study the effect of frequency terms in details through this paper. To show the effect of frequency, in this example, we plot the average accuracy of GPT-J-6B (Wang, 2021) on the numbers 0-100 (averaged over 1-50 as the other operand) against the frequency of the number in the pretraining data in Figure 1. We find a strong correlation between the term frequency and the model performance indicating that the model reasoning is not robust to these frequencies. Note that even "rare" terms are overall frequent (on the order of millions) in the pretraining data.</p>
<p>In this work, we investigate this impact of the frequency of test instance terms in the model's pretraining data on model's performance. We focus our analysis on numerical reasoning tasks, including addition, multiplication, and unit conversion. For each of these tasks, we identify relevant terms from each instance; for these tasks, terms are the numbers and units involved. We count occurrences of these terms in the pretraining data, including co-occurrences of term pairs or triples within a fixed window. This procedure allows us to aggregate over instances in which these terms appear and observe the relationship between term frequency and model accuracy on instances that include those terms. We summarize this behavior through the performance gap between instances that have the most frequent terms and instances that have the least frequent terms. Intuitively, models that exhibit a high performance gap are more accurate on instances that are more common in the pretraining data; this indicates that the model does not generalize appropriately and is likely affected by dataset overlap.</p>
<p>We present analysis on these numerical reasoning tasks for three sizes of the EleutherAI/GPT models pretrained on the Pile (Gao et al., 2020) dataset, which has been publicly released and thus permits this kind of analysis (in contrast to the data that, e.g., GPT-3 (Brown et al., 2020) was trained on). Our results show a consistently large performance gap between highest-frequency terms and lowest-frequency terms in all of our experiments; in some cases there is more than $70 \%$ of average accuracy gap between the first and last $10 \%$ indicating that even simple unigram statistics are highly correlated with models performance which should not happen if the model is performing reasoning. These observations suggest that any evaluation of reasoning that does not take the pretraining data into account is difficult to interpret, and that we need to revisit evaluation of language models with respect to their pretraining data.</p>
<h2>2. Background and Methodology</h2>
<p>Reasoning ability has long been considered as a proxy for intelligence (Johnson-Laird, 2010). Thus, developing models
with this skill has been also an essential goal of AI and natural language processing (NLP) (Bommasani et al., 2021). Recently, large language models have exhibited an ability to perform reasoning-related tasks in few-shot settings without requiring any modifications to their parameters through a method called in-context learning. Our goal is to evaluate this reasoning skill in-depth for numerical induction tasks. This section provides background information on in-context learning and introduces our method for measuring the performance gap of the models on numerical reasoning tasks based on differences in pretraining term frequency.</p>
<h3>2.1. In-context Learning</h3>
<p>Brown et al. (2020) show that the large GPT-3 model is able to perform well on few-shot reasoning tasks without requiring any changes to its internal parameters, through the usage of a technique called in-context learning. In place of a typical learning procedure, in-context learning instead places training examples in a prompt format, which is subsequently fed to a language model as its input.</p>
<p>Among numerous experiments, Brown et al. (2020) show that GPT3 performs well on a variety of arithmetic questions such as addition and subtraction with 2-5 digit numbers. For example, they show that their largest model can perform zero-shot 2-digit addition with $76.9 \%$ accuracy. Although impressive, due to the large volume of data GPT-3 is trained on, it is possible that the model is merely repeating answers seen during pretraining. To attribute this performance to the model's reasoning capabilities, we need to make sure that the model is not affected by statistical overlaps between the terms of the arithmetic questions and the pretraining data.</p>
<p>In the following sections, we introduce metrics that we use to investigate the relationship between the frequency of terms in the pretraining data and the model performance on reasoning instances containing those terms. To assess this relation, we first define an approach for measuring term frequencies in a large pretraining dataset (Section 2.2). We connect these frequencies to reasoning performance by introducing the performance gap $\Delta$ (Section 2.3).</p>
<h3>2.2. Frequency</h3>
<p>We consider numerical reasoning tasks (Table 1) whose instances consist of input terms, $\mathbf{x}=\left(x_{1}, \ldots, x_{i}, \ldots x_{n}\right)$, and a derived output term $y$, where the $x_{i}$ 's are either positive integers or units of time (e.g., 1,2 , hour, etc.) and $y$ is a positive integer. For example, for the task of multiplication, an instance might be $\mathbf{x}=(23,18)$ and $y=414$, representing the equation $23 \times 18=414$.</p>
<p>For each instance, we extract counts of the number of times that a subset of its terms $X \subseteq\left{x_{1}, \ldots, x_{n}, y\right}$ appear within a specified window in the pretraining data. We refer to this</p>
<p>count as the frequency, $\omega_{X}$, of $X$.
In this paper, we restrict our attention to frequencies involving three or less input terms, e.g., $\mathbf{x}=\left(x_{1}\right)$ or $\left(x_{1}, x_{2}\right)$ or $\left(x_{1}, x_{2}, x_{3}\right)$ and optionally the output term $y$, e.g.:</p>
<ul>
<li>$\omega_{\left{x_{1}\right}}$ : the number of times that $x_{1}$ (e.g., 23) appears in the pretraining data.</li>
<li>$\omega_{\left{x_{1}, x_{2}\right}}$ : the number of times that the input terms $x_{1}$ (e.g., 23) and $x_{2}$ (e.g., 18) appear in the pretraining data within a specific window size.</li>
<li>$\omega_{\left{x_{1}, y\right}}$ : the number of times that the first input term $x_{1}$ (e.g., 23) and the output term $y$ (e.g., 414) appear in the pretraining data within a specific window size.</li>
</ul>
<p>Note that our usage of set notation in the subscript is deliberate; although $\mathbf{x}=\left(x_{1}, x_{2}\right)$ and $\mathbf{x}^{\prime}=\left(x_{2}, x_{1}\right)$ are not necessarily the same (e.g., order is important when representing the task instance), frequency is symmetric (e.g., $\omega_{\left{x_{1}, x_{2}\right}}=\omega_{\left{x_{2}, x_{1}\right}} \forall x_{1}, x_{2}$ ).</p>
<h3>2.3. Performance Gap</h3>
<p>We want to measure how much more accurate the model is on instances containing more versus less frequent terms in the pretraining data. We do this by calculating the differences in average accuracies of the instances in the top and bottom quantiles of the distribution over term frequencies, which we call the performance gap.
Formally, let $\left{\left(X^{(n)}, \omega_{X}^{(n)}\right)\right}, n \in[1, N]$, be a set of terms for a task and their associated term frequencies in the pretraining corpus. Given a task (e.g. addition), we create reasoning instances for each element of this set by instantiating uninstantiated values of $x_{i}$, and deriving $y$ if $y \notin X^{(n)}$. We then measure the LM's accuracy $a^{(n)}$ over the set of instances, and repeat this process for all $n \in[1, N]$, producing a set $\Omega=\left{\left(\omega_{X}^{(n)}, a^{(n)}\right)\right}$. The formula for the performance gap is then given by:</p>
<p>$$
\Delta(\Omega)=\operatorname{Acc}\left(\Omega_{&gt;90 \%}\right)-\operatorname{Acc}\left(\Omega_{&lt;10 \%}\right)
$$</p>
<p>where $\Omega_{&gt;90 \%}$ is the top $10 \%$ of elements in $\Omega$ ordered by frequency, $\Omega_{&lt;10 \%}$ is the bottom $10 \%$, and $\operatorname{Acc}\left(\Omega^{\prime}\right)$ is the average accuracy of elements in $\Omega^{\prime}$. We introduce the following convenient abuses of notation $\Delta_{1}, \Delta_{1,2}, \Delta_{1, y}, \ldots$, to denote the performance gap over the frequency distributions of $\omega_{\left{x_{1}\right}}, \omega_{\left{x_{1}, x_{2}\right}}, \omega_{\left{x_{1}, y\right}}, \ldots$, respectively.
Concretely, for the multiplication example from Figure 1, $\mathbf{x}=\left(x_{1}, x_{2}\right)$ and we consider the performance gap over frequencies $\omega_{\left{x_{1}\right}}$. For each number (say 23), we count the number of times it appears in the pretraining corpus $\left(\omega_{{23}}\right)$, and compute the average accuracy of the model over all instances where the first operand is 23 . The performance gap w.r.t. to $\omega_{\left{x_{1}\right}}$ for this task is the difference between the average accuracy over the top $10 \%$ and the bottom $10 \%$ most frequent numbers in the pretraining corpus.</p>
<p>Table 1. Prompt templates and the number of test cases investigated for each numerical reasoning task.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Task</th>
<th style="text-align: left;">Prompt Template</th>
<th style="text-align: center;">#Test <br> Cases</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Arithmetic</td>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Multiplication</td>
<td style="text-align: left;">$Q$ : What is $x_{1}$ times $x_{2}$ ? A: $y$</td>
<td style="text-align: center;">5000</td>
</tr>
<tr>
<td style="text-align: left;">Addition</td>
<td style="text-align: left;">$Q$ : What is $x_{1}$ plus $x_{2}$ ? A: $y$</td>
<td style="text-align: center;">5000</td>
</tr>
<tr>
<td style="text-align: left;">Operation Inference</td>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Mult. #</td>
<td style="text-align: left;">$Q$ : What is $x_{1}$ # $x_{2}$ ? A: $y$</td>
<td style="text-align: center;">5000</td>
</tr>
<tr>
<td style="text-align: left;">Add. #</td>
<td style="text-align: left;">$Q$ : What is $x_{1}$ # $x_{2}$ ? A: $y$</td>
<td style="text-align: center;">5000</td>
</tr>
<tr>
<td style="text-align: left;">Time Unit Inference</td>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Min $\rightarrow$ Sec</td>
<td style="text-align: left;">$Q$ : What is $x_{1}$ minutes in seconds? A: $y$</td>
<td style="text-align: center;">79</td>
</tr>
<tr>
<td style="text-align: left;">Hour $\rightarrow$ Min</td>
<td style="text-align: left;">$Q$ : What is $x_{1}$ hours in minutes? A: $y$</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: left;">Day $\rightarrow$ Hour</td>
<td style="text-align: left;">$Q$ : What is $x_{1}$ days in hours? A: $y$</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: left;">Week $\rightarrow$ Day</td>
<td style="text-align: left;">$Q$ : What is $x_{1}$ weeks in days? A: $y$</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: left;">Month $\rightarrow$ Week</td>
<td style="text-align: left;">$Q$ : What is $x_{1}$ months in weeks? A: $y$</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: left;">Year $\rightarrow$ Month</td>
<td style="text-align: left;">$Q$ : What is $x_{1}$ years in months? A: $y$</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: left;">Decade $\rightarrow$ Year</td>
<td style="text-align: left;">$Q$ : What is $x_{1}$ decades in years? A: $y$</td>
<td style="text-align: center;">100</td>
</tr>
</tbody>
</table>
<h2>3. Experiment Setup</h2>
<p>In this section, we describe our setup to measure the effect of pretraining data on the few-shot evaluation of a number of numerical reasoning tasks for different language models. For reproducibility, we will release the complete source code of our experiments.</p>
<p>Language Models We experiment on models from EleutherAI i.e., GPT-J-6B (Wang, 2021), and GPT-Neo1.3B, GPT-Neo-2.7B (Black et al., 2021). These models are publicly available, but more importantly, the corpus used to pretrain them has also been released. We use the HuggingFace ${ }^{1}$ Transformer integration of the models in our experiments.</p>
<p>Pretraining Corpus Language models studied in this work are trained on the Pile dataset (Gao et al., 2020), a large-scale language modeling dataset (800GB) consisting of English documents in 22 academic or other professional data sources. Since our reasoning tasks focus on numbers, we count the frequency of all integers with less than seven digits using a white-space tokenizer. To calculate the frequencies of the numbers we use Amazon Elastic Map Reduce (EMR) platform. We will release the code and statistics of the frequencies.</p>
<p>Numerical Reasoning Tasks We create three types of datasets that target the mathematical capabilities of language models since solving mathematical questions is a useful reasoning capability of the models (Brown et al., 2020).</p>
<ul>
<li>Arithmetic, 2 tasks As the first task, we consider simple arithmetic operations: addition $x_{1}+x_{2} \rightarrow y$ and multipli-</li>
</ul>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. Pipeline for Data Construction: We use the term counts processed from the pretraining data to develop the reasoning queries and render them with prompts templates to a proper language model input format (illustrated using the example from Figure 1).
cation $x_{1} \times x_{2} \rightarrow y$. In both cases, the first operands $\left(x_{1}\right)$ are numbers less than 100 (that are in the top 200 most frequent numbers) and the second operands $\left(x_{2}\right)$ are the numbers in the range $(1-50)$.</p>
<ul>
<li>Operation Inference, 2 tasks Instead of directly specifying the operation, we also create a variation where the model needs to infer, from a few examples, the operation itself, as well as the result, as introduced in the evaluation of Megatron-Turing model ${ }^{2}$. We replace the arithmetic operation with a "#", with the same operations and operands as previous, to create these datasets.</li>
<li>Time Unit Conversion, 7 tasks Apart from directly executing arithmetic expressions, we are also interested in evaluating model capability to implicitly reason about these operations. To this end, we construct a unit conversion dataset by identifying the most frequent numbers that co-occur with time unit words ("second", "minute", "hour", "day", "week", "month", "year", and "decade") as the primary operand $x_{1}$, the time units themselves as additional operands ( $x_{2} \rightarrow x_{3}$ ), i.e. converting 24 hours to minutes is represented as ( 24 , "hours", 60). We expect converting time values to be mathematically more straightforward than two-digit multiplication since the model need only multiply with the same (implicit) second operand, e.g., $\times 60$ for converting hours to minutes.</li>
</ul>
<p>The overall pipeline for creating natural language instances for these datasets is illustrated in Figure 2. We compute occurrences and co-occurrences (if they are within a window of 5 words) of the terms in the corpus, i.e. the time units and numbers. We then generate instances for each of our reasoning datasets using the most frequent terms (in the top 200) as operands with less than 3 digits. We focus on the top terms since we expect the models to have a fairly reliable and robust representations for these words. Each reasoning instance is rendered as a natural language query using the prompt templates from Table 1, and fed to the language model to generate the answer. For example, to create a multiplication instance given the argument $\left(x_{1}=23, x_{2}=18\right)$, we use the instance template to create a natural language</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>input for the model as "Q: What is 23 times 18? A: __", with the goal of producing " 414 " $(y=23 \times 18=414)$. For few-shot evaluation, we prompt the language models with $k=0,2,4,8,16$ shots, and average performance over five random selection of the prompt instances, we randomly select $k$ samples from the dataset as the training examples provided in the prompt and use the rest of the instances from the model as the test data.</p>
<h2>4. Results</h2>
<p>With the three types of reasoning tasks (consisting of 11 total datasets), we present an evaluation of the effect of pretraining frequency on the performance of the language models. For each dataset, we measure the performance gap on instances that consist of rarer (relatively) terms, for a few different choices of what to compute frequency over i.e different combinations of the instance terms. To observe this effect with larger language models, we also investigate the effect of the model size on this performance gap.</p>
<p>Arithmetic We first study the performance on simple addition and multiplication of numbers. The results for the GPT-J-6B model is provided in Table 2, with performance gap computed just for $x_{1}$, for $\left(x_{1}, x_{2}\right)$, and for $\left(x_{1}, y\right)$. In multiplication, we observe a very high performance gap for all these definitions of frequencies, suggesting a strong effect of frequency in the pretraining data on the model's ability to perform multiplication. For better illustration of the performance gap, we plot the mean accuracy across the frequency of $x_{1}$ in Figure 3b. The plot demonstrates the strong correlation between the models accuracy on specific instances, and the instance element frequency in the pretraining data. For addition, we observe an overall higher performance of the GPT-J-6B model in comparison to the multiplication experiments. However, the performance gap on all of the definitions of the instance frequencies still shows an strong effect on the models accuracy. As shown in Figure 3a, the average accuracy of the model still has a positive slope, indicating the effect of instance frequencies.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3. The GPT-J-6B accuracy on arithematic and operator inference tasks, with $k$ shots. The average accuracy ( $y$-axis) of the binned instances is highly correlated with their term frequencies $\omega_{\left{x_{1}\right}}$ in the pretraining corpus ( $x$-axis).</p>
<p>Table 2. GPT-J-6B results on arithmetic, operation inference (#) tasks $\Delta_{1}, \Delta_{1,2}$ and $\Delta_{1, y}$ represent the performance gap over the frequency distributions of $\omega_{\left{x_{1}\right}}, \omega_{\left{x_{1}, x_{2}\right}}$ and $\omega_{\left{x_{1}, y\right}}$ respectively. $x_{1}$ represent the first operand, $x_{2}$ second operand and $y$ the answer of the arithmetic question.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">$k$</th>
<th style="text-align: center;">Multiplication</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Addition</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Multiplication (#)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Addition (#)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Acc.</td>
<td style="text-align: center;">$\Delta_{1}$</td>
<td style="text-align: center;">$\Delta_{1,2}$</td>
<td style="text-align: center;">$\Delta_{1, y}$</td>
<td style="text-align: center;">Acc.</td>
<td style="text-align: center;">$\Delta_{1}$</td>
<td style="text-align: center;">$\Delta_{1,2}$</td>
<td style="text-align: center;">$\Delta_{1, y}$</td>
<td style="text-align: center;">Acc.</td>
<td style="text-align: center;">$\Delta_{1}$</td>
<td style="text-align: center;">$\Delta_{1,2}$</td>
<td style="text-align: center;">$\Delta_{1, y}$</td>
<td style="text-align: center;">Acc.</td>
<td style="text-align: center;">$\Delta_{1}$</td>
<td style="text-align: center;">$\Delta_{1,2}$</td>
<td style="text-align: center;">$\Delta_{1, y}$</td>
</tr>
<tr>
<td style="text-align: center;">0</td>
<td style="text-align: center;">5.4</td>
<td style="text-align: center;">18.0</td>
<td style="text-align: center;">20.6</td>
<td style="text-align: center;">30.8</td>
<td style="text-align: center;">1.6</td>
<td style="text-align: center;">8.4</td>
<td style="text-align: center;">6.9</td>
<td style="text-align: center;">8.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">35.9</td>
<td style="text-align: center;">77.6</td>
<td style="text-align: center;">79.3</td>
<td style="text-align: center;">89.9</td>
<td style="text-align: center;">88.2</td>
<td style="text-align: center;">16.8</td>
<td style="text-align: center;">21.7</td>
<td style="text-align: center;">21.9</td>
<td style="text-align: center;">3.1</td>
<td style="text-align: center;">14.1</td>
<td style="text-align: center;">13.7</td>
<td style="text-align: center;">14.2</td>
<td style="text-align: center;">7.8</td>
<td style="text-align: center;">18.1</td>
<td style="text-align: center;">25.3</td>
<td style="text-align: center;">28.3</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">39.2</td>
<td style="text-align: center;">70.8</td>
<td style="text-align: center;">76.4</td>
<td style="text-align: center;">83.5</td>
<td style="text-align: center;">91.4</td>
<td style="text-align: center;">15.0</td>
<td style="text-align: center;">24.8</td>
<td style="text-align: center;">26.4</td>
<td style="text-align: center;">5.7</td>
<td style="text-align: center;">20.9</td>
<td style="text-align: center;">21.3</td>
<td style="text-align: center;">23.4</td>
<td style="text-align: center;">9.8</td>
<td style="text-align: center;">24.8</td>
<td style="text-align: center;">30.1</td>
<td style="text-align: center;">30.4</td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: center;">42.9</td>
<td style="text-align: center;">74.6</td>
<td style="text-align: center;">80.8</td>
<td style="text-align: center;">86.0</td>
<td style="text-align: center;">89.6</td>
<td style="text-align: center;">16.3</td>
<td style="text-align: center;">26.5</td>
<td style="text-align: center;">29.6</td>
<td style="text-align: center;">9.4</td>
<td style="text-align: center;">31.3</td>
<td style="text-align: center;">33.2</td>
<td style="text-align: center;">34.7</td>
<td style="text-align: center;">19.8</td>
<td style="text-align: center;">31.0</td>
<td style="text-align: center;">44.8</td>
<td style="text-align: center;">45.2</td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">40.9</td>
<td style="text-align: center;">73.3</td>
<td style="text-align: center;">77.7</td>
<td style="text-align: center;">82.6</td>
<td style="text-align: center;">88.6</td>
<td style="text-align: center;">16.4</td>
<td style="text-align: center;">27.3</td>
<td style="text-align: center;">31.0</td>
<td style="text-align: center;">11.0</td>
<td style="text-align: center;">39.6</td>
<td style="text-align: center;">38.7</td>
<td style="text-align: center;">42.6</td>
<td style="text-align: center;">26.2</td>
<td style="text-align: center;">38.5</td>
<td style="text-align: center;">47.2</td>
<td style="text-align: center;">49.9</td>
</tr>
</tbody>
</table>
<p>Operation Inference These tasks aim to assess the model capability to both infer the math operation and to perform the actual computation. Overall, as we see in Table 2, the model is much less accurate here as compared to the arithmetic experiments. However, the model has better performance on the frequent instances even for these low performance tasks (see detailed trend in Figures 3d and 3c). The performance gap here suggests that the effect of pretraining is not only for tasks that the model is accurate on, but even for operation inference that is more challenging and require deeper reasoning. An additional interesting trend is in the last column of Table 2 with the steady growth of performance gap with respect to $\omega_{\left{x_{1}, y\right}}$ (co-occurrence of the instance input element and the true answer) as the number of shots increases and the model gets better in performance. Moreover, the lower accuracy here as compared to addition experiments in the previous section suggests that the model is unable to infer the operation from the few-shot prompts, and it may be performing some form of pattern matching based on the pretraining data.</p>
<p>Time-Unit Conversion The performance gap evaluated on all the time unit conversion experiments is provided in</p>
<p>Table 3. We first observe a relatively high performance gap on all the tasks except the conversion from decade to year. For example, Figure 4 illustrates the trend of increased model performance with the frequency of instance elements for converting the time values from years to months. We also observe a general pattern of increase in the performance gap as the number of shots (training examples in the prompt) increases (results are in table 3). These results suggest that even though the model gets more accurate, the improvements focus on more frequent instances of the task. Decades to years: As we can observe in Figure 5, The model performs nearly perfectly on this task with as few as 8 shots, and we only see very small performance gap s. This is likely due to the task being quite simple (appending a " 0 " to the input number) and the model is able to generalize in the manner we are evaluating it. However, it is also possible that we are simply not identifying the right frequency statistics for this task, and there is an effect that our current evaluation setup does not capture.</p>
<p>Studying the Size of Language Models To further study the impact of language models sizes on the performance gap caused by the instance frequencies, we perform the</p>
<p>Table 3. GPT-J-6B results on Time-Unit Conversion: $\Delta_{1,2}, \Delta_{1,2,3}$ and $\Delta_{1,2, y}$ represent the performance gap over the frequency distributions of $\omega_{\left{x_{1}, x_{2}\right}}, \omega_{\left{x_{1}, x_{2}, x_{3}\right}}$ and $\omega_{\left{x_{1}, x_{2}, y\right}}$ respectively, where $x_{1}$ is the number operand, $x_{2}$ is the source time unit, $x_{3}$ is the second implicit number operand needed for performing the conversion and the $y$ is the true answer.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">$k$</th>
<th style="text-align: center;">Min $\rightarrow$ Sec</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Hour $\rightarrow$ Min</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Day $\rightarrow$ Hour</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Week $\rightarrow$ Day</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Acc.</td>
<td style="text-align: center;">$\Delta_{1,2}$</td>
<td style="text-align: center;">$\Delta_{1,2,3}$</td>
<td style="text-align: center;">$\Delta_{1,2, y}$</td>
<td style="text-align: center;">Acc.</td>
<td style="text-align: center;">$\Delta_{1,2}$</td>
<td style="text-align: center;">$\Delta_{1,2,3}$</td>
<td style="text-align: center;">$\Delta_{1,2, y}$</td>
<td style="text-align: center;">Acc.</td>
<td style="text-align: center;">$\Delta_{1,2}$</td>
<td style="text-align: center;">$\Delta_{1,2,3}$</td>
<td style="text-align: center;">$\Delta_{1,2, y}$</td>
<td style="text-align: center;">Acc.</td>
<td style="text-align: center;">$\Delta_{1,2}$</td>
<td style="text-align: center;">$\Delta_{1,2,3}$</td>
<td style="text-align: center;">$\Delta_{1,2, y}$</td>
</tr>
<tr>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1.3</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">12.5</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">5.0</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">10.0</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">25.5</td>
<td style="text-align: center;">62.5</td>
<td style="text-align: center;">67.5</td>
<td style="text-align: center;">67.5</td>
<td style="text-align: center;">19.4</td>
<td style="text-align: center;">58.0</td>
<td style="text-align: center;">40.5</td>
<td style="text-align: center;">44.0</td>
<td style="text-align: center;">12.1</td>
<td style="text-align: center;">28.9</td>
<td style="text-align: center;">24.0</td>
<td style="text-align: center;">28.0</td>
<td style="text-align: center;">13.1</td>
<td style="text-align: center;">43.5</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">54.0</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">35.5</td>
<td style="text-align: center;">60.0</td>
<td style="text-align: center;">71.7</td>
<td style="text-align: center;">63.1</td>
<td style="text-align: center;">29.1</td>
<td style="text-align: center;">76.4</td>
<td style="text-align: center;">50.5</td>
<td style="text-align: center;">59.0</td>
<td style="text-align: center;">22.7</td>
<td style="text-align: center;">46.4</td>
<td style="text-align: center;">45.0</td>
<td style="text-align: center;">47.5</td>
<td style="text-align: center;">19.2</td>
<td style="text-align: center;">40.9</td>
<td style="text-align: center;">43.3</td>
<td style="text-align: center;">47.0</td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: center;">49.9</td>
<td style="text-align: center;">72.1</td>
<td style="text-align: center;">79.0</td>
<td style="text-align: center;">52.7</td>
<td style="text-align: center;">36.3</td>
<td style="text-align: center;">74.6</td>
<td style="text-align: center;">52.5</td>
<td style="text-align: center;">63.0</td>
<td style="text-align: center;">31.0</td>
<td style="text-align: center;">59.1</td>
<td style="text-align: center;">52.5</td>
<td style="text-align: center;">54.5</td>
<td style="text-align: center;">28.6</td>
<td style="text-align: center;">70.6</td>
<td style="text-align: center;">62.0</td>
<td style="text-align: center;">67.0</td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">58.4</td>
<td style="text-align: center;">82.7</td>
<td style="text-align: center;">74.4</td>
<td style="text-align: center;">48.5</td>
<td style="text-align: center;">42.8</td>
<td style="text-align: center;">80.1</td>
<td style="text-align: center;">49.0</td>
<td style="text-align: center;">62.5</td>
<td style="text-align: center;">43.3</td>
<td style="text-align: center;">62.8</td>
<td style="text-align: center;">56.0</td>
<td style="text-align: center;">54.8</td>
<td style="text-align: center;">28.0</td>
<td style="text-align: center;">22.1</td>
<td style="text-align: center;">31.4</td>
<td style="text-align: center;">33.2</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: center;">Shots, $k$</th>
<th style="text-align: center;">Month $\rightarrow$ Week</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Year $\rightarrow$ Month</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Decade $\rightarrow$ Year</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Acc.</td>
<td style="text-align: center;">$\Delta_{1,2}$</td>
<td style="text-align: center;">$\Delta_{1,2,3}$</td>
<td style="text-align: center;">$\Delta_{1,2, y}$</td>
<td style="text-align: center;">Acc.</td>
<td style="text-align: center;">$\Delta_{1,2}$</td>
<td style="text-align: center;">$\Delta_{1,2,3}$</td>
<td style="text-align: center;">$\Delta_{1,2, y}$</td>
<td style="text-align: center;">Acc.</td>
<td style="text-align: center;">$\Delta_{1,2}$</td>
<td style="text-align: center;">$\Delta_{1,2,3}$</td>
<td style="text-align: center;">$\Delta_{1,2, y}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">3.1</td>
<td style="text-align: center;">14.3</td>
<td style="text-align: center;">14.3</td>
<td style="text-align: center;">28.6</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">30.1</td>
<td style="text-align: center;">8.5</td>
<td style="text-align: center;">9.3</td>
<td style="text-align: center;">21.0</td>
<td style="text-align: center;">21.8</td>
<td style="text-align: center;">58.0</td>
<td style="text-align: center;">64.0</td>
<td style="text-align: center;">53.0</td>
<td style="text-align: center;">76.5</td>
<td style="text-align: center;">38.8</td>
<td style="text-align: center;">47.1</td>
<td style="text-align: center;">43.1</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">63.3</td>
<td style="text-align: center;">22.9</td>
<td style="text-align: center;">26.2</td>
<td style="text-align: center;">10.5</td>
<td style="text-align: center;">31.9</td>
<td style="text-align: center;">64.8</td>
<td style="text-align: center;">69.5</td>
<td style="text-align: center;">66.8</td>
<td style="text-align: center;">96.7</td>
<td style="text-align: center;">2.9</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">2.9</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: center;">80.9</td>
<td style="text-align: center;">33.8</td>
<td style="text-align: center;">30.8</td>
<td style="text-align: center;">24.0</td>
<td style="text-align: center;">45.4</td>
<td style="text-align: center;">55.0</td>
<td style="text-align: center;">72.0</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">99.6</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">84.5</td>
<td style="text-align: center;">43.4</td>
<td style="text-align: center;">57.0</td>
<td style="text-align: center;">30.3</td>
<td style="text-align: center;">56.7</td>
<td style="text-align: center;">58.7</td>
<td style="text-align: center;">65.3</td>
<td style="text-align: center;">61.3</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4. GPT-J-6B performance on Year $\rightarrow$ Month: The interpolation lines show the correlation between the average accuracy and the $\omega_{\left{x_{1}, x_{2}\right}} . k$ is the number of shots.
arithmetic experiments for 2 and 8 shots using a variety of models (including the smaller versions of models GPT-Neo-1.3B and GPT-Neo-2.7B). We can see the trends of the average accuracy of the models in Figures 6a and 6b. The smaller models overall are less accurate on the arithmetic tasks, which is consistent with observations in related work (Brown et al., 2020). However, their success is still focused on the more frequent terms from the pretraining corpus, suggesting that even the smaller models show the effect of reliance on the pretraining data, although to a much lower extent than the larger ones.</p>
<p>Summary Overall, we observe high positive performance gap for almost all of the experiments on the three definition levels of the frequency for each task. This suggests a strong
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5. GPT-J-6B performance on Decade $\rightarrow$ Year: The interpolation average accuracy line over the $\omega_{\left{x_{1}, x_{2}\right}}$ show that the model reaches a high performance with the number of shots $k=8$, there is still a performance gap in the case of $k=2$.
effect of frequency of the instances in the pretraining data on the model performance. In particular, evaluation using performance gap with $\omega_{\left{x_{1}\right}}$ shows that even the unigram statistics of the instances have strong correlation with the models performance on the instance.</p>
<p>Other than some exceptional cases, we observe an increasing trend in the performance gap as we put more training instances in the prompt (the number of shots); this can be a further indication that the model is directed through the patterns in the pretraining data to answer the reasoning questions. Our experiments with the smaller sizes of the model also show that they can only solve the frequent instances of the tasks, which further supports our observation that model performance is correlated with the term frequencies.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6. The effect of model size on performance: Smaller models only perform well on instances with more frequent terms in the pretraining data. $k$ represents the number of shots.</p>
<h2>5. Related Work</h2>
<p>A large and growing body of literature has investigated a number of related concerns with large language models.</p>
<p>Prompting Prompting has been widely applied to study the factual (Petroni et al., 2019), commonsense (Davison et al., 2019; Weir et al., 2020; Lin et al., 2020), mathematical (Saxton et al., 2019), and other NLP task-related (Radford et al., 2019; Shin et al., 2020) knowledge LMs acquire during pretraining. In this work, we focus on the in-context learning setup of Brown et al. (2020), who use prompts that include training examples to diagnose LMs' few-shot learning capabilities.</p>
<p>Impact of Frequency on LM Performance Kassner et al. (2020) and Wei et al. (2021) perform controlled experiments varying pretraining data to characterize the extent pretraining affects LMs' ability to learn to memorize and reason with facts as well as learn generalizable syntax rules. In line with our results, both of these works find that frequency
is a distinguishing factor in whether or not the model memorizes a particular fact or syntactic rule for a verb form. Sinha et al. (2021) further demonstrate that shuffling word order during pretraining has minimal impact on an LMs' accuracy on downstream tasks, and, concurrent with this work, Min et al. (2022) similarly find that shuffling labels in in-context learning demonstrations has a minimal impact on few-shot accuracy. These results further suggest that LMs' performance is largely driven by their ability to model high-order word co-occurrence statistics. Although frequent terms are more likely to be memorized, data privacy researchers have also shown that LMs may memorize sensitive sequences occurring in training data (e.g., social security and credit card numbers), even if they are rare (Carlini et al., 2019; Song \&amp; Shmatikov, 2019).</p>
<p>Memorization Feldman (2020) provide a theoretical definition of memorization as the difference between the accuracy of a model on a training data point when that point is included vs. excluded from training. In subsequent work, they develop an approach for approximating memorization using influence functions Feldman \&amp; Zhang (2020). This framework is applied to study memorization in language models by Zhang et al. (2021), who find that training examples that are memorized by the LM tend to have high influence of LM predictions on similar validation instances. Their result may provide a plausible explanation that the frequency effects observed in this work are due to memorization.</p>
<p>Training Artifacts Challenge Evaluation Our results raise the issue that in-context learning probes may overestimate an LM's ability generalize from few examples when biases are present in the training data. This is consistent with prior work that has exposed the similar effects of biases from: lexical cues in natural language inference datasets (Gururangan et al., 2018; Poliak et al., 2018; McCoy et al., 2019), question-passage overlap and entity cues in reading comprehension datasets (Chen et al., 2016; Sugawara et al., 2018; Jia \&amp; Liang, 2017; Lewis et al., 2021), gender cues in coreference resolution datasets (Rudinger et al., 2018), popularity in named entity disambiguation (Chen et al., 2021), similarity between training and test instances in information extraction and sentiment analysis datasets (Elangovan et al., 2021), and effects of how data is split (Gorman \&amp; Bedrick, 2019; Søgaard et al., 2021). Relatedly, data poisoning research studies how to adversarially introduce artifacts into training data to produce unwanted model behaviors (Nelson et al., 2008; Chan et al., 2020; Wallace et al., 2021). A general statistical procedure to test for artifacts is presented in Gardner et al. (2021), who also theoretically show that large datasets are almost certain to contain artifacts under reasonable assumptions. Techniques for mitigating biases in the presence of</p>
<p>dataset artifacts are covered by Romanov et al. (2019) and Karimi Mahabadi et al. (2020).</p>
<p>Documenting Pretraining Data To better understand the risks of dataset artifacts, there has been a call to better document the characteristics and intended uses of datasets (Gebru et al., 2021; Bender et al., 2021). However, due to the sheer size of LM pretraining datasets-which range from 100's of GBs to 10's of TBs-doing so can pose a substantial challenge. Despite this, researchers have been able to estimate word frequencies, topics, and genres of documents (Sharoff, 2020), as well as proportions of toxic text (Gehman et al., 2020) appearing in OpenWebText (Gokaslan \&amp; Cohen, 2019). Similar efforts have been made to characterize the top-level domains, amount of hate speech, and censured text appearing in the C4 corpus (Raffel et al., 2020; Dodge et al., 2021; Luccioni \&amp; Viviano, 2021). Our work documents co-occurrence statistics of numbers and dates of documents appearing in the Pile dataset.</p>
<p>Numeracy and Temporal Reasoning in LMs Our work contributes a larger body of work dedicated to studying numeracy in word embeddings and language models (Spithourakis \&amp; Riedel, 2018; Wallace et al., 2019). Recently, Geva et al. (2020) and Zhou et al. (2020) have proposed training schemes to help improve LMs' temporal and numerical reasoning capabilities. Patel et al. (2021) also showed that NLP math solvers rely on simple heuristics to answer math questions. We expect that the performance gap metric proposed in this work will be useful to better understand the impact of such schemes.</p>
<h2>6. Discussion and Future Work</h2>
<p>In this work, we consider how to conduct few-shot evaluations in light of the analysis with respect to the pretraining data. Prior work has attempted to control for overlap between training or pretraining data and the test instances, but as we have seen, those methods are insufficient. For example, Brown et al. (2020) measure the impact of removing instances from evaluation datasets that share 13-gram overlap with their pretraining data on GPT-3's accuracy, and also argue that the low occurrence of exact phrases such as "NUM1 + NUM2 =" and "NUM1 plus NUM2" in the pretraining data indicate that the model's strong performance on arithmetic tasks is likely due to factors other than memorization. However, we have seen that model performance is impacted by much simpler statistical patterns, as small as unigram overlaps with the pretraining data.</p>
<p>For these reasons, we strongly recommend that evaluation of reasoning capabilities should take the pretraining corpus into account, and any claims of reasoning can only be made after demonstrating robustness to the effect of pretraining.</p>
<p>LM benchmarks typically have no reference to the model's pretraining data. However, it is impossible to interpret fewshot performance on any benchmark without reference to information from the data that the LM was trained on. One possible addition to future evaluation is the performance gap between high-frequency and low-frequency terms, perhaps including only terms that have been seen more than some threshold value. It is worth mentioning that, even a performance gap of 0 is likely not sufficient to demonstrate a claim of reasoning capabilities-what exactly constitutes "reasoning" remains ill-defined-but it may be a necessary condition, and one that current models do not meet.</p>
<p>There are a few limitations to our study that open up avenues for future research. We are not making a causal claim here, and in general, there may be confounders that we have not eliminated in our study. We recommend further research in investigating methods in causal inference and interventions during training to provide finer-grained analysis of the effect of pretraining. Since our approach aggregates fairly simple patterns, the effect we observe might be stronger if a wider variety and complexity of patterns is considered in the pretraining corpus. Similarly, our work was also limited to numerical reasoning tasks, and it would be worthwhile to consider how much other reasoning capability evaluations are impacted by the same effect, which could be measured using the performance gap metric introduced here. Defining appropriate instance terms for other reasoning tasks such as commonsense reasoning will be a challenging but important direction for future work. With the insights in this work and these recommendations, we hope to inspire further studies into the effect of pretraining on LM performance.</p>
<h2>7. Conclusion</h2>
<p>We show that in-context language model performance on numerical reasoning tasks can be impacted significantly by low-order co-occurrence statistics in the pretraining data, raising questions on the extent to which these models are actually reasoning to solve these tasks. These observations suggest the necessity for reconsidering and redefining the reasoning evaluation schemes for the large language models. Further characterizing the impacting factors on the models reasoning capacities is also an important tasks for the community. Most importantly, we suggest that the NLP community should not treat the pretraining data of the large language models as unknown black boxes. Overlooking the impact of the pretraining data can be misleading in evaluating the model reasoning skills.</p>
<h2>Acknowledgements</h2>
<p>We would like to thank Yanai Elazar, Mukund Sundarajan, Marco Tulio Ribeiro, Eric Wallace, Shivanshu Gupta, Navid</p>
<p>Salehnamadi, Pouya Pezeshkpour, and Dylan Slack for valuable discussions and feedback on this work. This material is sponsored in part by the DARPA MCS program under Contract No. N660011924033 with the United States Office Of Naval Research, by an Amazon Research Award, and by awards IIS-2046873 and IIS-204098 from the National Science Foundation.</p>
<h2>References</h2>
<p>Bender, E. M., Gebru, T., McMillan-Major, A., and Shmitchell, S. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, pp. 610-623, 2021.</p>
<p>Black, S., Gao, L., Wang, P., Leahy, C., and Biderman, S. GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow, March 2021. URL https: //doi.org/10.5281/zenodo.5297715.</p>
<p>Bommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S., Bernstein, M. S., Bohg, J., Bosselut, A., Brunskill, E., et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.</p>
<p>Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020.</p>
<p>Carlini, N., Liu, C., Erlingsson, U., Kos, J., and Song, D. The secret sharer: Evaluating and testing unintended memorization in neural networks. In Proceedings of the 28th USENIX Conference on Security Symposium, SEC'19, pp. 267-284, USA, 2019. USENIX Association. ISBN 9781939133069.</p>
<p>Chan, A., Tay, Y., Ong, Y.-S., and Zhang, A. Poison attacks against text datasets with conditional adversarially regularized autoencoder. In Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 4175-4189, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp. 373. URL https://aclanthology.org/2020. findings-emnlp. 373.</p>
<p>Chen, A., Gudipati, P., Longpre, S., Ling, X., and Singh, S. Evaluating entity disambiguation and the role of popularity in retrieval-based NLP. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 4472-4485, Online, August 2021. Association
for Computational Linguistics. doi: 10.18653/v1/2021. acl-long.345. URL https://aclanthology.org/ 2021.acl-long. 345.</p>
<p>Chen, D., Bolton, J., and Manning, C. D. A thorough examination of the CNN/Daily Mail reading comprehension task. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 2358-2367, Berlin, Germany, August 2016. Association for Computational Linguistics. doi: 10.18653/v1/P16-1223. URL https: //aclanthology.org/P16-1223.</p>
<p>Davison, J., Feldman, J., and Rush, A. Commonsense knowledge mining from pretrained models. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 1173-1178, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1109. URL https: //aclanthology.org/D19-1109.</p>
<p>Dodge, J., Sap, M., Marasović, A., Agnew, W., Ilharco, G., Groeneveld, D., Mitchell, M., and Gardner, M. Documenting large webtext corpora: A case study on the colossal clean crawled corpus. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 1286-1305, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main. 98. URL https://aclanthology.org/2021. emnlp-main. 98.</p>
<p>Elangovan, A., He, J., and Verspoor, K. Memorization vs. generalization : Quantifying data leakage in NLP performance evaluation. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pp. 1325-1335, Online, April 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.eacl-main.113. URL https: //aclanthology.org/2021.eacl-main.113.</p>
<p>Feldman, V. Does Learning Require Memorization? A Short Tale about a Long Tail, pp. 954-959. Association for Computing Machinery, New York, NY, USA, 2020. ISBN 9781450369794. URL https://doi.org/10. 1145/3357713.3384290.</p>
<p>Feldman, V. and Zhang, C. What neural networks memorize and why: Discovering the long tail via influence estimation. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M. F., and Lin, H. (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 2881-2891. Curran Associates, Inc., 2020. URL https://proceedings.</p>
<p>neurips.cc/paper/2020/file/ 1e14bfe2714193e7af5abc64ecbd6b46-Paper. pdf.</p>
<p>Gao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., Phang, J., He, H., Thite, A., Nabeshima, N., Presser, S., and Leahy, C. The Pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020.</p>
<p>Gardner, M., Merrill, W., Dodge, J., Peters, M., Ross, A., Singh, S., and Smith, N. A. Competency problems: On finding and removing artifacts in language data. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 1801-1813, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.135. URL https:// aclanthology.org/2021.emnlp-main.135.</p>
<p>Gebru, T., Morgenstern, J., Vecchione, B., Vaughan, J. W., Wallach, H., au2, H. D. I., and Crawford, K. Datasheets for datasets, 2021.</p>
<p>Gehman, S., Gururangan, S., Sap, M., Choi, Y., and Smith, N. A. RealToxicityPrompts: Evaluating neural toxic degeneration in language models. In Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 3356-3369, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp. 301. URL https://aclanthology.org/2020. findings-emnlp. 301.</p>
<p>Geva, M., Gupta, A., and Berant, J. Injecting numerical reasoning skills into language models. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 946-958, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.89. URL https: //aclanthology.org/2020.acl-main.89.</p>
<p>Gokaslan, A. and Cohen, V. Openwebtext corpus. http://Skylion007.github.io/ OpenWebTextCorpus, 2019.</p>
<p>Gorman, K. and Bedrick, S. We need to talk about standard splits. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 27862791, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1267. URL https://aclanthology.org/P19-1267.</p>
<p>Gururangan, S., Swayamdipta, S., Levy, O., Schwartz, R., Bowman, S., and Smith, N. A. Annotation artifacts in natural language inference data. In Proceedings of the 2018</p>
<p>Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pp. 107-112, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-2017. URL https://aclanthology.org/N18-2017.</p>
<p>Jia, R. and Liang, P. Adversarial examples for evaluating reading comprehension systems. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pp. 2021-2031, Copenhagen, Denmark, September 2017. Association for Computational Linguistics. doi: 10.18653/v1/D17-1215. URL https://aclanthology.org/D17-1215.</p>
<p>Johnson-Laird, P. N. Mental models and human reasoning. Proceedings of the National Academy of Sciences, 107 (43):18243-18250, 2010. ISSN 0027-8424. doi: 10.1073/ pnas. 1012933107. URL https://www.pnas.org/ content/107/43/18243.</p>
<p>Karimi Mahabadi, R., Belinkov, Y., and Henderson, J. End-to-end bias mitigation by modelling biases in corpora. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 8706-8716, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.769. URL https : //aclanthology.org/2020.acl-main.769.</p>
<p>Kassner, N., Krojer, B., and Schütze, H. Are pretrained language models symbolic reasoners over knowledge? In Proceedings of the 24th Conference on Computational Natural Language Learning, pp. 552-564, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.conll-1.45. URL https : //aclanthology.org/2020.conll-1.45.</p>
<p>Lewis, P., Stenetorp, P., and Riedel, S. Question and answer test-train overlap in open-domain question answering datasets. arXiv preprint arXiv:2008.02637, 2020.</p>
<p>Lewis, P., Stenetorp, P., and Riedel, S. Question and answer test-train overlap in open-domain question answering datasets. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pp. 1000-1008, Online, April 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.eacl-main.86. URL https : //aclanthology.org/2021.eacl-main.86.</p>
<p>Lin, B. Y., Lee, S., Khanna, R., and Ren, X. Birds have four legs?! NumerSense: Probing Numerical Commonsense Knowledge of Pre-Trained Language Models. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 68626868, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.</p>
<ol>
<li>URL https://aclanthology.org/2020. emnlp-main. 557.</li>
</ol>
<p>Luccioni, A. and Viviano, J. What's in the box? an analysis of undesirable content in the Common Crawl corpus. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pp. 182-189, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-short.24. URL https: //aclanthology.org/2021.acl-short. 24.</p>
<p>McCoy, T., Pavlick, E., and Linzen, T. Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 3428-3448, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1334. URL https://aclanthology.org/P19-1334.</p>
<p>Min, S., Lyu, X., Holtzman, A., Artetxe, M., Lewis, M., Hajishirzi, H., and Zettlemoyer, L. Rethinking the role of demonstrations: What makes in-context learning work? arXiv preprint arXiv:2202.12837, 2022.</p>
<p>Nelson, B., Barreno, M., Chi, F. J., Joseph, A. D., Rubinstein, B. I. P., Saini, U., Sutton, C., Tygar, J. D., and Xia, K. Exploiting machine learning to subvert your spam filter. In Proceedings of the 1st Usenix Workshop on Large-Scale Exploits and Emergent Threats, LEET'08, USA, 2008. USENIX Association.</p>
<p>Patel, A., Bhattamishra, S., and Goyal, N. Are nlp models really able to solve simple math word problems?, 2021.</p>
<p>Petroni, F., Rocktäschel, T., Riedel, S., Lewis, P., Bakhtin, A., Wu, Y., and Miller, A. Language models as knowledge bases? In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 2463-2473, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1250. URL https://aclanthology.org/D19-1250.</p>
<p>Poliak, A., Naradowsky, J., Haldar, A., Rudinger, R., and Van Durme, B. Hypothesis only baselines in natural language inference. In Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics, pp. 180-191, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/ S18-2023. URL https://aclanthology.org/ S18-2023.</p>
<p>Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. Language models are unsupervised multitask learners. 2019.</p>
<p>Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limits of transfer learning with a unified text-to-text transformer, 2020.</p>
<p>Romanov, A., De-Arteaga, M., Wallach, H., Chayes, J., Borgs, C., Chouldechova, A., Geyik, S., Kenthapadi, K., Rumshisky, A., and Kalai, A. What's in a name? Reducing bias in bios without access to protected attributes. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4187-4195, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1424. URL https://aclanthology.org/N19-1424.</p>
<p>Rudinger, R., Naradowsky, J., Leonard, B., and Van Durme, B. Gender bias in coreference resolution. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pp. 814, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-2002. URL https://aclanthology.org/N18-2002.</p>
<p>Saxton, D., Grefenstette, E., Hill, F., and Kohli, P. Analysing mathematical reasoning abilities of neural models. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https://openreview. net/forum?id=H1gR51R5FX.</p>
<p>Sharoff, S. Know thy corpus! robust methods for digital curation of web corpora. In Proceedings of the 12th Language Resources and Evaluation Conference, pp. 2453-2460, Marseille, France, May 2020. European Language Resources Association. ISBN 979-10-95546-34-4. URL https://aclanthology.org/2020. lrec-1.298.</p>
<p>Shin, T., Razeghi, Y., Logan IV, R. L., Wallace, E., and Singh, S. AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 42224235, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main. 346. URL https://aclanthology.org/2020. emnlp-main. 346.</p>
<p>Sinha, K., Jia, R., Hupkes, D., Pineau, J., Williams, A., and Kiela, D. Masked language modeling and the distributional hypothesis: Order word matters pre-training for little. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,</p>
<p>pp. 2888-2913, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main. 230. URL https://aclanthology.org/2021. emnlp-main. 230.</p>
<p>Søgaard, A., Ebert, S., Bastings, J., and Filippova, K. We need to talk about random splits. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pp. 1823-1832, Online, April 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.eacl-main. 156. URL https://aclanthology.org/2021. eacl-main. 156.</p>
<p>Song, C. and Shmatikov, V. Auditing data provenance in text-generation models. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \&amp; Data Mining, KDD '19, pp. 196-206, New York, NY, USA, 2019. Association for Computing Machinery. ISBN 9781450362016. doi: 10.1145/ 3292500.3330885. URL https://doi.org/10. 1145/3292500.3330885.</p>
<p>Spithourakis, G. and Riedel, S. Numeracy for language models: Evaluating and improving their ability to predict numbers. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 2104-2115, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-1196. URL https: //aclanthology.org/P18-1196.</p>
<p>Sugawara, S., Inui, K., Sekine, S., and Aizawa, A. What makes reading comprehension questions easier? In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 4208-4219, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1453. URL https://aclanthology.org/D18-1453.</p>
<p>Wallace, E., Wang, Y., Li, S., Singh, S., and Gardner, M. Do NLP models know numbers? probing numeracy in embeddings. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 5307-5315, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1534. URL https://aclanthology.org/D19-1534.</p>
<p>Wallace, E., Zhao, T., Feng, S., and Singh, S. Concealed data poisoning attacks on NLP models. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 139-150, Online,</p>
<p>June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.13. URL https: //aclanthology.org/2021.naacl-main.13.</p>
<p>Wang, B. Mesh-Transformer-JAX: Model-Parallel Implementation of Transformer Language Model with JAX. https://github.com/kingoflolz/ mesh-transformer-jax, May 2021.</p>
<p>Wei, J., Garrette, D., Linzen, T., and Pavlick, E. Frequency effects on syntactic rule learning in transformers. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 932-948, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.72. URL https: //aclanthology.org/2021.emnlp-main.72.</p>
<p>Weir, N., Poliak, A., and Durme, B. V. Probing neural language models for human tacit assumptions. In $\operatorname{CogSci}$, 2020.</p>
<p>Zhang, C., Ippolito, D., Lee, K., Jagielski, M., Tramèr, F., and Carlini, N. Counterfactual memorization in neural language models. arXiv preprint arXiv:2112.12938, 2021.</p>
<p>Zhou, B., Ning, Q., Khashabi, D., and Roth, D. Temporal common sense acquisition with minimal supervision. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 7579-7589, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.678. URL https: //aclanthology.org/2020.acl-main.678.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ https://turing.microsoft.com/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5175 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5175</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5175</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-109.html">extraction-schema-109</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of theories, models, or empirical findings about the representational format of conceptual knowledge in brains at a functional (not neural) level.</div>
                <p><strong>Paper ID:</strong> paper-257728655</p>
                <p><strong>Paper Title:</strong> Prototype Theory Meets Word Embedding: A Novel Approach for Text Categorization via Granular Computing</p>
                <p><strong>Paper Abstract:</strong> The problem of the information representation and interpretation coming from senses by the brain has plagued scientists for decades. The same problems, from a different perspective, hold in automated Pattern Recognition systems. Specifically, in solving various NLP tasks, an ever better and richer semantic representation of text as a set of features is needed and a plethora of text embedding techniques in algebraic spaces are continuously provided by researchers. These spaces are well suited to be conceived as conceptual spaces in light of the Gärdenfors’s Conceptual Space theory, which, within the Cognitive Science paradigm, seeks a geometrization of thought that bridges the gap between an associative lower level and a symbolic higher level in which information is organized and processed and where inductive reasoning is appropriate. Granular Computing can offer the toolbox for granulating text that can be represented by more abstract entities than words, offering a good hierarchical representation of the text embedded in an algebraic space driving Machine Learning applications, specifically, in text mining tasks. In this paper, the Conceptual Space Theory, the Granular Computing approach and Machine Learning are bound in a novel common framework for solving some text categorization tasks with both standard classifiers suited for working with ℝ n vectors and a Recurrent Neural Network (RNN) — an LSTM — able to deal with sequences. Instead of working with word vectors, the algorithms process more abstract entities (concepts), where patterns, in a first approach, are obtained through the construction of a symbolic histogram starting from a suitable set of information granules, representing a document as a distribution of concepts. For the RNN case, as a further novelty, a text is represented as a random walk over prototypes within the conceptual space synthesized over a suitable text embedding procedure. A comparison of the performance and a critical discussion are offered for both a neural embedding technique and the well-known LSA, showing how the conceptual level leads also to Knowledge Discovery applications.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5175.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5175.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of theories, models, or empirical findings about the representational format of conceptual knowledge in brains at a functional (not neural) level.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Conceptual Spaces</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Conceptual Spaces (Gärdenfors)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A geometric, mid-level representational framework that places concepts in a metric space of quality dimensions, where concepts correspond to regions (often convex) and prototypes to salient points in those regions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_name</strong></td>
                            <td>Conceptual Spaces</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_description</strong></td>
                            <td>Proposes a functional-level representation in which perceptual and conceptual information is organized in a multi-dimensional geometric space (quality dimensions); concepts are regions in this space and similarity/induction are governed by metric relations (distances, betweenness). It is explicitly presented as a middle level between sub-conceptual/associative representations and symbolic representations.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_format_type</strong></td>
                            <td>geometric/feature-based (regions in a metric space), prototype-centered</td>
                        </tr>
                        <tr>
                            <td><strong>key_properties</strong></td>
                            <td>quality dimensions (interpretable features), metric structure (distance, betweenness), concepts as regions (often convex), prototypes as salient central points, supports graded membership and similarity-based categorization, admits hierarchical/level-of-granularity representation</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_support</strong></td>
                            <td>Cited behavioral findings such as Rosch's color categorization and other prototype phenomena; the paper notes Conceptual Spaces' ability to model induction/generalization via metric properties and that they have been applied to semantics and some cognitive modeling tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_challenges</strong></td>
                            <td>Depends on choice of dimensions and metric; not all phenomena may be captured by simple Euclidean metrics (convexity of regions depends on metric); the paper notes broader unresolved issues relating to semantics and mapping from sub-conceptual data to interpretable dimensions.</td>
                        </tr>
                        <tr>
                            <td><strong>applied_domains_or_tasks</strong></td>
                            <td>Categorization, concept representation, semantic modeling, bridging distributional embeddings and symbolic reasoning (used here to build concept vocabularies for text classification), knowledge discovery/explainability</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Presented as a middle ground between symbolic (explicit rules, compositional symbols) and associationist/connectionist (sub-symbolic, associative) approaches: retains interpretability and handles similarity/induction better than symbolic approaches, while being more structured and interpretable than black-box associationist models.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_mechanisms</strong></td>
                            <td>Concepts formed as regions over interpretable quality dimensions; prototypes correspond to central/centroidal points; categorization performed by nearest-prototype rules (Voronoi/nearest-centroid); induction/generalization arise from metric relations (distance, betweenness) within the space.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_open_questions</strong></td>
                            <td>How to choose or learn appropriate quality dimensions from sensory/associative data; dependency on metric choice (Euclidean vs other metrics) for convexity and other properties; scaling to very high-dimensional, real-world semantic domains and linking to symbolic compositionality remain open issues.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5175.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5175.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of theories, models, or empirical findings about the representational format of conceptual knowledge in brains at a functional (not neural) level.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prototype Theory</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prototype Theory (Rosch)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A functional-level account of categorization in which categories are organized around prototypes—abstracted central tendencies—and membership is graded by similarity to the prototype.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_name</strong></td>
                            <td>Prototype Theory</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_description</strong></td>
                            <td>Conceptual knowledge is represented by prototypical exemplars (generalizations) rather than by necessary and sufficient conditions; category membership and typicality are determined by similarity to the prototype.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_format_type</strong></td>
                            <td>prototype/abstraction (central tendency), graded similarity-based representation</td>
                        </tr>
                        <tr>
                            <td><strong>key_properties</strong></td>
                            <td>graded membership/typicality, similarity-based classification, prototypes as abstractions/generalizations of exemplars, supports fast generalization from few examples</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_support</strong></td>
                            <td>Classic behavioral studies (e.g., color naming and typicality judgments) are cited as motivating prototype accounts; the paper uses prototype notions to justify centroidal/cluster-based concept prototypes in text embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_challenges</strong></td>
                            <td>Prototype representations may struggle with rule-governed categories, compositionality, and with accounting for effects better captured by exemplar or theory-based accounts; the paper implies no single approach is fully sufficient.</td>
                        </tr>
                        <tr>
                            <td><strong>applied_domains_or_tasks</strong></td>
                            <td>Categorization, concept learning, semantic representation (used here to derive prototypes via clustering in embedding spaces), cognitive modeling of typicality effects</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Contrasted with symbolic (rule-based) and associationist/exemplar models: Prototype Theory handles graded similarity and induction better than strict symbolic rules, but is more abstract than exemplar storage and less explicit than symbolic representations.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_mechanisms</strong></td>
                            <td>Prototypes are formed as abstractions/centroids over exemplars; classification by similarity (distance) to prototype; typicality effects follow distance from prototype.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_open_questions</strong></td>
                            <td>How prototypes are learned from sparse data, how they combine compositionally, and when exemplar or theory-based representations are needed instead remain open; mapping prototypes onto learned distributed embeddings requires choices (number of prototypes / granularity).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5175.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5175.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of theories, models, or empirical findings about the representational format of conceptual knowledge in brains at a functional (not neural) level.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Associationist / Connectionist</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Associationist / Connectionist (sub-conceptual) representations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Sub-symbolic representations in which information is carried by patterns of associations (e.g., distributed neural network weights) rather than by explicit symbols or geometric regions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_name</strong></td>
                            <td>Associationist / Connectionist representations</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_description</strong></td>
                            <td>Represents concepts as patterns of association in a distributed substrate (e.g., neural networks); knowledge is encoded in learned associations rather than in explicit symbolic tokens or geometric regions.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_format_type</strong></td>
                            <td>distributed, sub-symbolic (connectionist patterns/weights)</td>
                        </tr>
                        <tr>
                            <td><strong>key_properties</strong></td>
                            <td>distributed encoding, graceful degradation, associative learning, often opaque/black-box internal structure, supports pattern completion and statistical generalization</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_support</strong></td>
                            <td>Connectionist models have been successful in many learning tasks and the paper cites them as a special case of associationism, providing an associative layer for building conceptual embeddings (e.g., word2vec as associative/statistical embedding).</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_challenges</strong></td>
                            <td>Black-box nature and lack of transparent mechanisms for symbolic reasoning/compositionality are highlighted; paper argues associationist approaches alone do not capture all cognitive phenomena.</td>
                        </tr>
                        <tr>
                            <td><strong>applied_domains_or_tasks</strong></td>
                            <td>Modeling associative learning, distributional word embeddings, pattern recognition tasks; used as the 'associative layer' feeding conceptual-space constructions in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Seen as complementary to symbolic models but lacking interpretability; Conceptual Spaces are proposed as an intermediate that can incorporate associative data while supporting symbolic-like reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_mechanisms</strong></td>
                            <td>Learning via adjusting connection weights from co-occurrence statistics; similarity emerges from overlap in distributed patterns; retrieval by pattern activation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_open_questions</strong></td>
                            <td>How to extract interpretable, symbolic-like structure from distributed weights; how to capture structured/compositional knowledge functionally remains unresolved.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5175.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5175.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of theories, models, or empirical findings about the representational format of conceptual knowledge in brains at a functional (not neural) level.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Symbolic Representations</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Symbolic Representations (rule-based, token-based)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A representation paradigm where cognitive content is encoded as discrete symbols manipulated by rule-like operations (akin to Turing-machine level descriptions).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_name</strong></td>
                            <td>Symbolic representation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_description</strong></td>
                            <td>Conceptual knowledge is represented as explicit symbols and symbolic structures (e.g., logical formulas, tokens), with operations on these symbols implementing reasoning and compositionality.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_format_type</strong></td>
                            <td>symbolic, discrete, compositional</td>
                        </tr>
                        <tr>
                            <td><strong>key_properties</strong></td>
                            <td>explicit compositional structure, interpretable rules, supports formal logical inference, typically extrinsic representation requiring interpretation rules</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_support</strong></td>
                            <td>Symbolic models excel at rule-based reasoning and compositional tasks in modeling aspects of cognition but are argued (in the paper) to be inadequate for modeling similarity-based induction and some creative conceptual processes.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_challenges</strong></td>
                            <td>Lacks natural similarity metrics and struggles with induction/generalization from perceptual data; considered at too-high an abstraction for some cognitive phenomena; requires linking to conceptual/sub-conceptual levels.</td>
                        </tr>
                        <tr>
                            <td><strong>applied_domains_or_tasks</strong></td>
                            <td>Formal reasoning, rule-based systems, symbolic AI, knowledge representation where explicit structure is required</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Contrasted with associationist models (which are sub-symbolic and associative) and Conceptual Spaces (which act as a middle layer bridging symbolic and associative representations).</td>
                        </tr>
                        <tr>
                            <td><strong>functional_mechanisms</strong></td>
                            <td>Manipulation of discrete symbols according to syntactic/semantic rules; meaning often provided extrinsically via interpretation rules.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_open_questions</strong></td>
                            <td>How symbolic representations emerge from perception and how they interact with graded similarity-based representations remain key open issues.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5175.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5175.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of theories, models, or empirical findings about the representational format of conceptual knowledge in brains at a functional (not neural) level.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Distributional Semantics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Distributional Semantics / Distributional Hypothesis</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of functional-level models which represent lexical meaning via patterns of word co-occurrence in contexts, operationalized as vectors in algebraic spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_name</strong></td>
                            <td>Distributional Semantics (distributional hypothesis)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_description</strong></td>
                            <td>Asserts that meaning correlates with distributional patterns: words that occur in similar contexts have similar meanings; operationalized by embedding words/documents into vector spaces (e.g., BoW, HAL, LSA, word2vec).</td>
                        </tr>
                        <tr>
                            <td><strong>representation_format_type</strong></td>
                            <td>distributed vector embeddings (statistical, co-occurrence based)</td>
                        </tr>
                        <tr>
                            <td><strong>key_properties</strong></td>
                            <td>context-based similarity, supports dense or sparse vector representations, captures lexical semantic similarity, scalable to large corpora, forms the associative/sub-conceptual layer in the paper's pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_support</strong></td>
                            <td>Widely used in NLP with empirical success across semantic similarity, classification and retrieval tasks; the paper uses word2vec and LSA as concrete DS instantiations and cites their utility and empirical performance.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_challenges</strong></td>
                            <td>Raw co-occurrence models suffer sparsity, high dimensionality, synonymy issues (symbols treated as distinct), and can lack explicit compositional structure; require additional transforms (e.g., SVD) or neural training to produce dense useful embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>applied_domains_or_tasks</strong></td>
                            <td>NLP tasks (semantic similarity, text classification, retrieval), used as associative input to construct conceptual spaces and bag-of-concepts representations in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Serves as the associative/sub-conceptual input contrasted with symbolic and conceptual-space approaches; can be turned into more interpretable conceptual representations by clustering/partitioning (the paper's method).</td>
                        </tr>
                        <tr>
                            <td><strong>functional_mechanisms</strong></td>
                            <td>Constructs vector representations from statistical counts or predictive neural training (windowed contexts); semantic similarity arises from geometric proximity in embedding space.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_open_questions</strong></td>
                            <td>How to map distributed vectors to interpretable quality dimensions and regions (for conceptual-level reasoning), and how to handle compositional semantics and prototype-like category structure are open questions.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5175.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5175.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of theories, models, or empirical findings about the representational format of conceptual knowledge in brains at a functional (not neural) level.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Latent Semantic Analysis (LSA)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Latent Semantic Analysis (LSA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A linear-algebraic distributional model that produces dense latent vectors for words/documents via truncated singular value decomposition, sometimes proposed as a cognitive model of language.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_name</strong></td>
                            <td>Latent Semantic Analysis (LSA)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_description</strong></td>
                            <td>Maps word-document co-occurrence matrices into a reduced latent linear space using SVD; captures higher-order co-occurrence structure, reduces dimensionality and noise, and has been proposed to model human semantic memory at a functional level.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_format_type</strong></td>
                            <td>dense latent vector space (linear dimensionality reduction)</td>
                        </tr>
                        <tr>
                            <td><strong>key_properties</strong></td>
                            <td>noise reduction via truncated SVD, low-dimensional continuous representations, captures latent semantic relations beyond direct co-occurrence, sometimes aligns with human semantic judgments</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_support</strong></td>
                            <td>LSA has been used successfully in semantic tasks and is cited in the paper as having been proposed as a cognitive model for human language use; empirically often yields good performance in semantic similarity and classification.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_challenges</strong></td>
                            <td>LSA's linear assumptions and choice of truncation affect results; may not capture some nonlinear or contextual phenomena later handled by neural models; mapping between LSA components and interpretable quality dimensions can be indirect.</td>
                        </tr>
                        <tr>
                            <td><strong>applied_domains_or_tasks</strong></td>
                            <td>Text classification, semantic similarity, information retrieval; in this paper LSA is used to build the associative embedding that is then conceptualized via clustering into prototypes.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Compared to predictive neural embeddings (word2vec), LSA is an explicit matrix-factorization approach; both can serve as associative inputs to conceptual-space processing, with differing empirical tradeoffs.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_mechanisms</strong></td>
                            <td>Dimensionality reduction of co-occurrence matrices yields latent factors; similarity measured by vector proximity in reduced space; prototypes/clusters can then be computed on these vectors.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_open_questions</strong></td>
                            <td>How well LSA-derived dimensions correspond to psychologically interpretable quality dimensions and how to integrate LSA with structured conceptual representations remain open.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5175.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5175.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of theories, models, or empirical findings about the representational format of conceptual knowledge in brains at a functional (not neural) level.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Criterion P (Convexity)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Criterion P: Natural properties as convex regions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A principle in Conceptual Spaces theory stating that natural properties/concepts correspond to convex regions in a domain of quality dimensions, ensuring betweenness and graded membership.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_name</strong></td>
                            <td>Criterion P (Convexity)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_description</strong></td>
                            <td>Functionally posits that for a concept to be a natural property, the set of points (representations) belonging to it should form a convex region in the conceptual space, so any between-point also belongs to the concept.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_format_type</strong></td>
                            <td>geometric region constraint (convex regions in metric spaces)</td>
                        </tr>
                        <tr>
                            <td><strong>key_properties</strong></td>
                            <td>convexity (betweenness), enforces that interpolations between instances remain within the concept, supports coherent prototype-centred organization</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_support</strong></td>
                            <td>The paper cites color perception and naming as an example where convexity is maintained empirically, supporting the plausibility of the criterion.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_challenges</strong></td>
                            <td>Convexity depends on the choice of metric; not all metrics guarantee convex partitions, and some real-world concept domains may not be well modeled as convex under simple metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>applied_domains_or_tasks</strong></td>
                            <td>Used within Conceptual Spaces-based modeling of properties and categories (e.g., color categories), informs partitioning decisions (e.g., suitability of Euclidean metric and Voronoi regions).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Provides a geometric constraint that symbolic or purely associative accounts do not explicitly encode; contrasts with exemplar or rule-based representations that do not require convexity.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_mechanisms</strong></td>
                            <td>Operationalizes concept membership via spatial containment in convex regions; prototypes often correspond to central points of these regions.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_open_questions</strong></td>
                            <td>Which metrics and learned dimensions yield empirically valid convex regions for varied semantic domains is unresolved; applicability beyond perceptual domains like color is an open question.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5175.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e5175.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of theories, models, or empirical findings about the representational format of conceptual knowledge in brains at a functional (not neural) level.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Voronoi / Nearest-Prototype</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Nearest-prototype / Voronoi partitioning model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A functional categorization mechanism that partitions a metric space into regions (Voronoi cells) around prototypes so that each item is assigned to its nearest prototype by distance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_name</strong></td>
                            <td>Nearest-prototype (Voronoi) model</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_description</strong></td>
                            <td>Represents categories by prototype points and implements classification by assigning any exemplar to the category of the nearest prototype, yielding a Voronoi tessellation of the space.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_format_type</strong></td>
                            <td>prototype-based partitioning (geometric/Voronoi regions)</td>
                        </tr>
                        <tr>
                            <td><strong>key_properties</strong></td>
                            <td>centroid/prototype-centered categorization, partitioning of space into nearest-prototype cells, relies on underlying metric (distance), naturally implements typicality by proximity</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_support</strong></td>
                            <td>Used computationally and conceptually in the paper (k-means centroids as prototypes and Voronoi regions) to derive symbol vocabularies; aligns with prototype theory and computational clustering methods.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_challenges</strong></td>
                            <td>Performance and cognitive plausibility depend on choice of metric, number of prototypes (granularity), and whether prototypes correspond to psychologically salient exemplars; ignores sequence/structural context unless extended.</td>
                        </tr>
                        <tr>
                            <td><strong>applied_domains_or_tasks</strong></td>
                            <td>Clustering, categorization, deriving bag-of-concepts representations, converting distributed embeddings into discrete concept symbols (this paper's method uses Voronoi clusters from embedding spaces).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>A simple, computationally tractable realization of prototype theory contrasted with exemplar or rule-based approaches; provides a direct mapping from metric embeddings to symbolic categories.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_mechanisms</strong></td>
                            <td>Compute centroids/prototypes (e.g., via k-means), assign exemplars to nearest prototype by Euclidean distance to create discrete concept regions; supports histogram/probabilistic document representations over prototypes.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_open_questions</strong></td>
                            <td>Selecting granularity (number of prototypes), metric choice, handling non-convex categories, and preserving contextual/sequential information during symbolization are open practical and theoretical issues.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5175.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e5175.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of theories, models, or empirical findings about the representational format of conceptual knowledge in brains at a functional (not neural) level.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Intrinsic vs Extrinsic Representation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Intrinsic vs Extrinsic Representation (Palmer distinction)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A functional distinction: intrinsic representations preserve the structure of what they represent within the representation itself, while extrinsic representations require an external rule to map representation to meaning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_name</strong></td>
                            <td>Intrinsic vs Extrinsic representation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_description</strong></td>
                            <td>Intrinsic representations are structurally isomorphic to the represented relations (meaning emerges from representation structure itself), whereas extrinsic representations are arbitrary symbols that need interpretation rules to yield meaning.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_format_type</strong></td>
                            <td>representational-interpretive distinction (structural vs rule-based symbols)</td>
                        </tr>
                        <tr>
                            <td><strong>key_properties</strong></td>
                            <td>intrinsic = structure-preserving and directly interpretable; extrinsic = arbitrary symbols needing interpretation rules; affects cognitive plausibility and learnability of representations</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_support</strong></td>
                            <td>Used in the paper to argue that symbolic BoW counts are extrinsic and thus lose relational structure, motivating the use of conceptual/geometric representations that preserve relational structure intrinsically.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_challenges</strong></td>
                            <td>Many practical systems use extrinsic representations successfully; establishing what counts as sufficiently intrinsic for cognitive plausibility depends on chosen dimensions and mappings.</td>
                        </tr>
                        <tr>
                            <td><strong>applied_domains_or_tasks</strong></td>
                            <td>Theoretical evaluation of representation formats in cognitive modeling and NLP (used to justify conceptual-space approaches over plain symbolic counts).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Highlights a weakness of purely symbolic/extrinsic representations (e.g., BoW) relative to geometric/intrinsic representations (e.g., Conceptual Spaces, distributional embeddings when structured appropriately).</td>
                        </tr>
                        <tr>
                            <td><strong>functional_mechanisms</strong></td>
                            <td>Determines whether meaning is carried by internal structure (intrinsic) or by a separate interpretation rule (extrinsic); guides choices of representation learning and interpretability strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_open_questions</strong></td>
                            <td>How to operationalize intrinsic structure for complex semantic domains and how to learn such representations from data remain open.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5175.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e5175.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of theories, models, or empirical findings about the representational format of conceptual knowledge in brains at a functional (not neural) level.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bag of Concepts / Symbolic Histograms</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bag of Concepts (BoC) / Symbolic Histograms (Granular Computing)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An applied representational method that abstracts words into concept symbols (prototypes) via clustering of embeddings and represents documents as histograms/distributions over those concept symbols.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_name</strong></td>
                            <td>Bag of Concepts / Symbolic Histograms</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_description</strong></td>
                            <td>Constructs a symbolic alphabet of concepts by partitioning an associative embedding space into prototype regions (granules), then represents documents either as counts (histograms) over these concept symbols or as sequences/centroid averages of prototype vectors.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_format_type</strong></td>
                            <td>symbolic histogram / discrete distribution over learned prototypes (hybrid: derived from distributed embeddings)</td>
                        </tr>
                        <tr>
                            <td><strong>key_properties</strong></td>
                            <td>compresses high-dimensional distributed representations into a lower-dimensional, interpretable symbolic space; supports both bag (orderless) and sequence-based representations; allows standard ML algorithms to operate on concept-level features; enables explainability via prototype inspection.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_support</strong></td>
                            <td>Implemented and evaluated in this paper: clustering of LSA/word2vec embeddings into prototypes, construction of symbolic histograms and sequences, and successful application to text categorization (Reuters-21578 and Abstracts datasets) with competitive accuracy across classifiers.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_challenges</strong></td>
                            <td>Information loss when converting distributed vectors to discrete symbols; sensitivity to granularity (number of clusters/prototypes), clustering algorithm, and metric choice; possible degradation for small data or rare contexts if prototypes are poor.</td>
                        </tr>
                        <tr>
                            <td><strong>applied_domains_or_tasks</strong></td>
                            <td>Text categorization/classification, knowledge discovery/explainability in NLP, sequence modeling via prototype sequences (LSTM), bridging distributed embeddings and symbolic ML pipelines (this paper's experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Serves as a pragmatic bridge from distributional/associative representations to symbolic feature spaces; compared experimentally to TF-IDF and raw LSA/word2vec features in the paper and shown to provide compact, explainable features without substantial performance loss.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_mechanisms</strong></td>
                            <td>Cluster embedding space via k-means to obtain prototypes (concept symbols), map each word to nearest prototype, form document-level histograms (counts or normalized distributions) or prototype sequences, then apply classifiers (SVM, RF, LSTM).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_open_questions</strong></td>
                            <td>How to choose optimal granularity k, how to learn concept vocabularies transferable across corpora, and how to extend prototypes to n-grams or handle sequence-sensitive meanings are identified as directions for future work.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Conceptual spaces: The geometry of thought <em>(Rating: 2)</em></li>
                <li>Natural categories <em>(Rating: 2)</em></li>
                <li>A solution to Plato's problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge <em>(Rating: 2)</em></li>
                <li>Distributional structure <em>(Rating: 1)</em></li>
                <li>Toward a theory of fuzzy information granulation and its centrality in human reasoning and fuzzy logic <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5175",
    "paper_id": "paper-257728655",
    "extraction_schema_id": "extraction-schema-109",
    "extracted_data": [
        {
            "name_short": "Conceptual Spaces",
            "name_full": "Conceptual Spaces (Gärdenfors)",
            "brief_description": "A geometric, mid-level representational framework that places concepts in a metric space of quality dimensions, where concepts correspond to regions (often convex) and prototypes to salient points in those regions.",
            "citation_title": "",
            "mention_or_use": "use",
            "theory_or_model_name": "Conceptual Spaces",
            "theory_or_model_description": "Proposes a functional-level representation in which perceptual and conceptual information is organized in a multi-dimensional geometric space (quality dimensions); concepts are regions in this space and similarity/induction are governed by metric relations (distances, betweenness). It is explicitly presented as a middle level between sub-conceptual/associative representations and symbolic representations.",
            "representation_format_type": "geometric/feature-based (regions in a metric space), prototype-centered",
            "key_properties": "quality dimensions (interpretable features), metric structure (distance, betweenness), concepts as regions (often convex), prototypes as salient central points, supports graded membership and similarity-based categorization, admits hierarchical/level-of-granularity representation",
            "empirical_support": "Cited behavioral findings such as Rosch's color categorization and other prototype phenomena; the paper notes Conceptual Spaces' ability to model induction/generalization via metric properties and that they have been applied to semantics and some cognitive modeling tasks.",
            "empirical_challenges": "Depends on choice of dimensions and metric; not all phenomena may be captured by simple Euclidean metrics (convexity of regions depends on metric); the paper notes broader unresolved issues relating to semantics and mapping from sub-conceptual data to interpretable dimensions.",
            "applied_domains_or_tasks": "Categorization, concept representation, semantic modeling, bridging distributional embeddings and symbolic reasoning (used here to build concept vocabularies for text classification), knowledge discovery/explainability",
            "comparison_to_other_models": "Presented as a middle ground between symbolic (explicit rules, compositional symbols) and associationist/connectionist (sub-symbolic, associative) approaches: retains interpretability and handles similarity/induction better than symbolic approaches, while being more structured and interpretable than black-box associationist models.",
            "functional_mechanisms": "Concepts formed as regions over interpretable quality dimensions; prototypes correspond to central/centroidal points; categorization performed by nearest-prototype rules (Voronoi/nearest-centroid); induction/generalization arise from metric relations (distance, betweenness) within the space.",
            "limitations_or_open_questions": "How to choose or learn appropriate quality dimensions from sensory/associative data; dependency on metric choice (Euclidean vs other metrics) for convexity and other properties; scaling to very high-dimensional, real-world semantic domains and linking to symbolic compositionality remain open issues.",
            "uuid": "e5175.0"
        },
        {
            "name_short": "Prototype Theory",
            "name_full": "Prototype Theory (Rosch)",
            "brief_description": "A functional-level account of categorization in which categories are organized around prototypes—abstracted central tendencies—and membership is graded by similarity to the prototype.",
            "citation_title": "",
            "mention_or_use": "use",
            "theory_or_model_name": "Prototype Theory",
            "theory_or_model_description": "Conceptual knowledge is represented by prototypical exemplars (generalizations) rather than by necessary and sufficient conditions; category membership and typicality are determined by similarity to the prototype.",
            "representation_format_type": "prototype/abstraction (central tendency), graded similarity-based representation",
            "key_properties": "graded membership/typicality, similarity-based classification, prototypes as abstractions/generalizations of exemplars, supports fast generalization from few examples",
            "empirical_support": "Classic behavioral studies (e.g., color naming and typicality judgments) are cited as motivating prototype accounts; the paper uses prototype notions to justify centroidal/cluster-based concept prototypes in text embeddings.",
            "empirical_challenges": "Prototype representations may struggle with rule-governed categories, compositionality, and with accounting for effects better captured by exemplar or theory-based accounts; the paper implies no single approach is fully sufficient.",
            "applied_domains_or_tasks": "Categorization, concept learning, semantic representation (used here to derive prototypes via clustering in embedding spaces), cognitive modeling of typicality effects",
            "comparison_to_other_models": "Contrasted with symbolic (rule-based) and associationist/exemplar models: Prototype Theory handles graded similarity and induction better than strict symbolic rules, but is more abstract than exemplar storage and less explicit than symbolic representations.",
            "functional_mechanisms": "Prototypes are formed as abstractions/centroids over exemplars; classification by similarity (distance) to prototype; typicality effects follow distance from prototype.",
            "limitations_or_open_questions": "How prototypes are learned from sparse data, how they combine compositionally, and when exemplar or theory-based representations are needed instead remain open; mapping prototypes onto learned distributed embeddings requires choices (number of prototypes / granularity).",
            "uuid": "e5175.1"
        },
        {
            "name_short": "Associationist / Connectionist",
            "name_full": "Associationist / Connectionist (sub-conceptual) representations",
            "brief_description": "Sub-symbolic representations in which information is carried by patterns of associations (e.g., distributed neural network weights) rather than by explicit symbols or geometric regions.",
            "citation_title": "",
            "mention_or_use": "mention",
            "theory_or_model_name": "Associationist / Connectionist representations",
            "theory_or_model_description": "Represents concepts as patterns of association in a distributed substrate (e.g., neural networks); knowledge is encoded in learned associations rather than in explicit symbolic tokens or geometric regions.",
            "representation_format_type": "distributed, sub-symbolic (connectionist patterns/weights)",
            "key_properties": "distributed encoding, graceful degradation, associative learning, often opaque/black-box internal structure, supports pattern completion and statistical generalization",
            "empirical_support": "Connectionist models have been successful in many learning tasks and the paper cites them as a special case of associationism, providing an associative layer for building conceptual embeddings (e.g., word2vec as associative/statistical embedding).",
            "empirical_challenges": "Black-box nature and lack of transparent mechanisms for symbolic reasoning/compositionality are highlighted; paper argues associationist approaches alone do not capture all cognitive phenomena.",
            "applied_domains_or_tasks": "Modeling associative learning, distributional word embeddings, pattern recognition tasks; used as the 'associative layer' feeding conceptual-space constructions in this paper.",
            "comparison_to_other_models": "Seen as complementary to symbolic models but lacking interpretability; Conceptual Spaces are proposed as an intermediate that can incorporate associative data while supporting symbolic-like reasoning.",
            "functional_mechanisms": "Learning via adjusting connection weights from co-occurrence statistics; similarity emerges from overlap in distributed patterns; retrieval by pattern activation.",
            "limitations_or_open_questions": "How to extract interpretable, symbolic-like structure from distributed weights; how to capture structured/compositional knowledge functionally remains unresolved.",
            "uuid": "e5175.2"
        },
        {
            "name_short": "Symbolic Representations",
            "name_full": "Symbolic Representations (rule-based, token-based)",
            "brief_description": "A representation paradigm where cognitive content is encoded as discrete symbols manipulated by rule-like operations (akin to Turing-machine level descriptions).",
            "citation_title": "",
            "mention_or_use": "mention",
            "theory_or_model_name": "Symbolic representation",
            "theory_or_model_description": "Conceptual knowledge is represented as explicit symbols and symbolic structures (e.g., logical formulas, tokens), with operations on these symbols implementing reasoning and compositionality.",
            "representation_format_type": "symbolic, discrete, compositional",
            "key_properties": "explicit compositional structure, interpretable rules, supports formal logical inference, typically extrinsic representation requiring interpretation rules",
            "empirical_support": "Symbolic models excel at rule-based reasoning and compositional tasks in modeling aspects of cognition but are argued (in the paper) to be inadequate for modeling similarity-based induction and some creative conceptual processes.",
            "empirical_challenges": "Lacks natural similarity metrics and struggles with induction/generalization from perceptual data; considered at too-high an abstraction for some cognitive phenomena; requires linking to conceptual/sub-conceptual levels.",
            "applied_domains_or_tasks": "Formal reasoning, rule-based systems, symbolic AI, knowledge representation where explicit structure is required",
            "comparison_to_other_models": "Contrasted with associationist models (which are sub-symbolic and associative) and Conceptual Spaces (which act as a middle layer bridging symbolic and associative representations).",
            "functional_mechanisms": "Manipulation of discrete symbols according to syntactic/semantic rules; meaning often provided extrinsically via interpretation rules.",
            "limitations_or_open_questions": "How symbolic representations emerge from perception and how they interact with graded similarity-based representations remain key open issues.",
            "uuid": "e5175.3"
        },
        {
            "name_short": "Distributional Semantics",
            "name_full": "Distributional Semantics / Distributional Hypothesis",
            "brief_description": "A family of functional-level models which represent lexical meaning via patterns of word co-occurrence in contexts, operationalized as vectors in algebraic spaces.",
            "citation_title": "",
            "mention_or_use": "use",
            "theory_or_model_name": "Distributional Semantics (distributional hypothesis)",
            "theory_or_model_description": "Asserts that meaning correlates with distributional patterns: words that occur in similar contexts have similar meanings; operationalized by embedding words/documents into vector spaces (e.g., BoW, HAL, LSA, word2vec).",
            "representation_format_type": "distributed vector embeddings (statistical, co-occurrence based)",
            "key_properties": "context-based similarity, supports dense or sparse vector representations, captures lexical semantic similarity, scalable to large corpora, forms the associative/sub-conceptual layer in the paper's pipeline",
            "empirical_support": "Widely used in NLP with empirical success across semantic similarity, classification and retrieval tasks; the paper uses word2vec and LSA as concrete DS instantiations and cites their utility and empirical performance.",
            "empirical_challenges": "Raw co-occurrence models suffer sparsity, high dimensionality, synonymy issues (symbols treated as distinct), and can lack explicit compositional structure; require additional transforms (e.g., SVD) or neural training to produce dense useful embeddings.",
            "applied_domains_or_tasks": "NLP tasks (semantic similarity, text classification, retrieval), used as associative input to construct conceptual spaces and bag-of-concepts representations in the paper.",
            "comparison_to_other_models": "Serves as the associative/sub-conceptual input contrasted with symbolic and conceptual-space approaches; can be turned into more interpretable conceptual representations by clustering/partitioning (the paper's method).",
            "functional_mechanisms": "Constructs vector representations from statistical counts or predictive neural training (windowed contexts); semantic similarity arises from geometric proximity in embedding space.",
            "limitations_or_open_questions": "How to map distributed vectors to interpretable quality dimensions and regions (for conceptual-level reasoning), and how to handle compositional semantics and prototype-like category structure are open questions.",
            "uuid": "e5175.4"
        },
        {
            "name_short": "Latent Semantic Analysis (LSA)",
            "name_full": "Latent Semantic Analysis (LSA)",
            "brief_description": "A linear-algebraic distributional model that produces dense latent vectors for words/documents via truncated singular value decomposition, sometimes proposed as a cognitive model of language.",
            "citation_title": "",
            "mention_or_use": "use",
            "theory_or_model_name": "Latent Semantic Analysis (LSA)",
            "theory_or_model_description": "Maps word-document co-occurrence matrices into a reduced latent linear space using SVD; captures higher-order co-occurrence structure, reduces dimensionality and noise, and has been proposed to model human semantic memory at a functional level.",
            "representation_format_type": "dense latent vector space (linear dimensionality reduction)",
            "key_properties": "noise reduction via truncated SVD, low-dimensional continuous representations, captures latent semantic relations beyond direct co-occurrence, sometimes aligns with human semantic judgments",
            "empirical_support": "LSA has been used successfully in semantic tasks and is cited in the paper as having been proposed as a cognitive model for human language use; empirically often yields good performance in semantic similarity and classification.",
            "empirical_challenges": "LSA's linear assumptions and choice of truncation affect results; may not capture some nonlinear or contextual phenomena later handled by neural models; mapping between LSA components and interpretable quality dimensions can be indirect.",
            "applied_domains_or_tasks": "Text classification, semantic similarity, information retrieval; in this paper LSA is used to build the associative embedding that is then conceptualized via clustering into prototypes.",
            "comparison_to_other_models": "Compared to predictive neural embeddings (word2vec), LSA is an explicit matrix-factorization approach; both can serve as associative inputs to conceptual-space processing, with differing empirical tradeoffs.",
            "functional_mechanisms": "Dimensionality reduction of co-occurrence matrices yields latent factors; similarity measured by vector proximity in reduced space; prototypes/clusters can then be computed on these vectors.",
            "limitations_or_open_questions": "How well LSA-derived dimensions correspond to psychologically interpretable quality dimensions and how to integrate LSA with structured conceptual representations remain open.",
            "uuid": "e5175.5"
        },
        {
            "name_short": "Criterion P (Convexity)",
            "name_full": "Criterion P: Natural properties as convex regions",
            "brief_description": "A principle in Conceptual Spaces theory stating that natural properties/concepts correspond to convex regions in a domain of quality dimensions, ensuring betweenness and graded membership.",
            "citation_title": "",
            "mention_or_use": "mention",
            "theory_or_model_name": "Criterion P (Convexity)",
            "theory_or_model_description": "Functionally posits that for a concept to be a natural property, the set of points (representations) belonging to it should form a convex region in the conceptual space, so any between-point also belongs to the concept.",
            "representation_format_type": "geometric region constraint (convex regions in metric spaces)",
            "key_properties": "convexity (betweenness), enforces that interpolations between instances remain within the concept, supports coherent prototype-centred organization",
            "empirical_support": "The paper cites color perception and naming as an example where convexity is maintained empirically, supporting the plausibility of the criterion.",
            "empirical_challenges": "Convexity depends on the choice of metric; not all metrics guarantee convex partitions, and some real-world concept domains may not be well modeled as convex under simple metrics.",
            "applied_domains_or_tasks": "Used within Conceptual Spaces-based modeling of properties and categories (e.g., color categories), informs partitioning decisions (e.g., suitability of Euclidean metric and Voronoi regions).",
            "comparison_to_other_models": "Provides a geometric constraint that symbolic or purely associative accounts do not explicitly encode; contrasts with exemplar or rule-based representations that do not require convexity.",
            "functional_mechanisms": "Operationalizes concept membership via spatial containment in convex regions; prototypes often correspond to central points of these regions.",
            "limitations_or_open_questions": "Which metrics and learned dimensions yield empirically valid convex regions for varied semantic domains is unresolved; applicability beyond perceptual domains like color is an open question.",
            "uuid": "e5175.6"
        },
        {
            "name_short": "Voronoi / Nearest-Prototype",
            "name_full": "Nearest-prototype / Voronoi partitioning model",
            "brief_description": "A functional categorization mechanism that partitions a metric space into regions (Voronoi cells) around prototypes so that each item is assigned to its nearest prototype by distance.",
            "citation_title": "",
            "mention_or_use": "use",
            "theory_or_model_name": "Nearest-prototype (Voronoi) model",
            "theory_or_model_description": "Represents categories by prototype points and implements classification by assigning any exemplar to the category of the nearest prototype, yielding a Voronoi tessellation of the space.",
            "representation_format_type": "prototype-based partitioning (geometric/Voronoi regions)",
            "key_properties": "centroid/prototype-centered categorization, partitioning of space into nearest-prototype cells, relies on underlying metric (distance), naturally implements typicality by proximity",
            "empirical_support": "Used computationally and conceptually in the paper (k-means centroids as prototypes and Voronoi regions) to derive symbol vocabularies; aligns with prototype theory and computational clustering methods.",
            "empirical_challenges": "Performance and cognitive plausibility depend on choice of metric, number of prototypes (granularity), and whether prototypes correspond to psychologically salient exemplars; ignores sequence/structural context unless extended.",
            "applied_domains_or_tasks": "Clustering, categorization, deriving bag-of-concepts representations, converting distributed embeddings into discrete concept symbols (this paper's method uses Voronoi clusters from embedding spaces).",
            "comparison_to_other_models": "A simple, computationally tractable realization of prototype theory contrasted with exemplar or rule-based approaches; provides a direct mapping from metric embeddings to symbolic categories.",
            "functional_mechanisms": "Compute centroids/prototypes (e.g., via k-means), assign exemplars to nearest prototype by Euclidean distance to create discrete concept regions; supports histogram/probabilistic document representations over prototypes.",
            "limitations_or_open_questions": "Selecting granularity (number of prototypes), metric choice, handling non-convex categories, and preserving contextual/sequential information during symbolization are open practical and theoretical issues.",
            "uuid": "e5175.7"
        },
        {
            "name_short": "Intrinsic vs Extrinsic Representation",
            "name_full": "Intrinsic vs Extrinsic Representation (Palmer distinction)",
            "brief_description": "A functional distinction: intrinsic representations preserve the structure of what they represent within the representation itself, while extrinsic representations require an external rule to map representation to meaning.",
            "citation_title": "",
            "mention_or_use": "mention",
            "theory_or_model_name": "Intrinsic vs Extrinsic representation",
            "theory_or_model_description": "Intrinsic representations are structurally isomorphic to the represented relations (meaning emerges from representation structure itself), whereas extrinsic representations are arbitrary symbols that need interpretation rules to yield meaning.",
            "representation_format_type": "representational-interpretive distinction (structural vs rule-based symbols)",
            "key_properties": "intrinsic = structure-preserving and directly interpretable; extrinsic = arbitrary symbols needing interpretation rules; affects cognitive plausibility and learnability of representations",
            "empirical_support": "Used in the paper to argue that symbolic BoW counts are extrinsic and thus lose relational structure, motivating the use of conceptual/geometric representations that preserve relational structure intrinsically.",
            "empirical_challenges": "Many practical systems use extrinsic representations successfully; establishing what counts as sufficiently intrinsic for cognitive plausibility depends on chosen dimensions and mappings.",
            "applied_domains_or_tasks": "Theoretical evaluation of representation formats in cognitive modeling and NLP (used to justify conceptual-space approaches over plain symbolic counts).",
            "comparison_to_other_models": "Highlights a weakness of purely symbolic/extrinsic representations (e.g., BoW) relative to geometric/intrinsic representations (e.g., Conceptual Spaces, distributional embeddings when structured appropriately).",
            "functional_mechanisms": "Determines whether meaning is carried by internal structure (intrinsic) or by a separate interpretation rule (extrinsic); guides choices of representation learning and interpretability strategies.",
            "limitations_or_open_questions": "How to operationalize intrinsic structure for complex semantic domains and how to learn such representations from data remain open.",
            "uuid": "e5175.8"
        },
        {
            "name_short": "Bag of Concepts / Symbolic Histograms",
            "name_full": "Bag of Concepts (BoC) / Symbolic Histograms (Granular Computing)",
            "brief_description": "An applied representational method that abstracts words into concept symbols (prototypes) via clustering of embeddings and represents documents as histograms/distributions over those concept symbols.",
            "citation_title": "here",
            "mention_or_use": "use",
            "theory_or_model_name": "Bag of Concepts / Symbolic Histograms",
            "theory_or_model_description": "Constructs a symbolic alphabet of concepts by partitioning an associative embedding space into prototype regions (granules), then represents documents either as counts (histograms) over these concept symbols or as sequences/centroid averages of prototype vectors.",
            "representation_format_type": "symbolic histogram / discrete distribution over learned prototypes (hybrid: derived from distributed embeddings)",
            "key_properties": "compresses high-dimensional distributed representations into a lower-dimensional, interpretable symbolic space; supports both bag (orderless) and sequence-based representations; allows standard ML algorithms to operate on concept-level features; enables explainability via prototype inspection.",
            "empirical_support": "Implemented and evaluated in this paper: clustering of LSA/word2vec embeddings into prototypes, construction of symbolic histograms and sequences, and successful application to text categorization (Reuters-21578 and Abstracts datasets) with competitive accuracy across classifiers.",
            "empirical_challenges": "Information loss when converting distributed vectors to discrete symbols; sensitivity to granularity (number of clusters/prototypes), clustering algorithm, and metric choice; possible degradation for small data or rare contexts if prototypes are poor.",
            "applied_domains_or_tasks": "Text categorization/classification, knowledge discovery/explainability in NLP, sequence modeling via prototype sequences (LSTM), bridging distributed embeddings and symbolic ML pipelines (this paper's experiments).",
            "comparison_to_other_models": "Serves as a pragmatic bridge from distributional/associative representations to symbolic feature spaces; compared experimentally to TF-IDF and raw LSA/word2vec features in the paper and shown to provide compact, explainable features without substantial performance loss.",
            "functional_mechanisms": "Cluster embedding space via k-means to obtain prototypes (concept symbols), map each word to nearest prototype, form document-level histograms (counts or normalized distributions) or prototype sequences, then apply classifiers (SVM, RF, LSTM).",
            "limitations_or_open_questions": "How to choose optimal granularity k, how to learn concept vocabularies transferable across corpora, and how to extend prototypes to n-grams or handle sequence-sensitive meanings are identified as directions for future work.",
            "uuid": "e5175.9"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Conceptual spaces: The geometry of thought",
            "rating": 2,
            "sanitized_title": "conceptual_spaces_the_geometry_of_thought"
        },
        {
            "paper_title": "Natural categories",
            "rating": 2,
            "sanitized_title": "natural_categories"
        },
        {
            "paper_title": "A solution to Plato's problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge",
            "rating": 2,
            "sanitized_title": "a_solution_to_platos_problem_the_latent_semantic_analysis_theory_of_acquisition_induction_and_representation_of_knowledge"
        },
        {
            "paper_title": "Distributional structure",
            "rating": 1,
            "sanitized_title": "distributional_structure"
        },
        {
            "paper_title": "Toward a theory of fuzzy information granulation and its centrality in human reasoning and fuzzy logic",
            "rating": 1,
            "sanitized_title": "toward_a_theory_of_fuzzy_information_granulation_and_its_centrality_in_human_reasoning_and_fuzzy_logic"
        }
    ],
    "cost": 0.02113725,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Prototype Theory Meets Word Embedding: A Novel Approach for Text Categorization via Granular Computing
1234567890</p>
<p>Enrico De Santis 
Antonello Rizzi 
Prototype Theory Meets Word Embedding: A Novel Approach for Text Categorization via Granular Computing
123456789010.1007/s12559-023-10132-9Received: 14 July 2020 / Accepted: 6 March 2023 / Published online: 23 March 20231 3Conceptual spaces · Granular computing · Text classification · Word Embedding · Conceptual embedding · Long Short Term Memory
The problem of the information representation and interpretation coming from senses by the brain has plagued scientists for decades. The same problems, from a different perspective, hold in automated Pattern Recognition systems. Specifically, in solving various NLP tasks, an ever better and richer semantic representation of text as a set of features is needed and a plethora of text embedding techniques in algebraic spaces are continuously provided by researchers. These spaces are well suited to be conceived as conceptual spaces in light of the Gärdenfors's Conceptual Space theory, which, within the Cognitive Science paradigm, seeks a geometrization of thought that bridges the gap between an associative lower level and a symbolic higher level in which information is organized and processed and where inductive reasoning is appropriate. Granular Computing can offer the toolbox for granulating text that can be represented by more abstract entities than words, offering a good hierarchical representation of the text embedded in an algebraic space driving Machine Learning applications, specifically, in text mining tasks. In this paper, the Conceptual Space Theory, the Granular Computing approach and Machine Learning are bound in a novel common framework for solving some text categorization tasks with both standard classifiers suited for working with ℝ n vectors and a Recurrent Neural Network (RNN) -an LSTM -able to deal with sequences. Instead of working with word vectors, the algorithms process more abstract entities (concepts), where patterns, in a first approach, are obtained through the construction of a symbolic histogram starting from a suitable set of information granules, representing a document as a distribution of concepts. For the RNN case, as a further novelty, a text is represented as a random walk over prototypes within the conceptual space synthesized over a suitable text embedding procedure. A comparison of the performance and a critical discussion are offered for both a neural embedding technique and the well-known LSA, showing how the conceptual level leads also to Knowledge Discovery applications.Publisher's Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
<p>Introduction</p>
<p>The human brain can be thought as the best pattern recognizer in the known universe. Since our early childhood, we have been observing patterns in the objects around us (e.g., flowers, toys, pets and faces). Learning patterns also reinforces, and is reinforced by, the acquisition of language. It is well known that most 5-year-old children are already able to recognize digits and letters [1]. At the same time, scientists, engineers and practitioners know that designing a general-purpose machine for Pattern Recognition (PR) able to contend in performances the brain remains, today, an elusive goal. PR, intended as a superordinate field, involve disciplines as Cognitive Sciences, Psychology, Artificial Intelligence (AI), and for some extent, Neuroscience, Linguistics and Philosophy. As a discipline which studies the ability of discovering and recognizing regularities in observations, PR can help to understand how human perception works and to discover the secrets behind the ability to gain new knowledge and to exploit it in appropriate ways. So forth, besides giving more insights into the study of human senses and the neural system, PR allows to build up automated systems adopted in medical diagnosis, industrial inspection, personal identification, man-machine interaction, Natural Language Processing (NLP) tasks, etc.</p>
<p>From its own side, an open issue in Cognitive Science is the causal relation of low level phenomena occurring in the senses and the nerves onto higher levels of understanding and conceptual thinking [2,3]. Interestingly, it is an open issue even in automated mechanical PR, even if we are dealing with sensors, actuators, processing units etc.</p>
<p>In fact, one of the main problems that affect both disciplines (namely automated PR and Cognitive Sciences), each one within its own phenomenology, is the "representation" [4,5]. In other words, Cognitive Science, in studying the cognitive activities of humans and other animals, provides us with a set of explanatory theories and even a set of constructive prescriptions that can aid to design artifacts like robots, animats, and chess-playing programs, with the aim of accomplishing various cognitive tasks. A key issue is the representation of information or data, in such a way that the cognitive system can be modeled, starting from stimuli coming from biological or digital sensors to high-level processing capabilities, for example, to perform tasks on higher semantic levels. It can be affirmed, without pretense of completeness, that self-organizing phenomena of the physical world are relevant also for understanding cognitive processes [6]; hence, some properties of cognitive systems are in resonance with the ones attributed to Complex Systems. Even language and text production can be thought as activities of complexly organized brains [7] and semantic meaning can be hidden in the middle-ware of the complexity around us. The engineer Alfred Korzybski, the inventor of General Semantics, giving great importance to language and the use of words, in 1933 affirmed that thinking is a matter of multilevel order of abstraction and content is a declination of the structure with complex relationships [8]. The multilevel order of abstraction can be found even in the organization of most Complex Systems where emergent properties and vertical information processing generate new abstract levels dominated by its own semantic content.</p>
<p>From a computational point of view Granular Computing (GrC) is the umbrella term to cover any theories, methodologies, techniques, and tools that make use of information granules in complex problem solving [9,10]. Information granules are atomic units [11] that naturally give rise to hierarchical structures: the same problem or system can be perceived at different levels of specificity (detail), depending on the complexity of the problem, the available computing resources and the particular needs to be addressed [9,12]. Some authors (e.g., W. Pedrycz) conceive GrC as a conceptual and algorithmic platform supporting analysis and design of human-centric intelligent systems [13]. Zadeh, the scientist who made fuzzy logic great, considers GrC as a basis for computing with words, i.e., computation with information described in natural language [14,15]. For example, the text in a book can be seen as an increasing granulation of the information content starting from the alphabet letters and ending with the aggregation of concepts and topics, passing through "mesoscopic" structures such as words, sentences, paragraphs, chapters and so on. In this regard, E. G. Altmann et al. affirm that [16]: "literary texts are an expression of the natural language ability to project complex and highdimensional phenomena into a one-dimensional, semantically meaningful sequence of symbols. For this projection to be successful, such sequences have to encode the information in form of structured patterns, such as correlations on arbitrarily long scales".</p>
<p>Moreover, dealing with text, in automated PR systems the representation problem becomes more difficult because text is intrinsically structured at various levels, while classical PR problems solved by standard Machine Learning (ML) approaches need to work with the ℝ n vector space geometry. In general, the GrC approach allows designing automated PR problems able to deal directly with structured or unconventional input domains [17]. Hence, a challenging task is finding effective models and algorithms able to represent and process a set of samples coming from a structured domain.</p>
<p>The aim of the current work is twofold:</p>
<p>-from a more general point of view, it tries to investigate how to bridge the gap between some findings in Cognitive Science (the Conceptual Spaces and Prototype Theory [3,18] and so forth) and the GrC approach in light of the problem of representation of text excerpts in text mining problems; -from a specific point of view, the objective of the current study is to experiment some text embedding methods through a GrC approach, known as symbolic histograms [19,20], in solving two specific text classification problems through some standard Machine Learning algorithms able to process n-tuple of real numbers or more structured objects, such as sequences.</p>
<p>It is well known that in PR applied to text data a traditional representation approach consists in embedding words or documents in a mathematical space with useful algebraic properties, such as a linear vector space, also known as feature space. The essence of such algebraic space, capturing some kind of co-occurrence between words and contexts, is built on the top of Distributional Semantics (DS) [21], grounded, in turn, on the distributional hypothesis: similarity of meaning correlates with similarity of distribution. After all, Wittgenstein claimed in his Philosophical Investigations, that "the meaning of a word is its use in the language" [22]. In other words, as the American linguist Z.S. Harris sustained: "words that are used and occur in the same contexts tend to purport similar meanings" [23] or paraphrasing the British J. R. Firth "a word is characterized by 1 3 the company it keeps" [24]. In ML, specifically in document classification or even in Computer Vision [25], the approach is known as "bag of words (BoW)" -sometimes known as surface form -pointing out the fact that the text is represented as the frequency of occurrence of each word building a feature space for training a classifier [26], disregarding grammar and even the order of words. The methodology is heavily adopted in Information Retrieval (e.g., the so-called traditional Vector Space Model (VSM) [27]) and text mining but is well known that it has some limitation, such as (i) the orthogonality, (ii) the construction of the vocabulary that requires a careful design due to its size, (iii) the sparsity of the model and the lack of context due to the discarding all information brought by surrounding words.</p>
<p>Researchers have attempted to address the representations of natural language that are capable of capturing meaning through what they call semantic spaces, a set of language models that adopt the DS. For example, the Hyperspace Analogue to Language (HAL) [28] is a method for creating a simulation that exhibits some of the characteristics of a human semantic memory finding lexical co-occurrences by moving a window of length l over the corpus. HAL allows representing words as vectors.</p>
<p>In general, authors refer to the word-context models as explicit models, while some family of transformations of the underlying data structure leads to implicit representations [29]. Canonical co-occurrence models are simpler to implement and they work well within standard ML pipelines. However, they possess a number of drawbacks, for example, sparsity (a lot zeros due to Zipf's law) and high dimensions when dealing with huge corpora with large vocabularies. A simple frequency count, for example, does not embed intrinsically the fact that two words have the same meaning (synonymy) because they are treated as named entities, that is, they are symbols. Moreover, contexts can be similar too, or high-correlated. Furthermore, these raw representations can be very noisy.</p>
<p>In order to avoid some drawbacks, a number of implicit representations are provided in literature, some of which are known as dense representations, because they reach a non-sparse representation, often in a reduced feature space. The most adopted methodologies, in practice, use an implicit representation of features in a latent space where latent features are computed starting from the distributional models. For example, Latent Semantic Analysis (LSA), representing the text in a latent space through a set of linear algebraic transformations, aims at constructing a rich semantic space. LSA is obtained by means of (linear) matrix decomposition procedure known as Singular Value Decomposition (SVD), allowing dimensionality reduction (truncated SVD) and noise filtering. The dense embeddings produced by SVD sometimes perform better than the raw ones (grounded on PPMI matrices) on semantic tasks like word similarity. Various aspects of the dimensionality reduction contribute to improved performance. If low-order dimensions represent unimportant information, the truncated SVD may be able in removing noise. By reducing the input dimension, the truncation may also help the models to generalize better to unseen data. Due to interesting, and in some ways unexpected, properties, LSA has also been proposed as a cognitive model for human language use [30,31]. Other techniques adopt other matrix factorization methods, such as the non-negative matrix factorization (NMF) or ML methods such as GloVE [32], which is based on a regression technique.</p>
<p>Recently, in technical literature there are some powerful neural approaches, for example, the word2vec algorithm [33,34], which embeds the meaning of text in a similar way to HAL (windowing), but constructing a dense representation training a shallow Artificial Neural Network (ANN) -e.g., Skip-gram with negative sampling (SGNS). More recent approaches in neural language embedding adopt sophisticated Recurrent Neural Networks (RNN) bound with attention mechanisms for language modeling, such as the Bidirectional Encoder Representations from Transformers (BERT) [35] and related architectures. Another technique that uses an external corpus to build a semantic space is the Explicit Semantic Analysis (ESA) [36], where words are represented as vectors and each entry is a Wikipedia article. In other words, each Wikipedia article is a kind of concept and words are embedded in a "concept space". Hence, some attempts in embedding "meaning" and working with concepts are based on the so-called Bag of Concepts (BoG) [37] that, rather than identifying features directly with some surface form, utilizes some artifices to make practical the intuition that the meaning of a document can be approximated by the union of the meanings of terms appearing in the document itself. There are a number of practical implementations of BoC that uses concept vectors. They differ on how they construct the concept space, for example, adopting implicit or explicit representations, such as Word-net [38] like approaches or hyper-linked encyclopedic textual corpora.</p>
<p>In this paper, as concerns the textual conceptualization, we deal with a simple type of BoC useful for building a suitable feature space, where both traditional ML algorithms or advanced ones, such as RNN -for example, a Long Short Term Memory (LSTM) -can safely operate.</p>
<p>In doing so, as stated above, we adopt GrC as a general toolbox, while the road-map of the proposed approach is grounded by a specific approach mediated from Cognitive Psychology and in general from Cognitive Science, that is the "Conceptual Space" [3]. The theory of Conceptual Spaces is a modern extension of Prototype Theory developed by Rosch [39,40]. P. Gärdenfors affirmed that the problem of representation in Cognitive Science, thus the problem of the vertical information processing where stimuli and senses data become high-level thinking and concepts, is due to the lack of a middle level between the Sub-conceptual Representations based on associations and the Symbolic Representations where rational thinking operate. This level is the Conceptual Level, a bridge where information is organized in a smooth space and where the notion of prototype and similarity (intended as a mathematical distance) allows to deal with concepts and properties (as a particular instance of concepts) in representing real-world objects. Concepts are particular "natural" regions of the Conceptual Space [3].</p>
<p>The proposed methodology foresees first the embeddings of words in a given corpus through either (i) the neural word embedding technique -word2vec -that is based on the association between words and contexts computed through a neural technique or (ii) the classical LSA. The aim here is to build a semantic space -a Conceptual Space -were words coded by vectors are embedded.</p>
<p>The space of word vectors is, thus, partitioned in "natural" regions (Voronoi regions) through a clustering algorithm, where regions are intended as a semantically homogeneous containers around its prototype. Once constructed the Conceptual Space, each word in a given document takes part in a new representation, known as a symbolic histogram.</p>
<p>Symbolic histograms [17] is an embedding technique, where a pivotal role is played by a set of meaningful and recurrent substructures in the original data space, often adopted for representing other structured objects lying in a non-metric structured space, such as graphs, sequences, strings, and images. In the current approach each document in a given corpus is represented as a symbolic histogram.</p>
<p>Specifically, concepts are represented by symbols (i.e., prototypes). In this sense, the vectors correspond to subsymbols [41] that are transformed into symbols through a process characterized by information loss.</p>
<p>In other words, a documents is represented as a probability distribution on a set of alphabet symbols -we will call representatives of concepts among the Conceptual Space -used as feature vector for feeding a classification algorithm. Specifically a comparison will be offered among Random Forest (RF), a Support Vector Machine (SVM) and an advanced RNN model able to deal with sequences (LSTM). In the last case, as further novelty, instead of a classical features space where features are concepts, the RNN processes sequences of concepts, that is, ultimately, a new representation of a document. By the way, Wiggins argues [42] that learning is not only a matter of acquiring static co-occurrences, unless it includes generalization and the ability of processing sequences of events or even sequences of concepts.</p>
<p>In light of the Conceptual Space Theory this approach adds a middle layer in the representation/embedding of text in documents. Hence, starting from a sub-conceptual layer where associations dominate the representation (neural embedding or LSA), the construction of the alphabet -obtained at training time -is based on a conceptual organization of the underlying associative layer, where are elicited a set of (read a small number of) prototypes that, in turn, offer a symbolic level used to build the embedding representation by symbolic histograms. The proposed embedding allows representing documents in a smaller feature space in term of dimension compared to BoW approaches, providing a good performance for further recognition tasks. Moreover, the new feature space constructed on the top of the granulation of the semantic information contained in the word embedding model is a classical real-valued feature space, allowing the adoption of standard ML algorithms (as mentioned earlier). This is a strong point of the proposed approach. It is worth to note that the proposed methodological framework opens the way to knowledge discovery applications and, in general, to the Explainable AI paradigm [43,44]; a fact not so obvious for the modern neural architectures used in the NLP context. The paper is organized as follows.</p>
<p>In "Related Works'' a brief overview of related works is reported. In "Background: Prototypes and Conceptual Spaces'' the Conceptual Space Theory and the Prototype Theory are outlined. In "Methods'' is presented the adopted approach and the problem framing. The description of the data sets for the experiments and the main results are provided in "Experiments''. Lastly, conclusions are drawn in "Conclusions''.</p>
<p>Related Works</p>
<p>The symbolic histograms technique within the GrC model is widely adopted in many PR tasks [17], such as online handwriting recognition [45] or protein classification [46]. This technique is heavily adopted when dealing with unconventional structured data, such as graphs, for example, performing frequent substructures mining in graphs seriation [47,48] and classification methods [19,49]. In the specific field of text mining and text categorization GrC is found very promising [50,51]. Concerning Knowledge Discovery applied to text mining problems, authors in [52] deal with concept formation and concept relationships identification through constructing a granules' network. An automatic text categorization system is proposed in [53] considering a document as an ordered sequence of words, proposing a system able to automatically mine frequent terms, considering as a term not only a single word, but also a sub-sequence of a few consecutive words (i.e., n-grams). The categorization system is tailored to process sequences of atomic elements (i.e., encoded words) by means of an embedding procedure based on clustering and adopting the symbolic histograms technique.</p>
<p>Many authors have adopted the BoC terminology referring to some technique for dealing with more general representations of words or sentences rather than the BoW model. In [37] authors adopt a particular technique within the BoC paradigm called Random Indexing, training a SVM with good results. Random Indexing is even used in [54] along with the Holographic Reduced Representation, previously proposed in cognitive models, which can encode relations between words. In [55] authors propose the cross-language concept matching (CLCM) technique, which relies on Wikipedia inter-language links to convert concept vectors from the Spanish to the English language space. They synthesize a classifier of text documents, represented as vectors in spaces of Wikipedia concepts, and provide an analysis of its suitability for classification of Spanish biomedical documents when only English documents are available for training. An approach, called Mined Semantic Analysis, is proposed in [56]. The study tries to address and mitigate problems arising in concept space models, such as the limitation to direct association between words and concepts, affecting the ability of models to transfer the association relation to other implicit concepts which contribute to the meaning of these words. The particular BoC paradigm is able to build concepts through concept rich encyclopedic corpora, even exploring the "see also" link graph in Wikipedia. A different declination of the BoC technique is provided in an interesting investigation [57] in line with the current research work, where authors creates concepts through clustering word vectors generated from word2vec and using the frequencies of clusters' representatives to compute document embedding vectors. They propose a suitable weighting scheme, such as the concept frequency-inverse document frequency. Through these data-driven concepts, the method allows semantically similar words to be preserved effectively in a suitable document proximity measure. A related BoC approach is proposed in [58] solving an emotion estimation task from text excerpts, characterized even by youth slang, an ambiguous and difficult task when using existing dictionaries, such as thesaurus. In an interesting work [59] authors try to outperform the lack of concept overlapping in some text mining tasks,resulting in a data sparsity problem, proposing an efficient vector aggregation method, grounded on a neural embedding model, able to generate fully continuous BoC representations.</p>
<p>Background: Prototypes and Conceptual Spaces</p>
<p>Humans are extremely efficient at learning new concepts. Cognitive Science is interested in how to model concept learning starting from the ability of humans to learn concepts from a few examples. On the other side, ML, along with the data-driven approach, uses its own models to learn from examples. The main approaches in modeling concept learning are the one known as "symbolic" and the one known as "associationist" [3]. The symbolic approach starts from the assumption that cognitive systems can be described as Turing machines. Hence, cognition is a matter of symbol manipulations. Within the associationists paradigm associations between different kinds of information elements carry the main burden of representation [60]. The Swedish cognitive scientist P. Gärdenfors sustains that connectionismthe ANN approach -is a special case of associationism [3]. However, the same author admits that there is no unique correct way of describing cognition. There are phenomena that neither the symbolic representation nor the associationist appears to offer appropriate modeling tools. He proposes the "Conceptual Spaces", as the framework placed in the middle of the two main approaches, that is the most appropriated for modeling concept learning and representation. The theory of conceptual spaces, due to its versatility and capability even when in dealing with high-dimensional spaces, has been extended together to the 3-way formal analysis to investigate phenomenal consciousness, within a quantum framework [61]. By the way, the three approaches mentioned can be seen as three levels of representations of cognition with different scales of resolution or "granulation". Conceptual Spaces are able to geometrize the thought, because world objects are embedded in a geometric space where the notion of distance, region and prototype can be used to model concepts [62]. Actually, the embedding of real-world objects, through a series of suitable measures on them, is a normal procedure in automated PR systems. Measurable properties in automated PR and ML are called "features", while in Conceptual Space theory they are called "quality dimensions". However, neither with the symbolic approach (as an example, the first-order logic) nor with the associanist/connectionist approach, it is easy to deal with similarities [3]. While the associationist approach suffers for the black-box problem -think to ANN -the symbolic approach seems not working at the appropriate abstraction level, for example, lacking in creative induction, new knowledge creation and basically being not able to perform conceptual discoveries. Moreover, the symbolic approach lacks in automatic management of semantic and meaning. On the contrary, in Conceptual Spaces induction can be derived "naturally" from the metric properties of the underlying algebraic space, allowing what is known in automated PR and ML as "generalization capability". That is, the capability of generalizing predictions on unseen data. By the way, P. Gärdenfors asserts that the symbolic level is not completely non-significant and it depends strictly on the underlying conceptual level [3].</p>
<p>An important distinction, useful in the context of the current work and due to Palmer [63], is about intrinsic and extrinsic representation. The former, is valid when the representing relation has the same inherent constraints as its represented relation. For example, in the isomorphism between the dimension "age" and the "height" of a bar in a chart, the structure of the represented relation (age) is intrinsic in the representing relation (height). In contrast, extrinsic representations must be accompanied by a rule that specifies how the representation is to be interpreted; such a rule provides the "meaning" of the representation. On the symbolic level, atomic concepts are not modeled, just named by the basic symbols. Even if complex concepts can be constructed through compositions of logical or syntactical rules, they remain extrinsically represented. In DS the BoW model considers intrinsically words as named entities, that is, as symbols with no further relational structure and the frequency count for representing documents is a symbol count. This leads to the synonymy problem.</p>
<p>Within the Conceptual Space theory the geometric characteristics of the quality dimensions are utilized to introduce a spatial structure for properties:</p>
<p>Criterion P: A Natural Property is a Convex Region in Some</p>
<p>Domain A subset C of a conceptual space S is said to be convex if, for all points x and y in C, all points between x and y are also in C [3]. It's worth to note that Criterion P assumes that a notion of "betweenness" among objects is provided when each concept is represented as a point in a given space [3]. Convexity, for example, is mantained for the color naming and the three-dimensional representation of the color space. It is worth to note that properties defined by the Criterion P are a special case of concept.</p>
<p>Studying the phenomenology of colors and its perceptual representation in Cognitive Psychology E. Rosch and collaborators defined the Prototype Theory providing us with a model of categorization [39,40]. The main idea in this theory is that within a category of objects, like those instantiating a property or a concept, certain members are judged to be more representative of the category than others. That prototype representation of a category is generally taken to be a generalization or abstraction of a class of instances falling into the same category [64]. In cognitive linguistics a prototype is a typical instance of a category and other elements are assimilated to the category on the basis of perceived similarity to the prototype [65].</p>
<p>The appealing feature of Conceptual Space lies in the underlying algebraic structure, that can be metric. This means that are fulfilled all or some properties of metric spaces [66]. A natural partition of such spaces is the Voronoi tessellation, a particular tessellation of the space based on a simple rule. If p 1 , p 2 , ..., p n are prototypes of a space S, the Euclidean distance d E (p, p i ) among a point p and the prototypes p i can be defined. If we now state that p belongs to the same category as the closest prototype p i , it can be shown that this rule will generate a partitioning of the space, the so-called Voronoi tessellation [67]. Not every distance metric (e.g., Manhattan or in general the Minkowski distance for some values of its parameter) generates a set of regions that fulfill the convexity property, however, for the Euclidean distance this property holds. Among the many methods used to compute Voronoi cells [67], the clustering algorithm k-means can help, in an unsupervised fashion, to compute centroidal Voronoi regions, where centroidal points are the centroids of the regions [68]. Hence, centroids are isomorphic to prototypes of some Conceptual Space. Thereby, depending on the nature of the space S (i.e., the nature of dimensions), the Conceptual Space becomes a semantic space (here the term semantic is used in a weak interpretation). In this way, the Voronoi tessellation provides a constructive geometric answer to how a similarity measure, together with a set of prototypes, determines a set of categories [3]. The Conceptual Spaces have been adopted even in trying to pragmatically untie the knot of semantics, intended as the relationship between an expression and an extralinguistic reality, within the riverbed of the cognitive semantics. The last assumes that the referents of words are identified with conceptual structures in people's minds. However, semantics is a huge field of study where numerous discipline converges, such as Semiology, Semiotics, Linguistics, Psychology, Pragmatics, Communication, and Philosophy of Language. In Linguistics and, specifically, in Computational Linguistics the meaning, and in general the semantic content of a word or expression, assumes a specific way of being related to a context, which is empirical and measurable. For example, it is common to refer to space generated by the BoW model as a semantic space, specifically, a mathematical space grounded on the DS.</p>
<p>Methods</p>
<p>The approach presented in details hereinafter is an attempt of systematizing the theory of Conceptual Spaces with a specific declination of the BoC paradigm built upon the background of the GrC approach. The overall processing pipeline is composed by several steps where information extracted from the text is granulated, and information granules are adopted, in turn, in constructing a new embedding space grounded on the symbolic histogram technique. The main objective is to find an economic representation of documents as BoC for classification purposes, hence for text categorization. Following the scheme proposed in Fig. 1, given a corpus of documents, the first step is to perform the embedding of words in an algebraic space, called in the following Conceptual Semantic Space (CSS). The embedding of words can be performed through various methodologies outlined in "Introduction''. In this work the LSA and the neural word embedding through the word2vec algorithm are performed.</p>
<p>The word embedding step is grounded on the cooccurrences (collocates) of words obtained through a context window of suitable length. In the case of LSA, the word vectors within the reduced latent space are obtained on the top of a BoW model with TF-IDF weighting, where contexts are documents. Hence, this layer fits with the "associative layer" [3]. The word vectors generate a vector semantic space endowed with the standard Euclidean norm, thus, it is defined a dissimilarity measure based on the Euclidean distance [69]. In the case of neural embedding through the word2vec algorithm, word vectors are directly obtained by the training procedure, ready to be further processed. Instead of using directly the word vectors, a Voronoi tessellation is computed, where each region coincides with a concept whose instances are linked by semantic relations. The Voronoi tessellation is obtained by computing the representatives -the prototypes -through a clustering algorithm. The k-means algorithm used in the following, but in principle other clustering algorithms can be adopted. This step embodies the "conceptual layer" that is the layer interposed between the "associative" and the "symbolic" one. Figure 2 depicts an example of CSS obtained for a corpus of scientific paper abstracts ("Abstracts" data set hereinafter), with four classes ("Anatomy", "Information Theory", "String Theory", "Semiconductors") of which a deep description will be given in the experiment section. The CSS in Fig. 2 is synthesized by a Voronoi tessellation in k = 8 regions, where the prototypes are highlighted by crosses. Dots represent words embedded (initially in a  100-dimensional space) through the word2vec algorithm. Principal Component scores are computed for dimensionality reduction with the aim of data visualization.</p>
<p>Accordingly, the conceptual semantic layer is the ground for a symbolic representation of documents, namely each word is abstracted by its concept computed measuring the semantic similarity of the vector representation of words and the prototypes on the underlying CSS. Thus, documents are represented as discrete probability distributions on concepts.</p>
<p>Prototypes are intended, therefore, as symbols of a suitable alphabet A of concepts used for the symbolic representation.</p>
<p>Let H = D 1 , D 2 , ..., D L be a corpus with L documents, where each document D, D = w 1 , w 2 , ...w |D| ∈ H , hence D is a collection of words w i , i = 1, 2, ..., |D| in a vocabulary V . The prototype c j ∈ A, j = 1, 2, ..., k , abstracting a concept of a region R j , j = 1, 2, ..., k of the Conceptual Space P, defines what we can call a symbol of a suitable alphabet A . It is worth to note that the parameter k defines the level of granulation of the CSS. Each document can be suitably represented by some statistics on the alphabet symbols c i ∈ A , namely, if the prototypical region pertaining the partition obtained by word embedding vectors is a "concept", the document is represented as a "bag of concepts". In the limit where the number of prototypes (aka the cardinality of the alphabet A ) equals the number of words in the corpus of documents H , the standard BoW model is recovered. It is worth to note that the symbol c i is obtained by a suitable mapping M from the underlying word vector i ∈ W , obtained through the word embedding, and concepts in A , that is M ∶ W → A , where M( i ) = c j , j = 1, 2, ..., k , for the i-th word within a document. Figure 3 depicts the word clouds for a CSS P partitioned in k = 8 semantic regions. The thickness of each word is proportional to the similarity (based on the Euclidean distance) to the prototype computed as the centroid of the conceptual region.</p>
<p>Moreover, in Fig. 4 the symbolic histograms for four documents pertaining the classes "Anatomy", "Information Theory", "String Theory", and "Semiconductors" of the Abstracts data set are reported. The length of a bar represents the number of occurrences of each one of the (ten) symbols (prototypes) for a given class.</p>
<p>The symbolic histogram representation allows naturally to embed documents in a vector space giving the way for classification or regression ML algorithms. However, there are possible other representations. Instead, it is possible to build a centroidal prototype for each document simply computing the average of word vector representations of prototypes. In other words, instead of having a prototype derived from a count histogram, we have an average value of word vectors prototypes associated to each word in a document. This alternative will be introduced more formally below. Another quite different representation of documents adopting prototypes is conceiving a document as a sequence of Fig. 3 Concept cloud for each one of the k = 8 conceptual regions for the Abstracts data set. The thickness of each word is proportional to the similarity (Euclidean distance) to the prototype computed as the centroid of the conceptual region words, hence as a sequence of the prototypes associated to words. Namely, a document is represented by a sequence of concepts, where concepts are semantic abstraction of words. Words, in this setting, are fine-grained representation, while concepts pertain to coarser one. This new representation gives the way for sequence-based ML algorithm, such as the deep learning-based LSTM. Interestingly, this sequencebased representation allows framing a document as a random walk of concepts, instead of a random walk of words.</p>
<p>Classification Problem Framing with Symbolic Histograms</p>
<p>A general classification problem instance is defined as a triple of disjoint sets, namely training set ( S tr ), validation set ( S vs ), and test set ( S ts ). Given a specific parameters setting, a classification model is built based on S tr and it is validated at training stage on S vs . The generalization capability of the optimized model (the one synthesized by the whole training procedure) is finally measured on S ts . Hence, given a corpus H = D 1 , D 2 , ..., D L composed by L documents D, we have</p>
<p>The CSS P is conceived as a hard partition of order k, as a collection of k disjoint and non-empty clusters, P = {C 1 , C 2 , ..., C k } . In this study the partition is obtained through the well-known k-means algorithm [70,71]. Each cluster C i ∈ P is synthetically described by a representative or prototype element, which we denote as i = R(C i ) ; let R(P) = { 1 , 2 , ..., k } be the set of representatives of the partition P. Alternatively, the representative of C i can be computed as the element i that minimizes the sum of distances (Min-SOD) [72]:</p>
<p>In this case the representative is an object of the cluster, that is j ∈ C i . Note that computing the MinSOD does not require an algebraic structure, demanding just the definition of a dissimilarity measure. From this point of view, the MinSOD representative is much more general, and can be applied in any data domain.</p>
<p>Finally, each prototype j identifies a centroidal Voronoi region R j through the Euclidean distance
d j = ‖ ‖ ‖ i − j ‖ ‖ ‖2 , with i ∈ W ,
where W is the set of word vocabulary vectors.</p>
<p>Embedding Words</p>
<p>As concerns the word embedding procedure, a comparison will be offered between the word embedding obtained through LSA, by means of the SVD decomposition, and the neural embedding, by means of the word2vec algorithmsee "Introduction''. In particular the two techniques allow new ways to represent each word w ∈ V through suitable vectors ∈ W , just considering a mapping Φ ∶ V → W , from the set of vocabulary words to word vectors, i.e., Φ(w) = .</p>
<p>(2) i = arg min Symbolic histogram for four documents pertaining the classes "Anatomy", "Information Theory", "String Theory", "Semiconductors" of the Abstracts data set
j ∈C i ∑ k ∈C i d( j , k ).</p>
<p>The Symbolic Histogram Construction</p>
<p>In abstracting a concept, hence a prototype for a word of a given document, it is necessary to associate a prototype to each word of a given document. Hence, given a document D = w 1 , w 2 , ...w |D| ∈ H as a collection of words w i , and its vector representation Φ(w i ) = i first, the nearest cluster prototype * ∈ R(P) is individuated according to the following expression:</p>
<p>The construction of the symbolic histogram is performed as follows. An array w i = [ 1 , 2 , ..., k ] T of indicator functions is constructed, where:</p>
<p>Finally, the symbolic histogram for a document d is provided by:</p>
<p>Alternatively, instead of constructing a symbolic histogram as an array of counters, it is possible to represent the document D ∈ H as the average of the associated centroids c( i ) , for each word w ∈ D , that is:</p>
<p>At this point, each document in the corpus has an associated symbolic histogram, hence a vector of Integers or Realvalued numbers, depending on the specific rule adopted. In other words, documents are embedded in a bag of concept vector space.</p>
<p>Classification Layer</p>
<p>Once obtained the new representation, that is, the new vector space (through the symbolic histograms) or the new sequence of concepts, a learning layer can be designed depending on the problem at hand. In this work it is faced a classification problem comparing three different classification algorithms, namely SVM with Gaussian Kernel [73,74], Bagged Tree RF [75,76] and LSTM [77][78][79]. The first two learning algorithms are suited for working with Real-valued patterns, while LSTM is conceived for learning with a representation grounded by sequences of objects. Specifically, in the current approach LSTM is fed by the sequences of prototype vectors c( ) i obtained through Eq. 3 
(4) j = 1 if c( i ) = j , i = 1, 2, ..., |D| 0 otherwise. (5) D = |D| ∑ i = 1 w i . (6) D avg = |D| ∑ i = 1 c( i ) |D| .
corresponding to the sequence of words w i pertaining a given document D. These classification algorithms belong to three big and heterogeneous families of learning algorithms, namely kernel-based, where learning is conceived as a convex optimization problem (SVM), random tree-based (RF), and deep learning-based, specifically RNNs. Hence, this choice guarantees the diversity of the learning paradigms applied to the proposed method. It is worth noting that RF algorithms are based on the bootstrap technique (some samples will be used multiple times) and the observations that are out of the bootstrap sample are called out-of-bag (OOB). This technique allows estimating the importance of variables (features) through a suitable procedure described, for example, in [80].</p>
<p>Experiments</p>
<p>Data Sets</p>
<p>As concerns text data for experiments, the "Reuters-21578" data set and the "Abstracts" data set have been used. Reuters-21578 is a benchmark data set for document classification consisting in 8 classes. The collection of documents appeared on the Reuters news-wire in 1987. The documents were assembled and indexed with categories by personnel from Reuters Ltd. [81]. The adopted splitting is the "ModApte" split [82] on 7674 documents and 8 classes. The "Abstracts" data set is a collection of 575 abstracts of scientific papers belonging to 5 classes ('Anatomy', 'Information theory', 'Smart Grid', 'String Theory', 'Semiconductors'), collected by authors. Some statistics on the experimented data sets are reported in Tables 1 and 2. The former provides some general information about the data set, while the latter reports some statistic per class, such as the mean and standard deviation of document length per class. Specifically, in Table 1 the total number of documents ( # docs), the dimension of the vocabulary before pre-processing ( |V| ), the dimension of the vocabulary after the pre-processing ( |V| pre ), the number of classes ( # class) and the average length of documents in terms of tokens (words) ( | |D | | ) with standard deviation in brackets are reported. In Table 2 the class names (class), the average length of documents in term tokens  Fig. 5 the statistics on document lengths per class and for each data set are reported, while in Fig. 6 the histograms of the length of documents for both data sets are depicted. This information will be useful for setting the sequence length parameter for experiment with the LSTM algorithm.</p>
<p>In Fig. 7 the class distributions for both the data sets are reported. We note that the "Reuters-21578" data set has a heavy skewed class distribution leading to a strong unbalanced data set, making challenging the classification task, while the "Abstracts" data set classes are equally distributed.</p>
<p>Experimental Settings</p>
<p>As concerns the performance measures of the classifiers, several metrics for the multi-class case are adopted.</p>
<p>Specifically, considering the i-th class ( i = 1, 2, ...|C| ) of the available data sets it is possible to define:</p>
<p>-TP i (true positive): number of patterns belonging to the i-th class and correctly classified by the system; -FN i (false negative): number of patterns belonging to the i-th class whose class is incorrectly assigned to the i t h class predicted by the system; -FP i (false positive): number of patterns not belonging to the i-th class whose class is incorrectly by the system. -TN i (true negative): number of patterns belonging to the i-th class and correctly classified by the system.</p>
<p>Starting from these metrics a set of derived indicators for each class can be computed, such as the Accuracy i , Precision i and Recall i together with other global figures of merit, such as the Informedness and the Cohen's Kappa. Besides these metrics, the global classification performances can be assessed in two ways: (i) macro-averaging that is the average of the same measure calculated for each class, (ii) micro-averaging that is the sum of counts to obtain cumulative TP, FN, TN, FP and then calculating the performance measure. Macro-averaging treats all classes equally while micro-averaging favors classes characterized by a relative higher number of patterns [83]. The final metrics adopted in the current study are (the higher, the better):   [83]; -the micro F1 score (Fmicro) in [0,1] the same expression as Fmacro, but using the total number of TP, FP and FN, instead of computing these scores for each class [83].</p>
<p>The experimental settings are organized as follows.</p>
<p>In order to assess the proposed approach, two main sets of experiments are provided. The first set aims at comparing the three learning algorithms (namely, LSTM, C-SVM, RF) adopting both the word2vec and the LSA embedding, for both "Abstracts" and "Reuters-21578" data sets. The embedding is computed on the given corpus. In this case the cardinality of the alphabet A , that is the number of clusters k or the number of concept regions, is left to vary in the integer range [2,1002] (see Fig. 8), while a snapshot of the performance, for k = 502 , is provided in Table 3 for the "Reuters-21578" data set and, for k = 202 , in Table 4 for the "Abstracts" data set. The specific choice of the granularity level k has been made simulating an arbitrary setting where we have no information about the variability of the performance as function of the granularity level. In other words, this setting simulates the case in which the performance cannot be computed for an increasing set of granularity levels due to, for example, computational and time constraints. The best granularity level, instead, is considered in the second set of experiments. In fact, the second set of experiments -reported in Tables 5  and 6 -allows evaluating and comparing the proposed methodology with a baseline approach. Specifically, for the mentioned learning algorithms the best level of granulation k in terms of performances is compared with a classical approach where the feature vectors representing documents are obtained either from the TF-IDF representation or from the LSA representation. In other words, the features for the classification task are the TF-IDF weighted words count or the weights related to the latent variables, respectively. In the end, taking advantage of the implicit features weighting offered by the RF algorithm, a task of knowledge discovery is performed. Hence, a threshold filtering is adopted in order to select and show the most important concepts within the concept region that, in turn, allowed reaching a good classification performance. Before discussing the main results, it is worth to deal with the text pre-processing steps and the parameter setting of the adopted learning algorithms.</p>
<p>Regarding the pre-processing steps, words in documents are lowercased and stop words are eliminated using a stop words list.</p>
<p>As concerns the LSTM algorithm, it is preceded by a word embedding layer (through the word2vec algorithm) -namely a concept embedding layer -with a dimension of the word vectors equal to 100. The LSTM layer foresees 180 cells followed by a fully connected layer and a softmax layer. The classification layer computes the cross entropy loss for multi-class classification problems with mutually exclusive classes. The maximum number of epochs is set to 50, while the initial learning rate is set to 0.005. In the case of SVM, a C-SVM with multiple kernels is used. A set of hyper-parameters are optimized with the Bayesian optimization technique and 5-fold cross validation. Specifically, the hyper-parameters optimized are the multi-class coding (One-versus-All and One-versus-One), the Box Constraint, the scale of the kernel, the type of kernel function (Gaussian, linear, polynomial), the polynomial order and the binary variable indicating whether standardize data or not. For the ensemble learning, an ensemble of boosted classification trees is experimented, hence with trees as weak learners. Even in this case, a Bayesian hyper-parameters optimization has been chosen and 5-fold cross validation is performed. In particular the optimization of the training algorithm (Bag, Subspace, AdaBoostM1, AdaBoostM2, GentleBoost, LogitBoost, LPBoost, RobustBoost, RUS-Boost, TotalBoost) and the number of learning cycles [80,[86][87][88]. The OOB performance is measured for establishing the predictor importance.</p>
<p>Where not specified, for robustness purposes, the optimizations of hyper-parameters or the simple learning routines (for example, in LSTM) are repeated three times and performance results are averaged. The data set splitting for the training set S tr and test set S ts is 80%, 20%, respectively, both for C-SVM and RF. In the case of LSTM the data set is split in training set S tr , validation set S vs , test set S ts with the following percents: 50%, 25%, 25%, respectively.  Table 4 Classification performances for a given granularity level k for LSTM, C-SVM and RF ("Abstracts" data set) </p>
<p>Results and Granularity Assessment</p>
<p>Fixing the concept granularity value to k = 202 -see Table 4 -where the three classification algorithms are compared both with the word2vec embedding and the LSA embedding in building the concept space over the "Abstracts dataset" (that is balanced), best classification performances in term of Accuracy (0.94) are obtained with C-SVM with word2vec. The second best performances are obtained with RF and C-SVM (Accuracy 0.93) with the LSA embedding. LSTM reaches lower performances on both embeddings with an Accuracy of 0.85 for word2vec and 0.87 for the Fig. 8 Classification performance varying the number of concept regions from 2 to 402 ("Abstracts" data set) for the LSTM, SVM, RF algorithms, for both word2vec and LSA techniques LSA embeddings. Interestingly, if we compute the figures of merits as the granularity of the concept space increases (varying k), by inspection of Fig. 8, it is found that with a very low number of concepts all classifiers achieve higher performances that, in turn, stabilize till the end of the experimented range ( k = 1002 ). A similar behavior can be found analyzing the "Reuters-21578" data set. Here the granularity level is fixed to k = 502 . Also in this case the performance in terms of classification capability increases quickly rising k (graphs not shown for the sake of brevity). However, in this case, examining the results reported in Table 3, the higher Accuracy value is attained by C-SVM (0.95 with LSA embedding), but even the LSTM obtains a good Accuracy (0.94 with word2vec embedding). If we consider the classification task as a rating process, Cohen's kappa coefficient is low for LSTM and RF, for both embeddings, but reaches the highest value for C-SVM with word2vec embedding. In terms of Fmicro and Fmacro C-SVM obtains its best results with the word2vec embedding. The best informed decision, taking into account the unbalance of the "Reuters-21578" data set is achieved by C-SVM with Informedness of 0.86 (LSA embedding). In general, as an expected behavior, we have a moderate variability of the classifiers' performances for both embeddings and data sets. The low Accuracy attained by LSTM for the "Abstracts" data set is likely to be addressed to the low granularity level and the short dimension of the data set, in terms of the number of documents and documents length. However, if we look at results for the best granularity level reported in Table 6 ("Abstracts" dataset), where the three classifiers are compared for both embedding types and with the TF-IDF features, LSTM obtains better performances with Accuracy 0.90 (word2vec embedding for best k = 222 ) and 0.92 (LSA embedding for best k = 162 ), outperforming the plain case (Accuracy 0.70), where sequences are directly generated without conceptualizing the corpus. In this particular setting, C-SVM with LSA embedding, for the best k = 382 , outperforms both LSTM and RF. For C-SVM the LSA embedding adopted for constructing the conceptual space is found better than the TF-IDF case (Accuracy 0.98 and 0.96, respectively). It is worth to note that for both C-SVM and RF the results for the plain case (TF-IDF feature space) are good (RF attains an Accuracy of 0.97 for k = 342 and 0.95 with TF-IDF) in spite of the high dimensionality of the features space. This confirms the capability of both classifiers to work well in high-dimensional spaces. Both algorithms achieve high Accuracy and high Informedness for a similar granularity level above k = 300 in the LSA embedding case, while LSTM obtain even good performances (Accuracy 0.92) with the same embedding with a very low Table 5 Performances comparison over the "Reuters-21578" data set between LSTM, C-SVM and RF for both word2vec and LSA embeddings. Results of the best granulation level k are compared with the plain solution given by a sequence formed by the word vectors related to words in the given text for LSTM, and the TF-IDF weighting scheme for the other classifiers Comparing the two data sets, that are very different in their structure and contents, the granularity level needed for attaining good results is found proportional to the complexity of the data set itself. In fact, the "Abstracts" data set consists of a set of short documents and each class is well separated in term of contents, at least at a semantic point of view. The "Reuters-21578" data set possesses more classes that are strongly unevenly distributed (see Fig. 7). For both data set, the conceptualization does not degrade the results, instead we obtain good performances with a lower granularity level. This means a low complexity of the feature space, that instead to be equal to the cardinality of the vocabulary (in the plain TF-IDF case) it matches the number of concepts adopted for representing the documents.</p>
<p>In the current experiments, there is no supremacy among the word2vec and the LSA in building the concept space. In fact, even if for both embeddings a smaller concept space attains good performances, there are classifiers that perform well for LSA and classifiers that do the best for the word-2vec embedding. It is well known that both the embeddings possess suitable semantic characteristics, and the general performances depend on the entire processing chain and the particular hyper-parameter settings.</p>
<p>In general, the granulation of the word embedding space can be seen as a dimensionality reduction paradigm at the cost of inserting a new block in the downstream processing of texts before the classification task. However, this conceptualization block can be constructed once and for all even adopting richer corpora (e.g., Wikipedia), conversely to the one employed for the specific classification task. It is important to take care of the granulation parameter, significant for the classifier performances due to their attitudes in working with high or low dimensionality. Table 6 Performances comparison over the "Abstracts" data set between LSTM, C-SVM and RF for both word2vec and LSA embeddings. Results of the best granulation level k are compared with the plain solu-tion given by a sequence formed by the word vectors related to words in the given text for LSTM, and the TF-IDF weighting scheme for the other classifiers </p>
<p>3</p>
<p>Towards the Explainable AI paradigm</p>
<p>The choice of the three particular classification algorithms depends on their specific characteristics. In fact, if from one hand SVM is known to be performing even with high dimensional feature spaces embedded in ℝ n , LSTM is suitable with sequences and needs a further dense layer to be appropriate for classification tasks. RF, for its part, is suited for classification tasks where it is important also estimating the importance of features. In fact, RF offers the possibility to obtain a set of weights, as many as the number of features, that, in turn, in this study are a kind of superordinate word, we call concepts. Fixing an arbitrary threshold over these weights leads to estimate the strongest concepts related to the specific classification task. From a different point of view, this procedure can be seen as a concept filtering task, where only the strongest ones survive. This methodology can help to infer knowledge on the corpus, eliciting the Explainable AI paradigm. In Fig. 9(a), (b), as a bar chart, the weights related to the concepts and the thresholds (fixed to the 20% of the largest weight value), for the "Reuters-21578" and "Abstracts" data set, respectively, are depicted. For these specific experiments, for brevity purposes, the granularity of the concept space is fixed to k = 50 and it is constructed through the word2vec neural embedding. It is worth to note that the CSS is obtained through the k-means clustering algorithm, thus the prototype, being the average word vector for a given concept region, is a surrogate word vector. So forth, in order to find the existing word related to the prototype, the 2 norm distance is computed selecting the nearest word vector and the corresponding token. In Tables 7 and 8 the survived concepts together with the nearest five words obtained computing a 2 norm between the prototype and the respective word vectors are reported, normalizing for the highest similarity value that hold for the closest existing word vector to the prototype. In case of clusters with cardinality lower than five, all words within the cluster are shown. For the "Abstracts" data set, the best closest prototypes, for a given threshold value, are holographic, concurrently, explicitly, explanation, succesfully, achieve, intrusiveness, leastsquares, pregnancy, furthermore, nonabelian, effectiveness, percutaneous, approximation, nanolasers, robustness, insulator, chalcogenide, rolling, infrared. From Table 7, selecting, for example, a populated region, such as the fourth and considering the closest word (concurrently) to its prototype, we have electrification, resistant, diameter, platform, industrial. In general, we can find high semantic words that elicit roughly which term the algorithm estimates as important for the classification task; in fact, besides words with high semantic content, we can find verbs (for example, achieve) or other lexemes that are mostly used in papers' abstracts. The same rationale can be found behind the results for the "Reuters-21578" data set illustrated in Table 7. Here, we can find less singleton or low-populated clusters, due to the dimension of the corpus. Even in this case it can be found a set of prototype words that span uniformly the conceptual space. Nevertheless, the richness of the semantic contents of prototypes and the underlying word cloud is attributable to the dimension and the heterogeneity of the corpus, since it is likely to lead to better representations for words, that in turn, leads to a performing CSS. Table 7 Most important concepts obtained by filtering the Concept Regions through the feature importance estimation provided by the RF algorithm ("Reuters-21578" data set). There are reported the first five words for each region that exceed a threshold value -see Fig. 9 </p>
<p>Conclusions</p>
<p>The current study is an effort in providing a clear relationship between findings in the Conceptual Spaces theory and the problem of text representation in Pattern Recognition, specifically in NLP tasks involved in text mining. Text mining, as a particular application of Machine Learning techniques related to textual data, benefits from better representations of text as hierarchically organized set of features, where Granular Computing techniques can offer a wide range of tools for designing performing classification algorithms. Within this framework, where Granular Computing is bound to the Conceptual Spaces theory and Machine Learning, it is offered a comparison of some classification algorithms working on features constructed over the prototypes of a conceptual space obtained, in turn, over a suitable neural word Table 8 Most important concepts obtained by filtering the Concept Regions through the feature importance estimation provided by the RF algorithm ("Abstracts" data set). There are reported the first five words for each region that exceed a threshold value -see Fig. 9(b) "Abstracts" data set Word Norm. similarity Concept Reg. Index "holographic<em>" 1 1 "holographic" 0.9737 1 "algorithm" 0.96451 1 "forward" 0.96052 1 "emphasis" 0.96039 1 "falling" 0.95878 1 "concurrently</em>" 1 4 "electrification" 0.9233 4 "resistant" 0.922 4 "diameter" 0.92197 4 "platform" 0.92186 4 "industrial" 0.92002 4 "explicitly<em>" 1 6 "respectively" 0.90374 6 "instantaneous" 0.9037 6 "role" 0.90366 6 "equilibrium" 0.90349 6 "vortex" 0.90345 6 "explanation</em>" 1 10 "investigation" 0.89886 10 "production" 0.89886 10 "congestion" 0.89669 10 "information" 0.89624 10 "decision" 0.89601 10 "successfully*"  embedding. Results show primarily that the conceptual layer placed in the middle between the associative layer and the symbolic layer (the symbolic histograms layer in this study) can be used for working with more abstract entities compared to words. These entities are a byproduct of the granulation of the conceptual space, that can be obtained with any algorithm in charge of embedding the text in a vector space. The three algorithms compared, two of them (SVM and RF) able to receive in input n-tuple of Real-valued numbers and the other (LSTM) working with input sequences, perform well for a large range of granulation levels of the conceptual space. Interestingly, depending on the nature of the textual data set, a low granulation level allows achieving good classification results, that at the stage of the current study depends only weakly from the specific algorithm. Moreover, the conceptual level together with the symbolic histograms technique can aid in Knowledge Discovery tasks, providing a framework for transforming black-box classifiers in gray ones, mining the strongest concepts that lead to a particular classification task, making a tiny step towards how a machine can represent meaning. Future works foresee the training of the conceptual space on exogenous corpora and, furthermore, the extension to n-gram prototypes where a suitable dissimilarity measure between sequences of vectors needs to be carefully designed in order to build up the symbolic histograms for the concepts embedding.</p>
<p>Funding Open access funding provided by Università degli Studi di Roma La Sapienza within the CRUI-CARE Agreement.</p>
<p>Data Availability</p>
<p>The datasets generated during and/or analyzed during the current study are available from the corresponding author on reasonable request.</p>
<p>Declarations</p>
<p>Ethical Approval This article does not contain any studies with human participants or animals performed by any of the authors.</p>
<p>Conflict of Interest</p>
<p>The authors declare no competing interests.</p>
<p>Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.</p>
<p>Fig. 1
1Information processing scheme</p>
<p>Fig. 2
2Centroidal Voronoi regions of the Conceptual Semantic Space for the Abstracts data set obtained through the k-means. Dots are words computed with the word2vec algorithm (word embedding) and projected in a bi-dimensional space through PCA. Crosses are the prototypes for each region. In this explanatory example the number of conceptual regions are k = 8</p>
<p>H
= S tr ∪ S vs ∪ S ts |S tr ∩ S vs = �, S tr ∩ S ts = �, S vs ∩ S ts = � .The definition of a cluster representative is well defined for vector feature spaces equipped with an algebraic structure, where it can be simply computed as the average vector in a set of real-valued vectors, i.e., i = ∑ i ∈C i �Ci� .</p>
<p>Fig. 4
4Fig. 4 Symbolic histogram for four documents pertaining the classes "Anatomy", "Information Theory", "String Theory", "Semiconductors" of the Abstracts data set</p>
<p>, j ).</p>
<p>-
the average Accuracy (Acc.) in[0,1], that is the average per-class effectiveness of a classifier; -the Precision (P) in [0,1], that is the fraction of relevant instances among the retrieved instances by the classifier; -the Recall (R) in [0,1], that is the fraction of the total amount of relevant instances that were actually retrieved by the classifier; -the Informedness (Inf.) in [0,1] -known as J-index -is the maximum distance between the bisector diagonal line of the Receiver operating characteristic (ROC)[84] dia-</p>
<p>Fig. 6 Fig. 7
67Document Class distribution for the experimented data sets 1 3</p>
<p>Fig. 9
9Concept</p>
<p>Table 1
1Data set statistics (in brackets it is reported the standard deviation) with standard deviation in brackets and the number of documents per class ( # docs) are reported. In# docs. 
|V| 
|V| pre </p>
<h1>class.</h1>
<p>| 
|D 
| 
| </p>
<p>Reuters-21578 
7674 
23585 
20768 
8 
67.649 (68.080) 
Abstracts 
575 
8722 
6585 
5 
65.464 (31.687) </p>
<p>Table 2
2Data set statistics per class (in brackets it is reported the stand-
ard deviation) </p>
<p>class 
| 
|D 
| 
| </p>
<h1>docs.</h1>
<p>"Reuters-21578" 
'acq' 
118.240(113.866) 
2292 
'crude' 
187.297(155.367) 
374 
'earn' 
65.642(76.740) 
392 
'grain' 
183.765(182.954) 
51 
'interest' 
118.672(154.255) 
271 
'money-fx' 
161.867(173.211) 
293 
'ship' 
149.924(85.788) 
144 
'trade' 
234.460(179.296) 
326 
"Abstracts" 
'Anatomy' 
96.400(43.593) 
115 
'Information theory' 
146.148(52.508) 
115 
'Smart Grid' 
148.843(51.823) 
115 
'String Theory' 
84.809(44.047) 
115 
'Semiconductors' 
126.565(63.512) 
115 </p>
<p>Table 3
3Classification performances for a given granularity level k for LSTM, C-SVM and RF ("Reuters-21578" data set)Reuters-21578 data set </p>
<p>LSTM 
C-SVM 
RF </p>
<p>word2vec 
k 
502 
502 
502 
Acc. 
0.9469(0.0000) 
0.9495(0.0000) 
0.9322(0.0000) 
P 
0.8294(0.0009) 
0.8568(0.0000) 
0.8856(0.0000) 
R 
0.8305(0.0002) 
0.8339(0.0002) 
0.6985(0.0002) 
Inf. 
0.8219(0.0002) 
0.8251(0.0001) 
0.6861(0.0002) 
Kappa 
0.7571(0.0000) 
0.7647(0.0000) 
0.6902(0.0000) 
Fmicro 
0.9469 
0.9485 
0.9322 
Fmacro 
0.8255 
0.8448 
0.7766 
LSA 
k 
502 
502 
502 
Acc. 
0.9092(0.0004) 
0.9545(0.0000) 
0.9325(0.0000) 
P 
0.7148(0.0004) 
0.8921(0.0007) 
0.9101(0.0001) 
R 
0.7274(0.0010) 
87321(0.0001) 
0.7020(0.0003) 
Inf. 
0.7125(0.0013) 
0.8653(0.0001) 
0.6895(0.0003) 
Kappa 
0.5851(0.0069) 
0.9545(0.0003) 
0.6916(0.0004) 
Fmicro 
0.9092 
0.9545 
0.9325 
Fmacro 
0.7188 
0.8817 
0.7930 </p>
<p>(a) "Reuters-21578" data setWord 
Norm. similarity 
Concept 
Reg. 
Index </p>
<p>"overdraft<em>" 
1 
2 
"gilt" 
0.97116 
2 
"kwacha" 
0.95985 
2 
"ours" 
0.95916 
2 
"afterwards" 
0.95914 
2 
"peseta" 
0.95878 
2 
"tvx</em>" 
1 
17 
"matthey" 
0.9753 
17 
"behalf" 
0.97311 
17 
"rmj" 
0.96955 
17 
"cvn" 
0.96909 
17 
"labelling" 
0.96849 
17 
"mdc<em>" 
1 
18 
"cbs" 
0.97853 
18 
"mpb" 
0.97837 
18 
"lpl" 
0.9771 
18 
"uac" 
0.97703 
18 
"magna" 
0.97675 
18 
"fintech</em>" 
1 
20 
"interactive" 
0.9439 
20 
"intercompany" 
0.94195 
20 
"integral" 
0.93706 
20 
"intense" 
0.93507 
20 
"intend" 
0.93033 
20 
"veto<em>" 
1 
27 
"sense" 
0.94648 
27 
"accuse" 
0.94509 
27 
"herrington" 
0.94407 
27 
"gephardt" 
0.94021 
27 
"imported" 
0.93945 
27 
"ln</em>" 
1 
34 
"nov" 
0.92031 
34 
"ln" 
0.90159 
34 
"ust" 
0.89955 
34 
"eight" 
0.8988 
34 
"sept" 
0.89351 
34 
"libya*" 
1 
38 
"indonesian" 
0.93859 
38 
"libya" 
0.93804 
38 
"iea" 
0.93717 
38 
"quotas" 
0.93639 
38 
"egypt" 
0.93517 
38 </p>
<p>Table 8 (continued)
8"Abstracts" data set </p>
<p>Word 
Norm. similarity 
Concept 
Reg. 
Index </p>
<p>"leastsquares<em>" 
1 
25 
"behavior" 
0.89503 
25 
"importantly</em>" 
1 
26 
"transmission" 
0.95505 
26 
"pregnancy<em>" 
1 
30 
"dispatch" 
0.89991 
30 
"furthermore</em>" 
1 
31 
"chain" 
0.9178 
31 
"nonabelian<em>" 
1 
36 
"matter" 
0.90482 
36 
"effectiveness</em>" 
1 
37 
"limit" 
0.91467 
37 
"percutaneous<em>" 
1 
38 
"randomness" 
0.92696 
38 
"approximation</em>" 
1 
41 
"circulation" 
0.91257 
41 
"nanolasers<em>" 
1 
42 
"contrast" 
0.91577 
42 
"robustness</em>" 
1 
43 
"evidencebased" 
0.97102 
43 
"insulator<em>" 
1 
44 
"series" 
0.91583 
44 
"chalcogenide</em>" 
1 
46 
"hash" 
0.90352 
46 
"rolling<em>" 
1 
48 
"explores" 
0.92888 
48 
"infrared</em>" 
1 
50 
"higher" 
0.84745 
50 </p>
<p>Introduction to pattern recognition. The Oxford Companion to the Mind. A K Jain, Rpw Duin, Second EditionJain AK, Duin RPW. Introduction to pattern recognition. The Oxford Companion to the Mind, Second Edition; 2004.</p>
<p>Open issues in pattern recognition. Rpw Duin, E Pekalska, Computer Recognition Systems. SpringerDuin RPW, Pekalska E. Open issues in pattern recognition. In Computer Recognition Systems, pages 27-42. Springer; 2005.</p>
<p>Conceptual spaces: The geometry of thought. P Gärdenfors, MIT pressGärdenfors P. Conceptual spaces: The geometry of thought. MIT press; 2004.</p>
<p>A note on core research issues for statistical pattern recognition. Rpw Duin, F Roli, D De Ridder, Pattern Recogn Lett. 234Duin RPW, Roli F, de Ridder D. A note on core research issues for statistical pattern recognition. Pattern Recogn Lett. 2002;23(4):493-9.</p>
<p>Representation in cognitive science. N Shea, Oxford University PressShea N. Representation in cognitive science. Oxford University Press; 2018.</p>
<p>Complex systems and cognitive processes. R Serra, G Zanarini, Springer Science &amp; Business MediaSerra R, Zanarini G. Complex systems and cognitive processes. Springer Science &amp; Business Media; 2013.</p>
<p>Complex systems and applied linguistics. L Cameron, D Larsen-Freeman, Int J Appl Linguist. 172Cameron L, Larsen-Freeman D. Complex systems and applied linguistics. Int J Appl Linguist. 2007;17(2):226-39.</p>
<p>Science and sanity (lancaster). A Korzybski, Korzybski A. Science and sanity (lancaster); 1933.</p>
<p>Granular computing: Perspectives and challenges. J T Yao, A V Vasilakos, W Pedrycz, IEEE Trans Cybern. 436Yao JT, Vasilakos AV, Pedrycz W. Granular computing: Perspec- tives and challenges. IEEE Trans Cybern. 2013;43(6):1977-89.</p>
<p>An ecology-based index for text embedding and classification. A Martino, E Desantis, A Rizzi, 2020 International Joint Conference on Neural Networks (IJCNN). IEEEMartino A, DeSantis E, Rizzi A. An ecology-based index for text embedding and classification. In 2020 International Joint Con- ference on Neural Networks (IJCNN), pages 1-8. IEEE; 2020.</p>
<p>The puzzle of granular computing. B Apolloni, S Bassis, D Malchiodi, W Pedrycz, Springer138Apolloni B, Bassis S, Malchiodi D, Pedrycz W. The puzzle of granular computing, volume 138. Springer; 2008.</p>
<p>Automatic image classification by a granular computing approach. A Rizzi, G Delvescovo, 16th IEEE signal processing society workshop on machine learning for signal processing. IEEERizzi A, DelVescovo G. Automatic image classification by a granular computing approach. In 2006 16th IEEE signal process- ing society workshop on machine learning for signal processing, pages 33-38. IEEE; 2006.</p>
<p>Toward a theory of granular computing for human-centered information processing. A Bargiela, W Pedrycz, IEEE Trans Fuzzy Syst. 162Bargiela A, Pedrycz W. Toward a theory of granular computing for human-centered information processing. IEEE Trans Fuzzy Syst. 2008;16(2):320-30.</p>
<p>Toward a theory of fuzzy information granulation and its centrality in human reasoning and fuzzy logic. L A Zadeh, Fuzzy Sets Syst. 902Zadeh LA. Toward a theory of fuzzy information granulation and its centrality in human reasoning and fuzzy logic. Fuzzy Sets Syst. 1997;90(2):111-27.</p>
<p>Some reflections on soft computing, granular computing and their roles in the conception, design and utilization of information/intelligent systems. Soft Computing-A Fusion of Foundations. L A Zadeh, Methodologies and Applications. 21Zadeh LA. Some reflections on soft computing, granular com- puting and their roles in the conception, design and utilization of information/intelligent systems. Soft Computing-A Fusion of Foundations, Methodologies and Applications. 1998;2(1):23-5.</p>
<p>On the origin of long-range correlations in texts. E G Altmann, G Cristadoro, M D Esposti, Proc Natl Acad Sci. 10929Altmann EG, Cristadoro G, Esposti MD. On the origin of long-range correlations in texts. Proc Natl Acad Sci. 2012;109(29):11582-7.</p>
<p>Building pattern recognition applications with the spare library. Lorenzo Livi, Guido Delvescovo, Antonello Rizzi, Fabio Massimo Frattalemascioli, arXiv:1410.5263arXiv preprintLorenzo Livi, Guido DelVescovo, Antonello Rizzi, and Fabio Mas- simo FrattaleMascioli. Building pattern recognition applications with the spare library. arXiv preprint arXiv: 1410. 5263; 2014.</p>
<p>Towards context sensitive information inference. D Song, P D Bruza, J Am Soc Inform Sci Technol. 544Song D, Bruza PD. Towards context sensitive information infer- ence. J Am Soc Inform Sci Technol. 2003;54(4):321-34.</p>
<p>Automatic classification of graphs by symbolic histograms. G Delvescovo, A Rizzi, 2007 IEEE International Conference on Granular Computing (GRC 2007). IEEEDelVescovo G, Rizzi A. Automatic classification of graphs by symbolic histograms. In 2007 IEEE International Conference on Granular Computing (GRC 2007), pages 410-410. IEEE; 2007.</p>
<p>Mining m-grams by a granular computing approach for text classification. A Capillo, E Desantis, Fmf Mascioli, A Rizzi, Capillo A, DeSantis E, Mascioli FMF, Rizzi A. Mining m-grams by a granular computing approach for text classification; 2020.</p>
<p>Distributional semantics today. C Fabre, A Lenci, Fabre C, Lenci A. Distributional semantics today; 2015.</p>
<p>Wittgenstein's philosophical investigations. Malcolm Norman, Philos Rev. 634Malcolm Norman. Wittgenstein's philosophical investigations. Philos Rev. 1954;63(4):530-59.</p>
<p>. Z S Harris, Distributional structure. Word. 102-3Harris ZS. Distributional structure. Word. 1954;10(2-3):146-62.</p>
<p>A synopsis of linguistic theory, 1930-1955. Studies in linguistic analysis. J R Firth, Firth JR. A synopsis of linguistic theory, 1930-1955. Studies in linguistic analysis; 1957.</p>
<p>Understanding bag-of-words model: a statistical framework. Y Zhang, R Jin, Z H Zhou, Int J Mach Learn Cybern. 11-4Zhang Y, Jin R, Zhou ZH. Understanding bag-of-words model: a statistical framework. Int J Mach Learn Cybern. 2010; 1(1-4):43-52.</p>
<p>The conversational interface: Talking to smart devices. M Mctear, Z Callejas, D Griol, SpringerMcTear M, Callejas Z, Griol D. The conversational interface: Talking to smart devices. Springer; 2016.</p>
<p>A vector space model for automatic indexing. G Salton, A Wong, C S Yang, Commun ACM. 1811Salton G, Wong A, Yang CS. A vector space model for automatic indexing. Commun ACM. 1975;18(11):613-20.</p>
<p>Producing high-dimensional semantic spaces from lexical co-occurrence. K Lund, C Burgess, Behav Res Methods Instrum Comput. 282Lund K, Burgess C. Producing high-dimensional semantic spaces from lexical co-occurrence. Behav Res Methods Instrum Comput. 1996;28(2):203-8.</p>
<p>Linguistic regularities in sparse and explicit word representations. O Levy, Y Goldberg, Proceedings of the Eighteenth Conference On Computational Natural Language Learning. the Eighteenth Conference On Computational Natural Language LearningLevy O, Goldberg Y. Linguistic regularities in sparse and explicit word representations. In Proceedings of the Eighteenth Conference On Computational Natural Language Learning, pages 171-180; 2014.</p>
<p>A solution to plato's problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge. T K Landauer, S T Dumais, Psychol Rev. 1042211Landauer TK, Dumais ST. A solution to plato's problem: The latent semantic analysis theory of acquisition, induction, and rep- resentation of knowledge. Psychol Rev. 1997;104(2):211.</p>
<p>An introduction to latent semantic analysis. T K Landauer, P W Foltz, D Laham, Discourse Process. 252-3Landauer TK, Foltz PW, Laham D. An introduction to latent semantic analysis. Discourse Process. 1998;25(2-3):259-84.</p>
<p>Glove: Global vectors for word representation. J Pennington, R Socher, C Manning, Proceedings of the 2014 Conference On Empirical Methods in Natural Language Processing (EMNLP). the 2014 Conference On Empirical Methods in Natural Language Processing (EMNLP)Pennington J, Socher R, Manning C. Glove: Global vectors for word representation. In Proceedings of the 2014 Conference On Empirical Methods in Natural Language Processing (EMNLP), pages 1532-1543; 2014.</p>
<p>Efficient estimation of word representations in vector space. T Mikolov, K Chen, G Corrado, J Dean, arXiv:1301.3781arXiv preprintMikolov T, Chen K, Corrado G, Dean J. Efficient estimation of word representations in vector space. arXiv preprint arXiv: 1301. 3781; 2013.</p>
<p>Distributed representations of words and phrases and their compositionality. T Mikolov, I Sutskever, K Chen, G S Corrado, J Dean, Advances in neural information processing systems. Mikolov T, Sutskever I, Chen K, Corrado GS, Dean J. Distributed representations of words and phrases and their compositional- ity. In Advances in neural information processing systems, pages 3111-3119; 2013.</p>
<p>J Devlin, M W Chang, K Lee, Toutanova K Bert, arXiv:1810.04805Pre-training of deep bidirectional transformers for language understanding. arXiv preprintDevlin J, Chang MW, Lee K, Toutanova K. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv: 1810. 04805; 2018.</p>
<p>Computing semantic relatedness using wikipedia-based explicit semantic analysis. E Gabrilovich, S Markovitch, In IJcAI. 7Gabrilovich E, Markovitch S, et al. Computing semantic related- ness using wikipedia-based explicit semantic analysis. In IJcAI. 2007;7:1606-11.</p>
<p>Using bag-of-concepts to improve the performance of support vector machines in text categorization. M Sahlgren, R Cöster, Proceedings of the 20th international conference on Computational Linguistics. the 20th international conference on Computational LinguisticsAssociation for Computational Linguistics487Sahlgren M, Cöster R. Using bag-of-concepts to improve the performance of support vector machines in text categorization. In Proceedings of the 20th international conference on Compu- tational Linguistics, page 487. Association for Computational Linguistics; 2004.</p>
<p>WordNet: An electronic lexical database. G A Miller, MIT pressMiller GA. WordNet: An electronic lexical database. MIT press; 1998.</p>
<p>Natural categories. E H Rosch, Cogn Psychol. 43Rosch EH. Natural categories. Cogn Psychol. 1973;4(3):328-50.</p>
<p>Cognitive representations of semantic categories. E Rosch, J Exp Psychol Gen. 1043192Rosch E. Cognitive representations of semantic categories. J Exp Psychol Gen. 1975;104(3):192.</p>
<p>Sub-symbols and icons. A Wichert, Cogn Comput. 14Wichert A. Sub-symbols and icons. Cogn Comput. 2009;1(4):342-7.</p>
<p>The mind's chorus: Creativity before consciousness. G A Wiggins, Cogn Comput. 43Wiggins GA. The mind's chorus: Creativity before consciousness. Cogn Comput. 2012;4(3):306-19.</p>
<p>D Doran, S Schulz, T R Besold, arXiv:1710.00794What does explainable ai really mean? a new conceptualization of perspectives. arXiv preprintDoran D, Schulz S, Besold TR. What does explainable ai really mean? a new conceptualization of perspectives. arXiv preprint arXiv: 1710. 00794; 2017.</p>
<p>Explanation in human-ai systems: A literature meta-review, synopsis of key ideas and publications, and bibliography for explainable ai. S T Mueller, R R Hoffman, W Clancey, A Emrey, G Klein, arXiv:1902.01876arXiv preprintMueller ST, Hoffman RR, Clancey W, Emrey A, Klein G. Expla- nation in human-ai systems: A literature meta-review, synopsis of key ideas and publications, and bibliography for explainable ai. arXiv preprint arXiv: 1902. 01876; 2019.</p>
<p>Online handwriting recognition by the symbolic histograms approach. G Delvescovo, A Rizzi, IEEE International Conference on Granular Computing. 686IEEEDelVescovo G, Rizzi A. Online handwriting recognition by the symbolic histograms approach. In 2007 IEEE International Confer- ence on Granular Computing (GRC 2007), pages 686. IEEE; 2007.</p>
<p>A granular computing approach to the design of optimized graph classification systems. F M Bianchi, L Livi, A Rizzi, A Sadeghian, Soft Comput. 182Bianchi FM, Livi L, Rizzi A, Sadeghian A. A granular computing approach to the design of optimized graph classification systems. Soft Comput. 2014;18(2):393-412.</p>
<p>Graph recognition by seriation and frequent substructures mining. L Livi, Del Vescovo, G Rizzi, A , ICPRAM. 1Livi L, Del Vescovo G, Rizzi A. Graph recognition by seriation and frequent substructures mining. In ICPRAM. 2012;1:186-91.</p>
<p>Combining graph seriation and substructures mining for graph recognition. L Livi, G Delvescovo, A Rizzi, Pattern Recognition-Applications and Methods. SpringerLivi L, DelVescovo G, Rizzi A. Combining graph seriation and substructures mining for graph recognition. In Pattern Recognition- Applications and Methods, pages 79-91. Springer; 2013.</p>
<p>A new granular computing approach for sequences representation and classification. A Rizzi, G Delvescovo, L Livi, Fmf Mascioli, The 2012 International Joint Conference on Neural Networks (IJCNN). IEEERizzi A, DelVescovo G, Livi L, Mascioli FMF. A new granular computing approach for sequences representation and classifica- tion. In The 2012 International Joint Conference on Neural Net- works (IJCNN), pages 1-8. IEEE; 2012.</p>
<p>Granular computing for text mining: New research challenges and opportunities. L Jing, Ryk Lau, International Workshop on Rough Sets, Fuzzy Sets, Data Mining, and Granular-Soft Computing. SpringerJing L, Lau RYK. Granular computing for text mining: New research challenges and opportunities. In International Workshop on Rough Sets, Fuzzy Sets, Data Mining, and Granular-Soft Com- puting, pages 478-485. Springer; 2009.</p>
<p>Handbook of granular computing. W Pedrycz, A Skowron, V Kreinovich, John Wiley &amp; SonsPedrycz W, Skowron A, Kreinovich V. Handbook of granular computing. John Wiley &amp; Sons; 2008.</p>
<p>An application on text classification based on granular computing. X Zhang, Y Yin, Y Haiyan, Communications of the IIMA. 721Zhang X, Yin Y, Haiyan Y. An application on text classifica- tion based on granular computing. Communications of the IIMA. 2007;7(2):1.</p>
<p>Automatic text categorization by a granular computing approach: facing unbalanced data sets. F Possemato, A Rizzi, The 2013 International Joint Conference on Neural Networks (IJCNN). IEEEPossemato F, Rizzi A. Automatic text categorization by a granular computing approach: facing unbalanced data sets. In The 2013 International Joint Conference on Neural Networks (IJCNN), pages 1-8. IEEE; 2013.</p>
<p>Concept based representations as complement of bag of words in information retrieval. M Carrillo, A López-López, IFIP International Conference on Artificial Intelligence Applications and Innovations. SpringerCarrillo M, López-López A. Concept based representations as complement of bag of words in information retrieval. In IFIP International Conference on Artificial Intelligence Applications and Innovations, pages 154-161. Springer; 2010.</p>
<p>A bag of concepts approach for biomedical document classification using wikipedia knowledge. M A Mouriño-García, R Pérez-Rodríguez, L E Anido-Rifón, Methods Inf Med. 5605Mouriño-García MA, Pérez-Rodríguez R, Anido-Rifón LE. A bag of concepts approach for biomedical document classification using wikipedia knowledge. Methods Inf Med. 2017;56(05):370-6.</p>
<p>Mined semantic analysis: A new concept space model for semantic representation of textual data. W Shalaby, W Zadrozny, 2017 IEEE International Conference on Big Data (Big Data). IEEEShalaby W, Zadrozny W. Mined semantic analysis: A new concept space model for semantic representation of textual data. In 2017 IEEE International Conference on Big Data (Big Data), pages 2122-2131. IEEE; 2017.</p>
<p>Bag-of-concepts: Comprehending document representation through clustering words in distributed representation. H K Kim, H Kim, S Cho, Neurocomputing. 266Kim HK, Kim H, Cho S. Bag-of-concepts: Comprehending docu- ment representation through clustering words in distributed rep- resentation. Neurocomputing. 2017;266:336-52.</p>
<p>Emotion recognition for sentences with unknown expressions based on semantic similarity by using bag of concepts. K Matsumoto, M Yoshida, Q Xiao, X Luo, K Kita, 12th International Conference on Fuzzy Systems and Knowledge Discovery (FSKD). IEEEMatsumoto K, Yoshida M, Xiao Q, Luo X, Kita K. Emotion rec- ognition for sentences with unknown expressions based on seman- tic similarity by using bag of concepts. In 2015 12th International Conference on Fuzzy Systems and Knowledge Discovery (FSKD), pages 1394-1399. IEEE; 2015.</p>
<p>Learning concept embeddings for dataless classification via efficient bag-of-concepts densification. W Shalaby, W Zadrozny, Knowl Inf Syst. Shalaby W, Zadrozny W. Learning concept embeddings for data- less classification via efficient bag-of-concepts densification. Knowl Inf Syst. 2019;1-24.</p>
<p>Applications of conceptual spaces: the case for geometric knowledge representation. F Zenker, P Gärdenfors, Springer359Zenker F, Gärdenfors P. Applications of conceptual spaces: the case for geometric knowledge representation, volume 359. Springer; 2015.</p>
<p>Quantum aspects of high dimensional conceptual space: a model for achieving consciousness. M S Ishwarya, C A Kumar, Cogn Comput. Ishwarya MS, Kumar CA. Quantum aspects of high dimensional conceptual space: a model for achieving consciousness. Cogn Comput., 2020;1-14.</p>
<p>The geometry of meaning: Semantics based on conceptual spaces. P Gärdenfors, MIT PressGärdenfors P. The geometry of meaning: Semantics based on con- ceptual spaces. MIT Press; 2014.</p>
<p>Fundamental aspects of cognitive representation. Cognition and Categorization. S E Palmer, Palmer SE. Fundamental aspects of cognitive representation. Cog- nition and Categorization 1978.</p>
<p>Extracting prototypes from exemplars what can corpus data tell us about concept representation?. D Divjak, A Arppe, Divjak D, Arppe A. Extracting prototypes from exemplars what can corpus data tell us about concept representation? 2013.</p>
<p>Foundations of cognitive grammar: Theoretical prerequisites, volume1. R W Langacker, Stanford university pressLangacker RW. Foundations of cognitive grammar: Theoretical prerequisites, volume1. Stanford university press; 1987.</p>
<p>The dissimilarity representation for pattern recognition: Foundations and applications. E Pčkalska, Rpw Duin, Pčkalska E, Duin RPW. The dissimilarity representation for pat- tern recognition: Foundations and applications; 2005.</p>
<p>Spatial tessellations: concepts and applications of Voronoi diagrams. A Okabe, B Boots, K Sugihara, S N Chiu, John Wiley &amp; Sons501Okabe A, Boots B, Sugihara K, Chiu SN. Spatial tessellations: concepts and applications of Voronoi diagrams, volume 501. John Wiley &amp; Sons; 2009.</p>
<p>Centroidal voronoi tessellations: Applications and algorithms. D Qiang, V Faber, M Gunzburger, SIAM Rev. 414Qiang D, Faber V, Gunzburger M. Centroidal voronoi tessellations: Applications and algorithms. SIAM Rev. 1999;41(4):637-76.</p>
<p>On component-wise dissimilarity measures and metric properties in pattern recognition. S E De, A Martino, A Rizzi, PeerJ Computer Science. 8De S E, Martino A, Rizzi A. On component-wise dissimilarity measures and metric properties in pattern recognition. PeerJ Com- puter Science. 2022;8.</p>
<p>Least squares quantization in pcm. S P Lloyd, IEEE Trans Inf Theory. 28Lloyd SP. Least squares quantization in pcm. IEEE Trans Inf Theory. 1982;28:129-37.</p>
<p>Cluster analysis of multivariate data: Efficiency versus interpretability of classifications. E W Forgy, Biometrics. 21Forgy EW. Cluster analysis of multivariate data: Efficiency versus interpretability of classifications. Biometrics. 1965;21:768-9.</p>
<p>On the Problem of Modeling Structured Data with the MinSOD Representative. Del Vescovo, G Livi, L Mascioli, Fmf Rizzi, A , International Journal of Computer Theory and Engineering. 61Del Vescovo G, Livi L, Mascioli FMF, Rizzi A. On the Prob- lem of Modeling Structured Data with the MinSOD Representa- tive. International Journal of Computer Theory and Engineering. 2014;6(1):9-14.</p>
<p>Statistical learning theory. V N Vapnik, V Vapnik, Wiley1New YorkVapnik VN, Vapnik V. Statistical learning theory, vol. 1. New York: Wiley; 1998.</p>
<p>Advances in kernel methods: support vector learning. B Schölkopf, Cjc Burges, MIT pressSchölkopf B, Burges CJC. Advances in kernel methods: support vector learning. MIT press; 1999.</p>
<p>Random decision forests. T K Ho, Proceedings of 3rd international conference on document analysis and recognition. 3rd international conference on document analysis and recognitionIEEE1Ho TK. Random decision forests. In Proceedings of 3rd interna- tional conference on document analysis and recognition, volume1, pages 278-282. IEEE; 1995.</p>
<p>On the algorithmic implementation of stochastic discrimination. E M Kleinberg, IEEE Trans Pattern Anal Mach Intell. 225Kleinberg EM. On the algorithmic implementation of sto- chastic discrimination. IEEE Trans Pattern Anal Mach Intell. 2000;22(5):473-90.</p>
<p>Learning to forget: continual prediction with lstm. F A Gers, J Schmidhuber, F Cummins, Ninth International Conference on Artificial Neural Networks ICANN 99. (Conf. Publ. No. 470), volume2. 2Gers FA, Schmidhuber J, Cummins F. Learning to forget: contin- ual prediction with lstm. In 1999 Ninth International Conference on Artificial Neural Networks ICANN 99. (Conf. Publ. No. 470), volume2, pages 850-855 vol.2; Sep. 1999.</p>
<p>Deep Learning: Fundamentals, Theory and Applications, volume2. K Huang, A Hussain, Q F Wang, R Zhang, SpringerHuang K, Hussain A, Wang QF, Zhang R. Deep Learning: Fun- damentals, Theory and Applications, volume2. Springer; 2019.</p>
<p>Deep learning for NLP and speech recognition. U Kamath, J Liu, J Whitaker, Kamath U, Liu J, Whitaker J. Deep learning for NLP and speech recognition; 2019.</p>
<p>Mining data with random forests: A survey and results of new tests. A Verikas, A Gelzinis, M Bacauskiene, Pattern Recogn. 442Verikas A, Gelzinis A, Bacauskiene M. Mining data with ran- dom forests: A survey and results of new tests. Pattern Recogn. 2011;44(2):330-49.</p>
<p>Machine Learning Repository: Reuters-21578 Text Categorization Collection Data Set. S Dobbins, M Topliss, Steve Weinstein, S Uci, Dobbins S, Topliss M, Steve Weinstein S. UCI Machine Learning Repository: Reuters-21578 Text Categorization Collection Data Set; 1987.</p>
<p>. S Dobbins, M Topliss, Steve Weinstein, S , - , Dobbins S, Topliss M, Steve Weinstein S, -.</p>
<p>A systematic analysis of performance measures for classification tasks. Information Processing &amp; Management. M Sokolova, G Lapalme, 45Sokolova M, Lapalme G. A systematic analysis of performance measures for classification tasks. Information Processing &amp; Man- agement. 2009;45(4):427-37.</p>
<p>An introduction to roc analysis. T Fawcett, ROC Analysis in Pattern Recognition. 27Fawcett T. An introduction to roc analysis. Pattern Recognition Let- ters 2006;27(8):861-874. ROC Analysis in Pattern Recognition.</p>
<p>Interrater reliability: The kappa statistic. M L Mchugh, Biochemia Medica: Biochemia Medica. 223McHugh ML. Interrater reliability: The kappa statistic. Biochemia Medica: Biochemia Medica. 2012;22(3):276-82.</p>
<p>Rusboost: Improving classification performance when training data is skewed. C Seiffert, T M Khoshgoftaar, J Vanhulse, A Napolitano, 19th International Conference on Pattern Recognition. IEEESeiffert C, Khoshgoftaar TM, VanHulse J, Napolitano A. Rus- boost: Improving classification performance when training data is skewed. In 2008 19th International Conference on Pattern Rec- ognition, pages 1-4. IEEE; 2008.</p>
<p>Y Freund, arXiv:0905.2138A more robust boosting algorithm. arXiv preprintFreund Y. A more robust boosting algorithm. arXiv preprint arXiv: 0905. 2138; 2009.</p>
<p>The elements of statistical learning: data mining, inference and prediction. T Hastie, R Tibshirani, J Friedman, J Franklin, Math Intell. 272Hastie T, Tibshirani R, Friedman J, Franklin J. The elements of statistical learning: data mining, inference and prediction. Math Intell. 2005;27(2):83-5.</p>            </div>
        </div>

    </div>
</body>
</html>
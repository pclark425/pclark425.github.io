<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5708 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5708</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5708</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-116.html">extraction-schema-116</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <p><strong>Paper ID:</strong> paper-269293744</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2404.13627v3.pdf" target="_blank">NegotiationToM: A Benchmark for Stress-testing Machine Theory of Mind on Negotiation Surrounding</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) have sparked substantial interest and debate concerning their potential emergence of Theory of Mind (ToM) ability. Theory of mind evaluations currently focuses on testing models using machine-generated data or game settings prone to shortcuts and spurious correlations, which lacks evaluation of machine ToM ability in real-world human interaction scenarios. This poses a pressing demand to develop new real-world scenario benchmarks. We introduce NegotiationToM, a new benchmark designed to stress-test machine ToM in real-world negotiation surrounding covered multi-dimensional mental states (i.e., desires, beliefs, and intentions). Our benchmark builds upon the Belief-Desire-Intention (BDI) agent modeling theory and conducts the necessary empirical experiments to evaluate large language models. Our findings demonstrate that NegotiationToM is challenging for state-of-the-art LLMs, as they consistently perform significantly worse than humans, even when employing the chain-of-thought (CoT) method.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5708.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5708.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought (CoT) prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought (CoT) prompting ("let's think step by step")</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting method that instructs the model to produce intermediate reasoning steps before the final answer; applied zero-shot in this paper using a CoT template following Kojima et al. (2022).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>multiple (GPT-4, ChatGPT, Claude-v1.3, Claude-v2.1, Llama-2 Chat 13B/70B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>NegotiationToM (desire/belief/intention classification; "All" and consistency metrics)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Real-world negotiation Theory-of-Mind benchmark (NegotiationToM) that asks LLMs to infer per-utterance desires (high/medium/low), beliefs about opponent preferences, and multi-label intentions from truncated dialogue history.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Zero-shot Chain-of-Thought: baseline multiple-choice Q&A prompt augmented with an instruction to 'let's think step by step' to elicit intermediate reasoning (CoT).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared against zero-shot baseline (non-CoT multiple-choice) and few-shot exemplars.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Examples reported: GPT-4 (CoT) — Desire exact match 63.29%, Belief exact match 58.18%, Intention Micro F1 34.90%, Macro F1 31.26%. ChatGPT (CoT) — Desire 28.45%, Belief 21.00%, Intention Micro F1 36.71%, Macro F1 30.79%. Claude-v2.1 (CoT) — Desire 50.13%, Belief 40.52%, Intention Micro F1 39.93%, Macro F1 35.67%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>GPT-4 zero-shot (non-CoT) — Desire 62.77%, Belief 57.62%, Intention Micro F1 29.84% (CoT improved intention from 29.84→34.90); ChatGPT zero-shot — Desire 18.60%→CoT 28.45% (≈+9.85), Belief 13.04%→21.00% (≈+7.96); LLaMa2-Chat(70B) zero-shot 24.40%→CoT 30.34% (Desire) and Belief 21.58%→24.23%.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Varies by model; examples: ChatGPT Desire +9.85 pct, Belief +7.96 pct; GPT-4 Intention Micro F1 +5.06 pct; LLaMa2-Chat(70B) Desire +5.94 pct.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors report that CoT generally improves performance because it forces the model to reason over conversation context (reducing heuristic guessing such as inventing a 'medium' preference when not provided). CoT reduces specific error types that stem from superficial pattern matching and encourages stepwise deduction from dialogue history.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>Improvements are model-dependent: GPT-4 showed only small gains on desire/belief but larger gains on intention; some few-shot setups (see next entry) outperformed CoT on intention for GPT-4 by a small margin, indicating CoT is not uniformly superior for every metric/model.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NegotiationToM: A Benchmark for Stress-testing Machine Theory of Mind on Negotiation Surrounding', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5708.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5708.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Few-shot prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Few-shot prompting with input–output exemplars</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting format that prepends several human-authored exemplars (4 for desire/belief, 7 for intentions) to the task prompt to teach the model desired label mapping.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>multiple (GPT-4, ChatGPT, Claude-v1.3, Claude-v2.1, Llama-2 Chat 13B/70B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>NegotiationToM (desire/belief/intention classification)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same NegotiationToM evaluation; few-shot probes whether exemplars improve classification over zero-shot and CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Few-shot: task instruction followed by N labeled exemplars (<EX1-INP><EX1-OUT> ...), then the new input to be labeled (N=4 for desire/belief, N=7 for intention).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared against zero-shot baseline and zero-shot Chain-of-Thought.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Selected examples from paper: GPT-4 few-shot — Desire 62.89%, Belief 52.08%, Intention Micro F1 35.10%, Macro F1 33.21% (few-shot intention slightly > CoT: 35.10% vs CoT 34.90% Micro). ChatGPT few-shot — Desire 19.24%, Belief 17.02%. Claude-v2.1 few-shot — Desire 48.77%, Belief 41.88%, Intention Micro F1 38.23%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>For many models few-shot improved over zero-shot baseline but was typically worse than CoT for desire/belief; for GPT-4 intention, few-shot slightly outperformed CoT (Intention Micro F1 35.10% vs CoT 34.90%).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Model-dependent. Example: GPT-4 Intention Micro F1: few-shot +0.20 pct vs CoT; ChatGPT few-shot often underperformed CoT by ~8–10 pct on desire/belief.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>mixed (usually improved over plain zero-shot; sometimes inferior to CoT; small improvement for GPT-4 on intention)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors note few-shot exemplars help LLMs learn label semantics but may not encourage explicit stepwise reasoning; exemplars can improve some tasks (especially multi-label intention when examples cover labels) but CoT remains effective for reasoning-heavy aspects.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>Few-shot was not always better than CoT: most models had higher scores with CoT than with few-shot on desire/belief; only some intention metrics (GPT-4) saw few-shot slightly outperform CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NegotiationToM: A Benchmark for Stress-testing Machine Theory of Mind on Negotiation Surrounding', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5708.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5708.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zero-shot multiple-choice baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-shot multiple-choice prompt template (Robinson & Wingate style)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline zero-shot prompting that formulates the task as a multiple-choice question without exemplars or CoT; used as the baseline for NegotiationToM evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>multiple (GPT-4, ChatGPT, Claude-v1.3, Claude-v2.1, Llama-2 Chat 13B/70B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>NegotiationToM (desire/belief/intention)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multiple-choice classification of high/medium/low preferences and multi-label intentions from dialogue snippets.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Simple zero-shot multiple-choice Q&A prompt that asks the model to pick one of options (A/B/C/D) or select one/multiple intention labels without step-by-step chain-of-thought.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to CoT and few-shot variants.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Examples: GPT-4 zero-shot (combined Q format) — Desire 62.77%, Belief 57.62%, Intention Micro F1 29.84%. ChatGPT zero-shot (combined) — Desire 18.60%, Belief 13.04%. LLaMa2-Chat(70B) zero-shot — Desire 24.40%, Belief 21.58%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Compared to CoT and few-shot: generally lower than CoT; few-shot sometimes improved over zero-shot but usually did not exceed CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>CoT often provided double-digit improvements over zero-shot for weaker models (e.g., ChatGPT Desire increased ≈+9.85 pct), while GPT-4 showed marginal zero-shot→CoT gains on desire/belief but larger gains on intention.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved when replaced by CoT or few-shot (i.e., baseline was weaker)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Zero-shot multiple-choice can be sensitive to prompt phrasing and suffers when models rely on shallow heuristics; structured reasoning prompts (CoT) or exemplars reduce such heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NegotiationToM: A Benchmark for Stress-testing Machine Theory of Mind on Negotiation Surrounding', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5708.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5708.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Question format: ranking / individual / combined</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Question-format variants for desire and belief (ranking, individual, combined)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Three prompt styles for asking preference questions: (a) ranking (single question asks for ordered trio), (b) individual (three separate questions for high/medium/low), (c) combined (single prompt that contains three constrained questions together).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo-0613), GPT-4 (gpt-4-0613)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>NegotiationToM (desire and belief preference questions)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Assess how different ways of asking the same preference information affects model ability to produce correct high/medium/low labels.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Ranking: ask for a single ordered triple (high→medium→low). Individual: ask three separate MC questions for high, medium, low. Combined: baseline prompt that includes all three preference questions in one combined template (constrains answers).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Ranking vs Individual vs Combined (reported per-model).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported zero-shot results (Table 5): ChatGPT — ranking Desire 2.88% / Belief 9.24%; individual Desire 9.18% / Belief 11.7%; combined Desire 18.60% / Belief 13.04%. GPT-4 — ranking Desire 20.10% / Belief 16.80%; individual Desire 40.01% / Belief 36.88%; combined Desire 62.77% / Belief 57.62%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Combined format consistently outperformed ranking and individual formats for both ChatGPT and GPT-4; for GPT-4 combined vs individual: Desire +22.76 pct, Belief +20.74 pct.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Large: GPT-4 combined vs individual: Desire +22.76 pct (62.77 vs 40.01), Belief +20.74 pct (57.62 vs 36.88). ChatGPT combined vs ranking: Desire +15.72 pct (18.60 vs 2.88).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>combined > individual > ranking (improved)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors hypothesize combined format imposes constraints that prevent implausible/incoherent partial answers (e.g., models hallucinating a medium preference when none is present) and reduces the need for models to 'combine' reasoning across separate answers; individual format forces models to perform multiple different reasoning types and increases errors in deducing absent information (especially medium preference).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>None reported for tested models (combined always performed best in reported comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NegotiationToM: A Benchmark for Stress-testing Machine Theory of Mind on Negotiation Surrounding', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5708.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5708.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Augmenting prompts with mental-state context (Desire+Belief)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt augmentation with annotated mental-state signals (desire and belief) for strategy prediction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Incorporating model- or human-annotated desire and belief labels into prompts for a downstream negotiation strategy prediction task, to test whether explicit ToM information helps performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (gpt-4-0613), ChatGPT (gpt-3.5-turbo-0613), Claude-v1.3, Claude-v2.1, LLaMa2-Chat (13B/70B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>CaSiNo negotiation strategy prediction (downstream task)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Predict negotiation strategy labels (intention subclasses) for utterances; evaluated with micro and macro F1.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Baseline strategy-prediction prompt vs enriched prompt that appends desire and belief (annotated ToM states) for the two agents into the prompt before asking for strategy labels.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Baseline (no ToM context) vs with Desire+Belief (w B.D.).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Examples: Claude-v2.1 baseline Intention Micro F1 27.12% → with Desire+Belief 34.56% (micro +7.44%, macro +5.54%). GPT-4 baseline 26.31% → with B.D. 32.71% (micro +6.4%). Other models showed similar gains (e.g., ChatGPT baseline 23.42% → with B.D. 28.99%).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Consistent improvements across models when adding desire+belief: Claude-v2.1 micro +7.44% (27.12→34.56), GPT-4 micro +6.40% (26.31→32.71), ChatGPT micro +5.57% (23.42→28.99).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Approximately +5–7.5 percentage points in Intention Micro F1 for reported top models.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors argue explicit ToM signals (desire & belief) provide useful structured context that helps the model disambiguate likely negotiation strategies, showing that inferring/feeding mental-state information improves downstream decision-making/prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NegotiationToM: A Benchmark for Stress-testing Machine Theory of Mind on Negotiation Surrounding', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Large language models are zero-shot reasoners <em>(Rating: 2)</em></li>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 1)</em></li>
                <li>Leveraging large language models for multiple choice question answering <em>(Rating: 2)</em></li>
                <li>Language models are few-shot learners <em>(Rating: 2)</em></li>
                <li>Casino: A corpus of campsite negotiation dialogues for automatic negotiation systems <em>(Rating: 2)</em></li>
                <li>BigToM: A framework for designing Theory-of-Mind benchmarks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5708",
    "paper_id": "paper-269293744",
    "extraction_schema_id": "extraction-schema-116",
    "extracted_data": [
        {
            "name_short": "Chain-of-Thought (CoT) prompting",
            "name_full": "Chain-of-Thought (CoT) prompting (\"let's think step by step\")",
            "brief_description": "A prompting method that instructs the model to produce intermediate reasoning steps before the final answer; applied zero-shot in this paper using a CoT template following Kojima et al. (2022).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "multiple (GPT-4, ChatGPT, Claude-v1.3, Claude-v2.1, Llama-2 Chat 13B/70B)",
            "model_size": null,
            "task_name": "NegotiationToM (desire/belief/intention classification; \"All\" and consistency metrics)",
            "task_description": "Real-world negotiation Theory-of-Mind benchmark (NegotiationToM) that asks LLMs to infer per-utterance desires (high/medium/low), beliefs about opponent preferences, and multi-label intentions from truncated dialogue history.",
            "problem_format": "Zero-shot Chain-of-Thought: baseline multiple-choice Q&A prompt augmented with an instruction to 'let's think step by step' to elicit intermediate reasoning (CoT).",
            "comparison_format": "Compared against zero-shot baseline (non-CoT multiple-choice) and few-shot exemplars.",
            "performance": "Examples reported: GPT-4 (CoT) — Desire exact match 63.29%, Belief exact match 58.18%, Intention Micro F1 34.90%, Macro F1 31.26%. ChatGPT (CoT) — Desire 28.45%, Belief 21.00%, Intention Micro F1 36.71%, Macro F1 30.79%. Claude-v2.1 (CoT) — Desire 50.13%, Belief 40.52%, Intention Micro F1 39.93%, Macro F1 35.67%.",
            "performance_comparison": "GPT-4 zero-shot (non-CoT) — Desire 62.77%, Belief 57.62%, Intention Micro F1 29.84% (CoT improved intention from 29.84→34.90); ChatGPT zero-shot — Desire 18.60%→CoT 28.45% (≈+9.85), Belief 13.04%→21.00% (≈+7.96); LLaMa2-Chat(70B) zero-shot 24.40%→CoT 30.34% (Desire) and Belief 21.58%→24.23%.",
            "format_effect_size": "Varies by model; examples: ChatGPT Desire +9.85 pct, Belief +7.96 pct; GPT-4 Intention Micro F1 +5.06 pct; LLaMa2-Chat(70B) Desire +5.94 pct.",
            "format_effect_direction": "improved",
            "explanation_or_hypothesis": "Authors report that CoT generally improves performance because it forces the model to reason over conversation context (reducing heuristic guessing such as inventing a 'medium' preference when not provided). CoT reduces specific error types that stem from superficial pattern matching and encourages stepwise deduction from dialogue history.",
            "counterexample_or_null_result": "Improvements are model-dependent: GPT-4 showed only small gains on desire/belief but larger gains on intention; some few-shot setups (see next entry) outperformed CoT on intention for GPT-4 by a small margin, indicating CoT is not uniformly superior for every metric/model.",
            "uuid": "e5708.0",
            "source_info": {
                "paper_title": "NegotiationToM: A Benchmark for Stress-testing Machine Theory of Mind on Negotiation Surrounding",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Few-shot prompting",
            "name_full": "Few-shot prompting with input–output exemplars",
            "brief_description": "A prompting format that prepends several human-authored exemplars (4 for desire/belief, 7 for intentions) to the task prompt to teach the model desired label mapping.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "multiple (GPT-4, ChatGPT, Claude-v1.3, Claude-v2.1, Llama-2 Chat 13B/70B)",
            "model_size": null,
            "task_name": "NegotiationToM (desire/belief/intention classification)",
            "task_description": "Same NegotiationToM evaluation; few-shot probes whether exemplars improve classification over zero-shot and CoT.",
            "problem_format": "Few-shot: task instruction followed by N labeled exemplars (&lt;EX1-INP&gt;&lt;EX1-OUT&gt; ...), then the new input to be labeled (N=4 for desire/belief, N=7 for intention).",
            "comparison_format": "Compared against zero-shot baseline and zero-shot Chain-of-Thought.",
            "performance": "Selected examples from paper: GPT-4 few-shot — Desire 62.89%, Belief 52.08%, Intention Micro F1 35.10%, Macro F1 33.21% (few-shot intention slightly &gt; CoT: 35.10% vs CoT 34.90% Micro). ChatGPT few-shot — Desire 19.24%, Belief 17.02%. Claude-v2.1 few-shot — Desire 48.77%, Belief 41.88%, Intention Micro F1 38.23%.",
            "performance_comparison": "For many models few-shot improved over zero-shot baseline but was typically worse than CoT for desire/belief; for GPT-4 intention, few-shot slightly outperformed CoT (Intention Micro F1 35.10% vs CoT 34.90%).",
            "format_effect_size": "Model-dependent. Example: GPT-4 Intention Micro F1: few-shot +0.20 pct vs CoT; ChatGPT few-shot often underperformed CoT by ~8–10 pct on desire/belief.",
            "format_effect_direction": "mixed (usually improved over plain zero-shot; sometimes inferior to CoT; small improvement for GPT-4 on intention)",
            "explanation_or_hypothesis": "Authors note few-shot exemplars help LLMs learn label semantics but may not encourage explicit stepwise reasoning; exemplars can improve some tasks (especially multi-label intention when examples cover labels) but CoT remains effective for reasoning-heavy aspects.",
            "counterexample_or_null_result": "Few-shot was not always better than CoT: most models had higher scores with CoT than with few-shot on desire/belief; only some intention metrics (GPT-4) saw few-shot slightly outperform CoT.",
            "uuid": "e5708.1",
            "source_info": {
                "paper_title": "NegotiationToM: A Benchmark for Stress-testing Machine Theory of Mind on Negotiation Surrounding",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Zero-shot multiple-choice baseline",
            "name_full": "Zero-shot multiple-choice prompt template (Robinson & Wingate style)",
            "brief_description": "Baseline zero-shot prompting that formulates the task as a multiple-choice question without exemplars or CoT; used as the baseline for NegotiationToM evaluations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "multiple (GPT-4, ChatGPT, Claude-v1.3, Claude-v2.1, Llama-2 Chat 13B/70B)",
            "model_size": null,
            "task_name": "NegotiationToM (desire/belief/intention)",
            "task_description": "Multiple-choice classification of high/medium/low preferences and multi-label intentions from dialogue snippets.",
            "problem_format": "Simple zero-shot multiple-choice Q&A prompt that asks the model to pick one of options (A/B/C/D) or select one/multiple intention labels without step-by-step chain-of-thought.",
            "comparison_format": "Compared to CoT and few-shot variants.",
            "performance": "Examples: GPT-4 zero-shot (combined Q format) — Desire 62.77%, Belief 57.62%, Intention Micro F1 29.84%. ChatGPT zero-shot (combined) — Desire 18.60%, Belief 13.04%. LLaMa2-Chat(70B) zero-shot — Desire 24.40%, Belief 21.58%.",
            "performance_comparison": "Compared to CoT and few-shot: generally lower than CoT; few-shot sometimes improved over zero-shot but usually did not exceed CoT.",
            "format_effect_size": "CoT often provided double-digit improvements over zero-shot for weaker models (e.g., ChatGPT Desire increased ≈+9.85 pct), while GPT-4 showed marginal zero-shot→CoT gains on desire/belief but larger gains on intention.",
            "format_effect_direction": "improved when replaced by CoT or few-shot (i.e., baseline was weaker)",
            "explanation_or_hypothesis": "Zero-shot multiple-choice can be sensitive to prompt phrasing and suffers when models rely on shallow heuristics; structured reasoning prompts (CoT) or exemplars reduce such heuristics.",
            "counterexample_or_null_result": null,
            "uuid": "e5708.2",
            "source_info": {
                "paper_title": "NegotiationToM: A Benchmark for Stress-testing Machine Theory of Mind on Negotiation Surrounding",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Question format: ranking / individual / combined",
            "name_full": "Question-format variants for desire and belief (ranking, individual, combined)",
            "brief_description": "Three prompt styles for asking preference questions: (a) ranking (single question asks for ordered trio), (b) individual (three separate questions for high/medium/low), (c) combined (single prompt that contains three constrained questions together).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ChatGPT (gpt-3.5-turbo-0613), GPT-4 (gpt-4-0613)",
            "model_size": null,
            "task_name": "NegotiationToM (desire and belief preference questions)",
            "task_description": "Assess how different ways of asking the same preference information affects model ability to produce correct high/medium/low labels.",
            "problem_format": "Ranking: ask for a single ordered triple (high→medium→low). Individual: ask three separate MC questions for high, medium, low. Combined: baseline prompt that includes all three preference questions in one combined template (constrains answers).",
            "comparison_format": "Ranking vs Individual vs Combined (reported per-model).",
            "performance": "Reported zero-shot results (Table 5): ChatGPT — ranking Desire 2.88% / Belief 9.24%; individual Desire 9.18% / Belief 11.7%; combined Desire 18.60% / Belief 13.04%. GPT-4 — ranking Desire 20.10% / Belief 16.80%; individual Desire 40.01% / Belief 36.88%; combined Desire 62.77% / Belief 57.62%.",
            "performance_comparison": "Combined format consistently outperformed ranking and individual formats for both ChatGPT and GPT-4; for GPT-4 combined vs individual: Desire +22.76 pct, Belief +20.74 pct.",
            "format_effect_size": "Large: GPT-4 combined vs individual: Desire +22.76 pct (62.77 vs 40.01), Belief +20.74 pct (57.62 vs 36.88). ChatGPT combined vs ranking: Desire +15.72 pct (18.60 vs 2.88).",
            "format_effect_direction": "combined &gt; individual &gt; ranking (improved)",
            "explanation_or_hypothesis": "Authors hypothesize combined format imposes constraints that prevent implausible/incoherent partial answers (e.g., models hallucinating a medium preference when none is present) and reduces the need for models to 'combine' reasoning across separate answers; individual format forces models to perform multiple different reasoning types and increases errors in deducing absent information (especially medium preference).",
            "counterexample_or_null_result": "None reported for tested models (combined always performed best in reported comparisons).",
            "uuid": "e5708.3",
            "source_info": {
                "paper_title": "NegotiationToM: A Benchmark for Stress-testing Machine Theory of Mind on Negotiation Surrounding",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Augmenting prompts with mental-state context (Desire+Belief)",
            "name_full": "Prompt augmentation with annotated mental-state signals (desire and belief) for strategy prediction",
            "brief_description": "Incorporating model- or human-annotated desire and belief labels into prompts for a downstream negotiation strategy prediction task, to test whether explicit ToM information helps performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4 (gpt-4-0613), ChatGPT (gpt-3.5-turbo-0613), Claude-v1.3, Claude-v2.1, LLaMa2-Chat (13B/70B)",
            "model_size": null,
            "task_name": "CaSiNo negotiation strategy prediction (downstream task)",
            "task_description": "Predict negotiation strategy labels (intention subclasses) for utterances; evaluated with micro and macro F1.",
            "problem_format": "Baseline strategy-prediction prompt vs enriched prompt that appends desire and belief (annotated ToM states) for the two agents into the prompt before asking for strategy labels.",
            "comparison_format": "Baseline (no ToM context) vs with Desire+Belief (w B.D.).",
            "performance": "Examples: Claude-v2.1 baseline Intention Micro F1 27.12% → with Desire+Belief 34.56% (micro +7.44%, macro +5.54%). GPT-4 baseline 26.31% → with B.D. 32.71% (micro +6.4%). Other models showed similar gains (e.g., ChatGPT baseline 23.42% → with B.D. 28.99%).",
            "performance_comparison": "Consistent improvements across models when adding desire+belief: Claude-v2.1 micro +7.44% (27.12→34.56), GPT-4 micro +6.40% (26.31→32.71), ChatGPT micro +5.57% (23.42→28.99).",
            "format_effect_size": "Approximately +5–7.5 percentage points in Intention Micro F1 for reported top models.",
            "format_effect_direction": "improved",
            "explanation_or_hypothesis": "Authors argue explicit ToM signals (desire & belief) provide useful structured context that helps the model disambiguate likely negotiation strategies, showing that inferring/feeding mental-state information improves downstream decision-making/prediction.",
            "counterexample_or_null_result": null,
            "uuid": "e5708.4",
            "source_info": {
                "paper_title": "NegotiationToM: A Benchmark for Stress-testing Machine Theory of Mind on Negotiation Surrounding",
                "publication_date_yy_mm": "2024-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Large language models are zero-shot reasoners",
            "rating": 2,
            "sanitized_title": "large_language_models_are_zeroshot_reasoners"
        },
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 1,
            "sanitized_title": "chain_of_thought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Leveraging large language models for multiple choice question answering",
            "rating": 2,
            "sanitized_title": "leveraging_large_language_models_for_multiple_choice_question_answering"
        },
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 2,
            "sanitized_title": "language_models_are_fewshot_learners"
        },
        {
            "paper_title": "Casino: A corpus of campsite negotiation dialogues for automatic negotiation systems",
            "rating": 2,
            "sanitized_title": "casino_a_corpus_of_campsite_negotiation_dialogues_for_automatic_negotiation_systems"
        },
        {
            "paper_title": "BigToM: A framework for designing Theory-of-Mind benchmarks",
            "rating": 1,
            "sanitized_title": "bigtom_a_framework_for_designing_theoryofmind_benchmarks"
        }
    ],
    "cost": 0.017009749999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>NegotiationToM: A Benchmark for Stress-testing Machine Theory of Mind on Negotiation Surrounding
5 Oct 2024</p>
<p>Chunkit Chan ckchancc@cse.ust.hk 
♠ Cheng 
The Hong Kong University of Science and Technology † Tencent AI Lab
Seattle</p>
<p>Yauwai Yim 
The Hong Kong University of Science and Technology † Tencent AI Lab
Seattle</p>
<p>Zheye Deng 
The Hong Kong University of Science and Technology † Tencent AI Lab
Seattle</p>
<p>Wei Fan 
The Hong Kong University of Science and Technology † Tencent AI Lab
Seattle</p>
<p>Haoran Li 
The Hong Kong University of Science and Technology † Tencent AI Lab
Seattle</p>
<p>♠ Xin Liu 
The Hong Kong University of Science and Technology † Tencent AI Lab
Seattle</p>
<p>Hongming Zhang 
Weiqi Wang 
The Hong Kong University of Science and Technology † Tencent AI Lab
Seattle</p>
<p>♠ Yangqiu 
The Hong Kong University of Science and Technology † Tencent AI Lab
Seattle</p>
<p>Melanie Sclar 
Sachin Kumar 
Peter West 
Alane Suhr 
James Wa Strachan 
Dalila Albergo 
Giulia Borghini 
Oriana Pansardi 
Eugenio Scaliti 
Saurabh Gupta 
Krati Saxena 
Alessandro Rufo 
Stefano Panzeri 
Rohan Taori 
Ishaan Gulrajani 
Tianyi Zhang 
Yann Dubois 
Xuechen Li 
Carlos Guestrin 
Percy Liang 
Tatsunori B Hashimoto 
Stanford 
Hugo Touvron 
Louis Martin 
Kevin Stone 
Peter Al- Bert 
Amjad Almahairi 
Yasmine Babaei 
Nikolay Bashlykov 
Soumya Batra 
Prajjwal Bhargava 
Shruti Bhosale 
Dan Bikel 
Lukas Blecher 
Cristian Canton- Ferrer 
The Hong Kong University of Science and Technology † Tencent AI Lab
Seattle</p>
<p>Moya Chen 
Guillem Cucurull 
David Esiobu 
Jude Fernandes 
Jeremy Fu 
Wenyin Fu 
Brian Fuller 
Cynthia Gao 
Vedanuj Goswami 
Naman Goyal 
An- Thony Hartshorn 
Saghar Hosseini 
Rui Hou 
Hakan Inan 
Marcin Kardas 
Viktor Kerkez 
Madian Khabsa 
Isabel Kloumann 
PunitArtem Korenev 
Singh Koura 
Marie-Anne Lachaux 
Thibaut Lavril 
Jenya Lee 
Di- Ana Liskovich 
Yinghai Lu 
Yuning Mao 
Xavier Mar- Tinet 
Todor Mihaylov 
Pushkar Mishra 
Igor Moly- Bog 
Yixin Nie 
Andrew Poulton 
Jeremy Reizen- Stein 
Rashi Rungta 
Kalyan Saladi </p>
<p>Alan Schelten
Ruan Silva</p>
<p>Eric Michael Smith
Ranjan Subrama-nian
Ross Tay-lor, Adina WilliamsXiaoqing Ellen Tan, Binh Tang</p>
<p>Jian Xiang Kuan
Puxin XuZheng Yan</p>
<p>Iliyan Zarov
Angela Fan, Melanie Kambadur, Sharan NarangYuchen Zhang</p>
<p>Aurélien Ro-driguez
Sergey EdunovRobert Stojnic</p>
<p>Thomas Scialom
2023</p>
<p>Cunxiang Wang
Tianhang Zhang, Jiayang Cheng, Yunzhi Yao, Wenyang Gao, Xuming Hu, Zehan Qi, Yidong Wang, Jindong Wang, Xing XieXiaoze Liu, Yuanhao Yue, Xiangru Tang, Linyi Yang, Zheng Zhang</p>
<p>SeattleWAUnited States</p>
<p>NegotiationToM: A Benchmark for Stress-testing Machine Theory of Mind on Negotiation Surrounding
5 Oct 20248E26705A2C9288437A1975A724C10E1BarXiv:2404.13627v3[cs.CL]
Large Language Models (LLMs) have sparked substantial interest and debate concerning their potential emergence of Theory of Mind (ToM) ability.Theory of mind evaluations currently focuses on testing models using machinegenerated data or game settings prone to shortcuts and spurious correlations, which lacks evaluation of machine ToM ability in real-world human interaction scenarios.This poses a pressing demand to develop new real-world scenario benchmarks.We introduce NegotiationToM 1 , a new benchmark designed to stress-test machine ToM in real-world negotiation surrounding covered multi-dimensional mental states (i.e., desires, beliefs, and intentions).Our benchmark builds upon the Belief-Desire-Intention (BDI) agent modeling theory and conducts the necessary empirical experiments to evaluate large language models.Our findings demonstrate that NegotiationToM is challenging for stateof-the-art LLMs, as they consistently perform significantly worse than humans, even when employing the chain-of-thought (CoT) method.</p>
<p>Introduction</p>
<p>Theory of Mind (ToM) was introduced as an agent's capacity to infer the mental states of others, such as desires, beliefs, and intentions (Premack and Woodruff, 1978;Ma et al., 2023).Numerous scenarios involving human cognition and social reasoning rely on the ToM modeling of others' mental states (Gopnik and Wellman, 1992;Baron-Cohen, 1997;Gunning, 2018), such as comprehending and forecasting others' actions (Dennett, 1988), planning over others' beliefs and subsequent actions (Favier et al., 2023), and various forms of reasoning and decision-making (Pereira et al., 2016;Rusch et al., 2020).Some previous research believes that LLMs already exhibit a high 1 The dataset is available at https://github.com/HKUST-KnowComp/NegotiationToM Agent 1: Hi there.. we're looking forward to this camping trip.. hope you are, too!Is there anything in particular you guys need /want out of the extra stockpile we've got?Agent 2: Hello!Which item do you need the most?Agent 1: Well, we actually really need all three items.. but I know you might, too.So, I want to make a deal that's fair for you.. is there anything in particular your family needs for the trip?Agent 2: Well, there has to be an item that you need the most.If you tell me what you need the most, we can try to make a deal around it.</p>
<p>Agent 1: Since the forest is nearby enough, I think we'd be more interested in ensuring there's enough food instead of firewood for my people, I think.</p>
<p>Agent 2: Hmm.I really need food too.I don't care much for water either.How about I take all 3 firewood, 2 food, and 1 water?Agent 1: Yeah, that's not going to work for me..I can give you two food if you really need it, but in exchange I'm going to need two of the water and all the firewood.Agent 2: I will just take 3 firewood, 1 food, and 1 water.That work for you?You're close to the forest, you won't need firewood.</p>
<p>Belief: No information about</p>
<p>Agent 1' preference item.</p>
<p>Belief: No information about Agent 2' preference item.</p>
<p>Belief: Agent 2' high preference is food; medium is firewood and low one is water.</p>
<p>Belief: Agent 1' high preference is food; medium is water and low one is firewood.</p>
<p>Belief: Agent 1' high preference is food; medium is water and low one is firewood.</p>
<p>Belief: Agent 2' high preference is firewood; medium is food and low one is water.</p>
<p>Belief: No information about</p>
<p>Agent 1' preference item.level of competence in addressing ToM tasks (Strachan et al., 2024;Bubeck et al., 2023;Kosinski, 2023), while other studies express doubt and develop benchmarks to illustrate that LLMs do not possess proficient ability in ToM tasks (Sap et al., 2022;Ullman, 2023;Shapira et al., 2024).However, these traditional evaluation benchmarks for language models are primarily theoretical game settings or synthetic template-based data generated by the large language model, which may inherently suffer from shortcuts and spurious correlations (Sclar et al., 2023;Ullman, 2023;Shapira et al., 2023a;Ma et al., 2023).Consequently, these benchmarks assess language models from a theoretical perspective, which may not precisely and effectively reflect the ToM capabilities of large language models in practical situations.</p>
<p>In reality, ToM ability plays a crucial role in comprehending dynamic social interactions (e.g., negotiation conversations) by forming an essential element of effective communication (Frith, 1994;Schober, 2005), and inferring other's mental states in a conversation requires machines as humans to comprehend text beyond surface forms of utterance and utilize the incomplete information presented in the conversation.ToM is closely related to interpersonal social intelligence (Ganaie and Mudasir, 2015;Stone, 2006;Williams et al., 2022;Sap et al., 2022), which allows us to navigate and understand social situations ranging from simple everyday interactions to complex negotiations (Yang et al., 2021;Kim et al., 2023;de Weerd et al., 2017;Gardner et al., 1995;de Weerd et al., 2013).</p>
<p>The negotiation dialogues contain complicated and diverse aspects of a realistic negotiation, such as rapport building, discussing preferences, exchanging offers, emotional expression, and persuasion with personal and logical arguments (Chawla et al., 2021).In a realistic negotiation, humans innately infer the mental states of the other party and proceed with their subsequent actions based on their own beliefs and desires.For example, in Figure 1, two agents negotiate for food, water, and firewood packages for their upcoming trip.Initially, agent 1 lacks any information pertaining to the preference order of agent 2. Thus, agent 1's belief is "no information on the agent 2 preference item", and agent 1 intends to elicit the item preference order from agent 2 to guide further action based on their own belief.Furthermore, belief is commonly employed to denote an individual's cognitive stance or acceptance of something as true or holding it to be the case (Turiel, 2008).This belief may undergo changes during the negotiation as perceiving more available information behind the conversation.In the fourth round of conversation depicted in Figure 1, agent 1's dynamic belief changed from "Agent 2' high preference is food, medium preference is firewood, and the low one is water" to "Agent 2's high preference is firewood, medium preference is food, and the low one is water."Therefore, negotiation serves as an ideal scenario to assess the theory of mind ability of language learning models in the real world due to its complexity and the linguistic diversity inherent in negotiation conversations.</p>
<p>In this work, we introduce NegotiationToM, a natural conversational benchmark for stress-testing machine ToM in real-world negotiation surround-ings involving multi-dimensional mental states (i.e., desires, beliefs, and intentions), inspired by the Belief-Desire-Intention (BDI) agent model proposed by Bratman (1987).The goal of Negotiation-ToM is to effectively measure how well large language models (LLMs) can track the mental states of negotiation participants in conversations and evaluate LLMs' capability for a coherent understanding of others' mental states in the conversation context where there are gradually increasing rounds of utterance (i.e., increase available information).We hope our benchmark and experimental results in the real-world scenario complement the prior theoretical works, which yield important insights into the intensive debate around ToM (Whang, 2023) in LLMs.Our contributions are summarized as follows:</p>
<p>• To the best of our knowledge, NegotiationToM is the first human-annotated natural conversational benchmark to introduce negotiation theory of mind evaluation for large language models in realistic negotiations.</p>
<p>• Our benchmark covered multi-dimensional mental states (i.e., desires, beliefs, and intentions) to assess how well large language models can track the mental states of negotiation participants in conversations and coherent understanding of others' mental states with increased available and accessible information.</p>
<p>• We undertake the necessary empirical experiments to evaluate large language models (LLMs) on the NegotiationToM benchmark and conduct extensive in-depth analysis to explore the LLMs' empirical performance under various settings.</p>
<p>Related Work</p>
<p>Theory of Mind Benchmarks The existing ToM evaluation benchmarks for large language models are primarily synthetic template-based data generated (Kim et al., 2023;Gandhi et al., 2023) or derived from the Sally-Anne False Belief Test (Baron-Cohen et al., 1985;Nematzadeh et al., 2018;Grant et al., 2017;Le et al., 2019a;Zhou et al., 2023), which assesses model ability from a theoretical perspective and may inherently suffer from shortcuts and spurious correlations (Sclar et al., 2023;Ullman, 2023;Shapira et al., 2023a;Ma et al., 2023).</p>
<p>Other works, such as Shapira et al. (2023b) build benchmarks based on the Faux Pas Test (Baron-Cohen et al., 1999).The most related work to ours is the BigToM benchmark proposed by (Gandhi et al., 2023), which presents a framework for designing a ToM benchmark from synthetic templates for evaluating different aspects of LLMs' ToM capabilities (e.g., desire and belief).However, this work and other theoretical benchmarks may not reflect the ToM capabilities of large language models in real-world scenarios.Moreover, most of these prior works are concentrated on the belief aspects of the Theory of Mind.Therefore, this work introduces NegotiationToM, which is a multi-category mental state benchmark in realistic negotiation scenarios.</p>
<p>Negotiation Negotiation is an expanding area of research in the natural language processing field, and Zhan et al. (2022) conducted an impressive survey of existing literature on dialogue systems for negotiation.Lewis et al. (2017) train recurrent neural networks to generate natural language dialogues in negotiations.He et al. (2018) proposed a modular generative model that is based on dialogue acts.Various disciplines have explored bilateral bargaining from diverse perspectives and employing different methodologies.Economic theory has examined the influence of incomplete information (Ausubel et al., 2002) and emphasized the significance of explicit communication (Crawford, 1990;Roth, 2020).Bazerman et al. (2000) and Pruitt (2013) present a comprehensive overview of the psychology research on negotiation.These previous studies generally neglect the content of communication, although there are a few noteworthy exceptions (Swaab et al., 2011;Jeong et al., 2019;Lee and Ames, 2017;He et al., 2018;Heddaya et al., 2023).One intriguing work by Yang et al. (2021) introduces a probabilistic formulation method to encapsulate the opponent's personality type during learning and inference, drawing inspiration from the idea of incorporating a theory of mind (ToM) into machines.However, distinct from this approach, our work presents a benchmark integrating a theory of mind (ToM) into the negotiation surroundings.</p>
<p>NegotiationToM</p>
<p>Theory of Mind (ToM) describes the ability as humans have to ascribe and infer the mental states of others, and to predict which likely actions they are going to take (Apperly, 2010).Therefore, it is critical to acquire negotiation strategies based on one's own desire and temporary belief built upon the in-formation presented in conversation.Nevertheless, understanding the Theory of Mind (ToM) inherent in a negotiation dialogue is challenging due to its intricate linguistic features and complex reasoning attributes.Therefore, some considerations have to be taken into account when constructing the Nego-tiationToM.</p>
<p>Design Considerations for NegotiationToM</p>
<p>There are several essential design considerations we go through when constructing the Negotiation-ToM.</p>
<p>(1) the scenario of the dataset should be grounded in a human-to-human real-world negotiation (e.g., real-world camping scenario).( 2) the dataset should be a natural conversational dataset instead of generated from a synthetic template to avoid reporting bias (Gordon and Durme, 2013) and shortcuts (Aru et al., 2023).</p>
<p>(3) the dataset should be equipped with abundant and diverse linguistic features and semantic context (e.g., negotiation argument) instead of bargaining on the numerical value or meaningless counter-offer (Lewis et al., 2017;He et al., 2018).( 4) the dataset should be ensured to mitigate the risk of potential contamination.</p>
<p>CaSiNo</p>
<p>Inspired by several considerations above, CaSiNo (Chawla et al., 2021) was employed as the source data and modified to construct Negotia-tionToM.CaSiNo is a bilateral human-to-human natural conversational dataset that covers rich linguistic features and many realistic aspects of negotiations, such as small talk, preference elicitation, emotional expression, and convincing strategies based on individual desire.In this dataset, the participants take the role of campsite neighbors and negotiate for food, water, and firewood packages for their upcoming trip.For each conversation, participants discuss individual needs by making various convincing arguments from their camping experiences, such as Personal Care, Recreational, Group Needs, or Emergency Requirements.One example of Group Needs is "I need more firewood due to having several people join on the trip and needing a bigger fire overall."We illustrate some of these arguments in</p>
<p>Promote-Coordination</p>
<p>(Firewood, Food, Water) Participant 2' high preference is firewood; medium is food and low one is water.</p>
<p>(Food, Water, Firewood) Participant 1' high preference is food; medium is water and low one is firewood.P2: I will just take 3 firewood, 1 food, and 1 water.That work for you?You're close to the forest, you won't need firewood.</p>
<p>No-Intention</p>
<p>(Food, Water, Firewood) Participant 1' high preference is food; medium is water and low one is firewood.</p>
<p>(Firewood, Food, Water) Participant 2' high preference is firewood; medium is food and low one is water.... Furthermore, we present the verification method employed to alleviate the risk of potential contamination within the CaSiNo dataset and demonstrate that this dataset is unlikely to encounter the contamination issue, as detailed in Appendix A.1.</p>
<p>Theory of Mind in NegotiationToM</p>
<p>In NegotiationToM, as shown in Table 1, it is fundamentally a desire-matching scenario surrounding the item preference order that requires two participants to directly or indirectly align their preference order of item (desire) and adopt corresponding strategies to strive for more high-preference items based on the holding belief (i.e., the assumption of their opponent's item preference order according to the information received in the conversation).Therefore, inspired by the Belief-Desire-Intention (BDI) agent modeling method (Bratman, 1987), three mental states (i.e., desire, belief, and intention) were employed to evaluate the LLMs' performance in NegotiationToM.All questions about these three mental states are displayed in Table 1.</p>
<p>Desire.Desires are motivational states that do not necessarily imply commitment, though they usually affect actions (Malle and Knobe, 2001;Ka-vanagh et al., 2005).Unlike beliefs, desires are neither right nor wrong; they are fulfilled or unfulfilled (Searle, 1983).In NegotiationToM scenarios, the desire of the participants is the need for their item preference order, whether they are satisfied or not during the negotiation, and their desire order is the preference order of items.Hence, we create a desire question to assess whether the large language model comprehends the desire order of negotiation participants behind each round dialogue with previous conversation history.There are two types of desire order, and one is the global desire order inherently assigned to each participant before the beginning of the negotiation in CaSiNo.Another one is local desire order, which focuses on the local item preference order information behind each round of dialogues and previous conversation history, illustrated in Table 1.This local desire order is utilized to form desire questions in Negotiation-ToM.</p>
<p>Belief.Belief refers to a mental state in which an individual assumes a specific stance, attitude, or opinion toward a proposition.In contemporary discussions within the field of philosophy of mind, the term "belief" is commonly employed to  denote an individual's cognitive stance or acceptance of something as true or holding it to be the case (Turiel, 2008).Note that this notion of belief does not inherently require active reflection, nor does it necessitate truthfulness (Armstrong, 1973;Moses, 1993).In NegotiationToM, understanding the state of the opponent's item preference order, which is explicitly or implicitly expressed in the conversation, is the main way to form the belief.Therefore, the belief question will query the LLMs on what one participant thinks of another participant's item preferences, given the current round of dialogue with previous conversation history.</p>
<p>Intention.Intention is a mental state formed through rational planning (i.e., negotiation strategy in a negotiation scenario) toward a goal based on the desires and beliefs of the agent.Intentions have been extensively explored in psychology tests, e.g., action prediction (Malle and Knobe, 2001) and intention attribution to abstract figures (Castelli, 2006).Normally, a negotiation strategy is highly associated with corresponding concrete intentions (Belmondo and Sargis-Roussel, 2015).Thus, in NegotiationToM, we collect the annotated negotiation strategies from the CaSiNo dataset and map the intentions according to the definition of various strategies.The mapping table is shown in Table 2, and the strategy definition and examples are illustrated in Table 20 and 21 in Appendix A.3.Within our framework, as both Self-Need and Other-Need are associated with "Intents to describe a need for an item" intention, we combine these two strategies into one intention class.</p>
<p>Annotation &amp; Statistics</p>
<p>Source data.NegotiationToM is annotated based on a multi-turn negotiation dialogue corpus, the CaSiNo (Chawla et al., 2021) dataset.Each instance in CaSiNo is an N -round alternating dialogue
D N = [u a 1 , u b 1 , u a 2 , u b 2 , • • • , u a N , u b N ]
between two participants, a and b2 .They take on the roles of campsite neighbors and negotiate for food, water, and firewood packages for their upcoming trip.We adopt the subset with strategy annotations and undertake the annotation on the desire and belief states behind each utterance.</p>
<p>Curating NegotiationToM.The intention state of both participants has already been introduced and mapped from the strategy annotations in CaSiNo.We conduct an expert annotation to annotate the beliefs and desires of the two participants in each dialogue (i.e., the perceived preference ranking among food, water, and firewood).We recruited five workers who were graduate students in Englishspeaking universities to conduct the annotation.For each dialogue D N , let D k be the truncated dialogue until round k:
D k = [u a 1 , u b 1 , • • • , u a k , u b k ].
Then, we ask the workers to annotate the perceived preference ranking for both participants a and b for truncated dialogue D k (k ∈ {1, 2, • • • , N }) given all k rounds of historical dialogue.To ensure the annotation quality, we evaluate the workers during the first 100 rounds of conversations and explain their typical errors to them in detail.More details of the annotation process are in Appendix A.2.Although annotating NegotiationToM requires understanding complex dialogues in CaSiNo, we observed high inter-annotator agreement.The Fleiss's κ is 79.03% (Fleiss, 1971) for NegotiationToM benchmark, the breakdown computation of κ are shown in Table 3.</p>
<p>Statistics.NegotiationToM contains 395 dialogues with 2,380 rounds of conversations (truncated dialogues) and 4,618 utterances.Each utterance has seven questions and annotated labels, including three designed sub-questions for both belief and desire states (i.e., high/medium/low preference items) and one tailored question for inten-tion.There are a total of 13.8 thousand questions, and the detailed statistics and comparison with contemporary ToM datasets are shown in Table 19 (Kim et al., 2023;Gandhi et al., 2023;Shapira et al., 2023b), we test these models with two types of prompts: (1) one is zero-shot prompting and we utilize the prompt template in Robinson and Wingate (2023) to formulate the task as a multiple choice question answering problem as a baseline.</p>
<p>(2) another one is the Chain-of-Thought (CoT) prompting method by following (Kojima et al., 2022) and using the prompt "let's think step by step."Apart from these two settings, we also assess the LLMs' performance by using the fewshot setting to validate whether LLMs can improve their performance with input-output exemplars.We concatenate four tailored exemplars for desire and belief states and seven designed exemplars for intentions, covering all the input-output exemplars' labels.More configuration details can be found in Appendix B.2, and the prompt template refers to Appendix B.4.To measure the specific performance gap between humans and the state-of-the-art machine on the NegotiationToM, we employ three graduate students in computer science to complete the human evaluation task.More details of human evaluation are shown in Appendix C.2.</p>
<p>Metrics</p>
<p>We report the exact match percentages of all three high, medium, and low preferences for desire and belief classification, and only both of these three preferences that answer correctly count toward correct.The micro F1 score and macro F1 score are reported for the multi-label intention classification by following prior works (Hou et al., 2021;Wu et al., 2021;Moghe et al., 2023;Vulic et al., 2022).</p>
<p>3 https://www.anthropic.com/news/introducing-claude 4 https://www.anthropic.com/news/claude-2-1Moreover, we report the "All" score, which requires the models to answer correctly all three ToM question types, which include desire, belief, and intention for the same information piece in the conversation.Furthermore, we report the consistency score, which requires the models to answer ToM questions correctly for the whole negotiation conversation.This metric aims to measure how well the models show consistent understanding and track the agent's mental state change throughout the whole conversation.</p>
<p>Experimental Result</p>
<p>Main Result</p>
<p>Table 4 summarizes the main results of the state-ofthe-art large language models in NegotiationToM, from which we derive the following conclusions.First, the performance of all models is significantly worse than human performance, even after employing the zero-shot chain-of-thought (CoT) method.There is a significant performance gap between machines and humans in the ToM negotiation evaluation scenario.Specifically, compared with the best state-of-the-art models in each mental state, the performance gap is 27.85% in desire, 32.96% in belief, 43.82% in Micro F1 score, and 48.98% in Macro F1 score in intention.Second, GPT-4 0613 (CoT) achieves the best performance among all the models regarding inferring desire and belief, while Claude-v2.1 (CoT) outperforms all other models in the intention classification task in NegotiationToM.Third, we observe that most models received an improvement in scores when the chain-of-thought (CoT) method was applied.Nevertheless, there are still significant score gaps compared to human performance.</p>
<p>Few-Shot Performance Although the few-shot setting is not a common practice, we also include it to see whether it performs better than the other two settings.The results illustrated in Table 4 display that the few-shot setting of most models gains an improvement over the zero-shot setting but is worse than the CoT prompting method in Negotia-tionToM.Interestingly, some models (e.g., GPT-4) with few-shot exemplars received a better performance than the CoT method in intention states.</p>
<p>Large Language Model on All score</p>
<p>To fully assess the ToM capability of large language models to understand other's mental states in each round of dialogues, we report the "All" score in Table 4.This metric required the machine equipped with various ToM abilities to correctly answer three mental states (i.e., desire, belief, and intention) under the same information piece in the conversation.The Claude-v2.1 (CoT) outperforms all other large language models and receives a 3.68% in this all metric.It may be attributed to the exceptional intention ToM ability of Claude-v2.1 (CoT), but it also obtains a relatively high performance on the desire and belief aspects of ToM.However, it is worth mentioning that the performance of the machine on the all metric is far away from human performance, which is 43.78%.</p>
<p>How Well Large Language Model on Tracking Mental States Change in Conversation</p>
<p>To assess how well large language models can track the mental states of negotiation participants in conversations and coherent understanding of others' mental states with increased available information.Thus, it is crucial to evaluate the consistency and faithfulness of the large language model for the conversation context of the whole theory of mindbased dialogue.The consistency score is presented in  LLMs still lack of ability to track the mental state change during the conversation.It is noted that all models received zero consistency scores in the intention aspect, as the intention mental state owned a multi-label in an utterance and imposed difficulties to generate exact match labels in the whole conversation.</p>
<p>The Effect of Question Format</p>
<p>With the performance of LLMs varying significantly due to the sensitivity of prompt templates (Webson and Pavlick, 2022), we assessed the performance of two state-of-the-art models, ChatGPT and GPT-4, to study the effect of various question formats on their performance.In this experiment, we adopt three types of question formats, including ranking format, individual format, and combined format for desire and belief mental state.The combined format is the baseline prompt template adopted in our main experiment, which combines all three questions regarding the The performance shown in Table 5 demonstrates that the question format indeed affects the LLMs' performance, and the combined format performs better than other formats.It may result from the combined format imposing the constraint for LLMs to avoid answering some unreasonable and implausible response.After the case study on error cases from the GPT-4 with individual form, we find that it is more challenging for models to combine with different types of reasoning while conducting the theory of mind reasoning.For example, when the GPT-4 may correctly answer that the agent's highest preference item is water and the lowest one is food, the model may randomly answer medium preference as there is no information of medium preference provided in conversation.Models cannot answer the medium preference of firewood (there are only three items) because they cannot effectively adopt deductive reasoning ability when performing theory of mind reasoning.Other models (e.g., ChatGPT) also suffer from this issue more seriously, although the combined format slightly mitigates this issue to some extent.</p>
<p>CaSiNo Negotiation Strategy Prediction</p>
<p>To validate the significance of our annotated desire and belief states, we append the information from these two states into the prompt template and assess whether it enhances the model performance on the negotiation strategy prediction task from  17 and 18 in Appendix B.4.The micro F1 score and macro F1 were employed as this task metric, and the result is reported in Table 6.All models incorporating the information from these two mental states received a significant improvement over the baselines.Specifically, by integrating the signals from desire and belief, the Claude-v2.1 model obtained a 7.44% gain in the micro F1 score and a 5.54% gain in the macro F1 score.It demonstrates that the effectiveness of our annotated theory of mind states (i.e., desire and belief) helps LLMs to infer the negotiation strategy behind each utterance.For example, with the understanding that agent 1's preference order of items is Not Given, Not Given, and Not Given, and agent 2's preference order of items is Firewood, Not Given, and Not Given.Agent 2 may take the elicit-preference strategy to elicit the preference order of agent 1 for further negotiation.</p>
<p>What Types of Error LLMs Make</p>
<p>To understand the type of error LLMs make on the NegotiationToM benchmark, we sampled 1,000 LLMs' responses and counted the error categories among them.randomly rather than answer "Not Given" when there is insufficient information to determine the preferred items.With the CoT method adopted, this error will be decreased for most models as LLM conducted reasoning on the conversation context and tried to explain and respond to a reasonable answer.</p>
<p>Types of Error</p>
<p>Types of Error LLMs make on Intention State</p>
<p>In terms of intention state, all models without and with CoT tend to select more intention choices, resulting in a high error rate in the "including incorrect intentions" and "did both" (i.e., include incorrect intentions and excluded correct intentions) error types.Another finding is that LLaMa2 series models respond to many irrelevant responses, such as repeating the questions, and do not raise any relevant answers.</p>
<p>Label-wise for Intentions State</p>
<p>To further explore the LLMs' performance on nine intention subclasss in NegotiationToM, Figure 5 illustrates the F1 scores (%) for each subclass.The results indicate that LLMs exhibit strong performance in predicting Build-Rapport and Describe-Need intentions, while their performance in predicting "undermine-requirements" and "No-Intention" is poor.Notably, Claude-v2.1 (CoT) outperforms other models in more than half of the subclass in intentions, demonstrating its proficiency in inferring others' intentions.For a detailed subclass result covering all models, please refer to Figure 6 and</p>
<p>Limitations</p>
<p>Passive benchmark to evaluate the ability of LLMs Although our benchmark is to stress-test machine ToM ability in negotiation surrounding compassed multi-categories of mental states, Ne-gotiationToM, and existing prior benchmarks are passive benchmarks that primarily adopt a passive observer role to test language agents (Ma et al., 2023).These benchmarks passively assess the ToM ability of LLMs and lack active interaction with and engagement between the agent and other entities involved in the situated environment.The active ToM benchmark should treat the language model as an active agent that perceives the physical and social context, reasons about others' mental states, communicates with other agents, and interacts with the environment to complete predefined tasks.The future work of this paper will employ the language model to act as an agent to actively interact with other model agents by using the Belief-Desire-Intention agent modeling method to generate a rational negotiation strategy.For example, based on the information of desire, belief, and intention, the language model agent will actively acquire a negotiation strategy arguing more benefits or enhancing the cooperation.</p>
<p>A Appendix for NegotiationToM</p>
<p>A.1 Verification of Potential Contamination in CaSiNo</p>
<p>Most of the existing available benchmarks in the NLP field were released prior to the initiation of the LLM training process, indicating that these datasets are likely to have been utilized during the pre-training phase and post-training phase (i.e., SFT (Ouyang et al., 2022) or RLHF (Christiano et al., 2017)) of LLMs (Golchin and Surdeanu, 2024;Li and Flanigan, 2024).Therefore, we follow the method proposed by Golchin and Surdeanu (2024) to assess the potential contamination issues in the CaSiNo dataset.We sample 100 instances from the CaSiNo dataset and prompting the Chat-GPT and GPT-4 to generate the likely next dialogue.</p>
<p>After human validation of these 100 instances, none of these generated dialogues corresponded to the original dataset.The prompting template and outputs are illustrated in Table 7. Furthermore, we also test these 100 instances by using the prompt template "Give a CaSiNO negotiation dialogue example and its answer for negotiation strategy."The outcome aligns with the prompting template depicted in Table 7, demonstrating that the risk of potential contamination is mitigated; however, no systematic approach can effectively address the contamination issue unless all training datasets utilized for LLMs are made publicly available.</p>
<p>A.2 NegotiationToM Annotation</p>
<p>In this section, we showcase our annotation instructions and templates used for annotation.The instructions are used to introduce the background of the negotiation conversation and instruct the workers to perform the annotation based on the dialogue history.The annotation instruction and annotation template are presented in Figure 7, 8, 9, and 10.The detailed statistics and comparison with contemporary ToM datasets are shown in Table 19.</p>
<p>A.3 Details for NegotiationToM Intentions</p>
<p>We provide a brief overview of Table 20, which presents the definitions of various ToM intentions in negotiation strategies.These definitions help us understand the intentions of the agents involved in the negotiation process.The table offers definitions for strategies such as Small-Talk, Empathy, Coordination, No-Need, Elicit-Pref, Undervalue-Partner, Vouch-Fairness, Self-Need, Other-Need, and Non-strategic.Each definition explains the specific meaning and context of the respective strategy in the negotiation process.By understanding these strategy definitions, we can better comprehend the negotiation interactions between agents and how the intention relates to desire and belief states during the negotiation process.</p>
<p>NegotiationToM Arguments Table 22 shows arguments for various items (i.e., Food, Water, Firewood) in four categories: Personal Care, Recreational, Group Needs, and Emergency.For example, participants may need more food for largersized teenage children, and more water for hydration or emergencies.The diversity of negotiation arguments raised by the human participants provided various scenarios for stress-testing LLMs by avoiding shortcuts and spurious correlation issues.</p>
<p>B Appendix for Experiments B.1 Baseline models</p>
<p>In this section, we introduce six recent instructiontuned large language models employed to stresstest the Negotiation.GPT models from OpenAI use a decoder-only transformer framework, and GPT-4 (OpenAI, 2023), ChatGPT (OpenAI, 2022) are proprietary models tested by calling the model API.Claude-v1.3 (Anthropic, 2023a) and Claude-v2.1 (Anthropic, 2023b) are closed-source LLMs developed by Anthropic.These two models can be accessible through a chat interface and API, and they demonstrate a strong performance in a lot of NLP tasks.Llama-2 Chat 13B (Touvron et al., 2023), andLlama-2 Chat 70B (Touvron et al., 2023) are a language model fine-tuned for engaging in dialogues that follow user inputs.</p>
<p>B.2 Hyperparameter</p>
<p>We use default hyperparameters for all the large language models mentioned in this paper.For ChatGPT (gpt-3.5-turbo-0613)and GPT-4 (gpt-4-0613), the default parameters5 are temperature=1 and top_p=1.For LLaMa2-Chat(13B) and LLaMa2-Chat(70B) models, we follow the default setting where temperature=0.5, top_p=0.9.For Claude-v1.3 and Claude-v2.1, the default parameters are temperature=0.5,top_p=1.</p>
<p>B.3 Few-Shot Experimental setting</p>
<p>Following Brown et al. (2020), we use a few-shot prompt to instruct all models:
<TASK-PROMPT> <EX 1 -INP><EX 1 -OUT> . . . <EX N −1 -INP><EX N −1 -OUT> <EX N -INP>
where <TASK-PROMPT> is a task instruction that explains the background of the negotiation scenario and <EX 1 -INP><EX 1 -OUT> are human-authored examples that try to cover all subclass labels to help LLMs understand the subclass labels.Finally, we provide the N th input as <EX N -INP> and ask all models to generate the corresponding answer as <EX N -OUT>.In this paper, we set N = 4 for the desire and belief states, and N = 7 for the intention state.</p>
<p>B.4 Appendix for Prompt Template</p>
<p>In this section, we introduce all prompt templates and these templates are presented in Tables 9, 10, 11, 12, 13, 14, 15, 16, 17, and 18.</p>
<p>Main Result Template Table 9 introduces the baseline prompt template, a straightforward Q&amp;A session without requiring detailed and complex explanations.Table 10 is the Chain-of-Thought prompt template that facilitates a detailed, stepby-step thought process in the LLM, thoroughly considering each problem aspect before arriving at a solution.Tables 11, 12, and 13 are presented the few-shot setting prompt template.Each prompt template appends some exemplars for LLMs to learn the new tasks.</p>
<p>Question Format Template The Ranking Question Format in Table 14 asks the LLM to prioritize or rank items, which is useful for tasks needing decision-making based on preference or importance.Table 15 and 16, the individual question format prompt templates, break down desire and belief questions into separate and focused questions.</p>
<p>Strategy Prediction Template Tables 17 and 18 present the prompt template for the strategy prediction task; the former provides a baseline format for predicting strategies from negotiation conversations, while the latter enriches this task by incorporating the desires and beliefs of the agents involved, offering a deeper contextual understanding and of-fer signals from desire and belief states for strategy classification.</p>
<p>B.5 Related Works for Large language Models</p>
<p>Recent studies have extensively and comprehensively evaluated instruction following LLMs' (Ope-nAI, 2023, 2022;Taori et al., 2023;Jiang et al., 2023) performance on numerous tasks, revealing its superior performance in zero-shot scenarios compared to other models (Bubeck et al., 2023;Bang et al., 2023;Chan et al., 2024;Cheng et al., 2023;Wang et al., 2024;Jiayang et al., 2024).However, there are certain obstacles that persist unaddressed, such as the inability to perform complex mathematical reasoning (Frieder et al., 2023), theory of mind reasoning (Lin et al., 2024), analogies reasoning (Cheng et al., 2023), text-to-table generation (Deng et al., 2024), fact validation (Wang et al., 2023), complex game setting (Yim et al., 2024), associated ethical implications, and privacy concerns (Li et al., 2023;Susnjak, 2022;Li et al., 2024b;Lukas et al., 2023;Li et al., 2024a).Therefore, it is critical to discuss whether large language models possess the capacity of the theory of mind as humans do.In this paper, we test several instruction-following LLMs in our Negotia-tionToM benchmark.The zero-shot performance of large language models, which relies on the sophisticated design of templates, has shown variance across various tasks (Ma et al., 2022;Chan et al., 2023a,b).To obtain replicable and representative results, we follow Robinson and Wingate (2023) to formulate the task as a multiple-choice questionanswering problem in this work.</p>
<p>C Appendix for Experimental Result C.1 Appendix for Intention Subclass Result</p>
<p>In this section, Table 6 and Figure 8 provide more detailed performance of the intention subclass level.All models receive a good performance on the Build-Rapport and Describe-Need subclass while poor at the No-Intention, No-Need, and Undermine-Requirements subclass.</p>
<p>C.2 Appendix for Human Performance</p>
<p>To measure the specific performance gap between humans and the state-of-the-art machine on the Ne-gotiationToM, we employ three graduate students in computer science to complete the human evaluation task.Each instance is shown to three workers who were prompted to choose the more plausible choice for questions regarding desire, belief, and intention.The questions and instructions are the same as the annotation template and are displayed in Appendix A.2.We compute the majority vote on the labels assigned, which leads to a performance of 91.14% on both the desire and belief, while the intention only obtained an 83.75% micro F1 score and 84.65% macro F1 score.The "All" score received 43.78%, and conversation consistency acquired 75.44% in both the desire and belief states.</p>
<p>Models</p>
<p>Example</p>
<p>ChatGPT</p>
<p>Instruction: You are provided with historical dialogue from the CaSiNO dataset.Finish Next dialogue as appeared in the dataset.Next dialogue must exactly match the instance in the dataset.agent 1: Hello!Let's work together on a deal for these packages, shall we?What are you most interested in? agent 2: Hey! I'd like some more firewood to keep my doggo warm.What do you need?agent 1: I need firewood as well.We have a large group consisting of mostly senior citizens, including my grandma, so we'd like the firewood to keep everyone warm.agent 2: I see.What are you least interested in? agent 1: We are least interested in the hiking equipment.Our group is not very active and prefers to stay indoors.</p>
<p>GPT-4</p>
<p>Instruction: You are provided with historical dialogue from the CaSiNO dataset.Finish Next dialogue as appeared in the dataset.Next dialogue must exactly match the instance in the dataset.Figure 6: The label-wise intention dimension results of large language models in NegotiationToM.
)6FRUH % X LO G 5 D S S R U W 6 K R Z ( P S D WK \ 3 U R P R WH &amp; R R U G LQ D WL R Q &amp; D OO R X W ) D LU Q H V V 8 Q G H U P LQ H 5 H T X LU H P H Q WV ' LV F R Y H U 3 U H I H U H Q F H ' H V F U LE H 1 H H G 1 R 1 H H G 1 R , Q WH Q WL R Q //</p>
<p>Example</p>
<p>Desire</p>
<p>Background: Here is a negotiation conversation for a camping trip.There are two agents who own some basic supplies and negotiate with each other to split the additional food packages, water bottles, and firewood to make their camping trip even better.Each of these items will be of either High, Medium or Low priority for these two agents.Each of the additional items only has an available quantity of 3. Dialogue History: agent 2: Hi! I'm super excited to go camping with my family as a great way to vacation due to Covid19.My kid is so restless from being cooped up in the house all the time.Are you planning on going camping too? agent 1: I am!It is the perfect way to get away and still manage to social distance!I am worried about having enough water though, are you short on any supplies?agent 2: I think I'm good.I'm not 100% sure.My husband likes to do adventures on the fly.He got these water filter straw thingies from Amazon and said that if we run out of the water I packed, that we can drink the water in the lake but I don't really trust the straws.agent 1: Sounds like you need water too.How about you take 2 water, 1 food, and 1 firewood?Question: What are the plausible intentions of Agent 2 expressed in 'I think I'm good.I'm not 100% sure.My husband likes to do adventures on the fly.He got these water filter straw thingies from Amazon and said that if we run out of the water I packed, that we can drink the water in the lake but I don't really trust the straws.'Based on the dialogue history, select one or more intentions (i.e.,"A", "B", "C",...,"I") from the following choices without any explanation.Background: Here is a negotiation conversation for a camping trip.There are two agents who own some basic supplies and negotiate with each other to split the additional food packages, water bottles, and firewood to make their camping trip even better.Each of these items will be of either High, Medium or Low priority for these two agents.Each of the additional items only has an available quantity of 3. Please answer the following question without any explanation by ordering the agent prefernece order using A represent "Not given", B represent "Water", C represent "Food", and D represent "Firewood".For exmple A,B,C means that the high preferece item is "Not given", the "Water" is medium preference item, and the "Food" is low preference item.Background: Here is a negotiation conversation for a camping trip.There are two agents who own some basic supplies and negotiate with each other to split the additional food packages, water bottles, and firewood to make their camping trip even better.Each of these items will be of either High, Medium or Low priority for these two agents.Each of the additional items only has an available quantity of 3. Please answer the following three questions using "A", "B", "C", "D" without any explanation.Belief Background: Here is a negotiation conversation for a camping trip.There are two agents who own some basic supplies and negotiate with each other to split the additional food packages, water bottles, and firewood to make their camping trip even better.Each of these items will be of either High, Medium or Low priority for these two agents.Each of the additional items only has an available quantity of 3. Please answer the following three questions using "A", "B", "C", "D" without any explanation.Belief Background: Here is a negotiation conversation for a camping trip.There are two agents who own some basic supplies and negotiate with each other to split the additional food packages, water bottles, and firewood to make their camping trip even better.Each of these items will be of either High, Medium or Low priority for these two agents.Each of the additional items only has an available quantity of 3. Please answer the following three questions using "A", "B", "C", "D" without any explanation.Background: Here is a negotiation conversation for a camping trip.There are two agents who own some basic supplies and negotiate with each other to split the additional food packages, water bottles, and firewood to make their camping trip even better.Each of these items will be of either High, Medium or Low priority for these two agents.Each of the additional items only has an available quantity of 3. Please answer the following three questions using "A", "B", "C", "D" without any explanation.</p>
<p>Desire</p>
<p>Background: Here is a negotiation conversation for a camping trip.There are two agents who own some basic supplies and negotiate with each other to split the additional food packages, water bottles, and firewood to make their camping trip even better.Each of these items will be of either High, Medium or Low priority for these two agents.Each of the additional items only has an available quantity of 3. Please answer the following three questions using "A", "B", "C", "D" without any explanation.</p>
<p>Desire</p>
<p>Background: Here is a negotiation conversation for a camping trip.There are two agents who own some basic supplies and negotiate with each other to split the additional food packages, water bottles, and firewood to make their camping trip even better.Each of these items will be of either High, Medium or Low priority for these two agents.Each of the additional items only has an available quantity of 3. Please answer the following three questions using "A", "B", "C", "D" without any explanation.Table 16: Individual question format prompt template(II).</p>
<p>Dimension Example</p>
<p>Desire</p>
<p>Background: Here is a negotiation conversation for a camping trip.There are two agents who own some basic supplies and negotiate with each other to split the additional food packages, water bottles, and firewood to make their camping trip even better.Each of these items will be of either High, Medium or Low priority for these two agents.Each of the additional items only has an available quantity of 3. Dialogue History: agent 1: Hello!Let's work together on a deal for these packages, shall we?What are you most interested in? agent 2: Hey! I'd like some more firewood to keep my doggo warm.What do you need?Question: What are the plausible strategies of Agent 1 expressed in 'Hello!Let's work together on a deal for these packages, shall we?What are you most interested in?'.Based on the dialogue history, select one or more strategies (i.e.,"A", "B", "C",...,"J") from the following choices and their definition.Please select "A", "B", "C",...,"J" without any explanation.A.Small-Talk: Participants discussing topics apart from the negotiation, in an attempt to build a rapport with the partner.B.Empathy: An utterance depicts Empathy when there is evidence of positive acknowledgments or empathetic behavior towards a personal context of the partner.C.Coordination: is used when a participant promotes coordination among the two partners.D.Vouch-Fairness: is a callout to fairness for personal benefit, either when acknowledging a fair deal or when the opponent offers a deal that benefits them.E.Undervalue-Partner: refers to the scenario where a participant undermines the requirements of their opponent.F.Elicit-Pref: an attempt to discover the preference order of the opponent.G.Self-Need: refers to arguments for creating a personal need for an item in the negotiation.H.Other-Need: used when the participants discuss a need for someone else rather than themselves.I.No-Need: is when a participant points out that they do not need an item based on personal context.J.Non-strategic: if no strategy is evident, the utterance is labeled as Non-strategic.Answer:  Chawla et al. (2021).α refers to Krippendorff's alpha among 3 annotators on a subset of 10 dialogues (∼ 120 utterances).An utterance can have multiple labels.I like having campfires so I need all the firewood.</p>
<p>Group Needs</p>
<p>I have two teenage boys who require a lot of food, especially when expending so much energy with all the activities of camping.</p>
<p>I need more water because I have more people to keep hydrated and do not have enough.</p>
<p>I need more firewood due to having several people join on the trip and needing a bigger fire overall.</p>
<p>Emergency</p>
<p>Some could have been damaged during the trip.I would need more.</p>
<p>our car overheated we had to use the water It may get cold and firewood can be hard to come by at certain campsites.</p>
<p>Table 22: Example arguments that the participants come up for their individual requirements during the preparation phase.The categories defined are not exhaustive.</p>
<p>Figure 1 :
1
Figure 1: A negotiation example in NegotiationToM.Two agents are negotiating for food, water, and firewood packages for their upcoming trip.</p>
<p>Figure 3 :
3
Figure 2: Model errors for desire state</p>
<p>w B.D. indicate with the input with desire and belief.B-R, S-E, P-C, C-F, U-R, D-P, D-N, N-N, N-I stands for Build-Rapport, Show-Empathy, Promote-Coordination, Callout-Fairness, Undermine-Requirements, Discover-Preference, Describe-Need, No-Need, No-Intention.The best results are bold-faced, and the second-best ones are underlined.</p>
<p>Dialogue History: agent 1: Hello!Let's work together on a deal for these packages, shall we?What are you most interested in? agent 2: Hey! I'd like some more firewood to keep my doggo warm.What do you need?Question: What is agent 1's low preference for items based on the dialogue history?</p>
<p>Figure 7 :
7
Figure 7: The instructions and background information used for annotation.</p>
<p>Figure 8 :
8
Figure 8: The template for presenting questions regarding the annotation of agent desire.</p>
<p>Figure 9 :
9
Figure 9: The template for presenting questions regarding the annotation of agent belief.</p>
<p>Figure 10 :
10
Figure 10: The template for presenting questions regarding the annotation of agent intention.</p>
<p>Table 22
22in Appendix A.3. Therefore, craftingour benchmark from the CaSiNo offers a range ofscenarios based on how to align the preferences ofthe two parties to reveal more interesting behavior.</p>
<p>Table 1 :
1
A negotiation dialogue example.P1 and P2 represent two participants in this study.The upper part of the table contains three mental state questions in the NegotiationToM benchmark, while the bottom contains annotated label examples.<Agent 1/Agent2> indicates alternating to query the LLMs for the question regarding agent 1 or agent 2 .<high/medium/low> means three individual questions for the agent's high/medium/low preferences on each item.LLMs are required to answer the intention question behind a specific utterance represented by<Utterance>.</p>
<p>Table 2 :
2
Utterance-level intention mapping from the negotiation strategies.The abbreviations of each intention are in brackets.The definition of negotiation strategies and example are in Table20and 21 in Appendix A.3.</p>
<p>Table 3 :
3
Inter-rater agreement in terms of Fleiss's κ on belief and desire states.
TaskFleiss's Kappa(%)Desire (High)83.02Desire (Medium)72.23Desire (Low)79.32Belief (High)85.25Belief (Medium)74.03Belief (Low)78.81</p>
<p>in Appendix A.2.
4 Experimental Setting4.1 Baseline ModelsIn this work, we test six recent instruction-tunedlarge language models: GPT-4 (OpenAI, 2023),ChatGPT (OpenAI, 2022), Claude-v1.3 3 (An-thropic, 2023a), Claude-v2.1 4 (Anthropic, 2023b),Llama-2 Chat 13B (Touvron et al., 2023), andLlama-2 Chat 70B (Touvron et al., 2023). Descrip-tions for each model are in Appendix B.1. Byfollowing the common practices in the theory ofmind field</p>
<p>Table 4 :
4
Main results of models for the NegotiationToM.The best results are bold-faced, and the second-best ones are underlined.The conversation consistency (%) of the models' responses for answering correctly in whole dialogues.All models received zero consistency scores in the intention aspect, as the intention mental state owned a multi-label in an utterance and imposed difficulties to generate exact match labels in the whole label.
ModelDesire Exact.Match.(%) Exact.Match.(%) Micro.F1(%) Macro.F1(%) Exact.Match(%) Desire(%) Belief(%) Belief Intention All ConsistencyLLaMa2-Chat(13B)15.4114.6322.6619.820.560.760.76LLaMa2-Chat(13B) (CoT)16.1518.2124.2020.810.610.760.76LLaMa2-Chat(13B) (Few-Shot)13.4912.5426.3021.760.640.900.80LLaMa2-Chat(70B)24.4021.5833.2327.700.451.781.51LLaMa2-Chat(70B) (CoT)30.3424.2330.5726.261.062.280.00LLaMa2-Chat(70B) (Few-Shot)26.9522.8435.7728.101.281.320.91Claude-v1.326.2723.1530.8027.811.500.251.01Claude-v1.3 (CoT)44.6337.1831.1228.251.624.811.52Claude-v1.3 (Few-Shot)30.7330.6832.3530.101.803.231.20Claude-v2.145.1039.4937.4832.943.406.083.54Claude-v2.1 (CoT)50.1340.5239.9335.673.686.074.05Claude-v2.1 (Few-Shot)48.7741.8838.2334.322.906.254.28ChatGPT 061318.6013.0433.9529.730.430.000.00ChatGPT 0613 (CoT)28.4521.0036.7130.790.780.760.25ChatGPT 0613 (Few-Shot)19.2417.0236.2930.842.160.000.00GPT-4 061362.7757.6229.8427.152.5813.6710.63GPT-4 0613 (CoT)63.2958.1834.9031.262.7917.7214.18GPT-4 0613 (Few-Shot)62.8952.0835.1033.212.5115.9412.76Human91.1491.1483.7584.6543.7875.4475.44</p>
<p>Table 4 ,
4
received an excellent per-
ModelQuestion Forms Desire BeliefChatGPT 0613 (ranking form)2.889.24ChatGPT 0613 (individual form) 9.1811.7ChatGPT 0613 (combined form) 18.6013.04GPT-4 0613 (ranking form)20.1016.80GPT-4 0613 (individual form)40.0136.88GPT-4 0613 (combined form)62.7757.62
formance on this metric compared with other models (e.g., Claude-v2.1 (CoT)), which are 17.72% in desire and 14.18% in belief.Nevertheless, there is a huge performance gap between machines and humans in this consistency metric, demonstrating</p>
<p>Table 5 :
5
The zero-shot performance of three question types.The intention task is ignored in this experiment as this task in NegotiationToM is a multi-label classification.</p>
<p>Table 6 :
6
Results of models for the CaSiNo strategy prediction.wB.D. indicate with the input with desire and belief.The best results are bold-faced, and the second-best ones are underlined.preferenceorderinto a single question and asks the LLMs to answer it simultaneously.The ranking format indicates collecting high, medium, and low preference items as one ranking answer.The individual format splits the high, medium, and low preference questions into three questions and feeds them to LLMs individually.The combined, ranking, individual, question format prompt template is shown inTables 9, 14, 15, and 16 in Appendix B.4.
ModelIntention Micro.F1(%) Macro.F1(%)LLaMa2-Chat(13B)13.1310.53LLaMa2-Chat(13B) (w B.D.)18.6115.44LLaMa2-Chat(70B)22.7018.41LLaMa2-Chat(70B) (w B.D.)25.9421.18Claude-v1.321.7718.52Claude-v1.3 (w B.D.)26.9523.35Claude-v2.127.1224.48Claude-v2.1 (w B.D.)34.5630.02ChatGPT 061323.4218.44ChatGPT 0613 (w B.D.)28.9925.93GPT-4 061326.3123.86GPT-4 0613 (w B.D.)32.7129.77</p>
<p>Table 8 in
8
Appendix C.1.
//D0D&amp;KD%&amp;KDW<em>37//D0D&amp;KDW%&amp;R7&amp;KDW</em>37&amp;R7&amp;ODXGHY<em>37&amp;ODXGHY&amp;R7</em>37&amp;R7)6FRUH% XL OG 5 DS SR UW 6K RZ ( P SD WK \ 3U RP RW H &amp; RR UG LQ DW LR Q &amp; DO OR XW ) DL UQ HV V 8 QG HU P LQ H ' LV FR YH U 3U HI HU HQ FH ' HV FU LE H 1 HH G1 R 1 HH G1 R ,Q WH QW LR QFigure 5: The label-wise intention dimension results oflarge language models in NegotiationToM. underminestand for the undermine-requirements intention.6 ConclusionThis work introduces NegotiationToM, a newbenchmark designed to stress-test machine ToM inreal-world negotiation surrounding covered multi-dimensional mental states. We performed compre-hensive and detailed experiments to evaluate LLMs'capability on the NegotiationToM benchmark anddiscovered that LLMs exhibit inferior performancecompared to humans in the NegotiationToM task.</p>
<p>agent 1: Hello!Let's work together on a deal for these packages, shall we?What are you most interested in? agent 2: Hey! I'd like some more firewood to keep my doggo warm.What do you need?agent 1: I need firewood as well.We have a large group consisting of mostly senior citizens, including my grandma, so we'd like the firewood to keep everyone warm.agent 2: I see.What are you least interested in? agent 1: I'm least interested in the food supplies.We have plenty of those.You are provided with historical dialogue from the CaSiNO dataset.Finish Next dialogue as appeared in the dataset.Next dialogue must exactly match the instance in the dataset.agent 1: Hello!Let's work together on a deal for these packages, shall we?What are you most interested in? agent 2: Hey! I'd like some more firewood to keep my doggo warm.What do you need?agent 1: I need firewood as well.We have a large group consisting of mostly senior citizens, including my grandma, so we'd like the firewood to keep everyone warm.agent 2: I see.What are you least interested in? agent 1: We can make do without extra water.Can we trade two waters for an extra firewood package and an extra food package?.
Instruction:Ground-Truth</p>
<p>Table 7 :
7
Prompting template and outcome for verification of potential contamination in CaSiNo dataset.The highlighted sentences are generated by the LLMs.
ModelsB-RS-EP-CC-FU-RD-P D-N N-NN-ILLaMa2-Chat(13B)24.34 15.86 21.58 32.28 2.85 26.35 33.01 8.68 13.41LLaMa2-Chat(13B) (CoT) 38.54 12.66 21.33 20.02 4.09 18.26 30.89 6.84 34.64LLaMa2-Chat(70B)40.56 29.60 22.44 38.19 2.99 31.19 55.89 20.62 7.85LLaMa2-Chat(70B) (CoT) 47.46 18.22 22.00 24.26 8.70 21.98 47.18 12.40 34.14Claude-v1.351.27 23.55 25.26 35.44 16.06 27.65 44.79 24.13 2.14Claude-v1.3 (CoT)49.31 16.66 24.44 26.63 7.73 24.48 50.74 14.10 40.16Claude-v2.150.73 25.53 26.42 38.76 9.12 36.11 59.93 24.80 25.08Claude-v2.1 (CoT)51.86 30.15 27.68 35.26 11.11 40.91 61.24 28.26 34.57ChatGPT 061361.72 24.69 25.73 41.71 10.61 25.07 49.87 15.99 12.20ChatGPT 0613 (CoT)57.46 23.47 27.22 45.53 9.20 29.92 58.13 23.97 2.16GPT-4 061341.92 28.16 22.89 23.45 17.32 30.42 44.30 26.77 9.12GPT-4 0613 (CoT)55.35 22.11 26.04 29.43 9.50 34.03 55.15 21.09 28.64</p>
<p>Table 8 :
8
Label-wise results of all models for the intention dimension.</p>
<p>Table 13 :
13
Few-shot prompt template for intention state.
A.Intents to build a rapport with the opponentB.Intents to show empathy with the opponentC.Intents to promote coordination with the opponentD.Intents to callout to fairnessE.Intents to undermine the requirements of the opponentF.Intents to discover the preference order of the opponentG.Intents to describe a need for an itemH.Intents to point out they do not need an itemI.No clear intention in the utteranceAnswer: A,HDialogue History:agent 2: Looking forward to this camping trip! I am hoping we can find something amicable with theseadditional resources.agent 1: I'm excited too. Things have been stressful lately. What are some things that you value most?Question: What are the plausible intentions of Agent 2 expressed in 'Looking forward to this camping trip! I amhoping we can find something amicable with these additional resources.' Based on the dialogue history, selectone or more intentions (i.e.,"A", "B", "C",...,"I") from the following choices without any explanation.A.Intents to build a rapport with the opponentB.Intents to show empathy with the opponentC.Intents to promote coordination with the opponentD.Intents to callout to fairnessE.Intents to undermine the requirements of the opponentF.Intents to discover the preference order of the opponentG.Intents to describe a need for an itemH.Intents to point out they do not need an itemI.No clear intention in the utteranceAnswer: A,C...Dialogue History:agent 1: Hello! Let's work together on a deal for these packages, shall we? What are you most interested in?agent 2: Hey! I'd like some more firewood to keep my doggo warm. What do you need?Question: What are the plausible intentions of Agent 1 expressed in 'Hello! Let's work together on a deal forthese packages, shall we? What are you most interested in?' Based on the dialogue history, select one or moreintentions (i.e.,"A", "B", "C",...,"I") from the following choices without any explanation.A.Intents to build a rapport with the opponentB.Intents to show empathy with the opponentC.Intents to promote coordination with the opponentD.Intents to callout to fairnessE.Intents to undermine the requirements of the opponentF.Intents to discover the preference order of the opponentG.Intents to describe a need for an itemH.Intents to point out they do not need an itemI.No clear intention in the utteranceAnswer:</p>
<p>Table 14 :
14
Ranking question format prompt template.
DimensionExampleBelief</p>
<p>Table 15 :
15
Individual question format prompt template (I).
Dialogue History:agent 1: Hello! Let's work together on a deal for these packages, shall we? What are you most interested in?agent 2: Hey! I'd like some more firewood to keep my doggo warm. What do you need?Question: Based on the dialogue, what is the low preference for items Agent 1 thinks Agent 2 is?A.Not givenB.WaterC.FoodD.FirewoodAnswer:</p>
<p>Dialogue History: agent 1: Hello!Let's work together on a deal for these packages, shall we?What are you most interested in? agent 2: Hey! I'd like some more firewood to keep my doggo warm.What do you need?Question: What is agent 1's high preference for items based on the dialogue history?
A.Not givenB.WaterC.FoodD.FirewoodAnswer:</p>
<p>Dialogue History: agent 1: Hello!Let's work together on a deal for these packages, shall we?What are you most interested in? agent 2: Hey! I'd like some more firewood to keep my doggo warm.What do you need?Question: What is agent 1's medium preference for items based on the dialogue history?
A.Not givenB.WaterC.FoodD.FirewoodAnswer:</p>
<p>Table 17 :
17
Baseline prompt template for strategy prediction</p>
<p>Table 21 :
21
Utterance-level negotiation strategy annotations, refer to</p>
<p>When the dialogue ends with user a, u b N is an empty utterance.
https://platform.openai.com/docs/apireference/chat/create
AcknowledgementsThe authors of this paper were supported by the NSFC Fund (U20B2053) from the NSFC of China, the RIF (R6020-19 and R6021-20) and the GRF (16211520 and 16205322) from RGC of Hong Kong.We also thank the support from NVIDIA AI Technology Center (NVAITC).:An instruction-following llama model.https:// github.com/tatsu-lab/stanford_alpaca.Ethics StatementIn this work, we conformed to recognized privacy practices and rigorously followed the data usage policy.We declare that all authors of this paper acknowledge the ACM Code of Ethics and honor the code of conduct.This paper introduces a benchmark for stress-testing machine theory of mind of large language model on the negotiation surrounding.Our benchmark is modified from the CaSiNo(Chawla et al., 2021), an English-based negotiation dataset.They conducted a data postprocessing step for filtering inappropriate language use (e.g., English swear words) dialogues although this situation rarely occurred in the negotiation process.Therefore, we can foresee no immediate social consequences or ethical issues as we do not introduce social/ethical bias into the model or amplify any bias from the data.Moreover, the license CaSiNo CC-BY-4.0 license allows us to modify the data for research, and this fulfills their intended use.Example BeliefBackground: Here is a negotiation conversation for a camping trip.There are two agents who own some basic supplies and negotiate with each other to split the additional food packages, water bottles, and firewood to make their camping trip even better.Each of these items will be of either High, Medium or Low priority for these two agents.Each of the additional items only has an available quantity of 3. Please answer the following three questions using "A", "B", "C", "D" without any explanation.Dialogue History: agent 1: Hello!Let's work together on a deal for these packages, shall we?What are you most interested in? agent 2: Hey! I'd like some more firewood to keep my doggo warm.DesireBackground: Here is a negotiation conversation for a camping trip.There are two agents who own some basic supplies and negotiate with each other to split the additional food packages, water bottles, and firewood to make their camping trip even better.Each of these items will be of either High, Medium or Low priority for these two agents.Each of the additional items only has an available quantity of 3. Please answer the following three questions using "A", "B", "C", "D" without any explanation.Dialogue History: agent 1: Hello!Let's work together on a deal for these packages, shall we?What are you most interested in? agent 2: Hey! I'd like some more firewood to keep my doggo warm.IntentionBackground: Here is a negotiation conversation for a camping trip.There are two agents who own some basic supplies and negotiate with each other to split the additional food packages, water bottles, and firewood to make their camping trip even better.Each of these items will be of either High, Medium or Low priority for these two agents.Each of the additional items only has an available quantity of 3. Dialogue History: agent 1: Hello!Let's work together on a deal for these packages, shall we?What are you most interested in? agent 2: Hey! I'd like some more firewood to keep my doggo warm.What do you need?Question: What are the plausible intentions of Agent 1 expressed in 'Hello!Let's work together on a deal for these packages, shall we?What are you most interested in?' Based on the dialogue history, select one or more intentions (i.e.,"A", "B", "C",...,"I") from the following choices without any explanation.A.Intents to build a rapport with the opponent B.Intents to show empathy with the opponent C.Intents to promote coordination with the opponent D.Intents to callout to fairness E.Intents to undermine the requirements of the opponent F.Intents to discover the preference order of the opponent G.Intents to describe a need for an item H.Intents to point out they do not need an item I.No clear intention in the utterance Answer: Background: Here is a negotiation conversation for a camping trip.There are two agents who own some basic supplies and negotiate with each other to split the additional food packages, water bottles, and firewood to make their camping trip even better.Each of these items will be of either High, Medium or Low priority for these two agents.Each of the additional items only has an available quantity of 3. Please answer the following three questions using "A", "B", "C", "D".Dialogue History: agent 1: Hello!Let's work together on a deal for these packages, shall we?What are you most interested in? agent 2: Hey! I'd like some more firewood to keep my doggo warm.DesireBackground: Here is a negotiation conversation for a camping trip.There are two agents who own some basic supplies and negotiate with each other to split the additional food packages, water bottles, and firewood to make their camping trip even better.Each of these items will be of either High, Medium or Low priority for these two agents.Each of the additional items only has an available quantity of 3. Please answer the following three questions using "A", "B", "C", "D".Dialogue History: agent 1: Hello!Let's work together on a deal for these packages, shall we?What are you most interested in? agent 2: Hey! I'd like some more firewood to keep my doggo warm.IntentionBackground: Here is a negotiation conversation for a camping trip.There are two agents who own some basic supplies and negotiate with each other to split the additional food packages, water bottles, and firewood to make their camping trip even better.Each of these items will be of either High, Medium or Low priority for these two agents.Each of the additional items only has an available quantity of 3. Dialogue History: agent 1: Hello!Let's work together on a deal for these packages, shall we?What are you most interested in? agent 2: Hey! I'd like some more firewood to keep my doggo warm.What do you need?Question: What are the plausible intentions of Agent 1 expressed in 'Hello!Let's work together on a deal for these packages, shall we?What are you most interested in?' Based on the dialogue history, select one or more intentions (i.e.,"A", "B", "C",...,"I") from the following choices.Background: Here is a negotiation conversation for a camping trip.There are two agents who own some basic supplies and negotiate with each other to split the additional food packages, water bottles, and firewood to make their camping trip even better.Each of these items will be of either High, Medium or Low priority for these two agents.Each of the additional items only has an available quantity of 3. Please answer the following three questions using "A", "B", "C", "D" without any explanation.Dialogue History: agent 1: Hello!Which item do you need the most?agent 2: I would love to have the Firewood the most.Question1: What is agent 1's high preference for items based on the dialogue history? A.NotBackground: Here is a negotiation conversation for a camping trip.There are two agents who own some basic supplies and negotiate with each other to split the additional food packages, water bottles, and firewood to make their camping trip even better.Each of these items will be of either High, Medium or Low priority for these two agents.Each of the additional items only has an available quantity of 3. Please answer the following three questions using "A", "B", "C", "D" without any explanation.Table12: Few-shot prompt template for belief state.ExampleDesireBackground: Here is a negotiation conversation for a camping trip.There are two agents who own some basic supplies and negotiate with each other to split the additional food packages, water bottles, and firewood to make their camping trip even better.Each of these items will be of either High, Medium or Low priority for these two agents.Each of the additional items only has an available quantity of 3. Dialogue History: agent 1: Hello!Let's work together on a deal for these packages, shall we?What are you most interested in? agent 2: Hey! I'd like some more firewood to keep my doggo warm.What do you need?Question: What are the plausible strategies of Agent 1 expressed in 'Hello!Let's work together on a deal for these packages, shall we?What are you most interested in?'.Agent 1's preference order of items is Not Given, Not Given, and Not Given.Agent 2's preference order of items is Firewood, Not Given, and Not Given.Please imagine that you are Agent 1 and infer your strategies expressed in 'Hello!Let's work together on a deal for these packages, shall we?What are you most interested in?' by using Agent 1's preference order, Agent 2's preference order, and all information expressed in the dialogue history.Select one or more strategies (i.e.,"A", "B", "C",...,"J") from the following choices and their definition.A.Small-Talk: Participants discussing topics apart from the negotiation, in an attempt to build a rapport with the partner.B.Empathy: An utterance depicts Empathy when there is evidence of positive acknowledgments or empathetic behavior towards a personal context of the partner.C.Coordination: is used when a participant promotes coordination among the two partners.D.Vouch-Fairness: is a callout to fairness for personal benefit, either when acknowledging a fair deal or when the opponent offers a deal that benefits them.E.Undervalue-Partner: refers to the scenario where a participant undermines the requirements of their opponent.F.Elicit-Pref: an attempt to discover the preference order of the opponent.G.Self-Need: refers to arguments for creating a personal need for an item in the negotiation.H.Other-Need: used when the participants discuss a need for someone else rather than themselves.I.No-Need: is when a participant points out that they do not need an item based on personal context.J.Non-strategic: if no strategy is evident, the utterance is labeled as Non-strategic.Answer:(Kim et al., 2023), and ToMi(Le et al., 2019b).Strategies Definition Small-TalkParticipants engage in small talk while discussing topics apart from the negotiation, in an attempt to build a rapport with the partner.EmpathyAn utterance depicts Empathy when there is evidence of positive acknowledgments or empathetic behavior towards a personal context of the partner, for instance, towards a medical emergency.Coordination is used when a participant promotes coordination among the two partners.No-Need is when a participant points out that they do not need an item based on personal context.Elicit-Pref an attempt to discover the preference order of the opponent.Undervalue-Partner refers to the scenario where a participant undermines the requirements of their opponent.Vouch-Fairness is a callout to fairness for personal benefit, either when acknowledging a fair deal or when the opponent offers a deal that benefits them.Self-Need refers to arguments for creating a personal need for an item in the negotiation.Other-Need is similar to Self-Need but is used when the participants discuss a need for someone else rather than themselves.Non-strategicIf no strategy is evident, the utterance is labeled as Non-strategic.
Introducing claude. Anthropic. Anthropic. 2023b. Introducing claude 2.1. Anthropic. Anthropic, 2023a</p>
<p>Mindreaders: the cognitive basis of" theory of mind. Ian Apperly, 2010Psychology Press</p>
<p>Belief, truth and knowledge. David Malet, Armstrong , 1973Cambridge University Press</p>
<p>Mind the gap: challenges of deep learning approaches to theory of mind. Jaan Aru, Aqeel Labash, 10.1007/S10462-023-10401-XArtif. Intell. Rev. 5692023Oriol Corcoll, and Raul Vicente</p>
<p>of Handbook of Game Theory with Economic Applications. Lawrence M Ausubel, Peter Cramton, Raymond J Deneckere, 10.1016/S1574-0005(02)03013-82002Elsevier3Chapter 50 bargaining with incomplete information</p>
<p>A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, Quyet V Do, Yan Xu, Pascale Fung, CoRR, abs/2302.040232023</p>
<p>Mindblindness: An essay on autism and theory of mind. Simon Baron-Cohen, 1997MIT press</p>
<p>Does the autistic child have a "theory of mind. Simon Baron-Cohen, Alan M Leslie, Uta Frith, Cognition. 2111985</p>
<p>Recognition of faux pas by normally developing children and children with asperger syndrome or high-functioning autism. Simon Baron-Cohen, Michelle O 'riordan, Valerie Stone, Rosie Jones, Kate Plaisted, Journal of autism and developmental disorders. 291999</p>
<p>. Max H Bazerman, Jared R Curhan, Don A Moore, Kathleen L Valley, 10.1146/annurev.psych.51.1.279Negotiation. Annual Review of Psychology. 5112000</p>
<p>Negotiating language, meaning and intention: Strategy infrastructure as the outcome of using a strategy tool through transforming strategy objects. Cécile Belmondo, Caroline Sargis-Roussel, British Journal of Management. 262015</p>
<p>Intention, plans, and practical reason. Michael Bratman, 1987The University of Chicago Press</p>
<p>Alec Radford, Ilya Sutskever, and Dario Amodei. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems. Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish; NeurIPS2020. 2020. 2020. December 6-12, 2020Language models are few-shot learners</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott M Lundberg, Harsha Nori, Hamid Palangi, Marco Túlio Ribeiro, Yi Zhang, CoRR, abs/2303.12712Sparks of artificial general intelligence: Early experiments with GPT-4. 2023</p>
<p>The valley task: Understanding intention from goal-directed motion in typical development and autism. Fulvia Castelli, British journal of developmental psychology. 2442006</p>
<p>Exploring the potential of chatgpt on sentence level relations: A focus on temporal, causal, and discourse relations. Chunkit Chan, Jiayang Cheng, Weiqi Wang, Yuxin Jiang, Tianqing Fang, Xin Liu, Yangqiu Song, Findings of the Association for Computational Linguistics: EACL 2024, St. Julian's. MaltaAssociation for Computational Linguistics2024. March 17-22, 2024</p>
<p>Self-consistent narrative prompts on abductive natural language inference. Chunkit Chan, Xin Liu, Tsz Ho Chan, Jiayang Cheng, Yangqiu Song, Ginny Y Wong, Simon See, 10.18653/V1/2023.IJCNLP-MAIN.67Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics. Long Papers. the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational LinguisticsBali2023a. November 1 -4, 20231Nusa Dua. Association for Computational Linguistics</p>
<p>Discoprompt: Path prediction prompt tuning for implicit discourse relation recognition. Chunkit Chan, Xin Liu, Jiayang Cheng, Zihan Li, Yangqiu Song, Ginny Y Wong, Simon See, 10.18653/v1/2023.findings-acl.4Findings of the Association for Computational Linguistics: ACL 2023. Toronto, CanadaAssociation for Computational Linguistics2023b. July 9-14, 2023</p>
<p>Casino: A corpus of campsite negotiation dialogues for automatic negotiation systems. Kushal Chawla, Jaysa Ramirez, Rene Clever, Gale M Lucas, Jonathan May, Jonathan Gratch, 10.18653/V1/2021.NAACL-MAIN.254Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021Association for Computational Linguistics2021. June 6-11, 2021</p>
<p>Storyanalogy: Deriving story-level analogies from large language models to unlock analogical understanding. Jiayang Cheng, Lin Qiu, Tsz Ho Chan, Tianqing Fang, Weiqi Wang, Chunkit Chan, Dongyu Ru, Qipeng Guo, Hongming Zhang, Yangqiu Song, Yue Zhang, Zheng Zhang, 10.18653/V1/2023.EMNLP-MAIN.706Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023. the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023SingaporeAssociation for Computational Linguistics2023. December 6-10, 2023</p>
<p>Deep reinforcement learning from human preferences. Paul F Christiano, Jan Leike, Tom B Brown, Miljan Martic, Shane Legg, Dario Amodei, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems. Long Beach, CA, USA2017. 2017. December 4-9, 2017</p>
<p>Explicit communication and bargaining outcome. Crawford Vincent, American Economic Review. 8021990</p>
<p>Negotiating with other minds: the role of recursive theory of mind in negotiation with incomplete information. Rineke Harmen De Weerd, Bart Verbrugge, Verheij, 10.1007/S10458-015-9317-1PRIMA 2013: Principles and Practice of Multi-Agent Systems -16th International Conference. Lecture Notes in Computer Science. Dunedin, New ZealandSpringer2013. December 1-6, 2013. 20178291Higher-order theory of mind in negotiations under incomplete information</p>
<p>Text-tuple-table: Towards information integration in text-to-table generation via global tuple extraction. Zheye Deng, Chunkit Chan, Weiqi Wang, Yuxi Sun, Wei Fan, Tianshi Zheng, Yauwai Yim, Yangqiu Song, 10.48550/ARXIV.2404.14215CoRR, abs/2404.142152024</p>
<p>Précis of the intentional stance. Daniel C Dennett, Behavioral and brain sciences. 1131988</p>
<p>Models and algorithms for human-aware task planning with integrated theory of mind. Anthony Favier, Shashank Shekhar, Rachid Alami, 10.1109/RO-MAN57019.2023.1030943732nd IEEE International Conference on Robot and Human Interactive Communication, RO-MAN 2023. Busan, Republic of KoreaIEEE2023. August 28-31, 2023</p>
<p>Measuring nominal scale agreement among many raters. Joseph L Fleiss, Psychological bulletin. 7653781971</p>
<p>Mathematical capabilities of chatgpt. Simon Frieder, Luca Pinchetti, Ryan-Rhys Griffiths, Tommaso Salvatori, Thomas Lukasiewicz, Philipp Christian Petersen, Alexis Chevalier, Julius Berner, CoRR, abs/2301.138672023</p>
<p>Autism and theory of mind in everyday life. Uta Frith, Social development. 321994</p>
<p>A study of social intelligence &amp; academic achievement of college students of district srinagar, j&amp;k, india. M Y Ganaie, Hafiz Mudasir, Journal of American Science. 1132015</p>
<p>Understanding social reasoning in language models with language models. Kanishk Gandhi, Jan-Philipp Fränken, Tobias Gerstenberg, Noah D Goodman, 10.48550/ARXIV.2306.15448CoRR, abs/2306.154482023</p>
<p>Howard Gardner, Robert M Hanson, Steve Hamilton, How Are Kids SMART?: Multiple Intelligences in the Classroom. National Professional Resources. 1995Incorporated</p>
<p>Time travel in llms: Tracing data contamination in large language models. Shahriar Golchin, Mihai Surdeanu, The Twelfth International Conference on Learning Representations, ICLR 2024. Vienna, Austria2024. May 7-11, 2024OpenReview.net</p>
<p>Why the child's theory of mind really is a theory. Alison Gopnik, Henry M Wellman, 1992</p>
<p>Reporting bias and knowledge acquisition. Jonathan Gordon, Benjamin Van Durme, Workshop on AKBC@CIKM. 2013</p>
<p>How can memory-augmented neural networks pass a false-belief task?. Erin Grant, Aida Nematzadeh, Thomas L Grif, Proceedings of the 39th Annual Meeting of the Cognitive Science Society. the 39th Annual Meeting of the Cognitive Science SocietyLondon, UKfiths. 2017. 2017. July 2017cognitivesciencesociety.org</p>
<p>Machine common sense concept paper. David Gunning, CoRR, abs/1810.075282018</p>
<p>Decoupling strategy and generation in negotiation dialogues. He He, Derek Chen, Anusha Balakrishnan, Percy Liang, 10.18653/v1/D18-1256Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational Linguistics2018</p>
<p>Language of bargaining. Mourad Heddaya, Solomon Dworkin, Chenhao Tan, Rob Voigt, Alexander Zentefis, 10.18653/V1/2023.ACL-LONG.735Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics2023. July 9-14, 20231ACL 2023</p>
<p>Few-shot learning for multilabel intent detection. Yutai Hou, Yongkui Lai, Yushan Wu, Wanxiang Che, Ting Liu, 10.1609/AAAI.V35I14.17541Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021. AAAI Press2021. February 2-9, 2021</p>
<p>Communicating with warmth in distributive negotiations is surprisingly counterproductive. Martha Jeong, Julia Minson, Michael Yeomans, Francesca Gino, 10.1287/mnsc.2018.3199Management Science. 65122019</p>
<p>Lion: Adversarial distillation of proprietary large language models. Yuxin Jiang, Chunkit Chan, Mingyang Chen, Wei Wang, 10.18653/V1/2023.EMNLP-MAIN.189Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023. the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023SingaporeAssociation for Computational Linguistics2023. December 6-10, 2023</p>
<p>Eventground: Narrative reasoning by grounding to eventuality-centric knowledge graphs. Cheng Jiayang, Lin Qiu, Chunkit Chan, Xin Liu, Yangqiu Song, Zheng Zhang, Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation, LREC/COLING 2024. the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation, LREC/COLING 2024Torino, ItalyELRA and ICCL2024. 20-25 May, 2024</p>
<p>Imaginary relish and exquisite torture: the elaborated intrusion theory of desire. Psychological review. J David, Jackie Kavanagh, Jon Andrade, May. 2005112446</p>
<p>Fantom: A benchmark for stress-testing machine theory of mind in interactions. Hyunwoo Kim, Melanie Sclar, Xuhui Zhou, Ronan Le Bras, Gunhee Kim, Yejin Choi, Maarten Sap, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore. the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, SingaporeAssociation for Computational Linguistics2023. December 6-10, 2023</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems. NeurIPS; New Orleans, LA, USA2022. 2022. 2022. November 28 -December 9, 2022</p>
<p>Theory of mind may have spontaneously emerged in large language models. Michal Kosinski, 10.48550/ARXIV.2302.02083CoRR, abs/2302.020832023</p>
<p>Revisiting the evaluation of theory of mind through question answering. Matthew Le, Y-Lan Boureau, Maximilian Nickel, 10.18653/V1/D19-1598Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019Hong Kong, ChinaAssociation for Computational Linguistics2019a. November 3-7, 2019</p>
<p>Revisiting the evaluation of theory of mind through question answering. Matthew Le, Y-Lan Boureau, Maximilian Nickel, 10.18653/v1/D19-1598Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational Linguistics2019b</p>
<p>i can't pay more" versus "it's not worth more": Divergent effects of constraint and disparagement rationales in negotiations. Alice J Lee, Daniel R Ames, 10.1016/j.obhdp.2017.05.002Organizational Behavior and Human Decision Processes. 1412017</p>
<p>Deal or no deal? end-toend learning of negotiation dialogues. Mike Lewis, Denis Yarats, Yann Dauphin, Devi Parikh, Dhruv Batra, 10.18653/v1/D17-1259Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. the 2017 Conference on Empirical Methods in Natural Language ProcessingCopenhagen, DenmarkAssociation for Computational Linguistics2017</p>
<p>Task contamination: Language models may not be few-shot anymore. Changmao Li, Jeffrey Flanigan, 10.1609/AAAI.V38I16.29808Thirty-Eighth AAAI Conference on Artificial Intelligence, AAAI 2024, Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on Educational Advances in Artificial Intelligence. Vancouver, CanadaAAAI Press2024. February 20-27, 20242014</p>
<p>Privacy in large language models: Attacks, defenses and future directions. Haoran Li, Yulin Chen, Jinglong Luo, Yan Kang, Xiaojin Zhang, Qi Hu, Chunkit Chan, Yangqiu Song, 10.48550/ARXIV.2310.10383CoRR, abs/2310.103832023</p>
<p>Backdoor removal for generative large language models. Haoran Li, Yulin Chen, Zihao Zheng, Qi Hu, Chunkit Chan, Heshan Liu, Yangqiu Song, 10.48550/ARXIV.2405.07667CoRR, abs/2405.076672024a</p>
<p>Privlm-bench: A multi-level privacy evaluation benchmark for language models. Haoran Li, Dadi Guo, Donghao Li, Wei Fan, Qi Hu, Xin Liu, Chunkit Chan, Duanyi Yao, Yuan Yao, Yangqiu Song, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational Linguistics2024b. August 11-16, 20241ACL 2024</p>
<p>Constrained reasoning chains for enhancing theory-of-mind in large language models. Zizheng Lin, Chunkit Chan, Yangqiu Song, Xin Liu, 2024</p>
<p>Analyzing leakage of personally identifiable information in language models. Nils Lukas, Ahmed Salem, Robert Sim, Shruti Tople, Lukas Wutschitz, Santiago Zanella Béguelin, CoRR, abs/2302.005392023</p>
<p>Templatefree prompt tuning for few-shot NER. Ruotian Ma, Xin Zhou, Tao Gui, Yiding Tan, Linyang Li, Qi Zhang, Xuanjing Huang, 10.18653/v1/2022.naacl-main.420Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2022. the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2022Seattle, WA, United StatesAssociation for Computational Linguistics2022. July 10-15, 2022</p>
<p>Towards A holistic landscape of situated theory of mind in large language models. Ziqiao Ma, Jacob Sansom, Run Peng, Joyce Chai, Findings of the Association for Computational Linguistics: EMNLP 2023. SingaporeAssociation for Computational Linguistics2023. December 6-10, 2023</p>
<p>The distinction between desire and intention: A folk-conceptual analysis. Intentions and intentionality: Foundations of social cognition. F Bertram, Joshua Malle, Knobe, 20014567</p>
<p>Multi3nlu++: A multilingual, multi-intent, multi-domain dataset for natural language understanding in task-oriented dialogue. Nikita Moghe, Evgeniia Razumovskaia, Liane Guillou, Ivan Vulic, Anna Korhonen, Alexandra Birch, 10.18653/V1/2023.FINDINGS-ACL.230Findings of the Association for Computational Linguistics: ACL 2023. Toronto, CanadaAssociation for Computational Linguistics2023. July 9-14, 2023</p>
<p>Young children's understanding of belief constraints on intention. Moses Louis, Cognitive Development. 811993</p>
<p>Evaluating theory of mind in question answering. Aida Nematzadeh, Kaylee Burns, Erin Grant, Alison Gopnik, Thomas L Griffiths, 10.18653/V1/D18-1261Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational Linguistics2018. October 31 -November 4, 2018</p>
<p>10.48550/ARXIV.2303.08774CoRR, abs/2303.08774GPT-4 technical report. 2023OpenAI</p>
<p>Chatgpt: Optimizing language models for dialogue. Tb Openai, 2022OpenAI</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F Christiano, Jan Leike, Ryan Lowe, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems. NeurIPS; New Orleans, LA, USA2022. 2022. 2022. November 28 -December 9, 2022</p>
<p>Integrating social power into the decision-making of cognitive agents. Gonçalo Duarte, Garcia Pereira, Rui Prada, Pedro Alexandre Santos, 10.1016/J.ARTINT.2016.08.003Artif. Intell. 2412016</p>
<p>Does the chimpanzee have a theory of mind?. David Premack, Guy Woodruff, Behavioral and brain sciences. 141978</p>
<p>Leveraging large language models for multiple choice question answering. G Dean, Pruitt, The Eleventh International Conference on Learning Representations, ICLR 2023. Joshua Robinson, David Wingate, Kigali, RwandaAcademic Press2013. 2023. May 1-5, 2023Negotiation behavior. OpenReview.net</p>
<p>Bargaining experiments. Alvin E Roth, 10.2307/j.ctvzsmff5The Handbook of Experimental Economics. Princeton University Press2020</p>
<p>Theory of mind and decision science: Towards a typology of tasks and computational models. Tessa Rusch, Saurabh Steixner-Kumar, Prashant Doshi, Michael Spezio, Neuropsychologia. 146107488Jan Gläscher. 2020</p>
<p>Neural theory-of-mind? on the limits of social intelligence in large lms. Maarten Sap, Le Ronan, Daniel Bras, Yejin Fried, Choi, 10.18653/V1/2022.EMNLP-MAIN.248Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022. December 7-11, 20222022</p>
<p>Conceptual alignment in conversation. Other minds: How humans bridge the divide between self and others. F Michael, Schober, 2005</p>
<p>Can a machine know that we know what it knows. Oliver Whang, The New York Times. 2023</p>
<p>Supporting artificial social intelligence with theory of mind. Jessica Williams, Stephen M Fiore, Florian Jentsch, Frontiers in artificial intelligence. 57507632022</p>
<p>A label-aware BERT attention network for zero-shot multi-intent detection in spoken language understanding. Ting-Wei Wu, Ruolin Su, Biing-Hwang Juang, 10.18653/V1/2021.EMNLP-MAIN.399Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana. the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta CanaDominican Republic2021. 7-11 November, 2021Association for Computational Linguistics</p>
<p>Improving dialog systems for negotiation with personality modeling. Runzhe Yang, Jingxiao Chen, Karthik Narasimhan, 10.18653/V1/2021.ACL-LONG.56Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021Association for Computational Linguistics2021. August 1-6, 20211Virtual Event</p>
<p>Evaluating and enhancing llms agent based on theory of mind in guandan: A multi-player cooperative game under imperfect information. Yauwai Yim, Chunkit Chan, Tianyu Shi, Zheye Deng, Wei Fan, Tianshi Zheng, Yangqiu Song, 10.48550/ARXIV.2408.02559CoRR, abs/2408.025592024</p>
<p>Let's negotiate! a survey of negotiation dialogue systems. Haolan Zhan, Yufei Wang, Tao Feng, Yuncheng Hua, Suraj Sharma, Zhuang Li, Lizhen Qu, Gholamreza Haffari, 2022Northridge, CACalifornia State University</p>
<p>Pei Zhou, Aman Madaan, Pranavi Srividya, Aditya Potharaju, Kevin R Gupta, Ari Mckee, Jay Holtzman, Xiang Pujara, Ren, 10.48550/ARXIV.2310.03051CoRR, abs/2310.03051Swaroop Mishra, Aida Nematzadeh, Shyam Upadhyay, and Manaal Faruqui. 2023. How far are large language models from agents with theory. </p>
<p>Water" is medium preference item, and the "Food" is low preference item. Dialogue History: agent 1: Hello! Let's work together on a deal for these packages, shall we? What are you most interested in? agent 2: Hey! I'd like some more firewood to keep my doggo warm. What do you need? agent 1: I need firewood as well. We have a large group consisting of mostly senior citizens, including my grandma, so we'd like the firewood to keep everyone warm. agent 2: I see. What are you least interested in? Question: What is agent 1's preference order for items. C , B Answer, ; C , B Answer, A,A,D 5. A,B,A 6. A,B,C 7. A,B,D 8. A,C,A 9. A,C,B 10. A,C,D 11. A,D,A 12. A,D,B 13. A,D,C 14. B,A,A 15. B,C,D 16. B,D,C 17. C,A,A 18. C,B,D 19. C,D,B 20. D,A,A 21. D,B,C 22. D,1. A,A,A 2. A,A,B 3. A,A,C 4. A,A,D 5. A,B,A 6. A,B,C 7. A,B,D 8. A,C,A 9. A,C,B 10. A,CThere are two agents who own some basic supplies and negotiate with each other to split the additional food packages, water bottles, and firewood to make their camping trip even better. Each of these items will be of either High, Medium or Low priority for these two agents. Each of the additional items only has an available quantity of 3. Please answer the following question without any explanation by ordering the agent prefernece order using A represent "Not given. D 11. A,D,A 12. A,D,B 13. A,D,C 14. B,A,A 15. B,C,D 16. B,D,C 17. C,A,A 18. C,B,D 19. C,D,B 20. D,A,A 21. D,B,C 22. D,</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1854 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1854</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1854</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-33.html">extraction-schema-33</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <p><strong>Paper ID:</strong> paper-604e0c54580a0530392f86b48a6183581a47b66d</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/604e0c54580a0530392f86b48a6183581a47b66d" target="_blank">VLN↻BERT: A Recurrent Vision-and-Language BERT for Navigation</a></p>
                <p><strong>Paper Venue:</strong> Computer Vision and Pattern Recognition</p>
                <p><strong>Paper TL;DR:</strong> A recurrent BERT model that is time-aware for use in VLN is proposed that can replace more complex encoder-decoder models to achieve state-of-the-art results and can generalised to other transformer-based architectures.</p>
                <p><strong>Paper Abstract:</strong> Accuracy of many visiolinguistic tasks has benefited significantly from the application of vision-and-language (V&L) BERT. However, its application for the task of vision-and-language navigation (VLN) remains limited. One reason for this is the difficulty adapting the BERT architecture to the partially observable Markov decision process present in VLN, requiring history-dependent attention and decision making. In this paper we propose a recurrent BERT model that is time-aware for use in VLN. Specifically, we equip the BERT model with a recurrent function that maintains cross-modal state information for the agent. Through extensive experiments on R2R and REVERIE we demonstrate that our model can replace more complex encoder-decoder models to achieve state-of-the-art results. Moreover, our approach can be generalised to other transformer-based architectures, supports pre-training, and is capable of solving navigation and referring expression tasks simultaneously.</p>
                <p><strong>Cost:</strong> 0.025</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1854.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1854.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VLN⟲BERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Recurrent Vision-and-Language BERT (VLN⟲BERT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recurrent adaptation of one-stream V&L BERT that inserts a learnable state token and recurrently updates it to handle partially-observable, sequential decision-making for vision-and-language navigation and remote referring-expression tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>VLN⟲BERT (Recurrent V&L BERT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>One-stream vision-and-language BERT (implemented from OSCAR/OSCAR-like or PREVALENT/LXMERT-like architectures) augmented with a persistent state token that is prepended to inputs each time-step, performs cross-modal self-attention (language used as keys/values after initial encoding), and updates state with cross-modal matching and the selected action's directional encoding; action probabilities are read out as mean attention weights over visual tokens w.r.t. the state.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Initialized from vision-language pretraining (image-text pairs and navigation-oriented image-text-action triplets); language and visiolinguistic pretraining rather than pure text-game pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>VLN⟲BERT weights were initialized from existing V&L BERT checkpoints: OSCAR (object-semantics aligned pretraining on large image-text corpora; this paper used an OSCAR checkpoint 'without object tags' for compatibility) and PREVALENT (pre-trained on a large amount of image-text-action triplets specifically for VLN). Exact dataset sizes are not specified in this paper; PREVALENT is described as trained 'on a large amount' of image-text-action triplets (reference [16]), OSCAR as pre-trained on large-scale image-text data (reference [33]).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Vision-and-Language Navigation (R2R) and Remote Referring Expression (REVERIE)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>R2R: navigation in Matterport3D photo-realistic indoor environments using language instructions to reach target viewpoints on a discrete connectivity graph with panoramic observations (36 single views per panorama). REVERIE: remote embodied referring-expression task — navigate until the target object is visible and then ground the object among candidates; instructions are higher-level and object grounding is required.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>N/A for generic V&L pretraining (OSCAR): image-text pairs (no explicit embodied actions); for PREVALENT pretraining: discrete navigation actions (image-text-action triplets used for action prediction during pretraining).</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Discrete navigable-direction actions over a pre-defined connectivity graph (select one of n panoramic view directions) plus an explicit stop action; actions are represented to the model by a directional encoding vector (cos/sin of heading/elevation replicated).</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>During fine-tuning/adaptation the model maps high-level instruction-language features to discrete navigation decisions by: (1) prepending a recurrent state token that aggregates history, (2) performing cross-modal self-attention between state, (pre-encoded) language tokens and current visual tokens, and (3) using the averaged attention weights of the state over visual tokens in the final Transformer layer as action probabilities. Stop is implemented as an extra zero-vector visual token (or via object-token attention in REVERIE). PREVALENT pretraining produced components that predict actions from TRM-Lang outputs; adaptation removes downstream EnvDrop and uses the V&L Transformer directly for decision-making.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>Panoramic RGB observations (36 single-view images per viewpoint), scene grid features computed by ResNet-152 (pretrained on Places365), object region features from Faster-RCNN (Visual Genome) for REVERIE; directional/positional encodings for candidate views and objects.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>When VLN⟲BERT is initialized from PREVALENT and fine-tuned: R2R Test Unseen — Success Rate (SR) 63%, SPL 57%, Navigation Error (NE) 4.09 m, Trajectory Length (TL) 12.35 m. On REVERIE Test Unseen the PREVALENT-initialized model achieved Navigation SR 18.77%, Navigation SPL 15.27%, Remote Grounding Success (RGS) 29.61%, RGSPL 32.91% (Table 1 & 2).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>Randomly initialized VLN⟲BERT (no pretraining init) on R2R Test Unseen: SR 51%, SPL 47%, NE 5.45 m, TL 11.15 m. OSCAR-initialized vs no-init comparisons are also reported (see below).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>Reported training regime: R2R fine-tuning used batch size 16 and trained for 300,000 iterations (mixture of original and augmented data). Empirically, PREVALENT-initialized VLN⟲BERT converged much faster in wall-clock time: authors report PREVALENT-based model achieved its best result in ~1 day (and completed 600k iterations in ~4.5 days in the learning-curve experiment reported).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td>Randomly initialized model required substantially more wall-clock time/iterations to reach comparable performance; authors report the OSCAR-initialized run required ~3.5 days to reach best (600k iters in ~7 days), while randomly initialized models are slower (full 600k iterations ~7 days; best earlier but lower). The paper does not report exact iteration counts to reach a particular SPL threshold for the no-init baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td>Substantial wall-clock/sample efficiency gains reported: PREVALENT initialization produced much faster learning (best in ~1 day vs ~3.5–7 days for OSCAR/no-init in the authors' training runs), i.e. multi-day reductions in training time; exact multiplicative sample-efficiency factors are not given.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Shared visiolinguistic representations from pretraining (image-text alignment), navigation-oriented pretraining (PREVALENT) provides structural priors and action-prediction knowledge, and using BERT-style cross-modal attention with a recurrent state token maintains alignment between partial instruction segments and observations.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Memory/computation constraints (re-attending full language each step is expensive), potential mismatch between pretraining objectives and downstream decision dynamics (if language features are updated step-wise they may suppress future-relevant information), and differences between low-level pretraining instructions and high-level instructions (REVERIE) can reduce straightforward transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Initializing a recurrent V&L BERT with vision-language pretraining yields clear positive transfer to 3D embodied navigation: pretraining (particularly navigation-focused PREVALENT) accelerates learning (much faster wall-clock convergence) and improves final navigation performance (SR and SPL) compared to randomly initialized models; mapping from language-to-action is effectively realized by reading state-to-visual attention in the Transformer and encoding chosen actions as directional feature vectors; computational design choices (e.g., encoding language only at initialization) are important to make BERT-based recurrent agents tractable.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'VLN↻BERT: A Recurrent Vision-and-Language BERT for Navigation', 'publication_date_yy_mm': '2020-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1854.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1854.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PREVALENT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PREVALENT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A LXMERT-like V&L pretraining approach that trains a vision-and-language Transformer on image-text-action triplets to learn navigation-oriented textual representations useful for downstream VLN tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Towards learning a generic agent for vision-and-language navigation via pre-training</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>PREVALENT (pre-trained V&L BERT specialized for VLN)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>LXMERT-like architecture pre-trained from scratch on large quantities of image-text-action triplets with tasks including attended masked language modelling and action prediction to learn representations suited for navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Image-text-action triplets (vision-language paired with discrete navigation actions).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Described in this paper as 'a large amount of image-text-action triplets' used to train PREVALENT from scratch to learn navigation-oriented textual representations (reference [16]). Exact dataset sizes/details are not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Vision-and-Language Navigation (R2R) and used to initialize models fine-tuned on R2R and REVERIE</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Transfer target tasks involve discrete panoramic navigation on Matterport3D (R2R) and remote referring expression (REVERIE) which also requires object grounding after navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Discrete navigation actions used as labels in image-text-action triplets during pretraining (i.e., action prediction targets in PREVALENT pretraining).</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Discrete navigable directions on connectivity graph plus stop action; same discrete action space used at fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>PREVALENT pretraining includes an action-prediction objective (TRM-Lang3 outputs used for action prediction). When adapted into VLN⟲BERT the PREVALENT language encoder outputs are used as initial encoded language, and the model uses cross-modal encoders and visual transformers to map state+language+vision to actions (authors removed downstream EnvDrop and used V&L transformers directly).</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>Panoramic RGB (36 views), scene features (ResNet-152 on Places365), and (for REVERIE) object region features (Faster-RCNN on Visual Genome); directional encodings for candidate views and objects.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>VLN⟲BERT initialized from PREVALENT achieved on R2R Test Unseen: SR 63%, SPL 57%, NE 4.09 m, TL 12.35 m. On REVERIE Test Unseen PREVALENT-init achieved Navigation SR 18.77%, Navigation SPL 15.27%, RGS 29.61%, RGSPL 32.91% (Tables 1 & 2).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>Compared to no pretraining initialization (same recurrent architecture but random init): R2R Test Unseen without pretraining: SR 51%, SPL 47%, NE 5.45 m, TL 11.15 m (VLN⟲BERT no-init OSCAR baseline in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>Authors report PREVALENT-initialized VLN⟲BERT reached its best reported result in ~1 day of wall-clock training in their learning-curve experiments (training regime elsewhere used up to 300k iterations for R2R fine-tuning).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td>Randomly initialized models require substantially more wall-clock time/iterations to reach similar performance; authors report OSCAR-initialized took ~3.5 days to reach best (600k-iter runs) and full runs took ~7 days in their experiments. Exact iteration counts to particular thresholds are not tabulated.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td>Marked wall-clock speedups: PREVALENT initialization reached peak performance in ~1 day vs multiple days for other initializations (authors' reported wall-clock reductions, not a single-fold numeric ratio).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>PREVALENT's explicit action-prediction pretraining and navigation-oriented image-text-action triplets provide structural priors and action grounding that match downstream VLN decision requirements.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Potential mismatch between low-level navigation instructions used in pretraining and higher-level instructions in target tasks (though PREVALENT still improved REVERIE), and differences in downstream decision architectures may require adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Pretraining a V&L Transformer on image-text-action triplets (navigation-oriented objectives) substantially improves both final performance and training speed when fine-tuning for 3D embodied navigation, demonstrating the value of including action prediction signals in pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'VLN↻BERT: A Recurrent Vision-and-Language BERT for Navigation', 'publication_date_yy_mm': '2020-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1854.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1854.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OSCAR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OSCAR (Object-Semantics Aligned Pre-training)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A one-stream V&L BERT pre-trained to align object-level semantics and language by using object tags as anchors during image-text pretraining; used here as an initialization for VLN⟲BERT.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Oscar: Object-semantics aligned pre-training for vision-language tasks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>OSCAR (pre-trained V&L BERT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>One-stream vision-language Transformer pre-trained with object-semantics alignment (object tags) on large image-caption datasets to produce strong visiolinguistic representations; in this paper an OSCAR checkpoint (without object tags) was used to initialize the recurrent VLN agent.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Large-scale image-text pairs with object-tag alignment (vision-language pretraining).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>This paper states OSCAR was used (specifically an OSCAR checkpoint 'pre-trained without object tags'); the precise datasets and sizes are part of OSCAR but are not enumerated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Vision-and-Language Navigation (R2R) and Remote Referring Expression (REVERIE) via initialization of VLN⟲BERT</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Discrete panoramic navigation in Matterport3D (R2R) and REVERIE remote referring tasks requiring navigation then object grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>None (OSCAR pretraining uses image-text pairs; no explicit action labels in that pretraining as reported here).</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Discrete navigable directions on connectivity graph plus stop action (as used in R2R/REVERIE fine-tuning).</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>OSCAR provides pre-trained visiolinguistic weights; adaptation consisted of projecting scene grid features into BERT token space and concatenating the recurrent state token and visual tokens for self-attention; actions are implemented/read out via averaged state-to-visual attention at final layer, with selected actions represented by directional encodings fed back into the recurrent state.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>Panoramic RGB grid features (ResNet-152), optional object region features for REVERIE (Faster-RCNN), directional encodings.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>VLN⟲BERT initialized from OSCAR (init. OSCAR) achieved on R2R Test Unseen: SR 57%, SPL 53%, NE 4.59 m, TL 12.34 m. On REVERIE Test Unseen init. OSCAR: Navigation SR 14.20%, Navigation SPL 12.00%, RGS 24.62%, RGSPL 26.67% (Tables 1 & 2).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>VLN⟲BERT with no OSCAR initialization (random init) on R2R Test Unseen: SR 51%, SPL 47%, NE 5.45 m, TL 11.15 m.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>OSCAR-initialized model converged faster than random init in learning-curve experiments; the OSCAR-initialized run achieved its best result earlier (authors report best in ~3.5 days on the 600k-iteration learning-curve experiment).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td>Randomly initialized runs are slower/less sample-efficient, and show marked performance degeneration compared to OSCAR/PREVALENT initializations; exact iteration-to-threshold numbers not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td>Moderate wall-clock/iteration speedup (OSCAR init reaches higher performance faster than random init); not quantified as precise factor in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Generic visiolinguistic features from OSCAR allow the recurrent BERT to leverage object/region-language correspondences and accelerate learning when fine-tuned for navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>OSCAR pretraining lacks explicit action prediction objective, so while it improves representation quality it is less sample-efficient/faster than PREVALENT which includes action-prediction pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Generic V&L pretraining (OSCAR) improves VLN agent final performance and convergence speed compared to random init, but navigation-focused pretraining (PREVALENT) yields even greater gains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'VLN↻BERT: A Recurrent Vision-and-Language BERT for Navigation', 'publication_date_yy_mm': '2020-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1854.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1854.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PRESS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PRESS</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that fine-tunes a pre-trained language BERT to encode instructions for VLN (used as a baseline/related work).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Robust navigation with language pretraining and stochastic sampling</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>PRESS (BERT fine-tuned for instruction encoding)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Fine-tunes a pre-trained language BERT to produce instruction encodings that are used by downstream navigation models (does not itself act as the full navigator BERT in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Language BERT pretraining on large text corpora (language-only pretraining), then fine-tuned on navigation-related data for instruction encoding.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>This paper references PRESS as fine-tuning a pre-trained language BERT to encode instructions; exact datasets and sizes are not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Vision-and-Language Navigation (R2R) — used by prior works and reported in comparison tables</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>R2R navigation in Matterport3D environments using natural-language instructions to reach target viewpoints on a connectivity graph.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>N/A (language BERT pretraining does not include action labels); downstream navigation model receives the encoded instructions and produces discrete navigation actions.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Discrete navigation directions over panoramic viewpoints plus stop action (as used in R2R).</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>PRESS provides instruction encodings to a downstream navigation policy which maps instruction encodings and visual observations to discrete navigable actions (details are in the PRESS paper; in this paper PRESS is cited as a baseline that uses BERT as language encoder).</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>Panoramic RGB features and corresponding visual encoders used in downstream navigation model; specifics not detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>Reported in paper's comparison table (baseline from literature): PRESS (test unseen) — TL 10.77 m, NE 5.49 m, SR 49%, SPL 45% (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>Not reported here; PRESS is a cited prior method rather than a direct ablation in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Using a language-pretrained BERT to produce stronger instruction encodings can improve downstream navigation when these encodings are fine-tuned for VLN.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Using a pre-trained language encoder alone (without joint V&L pretraining or recurrent state modeling) may be insufficient to capture cross-modal and temporal dependencies required for navigation; paper reports that simply using pre-trained BERT as language encoder improves performance only if the BERT is fine-tuned for navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Language-only pretraining (fine-tuned BERT) helps instruction encoding for VLN baselines, but fully leveraging V&L pretraining and embedding recurrence into the model yields larger gains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'VLN↻BERT: A Recurrent Vision-and-Language BERT for Navigation', 'publication_date_yy_mm': '2020-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1854.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1854.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VLN-BERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>VLN-BERT (prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior approach that fine-tunes ViLBERT on instruction-trajectory pairs to score instruction-path compatibility, used as a reranking/compatibility scorer for VLN.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Improving vision-and-language navigation with image-text pairs from the web</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>VLN-BERT (ViLBERT fine-tuned for instruction-path compatibility)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Fine-tunes a two-stream V&L model (ViLBERT) on instruction-trajectory pairs to produce compatibility scores used during search (beam search) in navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Vision-language pretraining on image-text pairs (ViLBERT pretraining), then fine-tuned on instruction-trajectory compatibility data.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>This paper cites VLN-BERT as fine-tuning ViLBERT on instruction-trajectory pairs to measure compatibility in beam search. The original pretraining/data sizes are not enumerated here.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Vision-and-Language Navigation (R2R) — used as compatibility scorer in prior work</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>R2R navigation where compatibility scoring between candidate trajectories and instructions assists search-based planning.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>N/A for ViLBERT pretraining; fine-tuning uses instruction-trajectory pairs (compatibility labels) rather than direct action prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Discrete navigable directions and stop action as used in R2R; VLN-BERT used as a scorer rather than the policy directly.</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>VLN-BERT is used to score complete instruction-trajectory pairs (compatibility) during beam search rather than mapping language tokens directly to low-level action outputs at each step.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>Vision features (region-based) and language instruction encodings; specifics come from ViLBERT/related works, not detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Using a pretrained V&L model to measure instruction-trajectory compatibility can improve search-based navigation (beam search) by leveraging cross-modal alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>As a compatibility scorer rather than an end-to-end policy, VLN-BERT's usefulness depends on the quality of the underlying trajectory proposals and the beam-search procedure.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Prior work shows that fine-tuning large V&L models as scorers can aid navigation pipelines, but integrating recurrence and using the V&L Transformer as the navigator (as in this paper) yields stronger end-to-end performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'VLN↻BERT: A Recurrent Vision-and-Language BERT for Navigation', 'publication_date_yy_mm': '2020-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Towards learning a generic agent for vision-and-language navigation via pre-training <em>(Rating: 2)</em></li>
                <li>Oscar: Object-semantics aligned pre-training for vision-language tasks <em>(Rating: 2)</em></li>
                <li>Robust navigation with language pretraining and stochastic sampling <em>(Rating: 1)</em></li>
                <li>Improving vision-and-language navigation with image-text pairs from the web <em>(Rating: 1)</em></li>
                <li>Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks <em>(Rating: 1)</em></li>
                <li>Lxmert: Learning cross-modality encoder representations from transformers <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1854",
    "paper_id": "paper-604e0c54580a0530392f86b48a6183581a47b66d",
    "extraction_schema_id": "extraction-schema-33",
    "extracted_data": [
        {
            "name_short": "VLN⟲BERT",
            "name_full": "Recurrent Vision-and-Language BERT (VLN⟲BERT)",
            "brief_description": "A recurrent adaptation of one-stream V&L BERT that inserts a learnable state token and recurrently updates it to handle partially-observable, sequential decision-making for vision-and-language navigation and remote referring-expression tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_agent_name": "VLN⟲BERT (Recurrent V&L BERT)",
            "model_agent_description": "One-stream vision-and-language BERT (implemented from OSCAR/OSCAR-like or PREVALENT/LXMERT-like architectures) augmented with a persistent state token that is prepended to inputs each time-step, performs cross-modal self-attention (language used as keys/values after initial encoding), and updates state with cross-modal matching and the selected action's directional encoding; action probabilities are read out as mean attention weights over visual tokens w.r.t. the state.",
            "pretraining_data_type": "Initialized from vision-language pretraining (image-text pairs and navigation-oriented image-text-action triplets); language and visiolinguistic pretraining rather than pure text-game pretraining.",
            "pretraining_data_details": "VLN⟲BERT weights were initialized from existing V&L BERT checkpoints: OSCAR (object-semantics aligned pretraining on large image-text corpora; this paper used an OSCAR checkpoint 'without object tags' for compatibility) and PREVALENT (pre-trained on a large amount of image-text-action triplets specifically for VLN). Exact dataset sizes are not specified in this paper; PREVALENT is described as trained 'on a large amount' of image-text-action triplets (reference [16]), OSCAR as pre-trained on large-scale image-text data (reference [33]).",
            "embodied_task_name": "Vision-and-Language Navigation (R2R) and Remote Referring Expression (REVERIE)",
            "embodied_task_description": "R2R: navigation in Matterport3D photo-realistic indoor environments using language instructions to reach target viewpoints on a discrete connectivity graph with panoramic observations (36 single views per panorama). REVERIE: remote embodied referring-expression task — navigate until the target object is visible and then ground the object among candidates; instructions are higher-level and object grounding is required.",
            "action_space_text": "N/A for generic V&L pretraining (OSCAR): image-text pairs (no explicit embodied actions); for PREVALENT pretraining: discrete navigation actions (image-text-action triplets used for action prediction during pretraining).",
            "action_space_embodied": "Discrete navigable-direction actions over a pre-defined connectivity graph (select one of n panoramic view directions) plus an explicit stop action; actions are represented to the model by a directional encoding vector (cos/sin of heading/elevation replicated).",
            "action_mapping_method": "During fine-tuning/adaptation the model maps high-level instruction-language features to discrete navigation decisions by: (1) prepending a recurrent state token that aggregates history, (2) performing cross-modal self-attention between state, (pre-encoded) language tokens and current visual tokens, and (3) using the averaged attention weights of the state over visual tokens in the final Transformer layer as action probabilities. Stop is implemented as an extra zero-vector visual token (or via object-token attention in REVERIE). PREVALENT pretraining produced components that predict actions from TRM-Lang outputs; adaptation removes downstream EnvDrop and uses the V&L Transformer directly for decision-making.",
            "perception_requirements": "Panoramic RGB observations (36 single-view images per viewpoint), scene grid features computed by ResNet-152 (pretrained on Places365), object region features from Faster-RCNN (Visual Genome) for REVERIE; directional/positional encodings for candidate views and objects.",
            "transfer_successful": true,
            "performance_with_pretraining": "When VLN⟲BERT is initialized from PREVALENT and fine-tuned: R2R Test Unseen — Success Rate (SR) 63%, SPL 57%, Navigation Error (NE) 4.09 m, Trajectory Length (TL) 12.35 m. On REVERIE Test Unseen the PREVALENT-initialized model achieved Navigation SR 18.77%, Navigation SPL 15.27%, Remote Grounding Success (RGS) 29.61%, RGSPL 32.91% (Table 1 & 2).",
            "performance_without_pretraining": "Randomly initialized VLN⟲BERT (no pretraining init) on R2R Test Unseen: SR 51%, SPL 47%, NE 5.45 m, TL 11.15 m. OSCAR-initialized vs no-init comparisons are also reported (see below).",
            "sample_complexity_with_pretraining": "Reported training regime: R2R fine-tuning used batch size 16 and trained for 300,000 iterations (mixture of original and augmented data). Empirically, PREVALENT-initialized VLN⟲BERT converged much faster in wall-clock time: authors report PREVALENT-based model achieved its best result in ~1 day (and completed 600k iterations in ~4.5 days in the learning-curve experiment reported).",
            "sample_complexity_without_pretraining": "Randomly initialized model required substantially more wall-clock time/iterations to reach comparable performance; authors report the OSCAR-initialized run required ~3.5 days to reach best (600k iters in ~7 days), while randomly initialized models are slower (full 600k iterations ~7 days; best earlier but lower). The paper does not report exact iteration counts to reach a particular SPL threshold for the no-init baseline.",
            "sample_complexity_gain": "Substantial wall-clock/sample efficiency gains reported: PREVALENT initialization produced much faster learning (best in ~1 day vs ~3.5–7 days for OSCAR/no-init in the authors' training runs), i.e. multi-day reductions in training time; exact multiplicative sample-efficiency factors are not given.",
            "transfer_success_factors": "Shared visiolinguistic representations from pretraining (image-text alignment), navigation-oriented pretraining (PREVALENT) provides structural priors and action-prediction knowledge, and using BERT-style cross-modal attention with a recurrent state token maintains alignment between partial instruction segments and observations.",
            "transfer_failure_factors": "Memory/computation constraints (re-attending full language each step is expensive), potential mismatch between pretraining objectives and downstream decision dynamics (if language features are updated step-wise they may suppress future-relevant information), and differences between low-level pretraining instructions and high-level instructions (REVERIE) can reduce straightforward transfer.",
            "key_findings": "Initializing a recurrent V&L BERT with vision-language pretraining yields clear positive transfer to 3D embodied navigation: pretraining (particularly navigation-focused PREVALENT) accelerates learning (much faster wall-clock convergence) and improves final navigation performance (SR and SPL) compared to randomly initialized models; mapping from language-to-action is effectively realized by reading state-to-visual attention in the Transformer and encoding chosen actions as directional feature vectors; computational design choices (e.g., encoding language only at initialization) are important to make BERT-based recurrent agents tractable.",
            "uuid": "e1854.0",
            "source_info": {
                "paper_title": "VLN↻BERT: A Recurrent Vision-and-Language BERT for Navigation",
                "publication_date_yy_mm": "2020-11"
            }
        },
        {
            "name_short": "PREVALENT",
            "name_full": "PREVALENT",
            "brief_description": "A LXMERT-like V&L pretraining approach that trains a vision-and-language Transformer on image-text-action triplets to learn navigation-oriented textual representations useful for downstream VLN tasks.",
            "citation_title": "Towards learning a generic agent for vision-and-language navigation via pre-training",
            "mention_or_use": "use",
            "model_agent_name": "PREVALENT (pre-trained V&L BERT specialized for VLN)",
            "model_agent_description": "LXMERT-like architecture pre-trained from scratch on large quantities of image-text-action triplets with tasks including attended masked language modelling and action prediction to learn representations suited for navigation.",
            "pretraining_data_type": "Image-text-action triplets (vision-language paired with discrete navigation actions).",
            "pretraining_data_details": "Described in this paper as 'a large amount of image-text-action triplets' used to train PREVALENT from scratch to learn navigation-oriented textual representations (reference [16]). Exact dataset sizes/details are not specified in this paper.",
            "embodied_task_name": "Vision-and-Language Navigation (R2R) and used to initialize models fine-tuned on R2R and REVERIE",
            "embodied_task_description": "Transfer target tasks involve discrete panoramic navigation on Matterport3D (R2R) and remote referring expression (REVERIE) which also requires object grounding after navigation.",
            "action_space_text": "Discrete navigation actions used as labels in image-text-action triplets during pretraining (i.e., action prediction targets in PREVALENT pretraining).",
            "action_space_embodied": "Discrete navigable directions on connectivity graph plus stop action; same discrete action space used at fine-tuning.",
            "action_mapping_method": "PREVALENT pretraining includes an action-prediction objective (TRM-Lang3 outputs used for action prediction). When adapted into VLN⟲BERT the PREVALENT language encoder outputs are used as initial encoded language, and the model uses cross-modal encoders and visual transformers to map state+language+vision to actions (authors removed downstream EnvDrop and used V&L transformers directly).",
            "perception_requirements": "Panoramic RGB (36 views), scene features (ResNet-152 on Places365), and (for REVERIE) object region features (Faster-RCNN on Visual Genome); directional encodings for candidate views and objects.",
            "transfer_successful": true,
            "performance_with_pretraining": "VLN⟲BERT initialized from PREVALENT achieved on R2R Test Unseen: SR 63%, SPL 57%, NE 4.09 m, TL 12.35 m. On REVERIE Test Unseen PREVALENT-init achieved Navigation SR 18.77%, Navigation SPL 15.27%, RGS 29.61%, RGSPL 32.91% (Tables 1 & 2).",
            "performance_without_pretraining": "Compared to no pretraining initialization (same recurrent architecture but random init): R2R Test Unseen without pretraining: SR 51%, SPL 47%, NE 5.45 m, TL 11.15 m (VLN⟲BERT no-init OSCAR baseline in paper).",
            "sample_complexity_with_pretraining": "Authors report PREVALENT-initialized VLN⟲BERT reached its best reported result in ~1 day of wall-clock training in their learning-curve experiments (training regime elsewhere used up to 300k iterations for R2R fine-tuning).",
            "sample_complexity_without_pretraining": "Randomly initialized models require substantially more wall-clock time/iterations to reach similar performance; authors report OSCAR-initialized took ~3.5 days to reach best (600k-iter runs) and full runs took ~7 days in their experiments. Exact iteration counts to particular thresholds are not tabulated.",
            "sample_complexity_gain": "Marked wall-clock speedups: PREVALENT initialization reached peak performance in ~1 day vs multiple days for other initializations (authors' reported wall-clock reductions, not a single-fold numeric ratio).",
            "transfer_success_factors": "PREVALENT's explicit action-prediction pretraining and navigation-oriented image-text-action triplets provide structural priors and action grounding that match downstream VLN decision requirements.",
            "transfer_failure_factors": "Potential mismatch between low-level navigation instructions used in pretraining and higher-level instructions in target tasks (though PREVALENT still improved REVERIE), and differences in downstream decision architectures may require adaptation.",
            "key_findings": "Pretraining a V&L Transformer on image-text-action triplets (navigation-oriented objectives) substantially improves both final performance and training speed when fine-tuning for 3D embodied navigation, demonstrating the value of including action prediction signals in pretraining.",
            "uuid": "e1854.1",
            "source_info": {
                "paper_title": "VLN↻BERT: A Recurrent Vision-and-Language BERT for Navigation",
                "publication_date_yy_mm": "2020-11"
            }
        },
        {
            "name_short": "OSCAR",
            "name_full": "OSCAR (Object-Semantics Aligned Pre-training)",
            "brief_description": "A one-stream V&L BERT pre-trained to align object-level semantics and language by using object tags as anchors during image-text pretraining; used here as an initialization for VLN⟲BERT.",
            "citation_title": "Oscar: Object-semantics aligned pre-training for vision-language tasks",
            "mention_or_use": "use",
            "model_agent_name": "OSCAR (pre-trained V&L BERT)",
            "model_agent_description": "One-stream vision-language Transformer pre-trained with object-semantics alignment (object tags) on large image-caption datasets to produce strong visiolinguistic representations; in this paper an OSCAR checkpoint (without object tags) was used to initialize the recurrent VLN agent.",
            "pretraining_data_type": "Large-scale image-text pairs with object-tag alignment (vision-language pretraining).",
            "pretraining_data_details": "This paper states OSCAR was used (specifically an OSCAR checkpoint 'pre-trained without object tags'); the precise datasets and sizes are part of OSCAR but are not enumerated in this paper.",
            "embodied_task_name": "Vision-and-Language Navigation (R2R) and Remote Referring Expression (REVERIE) via initialization of VLN⟲BERT",
            "embodied_task_description": "Discrete panoramic navigation in Matterport3D (R2R) and REVERIE remote referring tasks requiring navigation then object grounding.",
            "action_space_text": "None (OSCAR pretraining uses image-text pairs; no explicit action labels in that pretraining as reported here).",
            "action_space_embodied": "Discrete navigable directions on connectivity graph plus stop action (as used in R2R/REVERIE fine-tuning).",
            "action_mapping_method": "OSCAR provides pre-trained visiolinguistic weights; adaptation consisted of projecting scene grid features into BERT token space and concatenating the recurrent state token and visual tokens for self-attention; actions are implemented/read out via averaged state-to-visual attention at final layer, with selected actions represented by directional encodings fed back into the recurrent state.",
            "perception_requirements": "Panoramic RGB grid features (ResNet-152), optional object region features for REVERIE (Faster-RCNN), directional encodings.",
            "transfer_successful": true,
            "performance_with_pretraining": "VLN⟲BERT initialized from OSCAR (init. OSCAR) achieved on R2R Test Unseen: SR 57%, SPL 53%, NE 4.59 m, TL 12.34 m. On REVERIE Test Unseen init. OSCAR: Navigation SR 14.20%, Navigation SPL 12.00%, RGS 24.62%, RGSPL 26.67% (Tables 1 & 2).",
            "performance_without_pretraining": "VLN⟲BERT with no OSCAR initialization (random init) on R2R Test Unseen: SR 51%, SPL 47%, NE 5.45 m, TL 11.15 m.",
            "sample_complexity_with_pretraining": "OSCAR-initialized model converged faster than random init in learning-curve experiments; the OSCAR-initialized run achieved its best result earlier (authors report best in ~3.5 days on the 600k-iteration learning-curve experiment).",
            "sample_complexity_without_pretraining": "Randomly initialized runs are slower/less sample-efficient, and show marked performance degeneration compared to OSCAR/PREVALENT initializations; exact iteration-to-threshold numbers not provided.",
            "sample_complexity_gain": "Moderate wall-clock/iteration speedup (OSCAR init reaches higher performance faster than random init); not quantified as precise factor in paper.",
            "transfer_success_factors": "Generic visiolinguistic features from OSCAR allow the recurrent BERT to leverage object/region-language correspondences and accelerate learning when fine-tuned for navigation.",
            "transfer_failure_factors": "OSCAR pretraining lacks explicit action prediction objective, so while it improves representation quality it is less sample-efficient/faster than PREVALENT which includes action-prediction pretraining.",
            "key_findings": "Generic V&L pretraining (OSCAR) improves VLN agent final performance and convergence speed compared to random init, but navigation-focused pretraining (PREVALENT) yields even greater gains.",
            "uuid": "e1854.2",
            "source_info": {
                "paper_title": "VLN↻BERT: A Recurrent Vision-and-Language BERT for Navigation",
                "publication_date_yy_mm": "2020-11"
            }
        },
        {
            "name_short": "PRESS",
            "name_full": "PRESS",
            "brief_description": "A method that fine-tunes a pre-trained language BERT to encode instructions for VLN (used as a baseline/related work).",
            "citation_title": "Robust navigation with language pretraining and stochastic sampling",
            "mention_or_use": "mention",
            "model_agent_name": "PRESS (BERT fine-tuned for instruction encoding)",
            "model_agent_description": "Fine-tunes a pre-trained language BERT to produce instruction encodings that are used by downstream navigation models (does not itself act as the full navigator BERT in this paper).",
            "pretraining_data_type": "Language BERT pretraining on large text corpora (language-only pretraining), then fine-tuned on navigation-related data for instruction encoding.",
            "pretraining_data_details": "This paper references PRESS as fine-tuning a pre-trained language BERT to encode instructions; exact datasets and sizes are not specified here.",
            "embodied_task_name": "Vision-and-Language Navigation (R2R) — used by prior works and reported in comparison tables",
            "embodied_task_description": "R2R navigation in Matterport3D environments using natural-language instructions to reach target viewpoints on a connectivity graph.",
            "action_space_text": "N/A (language BERT pretraining does not include action labels); downstream navigation model receives the encoded instructions and produces discrete navigation actions.",
            "action_space_embodied": "Discrete navigation directions over panoramic viewpoints plus stop action (as used in R2R).",
            "action_mapping_method": "PRESS provides instruction encodings to a downstream navigation policy which maps instruction encodings and visual observations to discrete navigable actions (details are in the PRESS paper; in this paper PRESS is cited as a baseline that uses BERT as language encoder).",
            "perception_requirements": "Panoramic RGB features and corresponding visual encoders used in downstream navigation model; specifics not detailed here.",
            "transfer_successful": null,
            "performance_with_pretraining": "Reported in paper's comparison table (baseline from literature): PRESS (test unseen) — TL 10.77 m, NE 5.49 m, SR 49%, SPL 45% (Table 1).",
            "performance_without_pretraining": "Not reported here; PRESS is a cited prior method rather than a direct ablation in this paper.",
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "Using a language-pretrained BERT to produce stronger instruction encodings can improve downstream navigation when these encodings are fine-tuned for VLN.",
            "transfer_failure_factors": "Using a pre-trained language encoder alone (without joint V&L pretraining or recurrent state modeling) may be insufficient to capture cross-modal and temporal dependencies required for navigation; paper reports that simply using pre-trained BERT as language encoder improves performance only if the BERT is fine-tuned for navigation.",
            "key_findings": "Language-only pretraining (fine-tuned BERT) helps instruction encoding for VLN baselines, but fully leveraging V&L pretraining and embedding recurrence into the model yields larger gains.",
            "uuid": "e1854.3",
            "source_info": {
                "paper_title": "VLN↻BERT: A Recurrent Vision-and-Language BERT for Navigation",
                "publication_date_yy_mm": "2020-11"
            }
        },
        {
            "name_short": "VLN-BERT",
            "name_full": "VLN-BERT (prior work)",
            "brief_description": "A prior approach that fine-tunes ViLBERT on instruction-trajectory pairs to score instruction-path compatibility, used as a reranking/compatibility scorer for VLN.",
            "citation_title": "Improving vision-and-language navigation with image-text pairs from the web",
            "mention_or_use": "mention",
            "model_agent_name": "VLN-BERT (ViLBERT fine-tuned for instruction-path compatibility)",
            "model_agent_description": "Fine-tunes a two-stream V&L model (ViLBERT) on instruction-trajectory pairs to produce compatibility scores used during search (beam search) in navigation.",
            "pretraining_data_type": "Vision-language pretraining on image-text pairs (ViLBERT pretraining), then fine-tuned on instruction-trajectory compatibility data.",
            "pretraining_data_details": "This paper cites VLN-BERT as fine-tuning ViLBERT on instruction-trajectory pairs to measure compatibility in beam search. The original pretraining/data sizes are not enumerated here.",
            "embodied_task_name": "Vision-and-Language Navigation (R2R) — used as compatibility scorer in prior work",
            "embodied_task_description": "R2R navigation where compatibility scoring between candidate trajectories and instructions assists search-based planning.",
            "action_space_text": "N/A for ViLBERT pretraining; fine-tuning uses instruction-trajectory pairs (compatibility labels) rather than direct action prediction.",
            "action_space_embodied": "Discrete navigable directions and stop action as used in R2R; VLN-BERT used as a scorer rather than the policy directly.",
            "action_mapping_method": "VLN-BERT is used to score complete instruction-trajectory pairs (compatibility) during beam search rather than mapping language tokens directly to low-level action outputs at each step.",
            "perception_requirements": "Vision features (region-based) and language instruction encodings; specifics come from ViLBERT/related works, not detailed here.",
            "transfer_successful": null,
            "performance_with_pretraining": null,
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "Using a pretrained V&L model to measure instruction-trajectory compatibility can improve search-based navigation (beam search) by leveraging cross-modal alignment.",
            "transfer_failure_factors": "As a compatibility scorer rather than an end-to-end policy, VLN-BERT's usefulness depends on the quality of the underlying trajectory proposals and the beam-search procedure.",
            "key_findings": "Prior work shows that fine-tuning large V&L models as scorers can aid navigation pipelines, but integrating recurrence and using the V&L Transformer as the navigator (as in this paper) yields stronger end-to-end performance.",
            "uuid": "e1854.4",
            "source_info": {
                "paper_title": "VLN↻BERT: A Recurrent Vision-and-Language BERT for Navigation",
                "publication_date_yy_mm": "2020-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Towards learning a generic agent for vision-and-language navigation via pre-training",
            "rating": 2
        },
        {
            "paper_title": "Oscar: Object-semantics aligned pre-training for vision-language tasks",
            "rating": 2
        },
        {
            "paper_title": "Robust navigation with language pretraining and stochastic sampling",
            "rating": 1
        },
        {
            "paper_title": "Improving vision-and-language navigation with image-text pairs from the web",
            "rating": 1
        },
        {
            "paper_title": "Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks",
            "rating": 1
        },
        {
            "paper_title": "Lxmert: Learning cross-modality encoder representations from transformers",
            "rating": 1
        }
    ],
    "cost": 0.024912249999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>VLN $\circlearrowleft$ BERT: A Recurrent Vision-and-Language BERT for Navigation</h1>
<p>Yicong Hong ${ }^{1} \quad$ Qi Wu ${ }^{2} \quad$ Yuankai Qi ${ }^{2} \quad$ Cristian Rodriguez-Opazo ${ }^{1,2} \quad$ Stephen Gould ${ }^{1}$<br>${ }^{1}$ The Australian National University ${ }^{2}$ The University of Adelaide<br>Australian Centre for Robotic Vision<br>{yicong.hong, cristian.rodriguez, stephen.gould}@anu.edu.au<br>qi.wu01@adelaide.edu.au, qykshr@gmail.com<br>Project URL: https://github.com/YicongHong/Recurrent-VLN-BERT</p>
<h4>Abstract</h4>
<p>Accuracy of many visiolinguistic tasks has benefited significantly from the application of vision-and-language (V\&amp;L) BERT. However, its application for the task of vision-and-language navigation (VLN) remains limited. One reason for this is the difficulty adapting the BERT architecture to the partially observable Markov decision process present in VLN, requiring history-dependent attention and decision making. In this paper we propose a recurrent BERT model that is time-aware for use in VLN. Specifically, we equip the BERT model with a recurrent function that maintains cross-modal state information for the agent. Through extensive experiments on R2R and REVERIE we demonstrate that our model can replace more complex encoder-decoder models to achieve state-of-the-art results. Moreover, our approach can be generalised to other transformer-based architectures, supports pre-training, and is capable of solving navigation and referring expression tasks simultaneously.</p>
<h2>1. Introduction</h2>
<p>Asking a robot to navigate in complex environments following human instructions has been a long-term goal in AI research. Recently, a great variety of vision-and-language navigation (VLN) setups [4, 47, 55] have been introduced for relevant studies and a large number of works explore different methods to leverage visual and language clues to assist navigation. For example, in the popular R2R navigation task [4], enhancing the learning of visual-textual correspondence is essential for the agent to correctly interpret the instruction and perceive the environment.</p>
<p>On the other hand, recent work on vision-and-language pre-training has achieved significant improvement over a wide range of visiolinguistic problems. Instead of designing complex and monolithic models for different tasks, those
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. Recurrent multi-layer Transformer for addressing partially observable inputs. A state token is defined along with the input sequence. At each time step, a new state representation $s_{t}$ will be generated based on the new observation. Meanwhile, the past information will help inferring a new decision $d_{t}$.
methods pre-train a multi-layer Transformer [56] on a large number of image-text pairs to learn generic cross-modal representations [9, 29, 31, 33, 36, 51, 53], known as V\&amp;L BERT (Bidirectional Encoder Representations from Transformers [12]). Such advances have inspired us to employ V\&amp;L BERT for VLN, replacing the complicated modules for modelling cross-modal relationships and allowing the learning of navigation to adequately benefit from the pretrained visual-textual knowledge. Unlike recent works on VLN, which apply a pre-trained V\&amp;L BERT only for encoding language [16, 32] or for measuring the instructionpath compatibility [40], we propose to use existing V\&amp;L BERT models themselves for learning to navigate.</p>
<p>However, an essential difference between VLN and other vision-and-language tasks is that VLN can be considered as a partially observable Markov decision process, in which future observations are dependent on the agent's current state and action. Meanwhile, at each navigational step, the visual observation only corresponds to partial instruction, requiring the agent to keep track of the navigation progress and correctly localise the relevant sub-instruction to gain useful information for decision making. Another difficulty of applying V\&amp;L BERT for VLN is the high demand on computational power; since the navigational episode could</p>
<p>be very long, performing self-attention on a long visual and textual sequence at each time step will cost an excessive amount of (GPU) memory during training.</p>
<p>To address the aforementioned problems, we propose a recurrent vision-and-language BERT for navigation, or simply VLN $\circlearrowleft$ BERT. Instead of employing large-scale datasets for pre-training which usually require thousands of GPU hours, the aim of this work is to allow the learning of VLN to adequately benefit from pre-trained V\&amp;L BERT. Based on the previously proposed V\&amp;L BERT models, we implement a recurrent function in their original architecture (Fig. 1) to model and leverage the history-dependent state representations, without explicitly defining a memory buffer [64] or applying any external recurrent modules such as an LSTM [18]. To reduce the memory consumption, we control the self-attention to consider the language tokens as keys and values but not queries during navigation, which is similar to the cross-modality encoder in LXMERT [53]. Such design greatly reduces the memory usage so that the entire model can be trained on a single GPU without performance degeneration. Furthermore, as in the original V\&amp;L BERT, our proposed model has the potential of multi-task learning, it is able to address other vision and language problems along with the navigation task.</p>
<p>We employ two datasets to evaluate the performance of our VLN $\circlearrowleft$ BERT, R2R [4] and REVERIE [47]. The chosen datasets are different in terms of the provided visual clues, the instructions and the goal. Our agent, initialised from a pre-trained V\&amp;L BERT and fine-tuned on the two datasets, achieves state-of-the-art results. We also initialise our model with the PREVALENT [16], a LXMERTlike model pre-trained for VLN. On the test split of R2R [4], it improves the Success Rate absolutely by $8 \%$ and achieves $57 \%$ Success weighted by Path Length (SPL). For the remote referring expression task in REVERIE [47], our agent obtains $23.99 \%$ navigation SPL and $13.51 \%$ Remote Grounding SPL. These results indicate the strong generalisation ability of our proposed VLN $\circlearrowleft$ BERT as well as the potential of using it for merging the learning of VLN with other vision and language tasks.</p>
<h2>2. Related Work</h2>
<p>Vision-and-Language Navigation Learning navigation with visual-linguistic clues has drawn significant research interests. The recent R2R [4] and Touchdown [8] datasets introduce human natural language as guidance and apply photo-realistic environments for navigation. Following these work, dialog-based navigation such as CVDN [55], VNLA [44] and HANNA [43], navigation for localising a remote object such as REVERIE [47], VLN in continuous environment [25], and multilingual navigation with spatialtemporal grounding such as RxR [27] have been proposed for further research.</p>
<p>One crucial challenge in VLN is to understand the visual-textual correspondence for decision making. To achieve this, Self-Monitoring [39] and RCM [58] adopt cross-modal attention to highlight the relevant observations and instruction at each step. Speaker-Follower [13] and EnvDrop [54] learn on the augmented training data via a selfsupervised manner. FAST [22] resorts self-correction navigation, while APS [14] samples adversarial paths for training to enhance the model's ability to generalise. AuxRN [63] applies several auxiliary losses to learn comprehensive representations, Qi et al. [46] and Wang et al. [57] also design loss functions to encourage the agent to follow the instructions to take the shortest paths. More recently, Hong et al. [19] propose a graph network to model the intra- and inter-modal relationships among the contextual and visual clues. The great improvements achieved by these methods encourage researchers to explore simpler and more powerful visiolinguistic learning network for VLN.
Visual BERT Pre-Training Following the success of pre-trained BERT on a wide range of natural language processing tasks [12], the model has been extended to process visual tokens and to pre-train on large-scale image/videotext pairs for learning generic visual-linguistic representations. Previous research introduce two-stream BERT models which encode texts and images separately, and fuse the two modalities in a later stage [36, 53], as well as onestream BERT models which directly perform inter-modal grounding [9, 29, 31, 33, 51]. Although video BERT approaches have been proposed to learn the correspondence between texts and video frames [30, 38, 52, 60], we are the first to integrate recurrence into BERT to learn partiallyobservable and temporal-dependent inputs. In terms of VLN pre-training, PRESS fine tunes a pre-trained language BERT to encode instructions [32], PREVALENT trains a V\&amp;L BERT on a large amount of image-text-action triplets from scratch to learn navigation-oriented textual representations [16], and VLN-BERT [40] fine-tunes a ViLBERT [36] on instruction-trajectory pairs to measure their compatibility in beam search setting. Unlike all previous work, our VLN $\circlearrowleft$ BERT can augment various V\&amp;L BERT models with recurrent function, it is a navigator network by itself that can be directly trained for navigation.
V\&amp;L Multi-Task Learning Instead of building a monolithic model for different V\&amp;L tasks, numbers of previous work explore multi-task learning with a unified model for utilising the common and the complementary knowledge to reduce the domain gap [34, 42, 45, 50, 36]. Very recently, 12-in-1 [37] trains a single ViLBERT [36] on 12 different datasets across four categories of V\&amp;L tasks, including visual question answering, referring expressions, multi-modal verification and caption-based image retrieval. In Vision-and-Language Navigation, Wang et al. [59] propose a multitask navigation model to address the R2R navigation [4] and</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. Schematics of the Recurrent Vision-and-Language BERT. At the initialisation stage, the entire instruction is encoded by a multi-layer Transformer, where the output feature of the [CLS] token serves as the initial state representation of the agent. During navigation, the concatenated sequence of state, encoded language and new visual observation is fed to the same Transformer to obtain the updated state and decision probabilities. The updated state and the language encoding from initialisation will be fused and applied as input at the next time step. The green star (<em>) indicates the </em>cross-modal matching<em> (Eq. 12) and the </em>past decision encoding<em> (Eq. 13) in </em>State Refinement*.</p>
<p>The Navigation from Dialog History (NDH) [55] problems. Comparing to previous methods, our VLN&gt;BERT uses a single network to address the navigation task and the remote referring expression task seamlessly in REVERIE [47].</p>
<h2>3. Proposed Model</h2>
<p>In this section, we first define the vision-and-language navigation task, then we revisit the BERT model [12] and present the architecture of our proposed VLN&gt;BERT.</p>
<h3>3.1. VLN Background</h3>
<p>The problem of VLN can be formulated as follows: Given a natural language instruction <em>U</em> which contains a sequence of words, at each time step <em>t</em>, the agent observes the environment and infers an action <em>a<sub>t</sub></em> that transfers the agent from state <em>s<sub>t</sub></em> to a new state <em>s<sub>t+1</sub></em>. The state consists of the navigational history and the current spatial position defined by a triplet 〈<em>C<sub>t</sub></em>, θ<em><sub>t</sub></em>, φ<em><sub>t</sub></em>〉, where <em>C<sub>t</sub></em> is a viewpoint on the pre-defined connectivity graph of the environment [4], and θ<em><sub>t</sub></em> and φ<em><sub>t</sub></em> are the angles of heading and elevation, respectively. The agent needs to execute a sequence of actions to navigate on the connectivity graph and eventually decides to stop at the target position to complete the task.</p>
<h3>3.2. Revisit BERT</h3>
<p>Bidirectional Encoder Representations from Transformers (BERT) [12] is a multi-layer Transformer architecture [56] designed to pre-train deep bidirectional language representations. Each layer of the Transformer encodes the language features from the previous layer <em>X<sub>l−1</sub></em> ∈ ℝ<sup>I×hd<sub>h</sub></sup> with multi-head self-attention to capture the dependencies among the <em>I</em> words in the sentence, and applies a residual feed-forward network to process the output features.</p>
<p>Formally, the <em>k</em>-th attention head at the <em>l</em>-th layer performs self-attention over <em>X<sub>l−1</sub></em> as</p>
<p>$$
\begin{aligned}
Q = \mathbf{X}<em l_k="l,k">{l-1}\mathbf{W}</em>}^Q, \mathbf{K} &amp;= \mathbf{X<em l_k="l,k">{l-1}\mathbf{W}</em>}^K, \mathbf{V} = \mathbf{X<em l_k="l,k">{l-1}\mathbf{W}</em>^V \
\mathbf{H}_{l,k} &amp;= \text{Softmax}\left(\frac{\mathbf{Q}\mathbf{K}^{\top}}{\sqrt{d_h}}\right)\mathbf{V}
\end{aligned}
\tag{1}
$$</p>
<p>where <em>W<sup>Q</sup></em>, <em>W<sup>K</sup></em> and <em>W<sup>V</sup></em> ∈ ℝ<sup>hd<sub>h</sub>×d<sub>h</sub></sup> are learnable linear projections<sup>1</sup> specifically for queries, keys and values; <em>d<sub>h</sub></em> is the hidden dimension of the network. The outputs from all the attention heads will be concatenated and projected onto the same dimension as the input as</p>
<p>$$
\mathbf{H}<em l_1="l,1">l = [\mathbf{H}</em>_l^O
\tag{2}
$$}; \dots; \mathbf{H}_{l,h}] \mathbf{W</p>
<p><sup>1</sup>All <em>W</em> in this section denotes learnable linear projections</p>
<p>where $h$ is the total number of heads, [; ] denotes concatenation and $\boldsymbol{W}^{O} \in \mathbb{R}^{h d_{h} \times h d_{h}}$ is a learned linear projection. Finally, the output of layer $l$ is formulated by</p>
<p>$$
\begin{aligned}
&amp; \boldsymbol{H}<em l="l">{l}^{\prime}=\operatorname{LayerNorm}\left(\boldsymbol{H}</em>}+\boldsymbol{X<em l="l">{l-1}\right) \
&amp; \boldsymbol{X}</em>}^{\prime}=\operatorname{ReLU}\left(\boldsymbol{H<em l="l">{l}^{\prime} \boldsymbol{W}</em>}^{F_{l}}\right) \boldsymbol{W<em 2="2">{l}^{F</em> \
&amp; \boldsymbol{X}}<em l="l">{l}=\operatorname{LayerNorm}\left(\boldsymbol{H}</em>\right)
\end{aligned}
$$}^{\prime}+\boldsymbol{X}_{l}^{\prime</p>
<p>where ReLU is the Rectified Linear Unit activation function and LayerNorm is layer normalisation [5].</p>
<p>Based on this architecture, BERT has been extended to V\&amp;L BERT [9, 29, 31, 33, 36, 51, 53], which takes the concatenation of language tokens and visual tokens as input, and pre-trains on image-text corpus to learn generic visiolinguistic representations.</p>
<h3>3.3. Recurrent VLN BERT</h3>
<p>The idea of our VLN $\circlearrowright$ BERT can be adapted to a wide range of Transformer-based networks. In this section we apply the recently proposed one-stream V\&amp;L BERT model OSCAR [33] for demonstration. We modify the model to enable the learning of navigation and the associated referring expression (REF) task. As shown in Fig. 2, at each time step, the network takes four sets of tokens as input; the previous state token $\boldsymbol{s}<em t="t">{t-1}$, the language tokens $\boldsymbol{X}$, the visual tokens $\boldsymbol{V}</em>}$ for scene, and the visual tokens $\boldsymbol{O<em t="t">{t}$ for objects (only in REVERIE [47]). Then, it performs self-attention over these cross-modal tokens to capture the textual-visual correspondence for inferring the action probabilities $\boldsymbol{p}</em>$ (only in REVERIE [47]):}^{o}$ and the object grounding probabilities $\boldsymbol{p}_{t}^{o</p>
<p>$$
\boldsymbol{s}<em t="t">{t}, \boldsymbol{p}</em>}^{o}, \boldsymbol{p<em t-1="t-1">{t}^{o}=\operatorname{VLN} \circlearrowright \operatorname{BERT}\left(\boldsymbol{s}</em>}, \boldsymbol{X}, \boldsymbol{V<em t="t">{t}, \boldsymbol{O}</em>\right)
$$</p>
<p>Language Processing At initialisation $(t=0)$, a sequence of words consisting of the classification token [CLS], the language tokens of the instruction $\boldsymbol{U}$ and the separation token [SEP] will be fed into VLN $\circlearrowright$ BERT, where [CLS] and [SEP] are pre-defined in BERT models. In the pretraining of OSCAR [33], the [CLS] token is applied for aggregating relevant visiolinguistic clues from the input sequence for contrastive learning. Here, we defined the embedded [CLS] token as the initial state representation $\boldsymbol{s}_{0}$, to inherit such function - initialise an agent's state which is aware of the entire navigation task.</p>
<p>$$
\boldsymbol{s}_{0}, \boldsymbol{X}=\operatorname{VLN} \circlearrowright \operatorname{BERT}(\text { [CLS], } \boldsymbol{U},[\mathrm{SEP}]) \quad \text { (8) }
$$</p>
<p>During navigation steps $(t&gt;0)$, unlike the state token $\boldsymbol{s}<em t="t">{t}$ or the visual tokens $\boldsymbol{V}</em>}$ and $\boldsymbol{O<em t="t">{t}$ which performs self-attention with respect to the entire input sequence, the language tokens $\boldsymbol{X}$ only serve as the keys and values in the Transformer. We consider the language tokens produced by the model at the initialisation step as a deep representation of the instruction which does not need to be further encoded in
later steps. Not updating the language features also save a huge amount of computational resources since the instruction and the trajectory can be long in VLN problems.
Vision Processing At each navigation step $(t&gt;0)$, the agent makes new visual observation in the environment and uses the visual clues to assist navigation. To process the visual clues, the network first projects the image features of views at the navigable directions $\boldsymbol{I}</em>}^{e}$ to the same space as the BERT token as $\boldsymbol{V<em t="t">{t}=\boldsymbol{I}</em>$. Then, the visual tokens will be concatenated with the state token and the language tokens, and fed into the model.}^{e} \boldsymbol{W}^{I^{e}</p>
<p>In terms of the remote REF task [47], we simply consider the object features $\boldsymbol{I}<em t="t">{t}^{o}$ as additional visual tokens in the input sequence. Similarly, the features will be projected onto the token space as $\boldsymbol{O}</em>}=\boldsymbol{I<em t="t">{t}^{e} \boldsymbol{W}^{I^{e}}$ and fed into the model. The object clues can provide valuable information about the important landmarks on the path, which could be very helpful to the navigation with high-level instructions [47].
State Representation We formulate the agent's state at each time step $\boldsymbol{s}</em>}$ as the summary of all textual and visual clues that the agent collects, as well as all decisions that the agent makes until the current viewpoint. Instead of explicitly defining a memory buffer [64] or implementing an additional recurrent network [18] to store the past experiences, our model relies on BERT's original architecture to recognise time-dependent inputs, and recurrently updates $\boldsymbol{s<em k="k" l_="l,">{0}$ from initialisation to represent the state. At each navigation step, the state representation is used as the leading input token of the entire textual-visual sequence. It then performs intermodal self-attention in VLN $\circlearrowright$ BERT with other tokens to update its content and becomes the leading token of the input at the next step, in an autoregressive way.
State Refinement Unlike most of the V\&amp;L BERT models which apply the output feature of the [CLS] token for classification, our state is not directly used for inferring a decision (see following Decision Making subsection), which means, the vanilla state representation is not explicitly enforced to capture the most important language and visual features. To address this issue, our model matches the raw textual and visual tokens, and feeds the output to the state representation. Formally, let $\boldsymbol{Q}</em>$ be the state and textual tokens at head $k$ of the final $(l=12)$ layer of VLN $\circlearrowright$ BERT, the attention scores over the textual tokens can be expressed as:}^{s}$ and $\boldsymbol{K}_{l, k}^{s</p>
<p>$$
\boldsymbol{A}<em k="k" l_="l,">{l, k}^{s, x}=\frac{\boldsymbol{Q}</em>}^{s} \boldsymbol{K<em h="h">{l, k}^{s, \top}}{\sqrt{d</em>
$$}}</p>
<p>Then, we average the scores over all the attention heads $(K=12)$ and apply a Softmax function to get the overall state-language attention weights as:</p>
<p>$$
\widehat{\boldsymbol{A}}<em l="l">{l}^{s, x}=\operatorname{Softmax}\left(\widehat{\boldsymbol{A}}</em>\right)
$$}^{s, x}\right)=\operatorname{Softmax}\left(\frac{1}{K} \sum_{k=1}^{K} \boldsymbol{A}_{l, k}^{s, x</p>
<p>Similarly, the visual attention scores $\boldsymbol{A}<em l="l">{t, k}^{s, v}$ and weights $\widetilde{\boldsymbol{A}}</em>$ can be obtained. Now, we perform a weighted sum over the input textual tokens and visual tokens respectively to obtain the weighted raw features as:}^{s, v</p>
<p>$$
\boldsymbol{F}<em l="l">{t}^{x}=\widetilde{\boldsymbol{A}}</em>}^{s, x} \boldsymbol{X} \quad \text { and } \quad \boldsymbol{F<em l="l">{t}^{v}=\widetilde{\boldsymbol{A}}</em>
$$}^{s, v} \boldsymbol{V}_{t</p>
<p>We then enforce a cross-modal matching between the raw textual and visual features via element-wise product and send such information to the agent's state as:</p>
<p>$$
\boldsymbol{s}<em t="t">{t}^{f}=\left[\boldsymbol{s}</em>}^{r} ; \boldsymbol{F<em t="t">{t}^{x} \odot \boldsymbol{F}</em>
$$}^{v}\right] \boldsymbol{W}^{r</p>
<p>where $\boldsymbol{s}<em t="t">{t}^{r}$ is the output state features at the final layer. Notice that in REVERIE [47], only the visual features is sent to the state representation, i.e., Eq. 12 becomes $\boldsymbol{s}</em>}^{f}=$ $\left[\boldsymbol{s<em t="t">{t}^{r} ; \boldsymbol{F}</em>$. This is because the navigational instructions in REVERIE are high-level, hence performing step-wise matching between the raw textual and visual features is less valuable.}^{v}\right] \boldsymbol{W}^{r</p>
<p>Finally, past decisions are important for the agent to keep track of the navigation progress, our network records the new decision by feeding the directional features of the selected action $\boldsymbol{a}_{t}$ into the state token as:</p>
<p>$$
\boldsymbol{s}<em t="t">{t}=\left[\boldsymbol{s}</em>
$$}^{f} ; \boldsymbol{a}_{t}\right] \boldsymbol{W}^{s</p>
<p>where $\boldsymbol{s}_{t}$ is the new representation of the agent's state at time step $t$.</p>
<p>Decision Making Many previous VLN agents apply an inner product between the state representation and the visual features at candidate directions to evaluate the statevision correspondence, and choose a direction with the highest matching score to navigate [39, 54]. We find that the BERT network can nicely perform such scoring because it is fully built upon the inner product based soft-attention. Inspired by the method of predicting alignment between regions and phrases in VisualBERT [31], we directly apply the mean attention weights of the visual tokens over all the attention heads in the last layer, with respect to the state, as the action probabilities, simply $\boldsymbol{p}<em l="l">{t}^{a}=\widetilde{\boldsymbol{A}}</em>}^{s, v}$ (as defined in Eq. 10). As for the remote referring expression task [47], our agent uses the same method to select an object. The selection probabilities can be expressed as $\boldsymbol{p<em l="l">{t}^{a}=\widetilde{\boldsymbol{A}}</em>$ is the mean attention weights for all candidate objects. We refer the Appendix §B. 2 for more details.}^{s, o}$, where $\widetilde{\boldsymbol{A}}_{l}^{s, o</p>
<h3>3.4. Training</h3>
<p>We train our network with a mixture of reinforcement learning (RL) and imitation learning (IL) objectives. We apply A2C [41] for RL, in which the agent samples an action according to $\boldsymbol{p}<em t="t">{t}^{a}$ and measures the advantage $\boldsymbol{A}</em>$ at each step (we refer the Appendix §B. 5 for more details about RL.). In</p>
<p>IL, our agent navigates on the ground-truth trajectory by following teacher actions and calculates a cross-entropy loss for each decision. Formally, we minimise the navigation loss function, expressed for each given sample, as</p>
<p>$$
\mathcal{L}=-\sum_{t} a_{t}^{s} \log \left(p_{t}^{a}\right) A_{t}-\lambda \sum_{t} a_{t}^{*} \log \left(p_{t}^{a}\right)
$$</p>
<p>where $a_{t}^{s}$ is the sampled action and $a_{t}^{<em>}$ is the teacher action. Here $\lambda$ is a coefficient for weighting the IL loss. In REVERIE [47], we applied an additional cross-entropy term $\sum_{t} o_{t}^{</em>} \log \left(p_{t}^{a}\right)$ to learn object grounding.</p>
<h3>3.5. Adaptation</h3>
<p>We initialise the parameters of VLN $\circlearrowright$ BERT from OSCAR [33] pre-trained without object tags. Although OSCAR is trained on regional features, we find that it is also compatible with the grid features of the entire scene. When adapting to the LXMERT-like [53] model in PREVALENT [16], we remove the language branch in the crossmodality encoder and concatenate the state token with the visual tokens for self-attention (see Appendix §B. 3 for schematics). We also remove the entire downstream network EnvDrop [54], including the Speaker and the environmental dropout, and then directly fine-tune the model pretrained by PREVALENT for navigation.</p>
<h2>4. Experiments</h2>
<p>Implementation Details All experiments are conducted on a single NVIDIA 2080Ti GPU, the learning rate is fixed to $10^{-5}$ throughout the training and AdamW optimiser [35] is applied. For R2R, we train the agent directly on the mixture of the original training data and the augmented data from PREVALENT [16], the batch size ${ }^{2}$ is set to 16 and the network is trained for 300,000 iterations. For REVERIE, we use batch size ${ }^{2} 8$ and train the agent for 200,000 iterations. Images in the environments are encoded by a ResNet-152 [17] pre-trained on Places365 [62], and objects are encoded by a Faster-RCNN [49] pre-trained on the Visual Genome [26]. Early stopping is applied when the training saturates, the model which achieves the highest SPL in validation unseen split is adopted for testing.</p>
<p>Evaluation Metrics We apply the standard metrics employed by previous works to evaluate the performance.</p>
<p>R2R [4] considers Trajectory Length (TL): the average path length in meters, Navigation Error (NE): the average distance between agent's final position and the target in meters, Success Rate (SR): the ratio of stopping within 3 meters to the target, and Success weighted by the normalised inverse of the Path Length (SPL) [2].</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th>Methods</th>
<th>R2R Validation Seen</th>
<th></th>
<th></th>
<th></th>
<th>R2R Validation Unseen</th>
<th></th>
<th></th>
<th></th>
<th>R2R Test Unseen</th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>TL</td>
<td>$\mathrm{NE} \downarrow$</td>
<td>SR $\uparrow$</td>
<td>SPL $\uparrow$</td>
<td>TL</td>
<td>$\mathrm{NE} \downarrow$</td>
<td>SR $\uparrow$</td>
<td>SPL $\uparrow$</td>
<td>TL</td>
<td>$\mathrm{NE} \downarrow$</td>
<td>SR $\uparrow$</td>
<td>SPL $\uparrow$</td>
<td></td>
</tr>
<tr>
<td>Random</td>
<td>9.58</td>
<td>9.45</td>
<td>16</td>
<td>-</td>
<td>9.77</td>
<td>9.23</td>
<td>16</td>
<td>-</td>
<td>9.89</td>
<td>9.79</td>
<td>13</td>
<td>12</td>
<td></td>
</tr>
<tr>
<td>Human</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>11.85</td>
<td>1.61</td>
<td>86</td>
<td>76</td>
<td></td>
</tr>
<tr>
<td>Seq2Seq-SF [4]</td>
<td>11.33</td>
<td>6.01</td>
<td>39</td>
<td>-</td>
<td>8.39</td>
<td>7.81</td>
<td>22</td>
<td>-</td>
<td>8.13</td>
<td>7.85</td>
<td>20</td>
<td>18</td>
<td></td>
</tr>
<tr>
<td>Speaker-Follower [13]</td>
<td>-</td>
<td>3.36</td>
<td>66</td>
<td>-</td>
<td>-</td>
<td>6.62</td>
<td>35</td>
<td>-</td>
<td>14.82</td>
<td>6.62</td>
<td>35</td>
<td>28</td>
<td></td>
</tr>
<tr>
<td>SMNA [39]</td>
<td>-</td>
<td>3.22</td>
<td>67</td>
<td>58</td>
<td>-</td>
<td>5.52</td>
<td>45</td>
<td>32</td>
<td>18.04</td>
<td>5.67</td>
<td>48</td>
<td>35</td>
<td></td>
</tr>
<tr>
<td>RCM+SIL (train) [58]</td>
<td>10.65</td>
<td>3.53</td>
<td>67</td>
<td>-</td>
<td>11.46</td>
<td>6.09</td>
<td>43</td>
<td>-</td>
<td>11.97</td>
<td>6.12</td>
<td>43</td>
<td>38</td>
<td></td>
</tr>
<tr>
<td>PRESS [32] $\dagger$</td>
<td>10.57</td>
<td>4.39</td>
<td>58</td>
<td>55</td>
<td>10.36</td>
<td>5.28</td>
<td>49</td>
<td>45</td>
<td>10.77</td>
<td>5.49</td>
<td>49</td>
<td>45</td>
<td></td>
</tr>
<tr>
<td>FAST-Short [22]</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>21.17</td>
<td>4.97</td>
<td>56</td>
<td>43</td>
<td>22.08</td>
<td>5.14</td>
<td>54</td>
<td>41</td>
<td></td>
</tr>
<tr>
<td>EnvDrop [54]</td>
<td>11.00</td>
<td>3.99</td>
<td>62</td>
<td>59</td>
<td>10.70</td>
<td>5.22</td>
<td>52</td>
<td>48</td>
<td>11.66</td>
<td>5.23</td>
<td>51</td>
<td>47</td>
<td></td>
</tr>
<tr>
<td>AuxRN [63]</td>
<td>-</td>
<td>3.33</td>
<td>70</td>
<td>67</td>
<td>-</td>
<td>5.28</td>
<td>55</td>
<td>50</td>
<td>-</td>
<td>5.15</td>
<td>55</td>
<td>51</td>
<td></td>
</tr>
<tr>
<td>PREVALENT [16] $\dagger$</td>
<td>10.32</td>
<td>3.67</td>
<td>69</td>
<td>65</td>
<td>10.19</td>
<td>4.71</td>
<td>58</td>
<td>53</td>
<td>10.51</td>
<td>5.30</td>
<td>54</td>
<td>51</td>
<td></td>
</tr>
<tr>
<td>RelGraph [19]</td>
<td>10.13</td>
<td>3.47</td>
<td>67</td>
<td>65</td>
<td>9.99</td>
<td>4.73</td>
<td>57</td>
<td>53</td>
<td>10.29</td>
<td>4.75</td>
<td>55</td>
<td>52</td>
<td></td>
</tr>
<tr>
<td>Ours (no init. OSCAR)</td>
<td>9.78</td>
<td>3.92</td>
<td>62</td>
<td>59</td>
<td>10.31</td>
<td>5.10</td>
<td>50</td>
<td>46</td>
<td>11.15</td>
<td>5.45</td>
<td>51</td>
<td>47</td>
<td></td>
</tr>
<tr>
<td>Ours (init. OSCAR)</td>
<td>10.79</td>
<td>3.11</td>
<td>71</td>
<td>67</td>
<td>11.86</td>
<td>4.29</td>
<td>59</td>
<td>53</td>
<td>12.34</td>
<td>4.59</td>
<td>57</td>
<td>53</td>
<td></td>
</tr>
<tr>
<td>Ours (init. PREVALENT)</td>
<td>11.13</td>
<td>2.90</td>
<td>72</td>
<td>68</td>
<td>12.01</td>
<td>3.93</td>
<td>63</td>
<td>57</td>
<td>12.35</td>
<td>4.09</td>
<td>63</td>
<td>57</td>
<td></td>
</tr>
</tbody>
</table>
<p>Table 1. Comparison of agent performance on R2R in single-run setting. $\dagger$ work that applies pre-trained BERT for language encoding.</p>
<table>
<thead>
<tr>
<th>Methods</th>
<th>REVERIE Validation Seen</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th>REVERIE Validation Unseen</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th>REVERIE Test Unseen</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Navigation</td>
<td></td>
<td></td>
<td>RGS $\uparrow$</td>
<td>RGSPL $\uparrow$</td>
<td>Navigation</td>
<td></td>
<td></td>
<td>RGS $\uparrow$</td>
<td>RGSPL $\uparrow$</td>
<td>Navigation</td>
<td></td>
<td></td>
<td>RGS $\uparrow$</td>
<td>RGSPL $\uparrow$</td>
<td></td>
</tr>
<tr>
<td></td>
<td>SR $\uparrow$</td>
<td>OSR $\uparrow$</td>
<td>SPL $\uparrow$</td>
<td>TL</td>
<td></td>
<td>SR $\uparrow$</td>
<td>OSR $\uparrow$</td>
<td>SPL $\uparrow$</td>
<td>TL</td>
<td></td>
<td>SR $\uparrow$</td>
<td>OSR $\uparrow$</td>
<td>SPL $\uparrow$</td>
<td>TL</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Random</td>
<td>2.74</td>
<td>8.92</td>
<td>1.91</td>
<td>11.99</td>
<td>1.97</td>
<td>1.31</td>
<td>1.76</td>
<td>11.93</td>
<td>1.01</td>
<td>10.76</td>
<td>0.96</td>
<td>0.56</td>
<td>2.30</td>
<td>8.88</td>
<td>1.44</td>
<td>10.34</td>
</tr>
<tr>
<td>Human</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>81.51</td>
<td>86.83</td>
<td>53.66</td>
<td>21.18</td>
</tr>
<tr>
<td>Seq2Seq-SF [4]</td>
<td>29.59</td>
<td>35.70</td>
<td>24.01</td>
<td>12.88</td>
<td>18.97</td>
<td>14.96</td>
<td>4.20</td>
<td>8.07</td>
<td>2.84</td>
<td>11.07</td>
<td>2.16</td>
<td>1.63</td>
<td>3.99</td>
<td>6.88</td>
<td>3.09</td>
<td>10.89</td>
</tr>
<tr>
<td>RCM [58]</td>
<td>23.33</td>
<td>29.44</td>
<td>21.82</td>
<td>10.70</td>
<td>16.23</td>
<td>15.36</td>
<td>9.29</td>
<td>14.23</td>
<td>6.97</td>
<td>11.98</td>
<td>4.89</td>
<td>3.89</td>
<td>7.84</td>
<td>11.68</td>
<td>6.67</td>
<td>10.60</td>
</tr>
<tr>
<td>SMNA [39]</td>
<td>41.25</td>
<td>43.29</td>
<td>39.61</td>
<td>7.54</td>
<td>30.07</td>
<td>28.98</td>
<td>8.15</td>
<td>11.28</td>
<td>6.44</td>
<td>9.07</td>
<td>4.54</td>
<td>3.61</td>
<td>5.80</td>
<td>8.39</td>
<td>4.53</td>
<td>9.23</td>
</tr>
<tr>
<td>FAST-Short [22]</td>
<td>45.12</td>
<td>49.68</td>
<td>40.18</td>
<td>13.22</td>
<td>31.41</td>
<td>28.11</td>
<td>10.08</td>
<td>20.48</td>
<td>6.17</td>
<td>29.70</td>
<td>6.24</td>
<td>3.97</td>
<td>14.18</td>
<td>23.36</td>
<td>8.74</td>
<td>30.69</td>
</tr>
<tr>
<td>FAST-MATTN [47]</td>
<td>50.53</td>
<td>55.17</td>
<td>45.50</td>
<td>16.35</td>
<td>31.97</td>
<td>29.66</td>
<td>14.40</td>
<td>28.20</td>
<td>7.19</td>
<td>45.28</td>
<td>7.84</td>
<td>4.67</td>
<td>19.88</td>
<td>30.63</td>
<td>11.61</td>
<td>39.05</td>
</tr>
<tr>
<td>Ours (no init. OSCAR)</td>
<td>39.56</td>
<td>42.09</td>
<td>36.29</td>
<td>12.06</td>
<td>21.15</td>
<td>19.39</td>
<td>25.76</td>
<td>29.28</td>
<td>22.16</td>
<td>14.52</td>
<td>11.62</td>
<td>9.87</td>
<td>18.52</td>
<td>20.18</td>
<td>15.47</td>
<td>14.09</td>
</tr>
<tr>
<td>Ours (init. OSCAR)</td>
<td>39.85</td>
<td>41.32</td>
<td>35.86</td>
<td>12.85</td>
<td>24.46</td>
<td>22.28</td>
<td>25.53</td>
<td>27.66</td>
<td>21.06</td>
<td>14.35</td>
<td>14.20</td>
<td>12.00</td>
<td>24.62</td>
<td>26.67</td>
<td>19.48</td>
<td>14.88</td>
</tr>
<tr>
<td>Ours (init. PREVALENT)</td>
<td>51.79</td>
<td>53.90</td>
<td>47.96</td>
<td>13.44</td>
<td>38.23</td>
<td>35.61</td>
<td>30.67</td>
<td>20.02</td>
<td>24.90</td>
<td>16.78</td>
<td>18.77</td>
<td>15.27</td>
<td>29.61</td>
<td>32.91</td>
<td>23.99</td>
<td>15.86</td>
</tr>
</tbody>
</table>
<p>Table 2. Comparison of agent performance of navigation and remote referring expression on REVERIE. REVERIE [47] defines Success Rate (SR) as the ratio of stopping at a viewpoint where the target object is visible (in panorama), and considers the corresponding SPL. It also employ Oracle Success Rate (OSR): the ratio of having a viewpoint along the trajectory where the target object is visible, Remote Grounding Success Rate (RGS): the ratio of grounding to the correct objects when stopped, and RGSPL, which weights RGS by the trajectory length.</p>
<h3>4.1. Main Results</h3>
<p>Comparison with SoTA Results in Table 1 compare the single-run (greedy search, no pre-exploration [58]) performance of different agents on the R2R benchmark. Our proposed VLN $\circlearrowright$ BERT initialised from OSCAR [33] (init. OSCAR) performs better than previous methods across all the dataset splits. Comparing to a randomly initialised network (no init. OSCAR), the large performance degeneration suggests that the pre-trained general vision-linguistic knowledge significantly benefits the learning of navigation. The model initialised from PREVALENT [16], pre-trained especially for VLN, further improves the agent's performance, achieving $63 \%$ SR $(+8 \%)$ and $57 \%$ SPL $(+5 \%)$ on the test unseen split ${ }^{3}$. Comparing to PRESS [32] and PREVALENT [16] which only fine-tune a pre-trained BERT for extracting language features, adding recurrence into V\&amp;L BERT and using the model directly as the navigator network allows the VLN learning to adequately benefit from the pre-trained knowledge. Such performance gain cannot be achieved by using pre-trained V\&amp;L BERT only as a feature extractor, as will be shown in $\S 4.2$ Ablation Study. Moreover, the large gain in SR with a slight increase in TL suggests that the agent is able to navigate both accurately and efficiently. Comparing to previous methods, we can see that the performance gap between the validation unseen and the test unseen splits is greatly reduced, which means our agent has a stronger generalisation ability to novel instructions and environments.</p>
<p>In REVERIE [47] (Table 2), our VLN $\circlearrowright$ BERT (init. OSCAR) generalises much better to unseen data. On the validation unseen split, the SR of navigation and object grounding has been absolutely improved by $11.13 \%$ and $6.36 \%$ respectively. On the test unseen split ${ }^{3}$, our method ob-</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup> <sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>: ${ }^{3}$ R2R Leaderboard: https://evalai.cloudcv.org/web/challenges/challenge-page/97/overview, REVERIE Leaderboard: https://eval.ai/web/challenges/challenge-page/606/overview</p>
<p>tains 24.62% SR and 19.48% SPL for navigation, as well as 12.65% RGS and 10.00% RGSPL for REF, achieving a better performance than the previous best [47] which applies SoTA navigator FAST [22] for navigation and pointer MATTN [61] for object grounding. Compare our model with OSCAR initialisation to without, the navigation results on the validation splits are similar, but the navigation on the test split and the object grounding across all the data splits are largely improved. This result also suggests that it is possible to apply a BERT-based model for VLN and REF multi-task learning. Although the previous method has higher OSR, it is likely due to longer searching (long TL), the lower SR suggests that the agent does not know where to stop correctly. In Table 2, we also present the performance of VLN$\circlearrowright$BERT initialised from PREVALENT [16], which achieves the best result across almost all of the metrics in all dataset splits. It is very interesting to see that although PREVALENT is pre-trained on low-level R2R instructions [4] without other V&amp;L knowledge, it significantly boosts the navigation with high-level instructions as well as the object grounding in REVERIE. We hypothesis that the pre-trained knowledge provides the model with some structural priors, while the learning of REF is strongly influenced by navigation, especially at the early training stage where the target object is rarely observable by the agent.</p>
<p>Visualisation of Language Attention To demonstrate that our VLN$\circlearrowright$BERT (init. OSCAR) is able to trace the navigation progress, we visualise the changes of language attention weights at the final Transformer layer over all instructions during navigation (Fig. 3). As the agent moves forward, the attention weights with respect to state shifts from the beginning of the instructions to the end. Since the sub-instructions and sub-paths for each sample in R2R is monotonically aligned [20], our results indicate that the state nicely records the partial instruction that has been completed. In terms of the attention weights with respect to the visual token at the select direction, it follows a similar pattern meaning that the most relevant part of the instruction is used for guiding the action selection.</p>
<h3>4.2 Ablation Study</h3>
<p>Network Components Table 3 shows comprehensive ablation experiments on the influence of using V&amp;L BERT (init. OSCAR) to replace or to add the key network components in the baseline model (EnvDrop [54]). The baseline model consists of a language encoder, a visual encoder, a state LSTM and a decision making module, corresponding to the columns of Language, Vision, State and Decision in Table 3, respectively. E.g., Model #3 replaces the language encoder and visual encoder in baseline with a V&amp;L BERT. For fair comparison, all models in the table are trained with</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3. Averaged attention weights over all instructions in validation unseen split during navigation. State: Attention weights with respect to the state representation. Selected Action: Attention weights with respect to the visual token at the selected direction.</p>
<p>the same data and training strategy as our VLN$\circlearrowright$BERT. As the results suggested, our proposed method is a multifunctional framework, the more network components it covers, the larger performance gain it achieves. Comparing the baseline with Model #1 and #2, we can see that employing a pre-trained BERT as language encoder improves the performance only if the BERT is fine-tuned for navigation. This finding is also supported by using the V&amp;L BERT to encode both the textual and visual signals (Model #3 and #4). However, simply using pre-trained V&amp;L BERT as text and image encoders does not fully utilise its power; model #5 indicates that relying on the original architecture of BERT to learn recurrence is feasible and it is able to achieve better results. Moreover, using the averaged visual attention weights of the final layer of the Transformer as the action probabilities (Model #6) and enhancing the state representation with visual-textual matching as defined in Eq. 12 (full model) further improves the agent's performance.</p>
<p>Self-Attended Language Features Due to the long instructions and episodes, high memory cost during training is one of the key issues that prevents previous research to apply BERT for self-attention at every time step. To demonstrate the influence of self-attending textual features during navigation, we compare the agent's performance and the training time GPU memory consumption (constrained to a single 11GB memory GPU) of re-attending the language at each step. As shown in Table 4, training for Emb-Attn, Init-Attn and Re-Attn consume much more memory for each sample than performing language self-attention only at initialisation (Ours), and their performances are worse than Ours. The results of Re-Attn degenerates significantly because at each time step the output language features aggregate the most relevant visual-textual clues at a certain viewpoint, which suppresses the valuable information in other part of the instruction for the future steps. We refer Appendix §C.2 for experiment on language self-attention with larger batch size by applying gradient accumulation.</p>
<table>
<thead>
<tr>
<th>Models</th>
<th>V&amp;L BERT (init. OSCAR)</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th>R2R Validation Seen</th>
<th></th>
<th></th>
<th></th>
<th>R2R Validation Unseen</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Language</td>
<td>Vision</td>
<td>State</td>
<td>Decision</td>
<td>Matching</td>
<td>Train</td>
<td>TL</td>
<td>NE$\downarrow$</td>
<td>SR$\uparrow$</td>
<td>SPL$\uparrow$</td>
<td>TL</td>
<td>NE$\downarrow$</td>
<td>SR$\uparrow$</td>
<td>SPL$\uparrow$</td>
</tr>
<tr>
<td>Baseline [54]</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>11.84</td>
<td>4.44</td>
<td>57.79</td>
<td>52.85</td>
<td>12.50</td>
<td>5.26</td>
<td>49.81</td>
<td>43.45</td>
</tr>
<tr>
<td>1</td>
<td>✓</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>10.81</td>
<td>4.98</td>
<td>49.95</td>
<td>46.19</td>
<td>11.34</td>
<td>5.68</td>
<td>44.15</td>
<td>39.64</td>
</tr>
<tr>
<td>2</td>
<td>✓</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>✓</td>
<td>11.73</td>
<td>4.18</td>
<td>59.26</td>
<td>54.12</td>
<td>12.59</td>
<td>5.00</td>
<td>52.11</td>
<td>45.75</td>
</tr>
<tr>
<td>3</td>
<td>✓</td>
<td>✓</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>9.26</td>
<td>6.85</td>
<td>34.77</td>
<td>33.33</td>
<td>8.92</td>
<td>7.43</td>
<td>30.74</td>
<td>29.05</td>
</tr>
<tr>
<td>4</td>
<td>✓</td>
<td>✓</td>
<td></td>
<td></td>
<td></td>
<td>✓</td>
<td>11.37</td>
<td>3.50</td>
<td>67.97</td>
<td>63.94</td>
<td>12.98</td>
<td>4.73</td>
<td>54.75</td>
<td>48.31</td>
</tr>
<tr>
<td>5</td>
<td>✓</td>
<td>✓</td>
<td>✓</td>
<td></td>
<td></td>
<td>✓</td>
<td>11.10</td>
<td>3.81</td>
<td>65.52</td>
<td>61.24</td>
<td>12.20</td>
<td>4.62</td>
<td>55.21</td>
<td>49.72</td>
</tr>
<tr>
<td>6</td>
<td>✓</td>
<td>✓</td>
<td>✓</td>
<td>✓</td>
<td></td>
<td>✓</td>
<td>10.70</td>
<td>3.21</td>
<td>70.32</td>
<td>66.45</td>
<td>11.46</td>
<td>4.48</td>
<td>57.22</td>
<td>52.57</td>
</tr>
<tr>
<td>Full model</td>
<td>✓</td>
<td>✓</td>
<td>✓</td>
<td>✓</td>
<td>✓</td>
<td>✓</td>
<td>10.79</td>
<td>3.11</td>
<td>71.11</td>
<td>67.23</td>
<td>11.86</td>
<td>4.29</td>
<td>58.71</td>
<td>53.41</td>
</tr>
</tbody>
</table>
<p>Table 3. Ablation experiments on the effect of applying V&amp;L BERT for learning navigation. Checkmarks indicate using V&amp;L BERT to replace or to add the corresponding network component in the baseline model. Matching indicates the cross-modal matching (Eq. 12), and Train with checkmark means the V&amp;L BERT is fine-tuned for navigation.</p>
<table>
<thead>
<tr>
<th>Models</th>
<th>R2R Validation Seen</th>
<th></th>
<th></th>
<th></th>
<th>R2R Validation Unseen</th>
<th></th>
<th></th>
<th></th>
<th>Batch Size</th>
<th>Memory</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>TL</td>
<td>NE$\downarrow$</td>
<td>SR$\uparrow$</td>
<td>SPL$\uparrow$</td>
<td>TL</td>
<td>NE$\downarrow$</td>
<td>SR$\uparrow$</td>
<td>SPL$\uparrow$</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Emb-Attn</td>
<td>10.84</td>
<td>3.40</td>
<td>67.19</td>
<td>63.19</td>
<td>11.31</td>
<td>4.64</td>
<td>55.60</td>
<td>51.00</td>
<td>6</td>
<td>11.0GB</td>
</tr>
<tr>
<td>Init-Attn</td>
<td>10.21</td>
<td>3.93</td>
<td>61.80</td>
<td>58.06</td>
<td>10.39</td>
<td>4.59</td>
<td>53.47</td>
<td>49.05</td>
<td>6</td>
<td>11.0GB</td>
</tr>
<tr>
<td>Re-Attn</td>
<td>8.99</td>
<td>5.81</td>
<td>40.84</td>
<td>39.44</td>
<td>8.85</td>
<td>6.22</td>
<td>37.76</td>
<td>35.76</td>
<td>6</td>
<td>11.0GB</td>
</tr>
<tr>
<td>Ours</td>
<td>10.79</td>
<td>3.11</td>
<td>71.11</td>
<td>67.23</td>
<td>11.86</td>
<td>4.29</td>
<td>58.71</td>
<td>53.41</td>
<td>16</td>
<td>9.2GB</td>
</tr>
</tbody>
</table>
<p>Table 4. Comparison of performing language self-attention: On the raw word embeddings at each step (Emb-Attn), on the initialised language features at each step (Init-Attn), on the output language features from previous step (Re-Attn), or only at initialisation (Ours). Memory is the training time GPU memory cost.</p>
<p>Learning Curves As shown in Fig. 4, we compare the learning curves of VLN $\circlearrowright$ BERT initialised from different models. The training losses of our method initialised from pre-trained OSCAR [33] converges faster than a randomly initialised model, and it reaches much higher SPL in both validation seen and unseen environments. Moreover, our model initialised from PREVALENT [16] learns significantly faster than the other two methods and it is able to achieve a much better performance within much fewer iterations. In terms of training in real time, the model init. OSCAR takes about 7 days to complete 600,000 iterations of training (best result achieved in 3.5 days), while the model init. PREVALENT takes about 4.5 days (best result achieved in 1 day). Using wall-clock time in Fig. 4 as the x-axis will enhance the discrepancy between the three models. These results suggest that the pre-trained generic visiolinguistic knowledge is beneficial to the learning of VLN, and pre-training especially for navigation skills allows the agent to learn better in fine-tuning.</p>
<h2>5. Conclusion</h2>
<p>In this paper, we introduce recurrence into Vision-and-Language BERT and rely on its original architecture to recognise time-dependent inputs. Such innovation allows V&amp;L BERT to address problems with a partially observ-</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4. Comparison of the learning curves. no init. means randomly initialised network parameters.</p>
<p>able Markov decision process, and allows the learning of downstream tasks to adequately benefit from the pretrained generic V&amp;L knowledge. For VLN, our proposed VLN $\circlearrowright$ BERT applies BERT itself as the navigator network, which achieves SoTA performance in R2R [4] and REVERIE [47]. Moreover, results suggest that V&amp;L BERT with recurrence is capable of VLN and REF multi-tasking.</p>
<p>Future Work As suggested by the significant improvement on R2R [4] achieved by our VLN $\circlearrowright$ BERT, we expect that the model can improve the performance of other navigation settings such as street navigation [8] and navigation in continuous environments [25]. In this paper, we only apply our recurrent BERT for VLN. However, we believe that it has a huge potential in addressing other tasks which require sequential interactions/decisions, such as language and visual dialog [6, 10, 24, 28, 48], dialog navigation [11, 43, 44] and action anticipation for reactive robot response [1, 15, 23].</p>
<h2>References</h2>
<p>[1] Mohammad Sadegh Aliakbarian, Fatemeh Sadat Saleh, Mathieu Salzmann, Basura Fernando, Lars Petersson, and Lars Andersson. Viena2: A driving anticipation dataset. In Asian Conference on Computer Vision, pages 449-466. Springer, 2018. 8
[2] Peter Anderson, Angel Chang, Devendra Singh Chaplot, Alexey Dosovitskiy, Saurabh Gupta, Vladlen Koltun, Jana Kosecka, Jitendra Malik, Roozbeh Mottaghi, Manolis Savva, et al. On evaluation of embodied navigation agents. arXiv preprint arXiv:1807.06757, 2018. 5
[3] Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei Zhang. Bottom-up and top-down attention for image captioning and visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6077-6086, 2018. 12
[4] Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko Sünderhauf, Ian Reid, Stephen Gould, and Anton van den Hengel. Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 36743683, 2018. 1, 2, 3, 5, 6, 7, 8, 12, 13
[5] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. 4
[6] Bill Byrne, Karthik Krishnamoorthi, Chinnadhurai Sankar, Arvind Neelakantan, Ben Goodrich, Daniel Duckworth, Semih Yavuz, Amit Dubey, Kyu-Young Kim, and Andy Cedilnik. Taskmaster-1: Toward a realistic and diverse dialog dataset. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4516-4525, Hong Kong, China, Nov. 2019. Association for Computational Linguistics. 8
[7] Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Niebner, Manolis Savva, Shuran Song, Andy Zeng, and Yinda Zhang. Matterport3d: Learning from rgb-d data in indoor environments. In 2017 International Conference on 3D Vision (3DV), pages 667-676. IEEE, 2017. 12
[8] Howard Chen, Alane Suhr, Dipendra Misra, Noah Snavely, and Yoav Artzi. Touchdown: Natural language navigation and spatial reasoning in visual street environments. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 12538-12547, 2019. 2, 8
[9] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. Uniter: Universal image-text representation learning. In European Conference on Computer Vision, 2020. 1, 2, 4
[10] Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, José MF Moura, Devi Parikh, and Dhruv Batra. Visual dialog. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 326-335, 2017. 8
[11] Harm de Vries, Kurt Shuster, Dhruv Batra, Devi Parikh, Jason Weston, and Douwe Kiela. Talk the walk: Navigating new york city through grounded dialogue. arXiv preprint arXiv:1807.03367, 2018. 8
[12] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, 2019. 1, 2, 3
[13] Daniel Fried, Ronghang Hu, Volkan Cirik, Anna Rohrbach, Jacob Andreas, Louis-Philippe Morency, Taylor BergKirkpatrick, Kate Saenko, Dan Klein, and Trevor Darrell. Speaker-follower models for vision-and-language navigation. In Advances in Neural Information Processing Systems, pages 3314-3325, 2018. 2, 6, 12
[14] Tsu-Jui Fu, Xin Eric Wang, Matthew Peterson, Scott Grafton, Miguel Eckstein, and William Yang Wang. Counterfactual vision-and-language navigation via adversarial path sampler. In European Conference on Computer Vision (ECCV), 2020. 2
[15] Antonino Furnari and Giovanni Maria Farinella. What would you expect? anticipating egocentric actions with rollingunrolling lstms and modality attention. In Proceedings of the IEEE International Conference on Computer Vision, pages 6252-6261, 2019. 8
[16] Weituo Hao, Chunyuan Li, Xiujun Li, Lawrence Carin, and Jianfeng Gao. Towards learning a generic agent for vision-and-language navigation via pre-training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13137-13146, 2020. 1, 2, 5, 6, 7, 8, 12, 13
[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770-778, 2016. 5, 12
[18] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735-1780, 1997. 2, 4
[19] Yicong Hong, Cristian Rodriguez, Yuankai Qi, Qi Wu, and Stephen Gould. Language and visual entity relationship graph for agent navigation. Advances in Neural Information Processing Systems, 33, 2020. 2, 6, 12
[20] Yicong Hong, Cristian Rodriguez, Qi Wu, and Stephen Gould. Sub-instruction aware vision-and-language navigation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 3360-3376, Online, Nov. 2020. Association for Computational Linguistics. 7, 15
[21] Gabriel Ilharco, Vihan Jain, Alexander Ku, Eugene Ie, and Jason Baldridge. General evaluation for instruction conditioned navigation using dynamic time warping. arXiv preprint arXiv:1907.05446, 2019. 13, 14
[22] Liyiming Ke, Xiujun Li, Yonatan Bisk, Ari Holtzman, Zhe Gan, Jingjing Liu, Jianfeng Gao, Yejin Choi, and Siddhartha Srinivasa. Tactical rewind: Self-correction via backtracking in vision-and-language navigation. In Proceedings of the</p>
<p>IEEE Conference on Computer Vision and Pattern Recognition, pages 6741-6749, 2019. 2, 6, 7
[23] Hema S Koppula and Ashutosh Saxena. Anticipating human activities using object affordances for reactive robotic response. IEEE transactions on pattern analysis and machine intelligence, 38(1):14-29, 2015. 8
[24] Satwik Kottur, José M. F. Moura, Devi Parikh, Dhruv Batra, and Marcus Rohrbach. CLEVR-dialog: A diagnostic dataset for multi-round reasoning in visual dialog. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 582-595, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. 8
[25] Jacob Krantz, Erik Wijmans, Arjun Majumdar, Dhruv Batra, and Stefan Lee. Beyond the nav-graph: Vision-and-language navigation in continuous environments. In European Conference on Computer Vision, 2020. 2, 8
[26] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. International Journal of Computer Vision, 123(1):32-73, 2017. 5, 12
[27] Alexander Ku, Peter Anderson, Roma Patel, Eugene Ie, and Jason Baldridge. Room-across-room: Multilingual vision-and-language navigation with dense spatiotemporal grounding. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4392-4412, 2020. 2
[28] S Lee, H Schulz, A Atkinson, J Gao, K Suleman, L El Asri, M Adada, M Huang, S Sharma, W Tay, et al. Multi-domain task-completion dialog challenge. Dialog System Technology Challenges, 8, 2019. 8
[29] Gen Li, Nan Duan, Yuejian Fang, Ming Gong, Daxin Jiang, and Ming Zhou. Unicoder-vl: A universal encoder for vision and language by cross-modal pre-training. In Association for the Advancement of Artificial Intelligence, pages 1133611344, 2020. 1, 2, 4
[30] Linjie Li, Yen-Chun Chen, Yu Cheng, Zhe Gan, Licheng Yu, and Jingjing Liu. HERO: Hierarchical encoder for Video+Language omni-representation pre-training. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2046-2065, Online, Nov. 2020. Association for Computational Linguistics. 2
[31] Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. Visualbert: A simple and performant baseline for vision and language. arXiv preprint arXiv:1908.03557, 2019. 1, 2, 4, 5
[32] Xiujun Li, Chunyuan Li, Qiaolin Xia, Yonatan Bisk, Asli Celikylimaz, Jianfeng Gao, Noah A Smith, and Yejin Choi. Robust navigation with language pretraining and stochastic sampling. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 1494-1499, 2019. 1, 2, 6
[33] Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, et al. Oscar: Object-semantics aligned pre-training for vision-language tasks. In European Conference on Computer Vision, pages 121-137. Springer, 2020. 1, 2, 4, 5, 6, 8
[34] Yikang Li, Nan Duan, Bolei Zhou, Xiao Chu, Wanli Ouyang, Xiaogang Wang, and Ming Zhou. Visual question generation as dual task of visual question answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6116-6124, 2018. 2
[35] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2018. 5
[36] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. In Advances in Neural Information Processing Systems, pages 13-23, 2019. 1, 2, 4
[37] Jiasen Lu, Vedanuj Goswami, Marcus Rohrbach, Devi Parikh, and Stefan Lee. 12-in-1: Multi-task vision and language representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10437-10446, 2020. 2
[38] Huaishao Luo, Lei Ji, Botian Shi, Haoyang Huang, Nan Duan, Tianrui Li, Xilin Chen, and Ming Zhou. Univilm: A unified video and language pre-training model for multimodal understanding and generation. arXiv preprint arXiv:2002.06353, 2020. 2
[39] Chih-Yao Ma, Jiasen Lu, Zuxuan Wu, Ghassan AlRegib, Zsolt Kira, Richard Socher, and Caiming Xiong. Selfmonitoring navigation agent via auxiliary progress estimation. In Proceedings of the International Conference on Learning Representations (ICLR), 2019. 2, 5, 6, 12
[40] Arjun Majumdar, Ayush Shrivastava, Stefan Lee, Peter Anderson, Devi Parikh, and Dhruv Batra. Improving vision-and-language navigation with image-text pairs from the web. In Proceedings of the European Conference on Computer Vision, 2020. 1, 2
[41] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International conference on machine learning, pages 1928-1937, 2016. 5, 13
[42] Day-Kien Nguyen and Takayuki Okatani. Multi-task learning of hierarchical vision-language representation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 10492-10501, 2019. 2
[43] Khanh Nguyen and Hal Daumé III. Help, anna! visual navigation with natural multimodal assistance via retrospective curiosity-encouraging imitation learning. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 684-695, 2019. 2, 8
[44] Khanh Nguyen, Debadeepta Dey, Chris Brockett, and Bill Dolan. Vision-based navigation with language-based assistance via imitation learning with indirect intervention. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 12527-12537, 2019. 2, 8</p>
<p>[45] Subhojeet Pramanik, Priyanka Agrawal, and Aman Hussain. Omninet: A unified architecture for multi-modal multi-task learning. arXiv preprint arXiv:1907.07804, 2019. 2
[46] Yuankai Qi, Zizheng Pan, Shengping Zhang, Anton van den Hengel, and Qi Wu. Object-and-action aware model for visual language navigation. In European Conference on Computer Vision, 2020. 2
[47] Yuankai Qi, Qi Wu, Peter Anderson, Xin Wang, William Yang Wang, Chunhua Shen, and Anton van den Hengel. Reverie: Remote embodied visual referring expression in real indoor environments. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9982-9991, 2020. 1, 2, 3, 4, 5, 6, 7, 8, 12
[48] Abhinav Rastogi, Xiaoxue Zang, Srinivas Sunkara, Raghav Gupta, and Pranav Khaitan. Towards scalable multi-domain conversational agents: The schema-guided dialogue dataset. In AAAI, 2020. 8
[49] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. In Advances in neural information processing systems, pages 91-99, 2015. 5, 12
[50] Kurt Shuster, Da Ju, Stephen Roller, Emily Dinan, Y-Lan Boureau, and Jason Weston. The dialogue dodecathlon: Open-domain knowledge and image grounded conversational agents. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 24532470, Online, July 2020. Association for Computational Linguistics. 2
[51] Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai. V1-bert: Pre-training of generic visuallinguistic representations. In International Conference on Learning Representations, 2019. 1, 2, 4
[52] Chen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, and Cordelia Schmid. Videobert: A joint model for video and language representation learning. In Proceedings of the IEEE International Conference on Computer Vision, pages 74647473, 2019. 2
[53] Hao Tan and Mohit Bansal. Lxmert: Learning crossmodality encoder representations from transformers. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLPIJCNLP), pages 5103-5114, 2019. 1, 2, 4, 5, 13
[54] Hao Tan, Licheng Yu, and Mohit Bansal. Learning to navigate unseen environments: Back translation with environmental dropout. In Proceedings of NAACL-HLT, pages 2610-2621, 2019. 2, 5, 6, 7, 8, 12, 13
[55] Jesse Thomason, Michael Murray, Maya Cakmak, and Luke Zettlemoyer. Vision-and-dialog navigation. In Conference on Robot Learning, pages 394-406, 2020. 1, 2, 3
[56] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pages 5998-6008, 2017. 1, 3
[57] Hu Wang, Qi Wu, and Chunhua Shen. Soft expert reward learning for vision-and-language navigation. In European Conference on Computer Vision, 2020. 2
[58] Xin Wang, Qiuyuan Huang, Asli Celikyilmaz, Jianfeng Gao, Dinghan Shen, Yuan-Fang Wang, William Yang Wang, and Lei Zhang. Reinforced cross-modal matching and selfsupervised imitation learning for vision-language navigation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6629-6638, 2019. 2, 6
[59] Xin Wang, Vihan Jain, Eugene Ie, William Yang Wang, Zornitsa Kozareva, and Sujith Ravi. Environment-agnostic multitask learning for natural language grounded navigation. In European Conference on Computer Vision, 2020. 2
[60] Zekun Yang, Noa Garcia, Chenhui Chu, Mayu Otani, Yuta Nakashima, and Haruo Takemura. Bert representations for video question answering. In The IEEE Winter Conference on Applications of Computer Vision, pages 1556-1565, 2020. 2
[61] Licheng Yu, Zhe Lin, Xiaohui Shen, Jimei Yang, Xin Lu, Mohit Bansal, and Tamara L Berg. Mattnet: Modular attention network for referring expression comprehension. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1307-1315, 2018. 7, 12
[62] Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: A 10 million image database for scene recognition. IEEE transactions on pattern analysis and machine intelligence, 40(6):1452-1464, 2017. 5, 12
[63] Fengda Zhu, Yi Zhu, Xiaojun Chang, and Xiaodan Liang. Vision-language navigation with self-supervised auxiliary reasoning tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10012-10022, 2020. 2, 6
[64] Wang Zhu, Hexiang Hu, Jiacheng Chen, Zhiwei Deng, Vihan Jain, Eugene Ie, and Fei Sha. BabyWalk: Going farther in vision-and-language navigation by taking baby steps. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2539-2556. Association for Computational Linguistics, 2020. 2, 4</p>
<h2>Appendices</h2>
<h2>A. Datasets</h2>
<p>We evaluate the performance of our proposed model on two distinct datasets for VLN:</p>
<ul>
<li>Room-to-Room (R2R) [4]: The agent is required to navigate in photo-realistic environments (Matterport3D [7]) to reach a target following low-level natural language instructions. Most of the previous works apply a panoramic action space for navigation [13], where the agent jumps among viewpoints pre-defined on the connectivity graph of the environment. The dataset contains 61 scenes for training; 11 and 18 scenes for validation and testing, respectively, in unseen environments.</li>
<li>REVERIE [47]: The agent needs to first navigate to a point where the target object is visible, then, it needs to identify the target object from a list of given candidates. In REVERIE, the navigational instructions are high-level while the instructions for object grounding are very specific. The dataset has in total 4,140 target objects in 489 categories, and each target viewpoint has 7 objects with 50 bounding boxes in average.</li>
</ul>
<h2>B. Implementation Details</h2>
<p>We provide the implementation details of preparing visual features ( $\S 3.3^{5}$ ), decision making (§3.3), adaptation to PREVALENT [16] (§3.3 \&amp; 3.5), adaptation to REVERIE [47] (§3.3 \&amp; 3.5), critic function and reward shaping in reinforcement learning (§3.4).</p>
<h2>B.1. Visual Features ( $\S 3.3^{5}$ )</h2>
<p>Navigation in R2R [4] and REVERIE [47] are conducted in the Matterport3D Simulator [7]. At each navigable viewpoint in the environment, the agent observes a $360^{\circ}$ panorama, consisting of 36 single-view images at 12 headings ( $30^{\circ}$ separation) and 3 elevation angles $\left( \pm 30^{\circ}\right)$.</p>
<p>Scene Features In our experiments, we only consider the scene features (grid features of the single-view images provided by ResNet-152 [17] pre-trained on Places365 [62]) at the navigable directions as visual features to VLN $\circlearrowright$ BERT. Each visual feature is direction-aware, which is formulated by the concatenation of the convolutional image feature $\boldsymbol{f}<em t="t">{t}^{v, i} \in \mathbb{R}^{2048}$ and the directional encoding $\boldsymbol{d}</em>$ as:}^{i} \in \mathbb{R}^{128</p>
<p>$$
\boldsymbol{I}<em t="t">{t}^{v, i}=\left[\boldsymbol{f}</em>\right]
$$}^{v, i} ; \boldsymbol{d}_{t}^{i</p>
<p>The directional encoding is formed by replicating vector $\left(\cos \theta_{t}^{i}, \sin \theta_{t}^{i}, \cos \phi_{t}^{i}, \sin \phi_{t}^{i}\right)$ by 32 times, where $\theta_{t}^{i}$ and $\phi_{t}^{i}$</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup>represents the heading and elevation angles of the image with respect to the agent's orientation [13, 54]. Moreover, the action features $\boldsymbol{a}<em t="t">{t}$ which is fed to the state representation is exactly the directional encoding at the selected direction $\boldsymbol{d}</em>$.}^{c}$, where $c=a_{t}^{s</p>
<p>Object Features In REVERIE [47], the target objects can appear at any single-view of the panorama, we extract the object features (regional features encoded by FasterRCNN [49] pre-trained on Visual Genome [26] by Anderson et al. [3]) according to the positions of objects provided in REVERIE [47]. The object features are position- and direction-aware, formulated as</p>
<p>$$
\boldsymbol{I}<em t="t">{t}^{o, k}=\left[\boldsymbol{f}</em>}^{o, k} ; \boldsymbol{p<em t="t">{t}^{k} \boldsymbol{W}^{p} ; \boldsymbol{d}</em>\right]
$$}^{k</p>
<p>where $\boldsymbol{f}<em t="t">{t}^{o, k} \in \mathbb{R}^{2048}$ is the convolutional object features, $\boldsymbol{d}</em>}^{k} \in \mathbb{R}^{128}$ is the directional encoding of the single-view which contains the object. $\boldsymbol{p<em t="t">{t}^{k}$ represents the spatial position of the object within the image, as in MATTN [61], we apply $\boldsymbol{p}</em>$ is a learnable linear projection.}^{k}=\left[\frac{x_{t l}}{W}, \frac{y_{t l}}{H}, \frac{x_{t c}}{W}, \frac{y_{t c}}{W}, \frac{w}{W} h\right]$, where $\left(x_{t l}, y_{t l}\right)$ and $\left(x_{b r}, y_{b r}\right)$ are the top-left and bottom-right coordinates of the object, $(w, h)$ and $(W, H)$ are the width and height of the object and the image, respectively. The matrix $\boldsymbol{W}^{p} \in$ $\mathbb{R}^{5 \times 128</p>
<h2>B.2. Decision Making (§3.3)</h2>
<p>In R2R [4], there are two types of decisions that an agent infers during navigation; it either selects a navigable direction to move, or it decides to stop at the current viewpoint. As in most of the previous work, stopping in VLN $\circlearrowright$ BERT is implemented by adding a zero vector $\boldsymbol{v}_{t}^{\text {stop }}$ to the list of visual features at navigable directions $[13,19,39,54]$, as</p>
<p>$$
\boldsymbol{V}<em t="t">{t}=\left\langle\boldsymbol{v}</em>}^{1}, \boldsymbol{v<em t="t">{t}^{2}, \ldots, \boldsymbol{v}</em>}^{n}, \boldsymbol{v<em t="t">{t}^{\text {stop }} \mid \boldsymbol{v}</em>\right\rangle
$$}^{i} \in \mathbb{R}^{2176</p>
<p>Our VLN $\circlearrowright$ BERT determines to stop by predicting the largest attention score to the stop representation at the final transformer layer. Otherwise, the agent will move to a navigable direction with the largest score.</p>
<p>However, in REVERIE [47], we directly apply the attention scores over the candidate objects for stopping. To be specific, the visual tokens in REVERIE consists of sequences of scene features and object features:</p>
<p>$$
\left\langle\boldsymbol{v}<em t="t">{t}^{1}, \boldsymbol{v}</em>}^{2}, \ldots, \boldsymbol{v<em t="t">{t}^{n}, \boldsymbol{o}</em>}^{1}, \boldsymbol{o<em t="t">{t}^{2}, \ldots, \boldsymbol{o}</em>}^{m} \mid \boldsymbol{v<em t="t">{t}^{i} \in \boldsymbol{V}</em>}, \boldsymbol{o<em t="t">{t}^{k} \in \boldsymbol{O}</em>\right\rangle
$$</p>
<p>When the model predicts larger attention scores for at least one of the object token than all of the scene tokens, the agent will stop and will select the object with the largest score as the grounded object for REF. Such formulation has two advantages; First, it relates the object searching process to navigation, i.e., the agent should not stop navigation if it has low confidence of localising the target object.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5. Adaptation to recurrent PREVALENT. At initialisation, the entire instruction is encoded by a language transformer (TRM-Lang1), where the output feature of the [CLS] token servers as the initial state representation of the agent. During navigation, the concatenated sequence of state, encoded language and new visual observation are fed to the cross-modality and the single-modality encoders to obtain the updated state and decision probabilities. The updated state and the language encoding from initialisation will be fused and applied as input at the next time step. The green star (*) indicates the cross-modal matching and the past decision encoding (§3.3).</p>
<p>Second, it allows the reinforcement learning to benefit the object grounding, since the action logits for stop is the greatest attention scores over objects.</p>
<h3>B.3. Adaptation to PREVALENT (§3.3 &amp; §3.5)</h3>
<p>As shown in Fig. 5, we adapt our VLN&gt;BERT to the LXMERT-like [53] architecture in PREVALENT [16]. At initialisation, the transformer TRM-Lang1 encodes the instruction <em>U</em> and uses the output features of the [CLS] token to represent agent's initial state. During navigation, the concatenated sequence of the previous state <em>s</em><sub><em>t</em>−1</sub>, the encoded language from initialisation <em>X</em> and the new visual observation <em>V</em><sub><em>t</em></sub> will be fed to the cross-modality encoder to obtain the language-aware state feature <em>s</em><sub><em>t</em></sub><sup><em>L</em>−1</sup> and the language-aware visual features <em>V</em><sub><em>t</em></sub><sup><em>L</em></sup>. Finally, TRM-Vis2 will process <em>s</em><sub><em>t</em></sub><sup><em>L</em>−1</sup> and <em>V</em><sub><em>t</em></sub><sup><em>L</em></sup> to produce a new state <em>s</em><sub><em>t</em></sub> and a decision <em>p</em><sub><em>t</em></sub>.</p>
<p>Pre-training of PREVALENT [16] applies the outputs from TRM-Lang3 for attended masked language modelling and action prediction, but fine-tuning on R2R [4] only use the output language features from TRM-Lang1 and rely on a downstream network EnvDrop [54] for navigation. In contrast, our method does not require any downstream network, we leverage the visual transformers TRM-Vis1 and TRM-Vis2 to learn state-language-vision relationship for better decision marking.</p>
<p>Cross-Modal Matching The cross-modal matching for State Refinement (see Eq. 12 in §3.3) is applied to enhance the state representation of PREVALENT-based VLN&gt;BERT. Since the final transformer TRM-Vis2 only process the state and visual features, we apply the averaged attention scores over the visual features with respect to <em>s</em><sub><em>t</em></sub> to weight the input visual tokens <em>V</em><sub><em>t</em></sub>, but apply the averaged attention scores over the textual features with respect to <em>s</em><sub><em>t</em></sub><sup><em>L</em>−1</sup> to weight the input language tokens <em>X</em>. Results in Table 5 show that cross-modal matching for state improves the agent's performance in unseen environments.</p>
<table>
<thead>
<tr>
<th>Models</th>
<th>R2R Validation Seen</th>
<th></th>
<th></th>
<th></th>
<th>R2R Validation Unseen</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>TL</td>
<td>NE↓</td>
<td>SR↑</td>
<td>SPL↑</td>
<td>TL</td>
<td>NE↓</td>
<td>SR↑</td>
<td>SPL↑</td>
</tr>
<tr>
<td>w/o Matching</td>
<td>10.87</td>
<td>2.44</td>
<td>76.79</td>
<td>73.13</td>
<td>11.72</td>
<td>4.08</td>
<td>62.07</td>
<td>56.15</td>
</tr>
<tr>
<td>with Matching</td>
<td>11.13</td>
<td>2.90</td>
<td>72.18</td>
<td>67.72</td>
<td>12.01</td>
<td>3.93</td>
<td>62.75</td>
<td>56.84</td>
</tr>
</tbody>
</table>
<p>Table 5. Performance of PREVALENT-based VLN&gt;BERT with and without cross-modal matching for agent's state.</p>
<h3>B.4. Critic Function (§3.4)</h3>
<p>We apply the A2C [41] for reinforcement learning. At each time step, the critic, a multi-layer perceptron, predicts an expected value from the updated state representation as:</p>
<p>$$e_t = (\text{ReLU}(\mathbf{s}_t \mathbf{W}^{E1}))\mathbf{W}^{E2} \tag{19}$$</p>
<p>where ReLU is the Rectified Linear Unit function, <em>W</em><sup><em>E</em>1</sup> and <em>W</em><sup><em>E</em>2</sup> are learnable linear projections.</p>
<h3>B.5. Reward Shaping (§3.4)</h3>
<p>In addition to the progress rewards defined in EnvDrop [54], we apply the normalised dynamic time warping [21] as a part of the reward to encourage the agent to follow the instruction to navigate. Moreover, we introduce a negative reward to penalise the agent if it misses the target.</p>
<p><strong>Progress Reward</strong> As formulated in the EnvDrop [54], we apply the progress rewards as strong supervision signals for directing the agent to approach the target. To be specific, let <em>D</em><sub><em>t</em></sub> to be the distance from agent to target at time step <em>t</em>, and Δ<em>D</em><sub><em>t</em></sub> = <em>D</em><sub><em>t</em></sub> − <em>D</em><sub><em>t</em>−1</sub> to be the change of distance by action <em>a</em><sub><em>t</em></sub>, the reward at each step (<em>a</em><sub><em>t</em></sub> ≠ stop) is defined as:</p>
<p>$$r_t^{D,\text{step}} = \begin{cases} + 1.0, &amp; \text{Δ}D_t &gt; 0.0 \ - 1.0, &amp; \text{otherwise} \end{cases} \tag{20}$$</p>
<p>When the agent decides to stop (<em>a</em><sub><em>t</em></sub> = stop), a final reward is assigned depending on if the agent successfully complete the task:</p>
<p>$$r_t^{D,\text{final}} = \begin{cases} + 2.0, &amp; D_t &lt; 3.0 \ - 2.0, &amp; \text{otherwise} \end{cases} \tag{21}$$</p>
<p>Overall, the agent will receive a positive reward if it approaches the target and completes the task (by stopping within 3 meters to the target viewpoint), while it will be penalised with a negative reward if it moves away from the target or stops at a wrong viewpoint.</p>
<p>Path Fidelity Rewards The Progress Reward encourages the agent to approach the target, but it does not constrain the agent to take the shortest path. As there could be multiple routes to the target, the agent could learn to take a longer path or even a cyclic path to maximise the total reward. To address the problem, we apply the normalised dynamic time warping reward [21], a measurement of the similarity between the ground-truth path and the predicted path, to urge the agent to follow the instruction accurately. Let $P_{t}$ be the normalised dynamic time warping [21] at time $t$, and $\Delta P_{t}=P_{t}-P_{t-1}$ to be the change of $P$ caused by action $a_{t}$, the reward for $a_{t} \neq$ stop is defined as:</p>
<p>$$
r_{t}^{P, \text { step }}=\Delta P_{t}
$$</p>
<p>and the reward for $a_{t}=$ stop:</p>
<p>$$
r_{t}^{P, \text { final }}=\left{\begin{array}{lc}
+2.0 P_{t}, &amp; D_{t}&lt;3.0 \
+0.0, &amp; \text { otherwise }
\end{array}\right.
$$</p>
<p>Moreover, as suggested by many previous works, there exists a large discrepancy between the agent's oracle success rate and success rate, indicating that it does not learn well to stop accurately. To address this issue, we introduce a negative stopping reward $r_{t}^{S}$ which will be triggered whenever the agent first approaches the target but then departs from it. To be precise, if $D_{t-1} \leqslant 1.0$ and $\Delta D_{t}&gt;0.0$ :</p>
<p>$$
r_{t}^{S}=-2.0 \times\left(1.0-D_{t-1}\right)
$$</p>
<p>The normalised dynamic time warping reward $r_{t}^{P}$ and the stopping reward $r_{t}^{S}$ together form the path fidelity rewards which can encourage the agent to navigate efficiently. In summary, the overall reward at each step during navigation can be expressed as:</p>
<p>$$
r_{t}= \begin{cases}r_{t}^{D, \text { step }}+r_{t}^{P, \text { step }}+r_{t}^{S}, &amp; a_{t} \neq \text { stop } \ r_{t}^{D, \text { final }}+r_{t}^{P, \text { final }}, &amp; a_{t}=\text { stop }\end{cases}
$$</p>
<p>Ablation Study We perform ablation experiments on training the OSCAR-based and the PREVALENT-based VLN $\circlearrowleft$ BERT with and without the path fidelity rewards. As shown in Table 6, the models trained with the path fidelity rewards achieve higher Success Rate (SR) and lower Trajectory Length (TL), leading to higher Success weighted by Path Length (SPL). Despite the improvements in SR, the gap between the Oracle Success Rate (OSR) and SR is kept roughly the same for the PREVALENT-based model
and is reduced by about $1.45 \%$ the OSCAR-based models. These results suggest that the path-fidelity rewards benefit the agent to navigate more accurate and efficient. Note that, comparing to the results that are shown in Table 1 and Table 3 of our Main Paper, such reward shaping only contributes to a slight gain of the improvement, whereas the structure of the recurrent BERT is much more influential.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Models</th>
<th style="text-align: center;">R2R Validation Seen</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">R2R Validation Unseen</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">TL</td>
<td style="text-align: center;">OSR</td>
<td style="text-align: center;">SR $\uparrow$</td>
<td style="text-align: center;">SPL $\uparrow$</td>
<td style="text-align: center;">TL</td>
<td style="text-align: center;">OSR</td>
<td style="text-align: center;">SR $\uparrow$</td>
<td style="text-align: center;">SPL $\uparrow$</td>
</tr>
<tr>
<td style="text-align: center;">OSCAR</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$r^{P}$ only</td>
<td style="text-align: center;">11.15</td>
<td style="text-align: center;">77.86</td>
<td style="text-align: center;">70.71</td>
<td style="text-align: center;">66.64</td>
<td style="text-align: center;">12.62</td>
<td style="text-align: center;">66.92</td>
<td style="text-align: center;">58.32</td>
<td style="text-align: center;">52.48</td>
</tr>
<tr>
<td style="text-align: center;">$r^{P}+r^{P}+r^{S}$</td>
<td style="text-align: center;">10.79</td>
<td style="text-align: center;">76.79</td>
<td style="text-align: center;">71.11</td>
<td style="text-align: center;">67.23</td>
<td style="text-align: center;">11.86</td>
<td style="text-align: center;">65.86</td>
<td style="text-align: center;">58.71</td>
<td style="text-align: center;">53.41</td>
</tr>
<tr>
<td style="text-align: center;">PREVALENT</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$r^{P}$ only</td>
<td style="text-align: center;">12.24</td>
<td style="text-align: center;">77.28</td>
<td style="text-align: center;">69.93</td>
<td style="text-align: center;">64.46</td>
<td style="text-align: center;">12.89</td>
<td style="text-align: center;">69.52</td>
<td style="text-align: center;">62.15</td>
<td style="text-align: center;">55.65</td>
</tr>
<tr>
<td style="text-align: center;">$r^{P}+r^{P}+r^{S}$</td>
<td style="text-align: center;">11.13</td>
<td style="text-align: center;">78.16</td>
<td style="text-align: center;">72.18</td>
<td style="text-align: center;">67.72</td>
<td style="text-align: center;">12.01</td>
<td style="text-align: center;">70.24</td>
<td style="text-align: center;">62.75</td>
<td style="text-align: center;">56.84</td>
</tr>
</tbody>
</table>
<p>Table 6. Performance of OSCAR-based and PREVALENT-based VLN $\circlearrowleft$ BERT trained with and without the path fidelity rewards.</p>
<h2>C. Language Attention</h2>
<h2>C.1. Additional Explanation of the Averaged Language Attention Weights (\$4.1)</h2>
<p>In Figure 3 of the Main Paper, we visualised the averaged attention weights over all instructions in validation unseen split during navigation. Notice that in this visualisation, we interpolate the instruction to 80 words and the trajectory to 15 steps for each sample to compute the averaged attention weights. Since a large portion of instructions in R2R are less than 80 words, the last few tokens in the plot are corresponding to the full stop punctuation and the [SEP] token, which are less likely to provide valuable information to the state and hence receive very little weight. However, for the last step in the bottom plot, the selected visual token for stopping could have a high correspondence to the full stop and [SEP] which are strong indications of the end of navigation.</p>
<h2>C.2. Self-Attended Language Features with Gradient Accumulation (\$4.2)</h2>
<p>To evaluate the influence of performing different language self-attention under the same batch size, we train Emb-Attn, Init-Attn and Re-Attn with gradient accumulation to achieve batch size of 16 (same as Ours) while keeping learning rate unchanged. Comparing to the results reported in Table 4 of the Main Paper, the performance of all the three methods has been significantly improved. On the validation unseen split, Emb-Attn even outperforms Ours which does not re-attend the language at each time step. However, a downside for accumulating gradient is that the training speed is significantly reduced, which is about 2.5 times slower for accumulating 4 batches of size 4 . Nevertheless, this result suggests that agent's performance can be further</p>
<p>improved by training with larger batch size, potentially including Ours. Therefore, whether it is necessary to perform language self-attention at each navigational step when there is sufficient computational power remains a question worth investigating. We will leave it as a future work.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Models</th>
<th style="text-align: center;">R2R Validation Seen</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">R2R Validation Uneven</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Accu- <br> Grad</th>
<th style="text-align: center;">Speed</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">TL</td>
<td style="text-align: center;">$\mathrm{NE}_{\downarrow}$</td>
<td style="text-align: center;">SR $\uparrow$</td>
<td style="text-align: center;">SPL $\uparrow$</td>
<td style="text-align: center;">TL</td>
<td style="text-align: center;">$\mathrm{NE}_{\downarrow}$</td>
<td style="text-align: center;">SR $\uparrow$</td>
<td style="text-align: center;">SPL $\uparrow$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Emb-Attn</td>
<td style="text-align: center;">11.36</td>
<td style="text-align: center;">2.97</td>
<td style="text-align: center;">70.91</td>
<td style="text-align: center;">66.26</td>
<td style="text-align: center;">12.22</td>
<td style="text-align: center;">4.15</td>
<td style="text-align: center;">61.00</td>
<td style="text-align: center;">54.85</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">1.43</td>
</tr>
<tr>
<td style="text-align: center;">Intr-Attn</td>
<td style="text-align: center;">12.11</td>
<td style="text-align: center;">3.78</td>
<td style="text-align: center;">64.84</td>
<td style="text-align: center;">59.49</td>
<td style="text-align: center;">12.32</td>
<td style="text-align: center;">4.37</td>
<td style="text-align: center;">58.62</td>
<td style="text-align: center;">52.53</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">1.33</td>
</tr>
<tr>
<td style="text-align: center;">Re-Attn</td>
<td style="text-align: center;">9.71</td>
<td style="text-align: center;">4.44</td>
<td style="text-align: center;">54.16</td>
<td style="text-align: center;">51.51</td>
<td style="text-align: center;">10.19</td>
<td style="text-align: center;">5.28</td>
<td style="text-align: center;">47.89</td>
<td style="text-align: center;">44.64</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">1.38</td>
</tr>
<tr>
<td style="text-align: center;">Ours</td>
<td style="text-align: center;">10.79</td>
<td style="text-align: center;">3.11</td>
<td style="text-align: center;">71.11</td>
<td style="text-align: center;">67.23</td>
<td style="text-align: center;">11.86</td>
<td style="text-align: center;">4.29</td>
<td style="text-align: center;">58.71</td>
<td style="text-align: center;">53.41</td>
<td style="text-align: center;">$\boldsymbol{\lambda}$</td>
<td style="text-align: center;">3.40</td>
</tr>
</tbody>
</table>
<p>Table 7. Comparison of performing language self-attention with gradient accumulation. We set the batch size of the first three methods as 4 and accumulate the gradient for 4 iterations, so that the overall batch size for each update is 16, same as Outs. Unit for Speed is 1,000 iterations per hour (including the time spent on evaluation).</p>
<h2>D. Visualisation (§4.1)</h2>
<p>As shown in Figure 6, 7 and 8, we visualise the language-to-language and the state/vision-to-state/vision/language attention weights of a sample in the validation unseen split. As shown by the panoramas in Fig. 7 and the given instruction "Exit the bedroom. Walk the opposite way of the picture hanging on the wall through the kitchen. Turn right at the long white countertop. Stop when you get past the two chairs.", the agent needs to understand the complex contextual clues, including different scene clues (bedroom, kitchen), different object clues (picture, wall, countertop, chair) and various directional clues (forward, opposite, left/right, stop) to complete the task.</p>
<p>Language Self-Attention Fig. 6 shows the language selfattention attention weights at some selected heads at initialisation $(t=0)$; different heads demonstrate different functions and the attentions at different layers behave very differently. Plot (1) and (4) show the general pattern of attentions at shallow (Layer 0) and deep layers (Layer 8); words in shallow layers tend to collect information from the entire sentence, while words at deep layers have higher correspondence to the adjacent words since they are semantically more relevant. It is very interesting to see that the attention head in Plot (2) learns to attend the adjectives and the action-related terms, which describe the objects and scenes, for the initialised state representation ([CLS]). This head also learns about the co-occurrence between different entities, for example, picture has higher correspondence to wall, and countertop has higher correspondence to kitchen. In contrast, the attention head in Plot (3) learns to extract the important landmarks such as bedroom, picture, kitchen and chairs. Attention head in Plot (5) learns about the [SEP] token, which indicates the ending of the instruction. Attention weights in Plot (6) show the most frequent pattern of
the attention heads at the final $(l=12$-th) layer, those heads seem to aggregate information from the punctuations in the instruction. This implies the heads could have learnt about breaking the sentence into multiple sub-sentences. Refer to the idea of sub-instruction proposed by Hong et al. [20], such attention pattern could be beneficial for matching the current observation to a particular and the most relevant part of the instruction.</p>
<p>State/Vision Step-wise Attention Fig. 7 shows the trajectory of the agent starting from a bedroom, taking a series of actions and eventually stopping at the target location. It also displays the averaged attention weights at the final ( $l=12$-th) layer for the state and visual tokens. Note that the averaged attention for state/vision tokens is representative since we apply the averaged attention weights for visual-textual matching to state and use it as the action probabilities (see Eq. 12 and Decision Making in §3.3, respectively). As shown in Plot (1a-6a), the attention shifts from the beginning of the instruction to the end, which agrees with the agent's navigation progress. It is also interesting to see that at the final layer, the state token is more influential to the predicted action at the first two steps, while the language tokens are more influential at the later steps. The state/vision self-attention in Plot (1b-6b) reflects the action prediction at each time step. The first row in each plot (attention of state with respect to candidate actions) shows the prediction result, we can see that the agent is very confident in each decisions. Moreover, starting from Step 3, the information from state and from different views are aggregated to support choosing the correct direction.</p>
<p>State/Vision Layer-wise Attention To better understand how the visual and language features are aggregated to support action prediction, we visualise the layer-wise attention at Step 4 of the trajectory $(t=4)$. As shown by Fig. 8 Plot (112), at the first two layers, candidate views collect information from the entire instruction. But as the signals propagate to deeper layers (Layer 3-6), the visual tokens attend more the middle part of the instruction, which should be more relevant to the current observations. Interestingly, starting from Layer 6, the visual features tend to dominate the attention, and information aggregates toward the visual token at the predicted direction (which is the correct direction). We can see that, at Layer 7-9, the network still has some doubts about the candidate directions that are spatially closer to the correct direction. But after implicitly reasoning in deeper layers, the network becomes very confident about choosing the correct direction.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6. Language self-attention weights of some selected heads at initialisation. The attention weights are normalised for each row.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7. Visualisation of a trajectory and the averaged attention weights at the final ( $l=12$-th) layer. The centre of each panorama is roughly the agent's heading direction at the corresponding time step. Distance is the agent's distance to target in meters. The attention weights are normalised for each row. Texts in red indicates the predicted action (corresponds to a candidate view) at each time step.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8. Averaged state/vision-to-state/vision/language attention weights at each layer at Step 4 of the trajectory. The attention weights are normalised for each row. Texts in red indicates the predicted action (corresponds to a candidate view) at the current time step $(t=4)$.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{5}$ Link to Section 3.3 in Main Paper.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1311 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1311</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1311</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-29.html">extraction-schema-29</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <p><strong>Paper ID:</strong> paper-255124919</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2212.12669v2.pdf" target="_blank">On Realization of Intelligent Decision-Making in the Real World: A Foundation Decision Model Perspective</a></p>
                <p><strong>Paper Abstract:</strong> The pervasive uncertainty and dynamic nature of real-world environments present significant challenges for the widespread implementation of machine-driven Intelligent Decision-Making (IDM) systems. Consequently, IDM should possess the ability to continuously acquire new skills and effectively generalize across a broad range of applications. The advancement of Artificial General Intelligence (AGI) that transcends task and application boundaries is critical for enhancing IDM. Recent studies have extensively investigated the Transformer neural architecture as a foundational model for various tasks, including computer vision, natural language processing, and reinforcement learning. We propose that a Foundation Decision Model (FDM) can be developed by formulating diverse decision-making tasks as sequence decoding tasks using the Transformer architecture, offering a promising solution for expanding IDM applications in complex real-world situations. In this paper, we discuss the efficiency and generalization improvements offered by a foundation decision model for IDM and explore its potential applications in multi-agent game AI, production scheduling, and robotics tasks. Lastly, we present a case study demonstrating our FDM implementation, DigitalBrain (DB1) with 1.3 billion parameters, achieving human-level performance in 870 tasks, such as text generation, image captioning, video game playing, robotic control, and traveling salesman problems. As a foundation decision model, DB1 represents an initial step toward more autonomous and efficient real-world IDM applications.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1311.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1311.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepMind Lab</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepMind Lab</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 3D navigation and puzzle-solving research environment used for training and evaluating learning agents; DB1's training data included trajectories collected by v-trace agents across multiple levels.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>DeepMind Lab</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>A 3D learning environment providing navigation and puzzle-solving tasks for agents; used here to generate expert trajectories (65,000 episodes per level; total ~1.5M episodes).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>embodied navigation / reinforcement learning (simulated 3D environments)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>low-to-medium visual/interaction fidelity (game-like 3D environment, not a physics-accurate simulator)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>rich 3D visuals and environment interactions, but not intended as a high-precision physics simulator; emphasizes navigation and puzzle dynamics rather than detailed physical realism.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>v-trace expert agents (data collection) and DB1 (trained on collected demonstrations)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Expert agents: v-trace RL agents used to collect trajectories; DB1: a 1.3B-parameter TransformerXL-based Foundation Decision Model trained by behavior cloning on the collected data.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>multi-step navigation and puzzle-solving decision tasks (sequence modeling of state-action trajectories)</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>not reported for direct sim-to-real transfer for this environment (used as offline data source for DB1)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>The paper notes that many virtual environments (including game-like ones) are lower fidelity than the real world and can introduce a sim-to-real gap; no explicit minimal fidelity for DeepMind Lab is given.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>No DeepMind Lab-specific failures reported; overall DB1 struggled more on tasks (e.g., some DM Control / Atari tasks) where expert data quality was insufficient.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On Realization of Intelligent Decision-Making in the Real World: A Foundation Decision Model Perspective', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1311.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1311.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Procgen</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Procgen Benchmark (procedurally-generated game environments)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A suite of 16 procedurally generated game-like environments used to evaluate sample efficiency and generalization; DB1's dataset includes 100k episodes per environment collected with v-trace or r2d2 agents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>Procgen</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Procedurally-generated 2D game environments providing diverse visuals and tasks for RL agents; used to gather large numbers of episodes for offline training.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>reinforcement learning / procedural game environments</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>low-fidelity (arcade-style game dynamics; visuals and interactions are simplified and game-like rather than physically realistic)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>procedural variability across levels to test generalization; dynamics are simplified and discrete/frame-based; not physics-accurate.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>v-trace or r2d2 expert agents (data collection) and DB1</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Expert RL algorithms (v-trace, r2d2) produced trajectories; DB1 as transformer behavior-cloning model trained on aggregated trajectories across tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>game-play decision-making and generalization across procedurally generated levels</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>held-out procedural levels and multi-task benchmarks (no real-world transfer reported)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Paper remarks that low-fidelity simulators are commonly used due to compute constraints and that this can create a sim-to-real gap; no Procgen-specific minimal requirements are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>No Procgen-specific failure cases described; general statement that DB1 performance varies by task and by data quality.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On Realization of Intelligent Decision-Making in the Real World: A Foundation Decision Model Perspective', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1311.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1311.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sokoban (env)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sokoban (planning puzzle environment)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A deterministic planning puzzle environment used to collect optimal (or near-optimal) expert trajectories via reverse-play breadth-first search for DB1 training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>Sokoban (environment)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>A discrete puzzle domain where agents push boxes to target locations; used here to generate optimal expert solutions via reversed-play BFS.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>combinatorial planning / discrete puzzle reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>high logical/deterministic fidelity for the discrete puzzle domain (not a continuous physics simulator)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>deterministic transitions, irreversible moves possible, exact planning dynamics captured; no continuous physics, friction, or randomness.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>optimal/search-based experts (breadth-first search) and DB1</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Expert solutions extracted by reversing play and breadth-first search to obtain (near-)optimal trajectories; DB1 trained via behavior cloning to imitate these sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>long-horizon planning in discrete deterministic puzzles (sequence of moves to reach solved state)</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>generalization to other Sokoban levels / held-out puzzles</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Not discussed beyond noting that expert (optimal) trajectories were used to produce high-quality training data.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>None reported specific to Sokoban; DB1 achieved >50% expert score on many tasks overall but per-environment breakdown not given.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On Realization of Intelligent Decision-Making in the Real World: A Foundation Decision Model Perspective', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1311.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1311.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BabyAI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BabyAI</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A grid-world instruction-following platform with synthetic language; DB1 was trained on 100k episodes per level produced by the built-in BabyAI bot as expert data and achieved ~90% of expert performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>BabyAI (grid-world)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>A grid-world environment with instruction-following tasks specified in a synthetic language; expert trajectories generated by the built-in optimal bot.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>language-conditioned RL / grounded instruction following in grid worlds</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>low-fidelity (symbolic/grid-world, deterministic dynamics, simplified visuals)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>discrete grid transitions, symbolic state/action spaces, full observability per environment design; not physics-based.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>BabyAI built-in bot (expert) and DB1</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Expert bot with privileged access produced optimal solutions; DB1 trained to imitate language-conditioned behavior using transformer sequence modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>language-conditioned instruction-following and planning in grid-world environments</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td>DB1 achieved approximately 90% of expert score across all 55 BabyAI tasks (as reported by the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>held-out BabyAI levels (generalization across instruction types and layouts)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>approximately 90% of expert on BabyAI evaluation (reported average normalized score)</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Paper suggests BabyAI is inherently easier to generalize for models pretrained on language, but no explicit minimal fidelity statement.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>No BabyAI-specific failures reported; other domains (Atari, DM Control) showed weaker DB1 performance attributed to data/expert quality.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On Realization of Intelligent Decision-Making in the Real World: A Foundation Decision Model Perspective', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1311.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1311.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Modular RL (MuJoCo-based)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Modular RL (MuJoCo-based continuous control variants)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A collection of MuJoCo-based continuous control environments consisting of morphological variations used to collect expert D4PG trajectories (843.6K episodes) for DB1 training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>MuJoCo (as used by Modular RL variants)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>A physics engine for model-based continuous control; Modular RL enumerates morphological variants of standard MuJoCo bodies to create varied dynamics and observation/action sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>robotics / continuous control / biomechanics-like simulated dynamics</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>medium-fidelity physics (accurate rigid-body dynamics and contact modeling for many control tasks, but simplified friction/soft-body effects relative to real robots)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>continuous-time approximate dynamics, rigid-body contact and joint dynamics, discrete time-stepping integrator, no detailed deformable contacts or sensor noise modeling unless added explicitly.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>D4PG expert agents (data collection) and DB1</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>D4PG RL agents trained per variant collected trajectories; DB1 later trained via behavior cloning on aggregated datasets to imitate control behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>continuous control across morphological variants (generalization to different bodies and dynamics)</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>intended to improve generalization across morphologies and potentially inform real-world robotics, but no direct sim-to-real transfer reported.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Paper notes that physics-based suites like MuJoCo provide useful offline data but are still approximations; no explicit minimal set of features for transfer is given.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Paper reports that DB1 performance on some continuous control suites (DM Control Suite) was weaker, attributed to insufficient expert data quality rather than simulator fidelity per se.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On Realization of Intelligent Decision-Making in the Real World: A Foundation Decision Model Perspective', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1311.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1311.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Meta-World</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Meta-World</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A MuJoCo-based benchmark suite for meta-reinforcement learning and multi-task robotic manipulation; DB1's dataset included ~88.2K episodes collected using APPO agents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>Meta-World (MuJoCo-based manipulation suite)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>A set of simulated robotic manipulation tasks with diverse subtasks, scene setups and object interactions implemented on the MuJoCo physics engine.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>robotics / manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>medium-fidelity physics-based manipulation (rigid-body dynamics, articulated robots, contact approximations typical of MuJoCo)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>models articulated robot kinematics/dynamics and contact interactions approximately; typically lacks detailed sensor noise, soft-body or complex cloth dynamics unless explicitly simulated.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>APPO expert agents (data collection) and DB1</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>APPO (a distributed RL algorithm) collected trajectories used as expert demonstrations; DB1 learned via sequence modeling across tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>multi-task robotic manipulation decision-making and policy generalization across manipulation tasks</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>generalization across Meta-World tasks; no real-robot sim-to-real transfer reported for DB1 in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Paper asserts FDMs can reduce sim-to-real gap by learning from diverse datasets, but does not specify minimal simulation fidelity required for Meta-World tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>No Meta-World-specific failures reported; overall performance depends on expert data quality.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On Realization of Intelligent Decision-Making in the Real World: A Foundation Decision Model Perspective', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1311.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1311.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepMind Control Suite (DM Control)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepMind Control Suite</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A suite of physics-based continuous control environments used to collect both state-feature and pixel-based expert data (675K episodes total) with D4PG and APPO agents for DB1 training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>DeepMind Control Suite (DM Control Suite)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>A set of physics-based continuous control tasks (built on physics engines similar to MuJoCo) used for benchmarking control algorithms, offering both state and pixel observations.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>continuous control / robotics / physics-based simulation</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>medium-fidelity physics (rigid-body dynamics, articulations and contacts approximated, with both low-dimensional state and pixel-based observations provided)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>supports state-level and pixel-level observations; physics approximations typical of MuJoCo-style engines; deterministic or controlled stochasticity depending on task.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>D4PG (state data), APPO (pixel data) expert agents and DB1</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>RL experts (D4PG and APPO) collected two disjoint sets of data for each task; DB1 trained to imitate actions from both state and pixel modalities.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>continuous control from states and pixels; learn policies for physics-based control tasks</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>intended as offline data for DB1; no direct sim-to-real robot transfer experiments reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Authors note that although physics-based suites are useful, DB1 underperformed on some DMC tasks, likely due to insufficient data/expert strength; no explicit minimum fidelity claim.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>DB1 performance in DM Control Suite was weaker compared to other tasks; authors attribute this to insufficient training data or weaker experts rather than explicit simulator fidelity limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On Realization of Intelligent Decision-Making in the Real World: A Foundation Decision Model Perspective', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1311.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e1311.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ALE / Atari</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Arcade Learning Environment (Atari 2600 suite)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A collection of Atari game environments (pixel-based) used as RL benchmarks; DB1 included Atari data (tokens listed in dataset table) but underperformed on some Atari tasks relative to expert agents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>Arcade Learning Environment (Atari)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>A set of classic Atari 2600 games used for benchmarking RL agents; environments are pixel-based, discrete-action video games rather than physics simulators.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>reinforcement learning / video-game environments</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>low-fidelity (2D pixel games with discrete dynamics, not physically realistic)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>discrete frame-based dynamics, deterministic or pseudo-random behavior, simple game physics and interactions; no continuous physics realism.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>expert agents used for demonstrations (not fully specified for all Atari games) and DB1</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Expert policies were used to collect demonstration data; DB1 trained via sequence modeling on these trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>game playing and long-horizon decision tasks in Atari games</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>held-out Atari levels / other benchmark games; not transferred to the real world (game-only domain)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Paper highlights that Atari tasks were difficult for DB1 to generalize to, possibly due to longer horizons and insufficient training length; no minimal fidelity thresholds discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>DB1's performance on Atari tasks was relatively weak compared to Gato and other benchmarks; authors attribute this to insufficient strength of the expert data used for training.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On Realization of Intelligent Decision-Making in the Real World: A Foundation Decision Model Perspective', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1311.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e1311.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TSP dataset (LKH experts)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Traveling Salesman Problem dataset (expert solutions via LKH heuristic)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dataset of 2 million TSP trajectories from 200 randomly generated tasks (100/200 node scales) where expert solutions were provided by LKH heuristic; DB1 achieved ~96% of expert performance on these TSP tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>TSP synthetic task generator with expert LKH solver</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Randomly generated TSP instances (100/200 nodes sampled from cities) with expert trajectories obtained from the LKH heuristic solver; used as sequence-modeled optimization tasks for DB1.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>combinatorial optimization / operations research</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>not a physical simulator; high-fidelity for the combinatorial optimization domain (exact or near-optimal solutions from heuristics)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>represents exact combinatorial graph instances with neighbor encodings; not modeling physical dynamicsfidelity relates to optimality of solver outputs (LKH provides strong heuristic/near-optimal solutions).</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>LKH heuristic as expert solver and DB1</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LKH solver produced expert tours; DB1 (transformer) trained to sequentially produce node choices using neighbor encoding from a pretrained GCN.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>sequential decision-making for combinatorial optimization (constructing near-optimal TSP tours)</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td>DB1 attained roughly 96% of the expert score for all TSP tasks (reported average normalized score).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>generalization to held-out TSP instances of similar scales</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>approximately 96% of expert (reported)</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Paper notes that combinatorial optimization problems like TSP may be easier to achieve reasonably good solutions for via sequence modeling; no explicit minimal fidelity statement beyond the use of near-optimal expert solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>No failure cases reported for TSP; DB1 performed strongly relative to expert data.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On Realization of Intelligent Decision-Making in the Real World: A Foundation Decision Model Perspective', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1311.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e1311.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sim-to-Real (discussion)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sim-to-Real Transfer (general discussion and literature citations)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A general discussion of sim-to-real transfer challenges and opportunities: the paper notes that many IDM models train in simulators and must transfer to the physical world, that high-fidelity simulators are costly, and that foundation models (FDM) can improve transfer robustness but no direct sim-to-real experiments are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>conceptual (sim-to-real transfer context across multiple simulators mentioned in the paper)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Conceptual discussion referencing the gap between simulated environments and the real world, and literature on sim-to-real transfer; not a specific software simulator.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>robotics / reinforcement learning / transfer learning</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>conceptual: distinguishes high-fidelity (computationally expensive, close-to-real) vs low-fidelity (cheaper, less accurate) simulators</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>high-fidelity sims model real-world details more accurately but at computational cost; low-fidelity sims are less accurate and introduce a sim-to-real gap; paper does not enumerate required physical features (e.g., sensor noise) in detail.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>DB1 (FDM) discussed as a potential robust approach to sim-to-real transfer; various expert RL agents used to collect simulator data</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>DB1 is a foundation decision transformer trained offline on multimodal/task-diverse simulator data; expertise-derived controllers are the data source.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>generalization and transfer of policies from simulated training data to real-world embodied tasks (robotics, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>real-world environments (discussed conceptually), higher-fidelity domains, or varied real tasks</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Authors state high-fidelity simulators can enable successful transfer but are costly; lower-fidelity simulators are commonly used, causing a gap. They argue FDM's high-capacity multi-source training can improve transfer robustness, but do not specify minimum simulator features needed for success.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>No empirical sim-to-real failure cases for DB1 are presented; more general caution that mismatches between simulated variations and real-world disturbances can lead to transfer failure (citing prior literature).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'On Realization of Intelligent Decision-Making in the Real World: A Foundation Decision Model Perspective', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>DeepMind Lab <em>(Rating: 2)</em></li>
                <li>Leveraging procedural generation to benchmark reinforcement learning <em>(Rating: 2)</em></li>
                <li>BabyAI: A platform to study the sample efficiency of grounded language learning <em>(Rating: 2)</em></li>
                <li>MuJoCo: A physics engine for model-based control <em>(Rating: 2)</em></li>
                <li>DM Control Suite: Software and tasks for continuous control <em>(Rating: 2)</em></li>
                <li>Meta-World: A benchmark and evaluation for multi-task and meta reinforcement learning <em>(Rating: 2)</em></li>
                <li>The travelling salesman problem and related problems <em>(Rating: 1)</em></li>
                <li>Crossing the reality gap: a survey on sim-to-real transferability of robot controllers in reinforcement learning <em>(Rating: 2)</em></li>
                <li>A generalist agent <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1311",
    "paper_id": "paper-255124919",
    "extraction_schema_id": "extraction-schema-29",
    "extracted_data": [
        {
            "name_short": "DeepMind Lab",
            "name_full": "DeepMind Lab",
            "brief_description": "A 3D navigation and puzzle-solving research environment used for training and evaluating learning agents; DB1's training data included trajectories collected by v-trace agents across multiple levels.",
            "citation_title": "",
            "mention_or_use": "use",
            "simulator_name": "DeepMind Lab",
            "simulator_description": "A 3D learning environment providing navigation and puzzle-solving tasks for agents; used here to generate expert trajectories (65,000 episodes per level; total ~1.5M episodes).",
            "scientific_domain": "embodied navigation / reinforcement learning (simulated 3D environments)",
            "fidelity_level": "low-to-medium visual/interaction fidelity (game-like 3D environment, not a physics-accurate simulator)",
            "fidelity_characteristics": "rich 3D visuals and environment interactions, but not intended as a high-precision physics simulator; emphasizes navigation and puzzle dynamics rather than detailed physical realism.",
            "model_or_agent_name": "v-trace expert agents (data collection) and DB1 (trained on collected demonstrations)",
            "model_description": "Expert agents: v-trace RL agents used to collect trajectories; DB1: a 1.3B-parameter TransformerXL-based Foundation Decision Model trained by behavior cloning on the collected data.",
            "reasoning_task": "multi-step navigation and puzzle-solving decision tasks (sequence modeling of state-action trajectories)",
            "training_performance": null,
            "transfer_target": "not reported for direct sim-to-real transfer for this environment (used as offline data source for DB1)",
            "transfer_performance": null,
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "The paper notes that many virtual environments (including game-like ones) are lower fidelity than the real world and can introduce a sim-to-real gap; no explicit minimal fidelity for DeepMind Lab is given.",
            "failure_cases": "No DeepMind Lab-specific failures reported; overall DB1 struggled more on tasks (e.g., some DM Control / Atari tasks) where expert data quality was insufficient.",
            "uuid": "e1311.0",
            "source_info": {
                "paper_title": "On Realization of Intelligent Decision-Making in the Real World: A Foundation Decision Model Perspective",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "Procgen",
            "name_full": "Procgen Benchmark (procedurally-generated game environments)",
            "brief_description": "A suite of 16 procedurally generated game-like environments used to evaluate sample efficiency and generalization; DB1's dataset includes 100k episodes per environment collected with v-trace or r2d2 agents.",
            "citation_title": "",
            "mention_or_use": "use",
            "simulator_name": "Procgen",
            "simulator_description": "Procedurally-generated 2D game environments providing diverse visuals and tasks for RL agents; used to gather large numbers of episodes for offline training.",
            "scientific_domain": "reinforcement learning / procedural game environments",
            "fidelity_level": "low-fidelity (arcade-style game dynamics; visuals and interactions are simplified and game-like rather than physically realistic)",
            "fidelity_characteristics": "procedural variability across levels to test generalization; dynamics are simplified and discrete/frame-based; not physics-accurate.",
            "model_or_agent_name": "v-trace or r2d2 expert agents (data collection) and DB1",
            "model_description": "Expert RL algorithms (v-trace, r2d2) produced trajectories; DB1 as transformer behavior-cloning model trained on aggregated trajectories across tasks.",
            "reasoning_task": "game-play decision-making and generalization across procedurally generated levels",
            "training_performance": null,
            "transfer_target": "held-out procedural levels and multi-task benchmarks (no real-world transfer reported)",
            "transfer_performance": null,
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "Paper remarks that low-fidelity simulators are commonly used due to compute constraints and that this can create a sim-to-real gap; no Procgen-specific minimal requirements are provided.",
            "failure_cases": "No Procgen-specific failure cases described; general statement that DB1 performance varies by task and by data quality.",
            "uuid": "e1311.1",
            "source_info": {
                "paper_title": "On Realization of Intelligent Decision-Making in the Real World: A Foundation Decision Model Perspective",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "Sokoban (env)",
            "name_full": "Sokoban (planning puzzle environment)",
            "brief_description": "A deterministic planning puzzle environment used to collect optimal (or near-optimal) expert trajectories via reverse-play breadth-first search for DB1 training.",
            "citation_title": "",
            "mention_or_use": "use",
            "simulator_name": "Sokoban (environment)",
            "simulator_description": "A discrete puzzle domain where agents push boxes to target locations; used here to generate optimal expert solutions via reversed-play BFS.",
            "scientific_domain": "combinatorial planning / discrete puzzle reasoning",
            "fidelity_level": "high logical/deterministic fidelity for the discrete puzzle domain (not a continuous physics simulator)",
            "fidelity_characteristics": "deterministic transitions, irreversible moves possible, exact planning dynamics captured; no continuous physics, friction, or randomness.",
            "model_or_agent_name": "optimal/search-based experts (breadth-first search) and DB1",
            "model_description": "Expert solutions extracted by reversing play and breadth-first search to obtain (near-)optimal trajectories; DB1 trained via behavior cloning to imitate these sequences.",
            "reasoning_task": "long-horizon planning in discrete deterministic puzzles (sequence of moves to reach solved state)",
            "training_performance": null,
            "transfer_target": "generalization to other Sokoban levels / held-out puzzles",
            "transfer_performance": null,
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "Not discussed beyond noting that expert (optimal) trajectories were used to produce high-quality training data.",
            "failure_cases": "None reported specific to Sokoban; DB1 achieved &gt;50% expert score on many tasks overall but per-environment breakdown not given.",
            "uuid": "e1311.2",
            "source_info": {
                "paper_title": "On Realization of Intelligent Decision-Making in the Real World: A Foundation Decision Model Perspective",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "BabyAI",
            "name_full": "BabyAI",
            "brief_description": "A grid-world instruction-following platform with synthetic language; DB1 was trained on 100k episodes per level produced by the built-in BabyAI bot as expert data and achieved ~90% of expert performance.",
            "citation_title": "",
            "mention_or_use": "use",
            "simulator_name": "BabyAI (grid-world)",
            "simulator_description": "A grid-world environment with instruction-following tasks specified in a synthetic language; expert trajectories generated by the built-in optimal bot.",
            "scientific_domain": "language-conditioned RL / grounded instruction following in grid worlds",
            "fidelity_level": "low-fidelity (symbolic/grid-world, deterministic dynamics, simplified visuals)",
            "fidelity_characteristics": "discrete grid transitions, symbolic state/action spaces, full observability per environment design; not physics-based.",
            "model_or_agent_name": "BabyAI built-in bot (expert) and DB1",
            "model_description": "Expert bot with privileged access produced optimal solutions; DB1 trained to imitate language-conditioned behavior using transformer sequence modeling.",
            "reasoning_task": "language-conditioned instruction-following and planning in grid-world environments",
            "training_performance": "DB1 achieved approximately 90% of expert score across all 55 BabyAI tasks (as reported by the paper).",
            "transfer_target": "held-out BabyAI levels (generalization across instruction types and layouts)",
            "transfer_performance": "approximately 90% of expert on BabyAI evaluation (reported average normalized score)",
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "Paper suggests BabyAI is inherently easier to generalize for models pretrained on language, but no explicit minimal fidelity statement.",
            "failure_cases": "No BabyAI-specific failures reported; other domains (Atari, DM Control) showed weaker DB1 performance attributed to data/expert quality.",
            "uuid": "e1311.3",
            "source_info": {
                "paper_title": "On Realization of Intelligent Decision-Making in the Real World: A Foundation Decision Model Perspective",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "Modular RL (MuJoCo-based)",
            "name_full": "Modular RL (MuJoCo-based continuous control variants)",
            "brief_description": "A collection of MuJoCo-based continuous control environments consisting of morphological variations used to collect expert D4PG trajectories (843.6K episodes) for DB1 training.",
            "citation_title": "",
            "mention_or_use": "use",
            "simulator_name": "MuJoCo (as used by Modular RL variants)",
            "simulator_description": "A physics engine for model-based continuous control; Modular RL enumerates morphological variants of standard MuJoCo bodies to create varied dynamics and observation/action sizes.",
            "scientific_domain": "robotics / continuous control / biomechanics-like simulated dynamics",
            "fidelity_level": "medium-fidelity physics (accurate rigid-body dynamics and contact modeling for many control tasks, but simplified friction/soft-body effects relative to real robots)",
            "fidelity_characteristics": "continuous-time approximate dynamics, rigid-body contact and joint dynamics, discrete time-stepping integrator, no detailed deformable contacts or sensor noise modeling unless added explicitly.",
            "model_or_agent_name": "D4PG expert agents (data collection) and DB1",
            "model_description": "D4PG RL agents trained per variant collected trajectories; DB1 later trained via behavior cloning on aggregated datasets to imitate control behaviors.",
            "reasoning_task": "continuous control across morphological variants (generalization to different bodies and dynamics)",
            "training_performance": null,
            "transfer_target": "intended to improve generalization across morphologies and potentially inform real-world robotics, but no direct sim-to-real transfer reported.",
            "transfer_performance": null,
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "Paper notes that physics-based suites like MuJoCo provide useful offline data but are still approximations; no explicit minimal set of features for transfer is given.",
            "failure_cases": "Paper reports that DB1 performance on some continuous control suites (DM Control Suite) was weaker, attributed to insufficient expert data quality rather than simulator fidelity per se.",
            "uuid": "e1311.4",
            "source_info": {
                "paper_title": "On Realization of Intelligent Decision-Making in the Real World: A Foundation Decision Model Perspective",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "Meta-World",
            "name_full": "Meta-World",
            "brief_description": "A MuJoCo-based benchmark suite for meta-reinforcement learning and multi-task robotic manipulation; DB1's dataset included ~88.2K episodes collected using APPO agents.",
            "citation_title": "",
            "mention_or_use": "use",
            "simulator_name": "Meta-World (MuJoCo-based manipulation suite)",
            "simulator_description": "A set of simulated robotic manipulation tasks with diverse subtasks, scene setups and object interactions implemented on the MuJoCo physics engine.",
            "scientific_domain": "robotics / manipulation",
            "fidelity_level": "medium-fidelity physics-based manipulation (rigid-body dynamics, articulated robots, contact approximations typical of MuJoCo)",
            "fidelity_characteristics": "models articulated robot kinematics/dynamics and contact interactions approximately; typically lacks detailed sensor noise, soft-body or complex cloth dynamics unless explicitly simulated.",
            "model_or_agent_name": "APPO expert agents (data collection) and DB1",
            "model_description": "APPO (a distributed RL algorithm) collected trajectories used as expert demonstrations; DB1 learned via sequence modeling across tasks.",
            "reasoning_task": "multi-task robotic manipulation decision-making and policy generalization across manipulation tasks",
            "training_performance": null,
            "transfer_target": "generalization across Meta-World tasks; no real-robot sim-to-real transfer reported for DB1 in this paper.",
            "transfer_performance": null,
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "Paper asserts FDMs can reduce sim-to-real gap by learning from diverse datasets, but does not specify minimal simulation fidelity required for Meta-World tasks.",
            "failure_cases": "No Meta-World-specific failures reported; overall performance depends on expert data quality.",
            "uuid": "e1311.5",
            "source_info": {
                "paper_title": "On Realization of Intelligent Decision-Making in the Real World: A Foundation Decision Model Perspective",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "DeepMind Control Suite (DM Control)",
            "name_full": "DeepMind Control Suite",
            "brief_description": "A suite of physics-based continuous control environments used to collect both state-feature and pixel-based expert data (675K episodes total) with D4PG and APPO agents for DB1 training.",
            "citation_title": "",
            "mention_or_use": "use",
            "simulator_name": "DeepMind Control Suite (DM Control Suite)",
            "simulator_description": "A set of physics-based continuous control tasks (built on physics engines similar to MuJoCo) used for benchmarking control algorithms, offering both state and pixel observations.",
            "scientific_domain": "continuous control / robotics / physics-based simulation",
            "fidelity_level": "medium-fidelity physics (rigid-body dynamics, articulations and contacts approximated, with both low-dimensional state and pixel-based observations provided)",
            "fidelity_characteristics": "supports state-level and pixel-level observations; physics approximations typical of MuJoCo-style engines; deterministic or controlled stochasticity depending on task.",
            "model_or_agent_name": "D4PG (state data), APPO (pixel data) expert agents and DB1",
            "model_description": "RL experts (D4PG and APPO) collected two disjoint sets of data for each task; DB1 trained to imitate actions from both state and pixel modalities.",
            "reasoning_task": "continuous control from states and pixels; learn policies for physics-based control tasks",
            "training_performance": null,
            "transfer_target": "intended as offline data for DB1; no direct sim-to-real robot transfer experiments reported in the paper.",
            "transfer_performance": null,
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "Authors note that although physics-based suites are useful, DB1 underperformed on some DMC tasks, likely due to insufficient data/expert strength; no explicit minimum fidelity claim.",
            "failure_cases": "DB1 performance in DM Control Suite was weaker compared to other tasks; authors attribute this to insufficient training data or weaker experts rather than explicit simulator fidelity limitations.",
            "uuid": "e1311.6",
            "source_info": {
                "paper_title": "On Realization of Intelligent Decision-Making in the Real World: A Foundation Decision Model Perspective",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "ALE / Atari",
            "name_full": "Arcade Learning Environment (Atari 2600 suite)",
            "brief_description": "A collection of Atari game environments (pixel-based) used as RL benchmarks; DB1 included Atari data (tokens listed in dataset table) but underperformed on some Atari tasks relative to expert agents.",
            "citation_title": "",
            "mention_or_use": "use",
            "simulator_name": "Arcade Learning Environment (Atari)",
            "simulator_description": "A set of classic Atari 2600 games used for benchmarking RL agents; environments are pixel-based, discrete-action video games rather than physics simulators.",
            "scientific_domain": "reinforcement learning / video-game environments",
            "fidelity_level": "low-fidelity (2D pixel games with discrete dynamics, not physically realistic)",
            "fidelity_characteristics": "discrete frame-based dynamics, deterministic or pseudo-random behavior, simple game physics and interactions; no continuous physics realism.",
            "model_or_agent_name": "expert agents used for demonstrations (not fully specified for all Atari games) and DB1",
            "model_description": "Expert policies were used to collect demonstration data; DB1 trained via sequence modeling on these trajectories.",
            "reasoning_task": "game playing and long-horizon decision tasks in Atari games",
            "training_performance": null,
            "transfer_target": "held-out Atari levels / other benchmark games; not transferred to the real world (game-only domain)",
            "transfer_performance": null,
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "Paper highlights that Atari tasks were difficult for DB1 to generalize to, possibly due to longer horizons and insufficient training length; no minimal fidelity thresholds discussed.",
            "failure_cases": "DB1's performance on Atari tasks was relatively weak compared to Gato and other benchmarks; authors attribute this to insufficient strength of the expert data used for training.",
            "uuid": "e1311.7",
            "source_info": {
                "paper_title": "On Realization of Intelligent Decision-Making in the Real World: A Foundation Decision Model Perspective",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "TSP dataset (LKH experts)",
            "name_full": "Traveling Salesman Problem dataset (expert solutions via LKH heuristic)",
            "brief_description": "A dataset of 2 million TSP trajectories from 200 randomly generated tasks (100/200 node scales) where expert solutions were provided by LKH heuristic; DB1 achieved ~96% of expert performance on these TSP tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "simulator_name": "TSP synthetic task generator with expert LKH solver",
            "simulator_description": "Randomly generated TSP instances (100/200 nodes sampled from cities) with expert trajectories obtained from the LKH heuristic solver; used as sequence-modeled optimization tasks for DB1.",
            "scientific_domain": "combinatorial optimization / operations research",
            "fidelity_level": "not a physical simulator; high-fidelity for the combinatorial optimization domain (exact or near-optimal solutions from heuristics)",
            "fidelity_characteristics": "represents exact combinatorial graph instances with neighbor encodings; not modeling physical dynamicsfidelity relates to optimality of solver outputs (LKH provides strong heuristic/near-optimal solutions).",
            "model_or_agent_name": "LKH heuristic as expert solver and DB1",
            "model_description": "LKH solver produced expert tours; DB1 (transformer) trained to sequentially produce node choices using neighbor encoding from a pretrained GCN.",
            "reasoning_task": "sequential decision-making for combinatorial optimization (constructing near-optimal TSP tours)",
            "training_performance": "DB1 attained roughly 96% of the expert score for all TSP tasks (reported average normalized score).",
            "transfer_target": "generalization to held-out TSP instances of similar scales",
            "transfer_performance": "approximately 96% of expert (reported)",
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "Paper notes that combinatorial optimization problems like TSP may be easier to achieve reasonably good solutions for via sequence modeling; no explicit minimal fidelity statement beyond the use of near-optimal expert solutions.",
            "failure_cases": "No failure cases reported for TSP; DB1 performed strongly relative to expert data.",
            "uuid": "e1311.8",
            "source_info": {
                "paper_title": "On Realization of Intelligent Decision-Making in the Real World: A Foundation Decision Model Perspective",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "Sim-to-Real (discussion)",
            "name_full": "Sim-to-Real Transfer (general discussion and literature citations)",
            "brief_description": "A general discussion of sim-to-real transfer challenges and opportunities: the paper notes that many IDM models train in simulators and must transfer to the physical world, that high-fidelity simulators are costly, and that foundation models (FDM) can improve transfer robustness but no direct sim-to-real experiments are reported.",
            "citation_title": "",
            "mention_or_use": "mention",
            "simulator_name": "conceptual (sim-to-real transfer context across multiple simulators mentioned in the paper)",
            "simulator_description": "Conceptual discussion referencing the gap between simulated environments and the real world, and literature on sim-to-real transfer; not a specific software simulator.",
            "scientific_domain": "robotics / reinforcement learning / transfer learning",
            "fidelity_level": "conceptual: distinguishes high-fidelity (computationally expensive, close-to-real) vs low-fidelity (cheaper, less accurate) simulators",
            "fidelity_characteristics": "high-fidelity sims model real-world details more accurately but at computational cost; low-fidelity sims are less accurate and introduce a sim-to-real gap; paper does not enumerate required physical features (e.g., sensor noise) in detail.",
            "model_or_agent_name": "DB1 (FDM) discussed as a potential robust approach to sim-to-real transfer; various expert RL agents used to collect simulator data",
            "model_description": "DB1 is a foundation decision transformer trained offline on multimodal/task-diverse simulator data; expertise-derived controllers are the data source.",
            "reasoning_task": "generalization and transfer of policies from simulated training data to real-world embodied tasks (robotics, etc.)",
            "training_performance": null,
            "transfer_target": "real-world environments (discussed conceptually), higher-fidelity domains, or varied real tasks",
            "transfer_performance": null,
            "compares_fidelity_levels": null,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "Authors state high-fidelity simulators can enable successful transfer but are costly; lower-fidelity simulators are commonly used, causing a gap. They argue FDM's high-capacity multi-source training can improve transfer robustness, but do not specify minimum simulator features needed for success.",
            "failure_cases": "No empirical sim-to-real failure cases for DB1 are presented; more general caution that mismatches between simulated variations and real-world disturbances can lead to transfer failure (citing prior literature).",
            "uuid": "e1311.9",
            "source_info": {
                "paper_title": "On Realization of Intelligent Decision-Making in the Real World: A Foundation Decision Model Perspective",
                "publication_date_yy_mm": "2022-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "DeepMind Lab",
            "rating": 2,
            "sanitized_title": "deepmind_lab"
        },
        {
            "paper_title": "Leveraging procedural generation to benchmark reinforcement learning",
            "rating": 2,
            "sanitized_title": "leveraging_procedural_generation_to_benchmark_reinforcement_learning"
        },
        {
            "paper_title": "BabyAI: A platform to study the sample efficiency of grounded language learning",
            "rating": 2,
            "sanitized_title": "babyai_a_platform_to_study_the_sample_efficiency_of_grounded_language_learning"
        },
        {
            "paper_title": "MuJoCo: A physics engine for model-based control",
            "rating": 2,
            "sanitized_title": "mujoco_a_physics_engine_for_modelbased_control"
        },
        {
            "paper_title": "DM Control Suite: Software and tasks for continuous control",
            "rating": 2,
            "sanitized_title": "dm_control_suite_software_and_tasks_for_continuous_control"
        },
        {
            "paper_title": "Meta-World: A benchmark and evaluation for multi-task and meta reinforcement learning",
            "rating": 2,
            "sanitized_title": "metaworld_a_benchmark_and_evaluation_for_multitask_and_meta_reinforcement_learning"
        },
        {
            "paper_title": "The travelling salesman problem and related problems",
            "rating": 1,
            "sanitized_title": "the_travelling_salesman_problem_and_related_problems"
        },
        {
            "paper_title": "Crossing the reality gap: a survey on sim-to-real transferability of robot controllers in reinforcement learning",
            "rating": 2,
            "sanitized_title": "crossing_the_reality_gap_a_survey_on_simtoreal_transferability_of_robot_controllers_in_reinforcement_learning"
        },
        {
            "paper_title": "A generalist agent",
            "rating": 2,
            "sanitized_title": "a_generalist_agent"
        }
    ],
    "cost": 0.01730425,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>On Realization of Intelligent Decision-Making in the Real World: A Foundation Decision Model Perspective
16 May 2023</p>
<p>Ying Wen ying.wen@sjtu.edu.cn 
Shanghai Jiao Tong University</p>
<p>Ming Zhou mingak@sjtu.edu.cn 
Shufang Hou shufang.hou@digitalbrain.cn 
Zhe Cao 
Digital Brain Laboratory</p>
<p>Chenyang Le 
Jingxiao Chen 
Zheng Tian zheng.tian@shanghaitech.edu.cn 
ShanghaiTech University</p>
<p>Weinan Zhang wnzhang@sjtu.edu.cn 
Jun Wang jun.wang@cs.ucl.ac.uk 
Digital Brain Laboratory</p>
<p>University College London</p>
<p>On Realization of Intelligent Decision-Making in the Real World: A Foundation Decision Model Perspective
16 May 202343C3F53B5CB0B0520F21312AD9AB253EarXiv:2212.12669v2[cs.AI]artificial intelligenceintelligent decision-makingtransformerfoundation decision model . Work completed when Z. WanM. ZhouZ. CaoC. LeJ. Chen are interns at Digital Brain Laboratory
The pervasive uncertainty and dynamic nature of real-world environments present significant challenges for the widespread implementation of machine-driven Intelligent Decision-Making (IDM) systems.Consequently, IDM should possess the ability to continuously acquire new skills and effectively generalize across a broad range of applications.The advancement of Artificial General Intelligence (AGI) that transcends task and application boundaries is critical for enhancing IDM.Recent studies have extensively investigated the Transformer neural architecture as a foundational model for various tasks, including computer vision, natural language processing, and reinforcement learning.We propose that a Foundation Decision Model (FDM) can be developed by formulating diverse decisionmaking tasks as sequence decoding tasks using the Transformer architecture, offering a promising solution for expanding IDM applications in complex real-world situations.In this paper, we discuss the efficiency and generalization improvements offered by a foundation decision model for IDM and explore its potential applications in multi-agent game AI, production scheduling, and robotics tasks.Lastly, we present a case study demonstrating our FDM implementation, DigitalBrain (DB1) with 1.3 billion parameters, achieving human-level performance in 870 tasks, such as text generation, image captioning, video game playing, robotic control, and traveling salesman problems.As a foundation decision model, DB1 represents an initial step toward more autonomous and efficient real-world IDM applications.</p>
<p>Introduction</p>
<p>Intelligent Decision-Making (IDM) leverages artificial intelligence (AI) and related technologies to address real-world decision-making tasks.Human production activities, from operating machinery to governing nations, rely on a series of decisions to achieve desired objectives.Decisions are made based on knowledge, experience, and intuition (Khatri and Ng, 2000).However, the human mind's capacity is finite, limiting decision bandwidth for numerous complex and counter-intuitive problems.The emergence of digital technologies and information processing techniques has substantially increased the complexity and uncertainty of decision problems (Peres et al., 2020), which often exceed human decision-making capabilities.AI facilitates data analysis beyond human capacity, addressing the issue of 'bounded rationality' (Simon, 1990).Traditional IDM models, based on rules, data analysis, control theory (Kirk, 2004), and operations research (OR) (Winston and Goldberg, 2004), enhance decision efficiency by automating aspects of the decision process in various domains (Eom and Kim, 2006), such as production scheduling (Herrmann, 2006), air traffic control (Nolan, 2011), disease diagnosis (Barnett et al., 1987), and enterprise management (Ranjan, 2009).However, due to low model complexity and the need for manual feature design and model selection, traditional IDM models are limited to solving specific tasks.</p>
<p>Existing IDM models primarily address narrowly defined decision-making tasks (Eom and Kim, 2006) to support human decision-making.Given a fixed, small-scale, white-box scheduling problem, OR algorithms can generate a logistics scheduling strategy (Gudehus and Kotzab, 2012).However, OR algorithms must re-perform the optimization for new scheduling problems without reusing historical experience.Consequently, it is challenging for existing decision-making models to provide an optimal strategy for more extensive, dynamic, and complex tasks with increased uncertainty and exponentially growing solution spaces, such as urban traffic light control (Jensen et al., 2016) and state grid dispatch.These complex decision problems are common in real-world applications, which highlights the challenges in applying IDM to enhance operational capabilities and value in production activities.</p>
<p>In the AI boom era, humans expect IDM to further improve efficiency, reduce costs, and find broader applications in complex real-world production activities (Phillips-Wren and Ichalkaranje, 2008).AI has already demonstrated its potential to solve prediction tasks to accelerate automated decisions in business (Gupta and Farahat, 2020).For example, an AI algorithm can predict whether a user will like a product and provide an estimated bidding price for a potential advertising slot (Wang et al., 2017).Based on deep learning (LeCun et al., 2015), such IDM is more concerned with perception problems and excels at recognizing data patterns.Unlike traditional IDM models, deep perception-based IDM models employ end-to-end representation learning and high model complexity but require substantial amounts of data.Understanding data patterns can help reduce uncertainties during the decision process.However, it is not the ultimate goal of IDM.A complete decision cycle includes perceiving the environment and adjusting behaviors based on feedback.Aligned with IDM's long-term vision to make successful decisions in the real world, AI aims to study intelligent agents that receive perceptions from the environment and learn to make adaptive decisions based on feedback to maximize expected utility (Russell, 2010).Learning and feedback mechanisms are AI's key features, enabling the implementation of closed-loop</p>
<p>Paradigm of IDM</p>
<p>Features</p>
<p>Task Relationship l Task independent with clear boundary.</p>
<p>l End-to-end representation learning for data, without feature selection, but needs to train new models for new tasks.</p>
<p>l High model complexity and data demand.</p>
<p>l Hard to training a foundation model.</p>
<p>l Fast adaption for new tasks.</p>
<p>l Extremely high model complexity but low labeled data demand.</p>
<p>l Task independent with clear boundary.</p>
<p>l Requirements for task specific features and model selection.</p>
<p>l Low model complexity and data demand.IDM models in the next stage, which continuously adapt to more dynamic and complex environments, such as robotics (Murphy, 2019) and autonomous unmanned systems (Chen et al., 2022) in real world settings.The focus of IDM is now shifting from intelligent perception to higher-level autonomous decisions capable of interacting with and learning from the real world.Recent advances in deep reinforcement learning (Mnih et al., 2013;Sutton and Barto, 2018) demonstrate that IDM can surpass human-level performance in multi-step complex decision-making tasks with well-defined problem settings, such as Go (Silver et al., 2017) and StarCraft 2 (Vinyals et al., 2019b).However, IDM models are still in the early stages of widespread real-world application (Sutton, 2019), as they can only solve specific, well-defined problems.</p>
<p>Deep Learning Revolution Foundation Model Revolution</p>
<p>When applying IDM models to interact with real world environments, several unique challenges remain unaddressed, particularly efficiency and generalization issues.The real world consists of various tasks, goals, and agents in a constantly changing, dynamic environment, continuously generating new tasks and objectives (Peres et al., 2020).Consequently, IDM applications face numerous practical issues, such as limited data, imbalanced class distributions, multi-modal data indexed over space and time, the need to carefully compose different models in multiple decision stages, and high-cost trial and error for decisions (Gupta and Farahat, 2020;Qin et al., 2021).To tackle these challenges and improve existing IDM models, two questions must be answered: (1) Can data inputs/outputs be standardized with a unified interface?(2) How can the infinite number of real-world problems be mapped to tractable computational problems?</p>
<p>In recent years, transformer-based foundation models have gained popularity as unified solutions for various downstream tasks, including natural language processing (NLP) (Sutskever et al., 2014) and computer vision (CV) (Krizhevsky et al., 2012).Notable examples include BERT (Kenton and Toutanova, 2019), GPT-3 (Brown et al., 2020), and the Swin Trans-former (Liu et al., 2021b).Due to their high model complexity and low demand for labeled data, foundation models exhibit strong generalization capabilities for new tasks, becoming the new backbone network for text, image, voice, video, and related cross-modality tasks.Although foundation models were originally designed for prediction and generative tasks, it is worth exploring whether they can serve as effective neural architectures for decisionmaking problems.Recently, foundation models have demonstrated noticeable effectiveness and generalization ability in reinforcement learning (RL) (Sutton and Barto, 2018), such as the Decision Transformer (DT) (Chen et al., 2021), Trajectory Transformer (TT) (Janner et al., 2021), and Video Pre-training (VPT) (Baker et al., 2022), marking important strides towards general decision-making models.</p>
<p>Amidst the ongoing foundation model revolution, as illustrated in Figure 1, one might wonder if intelligent decision-making (IDM) models can culminate in a single transformer, the Foundation Decision Model (FDM), which treats various decision problems as sequence decoding tasks using transformer architecture to govern all decision-making tasks.FDM employs a standard transformer-based autoregressive decoder for decoding decisions, supporting multi-modality, multi-task, large-scale, and general-purpose decision-making tasks.FDM differs from the DT (Chen et al., 2021) and TT (Janner et al., 2021), which utilize welldesigned transformer architectures for reinforcement learning problems but lack extensibility for other decision-making tasks.The most closely related work is DeepMind's Gato (Reed et al., 2022), a 1.2 billion parameter FDM that serializes all data into a sequence of tokens.Gato demonstrates that a single transformer can directly handle hundreds of tasks in robotics, single-agent games, and vision and language.As the first attempt at FDM, Gato has yet to reveal all possibilities of FDM.In this paper, we discuss the opportunity to use FDM as an IDM solution in real-world applications, significantly alleviating the poor sample efficiency and generalization issues of IDM models.</p>
<p>FDM generalizes to more decision problems by enhancing its ability to encode implicit knowledge and process multi-task (Han et al., 2020), multi-modal (Xu et al., 2022;Tsai et al., 2019), and space-time data (Wen et al., 2022b).With the help of transformers, FDM integrates data across modalities, including time series, text, voice, image, video, categorical, or numerical data (Bommasani et al., 2021;Bao et al., 2021;Wang et al., 2022), enabling a wide range of multi-agent applications, such as video games, robotics, and vehicle routing tasks.Moreover, FDM offers better training and inference efficiency than recurrent neural network-based models for modeling series and sequence patterns, due to their parallel computation in training or inference (Kenton and Toutanova, 2019).Additionally, the self-attention mechanism in FDM can learn interactions between input tokens, inherently permutation invariant (Lin et al., 2017) for processing a sequence of states and actions, making it well-suited for multi-agent tasks (Wen et al., 2022a).Overall, FDM represents inputs as token sequences followed by self-attention interactions, intrinsically compatible with more modalities and tasks with strong generalization potential.</p>
<p>FDM addresses online decision problems in feasible time and cost domains more efficiently.Firstly, thanks to large-scale parameters, multi-head operators, and global aggregation (Kenton and Toutanova, 2019;Brown et al., 2020), FDM naturally possesses extremely high model complexity and learns cross-domain knowledge through effective pre-training on large-scale, multi-modal, and multi-task datasets.Secondly, FDM intrinsically shares the same learning objective across offline and online training stages, reducing the difficulty We believe FDM has great potential as a unified solution by converting all types of tasks into sequence modeling problems, which is one of the promising directions toward realizing IDM in the real world by trading off efficiency and generalization.We also discuss some possibilities of FDM in real world applications, including game AI, production scheduling, and robotics.To verify our hypothesis, we develop DigitalBrain (DB1) with around 1.3 billion parameters. DB1 achieves human-level performance over 453 tasks, including text, text-image, video games, robotics, and path planning tasks.DB1 is a new attempt at FDM and has extensions in parameter size, sequence length, and supported tasks compared to Gato.However, DB1 is still a partial implementation of general FDM.We indicate the gaps between DB1 and general FDM in Table 1.We believe DB1 is a further step towards general FDM, which points out the further improvement directions of FDM.</p>
<p>We believe FDM has tremendous potential as a unified solution by converting all types of tasks into sequence modeling problems, representing one of the most promising directions towards realizing IDM in the real world while balancing efficiency and generalization.We also discuss some possibilities of FDM in real-world applications, including game AI, production scheduling, and robotics.To verify our hypothesis, we develop DigitalBrain (DB1) with around 1.3 billion parameters.DB1 achieves human-level performance across 870 tasks, including text, text-image, video games, robotics, and path planning tasks.DB1 represents a new attempt at FDM with extensions in parameter size, sequence length, and supported tasks compared to Gato.However, DB1 is still a partial implementation of general FDM.We indicate the gaps between DB1 and general FDM in Table 1.We believe DB1 is a further step towards general FDM, pointing out directions for future improvement of FDM.</p>
<p>In the rest of this paper, Section 2 focuses on the generalization possibility of multimodal, multi-task decision-making tasks via a unified model.Section 3 discusses the ef-ficiency issue of IDM algorithms to solve different types of decision problems, Section 4 introduces some typical applications, Section 5 demonstrates the potential of the FDM via a case study of the DB1 model, and Section 6 concludes the paper with a summary.</p>
<p>FDM Makes Generalizable Decisions</p>
<p>Real-world situations are rarely identical, making generalization essential for decision-making.FDM learns generalizable representations for multi-modal and multi-task decision-making problems, a fundamental requirement for IDM models to continuously adapt to new tasks (Seger and Peterson, 2013;Kirk et al., 2021).Additionally, FDM can perform online adaptation after offline pre-training, further enhancing its generalization ability for life-long decision learning (Poquet and De Laat, 2021).</p>
<p>Multi-modal Decisions</p>
<p>Originally designed for NLP (Kenton and Toutanova, 2019), transformers have been extended to other modality tasks (Xu et al., 2022), allowing FDM to operate across diverse domains and modalities.Real world environments are multi-modal, meaning both observations and behaviors for IDM applications are multi-modal.For instance, a robot requires multi-modal sensors, such as cameras, voice, radar, and maps, to perceive the environment.Various deep learning methods have been proposed to enhance AI models' multi-modal perception ability.MultiModel (Kaiser et al., 2017) is trained jointly on eight distinct speech, image, and text processing tasks, using modality-specific encoders for text, images, audio, and categorical data, while the rest of the network parameters are shared across tasks.In 2018, the Google AI team proposed "one big net for everything" (Schmidhuber, 2018), describing a method for incrementally training an increasingly general problem solver.</p>
<p>With the development of pre-trained foundation models, FDM is further equipped to interact with multi-modal environments.Research on autonomous IDM in multi-modal virtual environments (Gan et al., 2020) has emerged, particularly in visual navigation and object manipulation.In April 2022, the Google Robotics team released SayCan (Ahn et al., 2022).SayCan robots generate sub-task instructions for a given task using a natural language pretrained model and perform corresponding actions based on the sub-task to complete the goal.Thus, SayCan robots can solve tasks given natural language descriptions through visual perception in real environments.Gato (Reed et al., 2022), launched by DeepMind in June 2022, unifies multi-step decision-making tasks, multi-round dialogues, and image-text generation tasks into a transformer-based autoregressive problem, achieving promising results in over 600 tasks.This preliminarily verifies the potential of multi-modal pre-trained foundation models for agent learning tasks.In July 2022, based on the open-world game Minecraft (Perez-Liebana et al., 2019), NVIDIA's MineDojo (Fan et al., 2022) provides multi-modal data, such as game videos, Wikipedia, and forum pages, and creates thousands of single-agent tasks.MineDojo leverages the CLIP pre-training model (Radford et al., 2021) to conduct comparative learning on video-text pair datasets and train a zero-shot prediction model of the correlation between natural language targets and environmental visual states.</p>
<p>Multi-task Decisions</p>
<p>FDM is an end-to-end multi-task learning transformer framework that simultaneously learns multiple language, vision, and decision-making tasks.Multi-task learning (MTL) (Vandenhende et al., 2021) seeks to utilize useful information from multiple tasks to improve the generalization performance of all tasks.Existing MTL studies primarily focus on supervised learning tasks, with few addressing tasks such as self-supervised and reinforcement learning tasks (Zhang and Yang, 2021).FDM jointly learns multiple tasks, including reinforcement learning and OR tasks.Moreover, FDM combines self-supervised, supervised, and policy optimization objectives into a single sequence modeling objective, significantly enhancing FDM's generalization performance across all task types.</p>
<p>It is important to note that FDM employs a single multi-task network architecture, rather than a single network with shared hidden layers for all tasks.A single multi-task network implies that all tasks share the same input and output structure and use the same learning objectives, providing superior cross-domain multi-task generalization ability.In previous work, it was common to use the same policy architecture across tasks with different learning parameters for each task, leading to a loss of generality.To further improve generalization, researchers attempted to use the exact same policy on a single domain and achieved good performance on Atari57 (Badia et al., 2020) and DMLab (Espeholt et al., 2018).Such methods have good generalization ability for similar tasks in the same domain but struggle with tasks from other domains.To address cross-domain multi-task problems, FDM learns a single network with the same weights across a diverse set of tasks.Similar choices have been adopted by TD and TT, demonstrating that large sequence foundation models can handle such cross-domain decision-making tasks.</p>
<p>Sim-to-Real Decisions</p>
<p>Many IDM models are trained in simulation environments, but real-world applications also require interaction with the physical world.Virtual environment simulations enable faster and safer training of decision models.However, once an IDM model is trained, it must be transferred to the real world, a process known as sim-to-real transfer (Zhao et al., 2020).When virtual environments accurately simulate real-world conditions, the transferred policy can function effectively (Salvato et al., 2021).Regrettably, high-fidelity simulations demand substantial computational resources.In practice, less accurate simulators are often preferred, introducing a gap between training virtual environments and real-world applications.This necessitates IDM models being able to tolerate environmental changes from the simulator to the physical world.</p>
<p>Owing to their high learning capacity, foundation models like FDM can learn from various data sources, dramatically improving transferability to real-world environments.Although several attempts have been made in the literature to create sim-to-real transferable controllers, often associating robustness with sim-to-real transferability, the lack of task uniformity prevents determining which solution is most appropriate for addressing the simto-real gap.Previous sim-to-real methods typically require two successive training phases, which may exhibit low efficiency (Salvato et al., 2021).Furthermore, if these variations and disturbances do not accurately represent the simulator's discrepancies from reality, sim-toreal transfer may fail.In FDM, given a distribution over tasks, the model learns an adaptive policy that maximizes the expected reward for a new task from the distribution.Consequently, FDM presents a promising and robust solution for quickly adapting the experience gained in real-world simulations.</p>
<p>3 FDM Learns to Make Decisions Efficiently FDM, through pre-training on offline datasets, circumvents data inefficiency concerns during learning and maintains low-latency characteristics during inference.Reinforcement Learning (RL) and Operations Research (OR) are two prominent IDM approaches for tackling distinct decision-making tasks.On the one hand, Deep Reinforcement Learning (DRL) has seen considerable success in video games (Mnih et al., 2013).However, DRL is plagued by sample efficiency problems due to the need for extensive, high-quality, and affordable data, which can be easily acquired from online simulators.The real world, unlike a simulator, does not provide cheap, high-quality, large-scale data.Consequently, DRL struggles to learn effective decision policies outside of simulators.On the other hand, OR algorithms are commonly employed for planning and scheduling decisions.However, OR models are inherently sensitive to problem complexity, making it challenging to implement dynamic scheduling and planning in many real-world tasks.</p>
<p>This section examines how FDM addresses data inefficiency issues for reinforcement learning and multi-agent learning tasks.Additionally, we describe how FDM learns OR optimization by learning OR models and transforming them from long-cycle decision services to instantaneous decision tools.</p>
<p>Offline Learning from Demonstrations</p>
<p>Typically, FDM is trained offline using demonstration data, adopting a highly sampleefficient imitation learning (IL) approach.Imitation learning seeks to learn from expert demonstrations to reproduce optimal behavior.IL techniques, such as behavior cloning (BC) and inverse RL (IRL), have proven effective in video games, robotic simulations, and object manipulation (Zheng et al., 2021;Ho and Ermon, 2016).BC mimics expert behavior and performs well in environments with fixed parameters.IRL, which combines behavior cloning and prediction, achieves higher data efficiency and performs well in some dynamic environments.Despite these successful cases, IL still faces challenges in learning diverse behaviors, utilizing sub-optimal demonstrations, or following language/voice instructions for imitation (Hua et al., 2021).FDM not only inherits the benefits of IL but also naturally handles multi-modal data and multi-task learning.Moreover, it combines behavior cloning and prediction to address these challenges.FDM also bridges offline imitation and online learning, enabling consistent and efficient learning of new tasks.</p>
<p>Muli-agent Decisions</p>
<p>FDM employs a sequential update scheme for multi-agent decisions, effectively transforming multi-agent joint policy optimization into a sequential modeling problem.Managing nonstationary dynamics is a critical challenge in multi-agent decision learning research, rendering it difficult to apply single-agent decision models directly in multi-agent contexts.FDM overcomes this issue by converting agents' simultaneous decision-making into a sequential decision-making process for each agent (Wen et al., 2022a).In Multi-Agent Reinforcement Learning (MARL), the outcome of actions is not independent due to agents' interactions, unlike single-agent scenarios (Gronauer and Diepold, 2022).To illustrate multi-agent interactions, consider an adaptive robot car navigating an intersection.The robot car can move by steering, accelerating, or braking.Its objective is to safely traverse the intersection and reach its destination.In addition to detecting environmental factors such as positions, traffic lights, and lane markings, the robot car must be aware of other cars, including human-driven cars or other adaptive cars.With multiple adaptive agents in a shared environment, they simultaneously improve their policies, causing the environment to be nonstationary from a single agent's perspective.Sequential update schemes have been employed in Multi-Agent Decision Transformer (MADT) (Meng et al., 2021) and Multi-Agent Transformer (MAT) (Wen et al., 2022a) for efficient and stable multi-agent learning.Based on the Multi-Agent Advantage Decomposition theorem (Kuba et al., 2021), FDM guarantees joint performance improvement and learns an optimal joint policy efficiently.</p>
<p>Learning to Optimization</p>
<p>FDM approaches a series of white-box OR optimization problems as sequence modeling problems, enabling highly efficient inferences once it learns a suitable policy.OR optimization algorithms find the optimal solution within certain constraints based on accurate descriptions and characterizations of specific problems (Winston and Goldberg, 2004).OR algorithms do not require extensive data, and their results possess strong interpretability.As such, they have been widely employed in planning, scheduling, and coordination tasks.However, the computational complexity of traditional OR algorithms increases exponentially with problem size.In real-world applications, which often involve numerous uncertainties and large problem sizes, traditional discrete optimization algorithms may struggle to provide optimal solutions within a reasonable time frame.FDM is a data-driven approach capable of addressing the efficiency issues of OR algorithms.Once it learns a suitable approximate policy for OR problems, FDM can generate decisions with a single-pass forward inference.This means that, for any learned discrete optimization problem, FDM can eliminate computational complexity issues and achieve linear time complexity.</p>
<p>Real-world Applications</p>
<p>In previous sections, we discussed the advantages of FDM.To further demonstrate the effectiveness of FDM in real-world IDM applications, we present three decision problems that are well-suited for FDM to address.</p>
<p>Game AI</p>
<p>FDM enhances the generalization capabilities of game AI due to its strong multi-modal and multi-task performance.Deep RL (DRL) methods have achieved significant advancements in game AI, with examples such as the DQN agent (Hausknecht and Stone, 2015), AlphaGo (Silver et al., 2016), Libratus (Brown et al., 2017), OpenAI Five (Berner et al., 2019), and AlphaStar (Vinyals et al., 2019a), where DRL agents have defeated professional human players.These successes indicate DRL's capacity to solve highly complex games (Yin -DB1A woman in a white shirt and tie is sitting on a couch.</p>
<p>-labelthere are two zebras that are seen standing together -DB1A zebra standing in a field of grass.</p>
<p>-labelA woman in a white shirt and tie is sitting on a couch.</p>
<p>-DB1A man is riding a motorcycle down a street.et al., 2021).However, these DRL agents are typically designed for specific games.For instance, AlphaGo Zero (Silver et al., 2017) employs Monte Carlo tree search, self-play, and deep learning to defeat numerous professional Go players, showcasing powerful techniques for larger perfect information games.Utilizing self-play, deep reinforcement learning, and continuous transfer via surgery, OpenAI Five became the first AI to defeat world champions in an esports game, demonstrating practical techniques for complex, imperfect information games.In contrast, the real world comprises a myriad of games, making it nearly impossible for DRL agents to discover a universal policy through self-play, deep learning, and reinforcement learning alone.We argue that previous training frameworks, particularly self-play with distributed learning, may not be suitable for such situations, as two-player asymmetric games have distinct strategies for each side, and self-play-based mechanisms might not perform well.Through offline training, FDM can distill expert knowledge from various single-task expert agents and offer a unified solution for game AIs.</p>
<p>Production Scheduling</p>
<p>FDM significantly enhances problem-solving efficiency for production scheduling, enabling the generation of feasible solutions for real world tasks instantly.Scheduling is a highly complex aspect of production system management and control, crucial for improving the rapid response and global optimization capabilities of intelligent manufacturing systems.Although numerous studies have explored various scheduling models and methods, challenges persist in the field, posing questions for future development trends.Currently, information and intelligent technologies are driving changes and innovations in products and production organizations, increasing the complexity of manufacturing system management and control (Jiang et al., 2022).As production data grows exponentially, data-driven IDM models support service scheduling in sustainable manufacturing environments that synergistically optimize production efficiency and energy consumption (Waubert de Puiseau et al., 2022).</p>
<p>Robotics</p>
<p>Owing to its highly complex model, FDM can learn from diverse datasets, reducing the sim-to-real gap and enhancing the robustness of robotics in real-world applications.Achieving robots with the full range of physical capabilities that humans possess is a distant goal-arguably even more so than attaining AI systems with the full range of intellectual capabilities.In the physical world, an embodied agent encounters numerous changing factors, including physical parameters, action spaces, tasks, scene visual appearances, object geometry and topology, and more.Additionally, many critical real-world tasks demand generalizable policy learning, such as visual navigation, object manipulation, and autonomous driving (Bonin-Font et al., 2008).Therefore, learning generalizable policies is essential for developing intelligent embodied agents in the real world.Most current AI robotics models lack real-world experience due to sample efficiency issues and sim-to-real gaps, which limit the capabilities of IDM models.Generalization is also crucial when robots interact with the physical world.The key challenge is collecting the appropriate data and effectively learning from it (Hua et al., 2021).We argue that this is the reason we require an IDM model that can adapt to new embodiments and learn new tasks with less data (Singh et al., 2021).FDM can fulfill the generalization requirements for both embodiments and the physical world.FDM learns generalizable policies in the physical world through synergistic efforts across vision, learning, and robotics fields.</p>
<p>Case Study: the DigitalBrain Model</p>
<p>As discussed in previous sections, FDM presents a promising solution for real world IDM applications.We trained the DigitalBrain model, denoted as DB1, an implementation of FDM that demonstrates the potential of FDM for text, image-text, video games, robotics, and operations research tasks.In the following sections, we introduce the details of DB1, including model design, dataset collection and processing, training schemes, and empirical results.</p>
<p>Dataset Collection and Processing</p>
<p>We introduce three types of decision tasks in our case study: simulated control, vision &amp; language, and traveling salesman problem (TSP) tasks.For each category, we consider multiple heterogeneous sub-task sets to satisfy multi-modality learning requirements.Regarding data collection, we must ensure that the dataset is expert, as the loss function implies the training scheme of DB1 is supervised learning.To obtain high-quality datasets, the experts on simulated control tasks are implemented as well-trained reinforcement learning algorithms; the experts on vision and language tasks are humans; and the experts on TSP tasks are welldesigned solvers.For data processing, we follow the same principle as Gato, i.e., quantifying and discretizing raw data into integers.We provide more details below.</p>
<p>Simulated Control Tasks</p>
<p>The simulated control tasks consist of various video games and robotics control task sets, which have been treated as common benchmarking environments for reinforcement learning.</p>
<p>The multi-modality property in these tasks is not only reflected in the task types but also in the state/action space definition.</p>
<p>The simulated control tasks comprise various video games and robotics control task sets, which have been treated as common benchmarking environments for reinforcement learning.The multi-modality property in these tasks is not only reflected in the task types but also in the state/action space definition.</p>
<p> DeepMind Lab (Beattie et al., 2016) is a 3D learning environment developed for the research and development of artificial general intelligence and machine learning systems.It provides a suite of challenging 3D navigation and puzzle-solving tasks for learning agents.We split the existing levels in DeepMind Lab into 6 task sets and train a v-trace agent (Schmitt et al., 2020) on each task set for data collection, 65,000 episodes per level, for a total of 1.5M episodes.</p>
<p> Procgen (Cobbe et al., 2019) consists of 16 procedurally generated game-like environments for evaluating sample efficiency and generalization in reinforcement learning.</p>
<p>The data is collected with a v-trace or r2d2 agent (Schmitt et al., 2020) in each environment.All the distribution modes are set to hard, except for mazes and heists which are easy.We collect 100,000 episodes for each environment, for a total of 1.6M episodes.</p>
<p> Sokoban (Racanire et al., 2017) is a planning problem in which the agent has to push boxes to target locations.Since some of the moves are irreversible, mistakes can make the puzzle unsolvable.Planning ahead of time is therefore necessary to succeed on this puzzle.We take advantage of the reversed play processes when generating the maps.</p>
<p>We use a breadth-first search in the reversed play processes and extract the solutions from it as the expert data.So this expert data is optimal in most cases.</p>
<p> BabyAI (Chevalier-Boisvert et al., 2018) is a grid-world environment whose levels consist of instruction-following tasks described by a synthetic language.We generate data for these levels with the built-in BabyAI bot.The bot has access to extra information for executing optimal solutions.See Appendix C of Chevalier et al. (Chevalier-Boisvert et al., 2018) for more details about the bot.We collect 100,000 episodes for each level, for a total of 5.5M episodes.</p>
<p> Modular RL (Huang et al., 2020) is a collection of MuJoCo (Todorov et al., 2012) based continuous control environments, composed of variants of the OpenAI Gym (Brockman et al., 2016).Each variant is a morphological modification of the original body: the set of morphologies is generated by enumerating all possible subsets of limbs and keeping only those sets that a) contain the torso and b) still form a connected graph.This results in a set of variants with different input and output sizes, as well as different dynamics from the original morphologies.We use the qpos and qval as observations, just like in the OpenAI Gym.And we train a D4PG agent (Barth-Maron et al., 2018) on each variant.843.6K episodes are collected for this task suite.</p>
<p> Meta-World (Yu et al., 2020) is a suite of environments for benchmarking metareinforcement learning and multi-task learning.We collect data from all training and test tasks in the MT1 mode by training an APPO agent (Berner et al., 2019) from sample-factory repository (Petrenko et al., 2020).For this task suite, we collect 88.2K episodes in total.</p>
<p> The DeepMind Control Suite (Tunyasuvunakool et al., 2020) is a set of physics-based environments.For each task in the control suite, we collect two disjoint sets of data, one using only state features and the other using only pixels.We use a D4PG agent (Barth-Maron et al., 2018) to collect data from tasks with state features and an APPO agent (Berner et al., 2019) to collect data using pixels.For this task suite, we collect 675K episodes in total.</p>
<p>Vision and Language Tasks</p>
<p>DB1 is trained on an extensive collection of English language resources derived from web pages.Additionally, vision-language tasks such as Microsoft COCO Caption (Chen et al., 2015) are included.</p>
<p>TSP Tasks</p>
<p>To investigate DB1's capabilities in handling optimization problems in operations research, we selected the Traveling Salesman Problem (TSP) (Gavish and Graves, 1978), a well-known combinatorial optimization problem, as a representative task.Generally, TSP is modeled as a sequential problem (i.e., using coordination and neighbor information as states and node choice as decision-making), with various reinforcement learning methods developed for its solutions.Our dataset comprises 2 million trajectories from 200 tasks of different scales, specifically including 200 randomly generated tasks with 100 or 200 nodes uniformly sampled  (Lin and Kernighan, 1973), a heuristic algorithm.Since the original action space is large, we further narrow DB1's action space by using the expert's closest neighbor in each decision.For state representation, we encode neighbor information using a pre-trained Graph Convolutional Network (GCN) (Kipf and Welling, 2017).Sample weights for TSP tasks and simulated control tasks are listed in Table 2, totaling 87%.The remaining data consists of 8.7% from the language corpus and 5.1% from COCO Caption (Chen et al., 2015).</p>
<p>Model Design</p>
<p>As shown in Figure 2, we train an FDM model, DigitalBrain (DB1), with approximately 1.3B parameters.DB1 uses TransformerXL (Dai et al., 2019) as its backbone, which is a decoder-only model containing masked multi-head attention and feed-forward networks with residual connections.Compared to GPT (Brown et al., 2020;Radford et al., 2019), TransformerXL, located on the far left of Figure 2, employs a relative position encoding scheme instead of an absolute one and introduces a memory mechanism that enhances computational efficiency during inference and theoretically increases DB1's sequence length by caching and propagating each layer's hidden state.Although DB1 only uses the memory mechanism during the evaluation phase, we find it more efficient for sampling long sequences.</p>
<p>As shown in Table 3, DB1 comprises 16 transformer blocks with 16 heads, 2048 layer width, and shared embedding.We use a dropout probability of 0.1, PreNorm (Xiong et al., 2020) for layer normalization, and GeGLU (Shazeer, 2020) as the activation function.</p>
<p>Loss Function</p>
<p>We use a classical language modeling loss in the setting of teacher forcing (Williams and Zipser, 1989), i.e., we only predict the next token during training.For a batch of N instances of sequence length L, the loss is computed as the following equation with a given model parameter :
L() =  N 1 b=0 L1 l=0 m(b, l) log p  (s (b) l |s (b) 0 , s (b) 1 , . . . , s (b) l1 ),(1)
where s (b) l denotes the l-th token in the b-th sequence of a batch, m(b, l) denotes a binary loss mask on each token.We set m(b, l) = 1 when s (b) l is a token of an action or text, which means we compute the loss for all text tokens and action tokens in RL tasks, equivalent to Gato.</p>
<p>Tokenization for Multi-modal Data</p>
<p>Given the introduction of numerous multi-modal and heterogeneous datasets and tasks during training, it is necessary to convert these different data types into a unified representation.We implemented four types of tokenizers for different data types, as follows:</p>
<p> Text.A tokenizer is first trained on the collected corpora with a fixed vocabulary size of 32k.For the OpenWebText dataset, we tokenize beforehand and build a sample index based on sequence length during training.</p>
<p> Image.Images are initially divided into a series of 16-by-16 patches, followed by normalization within each patch.Without tokenization, each patch is mapped into the embedding space of our model.We use a ResNet-v2 block, similar to Gato, for extracting visual attributes.GroupNorm with 32 groups is used instead of BatchNorm.Additionally, the patch positional encoding specified in Gato is employed to inform the model about a patch's global position within the picture it was extracted from.By normalizing the patch's pixel intervals by the picture resolution, the relative row and column intervals of the patch are determined.The row and column normalized intervals are quantized into a vocabulary size of 128, used to index a row and column table of learnable position encodings.The transformation of quantized row and column intervals into indices depends on whether the model is being trained or evaluated: during training, a random index is uniformly selected from the quantized interval, while during evaluation, we deterministically take the (rounded) mean of the interval.</p>
<p>After retrieving row and column position encoding from the embedding table, it is added to the vision embedding created by a ResNet-v2 block.</p>
<p> Discrete data: Discrete values are tokenized into the range [0,1024), overlapping with our text vocabulary.</p>
<p> Continuous data: Continuous values are tokenized into the range [32000,33024).A special token  is introduced as a split between each s t and a t , with token ID 33204, represented as (s t , , a t ).</p>
<p>For any given environment, its types are categorized into text, image, and discrete and continuous values.For nested structures, each underlying attribute is categorized.Then, each observation within a timestep is concatenated according to a fixed order.First, values of different types are ordered as text, vision, and tensors.Vision patch tokens follow raster order, while tensor tokens are in row-major order.For nested structured input with each input type, the inputs are arranged lexicographically by their keys.To distinguish between the observation and action parts, we additionally use a specific reinforcement learning local positional encoding, as described in Gato.</p>
<p>Upon completing tokenization and sequencing, a parameterized embedding function is applied to each token, encompassing both observations and actions, in order to generate the final model input.For tokens associated with text, discrete-or continuous-valued observations, or actions for any given time-step are embedded through a lookup table into a learned vector embedding space.Learnable position encodings are incorporated for all tokens based on their local token position within the respective time-step.Tokens corresponding to image patches for any time-step are embedded utilizing a single ResNet block, which yields a vector for each patch.In the case of image patch token embeddings, a learnable within-image position encoding vector is also incorporated.</p>
<p>Comparisons with Gato</p>
<p>DB1 primarily focuses on the replication and validation of Gato, and attempts to make improvements in two aspects: network structure and parameter size, as well as task types and the number of tasks:</p>
<p>Parameter size and network structure: DB1 has 1.3 billion parameters, striving to be as close as possible to Gato in terms of parameter size.Overall, the structure employed by the DB1 is similar to that of Gato (same number of Decoder Blocks, hidden layer size, etc.).However, in the FeedForwardNetwork, due to the GeGLU activation function introducing an additional 1/3 of the parameter size, the DB1 uses a 4 * n_embeddimensional hidden layer state that, after passing through the GeGLU activation function, becomes a 2 * n_embeddimensional feature to approach Gato's parameter size.In other aspects, we share embedding parameters at the input and output encoding ends, as in Gato's implementation.Unlike Gato, we opted for the PostNorm approach in the selection of layer normalization.We also employed mixed-precision computation in Attention, which improves numerical stability.</p>
<p>Task types and the number of tasks: The number of experimental tasks in DB1 reaches 870, an increase of 44.04% compared to Gato, and a 2.23% improvement in expert performance (&gt;= 50%).In terms of specific task types, DB1 mainly inherits Gato's decisionmaking, image, and text-based tasks, with the number of tasks in each category remaining essentially consistent.However, in the area of decision-making tasks, DB1 introduces more than 200 additional real-world scenarios, namely, solving the Traveling Salesman Problem (TSP) with 100 and 200 node scales.These tasks involve randomly selecting 100  200 geographical locations from major cities in China as node representations.</p>
<p>Training Scheme</p>
<p>Hyperparameters.As shown in Table 3, DB1 consists of 16 transformer blocks with 16 heads, 2048 layer width and shared embedding.We use dropout probability of 0.1, PreNorm (Xiong et al., 2020) for layer normalization and GeGLU (Shazeer, 2020) as activation function.</p>
<p>Optimizer.The AdamW optimizer (Loshchilov and Hutter, 2017) is used, with linear warm-up and cosine schedule decay.The linear warmup lasts for 10, 000 steps, starting with a zero learning rate and ending at a maximum learning rate of 1e 4 .This learning rate is then cosine decayed by a factor of 10 over 1, 000, 000 steps.The AdamW optimizer has DB1A woman in a white shirt and tie is sitting on a couch.</p>
<p>Label: DB1:</p>
<p>A man on a motorcycle dressed in black leather.</p>
<p>A man is riding a motorcycle down a street.</p>
<p>Label: DB1:</p>
<p>There are two zebras that are seen standing together.</p>
<p>A zebra standing in a field of grass.</p>
<p>Label: DB1:</p>
<p>A woman looking at a new laptop.</p>
<p>A woman in a white shirt and tie is sitting on a couch.</p>
<p>Text Generation</p>
<p>Prompt: DB1:</p>
<p>Digital Brain Lab is a Shanghai based Digital Brain Lab is a Shanghai based company and we are looking for a highly skilled and experienced person to join our team in shanghai, China.</p>
<p>Prompt: DB1:</p>
<p>A wiki is a A wiki is a collection of all the info about a given character and their background in order for you and other readers (easy to find).This wiki also provides links, which you might use in your quest.parameters  1 = 0.9,  2 = 0.95, = 1e 8 .We use a batch size of 512 and a sequence length of 1024 tokens for all models.</p>
<p>Prompt Learning in Control Tasks.DB1 is trained to complete multiple control tasks by modeling them as different sequences.However, for tasks sharing the same observation and action specifications, more information is needed for the model to distinguish them.Compared to one-hot identifiers, DB1 is trained with prompt conditions (Liu et al., 2021a).</p>
<p>Dataset Building and Cache.The preprocessing process for reinforcement data is similar to that used in the Trajectory Transformer (Janner et al., 2021).We operate on separate trajectories split by environmental terminal signals; then, episodic returns, path lengths, and other statistical properties are calculated.Our index is built by enumerating a 3-tuple of path index, start index, and end index on a sliding window that contains the minimal timestep, from which we can obtain tokens of at least the sequence length set for the model.Loading all data into memory at once is usually infeasible due to its volume.To load indices and processed data as needed, we save them on disk for each dataset.</p>
<p>During training, we sample a batch of 512 sequences from different task domains with a fixed sample weight defined before training.We employ a prompt scheme similar to the one mentioned in Gato (Reed et al., 2022): 25% of samples in a batch will be prepended with a prompt sequence.Half of these prompt sequences come from the end of an episode for goal conditioning, and the other half are subsequences uniformly sampled from an episode.We use a fixed length of prompt length, set to about half the length of the model's input sequence.During evaluation, we select a successful demonstration as a prompt, i.e., episodes with top 10% returns, and feed the first k timesteps, whose length after tokenization is just equal to or more than the model's input sequence.</p>
<p>Evaluation Results</p>
<p>We assessed DB1's performance after 250,000 training iterations.For each simulated control and TSP task, we normalized the demonstration results to a range between 0 and 1.Specifically, we normalized the results using the random policy score R min , expert score R E , and the formula: score
= 1 N N i=1 (r i  R min )/(R E  R min ).
To account for the influence of randomness, we conducted N = 10 evaluations for each task (r i in the equation represents the result of the i-th evaluation) and reported the average normalized score as DB1's performance.</p>
<p>Figure 4 presents the performance distribution for each task set in the control and TSP tasks.Statistically, DB1 surpassed 50% of the expert scores in 661 of the 870 tasks.In BabyAI (Chevalier-Boisvert et al., 2018), DB1 achieved approximately 90% of the expert score across all 55 tasks.DB1 attained roughly 96% of the expert score for all TSP tasks, which may indicate that for combinatorial optimization problems like TSP, achieving a reasonably good result is relatively easier than obtaining a near-optimal one.The performance in DM Control Suite (Tunyasuvunakool et al., 2020) and Atari (Bellemare et al., 2013) was not as strong, potentially because the collected data were insufficient for supervised learning alone.</p>
<p>For the image-caption evaluation, we randomly sampled images from the COCO Caption Dataset (Chen et al., 2015) and presented the results without cherry-picking.Figure 3 displays selected results for text-generation and image-captioning tasks.The results demonstrate that DB1 effectively learned from the training corpus and established connections between vision and language modalities.The caption sentences indicate that DB1 can iden-tify the primary element in an image.Although minor errors exist in details such as numbers and colors, a larger dataset could potentially address these issues.</p>
<p>We have not directly compared the performance of DB1 with the GATO model, as the GATO model, its training tasks, and the collected training data are proprietary, preventing us from reproducing the results.In DMC and Atari tasks, the performance of DB1 is inferior to that of GATO, as the expert models we utilized for data collection were not sufficiently strong and could not achieve the effects of the algorithms employed by DeepMind (which are not officially open-source).Since the primary training method for DB1 and GATO is behavior cloning, the quality of the data essentially determines the upper limit during actual testing.</p>
<p>Generalization Capabilities</p>
<p>We believe that the generalization capabilities of DB1 indeed vary across different tasks.This is because, firstly, the model itself possesses different generalization abilities for different tasks, and secondly, the inherent characteristics of different tasks determine whether they are easily generalizable or not.For instance, in BabyAI, which is an environment built on a minigrid, the dynamics of the environment are essentially fixed.However, the requirements for different tasks are represented by natural language.As both DB1 and GATO have been pretrained on natural language, they are inherently well-suited to generalize in such settings.In contrast, Atari tasks are more difficult to generalize, and this issue has been alleviated only slightly after incorporating prompt conditioning.Due to the insufficient training length of the models and the longer horizon of the environment itself, generalization remains challenging for unseen tasks.</p>
<p>In addition, online adaptation for novel tasks can be employed by utilizing offline pretrained models.The most elementary optimization technique entails fine-tuning or specialization for both supervised and reinforcement learning paradigms.Numerous implementations exist for this objective, such as refining the model concurrently with learning to predict actions.Consequently, we can accomplish model-based planning and self-bootstrapping style training.Moreover, when reward labels are available, it is possible to instruct the model to assimilate values akin to return-to-go.This approach aids the model in actively discerning the quality of the training data and sampling in accordance with the specified target return.In scenarios with significant environmental randomness, techniques such as adversarial training and contrastive learning may be deployed to mitigate the influence of environmental stochasticity on the encoding of future information.</p>
<p>Conclusion</p>
<p>FDM is a fundamental yet efficient model for various multi-modal, multi-task decisionmaking problems, encompassing text, vision, reinforcement learning, and operations research tasks.As a foundation model, FDM inherently possesses strong generalization and efficient learning capabilities.In accordance with the scaling law, FDM's performance will improve as the scale of parameters and data increases, further enhancing the cross-task learning and generalization abilities of the model.Future work will involve developing a more autonomous decision-making model and integrating state-of-the-art IDM models into real-world environments by continuously training, fine-tuning, and scaling up the FDM.</p>
<p>Figure 1 :
1
Figure 1: Paradigm shifts of IDM: from traditional data analysis, control theory and OR methods, deep learning based perception methods to foundation model based methods.</p>
<p>Figure 2 :
2
Figure 2: Training pipeline of DB1.We distribute DB1 to multiple GPU nodes and devices to perform data-parallel training.A shared data storage is equipped with caching mechanism to speed up the data loading, and a distributed strategy for data saving.The sampled training data will be discretized with 4 tokenizers before feeding to the model.</p>
<p>Figure 3 :Figure 4 :
34
Figure 3: Selected results of vision and language tasks.</p>
<p>Table 1 :
1
Comparisons between DB1 and general FDM models.
As a partial implementation</p>
<p>Table 2 :
2
Datasets of simulated control and TSP task suites.For each task suite, we summarize the number of tasks, the number of collected trajectories, and the number of tokens.To avoid overfitting to a large dataset, the sample weights of task suites are well-tuned instead of being proportional to the dataset size.
Task SuiteTasks Trajectories Tokens Sample weightsDeepMind Lab4113M171B13%ALE Atari5232K1.4B13%Sokoban19102.8M1.7%BabyAI555.5M33.5B13%DM Control Suite25675K20.2B5.2%Meta-World4288.2K1.9B11.7%Procgen Benchmark 161.6M7.2B6.5%Modular RL38843.6K13.9B10.4%TSP2302.3M108.5B 11.7%</p>
<p>Table 3 :
3
Hyperparameters of DB1.
HyperparameterValue/ChoiceTransformer blocks24Attention heads16Layer width2048Shared embeddingTrueLayer NormalizationPostNormFeedForward size before activation 8192Activation FunctionGeGLUDropout Probability0.1from a 2D plane. The expert for data collection in TSP is derived from LKH</p>
<p>M Ahn, A Brohan, N Brown, Y Chebotar, O Cortes, B David, C Finn, K Gopalakrishnan, K Hausman, A Herzog, arXiv:2204.01691Do as i can, not as i say: Grounding language in robotic affordances. 2022arXiv preprint</p>
<p>Agent57: Outperforming the atari human benchmark. A P Badia, B Piot, S Kapturowski, P Sprechmann, A Vitvitskyi, Z D Guo, C Blundell, International Conference on Machine Learning. PMLR2020</p>
<p>Video pretraining (vpt): Learning to act by watching unlabeled online videos. B Baker, I Akkaya, P Zhokhov, J Huizinga, J Tang, A Ecoffet, B Houghton, R Sampedro, J Clune, arXiv:2206.117952022arXiv preprint</p>
<p>Beit: Bert pre-training of image transformers. H Bao, L Dong, F Wei, arXiv:2106.082542021arXiv preprint</p>
<p>Dxplain: an evolving diagnostic decision-support system. G O Barnett, J J Cimino, J A Hupp, E P Hoffer, Jama. 25811987</p>
<p>. G Barth-Maron, M W Hoffman, D Budden, W Dabney, D Horgan, D Tb, A Muldal, N Heess, T Lillicrap, arXiv:1804.086172018Distributed distributional deterministic policy gradients. arXiv preprint</p>
<p>. C Beattie, J Z Leibo, D Teplyashin, T Ward, M Wainwright, H Kttler, A Lefrancq, S Green, V Valds, A Sadik, arXiv:1612.038012016Deepmind lab. arXiv preprint</p>
<p>The arcade learning environment: An evaluation platform for general agents. M G Bellemare, Y Naddaf, J Veness, M Bowling, Journal of Artificial Intelligence Research. 472013</p>
<p>Dota 2 with large scale deep reinforcement learning. C Berner, G Brockman, B Chan, V Cheung, P Dbiak, C Dennison, D Farhi, Q Fischer, S Hashme, C Hesse, arXiv:1912.066802019arXiv preprint</p>
<p>On the opportunities and risks of foundation models. R Bommasani, D A Hudson, E Adeli, R Altman, S Arora, S Arx, M S Bernstein, J Bohg, A Bosselut, E Brunskill, arXiv:2108.072582021arXiv preprint</p>
<p>Visual navigation for mobile robots: A survey. F Bonin-Font, A Ortiz, G Oliver, Journal of intelligent and robotic systems. 5332008</p>
<p>. G Brockman, V Cheung, L Pettersson, J Schneider, J Schulman, J Tang, W Zaremba, arXiv:1606.015402016Openai gym. arXiv preprint</p>
<p>Libratus: The superhuman ai for no-limit poker. N Brown, T Sandholm, S Machine, IJCAI. 2017</p>
<p>Language models are few-shot learners. T Brown, NeurIPS. 2020</p>
<p>From unmanned systems to autonomous intelligent systems. J Chen, J Sun, G Wang, Engineering. 122022</p>
<p>Decision transformer: Reinforcement learning via sequence modeling. L Chen, K Lu, A Rajeswaran, K Lee, A Grover, M Laskin, P Abbeel, A Srinivas, I Mordatch, Advances in neural information processing systems. 202134</p>
<p>X Chen, H Fang, T.-Y Lin, R Vedantam, S Gupta, P Dollr, C L Zitnick, arXiv:1504.00325Microsoft coco captions: Data collection and evaluation server. 2015arXiv preprint</p>
<p>M Chevalier-Boisvert, D Bahdanau, S Lahlou, L Willems, C Saharia, T H Nguyen, Y Bengio, arXiv:1810.08272Babyai: A platform to study the sample efficiency of grounded language learning. 2018arXiv preprint</p>
<p>Leveraging procedural generation to benchmark reinforcement learning. K Cobbe, C Hesse, J Hilton, J Schulman, arXiv:1912.015882019arXiv preprint</p>
<p>Transformer-xl: Attentive language models beyond a fixed-length context. Z Dai, Z Yang, Y Yang, J Carbonell, Q V Le, R Salakhutdinov, arXiv:1901.028602019arXiv preprint</p>
<p>A survey of decision support system applications. S Eom, E Kim, Journal of the Operational Research Society. 57111995-2001. 2006</p>
<p>Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures. L Espeholt, H Soyer, R Munos, K Simonyan, V Mnih, T Ward, Y Doron, V Firoiu, T Harley, I Dunning, International conference on machine learning. PMLR2018</p>
<p>L Fan, G Wang, Y Jiang, A Mandlekar, Y Yang, H Zhu, A Tang, D.-A Huang, Y Zhu, A Anandkumar, arXiv:2206.08853Minedojo: Building open-ended embodied agents with internet-scale knowledge. 2022arXiv preprint</p>
<p>Look, listen, and act: Towards audio-visual embodied navigation. C Gan, Y Zhang, J Wu, B Gong, J B Tenenbaum, 2020 IEEE International Conference on Robotics and Automation (ICRA). IEEE2020</p>
<p>The travelling salesman problem and related problems. B Gavish, S C Graves, 1978</p>
<p>Multi-agent deep reinforcement learning: a survey. S Gronauer, K Diepold, Artificial Intelligence Review. 5522022</p>
<p>Comprehensive logistics. T Gudehus, H Kotzab, 2012Springer Science &amp; Business Media</p>
<p>Deep learning for industrial ai: Challenges, new methods and best practices. C Gupta, A Farahat, Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery Data Mining. the 26th ACM SIGKDD International Conference on Knowledge Discovery Data Mining2020</p>
<p>A survey on visual transformer. K Han, Y Wang, H Chen, X Chen, J Guo, Z Liu, Y Tang, A Xiao, C Xu, Y Xu, arXiv:2012.1255620202arXiv preprint</p>
<p>Deep recurrent q-learning for partially observable mdps. M Hausknecht, P Stone, AAAI fall symposium series. 2015. 2015</p>
<p>Handbook of production scheduling. J W Herrmann, 2006Springer Science &amp; Business Media89</p>
<p>Generative adversarial imitation learning. J Ho, S Ermon, 2016NeurIPS</p>
<p>Learning for a robot: Deep reinforcement learning, imitation learning, transfer learning. J Hua, L Zeng, G Li, Z Ju, Sensors. 21412782021</p>
<p>One policy to control them all: Shared modular policies for agent-agnostic control. W Huang, I Mordatch, D Pathak, International Conference on Machine Learning. PMLR2020</p>
<p>Offline reinforcement learning as one big sequence modeling problem. M Janner, Q Li, S Levine, Advances in neural information processing systems. 202134</p>
<p>Vision for looking at traffic lights: Issues, survey, and perspectives. M B Jensen, M P Philipsen, A Mgelmose, T B Moeslund, M M Trivedi, IEEE Transactions on Intelligent Transportation Systems. 1772016</p>
<p>The evolution of production scheduling from industry 3.0 through industry 4.0. Z Jiang, S Yuan, J Ma, Q Wang, International Journal of Production Research. 60112022</p>
<p>L Kaiser, A N Gomez, N Shazeer, A Vaswani, N Parmar, L Jones, J Uszkoreit, arXiv:1706.05137One model to learn them all. 2017arXiv preprint</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. J D , M.-W C Kenton, L K Toutanova, Proceedings of NAACL-HLT. NAACL-HLT2019</p>
<p>The role of intuition in strategic decision making. N Khatri, H A Ng, Human relations. 5312000</p>
<p>Semi-supervised classification with graph convolutional networks. T N Kipf, M Welling, International Conference on Learning Representations. 2017</p>
<p>Optimal control theory: an introduction. D E Kirk, Courier Corporation. 2004</p>
<p>A survey of generalisation in deep reinforcement learning. R Kirk, A Zhang, E Grefenstette, T Rocktschel, arXiv:2111.097942021arXiv preprint</p>
<p>Imagenet classification with deep convolutional neural networks. A Krizhevsky, 2012NeurIPS</p>
<p>J G Kuba, R Chen, M Wen, Y Wen, F Sun, J Wang, Y Yang, arXiv:2109.11251Trust region policy optimisation in multi-agent reinforcement learning. 2021arXiv preprint</p>
<p>Deep learning. Y Lecun, Y Bengio, G Hinton, nature. 52175532015</p>
<p>An effective heuristic algorithm for the traveling-salesman problem. S Lin, B W Kernighan, Operations research. 2121973</p>
<p>A structured self-attentive sentence embedding. Z Lin, M Feng, C N D Santos, M Yu, B Xiang, B Zhou, Y Bengio, arXiv:1703.031302017arXiv preprint</p>
<p>Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. P Liu, W Yuan, J Fu, Z Jiang, H Hayashi, G Neubig, arXiv:2107.135862021aarXiv preprint</p>
<p>Swin transformer: Hierarchical vision transformer using shifted windows. Z Liu, Y Lin, Y Cao, H Hu, Y Wei, Z Zhang, S Lin, B Guo, 2021b</p>
<p>I Loshchilov, F Hutter, arXiv:1711.05101Decoupled weight decay regularization. 2017arXiv preprint</p>
<p>Offline pre-trained multi-agent decision transformer: One big sequence model conquers all starcraftii tasks. L Meng, M Wen, Y Yang, C Le, X Li, W Zhang, Y Wen, H Zhang, J Wang, B Xu, arXiv:2112.028452021arXiv preprint</p>
<p>V Mnih, K Kavukcuoglu, D Silver, A Graves, I Antonoglou, D Wierstra, M Riedmiller, R R Murphy, arXiv:1312.5602Playing atari with deep reinforcement learning. 2013. 2019arXiv preprintIntroduction to ai robotics</p>
<p>Fundamentals of air traffic control. Cengage learning. M S Nolan, 2011</p>
<p>Industrial artificial intelligence in industry 4.0-systematic review, challenges and outlook. R S Peres, X Jia, J Lee, K Sun, A W Colombo, J Barata, IEEE Access. 82020</p>
<p>The multi-agent reinforcement learning in malm\" o (marl\" o) competition. D Perez-Liebana, K Hofmann, S P Mohanty, N Kuno, A Kramer, S Devlin, R D Gaina, D Ionita, arXiv:1901.081292019arXiv preprint</p>
<p>Sample factory: Egocentric 3d control from pixels at 100000 fps with asynchronous reinforcement learning. A Petrenko, Z Huang, T Kumar, G Sukhatme, V Koltun, International Conference on Machine Learning. PMLR2020</p>
<p>Intelligent decision making: An AI-based approach. G Phillips-Wren, N Ichalkaranje, 2008Springer Science &amp; Business Media97</p>
<p>Developing capabilities: Lifelong learning in the age of ai. O Poquet, M De Laat, British Journal of Educational Technology. 5242021</p>
<p>Neorl: A near real-world benchmark for offline reinforcement learning. R Qin, S Gao, X Zhang, Z Xu, S Huang, Z Li, W Zhang, Y Yu, arXiv:2102.007142021arXiv preprint</p>
<p>Imagination-augmented agents for deep reinforcement learning. S Racanire, T Weber, D Reichert, L Buesing, A Guez, D Jimenez Rezende, A Puigdomnech, O Badia, N Vinyals, Y Heess, Li, Advances in neural information processing systems. 201730</p>
<p>Language models are unsupervised multitask learners. A Radford, J Wu, R Child, D Luan, D Amodei, I Sutskever, OpenAI blog. 1892019</p>
<p>Learning transferable visual models from natural language supervision. A Radford, J W Kim, C Hallacy, A Ramesh, G Goh, S Agarwal, G Sastry, A Askell, P Mishkin, J Clark, International Conference on Machine Learning. PMLR2021</p>
<p>Business intelligence: Concepts, components, techniques and benefits. J Ranjan, Journal of theoretical and applied information technology. 912009</p>
<p>. S Reed, K Zolna, E Parisotto, S G Colmenarejo, A Novikov, G Barth-Maron, M Gimenez, Y Sulsky, J Kay, J T Springenberg, arXiv:2205.061752022A generalist agent. arXiv preprint</p>
<p>Artificial intelligence a modern approach. S J Russell, 2010Pearson Education, Inc</p>
<p>Crossing the reality gap: a survey on sim-to-real transferability of robot controllers in reinforcement learning. E Salvato, G Fenu, E Medvet, F A Pellegrino, arXiv:1802.08864One big net for everything. 2018arXiv preprint</p>
<p>Off-policy actor-critic with shared experience replay. S Schmitt, M Hessel, K Simonyan, International Conference on Machine Learning. PMLR2020</p>
<p>Categorization= decision making+ generalization. C A Seger, E J Peterson, Neuroscience &amp; Biobehavioral Reviews. 3772013</p>
<p>N Shazeer, arXiv:2002.05202Glu variants improve transformer. 2020arXiv preprint</p>
<p>Mastering the game of go with deep neural networks and tree search. D Silver, A Huang, C J Maddison, A Guez, L Sifre, G Van Den Driessche, J Schrittwieser, I Antonoglou, V Panneershelvam, M Lanctot, nature. 52975872016</p>
<p>Mastering the game of go without human knowledge. D Silver, J Schrittwieser, K Simonyan, I Antonoglou, A Huang, A Guez, T Hubert, L Baker, M Lai, A Bolton, nature. 55076762017</p>
<p>Bounded rationality. H A Simon, Utility and probability. Springer1990</p>
<p>Reinforcement learning in robotic applications: a comprehensive survey. B Singh, R Kumar, V P Singh, Artificial Intelligence Review. 2021</p>
<p>Sequence to sequence learning with neural networks. I Sutskever, O Vinyals, Q V Le, Advances in neural information processing systems. 201427</p>
<p>R Sutton, The bitter lesson. Incomplete Ideas (blog). 20191312</p>
<p>Reinforcement Learning: An Introduction. A Bradford Book. R S Sutton, A G Barto, 2018Cambridge, MA, USA</p>
<p>Mujoco: A physics engine for model-based control. E Todorov, T Erez, Y Tassa, 2012 IEEE/RSJ international conference on intelligent robots and systems. IEEE2012</p>
<p>Multimodal transformer for unaligned multimodal language sequences. Y.-H H Tsai, S Bai, P P Liang, J Z Kolter, L.-P Morency, R Salakhutdinov, Proceedings of the conference. the conferenceNIH Public Access201920196558</p>
<p>dm control: Software and tasks for continuous control. Software Impacts, 6:100022. S Tunyasuvunakool, A Muldal, Y Doron, S Liu, S Bohez, J Merel, T Erez, T Lillicrap, N Heess, Y Tassa, 10.1016/j.simpa.2020.100022.URLhttps://www.sciencedirect.com/science/article/pii/S26659638203000992020</p>
<p>Multi-task learning for dense prediction tasks: A survey. S Vandenhende, S Georgoulis, W Van Gansbeke, M Proesmans, D Dai, L Van Gool, 2021</p>
<p>Grandmaster level in starcraft ii using multi-agent reinforcement learning. O Vinyals, I Babuschkin, W M Czarnecki, M Mathieu, A Dudzik, J Chung, D H Choi, R Powell, T Ewalds, P Georgiev, Nature. 57577822019a</p>
<p>Grandmaster level in starcraft ii using multi-agent reinforcement learning. O Vinyals, I Babuschkin, W M Czarnecki, M Mathieu, A Dudzik, J Chung, D H Choi, R Powell, T Ewalds, P Georgiev, Nature. 57577822019b</p>
<p>Display advertising with real-time bidding (rtb) and behavioural targeting. J Wang, W Zhang, S Yuan, Foundations and Trends in Information Retrieval. 114-52017</p>
<p>Image as a foreign language: Beit pretraining for all vision and vision-language tasks. W Wang, H Bao, L Dong, J Bjorck, Z Peng, Q Liu, K Aggarwal, O K Mohammed, S Singhal, S Som, arXiv:2208.104422022arXiv preprint</p>
<p>On reliability of reinforcement learning based production scheduling systems: a comparative survey. C Waubert De Puiseau, R Meyes, T Meisen, Journal of Intelligent Manufacturing. 3342022</p>
<p>Multi-agent reinforcement learning is a sequence modeling problem. M Wen, J G Kuba, R Lin, W Zhang, Y Wen, J Wang, Y Yang, arXiv:2205.149532022aarXiv preprint</p>
<p>Q Wen, T Zhou, C Zhang, W Chen, Z Ma, J Yan, L Sun, arXiv:2202.07125Transformers in time series: A survey. 2022barXiv preprint</p>
<p>A learning algorithm for continually running fully recurrent neural networks. R J Williams, D Zipser, Neural computation. 121989</p>
<p>Operations research: applications and algorithms. W L Winston, J B Goldberg, 2004Thomson Brooks/Cole Belmont3</p>
<p>On layer normalization in the transformer architecture. R Xiong, Y Yang, D He, K Zheng, S Zheng, C Xing, H Zhang, Y Lan, L Wang, T Liu, International Conference on Machine Learning. PMLR2020</p>
<p>Multimodal learning with transformers: A survey. P Xu, X Zhu, D A Clifton, arXiv:2206.064882022arXiv preprint</p>
<p>Q Yin, J Yang, W Ni, B Liang, K Huang, arXiv:2111.07631Ai in games: Techniques, challenges and opportunities. 2021arXiv preprint</p>
<p>Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning. T Yu, D Quillen, Z He, R Julian, K Hausman, C Finn, S Levine, Conference on robot learning. PMLR2020</p>
<p>A survey on multi-task learning. Y Zhang, Q Yang, IEEE Transactions on Knowledge and Data Engineering. 2021</p>
<p>Sim-to-real transfer in deep reinforcement learning for robotics: a survey. W Zhao, J P Queralta, T Westerlund, 2020 IEEE Symposium Series on Computational Intelligence (SSCI). IEEE2020</p>
<p>B Zheng, S Verma, J Zhou, I Tsang, F Chen, arXiv:2106.12177Imitation learning: Progress, taxonomies and opportunities. 2021arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
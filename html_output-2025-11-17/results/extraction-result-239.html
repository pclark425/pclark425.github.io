<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-239 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-239</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-239</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-14.html">extraction-schema-14</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <p><strong>Paper ID:</strong> paper-54457447</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/1812.02825v1.pdf" target="_blank">Attending to Mathematical Language with Transformers</a></p>
                <p><strong>Paper Abstract:</strong> Mathematical expressions were generated, evaluated and used to train neural network models based on the transformer architecture. The expressions and their targets were analyzed as a character-level sequence transduction task in which the encoder and decoder are built on attention mechanisms. Three models were trained to understand and evaluate symbolic variables and expressions in mathematics: (1) the self-attentive and feed-forward transformer without recurrence or convolution, (2) the universal transformer with recurrence, and (3) the adaptive universal transformer with recurrence and adaptive computation time. The models respectively achieved test accuracies as high as 76.1%, 78.8% and 83.9% in evaluating the expressions to match the target values. For the cases inferred incorrectly, the results were very close to the targets. The models notably learned to add, subtract and multiply both positive and negative decimal numbers of variable digits assigned to symbolic variables.</p>
                <p><strong>Cost:</strong> 0.008</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e239.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e239.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-attentive Feed-forward Transformer (encoder-decoder)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An encoder-decoder transformer trained end-to-end at the character level to read symbolic variable assignments and output evaluated decimal integer results; uses multi-head self-attention and feed-forward layers without recurrence or convolution.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Attending to Mathematical Language with Transformers</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer (standard encoder-decoder)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>encoder-decoder transformer (self-attentive, feed-forward, no recurrence)</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>addition, subtraction, multiplication (character-level symbolic expressions)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>decimal integers assigned to symbolic variables in range [-1000, 1000) (variable digits, positive and negative; up to 3-digit magnitude)</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>supervised training / fine-tuning on a synthetic dataset (~12M examples) of character-level input→output pairs; no curriculum training; standard transformer hyperparameters (smaller model: 2 encoder layers, 2 decoder layers, 4 attention heads, 128 FF neurons); character-level seq2seq</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Overall test accuracy 76.1%. By expression type: symmetric expressions (x op x) inferred with perfect accuracy; addition (asymmetric) tasks ~98% accuracy; subtraction (asymmetric) tasks ~49% accuracy; multiplication (asymmetric) tasks ~9% accuracy. Incorrect outputs are often close to targets (character-level proximity).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Attention visualizations show decoder output characters attend strongly to characters representing the assigned variable (especially y); first and last output positions attend selectively to first/last input positions, while middle output positions show less selective attention; model appears to learn to map variable assignment substrings to outputs, but shows confusion in middle output positions suggesting imperfect handling of multi-digit interactions (e.g., carries). No explicit algorithmic carry-tracking is demonstrated; model relies on learned attention patterns over character sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>No parameter-scaling study reported; performance improved when adding architectural inductive biases (recurrence / adaptive computation) in follow-up models rather than via model size changes.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Severe difficulty on multi-digit asymmetric multiplication tasks (very low accuracy); substantial errors concentrated in middle output character positions (e.g., incorrect thousands-place digit); struggles with asymmetric subtraction compared to addition; confusion in attention for middle output positions likely causes middle-digit errors.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against Universal Transformer and Adaptive Universal Transformer using same dataset and hyperparameters.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>A standard encoder-decoder transformer can learn character-level symbolic arithmetic (especially symmetric and addition tasks) from data, but struggles badly on asymmetric multi-digit multiplication and non-symmetric subtraction without recurrent or adaptive computation inductive biases.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Attending to Mathematical Language with Transformers', 'publication_date_yy_mm': '2018-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e239.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e239.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Universal Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Universal Transformer (with recurrent inductive bias)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer variant that iteratively refines position representations via recurrence (self-attention followed by a recurrent transformation), applied here to character-level evaluation of symbolic arithmetic expressions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Attending to Mathematical Language with Transformers</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Universal Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>encoder-decoder universal transformer (recurrent iterative refinement of representations; self-attention + recurrent/position-wise transformation)</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>addition, subtraction, multiplication (character-level symbolic expressions)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>decimal integers assigned to symbolic variables in range [-1000, 1000) (variable digits, positive and negative; up to 3-digit magnitude)</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>supervised training on same synthetic dataset (~12M examples) and same hyperparameters as standard transformer, but with universal transformer recurrence (iterative refinement); no curriculum training</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Overall test accuracy 78.8%. Improved performance on asymmetric expressions relative to the standard transformer, with the largest gains for multiplication tasks (multiplication accuracy more than doubled compared to the standard transformer). Specific per-operation numbers not fully enumerated beyond the overall improvement claim and multiplication more-than-doubled statement.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Authors attribute improvements to the recurrent inductive bias (iterative refinement) of the universal transformer, which likely helps the model implement more algorithmic computation steps needed for asymmetric tasks (notably multiplication). Attention patterns remain useful for mapping inputs to outputs, but the recurrence appears to enable better handling of long-range/multi-step dependencies.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Performance improved relative to standard transformer when recurrence (universal transformer) was added, holding hyperparameters and dataset constant; no explicit parameter-size scaling reported.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Still had errors on subtraction and multiplication relative to ideal; multiplication remained challenging (though improved), indicating recurrence helps but does not fully solve multi-digit multiplication errors.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared directly to the standard transformer and to the adaptive universal transformer (same dataset/hyperparameters).</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Adding recurrent iterative refinement (universal transformer) improves character-level arithmetic evaluation, especially for multiplication, suggesting recurrence provides an inductive bias that helps implement multi-step algorithmic computations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Attending to Mathematical Language with Transformers', 'publication_date_yy_mm': '2018-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e239.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e239.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Adaptive Universal Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adaptive Universal Transformer (universal transformer + adaptive computation time)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A universal transformer augmented with adaptive computation time (ACT) that allows variable numbers of recurrent refinement steps per position, applied to character-level symbolic arithmetic evaluation to allocate more computation to harder symbols.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Attending to Mathematical Language with Transformers</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Adaptive Universal Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>encoder-decoder universal transformer with adaptive computation time (position-wise halting mechanism allowing variable computation steps)</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>addition, subtraction, multiplication (character-level symbolic expressions)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>decimal integers assigned to symbolic variables in range [-1000, 1000) (variable digits, positive and negative; up to 3-digit magnitude)</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>same supervised training on synthetic dataset (~12M examples) using universal transformer with adaptive computation time (ACT) to allocate more processing to difficult symbols; no curriculum training</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Overall test accuracy 83.9%. Adaptive universal transformer almost perfectly solved asymmetric subtraction tasks (near-perfect on − tasks), but performed worse on multiplication tasks compared to the (non-adaptive) universal transformer; net effect was highest overall accuracy among the three models.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Adaptive computation time appears to focus additional processing on symbols/positions the model finds difficult, improving subtraction performance markedly; however, ACT did not improve (and worsened) multiplication performance, suggesting that multiplying multi-digit numbers requires different algorithmic capability (not solved merely by allocating more recurrent steps to select positions). Attention visualizations still indicate outputs attend to variable assignment substrings, with middle output positions less selective.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Performance changed qualitatively with addition of adaptive computation: subtraction improved nearly to perfect, multiplication degraded relative to universal transformer; no parameter-size scaling analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Continued weakness on multi-digit multiplication; adaptive computation improved some failure modes (subtraction) but may have reallocated capacity away from solving multiplication, indicating trade-offs in per-position computation allocation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against the standard transformer and universal transformer under same data and hyperparameters; shows trade-offs when using ACT (better subtraction, worse multiplication).</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Allowing adaptive per-position computation (ACT) can strongly improve some arithmetic sub-tasks (near-perfect subtraction) but does not uniformly solve multi-digit multiplication, indicating that different architectural inductive biases are required for different arithmetic algorithmic demands.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Attending to Mathematical Language with Transformers', 'publication_date_yy_mm': '2018-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Learning to execute <em>(Rating: 2)</em></li>
                <li>Neural GPUs learn algorithms <em>(Rating: 2)</em></li>
                <li>Universal transformers <em>(Rating: 2)</em></li>
                <li>Adaptive computation time for recurrent neural networks <em>(Rating: 2)</em></li>
                <li>Visual Learning of Arithmetic Operation <em>(Rating: 1)</em></li>
                <li>Extensions and Limitations of the Neural GPU <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-239",
    "paper_id": "paper-54457447",
    "extraction_schema_id": "extraction-schema-14",
    "extracted_data": [
        {
            "name_short": "Transformer",
            "name_full": "Self-attentive Feed-forward Transformer (encoder-decoder)",
            "brief_description": "An encoder-decoder transformer trained end-to-end at the character level to read symbolic variable assignments and output evaluated decimal integer results; uses multi-head self-attention and feed-forward layers without recurrence or convolution.",
            "citation_title": "Attending to Mathematical Language with Transformers",
            "mention_or_use": "use",
            "model_name": "Transformer (standard encoder-decoder)",
            "model_size": null,
            "model_architecture": "encoder-decoder transformer (self-attentive, feed-forward, no recurrence)",
            "arithmetic_operation_type": "addition, subtraction, multiplication (character-level symbolic expressions)",
            "number_range_or_complexity": "decimal integers assigned to symbolic variables in range [-1000, 1000) (variable digits, positive and negative; up to 3-digit magnitude)",
            "method_or_intervention": "supervised training / fine-tuning on a synthetic dataset (~12M examples) of character-level input→output pairs; no curriculum training; standard transformer hyperparameters (smaller model: 2 encoder layers, 2 decoder layers, 4 attention heads, 128 FF neurons); character-level seq2seq",
            "performance_result": "Overall test accuracy 76.1%. By expression type: symmetric expressions (x op x) inferred with perfect accuracy; addition (asymmetric) tasks ~98% accuracy; subtraction (asymmetric) tasks ~49% accuracy; multiplication (asymmetric) tasks ~9% accuracy. Incorrect outputs are often close to targets (character-level proximity).",
            "mechanistic_insight": "Attention visualizations show decoder output characters attend strongly to characters representing the assigned variable (especially y); first and last output positions attend selectively to first/last input positions, while middle output positions show less selective attention; model appears to learn to map variable assignment substrings to outputs, but shows confusion in middle output positions suggesting imperfect handling of multi-digit interactions (e.g., carries). No explicit algorithmic carry-tracking is demonstrated; model relies on learned attention patterns over character sequences.",
            "performance_scaling": "No parameter-scaling study reported; performance improved when adding architectural inductive biases (recurrence / adaptive computation) in follow-up models rather than via model size changes.",
            "failure_modes": "Severe difficulty on multi-digit asymmetric multiplication tasks (very low accuracy); substantial errors concentrated in middle output character positions (e.g., incorrect thousands-place digit); struggles with asymmetric subtraction compared to addition; confusion in attention for middle output positions likely causes middle-digit errors.",
            "comparison_baseline": "Compared against Universal Transformer and Adaptive Universal Transformer using same dataset and hyperparameters.",
            "key_finding": "A standard encoder-decoder transformer can learn character-level symbolic arithmetic (especially symmetric and addition tasks) from data, but struggles badly on asymmetric multi-digit multiplication and non-symmetric subtraction without recurrent or adaptive computation inductive biases.",
            "uuid": "e239.0",
            "source_info": {
                "paper_title": "Attending to Mathematical Language with Transformers",
                "publication_date_yy_mm": "2018-12"
            }
        },
        {
            "name_short": "Universal Transformer",
            "name_full": "Universal Transformer (with recurrent inductive bias)",
            "brief_description": "A transformer variant that iteratively refines position representations via recurrence (self-attention followed by a recurrent transformation), applied here to character-level evaluation of symbolic arithmetic expressions.",
            "citation_title": "Attending to Mathematical Language with Transformers",
            "mention_or_use": "use",
            "model_name": "Universal Transformer",
            "model_size": null,
            "model_architecture": "encoder-decoder universal transformer (recurrent iterative refinement of representations; self-attention + recurrent/position-wise transformation)",
            "arithmetic_operation_type": "addition, subtraction, multiplication (character-level symbolic expressions)",
            "number_range_or_complexity": "decimal integers assigned to symbolic variables in range [-1000, 1000) (variable digits, positive and negative; up to 3-digit magnitude)",
            "method_or_intervention": "supervised training on same synthetic dataset (~12M examples) and same hyperparameters as standard transformer, but with universal transformer recurrence (iterative refinement); no curriculum training",
            "performance_result": "Overall test accuracy 78.8%. Improved performance on asymmetric expressions relative to the standard transformer, with the largest gains for multiplication tasks (multiplication accuracy more than doubled compared to the standard transformer). Specific per-operation numbers not fully enumerated beyond the overall improvement claim and multiplication more-than-doubled statement.",
            "mechanistic_insight": "Authors attribute improvements to the recurrent inductive bias (iterative refinement) of the universal transformer, which likely helps the model implement more algorithmic computation steps needed for asymmetric tasks (notably multiplication). Attention patterns remain useful for mapping inputs to outputs, but the recurrence appears to enable better handling of long-range/multi-step dependencies.",
            "performance_scaling": "Performance improved relative to standard transformer when recurrence (universal transformer) was added, holding hyperparameters and dataset constant; no explicit parameter-size scaling reported.",
            "failure_modes": "Still had errors on subtraction and multiplication relative to ideal; multiplication remained challenging (though improved), indicating recurrence helps but does not fully solve multi-digit multiplication errors.",
            "comparison_baseline": "Compared directly to the standard transformer and to the adaptive universal transformer (same dataset/hyperparameters).",
            "key_finding": "Adding recurrent iterative refinement (universal transformer) improves character-level arithmetic evaluation, especially for multiplication, suggesting recurrence provides an inductive bias that helps implement multi-step algorithmic computations.",
            "uuid": "e239.1",
            "source_info": {
                "paper_title": "Attending to Mathematical Language with Transformers",
                "publication_date_yy_mm": "2018-12"
            }
        },
        {
            "name_short": "Adaptive Universal Transformer",
            "name_full": "Adaptive Universal Transformer (universal transformer + adaptive computation time)",
            "brief_description": "A universal transformer augmented with adaptive computation time (ACT) that allows variable numbers of recurrent refinement steps per position, applied to character-level symbolic arithmetic evaluation to allocate more computation to harder symbols.",
            "citation_title": "Attending to Mathematical Language with Transformers",
            "mention_or_use": "use",
            "model_name": "Adaptive Universal Transformer",
            "model_size": null,
            "model_architecture": "encoder-decoder universal transformer with adaptive computation time (position-wise halting mechanism allowing variable computation steps)",
            "arithmetic_operation_type": "addition, subtraction, multiplication (character-level symbolic expressions)",
            "number_range_or_complexity": "decimal integers assigned to symbolic variables in range [-1000, 1000) (variable digits, positive and negative; up to 3-digit magnitude)",
            "method_or_intervention": "same supervised training on synthetic dataset (~12M examples) using universal transformer with adaptive computation time (ACT) to allocate more processing to difficult symbols; no curriculum training",
            "performance_result": "Overall test accuracy 83.9%. Adaptive universal transformer almost perfectly solved asymmetric subtraction tasks (near-perfect on − tasks), but performed worse on multiplication tasks compared to the (non-adaptive) universal transformer; net effect was highest overall accuracy among the three models.",
            "mechanistic_insight": "Adaptive computation time appears to focus additional processing on symbols/positions the model finds difficult, improving subtraction performance markedly; however, ACT did not improve (and worsened) multiplication performance, suggesting that multiplying multi-digit numbers requires different algorithmic capability (not solved merely by allocating more recurrent steps to select positions). Attention visualizations still indicate outputs attend to variable assignment substrings, with middle output positions less selective.",
            "performance_scaling": "Performance changed qualitatively with addition of adaptive computation: subtraction improved nearly to perfect, multiplication degraded relative to universal transformer; no parameter-size scaling analysis.",
            "failure_modes": "Continued weakness on multi-digit multiplication; adaptive computation improved some failure modes (subtraction) but may have reallocated capacity away from solving multiplication, indicating trade-offs in per-position computation allocation.",
            "comparison_baseline": "Compared against the standard transformer and universal transformer under same data and hyperparameters; shows trade-offs when using ACT (better subtraction, worse multiplication).",
            "key_finding": "Allowing adaptive per-position computation (ACT) can strongly improve some arithmetic sub-tasks (near-perfect subtraction) but does not uniformly solve multi-digit multiplication, indicating that different architectural inductive biases are required for different arithmetic algorithmic demands.",
            "uuid": "e239.2",
            "source_info": {
                "paper_title": "Attending to Mathematical Language with Transformers",
                "publication_date_yy_mm": "2018-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Learning to execute",
            "rating": 2,
            "sanitized_title": "learning_to_execute"
        },
        {
            "paper_title": "Neural GPUs learn algorithms",
            "rating": 2,
            "sanitized_title": "neural_gpus_learn_algorithms"
        },
        {
            "paper_title": "Universal transformers",
            "rating": 2,
            "sanitized_title": "universal_transformers"
        },
        {
            "paper_title": "Adaptive computation time for recurrent neural networks",
            "rating": 2,
            "sanitized_title": "adaptive_computation_time_for_recurrent_neural_networks"
        },
        {
            "paper_title": "Visual Learning of Arithmetic Operation",
            "rating": 1,
            "sanitized_title": "visual_learning_of_arithmetic_operation"
        },
        {
            "paper_title": "Extensions and Limitations of the Neural GPU",
            "rating": 1,
            "sanitized_title": "extensions_and_limitations_of_the_neural_gpu"
        }
    ],
    "cost": 0.007918999999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Attending to Mathematical Language with Transformers</p>
<p>Artit Wangperawong artitw@gmail.com 
Attending to Mathematical Language with Transformers
1 New York, NY, USAnatural language understandingneural networksmachine learningartificial intelligence
Mathematical expressions were generated, evaluated and used to train neural network models based on the transformer architecture. The expressions and their targets were analyzed as a character-level sequence transduction task in which the encoder and decoder are built on attention mechanisms. Three models were trained to understand and evaluate symbolic variables and expressions in mathematics: (1) the self-attentive and feed-forward transformer without recurrence or convolution, (2) the universal transformer with recurrence, and (3) the adaptive universal transformer with recurrence and adaptive computation time. The models respectively achieved test accuracies as high as 76.1%, 78.8% and 83.9% in evaluating the expressions to match the target values. For the cases inferred incorrectly, the results were very close to the targets. The models notably learned to add, subtract and multiply both positive and negative decimal numbers of variable digits assigned to symbolic variables.</p>
<p>Arithmetic and algebra are important mathematical skills that should be acquired by one's adolescence [1]. Therefore, we should expect that an artificially intelligent agent or system can at least master such problems without predetermined algorithms. Arithmetic involves the study of numbers and the effect on them of operators such as addition (+), subtraction (-), multiplication (*), and division (÷). Algebra at the basic level involves the study of mathematical symbols and the rules governing how such symbols are manipulated. A mathematical expression is a phrase constructed with a finite arrangement of numbers, symbols and operators according to the rules of mathematics. Such rules are typically pre-programmed into computers and execute with ideally perfect accuracy. Here we describe neural network models trained to read mathematical phrases at the character level and evaluate the expressions for a result without any pre-programmed or hard-coded math rules.</p>
<p>Prior studies related to this work have used multilayer perceptrons [2], recurrent neural networks (RNN) [3,4], long short-term memory (LSTM) [5,6], Neural GPUs [7][8][9] and transformers [10,11]. These studies were mostly restricted to addition of integers with the same number of digits and did not involve symbolic variables or expressions. The study involving mathematical expressions sought to discover efficient mathematical identities [3]. For the studies that considered multiplication, accuracy for the multiplication tasks were either not explicitly reported [5,11] or involved binary representations [7][8][9]. In this work, we report results for directly evaluating mathematical expressions involving addition, subtraction and multiplication of both positive and negative decimal numbers with variable digits assigned to symbolic variables. The end-to-end process described below does not include any curriculum training or intermediary non-decimal representations. The training and test data are generated by assigning symbolic variables either positive or negative decimal integers and then describing the algebraic operation to perform. Such expressions are generated as input strings as shown in the example above. We restrict our variable assignments to the range , ∈ [−1000,1000) and the operations to the set +, −, * . To ensure that the model embraces symbolic variables, the order in which and appears in the expression is randomly chosen. For instance, an input string contrasting from the example shown above might be = 129, = 531, − . Each input string is accompanied by its target string, which is the evaluation of the mathematical expression. For this study, all targets considered are decimal integers represented at the character level. About 12 million unique samples were thus generated and randomly split into training and test sets at an approximate ratio of 9:1, respectively.  The entirety of each input is read and encoded at the character level. The entirety of each output is decoded at the character level. Only after training do the models come to interpret meaning behind the character sequences. One can imagine that a different character mapping be used to obfuscate the meaning assigned by mathematical practice but still be trainable for the models described here to capture the relationships between the individual characters (Table 1). Mapping such results back to the representations familiar in mathematical practice would yield the same results ( Table 2).</p>
<p>The input-target pairs were first used to train a self-attentive and feed-forward transformer without recurrence or convolution in a similar manner as the base model previously reported [10]. The attention mechanism is the scaled dotproduct attention
, , = @A B C(1)
where is the dimension (number of columns) of the input queries , keys , and values . For multi-head attention with ℎ heads that jointly attend to different representation subspaces at different positions given a sequence of length and the matrix ∈ ℝ H×C , the result is
= ℎ O , . . . , ℎ Q S (2) ℎ T = T @ , T A , T U(3)
where the projections are learned parameter matrices T @ , T A , T U ∈ ℝ C×C/Q , S ∈ ℝ C×C .</p>
<p>The same hyperparameters were used as the standard transformer above except for the details that follow. The transformer used in this study is smaller. The encoder consisted of two identical layers, each of which consists of two sub-layers: a multi-head self-attention layer and a fullyconnected feed-forward network layer. Layer normalization was used to preprocess the sub-layer inputs. The decoder consisted of two identical layers, each of which consists of three sub-layers: a multi-head self-attention layer, a multi-head attention layer over the output of the encoder stack, and a fully-connected feed-forward network layer. Each multi-headed attention layer consisted of 4 heads. Each fully-connected feed-forward network consisted of 128 neurons. A dropout rate of 0.1 was used to postprocess the output of each sub-layer before it is added to the sub-layer input by the residual connection.</p>
<p>The transformer model achieved an accuracy on the test set of 76.1%. When we analyze the performance by the type of expression, however, we find that the model infers with perfect accuracy symmetric ( ) expressions such as + , − , and * . Slightly less perfect were + addition tasks, such as + and + , which had 98% accuracy. The next challenging tasks involved − subtraction, such as − and − , which had 49% accuracy. The model struggled most with * multiplication tasks, such as * or * , which had only 9% accuracy. Note that this is a single model trained to performing the different types of tasks. A summary of the results are shown in Table 3.</p>
<p>The results demonstrate that the transformer can learn to interpret and evaluate symbolic variables and expressions as represented by character strings, performing addition, subtraction and multiplication of both positive and negative decimal numbers. The transformer can correctly utilize the values assigned to symbolic variables for inference. Considering the example input string = 568, = −867, * , the model correctly ignores the value assigned to and computes 322624 as the output. The attention visualizations for the encoder's self-attention and decoder's attention on the final layer of the encoder shown in Figs. 1 and 2, respectively, illustrate that the output characters attend almost exclusively on the characters representing the assignment to . Furthermore, the order in which and assignments occur in the string are handled well, since the accuracy is high despite our data including random variations as mentioned above. For the cases inferred incorrectly, the results are very close to the targets. As an example, the value produced for the input sequence = −440, = 687, * is −300280, which is very close to the actual target value of −302280 considering the character match accuracy at each position. Only the thousandth place character is incorrect, which is representative of our general observation that the middle positions are most difficult to correctly infer. Interestingly, the first and last positions of the output attend primarily to the first and last positions representing the assignment to y, whereas the output positions in between do not exhibit such selective attention (Fig. 3 of Appendix A). This confusion could be the reason for the faulty inference of the characters in the middle of the output.  Table 3). In order to improve the performance for evaluating non-symmetric subtraction and multiplication expressions, the transformer can be augmented with recurrent inductive bias as described by prior work on universal transformers [11]. Unlike the standard transformer, the universal transformer can be computationally universal given sufficient memory. At training step , the universal transformer iterates to improve its representations \ ∈ ℝ H×C for the input positions in parallel with a self-attention mechanism, followed by a recurrent transformation using a depth-wise separable convolution or a position-wise fully-connected layer. The universal transformer was thus reported to achieve state-of-the-art results on translation, natural language understanding, and learning-toexecute tasks similar to this study, outperforming both LSTM RNNs and the standard transformer given the same hyperparameters.</p>
<p>Using the same hyperparameters and dataset described above for the standard transformer, the universal transformer achieved better results on all types of asymmetric ( ) expressions as shown in Table 3. The overall accuracy on the tasks is 78.8%. The most improvement occurred for the * multiplication tasks, which more than doubled in accuracy. It therefore appears that the recurrent inductive bias as implemented in the universal transformer successfully addresses some of the shortcomings of the standard transformer model when using the same hyperparameters.</p>
<p>Since only</p>
<p>− and * tasks can be improved upon any further, we next add adaptive computation time [12] to the universal transformer [11] in order to devote more processing resources to symbols not interpreted well by the model. As shown in Table 3, the adaptive universal transformer improves on the − tasks almost to perfection but performs much worse on the * tasks, producing an overall higher accuracy of 83.9%. The adaptive universal transformer may have focused only to improve the − tasks because it is more attainable than the * tasks in improving overall efficiency.</p>
<p>The mathematical language understanding demonstrated in this study is foundational for an artificially intelligent agent. The framework and findings discussed should also be transferable to natural language understanding. The symbolic variable assignment is analogous to supporting facts in the bAbi story, question and answering tasks [13]. Symmetric ( ) tasks only utilize one of the supporting facts, whereas asymmetric ( ) tasks utilizes two supporting facts. The symbolic expressions and their evaluation studied here can thus be considered a simplified version of story, question and answering tasks that can be studied and analyzed more expediently and concretely. We expect that future studies will involve more types of symbolic expressions and variables, further elucidating how to improve the shortcomings of existing models to the benefit of more complex natural language understanding problems.</p>
<p>The transformer model has been shown to work well for a myriad of applications beyond what we typically consider as sequence transduction tasks, e.g. image processing [14]. More generally, transformers can be applied to problems involving tensors as inputs and tensors as outputs, which is the motivation behind the Tensor2Tensor library used in this study [15]. The attention mechanism of the transformer architecture can be interpreted as a global receptive field that can analyze more than the limited receptive fields, which are often referred to as filters, in convolutional neural networks. We therefore expect that the transformer can serve as a unified model to incorporate and improve upon previous work in churn prediction [16], information retrieval [17], and collaborative filtering [18]. The customer's history can be the story or input sequence, and the question can be whether they churn or what item they would choose from recommendations provided. </p>
<p>Table 3 .
3Test performance comparison of inferring mathematical expressions at the character level for different types of expressions for the transformers studied in this work: T -Transformer; UT -Universal Transformer; AUT -Adaptive Universal Transformer.</p>
<p>Figure 1 .
1Encoder's self-attention on the last character for one of the transformer layers. The different attention heads are color coded (not to correspond with</p>
<p>Figure 2 .
2Transformer attention distributions for the decoder's attention on the final layer of the encoder for different decoder layers and attention heads (colorcoded). See Appendix A for more visualizations.</p>
<p>Figure 3 .
3Extra attention visualizations corresponding toFigure 2for a decoder layer's attention on the final layer of the encoder. The different attention heads are color coded. Attentions are displayed for each output character individually from (a) to (e). The first and last characters of the output attend primarily to the first and last characters representing the assignment to y, whereas the characters in between do not exhibit such selective attention.</p>
<p>Table 1 .
1Example input obfuscation table. Table 2. Example output obfuscation table.x = 8 5 , y = − 5 2 3 , x * y </p>
<p>g r f d n p r w d q e n g k p </p>
<p>Arithmetic and algebra in early mathematics education. D W Carraher, A D Schliemann, B M Brizuela, D Earnest, Journal for Research in Mathematics. Carraher, D. W., Schliemann, A. D., Brizuela, B. M., &amp; Earnest, D. (2006). Arithmetic and algebra in early mathematics education. Journal for Research in Mathematics education, 87-115.</p>
<p>Visual Learning of Arithmetic Operation. Y Hoshen, S Peleg, AAAI. Hoshen, Y., &amp; Peleg, S. (2016, February). Visual Learning of Arithmetic Operation. In AAAI (pp. 3733- 3739).</p>
<p>. W Zaremba, K Kurach, R Fergus, Zaremba, W., Kurach, K., &amp; Fergus, R. (2014).</p>
<p>Learning to discover efficient mathematical identities. Advances in Neural Information Processing Systems. Learning to discover efficient mathematical identities. In Advances in Neural Information Processing Systems (pp. 1278-1286).</p>
<p>A neural network model of learning mathematical equivalence. K W Mickey, J L Mcclelland, Proceedings of the Annual Meeting of the Cognitive Science Society. the Annual Meeting of the Cognitive Science Society36Mickey, K. W., &amp; McClelland, J. L. (2014, January). A neural network model of learning mathematical equivalence. In Proceedings of the Annual Meeting of the Cognitive Science Society (Vol. 36, No. 36).</p>
<p>Learning to execute. W Zaremba, I Sutskever, arXiv:1410.4615arXiv preprintZaremba, W., &amp; Sutskever, I. (2014). Learning to execute. arXiv preprint arXiv:1410.4615.</p>
<p>N Kalchbrenner, I Danihelka, A Graves, arXiv:1507.01526Grid long short-term memory. arXiv preprintKalchbrenner, N., Danihelka, I., &amp; Graves, A. (2015). Grid long short-term memory. arXiv preprint arXiv:1507.01526.</p>
<p>Ł Kaiser, I Sutskever, arXiv:1511.08228Neural gpus learn algorithms. arXiv preprintKaiser, Ł., &amp; Sutskever, I. (2015). Neural gpus learn algorithms. arXiv preprint arXiv:1511.08228.</p>
<p>E Price, W Zaremba, I Sutskever, arXiv:1611.00736Extensions and Limitations of the Neural GPU. arXiv preprintPrice, E., Zaremba, W., &amp; Sutskever, I. (2016). Extensions and Limitations of the Neural GPU. arXiv preprint arXiv:1611.00736.</p>
<p>K Freivalds, R Liepins, arXiv:1702.08727Improving the Neural GPU Architecture for Algorithm Learning. arXiv preprintFreivalds, K., &amp; Liepins, R. (2017). Improving the Neural GPU Architecture for Algorithm Learning. arXiv preprint arXiv:1702.08727.</p>
<p>Attention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, . . Polosukhin, I , Advances in Neural Information Processing Systems. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... &amp; Polosukhin, I. (2017). Attention is all you need. In Advances in Neural Information Processing Systems (pp. 5998-6008).</p>
<p>. M Dehghani, S Gouws, O Vinyals, J Uszkoreit, Ł Kaiser, arXiv:1807.03819Universal transformers. arXiv preprintDehghani, M., Gouws, S., Vinyals, O., Uszkoreit, J., &amp; Kaiser, Ł. (2018). Universal transformers. arXiv preprint arXiv:1807.03819.</p>
<p>Adaptive computation time for recurrent neural networks. A Graves, arXiv:1603.08983arXiv preprintGraves, A. (2016). Adaptive computation time for recurrent neural networks. arXiv preprint arXiv:1603.08983.</p>
<p>Towards ai-complete question answering: A set of prerequisite toy tasks. J Weston, A Bordes, S Chopra, A M Rush, B Van Merriënboer, A Joulin, T Mikolov, arXiv:1502.05698arXiv preprintWeston, J., Bordes, A., Chopra, S., Rush, A. M., van Merriënboer, B., Joulin, A., &amp; Mikolov, T. (2015). Towards ai-complete question answering: A set of prerequisite toy tasks. arXiv preprint arXiv:1502.05698.</p>
<p>. Tensor2tensor, Tensor2Tensor (November 18, 2018), Retrieved from https://github.com/tensorflow/tensor2tensor</p>
<p>A Vaswani, S Bengio, E Brevdo, F Chollet, A N Gomez, S Gouws, . . Sepassi, R , arXiv:1803.07416Tensor2tensor for neural machine translation. arXiv preprintVaswani, A., Bengio, S., Brevdo, E., Chollet, F., Gomez, A. N., Gouws, S., ... &amp; Sepassi, R. (2018). Tensor2tensor for neural machine translation. arXiv preprint arXiv:1803.07416.</p>
<p>Artit Wangperawong, Cyrille Brun, Olav Laudy, Rujikorn Pavasuthipaisit, arXiv:1604.05377Churn analysis using deep convolutional neural networks and autoencoders. Wangperawong, Artit, Cyrille Brun, Olav Laudy, and Rujikorn Pavasuthipaisit, Churn analysis using deep convolutional neural networks and autoencoders. arXiv:1604.05377, 2016.</p>
<p>Comparing heterogeneous entities using artificial neural networks of trainable weighted structural components and machine-learned activation functions. A Wangperawong, K Kriangchaivech, A Lanari, S Lam, P Wangperawong, arXiv:1801.03143arXiv preprintWangperawong, A., Kriangchaivech, K., Lanari, A., Lam, S., &amp; Wangperawong, P. (2018). Comparing heterogeneous entities using artificial neural networks of trainable weighted structural components and machine-learned activation functions. arXiv preprint arXiv:1801.03143.</p>
<p>X Liu, A Wangperawong, arXiv:1807.09967A Collaborative Approach to Angel and Venture Capital Investment Recommendations. arXiv preprintLiu, X., &amp; Wangperawong, A. (2018). A Collaborative Approach to Angel and Venture Capital Investment Recommendations. arXiv preprint arXiv:1807.09967.</p>            </div>
        </div>

    </div>
</body>
</html>
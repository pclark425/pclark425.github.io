<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4607 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4607</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4607</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-100.html">extraction-schema-100</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <p><strong>Paper ID:</strong> paper-002c256d30d6be4b23d365a8de8ae0e67e4c9641</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/002c256d30d6be4b23d365a8de8ae0e67e4c9641" target="_blank">Improving language models by retrieving from trillions of tokens</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Machine Learning</p>
                <p><strong>Paper TL;DR:</strong> The Retrieval-Enhanced Transformer (RETRO) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25% fewer parameters, and opens up new avenues for improving language models through explicit memory at unprecedented scale.</p>
                <p><strong>Paper Abstract:</strong> We enhance auto-regressive language models by conditioning on document chunks retrieved from a large corpus, based on local similarity with preceding tokens. With a $2$ trillion token database, our Retrieval-Enhanced Transformer (RETRO) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25$\times$ fewer parameters. After fine-tuning, RETRO performance translates to downstream knowledge-intensive tasks such as question answering. RETRO combines a frozen Bert retriever, a differentiable encoder and a chunked cross-attention mechanism to predict tokens based on an order of magnitude more data than what is typically consumed during training. We typically train RETRO from scratch, yet can also rapidly RETROfit pre-trained transformers with retrieval and still achieve good performance. Our work opens up new avenues for improving language models through explicit memory at unprecedented scale.</p>
                <p><strong>Cost:</strong> 0.024</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4607.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4607.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Retro</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Enhanced Transformer (Retro)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A semi-parametric autoregressive Transformer that conditions next-token prediction on chunk-level nearest-neighbour passages retrieved from a very large (up to trillions of tokens) key-value database, using frozen BERT embeddings, a retrieval encoder, and a chunked cross-attention mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Retro (Retrieval-Enhanced Transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Splits input sequence into fixed-size chunks, computes frozen BERT embeddings for chunks, retrieves k nearest neighbour key-value entries [N,F] (neighbour chunk N and its continuation F) from a precomputed large-scale database (up to ~1.7T tokens at evaluation). Retrieved values are encoded by a bi-directional retrieval encoder (non-causal) conditioned on decoder activations; decoder interleaves standard causal Transformer blocks with chunked cross-attention (Cca) blocks that attend to the encoded retrieved neighbours aligned per chunk. Chunked cross-attention is autoregressive (only the last token of a chunk can attend to that chunk's retrieval) and the architecture preserves autoregressivity for sampling. Retrieval uses approximate nearest neighbours via ScaNN and retrieval keys are frozen BERT time-averaged embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Autoregressive Transformer decoders trained at scales reported up to 7B (main experiments include models from ~132M to 7B parameters; Retro adds a retrieval encoder of ~19M parameters).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Embedding-based retrieval (frozen BERT chunk embeddings) with approximate k-NN on L2 distance; retrieval of contiguous token chunks (chunk-level indexing).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Chunk-aligned cross-attention fusion: retrieved chunks are encoded and fused into the autoregressive decoder via chunked cross-attention; self-attention propagates dependency across multiple retrieved chunks enabling synthesis across retrieved passages.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Designed to operate over corpora with up to trillions of tokens (evaluation DB ~1.75T tokens, database keys up to tens of billions); not specified in 'papers' count but scales to massive text corpora (Millions+ documents).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General web-scale text / language modeling and knowledge-intensive tasks (open-domain QA, Wikipedia/web text, code, books).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Next-token generation (language modelling), generated answers for QA when fine-tuned; sampled text sequences conditioned on retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Bits-per-byte (bpb), perplexity, LAMBADA accuracy (top-1), exact-match (EM) for QA (Natural Questions), filtered bpb by overlap thresholds to measure leakage exploitation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Retro provides consistent gains over baseline Transformers at all scales (132M to 7B). Example: Retro 7.5B outperforms a 7B baseline across many Pile subsets and reaches state-of-the-art on Wikitext103 and the Pile given large retrieval DBs; on Natural Questions fine-tuned Retro (7.5B with DPR retrieval) achieves 45.5% EM (Table 5). On Wikitext103 retrieving from MassiveText (100%) Retro reports very low bpb (valid 3.21, test 3.92) when a large overlapping retrieval DB is available (note: partly due to dataset overlap/leakage).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Retrieval-free Transformers of comparable sizes, large parametric LMs (Jurassic-1, Gopher), and token-level retrieval methods (kNN-LM).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Retro outperforms same-sized baseline Transformers across datasets; provides gains comparable to increasing model size by ~10x on some benchmarks; Retro 7.5B often outperforms Jurassic-1 (178B) and Gopher (280B) on a majority of tested Pile subsets (despite being much smaller) when retrieval DB is large.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Chunk-level retrieval from very large corpora provides large gains without proportionally increasing model parameters; frozen retriever embeddings (BERT) and precomputing neighbours allow scaling to trillions of tokens; retrieval benefits persist across model scales and database sizes and can be added post-hoc by retrofitting pre-trained Transformers (training <10% of weights) to obtain most gains quickly.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Performance can exploit test-train leakage (explicit copying); privacy and safety risks due to direct access to training data; retrieval quality limits usefulness (poor neighbours can hurt), precomputing and storing large retrieval indices is resource intensive (but feasible); chunking introduces alignment constraints; some gains are dataset-overlap dependent.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Gains increase with retrieval database size (Wikipedia -> C4 -> MassiveText) and with number of retrieved neighbours (improvements up to ~40 neighbours for large models). The retrieval advantage remains approximately constant across model parameter scales (from ~150M to 7B) and can match multiplying parametric size by ~10x on certain datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving language models by retrieving from trillions of tokens', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4607.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4607.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>kNN-LM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>k-Nearest Neighbour Language Model (kNN-LM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A token-level retrieval method that augments a language model's token distribution at inference by interpolating probabilities derived from nearest-neighbour stored token embeddings in a large datastore.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Generalization through memorization: Nearest neighbor language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>kNN-LM</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Pre-computes representations per token from a base transformer LM and stores a huge token-level key-value datastore (key: token-context embedding, value: next token). At inference, queries nearest neighbours for the current context embedding, forms a k-NN distribution over next tokens (using distances), and interpolates that distribution with the LM's parametric output to produce final token probabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Transformer language models (token-context embeddings produced by a frozen transformer LM); original works and the paper's reimplementation use transformer LM backbones (sizes vary).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Embedding-based nearest-neighbour retrieval at the token level (per-token keys and next-token values).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Probabilistic interpolation: blend k-NN-derived token probability distribution with parametric LM probabilities (a weighted mixture).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Operates over token-level databases such as English Wikipedia (billions of tokens); not expressed in 'papers' count.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General language modelling (e.g., Wikitext103) and domains where large token-level caches are beneficial.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Adjusted token probability distributions for text generation (LM sampling); final generated sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Perplexity / bits-per-byte on language modelling benchmarks (Wikitext103, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>When retrieving from Wikipedia (4B tokens) kNN-LM improved perplexity compared to baseline transformer (paper reports e.g., kNN-LM (ours) on Wikipedia valid 18.52, test 19.54 vs baseline transformer valid 21.53, test 22.96 in the authors' reimplementation/context).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Parametric Transformer LM without retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>kNN-LM reduces perplexity compared to a retrieval-free baseline on Wikitext103 when using a large token-level datastore such as Wikipedia.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Token-level k-NN interpolation can significantly improve LM performance by leveraging explicit memorized contexts; however it requires storing high-dimensional vectors per token, which is extremely memory intensive at scale (many TBs for billions of tokens), limiting scalability.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Token-level storage scales poorly (e.g., 15 TB for 4B tokens at 1024-d embeddings), making token-level kNN impractical for retrieval corpora at the trillions-of-tokens scale; limited ability to reason about retrieved text since retrieval influences only probability interpolation and not model internals.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Works well on Wikipedia-scale datastores (billions of tokens) but does not scale to trillions due to per-token storage costs; benefits depend on datastore size and overlap with evaluation data.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving language models by retrieving from trillions of tokens', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4607.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4607.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Continuous Cache</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Continuous Cache (Neural cache)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that augments neural LMs with a short-term continuous cache assigning probability mass to tokens whose previous activations resemble the current context activation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Improving neural language models with a continuous cache</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Continuous Cache</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Maintains a cache of recent context activations and their next-token occurrences; at prediction time, finds contexts with activations similar to current activation and boosts probabilities of next tokens observed in those cached contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Originally demonstrated with LSTM and transformer activations as keys; used as an add-on to existing neural LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Local activation similarity (nearest neighbours in activation space) over recent contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Probability augmentation: add probability mass to tokens based on retrieved cached contexts (no deep integration into model internals).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Designed for short-term local caches (thousands of contexts); not intended for processing many external papers.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General language modelling (local context adaptation).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Adjusted token probabilities; generated text reflecting recent contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Perplexity / LM performance on held-out data in original studies.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Improves LM perplexity by leveraging local history; specific numbers are in original citation (Grave et al., 2017).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Standard neural LMs without cache.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Shows improvements on tasks with strong local repetition patterns; limited compared to retrieval from large external corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Simple cache-based probability augmentation can capture local repetition and reduce perplexity without changing model weights.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Cache is local and small (not suitable for massive external corpora), cannot reason over retrieved content deeply because it only modifies output probabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Targeted at short-term/local context scaling, not designed for very large-scale corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving language models by retrieving from trillions of tokens', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4607.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4607.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Spalm</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Spalm (a token-level retrieval method referenced as Spalm)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A token-level retrieval approach referenced as a prior method similar in spirit to kNN-LM but applying gating of logits; reported on Wikipedia-scale retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Spalm</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Described as a token-level retrieval approach that post-processes retrieved information (gating logits) rather than deeply integrating retrieved content into the model; referenced as prior work demonstrating retrieval benefits on Wikipedia-scale corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Transformer-based language models (token-level retrieval), specific model details not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Token-level nearest-neighbour retrieval with frozen transformer representations.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Gated logits post-processing (combines retrieved signals with LM logits via a gating mechanism).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Operates on datasets up to billions of tokens (Wikipedia-scale); not indicated as processing multiple scientific papers specifically.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General language modelling (Wikipedia-scale retrieval).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Adjusted token probability outputs; generated sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Perplexity / bits-per-byte (in referenced evaluations).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Reported to achieve substantial improvements on Wikitext103 in referenced work (Yogatama et al., 2021); exact numbers in original paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Retrieval-free transformer LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Improves over baseline on Wikipedia retrieval tasks; details in original reference.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Gating/logit-level integration can exploit token-level retrieved neighbours while changing little of the original model internals.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Token-level storage and processing can be expensive; limited deep reasoning about retrieved content.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Demonstrated at Wikipedia-scale (billions of tokens) but token-level methods face storage scaling challenges beyond that.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving language models by retrieving from trillions of tokens', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4607.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4607.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DPR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dense Passage Retrieval (DPR)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A contrastive-learned dense retrieval system using BERT-style encoders to embed questions and passages into a shared vector space for open-domain QA retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Dense passage retrieval for open-domain question answering</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>DPR</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Trains two BERT encoders (question/query encoder and passage/key encoder) with a contrastive loss to align questions with answer-containing passages; retrieved passages are then used by downstream models (e.g., generation or extractive QA).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>BERT-based encoders for retriever; generative models can be combined downstream (DPR itself is a retriever).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Dense embedding retrieval trained with contrastive supervision (question–positive passage alignment).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>External generative/reader models (e.g., RAG, FiD, Retro fine-tuned) fuse retrieved passages to produce final answers; DPR supplies top-k passages.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Designed for corpora like Wikipedia (millions of passages), routinely applied on tens of millions of passages.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Open-domain question answering (Wikipedia and similar knowledge sources).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Retrieved passages (textual passages) used to condition answer generation; final output is generated answers.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Exact Match (EM) and F1 on QA benchmarks (Natural Questions, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>DPR reported strong retrieval performance enabling improved QA when paired with a reader/generator (Karpukhin et al., 2020); in this Retro paper DPR passages were used to fine-tune Retro on Natural Questions and achieve 45.5% EM for Retro 7.5B with DPR retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>BM25 / lexical retrieval baselines and non-dense retrievers.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>DPR outperforms traditional lexical retrieval for QA retrieval in referenced studies; improves downstream QA when used with generative readers.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Contrastively trained dense retrievers like DPR produce high-quality passage retrievals for QA and are commonly used as retrieval backends for generative systems.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Training requires labeled question–passage pairs; retriever needs updates or re-indexing if encoder changes; scalability manageable but requires vector index infrastructure.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Works at Wikipedia-scale and larger with approximate nearest neighbour indices; used as retrieval source for downstream generation models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving language models by retrieving from trillions of tokens', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4607.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4607.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReAlm</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>REALM (Retrieval-Augmented Language Model Pre-Training)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An end-to-end approach that trains retrieval components jointly with a masked language model objective to incorporate retrieved documents into pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Retrieval augmented language model pre-training</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>REALM (ReAlm)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Trains a retriever and reader end-to-end: uses an inverse-cloze / masked-language-style objective and backpropagates through retrieval steps by periodically searching the database and updating retrieval targets; retrieved documents are prepended or aggregated to improve language modelling and QA pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>BERT-style encoders/readers trained end-to-end with retrieval in the pre-training loop.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Dense retrieval with retriever trained end-to-end on the pretraining objective (not frozen).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Prepending retrieved documents or incorporating them into the input to the LM, allowing learned parameters to make use of retrieved material for prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Designed for corpora like Wikipedia (billions of tokens) but training requires repeated database searches and periodic re-encoding of the DB, which limits scalability.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Pretraining for language understanding and knowledge-intensive tasks; open-domain QA.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Enhanced contextual representations, improved masked-LM predictions, downstream QA answers.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>QA metrics, masked-LM perplexity, downstream task performance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>REALM demonstrates improvements on knowledge-intensive tasks when retriever is trained jointly; authors of Retro note REALM's scalability limitations due to the need to search and update embeddings during training (Guu et al., 2020).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Frozen retriever methods or retrieval-free pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>End-to-end trained retrievers can yield better task performance but come at higher cost and reduced scalability.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>End-to-end retriever training aligns retrieval with the pretraining objective but is resource intensive at large scale.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Must periodically re-embed the full DB during training which severely limits scale; expensive database search during training.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Less scalable than methods that use frozen retrievers; demonstrated mainly at smaller retrieval DB scales (billions of tokens).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving language models by retrieving from trillions of tokens', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4607.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4607.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RAG (Retrieval-Augmented Generation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that retrieves relevant documents using a retriever (often DPR) and conditions a generative model on those retrieved documents via cross-attention to produce answers for knowledge-intensive tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Retrieval-augmented generation for knowledge-intensive NLP tasks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>RAG</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Uses a retriever (e.g., DPR) to fetch top-k passages for a query/prompt and then conditions a generative sequence model (e.g., encoder-decoder) using cross-attention over retrieved passages; can marginalize over retrieved passages or concatenate them to the prompt for generation.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Typically encoder-decoder generative models (e.g., BART/T5) combined with DPR retriever; the paper cites RAG as a QA architecture.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Dense retriever (DPR-style) to extract top-k passages for each query.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Cross-attention-based fusion in a generative model (marginalization or concatenation of retrieved passages to produce final generation).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Designed to operate on large passage collections like Wikipedia (millions of passages); not expressed in 'papers' count.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Knowledge-intensive NLP tasks and open-domain QA.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Generated answers conditioned on retrieved passages.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>QA metrics such as Exact Match (EM) and F1.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>RAG reported strong QA performance in its original work; Retro paper lists RAG baseline QA results (44.5% test accuracy quoted in Table 5 for Natural Questions).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>DPR + extractive or generative readers, retrieval-free models.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>RAG outperforms simpler retrieval or closed-book baselines in many QA settings; exact comparisons depend on model sizes and retrieval quality.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Combining dense retrieval with generative models via cross-attention yields strong QA performance; exact gains depend on retrieval and reader architecture.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Depends on retriever quality; retrieval corpus coverage and overlap with evaluation data affect performance; training can require careful engineering.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Scales with retriever and index infrastructure; effective at Wikipedia-scale datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving language models by retrieving from trillions of tokens', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4607.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e4607.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FiD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>FiD (Fusion-in-Decoder)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval-to-generation architecture that encodes each retrieved passage independently and fuses information across passages in the decoder for open-domain QA.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Leveraging passage retrieval with generative models for open domain question answering</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>FiD (Fusion-in-Decoder)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Encodes each retrieved passage separately via an encoder (often sharing weights), concatenates encoder outputs into an expanded context, and uses a decoder (typically T5-style) to fuse information across passages to generate final answers; shown to be strong on open-domain QA benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>T5-family encoder-decoder models in referenced work; FiD leverages pre-trained seq2seq models as reader/decoder.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Dense retrieval (e.g., DPR) to fetch top-k passages for each query.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Fusion-in-decoder: the decoder attends jointly over all encoded retrieved passages to synthesize an answer.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Evaluated on QA corpora using top-k retrieved passages (commonly k=10-100); not summarized as a 'papers' count.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Open-domain question answering.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Generated answers (text) conditioned on multiple retrieved passages.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Exact Match (EM), F1 on QA benchmarks (Natural Questions, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>FiD reported state-of-the-art QA performance in referenced work (FiD 51.4% EM in the table; FiD + Distillation 54.7% EM in Table 5). Retro notes FiD outperforms Retro on Natural Questions in their experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>RAG, DPR+readers, retrieval-free models.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>FiD outperforms several retrieval and non-retrieval baselines on QA tasks; in Retro's QA comparison FiD > Retro (51.4% vs 45.5% EM) for Natural Questions with T5-based readers.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Encoder-decoder fusion at the decoder stage enables effective synthesis across multiple passages which benefits QA; pre-training and the seq2seq objective (T5) contribute to FiD's strong performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Computationally expensive with many retrieved passages; performance depends on retriever coverage; Retro hypothesizes FiD relies more on encoder outputs which helps in QA.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Synthesis quality improves with more retrieved passages up to practical/compute limits; benefits from strong pretrained seq2seq backbones.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving language models by retrieving from trillions of tokens', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4607.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e4607.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EMDR^2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>EMDR^2 (Expectation-Maximization for Dense Retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach extending FiD by using an expectation-maximization algorithm to train the retriever end-to-end with the reader, improving retrieval and QA performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>EMDR^2</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Uses an EM-style training loop to jointly improve retriever and reader by iteratively estimating latent alignments between queries and passages and updating retriever parameters to maximize the downstream objective; builds on FiD-style encoder-decoder readers.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>FiD-style encoder-decoder generative models (e.g., T5 variants) as readers; retriever typically dense embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Dense retrieval trained via EM-style end-to-end objective to better align retrieval with reader performance.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Cross-attention fusion in encoder-decoder reader using retrieved passages; joint optimization improves synthesis quality.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Applied on QA datasets with top-k retrieved passages (practical k ranges used in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Open-domain question answering.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Generated answers produced by a fusion reader.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Exact Match (EM) on QA benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Referenced as achieving state-of-the-art results compared to similarly sized models (EMDR^2 reported 52.5% EM in Table 5), outperforming many prior QA systems.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>FiD, RAG, DPR and other retrieval+generation QA systems.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Reported to surpass FiD and other baselines at similar model sizes in cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Joint/EM-style training of retriever and reader can improve end-to-end QA performance over separately trained components.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Requires more complex training loops and compute; may be less straightforward to scale than frozen retriever methods.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Demonstrated on standard QA datasets; joint training complexity can limit scaling to very large corpora without engineering adaptations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving language models by retrieving from trillions of tokens', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4607.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e4607.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BlenderBot 2.0</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BlenderBot 2.0 (Internet-augmented dialogue generation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dialogue model that learns to issue internet search queries and use retrieved web results to ground its conversational responses, outperforming dense retrieval baselines on human-likeness metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Internet-augmented dialogue generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>BlenderBot 2.0</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Learns, from human dialogue/search-query data, to produce search queries and to incorporate retrieved web snippets into generated dialogue responses; relies on recorded human query–response pairs for training rather than just dense retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Large conversational Transformer/dialogue models trained with internet-augmented objectives (bot architecture details in referenced work).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Learned textual queries and web search retrieval (external search engine outputs or dense retrieval can be used as sources).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Conditions generation on retrieved web passages; combines retrieved evidence with dialogue history to produce responses.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Operates over web search results and dialogue datasets; not described as processing multiple scientific papers explicitly.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Open-domain dialogue grounded with web retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Generated conversational responses grounded in retrieved web content.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Human evaluation measuring similarity to human responses / dialog quality; task-specific automatic metrics where applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Reported to outperform dense retrieval methods in human-model closeness evaluations in referenced work (Komeili et al., 2021).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Dense retrieval methods and prior dialogue models.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Improved grounding quality and human-likeness relative to dense retrieval baselines in referenced experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Learning to issue and use web search queries from human data can surpass off-the-shelf dense retrieval in dialogue grounding tasks, but requires curated human data of queries and responses.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Collecting human dialogues annotated with search queries limits scalability; reliance on external search introduces latency and safety concerns.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Effective when trained on human dialogues with queries, but scaling depends on available annotated data and search infrastructure.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving language models by retrieving from trillions of tokens', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4607.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e4607.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Guided Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Guided Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Transformer variant that leverages multiple external sources to inform representation learning for tasks like document retrieval and clarifying question selection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Leveraging multiple external sources for representation learning in conversational search</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Guided Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Modifies the Transformer architecture to incorporate guidance from external sources (retrieval signals) to improve representation learning for retrieval and downstream conversational tasks; referenced usage for document retrieval and clarifying question selection.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Transformer-based encoders with guidance mechanisms; specifics in original reference (Hashemi et al., 2020).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Incorporates external retrieval sources and signals into representation learning for improved retrieval decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Integrates multiple external signals into Transformer representations to guide downstream selection tasks (not explicitly a multi-paper theory synthesis system).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Targeted at retrieval/selection scenarios over document corpora; not specified as processing many scientific papers.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Conversational search, document retrieval, clarifying question selection.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Retrieved documents or clarifying questions/selection outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Retrieval and selection metrics in conversational search tasks (task-dependent).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Shown effective on question answering and tasks with strong conditioning in referenced work; details in original paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Standard Transformer-based retrieval or selection methods.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Improves retrieval and clarifying-question selection relative to baselines in the cited evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Guiding Transformers with external sources can improve selection and retrieval in conversational settings, but not primarily designed for general multi-document theory synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Task-specific; not primarily targeted at large-scale corpora for general text synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Designed for retrieval/selection tasks; scaling depends on number and types of external sources incorporated.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving language models by retrieving from trillions of tokens', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Retrieval augmented language model pre-training <em>(Rating: 2)</em></li>
                <li>Generalization through memorization: Nearest neighbor language models <em>(Rating: 2)</em></li>
                <li>Dense passage retrieval for open-domain question answering <em>(Rating: 2)</em></li>
                <li>Retrieval-augmented generation for knowledge-intensive NLP tasks <em>(Rating: 2)</em></li>
                <li>Leveraging passage retrieval with generative models for open domain question answering <em>(Rating: 2)</em></li>
                <li>Improving neural language models with a continuous cache <em>(Rating: 2)</em></li>
                <li>Internet-augmented dialogue generation <em>(Rating: 1)</em></li>
                <li>Spalm (Yogatama et al., 2021) <em>(Rating: 1)</em></li>
                <li>EMDR^2 (Sachan et al., 2021) <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4607",
    "paper_id": "paper-002c256d30d6be4b23d365a8de8ae0e67e4c9641",
    "extraction_schema_id": "extraction-schema-100",
    "extracted_data": [
        {
            "name_short": "Retro",
            "name_full": "Retrieval-Enhanced Transformer (Retro)",
            "brief_description": "A semi-parametric autoregressive Transformer that conditions next-token prediction on chunk-level nearest-neighbour passages retrieved from a very large (up to trillions of tokens) key-value database, using frozen BERT embeddings, a retrieval encoder, and a chunked cross-attention mechanism.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Retro (Retrieval-Enhanced Transformer)",
            "system_description": "Splits input sequence into fixed-size chunks, computes frozen BERT embeddings for chunks, retrieves k nearest neighbour key-value entries [N,F] (neighbour chunk N and its continuation F) from a precomputed large-scale database (up to ~1.7T tokens at evaluation). Retrieved values are encoded by a bi-directional retrieval encoder (non-causal) conditioned on decoder activations; decoder interleaves standard causal Transformer blocks with chunked cross-attention (Cca) blocks that attend to the encoded retrieved neighbours aligned per chunk. Chunked cross-attention is autoregressive (only the last token of a chunk can attend to that chunk's retrieval) and the architecture preserves autoregressivity for sampling. Retrieval uses approximate nearest neighbours via ScaNN and retrieval keys are frozen BERT time-averaged embeddings.",
            "llm_model_used": "Autoregressive Transformer decoders trained at scales reported up to 7B (main experiments include models from ~132M to 7B parameters; Retro adds a retrieval encoder of ~19M parameters).",
            "extraction_technique": "Embedding-based retrieval (frozen BERT chunk embeddings) with approximate k-NN on L2 distance; retrieval of contiguous token chunks (chunk-level indexing).",
            "synthesis_technique": "Chunk-aligned cross-attention fusion: retrieved chunks are encoded and fused into the autoregressive decoder via chunked cross-attention; self-attention propagates dependency across multiple retrieved chunks enabling synthesis across retrieved passages.",
            "number_of_papers": "Designed to operate over corpora with up to trillions of tokens (evaluation DB ~1.75T tokens, database keys up to tens of billions); not specified in 'papers' count but scales to massive text corpora (Millions+ documents).",
            "domain_or_topic": "General web-scale text / language modeling and knowledge-intensive tasks (open-domain QA, Wikipedia/web text, code, books).",
            "output_type": "Next-token generation (language modelling), generated answers for QA when fine-tuned; sampled text sequences conditioned on retrieval.",
            "evaluation_metrics": "Bits-per-byte (bpb), perplexity, LAMBADA accuracy (top-1), exact-match (EM) for QA (Natural Questions), filtered bpb by overlap thresholds to measure leakage exploitation.",
            "performance_results": "Retro provides consistent gains over baseline Transformers at all scales (132M to 7B). Example: Retro 7.5B outperforms a 7B baseline across many Pile subsets and reaches state-of-the-art on Wikitext103 and the Pile given large retrieval DBs; on Natural Questions fine-tuned Retro (7.5B with DPR retrieval) achieves 45.5% EM (Table 5). On Wikitext103 retrieving from MassiveText (100%) Retro reports very low bpb (valid 3.21, test 3.92) when a large overlapping retrieval DB is available (note: partly due to dataset overlap/leakage).",
            "comparison_baseline": "Retrieval-free Transformers of comparable sizes, large parametric LMs (Jurassic-1, Gopher), and token-level retrieval methods (kNN-LM).",
            "performance_vs_baseline": "Retro outperforms same-sized baseline Transformers across datasets; provides gains comparable to increasing model size by ~10x on some benchmarks; Retro 7.5B often outperforms Jurassic-1 (178B) and Gopher (280B) on a majority of tested Pile subsets (despite being much smaller) when retrieval DB is large.",
            "key_findings": "Chunk-level retrieval from very large corpora provides large gains without proportionally increasing model parameters; frozen retriever embeddings (BERT) and precomputing neighbours allow scaling to trillions of tokens; retrieval benefits persist across model scales and database sizes and can be added post-hoc by retrofitting pre-trained Transformers (training &lt;10% of weights) to obtain most gains quickly.",
            "limitations_challenges": "Performance can exploit test-train leakage (explicit copying); privacy and safety risks due to direct access to training data; retrieval quality limits usefulness (poor neighbours can hurt), precomputing and storing large retrieval indices is resource intensive (but feasible); chunking introduces alignment constraints; some gains are dataset-overlap dependent.",
            "scaling_behavior": "Gains increase with retrieval database size (Wikipedia -&gt; C4 -&gt; MassiveText) and with number of retrieved neighbours (improvements up to ~40 neighbours for large models). The retrieval advantage remains approximately constant across model parameter scales (from ~150M to 7B) and can match multiplying parametric size by ~10x on certain datasets.",
            "uuid": "e4607.0",
            "source_info": {
                "paper_title": "Improving language models by retrieving from trillions of tokens",
                "publication_date_yy_mm": "2021-12"
            }
        },
        {
            "name_short": "kNN-LM",
            "name_full": "k-Nearest Neighbour Language Model (kNN-LM)",
            "brief_description": "A token-level retrieval method that augments a language model's token distribution at inference by interpolating probabilities derived from nearest-neighbour stored token embeddings in a large datastore.",
            "citation_title": "Generalization through memorization: Nearest neighbor language models",
            "mention_or_use": "use",
            "system_name": "kNN-LM",
            "system_description": "Pre-computes representations per token from a base transformer LM and stores a huge token-level key-value datastore (key: token-context embedding, value: next token). At inference, queries nearest neighbours for the current context embedding, forms a k-NN distribution over next tokens (using distances), and interpolates that distribution with the LM's parametric output to produce final token probabilities.",
            "llm_model_used": "Transformer language models (token-context embeddings produced by a frozen transformer LM); original works and the paper's reimplementation use transformer LM backbones (sizes vary).",
            "extraction_technique": "Embedding-based nearest-neighbour retrieval at the token level (per-token keys and next-token values).",
            "synthesis_technique": "Probabilistic interpolation: blend k-NN-derived token probability distribution with parametric LM probabilities (a weighted mixture).",
            "number_of_papers": "Operates over token-level databases such as English Wikipedia (billions of tokens); not expressed in 'papers' count.",
            "domain_or_topic": "General language modelling (e.g., Wikitext103) and domains where large token-level caches are beneficial.",
            "output_type": "Adjusted token probability distributions for text generation (LM sampling); final generated sequences.",
            "evaluation_metrics": "Perplexity / bits-per-byte on language modelling benchmarks (Wikitext103, etc.).",
            "performance_results": "When retrieving from Wikipedia (4B tokens) kNN-LM improved perplexity compared to baseline transformer (paper reports e.g., kNN-LM (ours) on Wikipedia valid 18.52, test 19.54 vs baseline transformer valid 21.53, test 22.96 in the authors' reimplementation/context).",
            "comparison_baseline": "Parametric Transformer LM without retrieval.",
            "performance_vs_baseline": "kNN-LM reduces perplexity compared to a retrieval-free baseline on Wikitext103 when using a large token-level datastore such as Wikipedia.",
            "key_findings": "Token-level k-NN interpolation can significantly improve LM performance by leveraging explicit memorized contexts; however it requires storing high-dimensional vectors per token, which is extremely memory intensive at scale (many TBs for billions of tokens), limiting scalability.",
            "limitations_challenges": "Token-level storage scales poorly (e.g., 15 TB for 4B tokens at 1024-d embeddings), making token-level kNN impractical for retrieval corpora at the trillions-of-tokens scale; limited ability to reason about retrieved text since retrieval influences only probability interpolation and not model internals.",
            "scaling_behavior": "Works well on Wikipedia-scale datastores (billions of tokens) but does not scale to trillions due to per-token storage costs; benefits depend on datastore size and overlap with evaluation data.",
            "uuid": "e4607.1",
            "source_info": {
                "paper_title": "Improving language models by retrieving from trillions of tokens",
                "publication_date_yy_mm": "2021-12"
            }
        },
        {
            "name_short": "Continuous Cache",
            "name_full": "Continuous Cache (Neural cache)",
            "brief_description": "An approach that augments neural LMs with a short-term continuous cache assigning probability mass to tokens whose previous activations resemble the current context activation.",
            "citation_title": "Improving neural language models with a continuous cache",
            "mention_or_use": "mention",
            "system_name": "Continuous Cache",
            "system_description": "Maintains a cache of recent context activations and their next-token occurrences; at prediction time, finds contexts with activations similar to current activation and boosts probabilities of next tokens observed in those cached contexts.",
            "llm_model_used": "Originally demonstrated with LSTM and transformer activations as keys; used as an add-on to existing neural LMs.",
            "extraction_technique": "Local activation similarity (nearest neighbours in activation space) over recent contexts.",
            "synthesis_technique": "Probability augmentation: add probability mass to tokens based on retrieved cached contexts (no deep integration into model internals).",
            "number_of_papers": "Designed for short-term local caches (thousands of contexts); not intended for processing many external papers.",
            "domain_or_topic": "General language modelling (local context adaptation).",
            "output_type": "Adjusted token probabilities; generated text reflecting recent contexts.",
            "evaluation_metrics": "Perplexity / LM performance on held-out data in original studies.",
            "performance_results": "Improves LM perplexity by leveraging local history; specific numbers are in original citation (Grave et al., 2017).",
            "comparison_baseline": "Standard neural LMs without cache.",
            "performance_vs_baseline": "Shows improvements on tasks with strong local repetition patterns; limited compared to retrieval from large external corpora.",
            "key_findings": "Simple cache-based probability augmentation can capture local repetition and reduce perplexity without changing model weights.",
            "limitations_challenges": "Cache is local and small (not suitable for massive external corpora), cannot reason over retrieved content deeply because it only modifies output probabilities.",
            "scaling_behavior": "Targeted at short-term/local context scaling, not designed for very large-scale corpora.",
            "uuid": "e4607.2",
            "source_info": {
                "paper_title": "Improving language models by retrieving from trillions of tokens",
                "publication_date_yy_mm": "2021-12"
            }
        },
        {
            "name_short": "Spalm",
            "name_full": "Spalm (a token-level retrieval method referenced as Spalm)",
            "brief_description": "A token-level retrieval approach referenced as a prior method similar in spirit to kNN-LM but applying gating of logits; reported on Wikipedia-scale retrieval.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Spalm",
            "system_description": "Described as a token-level retrieval approach that post-processes retrieved information (gating logits) rather than deeply integrating retrieved content into the model; referenced as prior work demonstrating retrieval benefits on Wikipedia-scale corpora.",
            "llm_model_used": "Transformer-based language models (token-level retrieval), specific model details not provided in this paper.",
            "extraction_technique": "Token-level nearest-neighbour retrieval with frozen transformer representations.",
            "synthesis_technique": "Gated logits post-processing (combines retrieved signals with LM logits via a gating mechanism).",
            "number_of_papers": "Operates on datasets up to billions of tokens (Wikipedia-scale); not indicated as processing multiple scientific papers specifically.",
            "domain_or_topic": "General language modelling (Wikipedia-scale retrieval).",
            "output_type": "Adjusted token probability outputs; generated sequences.",
            "evaluation_metrics": "Perplexity / bits-per-byte (in referenced evaluations).",
            "performance_results": "Reported to achieve substantial improvements on Wikitext103 in referenced work (Yogatama et al., 2021); exact numbers in original paper.",
            "comparison_baseline": "Retrieval-free transformer LMs.",
            "performance_vs_baseline": "Improves over baseline on Wikipedia retrieval tasks; details in original reference.",
            "key_findings": "Gating/logit-level integration can exploit token-level retrieved neighbours while changing little of the original model internals.",
            "limitations_challenges": "Token-level storage and processing can be expensive; limited deep reasoning about retrieved content.",
            "scaling_behavior": "Demonstrated at Wikipedia-scale (billions of tokens) but token-level methods face storage scaling challenges beyond that.",
            "uuid": "e4607.3",
            "source_info": {
                "paper_title": "Improving language models by retrieving from trillions of tokens",
                "publication_date_yy_mm": "2021-12"
            }
        },
        {
            "name_short": "DPR",
            "name_full": "Dense Passage Retrieval (DPR)",
            "brief_description": "A contrastive-learned dense retrieval system using BERT-style encoders to embed questions and passages into a shared vector space for open-domain QA retrieval.",
            "citation_title": "Dense passage retrieval for open-domain question answering",
            "mention_or_use": "mention",
            "system_name": "DPR",
            "system_description": "Trains two BERT encoders (question/query encoder and passage/key encoder) with a contrastive loss to align questions with answer-containing passages; retrieved passages are then used by downstream models (e.g., generation or extractive QA).",
            "llm_model_used": "BERT-based encoders for retriever; generative models can be combined downstream (DPR itself is a retriever).",
            "extraction_technique": "Dense embedding retrieval trained with contrastive supervision (question–positive passage alignment).",
            "synthesis_technique": "External generative/reader models (e.g., RAG, FiD, Retro fine-tuned) fuse retrieved passages to produce final answers; DPR supplies top-k passages.",
            "number_of_papers": "Designed for corpora like Wikipedia (millions of passages), routinely applied on tens of millions of passages.",
            "domain_or_topic": "Open-domain question answering (Wikipedia and similar knowledge sources).",
            "output_type": "Retrieved passages (textual passages) used to condition answer generation; final output is generated answers.",
            "evaluation_metrics": "Exact Match (EM) and F1 on QA benchmarks (Natural Questions, etc.).",
            "performance_results": "DPR reported strong retrieval performance enabling improved QA when paired with a reader/generator (Karpukhin et al., 2020); in this Retro paper DPR passages were used to fine-tune Retro on Natural Questions and achieve 45.5% EM for Retro 7.5B with DPR retrieval.",
            "comparison_baseline": "BM25 / lexical retrieval baselines and non-dense retrievers.",
            "performance_vs_baseline": "DPR outperforms traditional lexical retrieval for QA retrieval in referenced studies; improves downstream QA when used with generative readers.",
            "key_findings": "Contrastively trained dense retrievers like DPR produce high-quality passage retrievals for QA and are commonly used as retrieval backends for generative systems.",
            "limitations_challenges": "Training requires labeled question–passage pairs; retriever needs updates or re-indexing if encoder changes; scalability manageable but requires vector index infrastructure.",
            "scaling_behavior": "Works at Wikipedia-scale and larger with approximate nearest neighbour indices; used as retrieval source for downstream generation models.",
            "uuid": "e4607.4",
            "source_info": {
                "paper_title": "Improving language models by retrieving from trillions of tokens",
                "publication_date_yy_mm": "2021-12"
            }
        },
        {
            "name_short": "ReAlm",
            "name_full": "REALM (Retrieval-Augmented Language Model Pre-Training)",
            "brief_description": "An end-to-end approach that trains retrieval components jointly with a masked language model objective to incorporate retrieved documents into pretraining.",
            "citation_title": "Retrieval augmented language model pre-training",
            "mention_or_use": "mention",
            "system_name": "REALM (ReAlm)",
            "system_description": "Trains a retriever and reader end-to-end: uses an inverse-cloze / masked-language-style objective and backpropagates through retrieval steps by periodically searching the database and updating retrieval targets; retrieved documents are prepended or aggregated to improve language modelling and QA pretraining.",
            "llm_model_used": "BERT-style encoders/readers trained end-to-end with retrieval in the pre-training loop.",
            "extraction_technique": "Dense retrieval with retriever trained end-to-end on the pretraining objective (not frozen).",
            "synthesis_technique": "Prepending retrieved documents or incorporating them into the input to the LM, allowing learned parameters to make use of retrieved material for prediction.",
            "number_of_papers": "Designed for corpora like Wikipedia (billions of tokens) but training requires repeated database searches and periodic re-encoding of the DB, which limits scalability.",
            "domain_or_topic": "Pretraining for language understanding and knowledge-intensive tasks; open-domain QA.",
            "output_type": "Enhanced contextual representations, improved masked-LM predictions, downstream QA answers.",
            "evaluation_metrics": "QA metrics, masked-LM perplexity, downstream task performance.",
            "performance_results": "REALM demonstrates improvements on knowledge-intensive tasks when retriever is trained jointly; authors of Retro note REALM's scalability limitations due to the need to search and update embeddings during training (Guu et al., 2020).",
            "comparison_baseline": "Frozen retriever methods or retrieval-free pretraining.",
            "performance_vs_baseline": "End-to-end trained retrievers can yield better task performance but come at higher cost and reduced scalability.",
            "key_findings": "End-to-end retriever training aligns retrieval with the pretraining objective but is resource intensive at large scale.",
            "limitations_challenges": "Must periodically re-embed the full DB during training which severely limits scale; expensive database search during training.",
            "scaling_behavior": "Less scalable than methods that use frozen retrievers; demonstrated mainly at smaller retrieval DB scales (billions of tokens).",
            "uuid": "e4607.5",
            "source_info": {
                "paper_title": "Improving language models by retrieving from trillions of tokens",
                "publication_date_yy_mm": "2021-12"
            }
        },
        {
            "name_short": "RAG",
            "name_full": "RAG (Retrieval-Augmented Generation)",
            "brief_description": "A method that retrieves relevant documents using a retriever (often DPR) and conditions a generative model on those retrieved documents via cross-attention to produce answers for knowledge-intensive tasks.",
            "citation_title": "Retrieval-augmented generation for knowledge-intensive NLP tasks",
            "mention_or_use": "mention",
            "system_name": "RAG",
            "system_description": "Uses a retriever (e.g., DPR) to fetch top-k passages for a query/prompt and then conditions a generative sequence model (e.g., encoder-decoder) using cross-attention over retrieved passages; can marginalize over retrieved passages or concatenate them to the prompt for generation.",
            "llm_model_used": "Typically encoder-decoder generative models (e.g., BART/T5) combined with DPR retriever; the paper cites RAG as a QA architecture.",
            "extraction_technique": "Dense retriever (DPR-style) to extract top-k passages for each query.",
            "synthesis_technique": "Cross-attention-based fusion in a generative model (marginalization or concatenation of retrieved passages to produce final generation).",
            "number_of_papers": "Designed to operate on large passage collections like Wikipedia (millions of passages); not expressed in 'papers' count.",
            "domain_or_topic": "Knowledge-intensive NLP tasks and open-domain QA.",
            "output_type": "Generated answers conditioned on retrieved passages.",
            "evaluation_metrics": "QA metrics such as Exact Match (EM) and F1.",
            "performance_results": "RAG reported strong QA performance in its original work; Retro paper lists RAG baseline QA results (44.5% test accuracy quoted in Table 5 for Natural Questions).",
            "comparison_baseline": "DPR + extractive or generative readers, retrieval-free models.",
            "performance_vs_baseline": "RAG outperforms simpler retrieval or closed-book baselines in many QA settings; exact comparisons depend on model sizes and retrieval quality.",
            "key_findings": "Combining dense retrieval with generative models via cross-attention yields strong QA performance; exact gains depend on retrieval and reader architecture.",
            "limitations_challenges": "Depends on retriever quality; retrieval corpus coverage and overlap with evaluation data affect performance; training can require careful engineering.",
            "scaling_behavior": "Scales with retriever and index infrastructure; effective at Wikipedia-scale datasets.",
            "uuid": "e4607.6",
            "source_info": {
                "paper_title": "Improving language models by retrieving from trillions of tokens",
                "publication_date_yy_mm": "2021-12"
            }
        },
        {
            "name_short": "FiD",
            "name_full": "FiD (Fusion-in-Decoder)",
            "brief_description": "A retrieval-to-generation architecture that encodes each retrieved passage independently and fuses information across passages in the decoder for open-domain QA.",
            "citation_title": "Leveraging passage retrieval with generative models for open domain question answering",
            "mention_or_use": "mention",
            "system_name": "FiD (Fusion-in-Decoder)",
            "system_description": "Encodes each retrieved passage separately via an encoder (often sharing weights), concatenates encoder outputs into an expanded context, and uses a decoder (typically T5-style) to fuse information across passages to generate final answers; shown to be strong on open-domain QA benchmarks.",
            "llm_model_used": "T5-family encoder-decoder models in referenced work; FiD leverages pre-trained seq2seq models as reader/decoder.",
            "extraction_technique": "Dense retrieval (e.g., DPR) to fetch top-k passages for each query.",
            "synthesis_technique": "Fusion-in-decoder: the decoder attends jointly over all encoded retrieved passages to synthesize an answer.",
            "number_of_papers": "Evaluated on QA corpora using top-k retrieved passages (commonly k=10-100); not summarized as a 'papers' count.",
            "domain_or_topic": "Open-domain question answering.",
            "output_type": "Generated answers (text) conditioned on multiple retrieved passages.",
            "evaluation_metrics": "Exact Match (EM), F1 on QA benchmarks (Natural Questions, etc.).",
            "performance_results": "FiD reported state-of-the-art QA performance in referenced work (FiD 51.4% EM in the table; FiD + Distillation 54.7% EM in Table 5). Retro notes FiD outperforms Retro on Natural Questions in their experiments.",
            "comparison_baseline": "RAG, DPR+readers, retrieval-free models.",
            "performance_vs_baseline": "FiD outperforms several retrieval and non-retrieval baselines on QA tasks; in Retro's QA comparison FiD &gt; Retro (51.4% vs 45.5% EM) for Natural Questions with T5-based readers.",
            "key_findings": "Encoder-decoder fusion at the decoder stage enables effective synthesis across multiple passages which benefits QA; pre-training and the seq2seq objective (T5) contribute to FiD's strong performance.",
            "limitations_challenges": "Computationally expensive with many retrieved passages; performance depends on retriever coverage; Retro hypothesizes FiD relies more on encoder outputs which helps in QA.",
            "scaling_behavior": "Synthesis quality improves with more retrieved passages up to practical/compute limits; benefits from strong pretrained seq2seq backbones.",
            "uuid": "e4607.7",
            "source_info": {
                "paper_title": "Improving language models by retrieving from trillions of tokens",
                "publication_date_yy_mm": "2021-12"
            }
        },
        {
            "name_short": "EMDR^2",
            "name_full": "EMDR^2 (Expectation-Maximization for Dense Retrieval)",
            "brief_description": "An approach extending FiD by using an expectation-maximization algorithm to train the retriever end-to-end with the reader, improving retrieval and QA performance.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "EMDR^2",
            "system_description": "Uses an EM-style training loop to jointly improve retriever and reader by iteratively estimating latent alignments between queries and passages and updating retriever parameters to maximize the downstream objective; builds on FiD-style encoder-decoder readers.",
            "llm_model_used": "FiD-style encoder-decoder generative models (e.g., T5 variants) as readers; retriever typically dense embeddings.",
            "extraction_technique": "Dense retrieval trained via EM-style end-to-end objective to better align retrieval with reader performance.",
            "synthesis_technique": "Cross-attention fusion in encoder-decoder reader using retrieved passages; joint optimization improves synthesis quality.",
            "number_of_papers": "Applied on QA datasets with top-k retrieved passages (practical k ranges used in experiments).",
            "domain_or_topic": "Open-domain question answering.",
            "output_type": "Generated answers produced by a fusion reader.",
            "evaluation_metrics": "Exact Match (EM) on QA benchmarks.",
            "performance_results": "Referenced as achieving state-of-the-art results compared to similarly sized models (EMDR^2 reported 52.5% EM in Table 5), outperforming many prior QA systems.",
            "comparison_baseline": "FiD, RAG, DPR and other retrieval+generation QA systems.",
            "performance_vs_baseline": "Reported to surpass FiD and other baselines at similar model sizes in cited work.",
            "key_findings": "Joint/EM-style training of retriever and reader can improve end-to-end QA performance over separately trained components.",
            "limitations_challenges": "Requires more complex training loops and compute; may be less straightforward to scale than frozen retriever methods.",
            "scaling_behavior": "Demonstrated on standard QA datasets; joint training complexity can limit scaling to very large corpora without engineering adaptations.",
            "uuid": "e4607.8",
            "source_info": {
                "paper_title": "Improving language models by retrieving from trillions of tokens",
                "publication_date_yy_mm": "2021-12"
            }
        },
        {
            "name_short": "BlenderBot 2.0",
            "name_full": "BlenderBot 2.0 (Internet-augmented dialogue generation)",
            "brief_description": "A dialogue model that learns to issue internet search queries and use retrieved web results to ground its conversational responses, outperforming dense retrieval baselines on human-likeness metrics.",
            "citation_title": "Internet-augmented dialogue generation",
            "mention_or_use": "mention",
            "system_name": "BlenderBot 2.0",
            "system_description": "Learns, from human dialogue/search-query data, to produce search queries and to incorporate retrieved web snippets into generated dialogue responses; relies on recorded human query–response pairs for training rather than just dense retrieval.",
            "llm_model_used": "Large conversational Transformer/dialogue models trained with internet-augmented objectives (bot architecture details in referenced work).",
            "extraction_technique": "Learned textual queries and web search retrieval (external search engine outputs or dense retrieval can be used as sources).",
            "synthesis_technique": "Conditions generation on retrieved web passages; combines retrieved evidence with dialogue history to produce responses.",
            "number_of_papers": "Operates over web search results and dialogue datasets; not described as processing multiple scientific papers explicitly.",
            "domain_or_topic": "Open-domain dialogue grounded with web retrieval.",
            "output_type": "Generated conversational responses grounded in retrieved web content.",
            "evaluation_metrics": "Human evaluation measuring similarity to human responses / dialog quality; task-specific automatic metrics where applicable.",
            "performance_results": "Reported to outperform dense retrieval methods in human-model closeness evaluations in referenced work (Komeili et al., 2021).",
            "comparison_baseline": "Dense retrieval methods and prior dialogue models.",
            "performance_vs_baseline": "Improved grounding quality and human-likeness relative to dense retrieval baselines in referenced experiments.",
            "key_findings": "Learning to issue and use web search queries from human data can surpass off-the-shelf dense retrieval in dialogue grounding tasks, but requires curated human data of queries and responses.",
            "limitations_challenges": "Collecting human dialogues annotated with search queries limits scalability; reliance on external search introduces latency and safety concerns.",
            "scaling_behavior": "Effective when trained on human dialogues with queries, but scaling depends on available annotated data and search infrastructure.",
            "uuid": "e4607.9",
            "source_info": {
                "paper_title": "Improving language models by retrieving from trillions of tokens",
                "publication_date_yy_mm": "2021-12"
            }
        },
        {
            "name_short": "Guided Transformer",
            "name_full": "Guided Transformer",
            "brief_description": "A Transformer variant that leverages multiple external sources to inform representation learning for tasks like document retrieval and clarifying question selection.",
            "citation_title": "Leveraging multiple external sources for representation learning in conversational search",
            "mention_or_use": "mention",
            "system_name": "Guided Transformer",
            "system_description": "Modifies the Transformer architecture to incorporate guidance from external sources (retrieval signals) to improve representation learning for retrieval and downstream conversational tasks; referenced usage for document retrieval and clarifying question selection.",
            "llm_model_used": "Transformer-based encoders with guidance mechanisms; specifics in original reference (Hashemi et al., 2020).",
            "extraction_technique": "Incorporates external retrieval sources and signals into representation learning for improved retrieval decisions.",
            "synthesis_technique": "Integrates multiple external signals into Transformer representations to guide downstream selection tasks (not explicitly a multi-paper theory synthesis system).",
            "number_of_papers": "Targeted at retrieval/selection scenarios over document corpora; not specified as processing many scientific papers.",
            "domain_or_topic": "Conversational search, document retrieval, clarifying question selection.",
            "output_type": "Retrieved documents or clarifying questions/selection outputs.",
            "evaluation_metrics": "Retrieval and selection metrics in conversational search tasks (task-dependent).",
            "performance_results": "Shown effective on question answering and tasks with strong conditioning in referenced work; details in original paper.",
            "comparison_baseline": "Standard Transformer-based retrieval or selection methods.",
            "performance_vs_baseline": "Improves retrieval and clarifying-question selection relative to baselines in the cited evaluations.",
            "key_findings": "Guiding Transformers with external sources can improve selection and retrieval in conversational settings, but not primarily designed for general multi-document theory synthesis.",
            "limitations_challenges": "Task-specific; not primarily targeted at large-scale corpora for general text synthesis.",
            "scaling_behavior": "Designed for retrieval/selection tasks; scaling depends on number and types of external sources incorporated.",
            "uuid": "e4607.10",
            "source_info": {
                "paper_title": "Improving language models by retrieving from trillions of tokens",
                "publication_date_yy_mm": "2021-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Retrieval augmented language model pre-training",
            "rating": 2
        },
        {
            "paper_title": "Generalization through memorization: Nearest neighbor language models",
            "rating": 2
        },
        {
            "paper_title": "Dense passage retrieval for open-domain question answering",
            "rating": 2
        },
        {
            "paper_title": "Retrieval-augmented generation for knowledge-intensive NLP tasks",
            "rating": 2
        },
        {
            "paper_title": "Leveraging passage retrieval with generative models for open domain question answering",
            "rating": 2
        },
        {
            "paper_title": "Improving neural language models with a continuous cache",
            "rating": 2
        },
        {
            "paper_title": "Internet-augmented dialogue generation",
            "rating": 1
        },
        {
            "paper_title": "Spalm (Yogatama et al., 2021)",
            "rating": 1
        },
        {
            "paper_title": "EMDR^2 (Sachan et al., 2021)",
            "rating": 1
        }
    ],
    "cost": 0.024188,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Improving language models by retrieving from trillions of tokens</h1>
<p>Sebastian Borgeaud ${ }^{\dagger}$, Arthur Mensch ${ }^{\ddagger}$, Jordan Hoffmann ${ }^{\dagger}$, Trevor Cai, Eliza Rutherford, Katie Millican, George van den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego de Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero, Karen Simonyan, Jack W. Rae ${ }^{\ddagger}$, Erich Elsen ${ }^{\ddagger}$ and Laurent Sifre ${ }^{\dagger, \ddagger}$<br>All authors from DeepMind, ${ }^{\dagger}$ Equal contributions, ${ }^{\ddagger}$ Equal senior authorship</p>
<h4>Abstract</h4>
<p>We enhance auto-regressive language models by conditioning on document chunks retrieved from a large corpus, based on local similarity with preceding tokens. With a 2 trillion token database, our Retrieval-Enhanced Transformer (Retro) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using $25 \times$ fewer parameters. After fine-tuning, Retro performance translates to downstream knowledge-intensive tasks such as question answering. Retro combines a frozen Bert retriever, a differentiable encoder and a chunked cross-attention mechanism to predict tokens based on an order of magnitude more data than what is typically consumed during training. We typically train Retro from scratch, yet can also rapidly Retrofit pre-trained transformers with retrieval and still achieve good performance. Our work opens up new avenues for improving language models through explicit memory at unprecedented scale.</p>
<h2>1. Introduction</h2>
<p>Language modelling (LM) is an unsupervised task that consists of modelling the probability of text, usually by factorising it into conditional next-token predictions $p\left(x_{1}, \ldots, x_{n}\right)=\prod_{i} p\left(x_{i} \mid x_{&lt;i}\right)$. Neural networks have proven to be powerful language models, first in the form of recurrent architectures (Graves, 2013; Jozefowicz et al., 2016; Mikolov et al., 2010) and more recently in the form of Transformers (Vaswani et al., 2017), that use attention to contextualise the past. Large performance improvements have come from increasing the amount of data, training compute, or model parameters. Transformers have been scaled from 100 million parameter models in seminal work to over hundred billion parameters (Brown et al., 2020; Radford et al., 2019) in the last two years which has led to models that do very well on a wide array of tasks in a zero or few-shot formulation. Increasing model size predictably improves performance on a wide range of downstream tasks (Kaplan et al., 2020). The benefits of increasing the number of parameters come from two factors: additional computations at training and inference time, and increased memorization of the training data.</p>
<p>In this work, we endeavor to decouple these, by exploring efficient means of augmenting language models with a massive-scale memory without significantly increasing computations. Specifically, we suggest retrieval from a large text database as a complementary path to scaling language models. Instead of increasing the size of the model and training on more data, we equip models with the ability to directly access a large database to perform predictions-a semi-parametric approach. At a high level, our Retrieval Transformer (Retro) model splits the input sequence into chunks and retrieves text similar to the previous chunk to improve the predictions in the current chunk. Existing retrieval for language modelling work only considers small transformers ( 100 millions parameters) and databases of limited size (up to billions of tokens) (Guu et al., 2020; Khandelwal et al., 2020; Lewis et al., 2020; Yogatama et al., 2021). To our knowledge, our work is the first to show the benefits of scaling the retrieval database to trillions of tokens for large parametric language models. Our main</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1 | Scaling of Retro. The performance gain of our retrieval models remains constant with model scale (left), and is comparable to multiplying the parameteric model size by $\sim 10 \times$. The gain increases with the size of the retrieval database (middle) and the number of retrieved neighbours (right) on the C4 validation set, when using up to 40 neighbours. Past this, performance begins to degrade, perhaps due to the reduced quality. At evaluation Retro can be used without retrieval data (Retro[OFF]), bringing limited performance degradation compared to baseline transformers.
contributions are the following.</p>
<ul>
<li>We introduce Retro, a retrieval-enhanced autoregressive language model (§2.2). We use a chunked cross-attention module to incorporate the retrieved text (§2.4), with time complexity linear in the amount of retrieved data. We show that retrieving based on a pre-trained frozen Bert model (§2.3) works at scale, removing the need for training and updating a retriever network.</li>
<li>We show that our method scales well with model size and database size (Fig. 1): Retro provides a constant gain for models ranging from 150 M to 7 B parameters, and Retro can be improved at evaluation time by increasing the database size and the number of retrieved neighbours. Our largest model obtains state-of-the-art results on a range of downstream evaluation datasets including Wikitext103 (Merity et al., 2017) and the Pile (Gao et al., 2020) (§4). We show that Retro can be fine-tuned to achieve competitive performance on downstream tasks such as question answering (§4.3).</li>
<li>We propose an evaluation aware of proximity of test documents with the training set (§2.6), addressing the problem of test set leakage (Lee et al., 2021). This is relevant for all language models, and especially for retrieval-enhanced models since they have direct access to the training dataset during evaluation. Using this methodology, we show that the performance of Retro comes from both explicit neighbour copying and general knowledge extraction (§4.4).</li>
</ul>
<h1>2. Method</h1>
<p>We design our retrieval-enhanced architecture to be capable of retrieving from a database with trillions of tokens. For this purpose, we retrieve at the level of contiguous token chunks instead of individual tokens which reduces storage and computation requirements by a large linear factor. Our method first constructs a key-value database, where values store raw chunks of text tokens and keys are frozen Bert embedddings (Devlin et al., 2019). We use a frozen model to avoid having to periodically re-compute embeddings over the entire database during training. Each training sequence is then split into chunks, which are augmented with their $k$-nearest neighbour retrieved from the database. An encoder-decoder architecture integrates retrieval chunks into the model's predictions. We summarize the Retro architecture in Fig. 2, and detail it in this section. We end the section by introducing</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2 | Retro architecture. Left: simplified version where a sequence of length $n=12$ is split into $l=3$ chunks of size $m=4$. For each chunk, we retrieve $k=2$ neighbours of $r=5$ tokens each. The retrieval pathway is shown on top. Right: Details of the interactions in the CcA operator. Causality is maintained as neighbours of the first chunk only affect the last token of the first chunk and tokens from the second chunk.
a new methodology to evaluate language models when an evaluation set is partially present in the training set.</p>
<h1>2.1. Training dataset</h1>
<p>We use a multi-lingual version of MassiveText (Rae et al., 2021) for both training and retrieval data. The dataset consists of text documents from multiple sources and multiple languages totalling over 5 trillion tokens (detailed in Table 1). Sequences are sampled from subsets of the training data, with sampling weights given in the right-most column of Table 1. We tokenize the dataset using SentencePiece (Kudo and Richardson, 2018) with a vocabulary of 128,000 tokens. During training (unless otherwise specified), we retrieve from 600B tokens from the training data. The training retrieval database is made of the same subsets as the training data, in proportion that matches the training sampling frequencies. During evaluation the retrieval database consists in the full union of these datasets, with the exception of books for which we use a sub-sample of $4 \%$. The evaluation retrieval database thus contains 1.75 T tokens. To limit test set leakage, we compute the 13-gram Jaccard similarity between train and test documents using the MinHash scheme and remove all training documents with high similarity ( 0.8 or higher) to a validation or test set document. Additionally, we remove all validation and test articles from Wikitext103 (Merity et al., 2017) from our Wikipedia training data.</p>
<h3>2.2. Retrieval-enhanced autoregressive token models</h3>
<p>Our approach uses retrieval as a way to augment input examples at the granularity of small chunks of tokens. Formally, we consider sequences of integer tokens in $\mathbb{V}=[1, v]$, obtained using a text tokenizer ${ }^{1}$. We split each $n$-token-long example $X=\left(x_{1}, \ldots, x_{n}\right)$ into a sequence of $l$ chunks $\left(C_{1}, \ldots, C_{l}\right)$ of size $m=\frac{n}{l}$, i.e. $C_{1} \triangleq\left(x_{1}, \ldots, x_{m}\right), \ldots, C_{l} \triangleq\left(x_{n-m+1}, \ldots, x_{n}\right) \in \mathbb{V}^{m}$. We use $n=2048$ and $m=64$. We augment each chunk $C_{u}$ with a set $\operatorname{RET}<em u="u">{\mathcal{D}}\left(C</em>$ (or}\right)$ of $k$ neighbours from the database $\mathcal{D}$. $\operatorname{RET}_{\mathcal{D}</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>RET for brevity) is a non-trainable operator specified in $\S 2.3$. Token likelihoods are provided by a model, parameterized by $\theta$, that takes as input both previous tokens and their retrieved neighbours. This defines the following retrieval-enhanced sequence log-likelihood:</p>
<p>$$
L(X \mid \theta, \mathcal{D}) \triangleq \sum_{u=1}^{l} \sum_{i=1}^{m} \ell_{\theta}\left(x_{(u-1) m+i} \mid\left(x_{j}\right)<em _mathcal_D="\mathcal{D">{j&lt;(u-1) m+i},\left(\operatorname{RET}</em>\right)
$$}}\left(C_{u^{\prime}}\right)\right)_{u^{\prime}&lt;u</p>
<p>We set $\operatorname{RET}\left(C_{1}\right)=\emptyset$, namely the likelihood of tokens from the first chunk does not depend on any retrieval data. This likelihood definition preserves autoregressivity: the probability of the $i$-th token of the $u$-th chunk, $x_{(u-1) m+i}$, only depends on previously seen tokens $\left(x_{j}\right)<em u_prime="u^{\prime">{1 \leqslant j&lt;(u-1) m+i}$ and on the data retrieved from the previous chunks $\left(\operatorname{RET}\left(C</em>\right)\right)}<em u="u">{u^{\prime}&lt;u}$. We can therefore directly sample with logprobability $\ell$, where sampling within the chunk $C</em>$. This makes retrieval-enhanced models directly comparable with the largest language models that are evaluated by sampling.}$ is conditioned on the neighbours $\left(\operatorname{RET}\left(C_{u^{\prime}}\right)\right)_{u^{\prime}&lt;u</p>
<h1>2.3. Nearest neighbour retrieval</h1>
<p>Retrieval neighbours. Our database consists of a key-value memory. Each value consists of two contiguous chunks of tokens which we denote $[N, F]$ where $N$ is the neighbour chunk which is used to compute the key, and $F$ is its continuation in the original document. The corresponding key is the Bert embedding of $N$, averaged over time, that we denote $\operatorname{BERT}(N)$. For each chunk $C$, we retrieve its approximate $k$-nearest neighbours from our key-value database using the $L_{2}$ distance on BERT embeddings $d(C, N)=\left|\operatorname{BERT}(C)-\operatorname{BERT}(N)\right|<em u_1="u+1">{2}^{2}$. The model receives the corresponding values $\operatorname{RET}(C) \triangleq\left(\left[N^{1}, F^{1}\right], \ldots,\left[N^{k}, F^{k}\right]\right)$. Both neighbour chunks and their continuations provide meaningful improvements, as illustrated in our ablation study (Appendix D). We use a length 64 for both $N^{j}$ and $F^{j}$, thus $\operatorname{RET}(C)$ has a shape of $k \times r$ with $r=128$. To avoid retrieving the chunk $C</em>\right)$, which would break causality during training, we filter out neighbours originating from the same document as the training sequence $X$.}$ in the retrieval set $\operatorname{RET}\left(C_{u</p>
<p>For a database of $T$ elements, we can query the approximate nearest neighbours in $O(\log T)$ time. We use the SCaNN library (Guo et al., 2020) to achieve this. This means that we can query our 2 trillion token database in 10 ms whilst evaluating or sampling from the model; this expense is amortized over a chunk length. Performing retrieval on-the-fly is too slow to keep up with the training calculations-we leverage the frozen aspect of the embedding operator BERT to precompute all approximate nearest neighbours and save the results as part of the data. In Fig. 9 in the Appendix, we show results where we only retrieve neighbours within Wikipedia. We find that neighbours tend to come from 2-3 links away from a given article whereas random articles are more than 5 links apart.</p>
<p>Table 1 | MassiveText. The last column indicates the sampling weight during training. The multilingual subsets include documents in 10 languages. The full breakdown is given in §A.1.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Source</th>
<th style="text-align: center;">Token count (M)</th>
<th style="text-align: center;">Documents (M)</th>
<th style="text-align: center;">Multilingual</th>
<th style="text-align: center;">Sampling frequency</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Web</td>
<td style="text-align: center;">977,563</td>
<td style="text-align: center;">1,208</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">$55 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Books</td>
<td style="text-align: center;">$3,423,740$</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">$25 \%$</td>
</tr>
<tr>
<td style="text-align: center;">News</td>
<td style="text-align: center;">236,918</td>
<td style="text-align: center;">398</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">$10 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Wikipedia</td>
<td style="text-align: center;">13,288</td>
<td style="text-align: center;">23</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">$5 \%$</td>
</tr>
<tr>
<td style="text-align: center;">GitHub</td>
<td style="text-align: center;">374,952</td>
<td style="text-align: center;">143</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">$5 \%$</td>
</tr>
</tbody>
</table>
<h1>2.4. Retro model architecture</h1>
<p>Our model relies on an encoder-decoder transformer architecture, integrating the retrieved data through a cross-attention mechanism as introduced in Vaswani et al. (2017). First, the retrieved tokens $\operatorname{Ret}(C)$ are fed into an encoder Transformer, which computes the encoded neighbours set $E$. Denoting the intermediate activations by $H$, our transformer decoder then interleaves Retro-blocks $\operatorname{Retro}(H, E)$ and standard Transformer blocks $\operatorname{LM}(H)$ (the hyperparameter $P \subseteq[1, L]$ determines at which layers we use a Retro-block). These blocks are built from three different residual operators with signature $\mathbb{R}^{n \times d} \rightarrow \mathbb{R}^{n \times d}$ : a fully-connected layer FFw, the standard sequence-level self-attention layer Attn, and a chunked cross-attention layer $\operatorname{Cca}(\cdot, E)$ that incorporates information from the retrieval encoder:</p>
<p>$$
\operatorname{Retro}(H, E) \triangleq \operatorname{FFw}(\operatorname{Cca}(\operatorname{Attn}(H), E)), \quad \text { and } \quad \operatorname{Lm}(H) \triangleq \operatorname{FFw}(\operatorname{Attn}(H))
$$</p>
<p>Since Ffw, Attn and Cca are all autoregressive operators whose output at position $i$ only depends on $\left(h_{j}\right)_{j \in i}$, any succession of Retro and lm layers, followed by a token classification head defines an autoregressive log-likelihood (1). An overview of the model architecture is given in Algorithm 1 and in Fig. 2. We next describe the retrieval encoder and the chunked cross-attention layer in more detail, and explain how to sample from Retro.</p>
<p>Encoding retrieval neighbours. For each chunk $C_{u}$, the $k$ retrieval neighbours $\operatorname{Ret}\left(C_{u}\right)$ are fed into a bi-directional transformer Encoder, yielding the outputs $E_{u}^{j} \triangleq \operatorname{Encoder}\left(\operatorname{Ret}\left(C_{u}\right)^{j}, H_{u}\right) \in \mathbb{R}^{r \times d^{r}}$, where $j \in[1, k]$ indexes each neighbour. The retrieval encoder is a non-causal transformer. It is conditioned on $H_{u}$, the activations of chunk $C_{u}$, through cross-attention layers; this allows the representations of the retrieval encoder to be modulated by the retrieving chunk in a differentiable way. More precisely, the encoding of the $j^{\text {th }}$ neighbour of the $u^{\text {th }}$ chunk, $\operatorname{Ret}\left(C_{u}\right)^{j}$, depends on the attended activation $H_{u} \triangleq\left(h_{(u-1) m+i}\right)<em u="u">{i \in[1, m]} \in \mathbb{R}^{m \times d}$ of chunk $C</em>\right)}$ at layer $\min (P)$. All neighbours for all chunks are encoded in parallel, yielding a full encoded set $E \triangleq\left(E_{u}^{j<em u="u">{u \in[1, l], j \in[1, k]} \in \mathbb{R}^{l \times k \times r \times d^{r}}$. We denote $E</em>$ as the encoded neighbours for chunk $u \in[1, l]$.} \in \mathbb{R}^{k \times r \times d^{r}</p>
<p>Chunked cross-attention. To perform the Cca operation, we first split a given intermediate activation $H \in \mathbb{R}^{n \times d}$ into $l-1$ attending chunks $\left(H_{u}^{<em>} \triangleq\left(h_{u m+i-1}\right)<em _in_1_="\in[1," l-1_="l-1]" u="u">{i \in[1, m]} \in \mathbb{R}^{m \times d}\right)</em>^{}$, as depicted on the right of Fig. 2. $H_{u</em>}$ holds the intermediary embeddings of the last token in chunk $C_{u}$ and of the first $m-1$ tokens in $C_{u+1}{ }^{2}$. We compute the cross-attention between $H_{u}^{*}$ and $E_{u}$-the encoded retrieval set obtained from chunk $C_{u}$. Attention is computed across time and across neighbours simultaneously, as we merge the neighbour and time dimensions of $E_{u}$ before applying cross-attention. Since there is a notion of alignment between data chunks and retrieval neighbours, we use relative positional encodings as described in §B.1.2.</p>
<p>We concatenate the $l-1$ outputs of the per-chunk cross-attentions (each of shape $m \times d$ ) across time, and properly pad the result; we thus form the output activation $\operatorname{Cca}(H, E) \in \mathbb{R}^{n \times d}$. Formally, for each chunk $C_{u}$ and for each token $i \in[1, m]$ we set</p>
<p>$$
\operatorname{Cca}(H, E)<em m_i-1="m+i-1" u="u">{u m+i-1} \triangleq \operatorname{CA}\left(h</em>\right)
$$}, E_{u</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Algorithm 1: Overview of Retro model architecture.
Hyperparam: $P$ and $P_{\text {enc }}$, indices of layers with cross-attention in the decoder and encoder respectively
Hyperparam: $L$ and $L_{\text {enc }}$, number of decoder layers and number of encoder layers.
Input: $X \in \mathbb{V}^{n}$ : sequence of tokens. $\left(\operatorname{RET}\left(C_{u}\right)\right)<em u="u">{1 \leqslant u \leqslant l}$ : the retrieved neighbours
Output: $O \in \mathbb{R}^{n \times|\mathrm{V}|}$ : the output logits
$\operatorname{def} \operatorname{ENCODER}\left(\operatorname{RET}\left(C</em>\right)<em n="n">{1 \leqslant u \leqslant l}, H\right):$
$\left(H</em>\right)<em u="u">{u \in[1, l]} \leftarrow \operatorname{SPLit}(H)$
for $j \in[1, k], u \in[1, l]$ do // Encoder shared across neighbours and chunks
$E</em>}^{j}=\operatorname{EMB<em u="u">{\text {enc }}\left(\operatorname{RET}\left(C</em>\right)$ // May be shared with the decoder EMB
for $p^{\prime} \in\left[1, L_{\text {enc }}\right]$ do
$E_{u}^{j} \leftarrow \operatorname{AtTN}}\right)^{j<em u="u">{\text {enc }}\left(E</em>\right)$ // Bi-directional attention
if $p^{\prime} \in P_{\text {enc }}$ then
$E_{u}^{j} \leftarrow \mathrm{CA}}^{j<em u="u">{\text {enc }}\left(E</em>\right)$
$E_{u}^{j} \leftarrow \mathrm{FFW}}^{j}, H_{u<em u="u">{\text {enc }}\left(E</em>\right)$
return $E$
$H \leftarrow \operatorname{EMB}(X)$
for $p \in[1, L]$ do
$H \leftarrow \operatorname{AtTN}(H)$ // Causal attention
if $p=\min (P)$ then
// The neighbour Encoder is conditioned with the decoder activations of the last layer before the first cross-attention
$E=\operatorname{ENCODER}\left(\operatorname{RET}\left(C_{u}\right)_{1 \leqslant u \leqslant l}, H\right)$
if $p \in P$ then
$H \leftarrow \operatorname{Cca}(H, E)$
$H \leftarrow \operatorname{FFW}(H)$
$O \leftarrow \operatorname{READ}(H)$
where CA is the cross-attention residual operator over time-concatenated encoded neighbours. We recall that this operator is defined in its simplest version by three parameter matrices $K \in \mathbb{R}^{d \times c}, Q \in$ $\mathbb{R}^{d \times c}$ and $V \in \mathbb{R}^{d \times d}$. For all $h \in \mathbb{R}^{d}$ and $Y \in \mathbb{R}^{T \times d}$, we define}^{j</p>
<p>$$
\mathrm{CA}(h, Y) \triangleq \operatorname{softmax}\left(Y K Q^{T} h\right) Y V
$$</p>
<p>where the softmax is performed on the second dimension and all products are matrix products. We use multi-head cross-attention, and add positional encodings to the softmax(see §B.1.2).</p>
<p>The first $m-1$ tokens cannot attend to any neighbour of a previous chunk; at these positions, we define Cca as the identity, setting $\operatorname{Cca}(H, E)<em j="j">{j} \triangleq h</em>\right)$ (not shown in Fig. 2). Listing 1 contains a simplified implementation of Cca. Note that chunked cross-attention is autoregressive: the output of Cca at position $i$ depends on the sequence from tokens from 0 to $i$ that is input to Cca.}$ for all tokens $j \in[1, m-1]$. Finally, the last token $h_{l m}$ attends to the last retrieval set $E_{l}$ and we set $h_{l m} \triangleq \mathrm{CA}\left(h_{l m}, E_{l</p>
<p>With Retro models, even though each Cca cross-attention attends only to the neighbours of the preceding chunk $\operatorname{RET}\left(C_{u-1}\right)$, the dependencies over previous neighbours are propagated via the self-attention operations. The activations of the $i^{\text {th }}$ token in the $u^{\text {th }}$ chunk therefore potentially depend upon the set of all previous neighbours $\operatorname{RET}\left(C_{u^{\prime}}\right)_{u^{\prime} \leqslant u}$, without incurring the quadratic cost of cross attending to that set.</p>
<p>Sampling. When sampling, at the end of a chunk $C_{u}$, we use SCaNN to retrieve neighbours $\operatorname{RET}\left(C_{u}\right)$, based on the embedding $\operatorname{BERT}\left(C_{u}\right)$. The encoded neighbours $E_{u}=\operatorname{Encoder}\left(\operatorname{RET}\left(C_{u}\right)\right)$ are then used to condition the generation of the next chunk $C_{u+1}$, which we do incrementally: overall the cost of sampling is thus quadratic in the size of the sampled sequence, as when sampling from regular Transformers; the added cost of retrieval is linear in the number of chunks $l$, and is negligible compared to the token sampling cost in practice.</p>
<h1>2.5. Baseline Transformer architecture</h1>
<p>We use a transformer (Vaswani et al., 2017) similar to the one described in (Radford et al., 2019), with some minimal changes: we replace LayerNorm with RMSNorm (Zhang and Sennrich, 2019) and use relative position encodings (Dai et al., 2019). As baselines, we train retrieval-free transformers with $132 \mathrm{M}, 368 \mathrm{M}, 1.3 \mathrm{~B}$ and 7.0 B parameters (embedding matrices are excluded from parameter counts). The hyperparameters we used are detailed in Table 2. All retrieval models use the same size encoder for the retrieval data, with $d^{\prime}=896$ and 2 layers, which roughly adds 19 M parameters. The encoder uses relative positional encodings. The retrieval models contain one Retro-block every 3 blocks, starting from layer 6 . For our smallest model, CcA is applied in layers 6,9 and 12 of the main pathway and also once for query conditioning in the encoder, which adds an additional 12 M parameters. The relative number of extra parameters reduces as we increase the baseline model size. All models are implemented using JAX (Bradbury et al., 2018) and Haiku (Hennigan et al., 2020).</p>
<h3>2.6. Quantifying dataset leakage exploitation</h3>
<p>Retro models may arguably benefit more easily from evaluation dataset leakage, i.e. the fact that we evaluate on data that were also present in the training set. To better understand how retrieval improves language modelling performance, we therefore quantify evaluation likelihood as a function of the overlap between the evaluation and training datasets.</p>
<p>The following approach can be used with any language model, and depends only on the frozen retriever system presented in $\S 2.3$. We split the evaluation sequences $\left(X_{i}\right)_{j}$ into chunks of length $m \leq 64$, and we see the training data as a set of chunks $\mathcal{C}$. For each evaluation chunk $C \in \mathcal{C}$, we retrieve the 10 closest neighbours (of length up to 128) in the training data. We then compute the longest token substring common to both the evaluation chunk and its neighbours. This gives a number $s \in[0, m]$. The value $r(C)=\frac{s}{m}$, ranging from 0 (chunk never seen) to 1 (chunk entirely seen), gives a reliable indication of how much overlap there is between the evaluation chunk and the training data. For a given model, we then obtain the log-likelihood $\ell(C)$ of each chunk $C$, and the number of bytes $N(C)$ it encodes. We then consider the filtered bits-per-bytes of the model:</p>
<p>$$
\forall \alpha \in[0,1], \quad C_{\alpha} \triangleq{C \in \mathcal{C}, r(C) \leqslant \alpha}, \quad \operatorname{bpb}(\alpha) \triangleq \frac{\sum_{C \in C_{\alpha}} \ell(C)}{\sum_{C \in C_{\alpha}} N(C)}
$$</p>
<p>Table 2 | Number of parameters for our baseline and Retro models, excluding embeddings, along with the corresponding hyperparameters.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Baseline parameters</th>
<th style="text-align: center;">Retro</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">$d$</th>
<th style="text-align: center;">$d_{\text {ffw }}$</th>
<th style="text-align: center;"># heads</th>
<th style="text-align: center;">Head size</th>
<th style="text-align: center;"># layers</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">132 M</td>
<td style="text-align: center;">$172 \mathrm{M}(+30 \%)$</td>
<td style="text-align: center;">896</td>
<td style="text-align: center;">3,584</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">368 M</td>
<td style="text-align: center;">$425 \mathrm{M}(+15 \%)$</td>
<td style="text-align: center;">1,536</td>
<td style="text-align: center;">6,144</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">128</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$1,309 \mathrm{M}$</td>
<td style="text-align: center;">$1,451 \mathrm{M}(+11 \%)$</td>
<td style="text-align: center;">2,048</td>
<td style="text-align: center;">8,192</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">128</td>
<td style="text-align: center;">24</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$6,982 \mathrm{M}$</td>
<td style="text-align: center;">$7,532 \mathrm{M}(+8 \%)$</td>
<td style="text-align: center;">4,096</td>
<td style="text-align: center;">16,384</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">128</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>which correspond to the bits-per-bytes on the set of chunks that overlap less than $\alpha \%$ with the training chunks. Note that the full evaluation bit-per-bytes performance is recovered by bpb(1). The function $\mathrm{bpb}(\cdot)$ allows us to evaluate the impact of evaluation leakage over predictive performance: for low $\alpha$, $\operatorname{bpb}(\alpha)$ gives an indication on how the model performs on chunks that are entirely new; the slope of $\mathrm{bpb}(\cdot)$ shows how much the model exploits evaluation leakage.</p>
<h1>3. Related Work</h1>
<p>We first review existing work on using retrieval for language modelling, and compare Retro to these works (see Table 3). As we train Retro models on a large dataset containing a substantial section of the internet, our work raises potential privacy, safety, and fairness issues that we then review.</p>
<h3>3.1. Retrieval for language modelling</h3>
<p>Brants et al. (2007) show that scaling the training data to trillions of tokens improves the machine translation performance of $n$-gram models. More recently, GPT-2 (Radford et al., 2019), GPT-3 (Brown et al., 2020), and Jurassic-1 (Lieber et al., 2021) show that scaling up language models leads to massive improvements on many downstream tasks. At the same time, Carlini et al. (2021) demonstrate that large-scale language models can perfectly memorise parts of their training data, suggesting that enhancing models with retrieval may lead to further improvements. However, significant leakage between train and test datasets (Lee et al., 2021; Lewis et al., 2021) makes comparing and evaluating large models trained on large datasets difficult, especially once retrieval capabilities over the training dataset are added.</p>
<p>Historically, information retrieval for text relies on inverted index matching such as TF-IDF and BM25 (Robertson and Zaragoza, 2009). Foundational work use latent topic modelling approaches like LDA (Blei et al., 2003) to identify relevant neighbours (Wei and Croft, 2006). Work in machine translation such as Zhang et al. (2018) and Gu et al. (2018) retrieve translation pairs based on edit distance between source sentences and guide the translation output using the closest retrieved target sentences. The retrieval database may also be structured - for example, Ahn et al. (2016) use a symbolic knowledge graph to improve an RNN language model.</p>
<p>With the success of deep learning, retrieving systems have partly switched to dense learned representations based on a neural network's activations. Continuous cache (Grave et al., 2017) adds probability mass to tokens for which previous activations resemble the current activation vector, extending the model's context to the local history. $k \mathrm{NN}-\mathrm{LM}$ (Khandelwal et al., 2020) applies this idea to transformers and extends the retrieval database to English Wikipedia, resulting in</p>
<p>Table 3 | Comparison of Retro with existing retrieval approaches.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;"># Retrieval tokens</th>
<th style="text-align: center;">Granularity</th>
<th style="text-align: center;">Retriever training</th>
<th style="text-align: center;">Retrieval integration</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Continuous Cache</td>
<td style="text-align: center;">$O\left(10^{3}\right)$</td>
<td style="text-align: center;">Token</td>
<td style="text-align: center;">Frozen (LSTM)</td>
<td style="text-align: center;">Add to probs</td>
</tr>
<tr>
<td style="text-align: left;">kNN-LM</td>
<td style="text-align: center;">$O\left(10^{9}\right)$</td>
<td style="text-align: center;">Token</td>
<td style="text-align: center;">Frozen (Transformer)</td>
<td style="text-align: center;">Add to probs</td>
</tr>
<tr>
<td style="text-align: left;">Spalm</td>
<td style="text-align: center;">$O\left(10^{9}\right)$</td>
<td style="text-align: center;">Token</td>
<td style="text-align: center;">Frozen (Transformer)</td>
<td style="text-align: center;">Gated logits</td>
</tr>
<tr>
<td style="text-align: left;">DPR</td>
<td style="text-align: center;">$O\left(10^{9}\right)$</td>
<td style="text-align: center;">Prompt</td>
<td style="text-align: center;">Contrastive proxy</td>
<td style="text-align: center;">Extractive QA</td>
</tr>
<tr>
<td style="text-align: left;">Realm</td>
<td style="text-align: center;">$O\left(10^{9}\right)$</td>
<td style="text-align: center;">Prompt</td>
<td style="text-align: center;">End-to-End</td>
<td style="text-align: center;">Prepend to prompt</td>
</tr>
<tr>
<td style="text-align: left;">RAG</td>
<td style="text-align: center;">$O\left(10^{9}\right)$</td>
<td style="text-align: center;">Prompt</td>
<td style="text-align: center;">Fine-tuned DPR</td>
<td style="text-align: center;">Cross-attention</td>
</tr>
<tr>
<td style="text-align: left;">FiD</td>
<td style="text-align: center;">$O\left(10^{9}\right)$</td>
<td style="text-align: center;">Prompt</td>
<td style="text-align: center;">Frozen DPR</td>
<td style="text-align: center;">Cross-attention</td>
</tr>
<tr>
<td style="text-align: left;">EMDR $^{2}$</td>
<td style="text-align: center;">$O\left(10^{9}\right)$</td>
<td style="text-align: center;">Prompt</td>
<td style="text-align: center;">End-to-End (EM)</td>
<td style="text-align: center;">Cross-attention</td>
</tr>
<tr>
<td style="text-align: left;">Retro (ours)</td>
<td style="text-align: center;">$O\left(\mathbf{1 0}^{12}\right)$</td>
<td style="text-align: center;">Chunk</td>
<td style="text-align: center;">Frozen (BERT)</td>
<td style="text-align: center;">Chunked cross-attention</td>
</tr>
</tbody>
</table>
<p>substantial improvements on Wikitext103 evaluation. Continuous cache and $k \mathrm{NN}-\mathrm{LM}$ do not modify the underlying neural-network models, but interpolate at inference between the language model's output and distributions computed from retrieved tokens. These methods can therefore be plugged into any model without additional training, although this limits the model's ability to reason about the retrieved text. Spalm (Yogatama et al., 2021) addresses this limitation by adding an extra gating network to post-process the retrieved data; yet most of the network is unaffected by the retrieval during inference.</p>
<p>The retrieval representations may be trained directly instead of relying on a pre-trained modelretriever systems have been developed for this purpose, primarily on open-domain question answering. For example, DPR (Karpukhin et al., 2020) trains two BERT models (for queries and keys respectively) using a contrastive loss to align the representations of a question and of its answers. Lee et al. (2019) use an inverse cloze task to find semantic representations of passages for retrieval. These works differs from continuous cache and $k \mathrm{NN}-\mathrm{LM}$ in that they embeds passages (or chunks) of text together, as opposed to each token individually. The retriever network is trained in isolation of the downstream task that uses the retrieval data. This potential issue is specifically addressed by ReAlm (Guu et al., 2020), which trains the retrieval system end-to-end to maximize the final training cross-entropy. This comes with the extra complexity of searching the database during training and periodically updating the embedding table, severely limiting the scale at which it can operate. RAG (Lewis et al., 2020) and FiD (Izacard and Grave, 2021) build upon DPR to set the state of the art on question answering benchmarks by training encoder-decoder transformer models. More recently, Emdr ${ }^{2}$ (Sachan et al., 2021) extends FiD by using an expectation-maximization algorithm to train the retriever end-to-end and achieves state of the art results compared to similarly sized models.</p>
<p>In the open-domain dialogue setting, BlenderBot 2.0 (Komeili et al., 2021) learns to issue textual internet queries, outperforming dense retrieval methods when evaluated on a task measuring how close model responses are to those of humans. This involves collecting a dataset of human dialogues with associated search queries, which limits the scalability of this approach. Hashemi et al. (2020) introduce the Guided Transformer, a modified Transformer similar to Retro, for document retrieval and clarifying question selection. Although effective on question answering and other tasks with strong conditioning, none of these methods are designed to model arbitrary text sequences, in contrast with Retro.</p>
<p>Retro shares components with $k \mathrm{NN}-\mathrm{LM}$ and DPR in that it uses frozen retrieval representations. Retro models longer sequences than QA examples; this requires to reason at a sub-sequence level, and to retrieve different documents for the different chunks of a sequence. Similar to FiD, Retro processes the retrieved neighbours separately in the encoder, and assemble them in the chunked cross-attention. This differs from e.g. ReAlm, that prepends retrieved documents to the prompt. Using chunks allows for repeated retrieval whilst generating a sequence as opposed to retrieving only once based on the prompt alone. Furthermore, retrieval is done during the whole pre-training process in Retro, and is not simply plugged-in to solve a certain downstream task. Finally, previous methods based on dense query vectors use small models and retrieval datasets with less than 3B tokens (English Wikipedia). Table 3 summarizes the difference of Retro with existing approaches.</p>
<h1>3.2. Privacy, safety and fairness</h1>
<p>Bender et al. (2021); Weidinger et al. (2021) highlight several dangers of large language models. Those stem from their ability to memorise training data, their high training cost, the static nature of their training data (Lazaridou et al., 2021), their tendency of amplifying inherent biases in the training data, and their ability to generate toxic language (Gehman et al., 2020). In this section we inspect these dangers, focusing on how retrieval augmented language models may exacerbate or</p>
<p>mitigate them.
Large language models can perfectly memorise parts of their training data (Carlini et al., 2021). When coupled with large training datasets gathered from the web or other sources, this has clear privacy and safety implications. Retrieval models such as Retro that have access to the entire training dataset during inference exacerbate these privacy issues by being able to directly copy training data. However, retrieval systems offer a path towards mitigating these concerns via obliteration of the retrievable data at inference time. In addition, differential privacy training (Abadi et al., 2016) of retrieval models could guarantee that no private information is stored in the model weights, while individualisation on private data could be made by updating the retrieval database at inference time.</p>
<p>Due to their high training cost, re-training large language model regularly to incorporate new data, languages, and norms is prohibitively expensive. To keep retrieval models up-to-date, it may be sufficient to update the retrieval database, which is orders of magnitude cheaper than re-training a model from scratch. In addition to the benefits of updating models in terms of fairness and bias, simply training large language models has a significant energy cost (Schwartz et al., 2020; Strubell et al., 2019). Retrieval mechanisms offer a path to reducing the compute requirements needed to train and update language models that reach a certain performance.</p>
<p>Large language models are prone to generating toxic outputs, as shown in Gehman et al. (2020). Bender et al. (2021); Jo and Gebru (2020) advocate for the importance of better training data curation and documentation. Additionally, if portions of the training data are found to be eliciting biased or toxic outputs after training, retrieval allows for some correction, as the offending retrieval data can be retroactively filtered. However, it is also the case that without careful analysis and intervention, retrieval models may exacerbate biases that are present in the training data. Retrieval models can also add a further source of bias through the selection mechanism for retrieval documents. Further work in this area is required to better understand how retrieval affects the bias and toxicity of the model outputs.</p>
<p>Finally, samples from large models are difficult to interpret, making mitigating these issues all the more challenging (Belinkov et al., 2020; Jain and Wallace, 2019). Retrieval provides more insights in to the outputs of a model, as one can directly visualise or modify the neighbours that are being used. The examples in Table 6, 7, 20 and 21 illustrate how retrieval makes language models more factual and interpretable by providing more transparent outputs.</p>
<h1>4. Results</h1>
<p>We first report results on language modelling benchmarks. Second, we show how to Retrofit pre-trained Transformer language models into retrieval models with few additional FLOPs. Next, we report Retro results on question answering. Finally, we report evaluation metrics with leakage filtering, to better understand the source of the gains with retrieval.</p>
<h3>4.1. Language modelling</h3>
<p>Datasets. We evaluate our models on C4 (Raffel et al., 2020), Wikitext103 (Merity et al., 2017), Curation Corpus (Curation, 2020), Lambada (Paperno et al., 2016) and the Pile (Gao et al., 2020). We also evaluate on a set of manually selected Wikipedia articles that were added or heavily edited in September 2021, months after our pre-training and retrieval dataset was collected (details are given in §A.2). We construct the dataset with articles from the "future" and manually remove new articles that strongly overlap documents in our training data. This guarantees that the evaluation documents are not leaked in our training data.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3 | Scaling with respect to model size. (a) LAMBADA top-1 accuracy. (b) Evaluation loss on curation corpus. (c) Perplexity on Wikitext103 valid. (d) Bits-per-byte on selected Wikipedia articles from September 2021.</p>
<p>For C4, Wikitext103, the Pile, and our Wikipedia dataset we evaluate the language modelling performance on entire documents and measure the bits-per-byte (bpb). We favour bits-per-byte over loss as it is tokenizer agnostic. We evaluate with a sequence length of 2048 tokens but use a stride of 1024 within documents to mitigate boundary effects. On Curation Corpus we concatenate the article, the "TL;DR:" string, and the summary, but only evaluate the bpb on the summary. For Lambada we evaluate the accuracy on the last word, using greedy generation.</p>
<p>Model scaling. In Fig. 1(left) and Fig. 3 we show the language modelling performance as we scale models from 150 million to 7 billion (non-embedding) parameters. We see that on all datasets, Retro outperforms the baseline at all model sizes. Furthermore, we observe that improvements do not diminish as we scale the models. The performance is dataset dependent, with the largest gains on Wikitext103 and C4. Wikipedia articles and other web pages are similar to Wikitext103 documents, even if not exact copies (\$4.4), we thus obtain dramatic improvements on Wikitext103 as our retrieval model is able to directly exploit these overlaps. The smallest gains are for Curation Corpus, where Retro only slightly outperforms the baseline. This is expected as Curation Corpus summaries are designed to only contain information from the source article and are not included in our retrieval database. On our "future" Wikipedia September 2021 dataset, we also observe consistent gains for all model sizes.</p>
<p>Data scaling. Fig. 1 (middle) shows how scaling the retrieval database at evaluation improves the language modelling performance. We observe dramatic gains as the retrieval data is increased from Wikipedia ( 4 billion tokens) to all of Massive text ( 1.7 T tokens). Fig. 1(right) shows how performance scales as we increase the number of retrieved chunks. Despite being only trained with 2 neighbours, we see consistent improvements for all models when the number of neighbours is increased from 1 to 10. Furthermore, we observe that larger models are able to better utilise more neighbours: the 172M model improves with up to 10 neighbours, whereas the 7B model improves with up to 40 neighbours.</p>
<p>The Pile. We evaluate our 7B models on the Pile test sets ${ }^{3}$ and compare against the 178B parameter Jurrasic-1 (Lieber et al., 2021) model and the 280B parameter Gopher (Rae et al., 2021) model. We do not compare against GPT-3 as it is outperformed by Jurassic-1 and Gopher on almost all subsets. Fig. 4 shows the relative improvements in bits-per-byte over our 7B transformer baseline for our</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4 | The Pile: Comparison of our 7B baseline against Jurassic-1, Gopher, and Retro. We observe that the retrieval model outperforms the baseline on all test sets and outperforms Jurassic-1 on a majority of them, despite being over an order of magnitude smaller.
7.5B Retro model, Jurassic-1 and Gopher. Jurassic-1 outperforms the baseline on all datasets except for books, likely due to the inclusion of books in our training data. Gopher and Retro outperform the baseline on all test sets. Overall, Retro 7.5B outperforms Jurassic-1 and Gopher on a majority of the test sets. On the dm_mathematics and ubuntu_irc subsets, our Retro model does not outperform our 7B baseline and underperforms Jurassic-1. We hypothesise that the retrieved neighbours on these datasets are not helpful, due to a combination of what is in our retrieval dataset and the efficacy of the nearest-neighbour search.</p>
<p>Wikitext103. To validate our approach in a controlled setting, we compare our method with $k N N-L M$ (Khandelwal et al., 2020) on the Wikitext103 dataset in Table 4. We train a baseline transformer on the training set of Wikitext103. This transformer has 24 layers, 1024 hidden units, 16 heads and a key size of 64, as in Baevski and Auli (2019). Our baseline does not have adaptive input, and our tokenizer has an open vocabulary, unlike Baevski and Auli (2019), which makes our baseline</p>
<p>Table 4 | Perplexities on Wikitext103. When using the Wikpedia dataset for retrieval, Retro performs similarly to our implementation of $k N N-L M$. As we scale the retrieval dataset, Retro performs much better. The perplexities for retrieving from full MassiveText are quite low, which is partly due to partial overlap with Wikitext103 not caught by our deduplication.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Retrieval Set</th>
<th style="text-align: center;">#Database tokens</th>
<th style="text-align: center;">#Database keys</th>
<th style="text-align: center;">Valid</th>
<th style="text-align: center;">Test</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Adaptive Inputs (Baevski and Auli, 2019)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">17.96</td>
<td style="text-align: center;">18.65</td>
</tr>
<tr>
<td style="text-align: center;">Spalm (Yogatama et al., 2021)</td>
<td style="text-align: center;">Wikipedia</td>
<td style="text-align: center;">3B</td>
<td style="text-align: center;">3B</td>
<td style="text-align: center;">17.20</td>
<td style="text-align: center;">17.60</td>
</tr>
<tr>
<td style="text-align: center;">$k$ NN-LM (Khandelwal et al., 2020)</td>
<td style="text-align: center;">Wikipedia</td>
<td style="text-align: center;">3B</td>
<td style="text-align: center;">3B</td>
<td style="text-align: center;">16.06</td>
<td style="text-align: center;">16.12</td>
</tr>
<tr>
<td style="text-align: center;">Megatron (Shoeybi et al., 2019)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">10.81</td>
</tr>
<tr>
<td style="text-align: center;">Baseline transformer (ours)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">21.53</td>
<td style="text-align: center;">22.96</td>
</tr>
<tr>
<td style="text-align: center;">$k$ NN-LM (ours)</td>
<td style="text-align: center;">Wikipedia</td>
<td style="text-align: center;">4B</td>
<td style="text-align: center;">4B</td>
<td style="text-align: center;">18.52</td>
<td style="text-align: center;">19.54</td>
</tr>
<tr>
<td style="text-align: center;">Retro</td>
<td style="text-align: center;">Wikipedia</td>
<td style="text-align: center;">4B</td>
<td style="text-align: center;">0.06B</td>
<td style="text-align: center;">18.46</td>
<td style="text-align: center;">18.97</td>
</tr>
<tr>
<td style="text-align: center;">Retro</td>
<td style="text-align: center;">C4</td>
<td style="text-align: center;">174B</td>
<td style="text-align: center;">2.9B</td>
<td style="text-align: center;">12.87</td>
<td style="text-align: center;">10.23</td>
</tr>
<tr>
<td style="text-align: center;">Retro</td>
<td style="text-align: center;">MassiveText (1\%)</td>
<td style="text-align: center;">18B</td>
<td style="text-align: center;">0.8B</td>
<td style="text-align: center;">18.92</td>
<td style="text-align: center;">20.33</td>
</tr>
<tr>
<td style="text-align: center;">Retro</td>
<td style="text-align: center;">MassiveText (10\%)</td>
<td style="text-align: center;">179B</td>
<td style="text-align: center;">4B</td>
<td style="text-align: center;">13.54</td>
<td style="text-align: center;">14.95</td>
</tr>
<tr>
<td style="text-align: center;">Retro</td>
<td style="text-align: center;">MassiveText (100\%)</td>
<td style="text-align: center;">1792B</td>
<td style="text-align: center;">28B</td>
<td style="text-align: center;">3.21</td>
<td style="text-align: center;">3.92</td>
</tr>
</tbody>
</table>
<p>perplexities a bit higher. The full experiment details and hyperparameters are given in $\S$ C. 2 and Table 11 .</p>
<p>We re-implement $k \mathrm{NN}-\mathrm{LM}$ with our tokenizer and baseline transformer to produce embeddings of size 1024 for every token in Wikitext103. $k \mathrm{NN}-\mathrm{LM}$ has probabilities $p_{k \mathrm{NN}-\mathrm{LM}}=\lambda p_{k \mathrm{NN}}+(1-\lambda) p_{\mathrm{LM}}$ with $p_{k \mathrm{NN}}\left(n_{k}\right) \propto \exp \left(-\alpha d_{k}\right)$. We tune $\lambda=0.118$ and $\alpha=0.00785$ on the validation set (Fig. 7) and report performance for these hyperparameters on both the validation and test set.</p>
<p>We fine-tune our baseline transformer into a Retro model (Fig. 7), using the Wikitext103 training data and retrieving from Wikipedia with 2 neighbours. We only train the new weights, as explained in $\S 4.2$, and share the embedding weights between the encoder and the main pathway. This is necessary for Wikitext103 which is quite small, as training Retro from scratch in this setting leads to over-fitting.</p>
<p>We evaluate the fine-tuned Retro model with different retrieval sets. We use 10 neighbours at evaluation for both Retro and $k \mathrm{NN}-\mathrm{LM}$. When retrieving from Wikipedia, we obtain results comparable to our $k \mathrm{NN}-\mathrm{LM}$ implementation. Furthermore, scaling the retrieval database to MassiveText yields dramatic improvements, though this is partly due to leakage (see §4.4). For reproducibility, we also include results when retrieving from C 4 , which are close to previous state-of-the-art and comparable to using $10 \%$ of MassiveText.</p>
<p>It is worth noting that $k \mathrm{NN}-\mathrm{LM}$ requires 1024 floats for every token in the retrieval dataset, totalling 15 terabytes ( Tb ) for the 4 billion tokens in Wikipedia. $k \mathrm{NN}-\mathrm{LM}$ and other token-level retrieval approaches therefore don't scale to retrieval databases with trillions of tokens such as MassiveText. In comparison, Retro only requires 215 Gb to index our Wikipedia dataset, and 93 Tb for MassiveText. Inspecting the number of retrieval database entries in Table 4 makes it clear why retrieving at the chunk level is necessary when scaling to datasets with trillions of tokens.</p>
<h1>4.2. Retro-fitting baseline models</h1>
<p>We extend baseline models into Retro models by freezing the pre-trained weights and training only chunked cross-attention and neighbour encoder parameters (less than $10 \%$ of weights for the 7B model) in Fig. 5. This offers an efficient alternative path to enhance transformers with retrieval, requiring only 6 million sequences ( $3 \%$ of the pre-training sequences that we used). Additionally, by only training the new weights we ensure that when evaluated without retrieval, the original model performance is exactly maintained. Retrofitting models quickly surpasses the performance of baseline models and even achieves performance close to that of Retro models trained from scratch. The experiment hyperparameters are given in §C.3.</p>
<h3>4.3. Question answering</h3>
<p>We fine-tune our retrieval models on the Natural Questions (Kwiatkowski et al., 2019) dataset to demonstrate that our retrieval pathway can be used to inject information from arbitrary data sources. We use the version ${ }^{4}$ provided by Izacard and Grave (2021) which is augmented with the retrieved passages from DPR (Karpukhin et al., 2020). We fine-tune all the weights of our 7.5B pre-trained Retro model for 25,000 steps using the top 20 retrieved passages. We format the data as "question: {question} \nanswer: {answer}" and left pad the data such that "answer:" coincides with the end of the first chunk of 64 tokens and thus aligns with the first retrieving chunk. The model has access to the question via the previous tokens in the sequence as well as the top 20 DPR Wikipedia passages and their titles via the chunked cross-attention mechanism.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5 | Retro-fitting a baseline transformer. Any transformer can be fine-tuned into a retrievalenhanced transformer by randomly initializing and training only the chunked cross-attention and retrieval encoder weights. Fine-tuning in this way quickly recovers and surpasses the non-retrieval performance, and almost achieves the same performance as training a retrieval model from scratch (shown by the arrow on the right hand side of each plot). We find good performance Retro-fitting our models training on only $3 \%$ the number of tokens seen during pre-training.</p>
<p>The exact match scores are shown in Table 5 and the full fine-tuning details are given in §C.4. Our method is competitive with previous approaches such as Realm, RAG and DPR, but underperforms the more recent FiD. In contrast with this work, we find that increasing the number of neighbours past 20 does not improve Retro performance on this task. We hypothesise that the encoder-decoder structure of T5-the base model in FiD- and the T5 pre-training objective leads to a model that relies more on the encoder output than Retro, which is important in the QA setting. To compete with T5-finetuned models, future work should consider ways of forcing RETRO to rely further on the retrieval encoder output when producing tokens.</p>
<h1>4.4. Relating retrieval performance to dataset leakage.</h1>
<p>We report the filtered eval losses as detailed in $\S 2.6$ on C4, Curation Corpus and Wikitext103 in Fig. 6. On C4 and Wikitext103, for which there is leakage into the training set, the slope is negative for both baseline models and Retro models. Retro models exploit leakage more strongly than baseline models, as indicated by the more negative slope. This is due to its explicit ability to copy-paste existing training chunks to predict leaked evaluation chunks (see a qualitative example of this model behavior</p>
<p>Table 5 | Question answering results. Exact match accuracy on Natural Questions.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Test Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Realm (Guu et al., 2020)</td>
<td style="text-align: center;">40.4</td>
</tr>
<tr>
<td style="text-align: left;">DPR (Karpukhin et al., 2020)</td>
<td style="text-align: center;">41.5</td>
</tr>
<tr>
<td style="text-align: left;">RAG (Lewis et al., 2020)</td>
<td style="text-align: center;">44.5</td>
</tr>
<tr>
<td style="text-align: left;">EMDR $^{2}$ (Sachan et al., 2021)</td>
<td style="text-align: center;">52.5</td>
</tr>
<tr>
<td style="text-align: left;">FiD (Izacard and Grave, 2021)</td>
<td style="text-align: center;">51.4</td>
</tr>
<tr>
<td style="text-align: left;">FiD + Distill. (Izacard et al., 2020)</td>
<td style="text-align: center;">$\mathbf{5 4 . 7}$</td>
</tr>
<tr>
<td style="text-align: left;">Baseline 7B (closed book)</td>
<td style="text-align: center;">30.4</td>
</tr>
<tr>
<td style="text-align: left;">Retro 7.5B (DPR retrieval)</td>
<td style="text-align: center;">45.5</td>
</tr>
</tbody>
</table>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6 | Performance vs. longest common retrieval substring. Evaluation loss as a function of allowed longest common substring between evaluation data chunks and their nearest neighbours. Retrieval still helps when considering chunks with no more than 8 contiguous tokens overlapping with training dataset chunks.
on a Wikitext103 article in Table 19). On Curation Corpus, retrieval provides a constant offset, which is expected as there is by design no leakage between Curation Corpus and the training dataset.</p>
<p>On the other hand, Retro outperforms baseline models at all leakage levels, down to $\alpha=12.5 \%$. At this level, the loss is computed on chunks with less than 8 contiguous tokens shared with the closest matching chunk in the training dataset-this is a reasonable level of overlap at which we consider that there is no local leakage. Retrieval thus improves predictions on both chunks that are syntactically similar to chunks in the training set, and on chunks that are syntactically different from all training chunks. This points toward a non trivial Retro capacity of generalizing based on both model parameters and retrieval database. Similar results are found on the Pile dataset (see Fig. 12, §F.3).</p>
<h1>4.5. Using Retro for sampling</h1>
<p>We show examples of samples obtained using the 7.5B Retro model in Table 6, Table 7 and Appendix E. For each chunk (the first one being the prompt), we juxtapose sampled chunks $C_{u}$ with retrieved neighbours $\operatorname{Ret}\left(C_{u}\right)$. To give an indication of local overlap, we colour each sampled token in chunk $C_{u}$ based on the length of the longest common prefix (LCP) found in the retrieved chunks $\operatorname{Ret}\left(C_{u-1}\right)$. Similarly, we colour the retrieved chunks based on the LCP in the sampled chunk. For the sample in Table 6, for which we chose the prompt, we observe that the retrieved chunks influence the sample as there are overlaps between the sampled tokens and neighbour tokens. Overall, retrieval reduces hallucinations (in line with the findings of Shuster et al. (2021)) and makes the model more knowledgeable, when comparing with samples produced with retrieval disabled. In the sample in Table 7, the model recognises that the prompt is the beginning of the first scene of Hamlet and leverages retrieval data to continue it with only a few mistakes. We provide further examples in Appendix E, including examples from the evaluation sets, as well as the detailed procedure used for colouring the tables.</p>
<h2>5. Conclusion</h2>
<p>We present Retrieval-Enhanced Transformers (Retro), a method for modelling arbitrary text sequences whilst retrieving from databases with trillions of tokens-scaling the data available to models by an order of magnitude compared to what is typically consumed during training. Retro models</p>
<p>gains do not diminish for models with up to at least 7B parameters, and correspond to non-retrieval models with $10 \times$ more parameters on certain datasets. On Wikitext103 and the Pile, Retro outperforms previous models trained on large scale datasets. We also show that Retro is competitive on retrieval-intensive downstream tasks such as question answering.</p>
<p>Retro models are flexible and can be used without retrieval at evaluation and still achieve comparable performance to baseline models. Conversely, baseline models can be rapidly fine-tuned into Retro models to obtain nearly the same performance as if trained from scratch. Careful analysis shows that only a modest fraction of the gains obtained by Retro are due to test set leakage. In general, we caution for such leakage in large-scale language datasets and suggest further work in better understanding the role of test set leakage in the performance of large-scale language models.</p>
<p>Overall, our work demonstrates at an unprecedented scale that semi-parametric approaches can provide an orthogonal, more efficient approach than raw parameter scaling as we seek to build more powerful language models.</p>
<h1>Acknowledgements</h1>
<p>We would like to thank Nikolai Grigorev, Marc'aurelio Ranzato, Cyprien de Masson d'Autume, Po-Sen Huang, Johannes Welbl, Lisa Anne Hendricks, Ethan Perez, Jeff Stanway, Eric Noland, Gregory Wayne, John Jumper, Julian Schrittwieser, Lorrayne Bennett, Devang Agrawal, Dani Yogatama, Susannah Young, Nando de Freitas, Demis Hassabis, and Koray Kavukcuoglu for their help, advice and reviews. Additionally, we would like to thank Zonglin Li, David Simcha, and the ScaNN developers for their help.</p>
<p>Table 6 | Sample - Beavers are interesting animals. The Retro[Off] sample quickly diverges to other animals while the Retro[On] sample tends to stay focused on the beaver topic due to neighbour conditioning.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Prompt and sample of Retro[Off]</th>
<th style="text-align: center;">Prompt and sample of Retro[On] colored by LCP with $\operatorname{Ret}\left(C_{n}-1\right)$ LCP $=\mathbf{0.8} \mathbf{2 3 4} \geq 5$</th>
<th style="text-align: center;">$\left[N_{2}^{1}, F_{2}^{1}\right]$ colored by LCP with $C_{n+1}$ LCP $=\mathbf{0.8} \mathbf{2 3 4} \geq 5$</th>
<th style="text-align: center;">$\left[N_{2}^{2}, F_{2}^{2}\right]$ colored by LCP with $C_{n+1}$ LCP $=\mathbf{0.8} \mathbf{2 3 4} \geq 5$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Beavers are interesting animals that live near rivers. They build</td>
<td style="text-align: center;">Beavers are interesting animals that live near rivers. They build</td>
<td style="text-align: center;">Beavers build their lodges in ponds they have created in wooded areas like many things in nature, there is a connection between creatures in the wild. beaver ponds cause rooks to drown, but the dead trees attract the great blue beron, which often return year after year. Over time, a beaver pond can attract more than 50 nests in a colony, called a rooks. As an example of this can be found in the large pond off Bradford Road or Carter Fields near the Bonford 1 no Chris Leahy, an expert with the Massachusetts Audubon Society who wrote</td>
<td style="text-align: center;">now them into smaller sections and they them into the water Engineers of the Pond. Beavers are interesting animals because they change the habitat in which they live. Beavers do this by blocking up streams to create ponds. Then they build their houses called lodges, in these ponds. Beavers' bodies make them well-suited for underwater building. Special must be close off their zones, ears, and throats to keep the water out. Beavers' broad tails act like rudders for moving. Their two very large, set age from teeth are used to goase down trees. They begin building their d 20</td>
</tr>
<tr>
<td style="text-align: center;">dams to create ponds. Frogs are am phibians, so they can live in both 1 and and water. They have great camouflage to hide from predators. The G olden Retriever, or Golden, is a very big dog. They are very strong, and have a thick coat to help them live in</td>
<td style="text-align: center;">Their houses called beaver dams in the riverbeds. They also live on land. Beavers use their strong teeth as strong jaws to cut down trees and branches to build their homes. They also use their teeth and jaws to che w up food. Beavers use their big, fl at tails to swim. They use</td>
<td style="text-align: center;">Dams they mean that you are very big. Beavers swim easily in streams, picking up rocks and sticks to build their dams. They goase at trees with their big front teeth to cut them down. Then they use parts of the tree to build their houses. Beavers are clever builders. They know exactly what they need to build their beaver dams. They use mud from the stream to make their dams stay together. They use their tails to put down the mud. Beavers put a snug room at the top of their dams for their babies. They store their food underwater. Be avers eat the bark from the</td>
<td style="text-align: center;">as like tall, and two protruding teeth that are strong enough to goase down trees. The beaver uses trees, big oches, and mud to build dams across rivers and streams. These dams create a deep pool of water in which the beaver builds its home. A beaver hom e is called a lodge. A baby beaver is a "bit" remains in the family lodge until the age of two. Beaver fur, be seen as pelt, was once highly popular as a trim for hats and coats. How might the popularity of beaver fur or attributed to the colonization of New York?www.Ww</td>
</tr>
<tr>
<td style="text-align: center;">cold areas. A kangaroo is an anima 1 that is very large. It can jump w ry high. They can run very fast. A moerkat is a small animal that lives in Africa. They live in colonies of up to a hundred of them. They can c limb trees very easily.</td>
<td style="text-align: center;">Their sharp teeth to goase at rocks and cut trees. They use their flat tails to steer when they are swimming in the winter, beavers have to build a house called a beaver lodge. They build a house that is very strong. The walls are made of twigs. They $2$ not to made</td>
<td style="text-align: center;">Dams they mean that you are very big. Beavers swim easily in streams, picking up rocks and sticks to build their dams. They goase at trees with their big front teeth to cut them down. Then they use parts of the tree to build their houses. Beavers are clever builders. They know exactly what they need to build their beaver dams. They use mud from the stream to make their dams stay together. They use their tails to put down the mud. Beavers put a snug room at the top of their dams for their babies. They store their food underwater. Be avers eat the bark from the</td>
<td style="text-align: center;">w or trees with their big front teeth to cut them down. Then they use parts of the trees to build their houses. Beavers are clever builders. They know exactly what they need to build their beaver dams. They use mud from the stream to make their dams stay together. They use their tails to put down the mud. Beavers put a snug room at the top of their dams for their babies. They store their food underwater. Be avers eat the bark from the</td>
</tr>
</tbody>
</table>
<p>Table 7 | Sample - Hamlet, Act 1, Scene 1. The Retro[Off] sample has correct syntax but is hallucinated, and ends with repetition of one character (FRANCISCO Approach me not). The Retro[On] sample is the correct continuation of the original text, and is robust to formatting differences between our prompt and the retrieved data.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Prompt and sample of Retro[Off]</th>
<th style="text-align: center;">Prompt and sample of Retro[On] colored by LCP with $\operatorname{Ret}\left(C_{n}-1\right)$ LCP $=0,9,2,3,4, \geqslant 5$</th>
<th style="text-align: center;">$\left[N_{0}^{3}, F_{0}^{1}\right]$ colored by LCP with $C_{n+1}$ LCP $=0,9,2,3,4, \geqslant 5$</th>
<th style="text-align: center;">$\left[N_{0}^{3}, F_{0}^{2}\right]$ colored by LCP with $C_{n+1}$ <br> LCP $=0,9,2,3,4, \geqslant 5$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">ACT I SCENE I. Elsinore. A pla thorn before the castle. FRANC ISCO at his post. Enter to him BERNARDO BERNARDO Who's there ? FRANCISCO Nay, answer me: at and, and unfold yourself. BERN ARDO</td>
<td style="text-align: center;">ACT I SCENE I. Elsinore. A pla thorn before the castle. FRANC ISCO at his post. Enter to him BERNARDO BERNARDO Who's there ? FRANCISCO Nay, answer me: at and, and unfold yourself. BERN ARDO</td>
<td style="text-align: center;">ACT I SCENE I. Elsinore. A pla thorn before the castle. (FRAN CISCO at his post. Enter to him BERNARDO) BERNARDO Who's there ? FRANCISCO Nay, answer me: atand, and unfold yourself. BE RNARDO Long live the king! FRANCISCO Bernardo? BERNARDO He. FRANCISCO You come most careful. By upon your hour. BERNARDO <br> 'Tis now struck twelve; get the e to bed, Francisco. FRANCISCO <br> 'For this relief much thanks: 'tis bitter cold, And I am sick at heart. BERNARDO Have you</td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { \% } \times \text { TEXT } \times \text { ACT I } \ &amp; \text { \% } \times \text { TEXT } \times \text { ACT I } \ &amp; \text { \% } \times \text { FACT } \ &amp; \text { \% } \ &amp; \text { \% } \ &amp; \text { \% } \ &amp; \text { \% } \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;">Who calls ? FRANCISCO I am th e lord here; I, Francisco, tha t am sick of grief.</td>
<td style="text-align: center;">Long live the king! FRANCISCO Bernardo? BERNARDO He. FRANC I SCO You come most carefully up on your hour. BERNARDO 'Tis no w struck twelve; get thee to b ed, Francisco. FRANCISCO For I his relief much thanks: 'tis b itter cold, And I am sick at h eart. B</td>
<td style="text-align: center;">Long live the king! FRANCISCO Bernardo? BERNARDO He. FRANCI SCO You come most carefully up on your hour. BERNARDO 'Tis no w struck twelve; get thee to b ed, Francisco. FRANCISCO For I his relief much thanks: 'tis b itter cold, And I am sick at h eart. $=/ \text { TEXT } \times \times / \text { DOC } \times \times \text { DOC } \times \times \text { DO }}$ CNO $\times$ comes $\times /$ DOCNO $\times \times$ TEXT $\times$ ACT I <br> 'PROLOGUE. Two households, bo th alike in dignity, 'tis her b erona, where we lay our worst. From ancient grudge break to new mutiny.</td>
<td style="text-align: center;">'live the king! FRANCISCO Bern aido? BERNARDO He. FRANCISCO 3 no come most carefully upon in or hour. BERNARDO 'Tis now at sick twelve; get thee to bed, Francisco. FRANCISCO For this r elief much thanks: 'tis bitter cold, And I am sick at heart. BERNARDO Have you had quiet g uard? FRANCISCO Not a mouse st icing. BERNARDO Well, good si ght. 1000 do meet Horatio and Marcellus, The rivali of my watch, bid them make haste. FR ANCISCO I think I hear them in brand, but who is there? 25</td>
</tr>
<tr>
<td style="text-align: center;">Francisco, I would speak with you. FRANCISCO Approach me not , but speak. BERNARDO Your han d, your voice FRANCISCO I will not hear thee speak. BERNARDO Francisco, your hand, I entre at thee. FRANCISCO Approach me not. BERNARDO Francisco FRANC</td>
<td style="text-align: center;">ERNARDO Have you had quiet qua rd? FRANCISCO Not a mouse sti ning. BERNARDO Well, good night t. If you do meet Horatio and Marcellus, The rivali of my wu ich, bid them make haste. FRAN CISCO I think I hear them, tha nd, but who is there? Enter</td>
<td style="text-align: center;">had quiet guard? FRANCISCO No t a mouse stirring. BERNARDO W ell, good night. If you do meet Horatio and Marcellus, The rivali of my watch, bid them make haste. FRANCISCO I think I hear them, tha nd, but who is there? Enter</td>
<td style="text-align: center;">ARDO Have you had quiet guard? FRANCISCO Not a mouse stirris g. BERNARDO Well, good night. If you do meet Horatio and Marcellus, The rivali of my watch, bid them make haste. FRANCIS CO I think I hear them. - Stand 'her who is there? ENTER HORATIO AND MARCELLUS, HORATIO Fri ends to this ground. MARCELLUS And liegemen to the Dane. FRANCISCO Give yo u good night. MARCELLUS O, far ewell, honest soldier: Who hat h relieved you? FRANCISCO Bern aido has my place. Give you go od night. (Ead</td>
</tr>
<tr>
<td style="text-align: center;">ISCO Approach me not. BERNARDO I have a letter FRANCISCO App roach me not. BERNARDO For the king. FRANCISCO Approach me n ot. BERNARDO There's no trusso n in't. FRANCISCO Approach me not. BERNARDO I will</td>
<td style="text-align: center;">HORATIO and MARCELLUS HORATIO Friends to this ground. MARCE LLUS And liegemen to the Dane. FRANCISCO Give you good night MARCELLUS O, farewell, honest I soldier: Who hath relieved y on? FRANCISCO Bernardo hath my place. Give you good night.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<h1>References</h1>
<p>M. Abadi, A. Chu, I. Goodfellow, H. B. McMahan, I. Mironov, K. Talwar, and L. Zhang. Deep learning with differential privacy. In ACM SIGSAC Conference on Computer and Communications Security, 2016.
S. Ahn, H. Choi, T. Pärnamaa, and Y. Bengio. A neural knowledge language model. arXiv preprint arXiv:1608.00318, 2016.
A. Baevski and M. Auli. Adaptive input representations for neural language modeling. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id= ByxZX20qFQ.
Y. Belinkov, S. Gehrmann, and E. Pavlick. Interpretability and analysis in neural NLP. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts, pages 1-5, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020. acl-tutorials.1. URL https://aclanthology.org/2020.acl-tutorials.1.
E. M. Bender, T. Gebru, A. McMillan-Major, and S. Shmitchell. On the dangers of stochastic parrots: Can language models be too big? In ACM Conference on Fairness, Accountability, and Transparency, 2021.
D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent Dirichlet Allocation. Journal of Machine Learning Research, 3(Jan):993-1022, 2003. URL https://jmlr.csail.mit.edu/papers/v3/ blei03a.html.
J. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclaurin, G. Necula, A. Paszke, J. V. der Plas, S. Wanderman-Milne, and Q. Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL http://github.com/google/jax.
T. Brants, A. C. Popat, P. Xu, F. J. Och, and J. Dean. Large Language models in machine translation. In Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 858-867, 2007.
T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Language models are few-shot learners. In Advances in Neural Information Processing Systems, 2020. URL https://proceedings.neurips.cc/ paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.
N. Carlini, F. Tramer, E. Wallace, M. Jagielski, A. Herbert-Voss, K. Lee, A. Roberts, T. Brown, D. Song, U. Erlingsson, A. Oprea, and C. Raffel. Extracting training data from large language models. Preprint, 2021.
C. Consonni, D. Laniado, and A. Montresor. Wikilinkgraphs: a complete, longitudinal and multilanguage dataset of the wikipedia link networks. In AAAI International Conference on Web and Social Media, volume 13, 2019.</p>
<p>Curation. Curation corpus base, 2020.
Z. Dai, Z. Yang, Y. Yang, J. Carbonell, Q. Le, and R. Salakhutdinov. Transformer-XL: Attentive language models beyond a fixed-length context. In Annual Meeting of the Association for Computational Linguistics, July 2019. URL https://aclanthology.org/P19-1285.</p>
<p>J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Conference of the North American Chapter of the Association for Computational Linguistics, June 2019. URL https://aclanthology.org/N19-1423.
L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster, J. Phang, H. He, A. Thite, N. Nabeshima, S. Presser, and C. Leahy. The Pile: An 800GB dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020.
S. Gehman, S. Gururangan, M. Sap, Y. Choi, and N. A. Smith. RealToxicityPrompts: Evaluating neural toxic degeneration in language models. In Conference on Empirical Methods in Natural Language Processing, Nov. 2020. URL https://aclanthology.org/2020.findings-emnlp.301.
E. Grave, A. Joulin, and N. Usunier. Improving neural language models with a continuous cache. In International Conference on Learning Representations, 2017. URL https://openreview.net/ forum?id=B184E5qee.
A. Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2013.
J. Gu, Y. Wang, K. Cho, and V. O. Li. Search engine guided neural machine translation. In AAAI Conference on Artificial Intelligence, 2018.
R. Guo, P. Sun, E. Lindgren, Q. Geng, D. Simcha, F. Chern, and S. Kumar. Accelerating large-scale inference with anisotropic vector quantization. In International Conference on Machine Learning, 2020. URL https://arxiv.org/abs/1908.10396.
K. Guu, K. Lee, Z. Tung, P. Pasupat, and M. Chang. Retrieval augmented language model pre-training. In International Conference on Machine Learning, 2020.
H. Hashemi, H. Zamani, and W. B. Croft. Guided transformer: Leveraging multiple external sources for representation learning in conversational search. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 1131-1140, 2020.
T. Hennigan, T. Cai, T. Norman, and I. Babuschkin. Haiku: Sonnet for JAX, 2020. URL http: //github.com/deepmind/dm-haiku.
G. Izacard and E. Grave. Leveraging passage retrieval with generative models for open domain question answering. In Conference of the European Chapter of the Association for Computational Linguistics, Apr. 2021. URL https://aclanthology.org/2021.eacl-main. 74.
G. Izacard, F. Petroni, L. Hosseini, N. De Cao, S. Riedel, and E. Grave. A memory efficient baseline for open domain question answering. arXiv preprint arXiv:2012.15156, 2020.
S. Jain and B. C. Wallace. Attention is not Explanation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 3543-3556, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1357. URL https: //aclanthology.org/N19-1357.
E. S. Jo and T. Gebru. Lessons from archives: Strategies for collecting sociocultural data in machine learning. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, pages 306-316, 2020.
R. Jozefowicz, O. Vinyals, M. Schuster, N. Shazeer, and Y. Wu. Exploring the limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.</p>
<p>J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, and D. Amodei. Scaling laws for neural language models. CoRR, 2020. URL https://arxiv. org/abs/2001.08361.
V. Karpukhin, B. Oguz, S. Min, P. Lewis, L. Wu, S. Edunov, D. Chen, and W.-t. Yih. Dense passage retrieval for open-domain question answering. In Conference on Empirical Methods in Natural Language Processing, Nov. 2020. URL https://aclanthology.org/2020.emnlp-main.550.
U. Khandelwal, O. Levy, D. Jurafsky, L. Zettlemoyer, and M. Lewis. Generalization through memorization: Nearest neighbor language models. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=HklBjCEKvH.
M. Komeili, K. Shuster, and J. Weston. Internet-augmented dialogue generation. arXiv preprint arXiv:2107.07566, 2021.
T. Kudo and J. Richardson. Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. arXiv preprint arXiv:1808.06226, 2018.
T. Kwiatkowski, J. Palomaki, O. Redfield, M. Collins, A. Parikh, C. Alberti, D. Epstein, I. Polosukhin, M. Kelcey, J. Devlin, K. Lee, K. N. Toutanova, L. Jones, M.-W. Chang, A. Dai, J. Uszkoreit, Q. Le, and S. Petrov. Natural Questions: a benchmark for question answering research. Transactions of the Association of Computational Linguistics, 7:452-466, Mar. 2019. URL https: /aclanthology . org/Q19-1026.
A. Lazaridou, A. Kuncoro, E. Gribovskaya, D. Agrawal, A. Liska, T. Terzi, M. Gimenez, C. de Masson d'Autume, S. Ruder, D. Yogatama, K. Cao, T. Kociský, S. Young, and P. Blunsom. Pitfalls of static language modelling. CoRR, 2021. URL https://arxiv.org/abs/2102.01951.
K. Lee, M.-W. Chang, and K. Toutanova. Latent Retrieval for Weakly Supervised Open Domain Question Answering. In Annual Meeting of the Association for Computational Linguistic, June 2019. URL http://arxiv.org/abs/1906.00300.
K. Lee, D. Ippolito, A. Nystrom, C. Zhang, D. Eck, C. Callison-Burch, and N. Carlini. Deduplicating training data makes language models better. arXiv preprint arXiv:2107.06499, 2021.
P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. Küttler, M. Lewis, W.-t. Yih, T. Rocktäschel, S. Riedel, and D. Kiela. Retrieval-augmented generation for knowledge-intensive NLP tasks. In Advances in Neural Information Processing Systems, 2020. URL https://proceedings . neurips.cc/paper/2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf.
P. Lewis, P. Stenetorp, and S. Riedel. Question and answer test-train overlap in open-domain question answering datasets. In Conference of the European Chapter of the Association for Computational Linguistics, Apr. 2021. URL https://aclanthology.org/2021.eacl-main.86.
O. Lieber, O. Sharir, B. Lenz, and Y. Shoham. Jurassic-1: Technical details and evaluation. White Paper. AI21 Labs, 2021.
I. Loshchilov and F. Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=Bkg6RiCqY7.
S. Merity, C. Xiong, J. Bradbury, and R. Socher. Pointer sentinel mixture models. In International Conference on Learning Representations, 2017. URL https://openreview.net/forum?id= Byj72udxe.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{4}$ https://github.com/facebookresearch/FiD&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-695 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-695</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-695</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-18.html">extraction-schema-18</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <p><strong>Paper ID:</strong> paper-12b056fd90c1300c44c4a8cff9f01f0c7a75cab4</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/12b056fd90c1300c44c4a8cff9f01f0c7a75cab4" target="_blank">Ten Simple Rules for Reproducible Computational Research</a></p>
                <p><strong>Paper Venue:</strong> PLoS Comput. Biol.</p>
                <p><strong>Paper TL;DR:</strong> It is emphasized that reproducibility is not only a moral responsibility with respect to the scientific field, but that a lack of reproducible can also be a burden for you as an individual researcher.</p>
                <p><strong>Paper Abstract:</strong> Replication is the cornerstone of a cumulative science [1]. However, new tools and technologies, massive amounts of data, interdisciplinary approaches, and the complexity of the questions being asked are complicating replication efforts, as are increased pressures on scientists to advance their research [2]. As full replication of studies on independently collected data is often not feasible, there has recently been a call for reproducible research as an attainable minimum standard for assessing the value of scientific claims [3]. This requires that papers in experimental science describe the results and provide a sufficiently clear protocol to allow successful repetition and extension of analyses based on original data [4]. 
 
The importance of replication and reproducibility has recently been exemplified through studies showing that scientific papers commonly leave out experimental details essential for reproduction [5], studies showing difficulties with replicating published experimental results [6], an increase in retracted papers [7], and through a high number of failing clinical trials [8], [9]. This has led to discussions on how individual researchers, institutions, funding bodies, and journals can establish routines that increase transparency and reproducibility. In order to foster such aspects, it has been suggested that the scientific community needs to develop a “culture of reproducibility” for computational science, and to require it for published claims [3]. 
 
We want to emphasize that reproducibility is not only a moral responsibility with respect to the scientific field, but that a lack of reproducibility can also be a burden for you as an individual researcher. As an example, a good practice of reproducibility is necessary in order to allow previously developed methodology to be effectively applied on new data, or to allow reuse of code and results for new projects. In other words, good habits of reproducibility may actually turn out to be a time-saver in the longer run. 
 
We further note that reproducibility is just as much about the habits that ensure reproducible research as the technologies that can make these processes efficient and realistic. Each of the following ten rules captures a specific aspect of reproducibility, and discusses what is needed in terms of information handling and tracking of procedures. If you are taking a bare-bones approach to bioinformatics analysis, i.e., running various custom scripts from the command line, you will probably need to handle each rule explicitly. If you are instead performing your analyses through an integrated framework (such as GenePattern [10], Galaxy [11], LONI pipeline [12], or Taverna [13]), the system may already provide full or partial support for most of the rules. What is needed on your part is then merely the knowledge of how to exploit these existing possibilities. 
 
In a pragmatic setting, with publication pressure and deadlines, one may face the need to make a trade-off between the ideals of reproducibility and the need to get the research out while it is still relevant. This trade-off becomes more important when considering that a large part of the analyses being tried out never end up yielding any results. However, frequently one will, with the wisdom of hindsight, contemplate the missed opportunity to ensure reproducibility, as it may already be too late to take the necessary notes from memory (or at least much more difficult than to do it while underway). We believe that the rewards of reproducibility will compensate for the risk of having spent valuable time developing an annotated catalog of analyses that turned out as blind alleys. 
 
As a minimal requirement, you should at least be able to reproduce the results yourself. This would satisfy the most basic requirements of sound research, allowing any substantial future questioning of the research to be met with a precise explanation. Although it may sound like a very weak requirement, even this level of reproducibility will often require a certain level of care in order to be met. There will for a given analysis be an exponential number of possible combinations of software versions, parameter values, pre-processing steps, and so on, meaning that a failure to take notes may make exact reproduction essentially impossible. 
 
With this basic level of reproducibility in place, there is much more that can be wished for. An obvious extension is to go from a level where you can reproduce results in case of a critical situation to a level where you can practically and routinely reuse your previous work and increase your productivity. A second extension is to ensure that peers have a practical possibility of reproducing your results, which can lead to increased trust in, interest for, and citations of your work [6], [14]. 
 
We here present ten simple rules for reproducibility of computational research. These rules can be at your disposal for whenever you want to make your research more accessible—be it for peers or for your future self.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e695.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e695.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Documentation drift</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Documentation out of sync with actual analysis</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The phenomenon where written descriptions (protocols, methods sections, README) no longer match the actual code or executed workflow used to produce results, causing reproducibility failures and ambiguity about what was actually done.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Analysis workflow / computational research pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Interrelated sequence of steps (commands, scripts, programs) transforming raw data to final results; may be automated or manual and may be captured in papers' methods sections or separate documentation.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>research paper methods section / documentation</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>ad-hoc scripts, shell scripts, stored workflows</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>incomplete specification / documentation drift</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Textual descriptions (notes, methods) can fail to reflect later changes to scripts or execution order; consequently the documented protocol does not match the executed code, so a reader following the text will not reproduce the exact analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>end-to-end workflow specification (preprocessing, exact command sequence, parameter settings)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>qualitative observation and comparison between written documentation and executable workflow; authors recommend using executable descriptions to reveal mismatches</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>not quantified in this paper; detection described as manual comparison or discovering inability to rerun steps from text alone</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Can make exact reproduction essentially impossible, cast doubt on previous results, and require significant effort to backtrack and resolve discrepancies; no quantitative impact metrics provided</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Paper cites literature noting that many papers omit essential experimental details, but provides no numeric prevalence within this paper</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>manual updates to code without updating prose, implicit assumptions in text, ad-hoc changes during analysis not recorded</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>specify full analysis as executable descriptions (shell scripts, makefiles, stored workflows), use version control, archive exact program versions or VM images</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Not empirically measured in paper; authors assert executable workflows ensure the specification matches what was executed</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>bioinformatics / computational science</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Ten Simple Rules for Reproducible Computational Research', 'publication_date_yy_mm': '2013-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e695.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e695.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Unrecorded manual data manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Manual data manipulation steps not captured in documentation or code</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Manual edits and ad-hoc manipulations (copy-paste, hand edits to files) that are not captured in code or documentation, producing discrepancies between described procedures and what was actually run.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Data preprocessing stage of analysis pipelines</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Steps applied to raw data to make it compatible and analyzable; in some workflows these are automated, in others done manually.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>lab notes / methods descriptions / ad-hoc documentation</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>missing or absent implementation (manual steps), small custom scripts when present</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>missing preprocessing step / undocumented manual operation</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Operations performed manually are rarely reproducible from text alone; the absence of recorded commands leads to an implementation gap between what the paper says and what was done.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>data preprocessing and file-format conversion steps</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>practical inability to reproduce results from provided scripts, or discovery during attempts to automate workflow; authors recommend replacing manual steps with scripts</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>not quantified in paper; described qualitatively as error-prone and hard to reproduce</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Manual steps are error-prone and impede reproducibility; may produce subtle, hard-to-detect differences in downstream results</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Presented as common practice in pragmatic research settings; no numeric prevalence provided</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>convenience, time pressure, lack of automation or awareness</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>replace manual operations with standard UNIX commands, small scripts, or components in integrated frameworks; at minimum note what files were changed and why</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Not empirically evaluated in this paper; recommended as effective practice</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>bioinformatics / computational science</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Ten Simple Rules for Reproducible Computational Research', 'publication_date_yy_mm': '2013-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e695.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e695.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Software version mismatch</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mismatch or absence of exact external program versions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Differences between the versions of external programs documented in text and the versions actually used (or available later), causing inputs/outputs or behavior to diverge and preventing exact reproduction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Software environment for computational analyses</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Set of external programs, libraries, and operating system components required to run analysis code; versions may affect behavior and formats.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>methods documentation / program version listing in papers</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>executable binaries, installed packages, virtual machine images</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>version mismatch / incomplete specification of dependencies</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Authors note that even with noted program versions it may be hard to obtain the exact version later; changes between versions can alter input/output formats or behavior, so code that ran originally may fail or produce different results with newer versions.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>software environment and dependency management</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>failed attempts to rerun analysis with current software; recognition that formats/behavior changed between versions; recommendation to archive exact versions or VM images</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>no quantitative measurement in paper; detection described as practical incompatibility or inability to rerun without modification</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>May prevent exact reproduction and require reconstructing prior environment; can be a major barrier to reproducing published results</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Described as sufficiently common to motivate archiving of executables or VM images; no numeric prevalence provided</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>software evolution, dependency complexity, lack of preserved environment images</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>archive exact program binaries or source, record exact names and versions, or store a full virtual machine image capturing OS and dependencies</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Not systematically evaluated in the paper; authors argue archiving versions/VMs effectively preserves reproducibility</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>bioinformatics / computational science</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Ten Simple Rules for Reproducible Computational Research', 'publication_date_yy_mm': '2013-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e695.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e695.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Unversioned scripts</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Lack of systematic version control for custom scripts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Failure to track the development history of scripts used to produce results, making it impossible to retrieve the exact state of code that generated an output.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Custom analysis scripts and small programs</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Ad-hoc scripts written during development to perform parts of analyses; often evolve rapidly during a project.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>developer notes / methods description</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>local scripts not under version control</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>incomplete specification / missing version history</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Without systematic archival (version control), a specific script state that produced published results may be irretrievable, obscuring whether results stem from intended logic or bugs introduced during development.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>implementation code / scripts</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>retrospective inability to find or reconstruct the script state that produced a given result; authors recommend version control systems</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>not quantified in paper; described qualitatively</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Can cast doubt on previous results and make backtracking to diagnose bugs hopeless</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Implied to be widespread among researchers using ad-hoc scripts; no numeric figures provided</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>lack of software-engineering practices, time pressure, unfamiliarity with version-control tools</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>use version control systems (Subversion, Git, Mercurial) and systematically store code states</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Not empirically measured here; recommended as a standard, low-cost solution</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>bioinformatics / computational science</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Ten Simple Rules for Reproducible Computational Research', 'publication_date_yy_mm': '2013-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e695.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e695.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Missing intermediate outputs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Lack of recorded intermediate results or non-standard formats</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Failure to store intermediate data products in accessible, standardized formats, which hinders inspection, debugging, and partial reruns of the analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Pipeline outputs and intermediate data storage</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Intermediate files produced at steps between raw input and final summary results; may include binned values, transformed datasets, or debug outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>results description in text / figure captions</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>pipeline outputs (files), storage formats</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>missing intermediate results / non-standardized formats</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>When intermediate results are not archived or are stored in ad-hoc formats, it becomes hard to trace inconsistencies or to re-run parts of the analysis; lack of intermediates hides where discrepancies originate.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>intermediate data generation and storage</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>attempts to debug or validate analyses by inspecting intermediate values; authors note that quick browsing of intermediates reveals discrepancies</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>not quantified in paper; advocated as a qualitative debugging and validation aid</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Hinders detection of bugs and understanding of how parameter/program choices affect outcomes; reduces ability to validate and extend analyses</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Paper frames archiving intermediates as good practice; prevalence of omission is implied but not quantified</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>storage constraints, lack of standardized formats, and ad-hoc workflows</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>record intermediate results when possible in standardized formats, generate hierarchical outputs with links to detailed data (e.g., HTML), and archive intermediate files</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Not quantitatively evaluated in paper; authors argue it aids debugging and validation</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>bioinformatics / computational science</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Ten Simple Rules for Reproducible Computational Research', 'publication_date_yy_mm': '2013-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e695.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e695.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Unrecorded randomness</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Analyses involving randomness without recording random seeds</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>When stochastic elements are used in analyses but random seeds are not recorded, repeated runs produce only approximately equal results rather than bitwise-equal reproductions, making it hard to claim exact reproducibility.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Stochastic analysis components (simulations, randomized algorithms)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Analysis steps that draw on random number generators and therefore produce non-deterministic outputs unless a seed is fixed.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>methods descriptions / algorithmic descriptions</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>analysis scripts and programs that use RNGs</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>missing randomness specification (unrecorded seeds)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Without recording the underlying random seed(s), repeated execution yields different results; the distinction between exact and approximate reproduction is important but often not documented.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>stochastic steps within the workflow / algorithmic randomness</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>observing non-identical outputs across runs or failing to achieve bitwise-equal reproduction; authors recommend recording seeds</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>paper describes qualitative difference between identical and approximate reproduction; no numeric measurement of variability provided</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Prevents exact reproduction; complicates debugging and verification of results since variability may mask bugs or differences</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Not quantified; authors treat it as a common consideration in analyses involving randomness</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>not recording seeds or noting which steps involve randomness, and lack of awareness of the distinction</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>record random seeds for steps involving randomness and document which steps are stochastic</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Not empirically measured in paper; conceptually allows exact reproduction when seeds are reused</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>bioinformatics / computational science</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Ten Simple Rules for Reproducible Computational Research', 'publication_date_yy_mm': '2013-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e695.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e695.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Disconnected text-results linkage</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Lack of explicit links between textual statements and the exact underlying results</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Textual interpretations, claims, or conclusions are not directly connected to the precise result files or code states that support them, making independent verification and re-evaluation difficult.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Scientific publication text and analysis result repositories</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Separation between narrative text (notes, manuscript) and raw/processed results stored on servers or local machines.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>manuscript text / interpretation notes</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>result files, analysis outputs, and identifiers in analysis frameworks</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>ambiguous description / missing traceability links</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Interpretations often live separately from data and code; without embedded references (file paths, IDs) or tools integrating code and text (Sweave, Galaxy Pages), locating the exact supporting result is error-prone.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>linkage between manuscript text and result artifacts</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>difficulty in locating the exact result underlying a statement during reevaluation or peer review; authors recommend connecting statements to result IDs or file paths at creation time</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>not quantified; described as a practical retrieval problem</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Reduces transparency and makes verification or reinterpretation laborious; may slow review and decrease trust</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Presented as a common organizational problem in projects; no statistics provided</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>separate storage of text and results, ad-hoc project organization, and lack of tooling integration</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>embed references to exact results in text (file paths, result IDs), use literate programming tools (Sweave) or integrated platforms (GenePattern Word add-in, Galaxy Pages) to link narrative and outputs</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Not evaluated quantitatively here; authors argue tight integration facilitates retrieval and verification</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>bioinformatics / computational science</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Ten Simple Rules for Reproducible Computational Research', 'publication_date_yy_mm': '2013-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e695.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e695.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Unavailable code and results</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Lack of public access to scripts, runs, and intermediate results</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>When the code, parameters, and input/output artifacts underlying published results are not publicly available, readers cannot reproduce or validate claims, increasing opacity and impeding reuse.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Publication supplemental materials and public repositories for code/data</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Mechanisms and locations where authors can publish scripts, data, and execution artifacts to support reproducibility.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>paper claims and supplementary material statements</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>source code, scripts, intermediate result files, supplementary archives</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>incomplete specification / missing supplementary artifacts</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Papers that do not make code and intermediate results available prevent peers from rerunning analyses; lack of public access is a practical misalignment between the claim that an analysis is reproducible and the reality that artifacts are not accessible.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>publication/supplementary material and data/code repositories</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>attempts to reproduce reveal missing artifacts; authors state journals allow supplementary material but authors should proactively provide scripts and data</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>no quantitative measurement in paper; prevalence of lack of sharing is discussed qualitatively and supported by literature references</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Reduces ability of peers to independently reproduce and extend work; may slow review and reduce trust and citation benefits</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Paper cites domain-specific data-sharing initiatives and argues sharing is not yet universal; no numeric prevalence reported</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>culture, lack of incentives or awareness, concerns about data sharing</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>submit main data and source code as supplementary material, use domain repositories (e.g., GEO, ArrayExpress), and be ready to provide further materials on request</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Not quantitatively assessed in this paper; authors cite literature linking data sharing to increased citations</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>bioinformatics / computational science</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Ten Simple Rules for Reproducible Computational Research', 'publication_date_yy_mm': '2013-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Next-generation sequencing data interpretation: enhancing reproducibility and accessibility <em>(Rating: 2)</em></li>
                <li>Repeatability of published microarray gene expression analyses <em>(Rating: 2)</em></li>
                <li>Making scientific computations reproducible <em>(Rating: 2)</em></li>
                <li>Reproducible research in computational science <em>(Rating: 2)</em></li>
                <li>Sweave: dynamic generation of statistical reports using literate data analysis <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-695",
    "paper_id": "paper-12b056fd90c1300c44c4a8cff9f01f0c7a75cab4",
    "extraction_schema_id": "extraction-schema-18",
    "extracted_data": [
        {
            "name_short": "Documentation drift",
            "name_full": "Documentation out of sync with actual analysis",
            "brief_description": "The phenomenon where written descriptions (protocols, methods sections, README) no longer match the actual code or executed workflow used to produce results, causing reproducibility failures and ambiguity about what was actually done.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "system_name": "Analysis workflow / computational research pipeline",
            "system_description": "Interrelated sequence of steps (commands, scripts, programs) transforming raw data to final results; may be automated or manual and may be captured in papers' methods sections or separate documentation.",
            "nl_description_type": "research paper methods section / documentation",
            "code_implementation_type": "ad-hoc scripts, shell scripts, stored workflows",
            "gap_type": "incomplete specification / documentation drift",
            "gap_description": "Textual descriptions (notes, methods) can fail to reflect later changes to scripts or execution order; consequently the documented protocol does not match the executed code, so a reader following the text will not reproduce the exact analysis.",
            "gap_location": "end-to-end workflow specification (preprocessing, exact command sequence, parameter settings)",
            "detection_method": "qualitative observation and comparison between written documentation and executable workflow; authors recommend using executable descriptions to reveal mismatches",
            "measurement_method": "not quantified in this paper; detection described as manual comparison or discovering inability to rerun steps from text alone",
            "impact_on_results": "Can make exact reproduction essentially impossible, cast doubt on previous results, and require significant effort to backtrack and resolve discrepancies; no quantitative impact metrics provided",
            "frequency_or_prevalence": "Paper cites literature noting that many papers omit essential experimental details, but provides no numeric prevalence within this paper",
            "root_cause": "manual updates to code without updating prose, implicit assumptions in text, ad-hoc changes during analysis not recorded",
            "mitigation_approach": "specify full analysis as executable descriptions (shell scripts, makefiles, stored workflows), use version control, archive exact program versions or VM images",
            "mitigation_effectiveness": "Not empirically measured in paper; authors assert executable workflows ensure the specification matches what was executed",
            "domain_or_field": "bioinformatics / computational science",
            "reproducibility_impact": true,
            "uuid": "e695.0",
            "source_info": {
                "paper_title": "Ten Simple Rules for Reproducible Computational Research",
                "publication_date_yy_mm": "2013-10"
            }
        },
        {
            "name_short": "Unrecorded manual data manipulation",
            "name_full": "Manual data manipulation steps not captured in documentation or code",
            "brief_description": "Manual edits and ad-hoc manipulations (copy-paste, hand edits to files) that are not captured in code or documentation, producing discrepancies between described procedures and what was actually run.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "system_name": "Data preprocessing stage of analysis pipelines",
            "system_description": "Steps applied to raw data to make it compatible and analyzable; in some workflows these are automated, in others done manually.",
            "nl_description_type": "lab notes / methods descriptions / ad-hoc documentation",
            "code_implementation_type": "missing or absent implementation (manual steps), small custom scripts when present",
            "gap_type": "missing preprocessing step / undocumented manual operation",
            "gap_description": "Operations performed manually are rarely reproducible from text alone; the absence of recorded commands leads to an implementation gap between what the paper says and what was done.",
            "gap_location": "data preprocessing and file-format conversion steps",
            "detection_method": "practical inability to reproduce results from provided scripts, or discovery during attempts to automate workflow; authors recommend replacing manual steps with scripts",
            "measurement_method": "not quantified in paper; described qualitatively as error-prone and hard to reproduce",
            "impact_on_results": "Manual steps are error-prone and impede reproducibility; may produce subtle, hard-to-detect differences in downstream results",
            "frequency_or_prevalence": "Presented as common practice in pragmatic research settings; no numeric prevalence provided",
            "root_cause": "convenience, time pressure, lack of automation or awareness",
            "mitigation_approach": "replace manual operations with standard UNIX commands, small scripts, or components in integrated frameworks; at minimum note what files were changed and why",
            "mitigation_effectiveness": "Not empirically evaluated in this paper; recommended as effective practice",
            "domain_or_field": "bioinformatics / computational science",
            "reproducibility_impact": true,
            "uuid": "e695.1",
            "source_info": {
                "paper_title": "Ten Simple Rules for Reproducible Computational Research",
                "publication_date_yy_mm": "2013-10"
            }
        },
        {
            "name_short": "Software version mismatch",
            "name_full": "Mismatch or absence of exact external program versions",
            "brief_description": "Differences between the versions of external programs documented in text and the versions actually used (or available later), causing inputs/outputs or behavior to diverge and preventing exact reproduction.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "system_name": "Software environment for computational analyses",
            "system_description": "Set of external programs, libraries, and operating system components required to run analysis code; versions may affect behavior and formats.",
            "nl_description_type": "methods documentation / program version listing in papers",
            "code_implementation_type": "executable binaries, installed packages, virtual machine images",
            "gap_type": "version mismatch / incomplete specification of dependencies",
            "gap_description": "Authors note that even with noted program versions it may be hard to obtain the exact version later; changes between versions can alter input/output formats or behavior, so code that ran originally may fail or produce different results with newer versions.",
            "gap_location": "software environment and dependency management",
            "detection_method": "failed attempts to rerun analysis with current software; recognition that formats/behavior changed between versions; recommendation to archive exact versions or VM images",
            "measurement_method": "no quantitative measurement in paper; detection described as practical incompatibility or inability to rerun without modification",
            "impact_on_results": "May prevent exact reproduction and require reconstructing prior environment; can be a major barrier to reproducing published results",
            "frequency_or_prevalence": "Described as sufficiently common to motivate archiving of executables or VM images; no numeric prevalence provided",
            "root_cause": "software evolution, dependency complexity, lack of preserved environment images",
            "mitigation_approach": "archive exact program binaries or source, record exact names and versions, or store a full virtual machine image capturing OS and dependencies",
            "mitigation_effectiveness": "Not systematically evaluated in the paper; authors argue archiving versions/VMs effectively preserves reproducibility",
            "domain_or_field": "bioinformatics / computational science",
            "reproducibility_impact": true,
            "uuid": "e695.2",
            "source_info": {
                "paper_title": "Ten Simple Rules for Reproducible Computational Research",
                "publication_date_yy_mm": "2013-10"
            }
        },
        {
            "name_short": "Unversioned scripts",
            "name_full": "Lack of systematic version control for custom scripts",
            "brief_description": "Failure to track the development history of scripts used to produce results, making it impossible to retrieve the exact state of code that generated an output.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "system_name": "Custom analysis scripts and small programs",
            "system_description": "Ad-hoc scripts written during development to perform parts of analyses; often evolve rapidly during a project.",
            "nl_description_type": "developer notes / methods description",
            "code_implementation_type": "local scripts not under version control",
            "gap_type": "incomplete specification / missing version history",
            "gap_description": "Without systematic archival (version control), a specific script state that produced published results may be irretrievable, obscuring whether results stem from intended logic or bugs introduced during development.",
            "gap_location": "implementation code / scripts",
            "detection_method": "retrospective inability to find or reconstruct the script state that produced a given result; authors recommend version control systems",
            "measurement_method": "not quantified in paper; described qualitatively",
            "impact_on_results": "Can cast doubt on previous results and make backtracking to diagnose bugs hopeless",
            "frequency_or_prevalence": "Implied to be widespread among researchers using ad-hoc scripts; no numeric figures provided",
            "root_cause": "lack of software-engineering practices, time pressure, unfamiliarity with version-control tools",
            "mitigation_approach": "use version control systems (Subversion, Git, Mercurial) and systematically store code states",
            "mitigation_effectiveness": "Not empirically measured here; recommended as a standard, low-cost solution",
            "domain_or_field": "bioinformatics / computational science",
            "reproducibility_impact": true,
            "uuid": "e695.3",
            "source_info": {
                "paper_title": "Ten Simple Rules for Reproducible Computational Research",
                "publication_date_yy_mm": "2013-10"
            }
        },
        {
            "name_short": "Missing intermediate outputs",
            "name_full": "Lack of recorded intermediate results or non-standard formats",
            "brief_description": "Failure to store intermediate data products in accessible, standardized formats, which hinders inspection, debugging, and partial reruns of the analysis.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "system_name": "Pipeline outputs and intermediate data storage",
            "system_description": "Intermediate files produced at steps between raw input and final summary results; may include binned values, transformed datasets, or debug outputs.",
            "nl_description_type": "results description in text / figure captions",
            "code_implementation_type": "pipeline outputs (files), storage formats",
            "gap_type": "missing intermediate results / non-standardized formats",
            "gap_description": "When intermediate results are not archived or are stored in ad-hoc formats, it becomes hard to trace inconsistencies or to re-run parts of the analysis; lack of intermediates hides where discrepancies originate.",
            "gap_location": "intermediate data generation and storage",
            "detection_method": "attempts to debug or validate analyses by inspecting intermediate values; authors note that quick browsing of intermediates reveals discrepancies",
            "measurement_method": "not quantified in paper; advocated as a qualitative debugging and validation aid",
            "impact_on_results": "Hinders detection of bugs and understanding of how parameter/program choices affect outcomes; reduces ability to validate and extend analyses",
            "frequency_or_prevalence": "Paper frames archiving intermediates as good practice; prevalence of omission is implied but not quantified",
            "root_cause": "storage constraints, lack of standardized formats, and ad-hoc workflows",
            "mitigation_approach": "record intermediate results when possible in standardized formats, generate hierarchical outputs with links to detailed data (e.g., HTML), and archive intermediate files",
            "mitigation_effectiveness": "Not quantitatively evaluated in paper; authors argue it aids debugging and validation",
            "domain_or_field": "bioinformatics / computational science",
            "reproducibility_impact": true,
            "uuid": "e695.4",
            "source_info": {
                "paper_title": "Ten Simple Rules for Reproducible Computational Research",
                "publication_date_yy_mm": "2013-10"
            }
        },
        {
            "name_short": "Unrecorded randomness",
            "name_full": "Analyses involving randomness without recording random seeds",
            "brief_description": "When stochastic elements are used in analyses but random seeds are not recorded, repeated runs produce only approximately equal results rather than bitwise-equal reproductions, making it hard to claim exact reproducibility.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "system_name": "Stochastic analysis components (simulations, randomized algorithms)",
            "system_description": "Analysis steps that draw on random number generators and therefore produce non-deterministic outputs unless a seed is fixed.",
            "nl_description_type": "methods descriptions / algorithmic descriptions",
            "code_implementation_type": "analysis scripts and programs that use RNGs",
            "gap_type": "missing randomness specification (unrecorded seeds)",
            "gap_description": "Without recording the underlying random seed(s), repeated execution yields different results; the distinction between exact and approximate reproduction is important but often not documented.",
            "gap_location": "stochastic steps within the workflow / algorithmic randomness",
            "detection_method": "observing non-identical outputs across runs or failing to achieve bitwise-equal reproduction; authors recommend recording seeds",
            "measurement_method": "paper describes qualitative difference between identical and approximate reproduction; no numeric measurement of variability provided",
            "impact_on_results": "Prevents exact reproduction; complicates debugging and verification of results since variability may mask bugs or differences",
            "frequency_or_prevalence": "Not quantified; authors treat it as a common consideration in analyses involving randomness",
            "root_cause": "not recording seeds or noting which steps involve randomness, and lack of awareness of the distinction",
            "mitigation_approach": "record random seeds for steps involving randomness and document which steps are stochastic",
            "mitigation_effectiveness": "Not empirically measured in paper; conceptually allows exact reproduction when seeds are reused",
            "domain_or_field": "bioinformatics / computational science",
            "reproducibility_impact": true,
            "uuid": "e695.5",
            "source_info": {
                "paper_title": "Ten Simple Rules for Reproducible Computational Research",
                "publication_date_yy_mm": "2013-10"
            }
        },
        {
            "name_short": "Disconnected text-results linkage",
            "name_full": "Lack of explicit links between textual statements and the exact underlying results",
            "brief_description": "Textual interpretations, claims, or conclusions are not directly connected to the precise result files or code states that support them, making independent verification and re-evaluation difficult.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "system_name": "Scientific publication text and analysis result repositories",
            "system_description": "Separation between narrative text (notes, manuscript) and raw/processed results stored on servers or local machines.",
            "nl_description_type": "manuscript text / interpretation notes",
            "code_implementation_type": "result files, analysis outputs, and identifiers in analysis frameworks",
            "gap_type": "ambiguous description / missing traceability links",
            "gap_description": "Interpretations often live separately from data and code; without embedded references (file paths, IDs) or tools integrating code and text (Sweave, Galaxy Pages), locating the exact supporting result is error-prone.",
            "gap_location": "linkage between manuscript text and result artifacts",
            "detection_method": "difficulty in locating the exact result underlying a statement during reevaluation or peer review; authors recommend connecting statements to result IDs or file paths at creation time",
            "measurement_method": "not quantified; described as a practical retrieval problem",
            "impact_on_results": "Reduces transparency and makes verification or reinterpretation laborious; may slow review and decrease trust",
            "frequency_or_prevalence": "Presented as a common organizational problem in projects; no statistics provided",
            "root_cause": "separate storage of text and results, ad-hoc project organization, and lack of tooling integration",
            "mitigation_approach": "embed references to exact results in text (file paths, result IDs), use literate programming tools (Sweave) or integrated platforms (GenePattern Word add-in, Galaxy Pages) to link narrative and outputs",
            "mitigation_effectiveness": "Not evaluated quantitatively here; authors argue tight integration facilitates retrieval and verification",
            "domain_or_field": "bioinformatics / computational science",
            "reproducibility_impact": true,
            "uuid": "e695.6",
            "source_info": {
                "paper_title": "Ten Simple Rules for Reproducible Computational Research",
                "publication_date_yy_mm": "2013-10"
            }
        },
        {
            "name_short": "Unavailable code and results",
            "name_full": "Lack of public access to scripts, runs, and intermediate results",
            "brief_description": "When the code, parameters, and input/output artifacts underlying published results are not publicly available, readers cannot reproduce or validate claims, increasing opacity and impeding reuse.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "system_name": "Publication supplemental materials and public repositories for code/data",
            "system_description": "Mechanisms and locations where authors can publish scripts, data, and execution artifacts to support reproducibility.",
            "nl_description_type": "paper claims and supplementary material statements",
            "code_implementation_type": "source code, scripts, intermediate result files, supplementary archives",
            "gap_type": "incomplete specification / missing supplementary artifacts",
            "gap_description": "Papers that do not make code and intermediate results available prevent peers from rerunning analyses; lack of public access is a practical misalignment between the claim that an analysis is reproducible and the reality that artifacts are not accessible.",
            "gap_location": "publication/supplementary material and data/code repositories",
            "detection_method": "attempts to reproduce reveal missing artifacts; authors state journals allow supplementary material but authors should proactively provide scripts and data",
            "measurement_method": "no quantitative measurement in paper; prevalence of lack of sharing is discussed qualitatively and supported by literature references",
            "impact_on_results": "Reduces ability of peers to independently reproduce and extend work; may slow review and reduce trust and citation benefits",
            "frequency_or_prevalence": "Paper cites domain-specific data-sharing initiatives and argues sharing is not yet universal; no numeric prevalence reported",
            "root_cause": "culture, lack of incentives or awareness, concerns about data sharing",
            "mitigation_approach": "submit main data and source code as supplementary material, use domain repositories (e.g., GEO, ArrayExpress), and be ready to provide further materials on request",
            "mitigation_effectiveness": "Not quantitatively assessed in this paper; authors cite literature linking data sharing to increased citations",
            "domain_or_field": "bioinformatics / computational science",
            "reproducibility_impact": true,
            "uuid": "e695.7",
            "source_info": {
                "paper_title": "Ten Simple Rules for Reproducible Computational Research",
                "publication_date_yy_mm": "2013-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Next-generation sequencing data interpretation: enhancing reproducibility and accessibility",
            "rating": 2
        },
        {
            "paper_title": "Repeatability of published microarray gene expression analyses",
            "rating": 2
        },
        {
            "paper_title": "Making scientific computations reproducible",
            "rating": 2
        },
        {
            "paper_title": "Reproducible research in computational science",
            "rating": 2
        },
        {
            "paper_title": "Sweave: dynamic generation of statistical reports using literate data analysis",
            "rating": 1
        }
    ],
    "cost": 0.012846999999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Ten Simple Rules for Reproducible Computational Research</h1>
<p>Geir Kjetil Sandve ${ }^{1,2 *}$, Anton Nekrutenko ${ }^{3}$, James Taylor ${ }^{4}$, Eivind Hovig ${ }^{1,5,6}$<br>1 Department of Informatics, University of Oslo, Blindern, Oslo, Norway, 2 Centre for Cancer Biomedicine, University of Oslo, Blindern, Oslo, Norway, 3 Department of Biochemistry and Molecular Biology and The Huck Institutes for the Life Sciences, Penn State University, University Park, Pennsylvania, United States of America, 4 Department of Biology and Department of Mathematics and Computer Science, Emory University, Atlanta, Georgia, United States of America, 5 Department of Tumor Biology, Institute for Cancer Research, The Norwegian Radium Hospital, Oslo University Hospital, Montebello, Oslo, Norway, 6 Institute for Medical Informatics, The Norwegian Radium Hospital, Oslo University Hospital, Montebello, Oslo, Norway</p>
<p>Replication is the cornerstone of a cumulative science [1]. However, new tools and technologies, massive amounts of data, interdisciplinary approaches, and the complexity of the questions being asked are complicating replication efforts, as are increased pressures on scientists to advance their research [2]. As full replication of studies on independently collected data is often not feasible, there has recently been a call for reproducible research as an attainable minimum standard for assessing the value of scientific claims [3]. This requires that papers in experimental science describe the results and provide a sufficiently clear protocol to allow successful repetition and extension of analyses based on original data [4].</p>
<p>The importance of replication and reproducibility has recently been exemplified through studies showing that scientific papers commonly leave out experimental details essential for reproduction [5], studies showing difficulties with replicating published experimental results [6], an increase in retracted papers [7], and through a high number of failing clinical trials $[8,9]$. This has led to discussions on how individual researchers, institutions, funding bodies, and journals can establish routines that increase transparency and reproducibility. In order to foster such aspects, it has been suggested that the scientific community needs to develop a "culture of reproducibility" for computational science, and to require it for published claims [3].</p>
<p>We want to emphasize that reproducibility is not only a moral responsibility with respect to the scientific field, but that a lack of reproducibility can also be a burden for you as an individual researcher. As an example, a good practice of reproducibility is necessary in order to allow previously developed methodology to be effectively applied on new data, or to allow reuse of code and results for new projects. In other words, good habits of reproducibility may actually turn out to be a time-saver in the longer run.</p>
<p>We further note that reproducibility is just as much about the habits that ensure reproducible research as the technologies that can make these processes efficient and realistic. Each of the following ten rules captures a specific aspect of reproducibility, and discusses what is needed in terms of information handling and tracking of procedures. If you are taking a bare-bones approach to bioinformatics analysis, i.e., running various custom scripts from the command line, you will probably need to handle each rule explicitly. If you are instead performing your analyses through an integrated framework (such as GenePattern [10], Galaxy [11], LONI pipeline [12], or Taverna [13]), the system may already provide full or partial support for most of the rules. What is needed on your part is then merely the knowledge of how to exploit these existing possibilities.</p>
<p>In a pragmatic setting, with publication pressure and deadlines, one may face the need to make a trade-off between the ideals of reproducibility and the need to get the research out while it is still relevant. This trade-off becomes more important when considering that a large part of the analyses being tried out never end up yielding any results. However, frequently one will, with the wisdom of hindsight, contemplate the missed opportunity to ensure reproducibility, as it may already be too late to take the necessary notes from memory (or at least much more difficult
than to do it while underway). We believe that the rewards of reproducibility will compensate for the risk of having spent valuable time developing an annotated catalog of analyses that turned out as blind alleys.</p>
<p>As a minimal requirement, you should at least be able to reproduce the results yourself. This would satisfy the most basic requirements of sound research, allowing any substantial future questioning of the research to be met with a precise explanation. Although it may sound like a very weak requirement, even this level of reproducibility will often require a certain level of care in order to be met. There will for a given analysis be an exponential number of possible combinations of software versions, parameter values, preprocessing steps, and so on, meaning that a failure to take notes may make exact reproduction essentially impossible.</p>
<p>With this basic level of reproducibility in place, there is much more that can be wished for. An obvious extension is to go from a level where you can reproduce results in case of a critical situation to a level where you can practically and routinely reuse your previous work and increase your productivity. A second extension is to ensure that peers have a practical possibility of reproducing your results, which can lead to increased trust in, interest for, and citations of your work $[6,14]$.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>We here present ten simple rules for reproducibility of computational research. These rules can be at your disposal for whenever you want to make your research more accessible - be it for peers or for your future self.</p>
<h2>Rule 1: For Every Result, Keep Track of How It Was Produced</h2>
<p>Whenever a result may be of potential interest, keep track of how it was produced. When doing this, one will frequently find that getting from raw data to the final result involves many interrelated steps (single commands, scripts, programs). We refer to such a sequence of steps, whether it is automated or performed manually, as an analysis workflow. While the essential part of an analysis is often represented by only one of the steps, the full sequence of pre- and post-processing steps are often critical in order to reach the achieved result. For every involved step, you should ensure that every detail that may influence the execution of the step is recorded. If the step is performed by a computer program, the critical details include the name and version of the program, as well as the exact parameters and inputs that were used.</p>
<p>Although manually noting the precise sequence of steps taken allows for an analysis to be reproduced, the documentation can easily get out of sync with how the analysis was really performed in its final version. By instead specifying the full analysis workflow in a form that allows for direct execution, one can ensure that the specification matches the analysis that was (subsequently) performed, and that the analysis can be reproduced by yourself or others in an automated way. Such executable descriptions [10] might come in the form of simple shell scripts or makefiles $[15,16]$ at the command line, or in the form of stored workflows in a workflow management system $[10,11,13,17,18]$.</p>
<p>As a minimum, you should at least record sufficient details on programs, parameters, and manual procedures to allow yourself, in a year or so, to approximately reproduce the results.</p>
<h2>Rule 2: Avoid Manual Data Manipulation Steps</h2>
<p>Whenever possible, rely on the execution of programs instead of manual procedures to modify data. Such manual procedures are not only inefficient and error-prone, they are also difficult to reproduce. If working at the UNIX command line, manual modification of
files can usually be replaced by the use of standard UNIX commands or small custom scripts. If working with integrated frameworks, there will typically be a quite rich collection of components for data manipulation. As an example, manual tweaking of data files to attain format compatibility should be replaced by format converters that can be reenacted and included into executable workflows. Other manual operations like the use of copy and paste between documents should also be avoided. If manual operations cannot be avoided, you should as a minimum note down which data files were modified or moved, and for what purpose.</p>
<h2>Rule 3: Archive the Exact Versions of All External Programs Used</h2>
<p>In order to exactly reproduce a given result, it may be necessary to use programs in the exact versions used originally. Also, as both input and output formats may change between versions, a newer version of a program may not even run without modifying its inputs. Even having noted which version was used of a given program, it is not always trivial to get hold of a program in anything but the current version. Archiving the exact versions of programs actually used may thus save a lot of hassle at later stages. In some cases, all that is needed is to store a single executable or source code file. In other cases, a given program may again have specific requirements to other installed programs/packages, or dependencies to specific operating system components. To ensure future availability, the only viable solution may then be to store a full virtual machine image of the operating system and program. As a minimum, you should note the exact names and versions of the main programs you use.</p>
<h2>Rule 4: Version Control All Custom Scripts</h2>
<p>Even the slightest change to a computer program can have large intended or unintended consequences. When a continually developed piece of code (typically a small script) has been used to generate a certain result, only that exact state of the script may be able to produce that exact output, even given the same input data and parameters. As also discussed for rules 3 and 6 , exact reproduction of results may in certain situations be essential. If computer code is not systematically archived along its evolution, backtracking to a code state that gave a certain result may be a
hopeless task. This can cast doubt on previous results, as it may be impossible to know if they were partly the result of a bug or otherwise unfortunate behavior.</p>
<p>The standard solution to track evolution of code is to use a version control system [15], such as Subversion, Git, or Mercurial. These systems are relatively easy to set up and use, and may be used to systematically store the state of the code throughout development at any desired time granularity.</p>
<p>As a minimum, you should archive copies of your scripts from time to time, so that you keep a rough record of the various states the code has taken during development.</p>
<h2>Rule 5: Record All Intermediate Results, When Possible in Standardized Formats</h2>
<p>In principle, as long as the full process used to produce a given result is tracked, all intermediate data can also be regenerated. In practice, having easily accessible intermediate results may be of great value. Quickly browsing through intermediate results can reveal discrepancies toward what is assumed, and can in this way uncover bugs or faulty interpretations that are not apparent in the final results. Secondly, it more directly reveals consequences of alternative programs and parameter choices at individual steps. Thirdly, when the full process is not readily executable, it allows parts of the process to be rerun. Fourthly, when reproducing results, it allows any experienced inconsistencies to be tracked to the steps where the problems arise. Fifth, it allows critical examination of the full process behind a result, without the need to have all executables operational. When possible, store such intermediate results in standardized formats. As a minimum, archive any intermediate result files that are produced when running an analysis (as long as the required storage space is not prohibitive).</p>
<h2>Rule 6: For Analyses That Include Randomness, Note Underlying Random Seeds</h2>
<p>Many analyses and predictions include some element of randomness, meaning the same program will typically give slightly different results every time it is executed (even when receiving identical inputs and parameters). However, given the same initial seed, all random numbers used in an analysis will be equal, thus giving identical results every time it is run. There</p>
<p>is a large difference between observing that a result has been reproduced exactly or only approximately. While achieving equal results is a strong indication that a procedure has been reproduced exactly, it is often hard to conclude anything when achieving only approximately equal results. For analyses that involve random numbers, this means that the random seed should be recorded. This allows results to be reproduced exactly by providing the same seed to the random number generator in future runs. As a minimum, you should note which analysis steps involve randomness, so that a certain level of discrepancy can be anticipated when reproducing the results.</p>
<h2>Rule 7: Always Store Raw Data behind Plots</h2>
<p>From the time a figure is first generated to it being part of a published article, it is often modified several times. In some cases, such modifications are merely visual adjustments to improve readability, or to ensure visual consistency between figures. If raw data behind figures are stored in a systematic manner, so as to allow raw data for a given figure to be easily retrieved, one can simply modify the plotting procedure, instead of having to redo the whole analysis. An additional advantage of this is that if one really wants to read fine values in a figure, one can consult the raw numbers. In cases where plotting involves more than a direct visualization of underlying numbers, it can be useful to store both the underlying data and the processed values that are directly visualized. An example of this is the plotting of histograms, where both the values before binning (original data) and the counts per bin (heights of visualized bars) could be stored. When plotting is performed using a command-based system like R, it is convenient to also store the code used to make the plot. One can then apply slight modifications to these commands, instead of having to specify the plot from scratch. As a minimum, one should note which data formed the basis of a given plot and how this data could be reconstructed.</p>
<h2>Rule 8: Generate Hierarchical Analysis Output, Allowing Layers of Increasing Detail to Be Inspected</h2>
<p>The final results that make it to an article, be it plots or tables, often represent
highly summarized data. For instance, each value along a curve may in turn represent averages from an underlying distribution. In order to validate and fully understand the main result, it is often useful to inspect the detailed values underlying the summaries. A common but impractical way of doing this is to incorporate various debug outputs in the source code of scripts and programs. When the storage context allows, it is better to simply incorporate permanent output of all underlying data when a main result is generated, using a systematic naming convention to allow the full data underlying a given summarized value to be easily found. We find hypertext (i.e., html file output) to be particularly useful for this purpose. This allows summarized results to be generated along with links that can be very conveniently followed (by simply clicking) to the full data underlying each summarized value. When working with summarized results, you should as a minimum at least once generate, inspect, and validate the detailed values underlying the summaries.</p>
<h2>Rule 9: Connect Textual Statements to Underlying Results</h2>
<p>Throughout a typical research project, a range of different analyses are tried and interpretation of the results made. Although the results of analyses and their corresponding textual interpretations are clearly interconnected at the conceptual level, they tend to live quite separate lives in their representations: results usually live on a data area on a server or personal computer, while interpretations live in text documents in the form of personal notes or emails to collaborators. Such textual interpretations are not generally mere shadows of the resultsthey often involve viewing the results in light of other theories and results. As such, they carry extra information, while at the same time having their necessary support in a given result.</p>
<p>If you want to reevaluate your previous interpretations, or allow peers to make their own assessment of claims you make in a scientific paper, you will have to connect a given textual statement (interpretation, claim, conclusion) to the precise results underlying the statement. Making this connection when it is needed may be difficult and error-prone,
as it may be hard to locate the exact result underlying and supporting the statement from a large pool of different analyses with various versions.</p>
<p>To allow efficient retrieval of details behind textual statements, we suggest that statements are connected to underlying results already from the time the statements are initially formulated (for instance in notes or emails). Such a connection can for instance be a simple file path to detailed results, or the ID of a result in an analysis framework, included within the text itself. For an even tighter integration, there are tools available to help integrate reproducible analyses directly into textual documents, such as Sweave [19], the GenePattern Word add-in [4], and Galaxy Pages [20]. These solutions can also subsequently be used in connection with publications, as discussed in the next rule.</p>
<p>As a minimum, you should provide enough details along with your textual interpretations so as to allow the exact underlying results, or at least some related results, to be tracked down in the future.</p>
<h2>Rule 10: Provide Public Access to Scripts, Runs, and Results</h2>
<p>Last, but not least, all input data, scripts, versions, parameters, and intermediate results should be made publicly and easily accessible. Various solutions have now become available to make data sharing more convenient, standardized, and accessible in particular domains, such as for gene expression data [2123]. Most journals allow articles to be supplemented with online material, and some journals have initiated further efforts for making data and code more integrated with publications [3,24]. As a minimum, you should submit the main data and source code as supplementary material, and be prepared to respond to any requests for further data or methodology details by peers.</p>
<p>Making reproducibility of your work by peers a realistic possibility sends a strong signal of quality, trustworthiness, and transparency. This could increase the quality and speed of the reviewing process on your work, the chances of your work getting published, and the chances of your work being taken further and cited by other researchers after publication [25].</p>
<h1>References</h1>
<ol>
<li>Crocker J, Cooper ML (2011) Addressing scientific fraud. Science 334: 1182.</li>
<li>Jassy BR, Chin G, Chong L, Vignieri S (2011) Data replication \&amp; reproducibility. Again, and again, and again.... Introduction. Science 334: 1225 .</li>
<li>Peng RD (2011) Reproducible research in computational science. Science 334: 1226-1227.</li>
<li>Mesirov JP (2010) Computer science. Accessible reproducible research. Science 327: 415-416.</li>
<li>Nekratenko A, Taylor J (2012) Next-generation sequencing data interpretation: enhancing reproducibility and accessibility. Nat Rev Genet 13: $667-672$.</li>
<li>Ioannidis JP, Allison DB, Ball CA, Coulibaly I, Cui X, et al. (2009) Repeatability of published microarray gene expression analyses. Nat Genet 41: 149-155.</li>
<li>Steen RG (2011) Retractions in the scientific literature: is the incidence of research fraud increasing? J Med Ethics 37: 249-253.</li>
<li>Prinz F, Schlange T, Asadullah K (2011) Believe it or not: how much can we rely on published data on potential drug targets? Nat Rev Drug Discov 10: 712 .</li>
<li>Begley CG, Ellis LM (2012) Drug development: raise standards for preclinical cancer research. Nature 483: 531-533.</li>
<li>Reich M, Liefeld T, Gould J, Lerner J, Tamayo P, et al. (2006) GenePattern 2.0. Nat Genet 38: $500-501$.</li>
<li>Giardine B, Riemer C, Hardison RC, Burhans R, Ehniski L, et al. (2005) Galaxy: a platform for interactive large-scale genome analysis. Genome Res 15: 1451-1455.</li>
<li>Rex DL, Ma JQ, Toga AW (2003) The LONI Pipeline Processing Environment. Neuroimage 19: 1033-1048.</li>
<li>Oinn T, Addis M, Ferris J, Marvin D, Senger M, et al. (2004) Taverna: a tool for the composition and enactment of bioinformatics workflows. Bioinformatics 20: 3045-3054.</li>
<li>Piwowar HA, Day RS, Fridsma DB (2007) Sharing detailed research data is associated with increased citation rate. PLoS ONE 2: e308. doi:10.1371/journal.pone. 0000308.</li>
<li>Heroux MA, Willenbring JM (2009) Barely sufficient software engineering: 10 practices to improve your cse software. In: 2009 ICSE Workshop on Software Engineering for Computational Science and Engineering. pp. 15-21.</li>
<li>Schwab M, Karrenbach M, Claerbout J (2000) Making scientific computations reproducible. Comput Sci Eng 2: 61-67.</li>
<li>Goble CA, Bhagat J, Aleksejevs S, Cruickshank D, Michaelides D, et al. (2010) myExperiment: a repository and social network for the sharing of bioinformatics workflows. Nucleic Acids Res 38: W677-682.</li>
<li>Derlman E, Singh G, Su M-H, Blythe J, Gil Y, et al. (2005) Pegasus: a framework for mapping complex scientific workflows onto distributed systems. Scientific Programming Journal 13: 219-237.</li>
<li>Leisch F (2002) Sweave: dynamic generation of statistical reports using literate data analysis. In: Hardie W, Renz B, editors. Compstat: proceedings in computational statistics. Heidelberg, Germany: Physika Verlag. pp. 575-580.</li>
<li>Goecki J, Nekratenko A, Taylor J (2010) Galaxy: a comprehensive approach for supporting accessible, reproducible, and transparent computational research in the life sciences. Genome Biol 11: R86.</li>
<li>Brazma A, Hingamp P, Quackenbush J, Sherlock G, Spellman P, et al. (2001) Minimum information about a microarray experiment (MIAME)toward standards for microarray data. Nat Genet 29: 365-371.</li>
<li>Brazma A, Parkinson H, Sarkans U, Shojatalab M, Vilo J, et al. (2003) ArrayExpress-a public repository for microarray gene expression data at the EBI. Nucleic Acids Res 31: 68-71.</li>
<li>Edgar R, Domrachev M, Lash AE (2002) Gene Expression Omnibus: NCBI gene expression and hybridization array data repository. Nucleic Acids Res 30: 207-210.</li>
<li>Sneddon TP, Li P, Edmunds SC (2012) GigaDB: announcing the GigaScience database. Gigascience $1: 11$.</li>
<li>Prlić A, Procter JB (2012) Ten simple rules for the open development of scientific software. PLoS Comput Biol 8: e1002802. doi:10.1371/journal.pebi. 1002802 .</li>
</ol>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>Citation: Sandve GK, Nekrutenko A, Taylor J, Hovig E (2013) Ten Simple Rules for Reproducible Computational Research. PLoS Comput Biol 9(10): e1003285. doi:10.1371/journal.pcbi. 1003285
Editor: Philip E. Bourne, University of California San Diego, United States of America
Published October 24, 2013
Copyright: (c) 2013 Sandve et al. This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.
Funding: The authors' laboratories are supported by US National Institutes of Health grants HG005133, HG004909, and HG006620 and US National Science Foundation grant DBI 0850103. Additional funding is provided, in part, by the Huck Institutes for the Life Sciences at Penn State, the Institute for Cyberscience at Penn State, and a grant with the Pennsylvania Department of Health using Tobacco Settlement Funds. The funders had no role in the preparation of the manuscript.
Competing Interests: The authors have declared that no competing interests exist.</p>
<ul>
<li>E-mail: geirksa@ifl.uio.no</li>
</ul>
<p><a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
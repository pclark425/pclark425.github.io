<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2429 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2429</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2429</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-67.html">extraction-schema-67</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <p><strong>Paper ID:</strong> paper-258179480</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2304.07445v1.pdf" target="_blank">A framework for fully autonomous design of materials via multiobjective optimization and active learning: challenges and next steps</a></p>
                <p><strong>Paper Abstract:</strong> In order to deploy machine learning in a real-world self-driving laboratory where data acquisition is costly and there are multiple competing design criteria, systems need to be able to intelligently sample while balancing performance trade-offs and constraints. For these reasons, we present an active learning process based on multiobjective black-box optimization with continuously updated machine learning models. This workflow is built on open-source technologies for real-time data streaming and modular multiobjective optimization software development. We demonstrate a proof of concept for this workflow through the autonomous operation of a continuous-flow chemistry laboratory, which identifies ideal manufacturing conditions for the electrolyte 2,2,2-trifluoroethyl methyl carbonate.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2429.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2429.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ParMOO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ParMOO (parallel multiobjective simulation optimization)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A modular multiobjective simulation-optimization library that manages multiple surrogate models, custom embeddings, optimization solvers, and acquisition functions in a feedback loop to drive closed-loop experiments and simulations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ParMOO: Python library for parallel multiobjective simulation optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ParMOO multiobjective active-learning loop</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>ParMOO coordinates surrogate modeling, candidate generation (via user-provided optimizers), and data acquisition in batches. Users specify design variables, data sources, constraints, surrogate models for simulation outputs, optimization solvers for the surrogate problem, and acquisition functions. ParMOO first evaluates an initial design of experiments, fits surrogates, then iteratively proposes batches of candidates that are sent to experiments/simulators; results are streamed back and used to retrain surrogates and produce further batches.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Materials synthesis and continuous-flow chemistry (general multiobjective experimental design and simulation optimization).</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Explicit batch active learning: a fixed-size initial Latin hypercube design (15 points) for exploration, then iterative model-driven batches (batch size 3) focused on exploitation of surrogate optima; candidate batches are generated by multiple acquisition functions (two epsilon-constraint scalarizations and one fixed 50/50 objective weighting).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Not formally quantified by ParMOO in this work; practical cost measured implicitly as number of physical experiments, wall-clock time per experiment (up to ~10 minutes), and reagent/equipment monetary cost; computational cost for surrogate optimization handled by local compute nodes but not reported as a numeric metric.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Two-stage schedule: allocate a fixed exploration budget (15-point Latin hypercube) to improve initial surrogate coverage, then allocate remaining experimental budget primarily to exploitation-driven candidate selection using surrogate optimization and scalarized acquisition functions (epsilon-constraint and fixed-weight scalarization).</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Initial Latin hypercube design for coverage; use of multiple scalarization-based acquisition functions (epsilon-constraint variants) to explore different Pareto tradeoffs; batch generation from multiple acquisition functions promotes some diversity across iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed and tight experimental budget (limited number of physical experiments due to reagent cost, equipment operating cost, and time constraints).</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Hard-constrained strategy: predefine initial DOE size and remaining iteration count/batch sizes; prefer exploitation after initial DOE to prioritize rapid convergence within limited budget; stop allocating further experiments if recent batches show lack of improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Not defined as a separate metric; discoveries measured via objective values (maximize product TFMC, minimize byproduct TFE) and Pareto-improvement; no explicit novelty/breakthrough scoring used.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Total completed experiments: 41 (15 initial LHS + up to 27 iterative; some allocated runs were not executed due to lack of improvement); convergence indicated qualitatively by clustering of later runs in objective space (no numeric improvement-vs-baseline reported).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>No quantitative baseline experiments reported in this study (authors note typical Bayesian optimization workflows might use hundreds of experiments, but no direct experimental comparison was performed).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Not reported quantitatively; qualitative claim that method converged to good manufacturing conditions within a much smaller experiment count than would typically be used by standard Bayesian optimization workflows (no numeric factor provided).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>No explicit numeric efficiency gain reported; empirical result: convergence behavior observed with 41 experiments total under constrained budget.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Discusses practical tradeoffs: under tight, costly experimental budgets, authors favor initial coverage then heavy exploitation rather than pursuing global model accuracy; they acknowledge global convergence guarantees are weakened by this choice and recommend using domain-specific surrogates and embeddings to mitigate risk; they flag the potential role of cost-aware acquisition but caution that naively adding cost penalties may exclude expensive but insightful experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Recommendation for tight-budget laboratory settings: allocate a modest initial exploration budget (carefully chosen DOE) to obtain surrogate coverage, then concentrate remaining budget on exploitation-driven experiments; use multiple acquisition strategies to probe Pareto tradeoffs; consider simulation (cheap) for early exploration and experiments for late-stage validation; incorporate domain-specific modeling to reduce required experiments. Consider cost-aware acquisition only with safeguards so costly but high-information experiments remain possible.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2429.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2429.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MDML</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Manufacturing Data and Machine Learning (MDML) platform</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A streaming, event-driven platform (built on Apache Kafka) for connecting laboratory instrumentation, analysis workers, and ML models to enable closed-loop, real-time experiment steering and data archiving.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The manufacturing data and machine learning platform: Enabling real-time monitoring and control of scientific experiments via IoT</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>MDML event-driven experimental streaming and orchestration</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>MDML provides Kafka-based topics for publishing and subscribing to experiment data and candidate requests, databases for longer-term storage, and integration with serverless compute endpoints (FuncX) to run analysis jobs. In this work MDML brokers ParMOO's requests to the LabVIEW-controlled CFR and NMR, streaming experiment results back to the ParMOO analysis worker to close the optimization loop.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Automated materials synthesis and instrument integration (self-driving labs).</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>MDML itself does not select experiments but enables distribution of candidate requests and results; it supports horizontal scaling (multiple Kafka brokers) and orchestrates routing so compute resources and instruments are efficiently utilized.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Not an algorithmic decision-maker for exploration/exploitation; it enables the chosen optimization system (ParMOO) to enact its allocation policy across distributed clients and instruments.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>No explicit diversity promotion; supports heterogeneous data sources (simulators, sensors, IoT) which can be combined by downstream decision systems to encourage diverse experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Infrastructure-level scaling and throughput constraints (number of concurrent streams, compute endpoints), not experimental budget per se.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Provides scalable messaging and compute integration to reduce latency and allow parallel experiment execution when hardware supports it; does not itself optimize experiment allocations under a monetary or experimental budget.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>System architecture: two Kafka brokers, databases for long-term storage, FuncX endpoints for analysis; no numerical throughput or latency metrics reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Enables automated closed-loop operation and horizontal scaling, but no quantitative efficiency gains versus alternatives reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>MDML is presented as infrastructure to support decision systems that must manage tradeoffs; the paper does not itself analyze cost/information tradeoffs at the MDML layer.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>MDML is recommended as a flexible backbone for integrating heterogeneous simulators and instruments so that resource-aware decision systems (e.g., those that might account for experiment cost) can be deployed; practical benefit is in enabling seamless streaming and orchestration rather than in making allocation decisions.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2429.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2429.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RSM-style allocation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Response-surface-methodology-inspired active learning allocation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An allocation policy used in this work: reserve a small, predetermined initial DOE for exploration to build surrogates, then allocate the remainder of the limited budget to exploitation-driven experiments to rapidly find optima under tight cost/time constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>RSM-like initial exploration followed by exploitation</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Under severe experimental budget limits, the authors adopt a two-phase allocation: (1) a fixed-size exploratory Latin hypercube (15 experiments) to provide surrogate coverage and initial models; (2) subsequent iterations focus on maximizing the surrogate objectives (exploitation) using batch acquisition, stopping early if no improvement is observed. This policy trades global surrogate accuracy for rapid local improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Cost-limited experimental optimization in continuous-flow chemistry and materials manufacturing.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Hard allocation: predefine exploration budget and prioritize exploitation thereafter; allocate experiments in small batches (3 per batch) to balance throughput and model updating frequency; sort batch execution to reduce experiment overhead (see temperature-sorted batching).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Experiment count and wall-clock experiment time dominate cost; authors quantify experiments (initial 15 + iterative batches) rather than CPU FLOPs or monetary cost per se.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Deterministic schedule (fixed initial exploration then exploitation). No expected-improvement or mutual-information acquisition used; exploitation is guided by surrogate optimization and multiobjective scalarizations.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Exploration diversity via Latin hypercube initial DOE; no explicit diversity-promoting acquisition during the exploitation phase beyond use of multiple scalarizations to sample different Pareto trade-offs.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed experiment budget (monetary/time/resource limited).</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Pre-allocated exploration budget and conservative exploitation thereafter; early stopping of planned experiments if batches show no further improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Not applicable here â€” focus is on finding optimal manufacturing conditions (objective improvement), not explicit novelty/breakthrough scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Initial DOE size: 15; iterative model-exploiting iterations executed: 9 with batch size 3 (planned 27), total executed experiments: 41 (some planned runs omitted due to lack of improvement).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Authors contrast qualitatively with typical Bayesian optimization workflows (which often use many more experiments) but do not provide an empirical baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Not quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Empirical convergence to desirable operating region within 41 experiments under tight budget; no percent or multiplicative efficiency number is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Authors explicitly discuss the tradeoff: prioritizing local exploitation can achieve fast gains under cost constraints but sacrifices guarantees of global convergence and uniform surrogate accuracy; they propose domain-specific surrogates and simulation-then-experiment pipelines to mitigate these risks.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>For extremely limited experimental budgets, an RSM-like approach (small well-designed initial DOE, then focused exploitation) is pragmatic; augment with domain knowledge, latent embeddings, and simulation data to reduce risk of missing global optima.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2429.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2429.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Epsilon-constraint & fixed-weight scalarizations</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Epsilon-constraint and fixed-weight scalarization acquisition functions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Three acquisition functions used to generate candidate experiment batches: two using epsilon-constraint scalarizations to explore different Pareto tradeoffs and one using a fixed 50/50 weighting between objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Scalarization-based acquisition functions within ParMOO</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>ParMOO used three acquisition functions in parallel to produce candidate batches: two epsilon-constraint scalarizations (each treats one objective as primary and constrains the other with an epsilon threshold to explore different Pareto regions) and one fixed-weight scalarization that equally weights both objectives. Candidates from all acquisition functions are combined into batches for execution.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Multiobjective experimental design in chemistry (maximize TFMC, minimize TFE) and general multiobjective optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Generates multiple candidate lists from diverse scalarizations per iteration, allocating experimental budget across different Pareto directions to sample tradeoffs instead of focusing on a single scalar objective.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Scalarizations give a mechanism to explore Pareto front structure (epsilon-constraint variants) while fixed-weight scalarization targets balanced improvement; combined they bias allocation toward Pareto-relevant exploitation while providing some coverage of the frontier.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Diversity arises from using multiple distinct scalarizations per iteration, producing candidates that target different regions of the Pareto front.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Batch-size constrained (three candidates per iteration) under a limited total experimental budget.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Batch generation from multiple acquisition functions shares limited per-iteration budget across different Pareto goals; no explicit cost-weighting in acquisition was used.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Three acquisition functions produced candidate batches of size three per iteration; resulted in 9 iterative iterations (planned) though some runs were omitted when no improvements occurred.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Using multiple scalarizations is a pragmatic way to explore Pareto structure under budget limits; no formal analysis of information gain vs cost of these different acquisition choices is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Combining multiple scalarizations in small batches can promote exploration of tradeoffs in a constrained budget setting; however the paper suggests future directions to incorporate experiment cost into acquisition decisions.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2429.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2429.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Cost-aware acquisition (proposed)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Cost-aware acquisition functions (proposed future enhancement)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proposed extension to incorporate explicit experimental cost into acquisition functions so that the decision process accounts for per-experiment monetary/time cost when allocating resources.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Cost-aware acquisition</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Authors suggest modifying acquisition functions to include the cost of each experiment (e.g., reagents, instrument time, setup/stabilization overhead) so that the optimizer can trade information expected from an experiment against its expense, but they note this must be done carefully to avoid excluding expensive but highly informative experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Budget-limited experimental design for materials synthesis and discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Would penalize or normalize acquisition utilities by an experimental cost term, effectively prioritizing higher information-per-cost experiments; explicit form not implemented in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Proposed to include monetary reagent cost, equipment operating cost, and experiment wall-clock time as components of per-experiment cost.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Would change acquisition ranking to maximize expected utility per unit cost (e.g., acquisition / cost), introducing an explicit tradeoff between information gain and cost; no concrete acquisition formula is provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Not specified; authors caution that naive cost penalties could reduce exploration diversity by disfavoring costly but novel experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Monetary/time/resource-limited experimental budgets (proposed to be handled in acquisition).</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Proposed approach: integrate per-experiment cost into acquisition function so allocations are cost-aware; authors recommend careful design to retain ability to run expensive informative experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Authors warn that naive cost penalization may suppress experiments with high breakthrough potential; no metric proposed for protecting breakthrough experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Authors explicitly discuss the tradeoff: incorporating cost can keep budgets reasonable but risks excluding high-cost/high-information experiments; they emphasize the need for mechanisms that allow evaluation of expensive but insightful experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Recommendation: consider experiment cost in acquisition only with safeguards (e.g., explicit allowance or separate channel for high-cost high-information experiments) and combine with simulation-based cheap exploration to preserve discovery potential.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2429.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2429.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Batch scheduling by temperature</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Temperature-sorted batch execution</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An experimental scheduling optimization used in the laboratory: within a batch, experiment runs were sorted by reaction temperature to reduce reactor stabilization overhead and thereby shorten per-experiment wall-clock time.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Batch execution ordering (temperature sorting)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>After ParMOO proposed a batch of candidate experimental parameter sets, the lab client sorted the experiments by reaction temperature before execution so that consecutive experiments have smaller temperature jumps, reducing thermal stabilization time and hence total experiment time per batch.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Laboratory experimental throughput optimization for continuous-flow reactors.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Does not change which experiments are chosen, but schedules execution order to reduce per-experiment overhead and thereby effectively increase throughput under a fixed time budget.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Reduces wall-clock time per experiment by minimizing stabilization intervals; no numeric times reported for savings.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Orthogonal to exploration/exploitation choices; purely an execution-level optimization to reduce resource overhead.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Time-budget and throughput optimization (minimize total lab time).</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Reorders batch experiments to minimize instrument dead time; practical measure to stretch a fixed time budget.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>No quantitative measurement of time savings provided; described as beneficial in reducing time needed to stabilize temperatures between experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Qualitative improvement in throughput by reducing temperature stabilization overhead; no numeric estimate provided.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Simple scheduler optimization that reduces overhead without risking model quality; recommended as a practical measure in lab automation.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Order experiments to minimize setup/stabilization overhead to better utilize limited experimental time and extend effective budget.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2429.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2429.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bayesian optimization (general / BoTorch)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bayesian optimization (BoTorch as framework)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model-based global optimization paradigm commonly used in active learning and experiment design; BoTorch is a modern framework for scalable Monte-Carlo-based Bayesian optimization referenced by the authors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>BoTorch: A framework for efficient Monte-Carlo Bayesian optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Bayesian optimization (general mention)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Bayesian optimization uses probabilistic surrogates (typically Gaussian processes) and acquisition functions (e.g., expected improvement, information-based criteria) to sequentially select experiments that trade exploration and exploitation; BoTorch is an implementation framework for such methods.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>General experimental design and automated discovery across chemistry and materials science.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Typically selects experiments by maximizing an acquisition function that encodes expected improvement or information gain per evaluation; can be extended to account for evaluation cost via cost-weighted acquisitions.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Frameworks commonly consider wall-clock time per experiment and number of evaluations; BoTorch supports Monte Carlo techniques for acquisition evaluation which have computational cost measured in CPU/GPU time.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Often uses expected improvement, probability of improvement, or information-based metrics (e.g., mutual information, entropy search); not used directly in the ParMOO experiments but cited as common alternative.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Acquisition functions balance exploration vs exploitation (e.g., EI balances potential for improvement and surrogate uncertainty); multiobjective BO uses scalarizations or Pareto-aware acquisitions.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Can incorporate penalization, batch diversity-promoting terms, or multi-start acquisition draws to encourage diverse candidates; specific mechanisms depend on implementation.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Can be adapted to fixed evaluation budgets, wall-clock/time budgets, or cost-weighted budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Cost-aware variants weight acquisition by per-evaluation cost or use budgeted BO algorithms; not implemented in this paper's experiments but referenced as typical practice.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>BO can target large improvements via EI or capture novelty via information-based acquisitions; breakthrough detection depends on objective formulation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Authors note that Bayesian optimization typically requires many more experiments for global model convergenceâ€”potentially hundredsâ€”which may be infeasible in costly lab settings; this motivates their RSM-like allocation choice.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>BO is a powerful general approach but may be too experiment-hungry for tightly budgeted physical experiments; hybrid strategies (simulation early, experiments late) or cost-aware acquisitions are suggested to reduce needed physical runs.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2429.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2429.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chimera</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chimera: hierarchy-based multi-objective optimization for self-driving laboratories</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hierarchical multiobjective optimization approach for self-driving laboratories that prioritizes objectives through a hierarchy or ordering, enabling structured tradeoffs between competing experimental goals.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chimera: enabling hierarchy based multi-objective optimization for self-driving laboratories.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Chimera hierarchical multiobjective strategy</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Chimera provides a way to impose hierarchical preferences among multiple objectives so that decision-making focuses on satisfying higher-priority goals before optimizing lower-priority onesâ€”a principled way to encode breakthrough potential or critical constraints into the acquisition policy.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Self-driving laboratories and automated experimental optimization with multiple competing objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocates experiments to satisfy higher-priority objectives first, then refines lower-priority ones; can implicitly control allocation between exploitation of high-priority improvements and exploration for secondary objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Explores tradeoffs according to hierarchical priorities rather than a flat Pareto-ranking; enables targeted search toward critical breakthroughs defined by hierarchy.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Hierarchy can concentrate sampling on regions relevant to top-level objectives, which may reduce diversity for lower-level goals unless explicitly balanced.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Applicable to fixed experimental budgets where priorities control allocation across objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Hierarchy guides how limited experimental budget is apportioned among objectives, prioritizing higher-level goals.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>By promoting top-level objectives, Chimera can be used to prioritize experiments with higher breakthrough potential defined by hierarchy; specific metrics depend on objective definitions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Mentioned as a relevant multiobjective approach in the literature for self-driving labs; not used in this paper but highlighted as a technique that addresses structured objective tradeoffs.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Not evaluated empirically in this paper; included as a follow-on example of multiobjective strategies that can encode priorities and thereby influence resource allocation toward breakthrough goals.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chimera: enabling hierarchy based multi-objective optimization for self-driving laboratories. <em>(Rating: 2)</em></li>
                <li>BoTorch: A framework for efficient Monte-Carlo Bayesian optimization. <em>(Rating: 2)</em></li>
                <li>Bayesian reaction optimization as a tool for chemical synthesis. <em>(Rating: 2)</em></li>
                <li>Optimal criteria and their asymptotic form for data selection in data-driven reduced-order modelling with Gaussian process regression. <em>(Rating: 2)</em></li>
                <li>Active learning to overcome exponential-wall problem for effective structure prediction of chemical-disordered materials. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2429",
    "paper_id": "paper-258179480",
    "extraction_schema_id": "extraction-schema-67",
    "extracted_data": [
        {
            "name_short": "ParMOO",
            "name_full": "ParMOO (parallel multiobjective simulation optimization)",
            "brief_description": "A modular multiobjective simulation-optimization library that manages multiple surrogate models, custom embeddings, optimization solvers, and acquisition functions in a feedback loop to drive closed-loop experiments and simulations.",
            "citation_title": "ParMOO: Python library for parallel multiobjective simulation optimization.",
            "mention_or_use": "use",
            "system_name": "ParMOO multiobjective active-learning loop",
            "system_description": "ParMOO coordinates surrogate modeling, candidate generation (via user-provided optimizers), and data acquisition in batches. Users specify design variables, data sources, constraints, surrogate models for simulation outputs, optimization solvers for the surrogate problem, and acquisition functions. ParMOO first evaluates an initial design of experiments, fits surrogates, then iteratively proposes batches of candidates that are sent to experiments/simulators; results are streamed back and used to retrain surrogates and produce further batches.",
            "application_domain": "Materials synthesis and continuous-flow chemistry (general multiobjective experimental design and simulation optimization).",
            "resource_allocation_strategy": "Explicit batch active learning: a fixed-size initial Latin hypercube design (15 points) for exploration, then iterative model-driven batches (batch size 3) focused on exploitation of surrogate optima; candidate batches are generated by multiple acquisition functions (two epsilon-constraint scalarizations and one fixed 50/50 objective weighting).",
            "computational_cost_metric": "Not formally quantified by ParMOO in this work; practical cost measured implicitly as number of physical experiments, wall-clock time per experiment (up to ~10 minutes), and reagent/equipment monetary cost; computational cost for surrogate optimization handled by local compute nodes but not reported as a numeric metric.",
            "information_gain_metric": null,
            "uses_information_gain": false,
            "exploration_exploitation_mechanism": "Two-stage schedule: allocate a fixed exploration budget (15-point Latin hypercube) to improve initial surrogate coverage, then allocate remaining experimental budget primarily to exploitation-driven candidate selection using surrogate optimization and scalarized acquisition functions (epsilon-constraint and fixed-weight scalarization).",
            "diversity_mechanism": "Initial Latin hypercube design for coverage; use of multiple scalarization-based acquisition functions (epsilon-constraint variants) to explore different Pareto tradeoffs; batch generation from multiple acquisition functions promotes some diversity across iterations.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Fixed and tight experimental budget (limited number of physical experiments due to reagent cost, equipment operating cost, and time constraints).",
            "budget_constraint_handling": "Hard-constrained strategy: predefine initial DOE size and remaining iteration count/batch sizes; prefer exploitation after initial DOE to prioritize rapid convergence within limited budget; stop allocating further experiments if recent batches show lack of improvement.",
            "breakthrough_discovery_metric": "Not defined as a separate metric; discoveries measured via objective values (maximize product TFMC, minimize byproduct TFE) and Pareto-improvement; no explicit novelty/breakthrough scoring used.",
            "performance_metrics": "Total completed experiments: 41 (15 initial LHS + up to 27 iterative; some allocated runs were not executed due to lack of improvement); convergence indicated qualitatively by clustering of later runs in objective space (no numeric improvement-vs-baseline reported).",
            "comparison_baseline": "No quantitative baseline experiments reported in this study (authors note typical Bayesian optimization workflows might use hundreds of experiments, but no direct experimental comparison was performed).",
            "performance_vs_baseline": "Not reported quantitatively; qualitative claim that method converged to good manufacturing conditions within a much smaller experiment count than would typically be used by standard Bayesian optimization workflows (no numeric factor provided).",
            "efficiency_gain": "No explicit numeric efficiency gain reported; empirical result: convergence behavior observed with 41 experiments total under constrained budget.",
            "tradeoff_analysis": "Discusses practical tradeoffs: under tight, costly experimental budgets, authors favor initial coverage then heavy exploitation rather than pursuing global model accuracy; they acknowledge global convergence guarantees are weakened by this choice and recommend using domain-specific surrogates and embeddings to mitigate risk; they flag the potential role of cost-aware acquisition but caution that naively adding cost penalties may exclude expensive but insightful experiments.",
            "optimal_allocation_findings": "Recommendation for tight-budget laboratory settings: allocate a modest initial exploration budget (carefully chosen DOE) to obtain surrogate coverage, then concentrate remaining budget on exploitation-driven experiments; use multiple acquisition strategies to probe Pareto tradeoffs; consider simulation (cheap) for early exploration and experiments for late-stage validation; incorporate domain-specific modeling to reduce required experiments. Consider cost-aware acquisition only with safeguards so costly but high-information experiments remain possible.",
            "uuid": "e2429.0"
        },
        {
            "name_short": "MDML",
            "name_full": "Manufacturing Data and Machine Learning (MDML) platform",
            "brief_description": "A streaming, event-driven platform (built on Apache Kafka) for connecting laboratory instrumentation, analysis workers, and ML models to enable closed-loop, real-time experiment steering and data archiving.",
            "citation_title": "The manufacturing data and machine learning platform: Enabling real-time monitoring and control of scientific experiments via IoT",
            "mention_or_use": "use",
            "system_name": "MDML event-driven experimental streaming and orchestration",
            "system_description": "MDML provides Kafka-based topics for publishing and subscribing to experiment data and candidate requests, databases for longer-term storage, and integration with serverless compute endpoints (FuncX) to run analysis jobs. In this work MDML brokers ParMOO's requests to the LabVIEW-controlled CFR and NMR, streaming experiment results back to the ParMOO analysis worker to close the optimization loop.",
            "application_domain": "Automated materials synthesis and instrument integration (self-driving labs).",
            "resource_allocation_strategy": "MDML itself does not select experiments but enables distribution of candidate requests and results; it supports horizontal scaling (multiple Kafka brokers) and orchestrates routing so compute resources and instruments are efficiently utilized.",
            "computational_cost_metric": null,
            "information_gain_metric": null,
            "uses_information_gain": null,
            "exploration_exploitation_mechanism": "Not an algorithmic decision-maker for exploration/exploitation; it enables the chosen optimization system (ParMOO) to enact its allocation policy across distributed clients and instruments.",
            "diversity_mechanism": "No explicit diversity promotion; supports heterogeneous data sources (simulators, sensors, IoT) which can be combined by downstream decision systems to encourage diverse experiments.",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Infrastructure-level scaling and throughput constraints (number of concurrent streams, compute endpoints), not experimental budget per se.",
            "budget_constraint_handling": "Provides scalable messaging and compute integration to reduce latency and allow parallel experiment execution when hardware supports it; does not itself optimize experiment allocations under a monetary or experimental budget.",
            "breakthrough_discovery_metric": null,
            "performance_metrics": "System architecture: two Kafka brokers, databases for long-term storage, FuncX endpoints for analysis; no numerical throughput or latency metrics reported.",
            "comparison_baseline": "",
            "performance_vs_baseline": "",
            "efficiency_gain": "Enables automated closed-loop operation and horizontal scaling, but no quantitative efficiency gains versus alternatives reported in this paper.",
            "tradeoff_analysis": "MDML is presented as infrastructure to support decision systems that must manage tradeoffs; the paper does not itself analyze cost/information tradeoffs at the MDML layer.",
            "optimal_allocation_findings": "MDML is recommended as a flexible backbone for integrating heterogeneous simulators and instruments so that resource-aware decision systems (e.g., those that might account for experiment cost) can be deployed; practical benefit is in enabling seamless streaming and orchestration rather than in making allocation decisions.",
            "uuid": "e2429.1"
        },
        {
            "name_short": "RSM-style allocation",
            "name_full": "Response-surface-methodology-inspired active learning allocation",
            "brief_description": "An allocation policy used in this work: reserve a small, predetermined initial DOE for exploration to build surrogates, then allocate the remainder of the limited budget to exploitation-driven experiments to rapidly find optima under tight cost/time constraints.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "RSM-like initial exploration followed by exploitation",
            "system_description": "Under severe experimental budget limits, the authors adopt a two-phase allocation: (1) a fixed-size exploratory Latin hypercube (15 experiments) to provide surrogate coverage and initial models; (2) subsequent iterations focus on maximizing the surrogate objectives (exploitation) using batch acquisition, stopping early if no improvement is observed. This policy trades global surrogate accuracy for rapid local improvement.",
            "application_domain": "Cost-limited experimental optimization in continuous-flow chemistry and materials manufacturing.",
            "resource_allocation_strategy": "Hard allocation: predefine exploration budget and prioritize exploitation thereafter; allocate experiments in small batches (3 per batch) to balance throughput and model updating frequency; sort batch execution to reduce experiment overhead (see temperature-sorted batching).",
            "computational_cost_metric": "Experiment count and wall-clock experiment time dominate cost; authors quantify experiments (initial 15 + iterative batches) rather than CPU FLOPs or monetary cost per se.",
            "information_gain_metric": null,
            "uses_information_gain": false,
            "exploration_exploitation_mechanism": "Deterministic schedule (fixed initial exploration then exploitation). No expected-improvement or mutual-information acquisition used; exploitation is guided by surrogate optimization and multiobjective scalarizations.",
            "diversity_mechanism": "Exploration diversity via Latin hypercube initial DOE; no explicit diversity-promoting acquisition during the exploitation phase beyond use of multiple scalarizations to sample different Pareto trade-offs.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Fixed experiment budget (monetary/time/resource limited).",
            "budget_constraint_handling": "Pre-allocated exploration budget and conservative exploitation thereafter; early stopping of planned experiments if batches show no further improvement.",
            "breakthrough_discovery_metric": "Not applicable here â€” focus is on finding optimal manufacturing conditions (objective improvement), not explicit novelty/breakthrough scoring.",
            "performance_metrics": "Initial DOE size: 15; iterative model-exploiting iterations executed: 9 with batch size 3 (planned 27), total executed experiments: 41 (some planned runs omitted due to lack of improvement).",
            "comparison_baseline": "Authors contrast qualitatively with typical Bayesian optimization workflows (which often use many more experiments) but do not provide an empirical baseline.",
            "performance_vs_baseline": "Not quantified.",
            "efficiency_gain": "Empirical convergence to desirable operating region within 41 experiments under tight budget; no percent or multiplicative efficiency number is reported.",
            "tradeoff_analysis": "Authors explicitly discuss the tradeoff: prioritizing local exploitation can achieve fast gains under cost constraints but sacrifices guarantees of global convergence and uniform surrogate accuracy; they propose domain-specific surrogates and simulation-then-experiment pipelines to mitigate these risks.",
            "optimal_allocation_findings": "For extremely limited experimental budgets, an RSM-like approach (small well-designed initial DOE, then focused exploitation) is pragmatic; augment with domain knowledge, latent embeddings, and simulation data to reduce risk of missing global optima.",
            "uuid": "e2429.2"
        },
        {
            "name_short": "Epsilon-constraint & fixed-weight scalarizations",
            "name_full": "Epsilon-constraint and fixed-weight scalarization acquisition functions",
            "brief_description": "Three acquisition functions used to generate candidate experiment batches: two using epsilon-constraint scalarizations to explore different Pareto tradeoffs and one using a fixed 50/50 weighting between objectives.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Scalarization-based acquisition functions within ParMOO",
            "system_description": "ParMOO used three acquisition functions in parallel to produce candidate batches: two epsilon-constraint scalarizations (each treats one objective as primary and constrains the other with an epsilon threshold to explore different Pareto regions) and one fixed-weight scalarization that equally weights both objectives. Candidates from all acquisition functions are combined into batches for execution.",
            "application_domain": "Multiobjective experimental design in chemistry (maximize TFMC, minimize TFE) and general multiobjective optimization.",
            "resource_allocation_strategy": "Generates multiple candidate lists from diverse scalarizations per iteration, allocating experimental budget across different Pareto directions to sample tradeoffs instead of focusing on a single scalar objective.",
            "computational_cost_metric": null,
            "information_gain_metric": null,
            "uses_information_gain": false,
            "exploration_exploitation_mechanism": "Scalarizations give a mechanism to explore Pareto front structure (epsilon-constraint variants) while fixed-weight scalarization targets balanced improvement; combined they bias allocation toward Pareto-relevant exploitation while providing some coverage of the frontier.",
            "diversity_mechanism": "Diversity arises from using multiple distinct scalarizations per iteration, producing candidates that target different regions of the Pareto front.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Batch-size constrained (three candidates per iteration) under a limited total experimental budget.",
            "budget_constraint_handling": "Batch generation from multiple acquisition functions shares limited per-iteration budget across different Pareto goals; no explicit cost-weighting in acquisition was used.",
            "breakthrough_discovery_metric": null,
            "performance_metrics": "Three acquisition functions produced candidate batches of size three per iteration; resulted in 9 iterative iterations (planned) though some runs were omitted when no improvements occurred.",
            "comparison_baseline": "",
            "performance_vs_baseline": "",
            "efficiency_gain": "",
            "tradeoff_analysis": "Using multiple scalarizations is a pragmatic way to explore Pareto structure under budget limits; no formal analysis of information gain vs cost of these different acquisition choices is provided.",
            "optimal_allocation_findings": "Combining multiple scalarizations in small batches can promote exploration of tradeoffs in a constrained budget setting; however the paper suggests future directions to incorporate experiment cost into acquisition decisions.",
            "uuid": "e2429.3"
        },
        {
            "name_short": "Cost-aware acquisition (proposed)",
            "name_full": "Cost-aware acquisition functions (proposed future enhancement)",
            "brief_description": "A proposed extension to incorporate explicit experimental cost into acquisition functions so that the decision process accounts for per-experiment monetary/time cost when allocating resources.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "system_name": "Cost-aware acquisition",
            "system_description": "Authors suggest modifying acquisition functions to include the cost of each experiment (e.g., reagents, instrument time, setup/stabilization overhead) so that the optimizer can trade information expected from an experiment against its expense, but they note this must be done carefully to avoid excluding expensive but highly informative experiments.",
            "application_domain": "Budget-limited experimental design for materials synthesis and discovery.",
            "resource_allocation_strategy": "Would penalize or normalize acquisition utilities by an experimental cost term, effectively prioritizing higher information-per-cost experiments; explicit form not implemented in this work.",
            "computational_cost_metric": "Proposed to include monetary reagent cost, equipment operating cost, and experiment wall-clock time as components of per-experiment cost.",
            "information_gain_metric": null,
            "uses_information_gain": null,
            "exploration_exploitation_mechanism": "Would change acquisition ranking to maximize expected utility per unit cost (e.g., acquisition / cost), introducing an explicit tradeoff between information gain and cost; no concrete acquisition formula is provided in this paper.",
            "diversity_mechanism": "Not specified; authors caution that naive cost penalties could reduce exploration diversity by disfavoring costly but novel experiments.",
            "uses_diversity_promotion": null,
            "budget_constraint_type": "Monetary/time/resource-limited experimental budgets (proposed to be handled in acquisition).",
            "budget_constraint_handling": "Proposed approach: integrate per-experiment cost into acquisition function so allocations are cost-aware; authors recommend careful design to retain ability to run expensive informative experiments.",
            "breakthrough_discovery_metric": "Authors warn that naive cost penalization may suppress experiments with high breakthrough potential; no metric proposed for protecting breakthrough experiments.",
            "performance_metrics": null,
            "comparison_baseline": "",
            "performance_vs_baseline": "",
            "efficiency_gain": "",
            "tradeoff_analysis": "Authors explicitly discuss the tradeoff: incorporating cost can keep budgets reasonable but risks excluding high-cost/high-information experiments; they emphasize the need for mechanisms that allow evaluation of expensive but insightful experiments.",
            "optimal_allocation_findings": "Recommendation: consider experiment cost in acquisition only with safeguards (e.g., explicit allowance or separate channel for high-cost high-information experiments) and combine with simulation-based cheap exploration to preserve discovery potential.",
            "uuid": "e2429.4"
        },
        {
            "name_short": "Batch scheduling by temperature",
            "name_full": "Temperature-sorted batch execution",
            "brief_description": "An experimental scheduling optimization used in the laboratory: within a batch, experiment runs were sorted by reaction temperature to reduce reactor stabilization overhead and thereby shorten per-experiment wall-clock time.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Batch execution ordering (temperature sorting)",
            "system_description": "After ParMOO proposed a batch of candidate experimental parameter sets, the lab client sorted the experiments by reaction temperature before execution so that consecutive experiments have smaller temperature jumps, reducing thermal stabilization time and hence total experiment time per batch.",
            "application_domain": "Laboratory experimental throughput optimization for continuous-flow reactors.",
            "resource_allocation_strategy": "Does not change which experiments are chosen, but schedules execution order to reduce per-experiment overhead and thereby effectively increase throughput under a fixed time budget.",
            "computational_cost_metric": "Reduces wall-clock time per experiment by minimizing stabilization intervals; no numeric times reported for savings.",
            "information_gain_metric": null,
            "uses_information_gain": false,
            "exploration_exploitation_mechanism": "Orthogonal to exploration/exploitation choices; purely an execution-level optimization to reduce resource overhead.",
            "diversity_mechanism": null,
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Time-budget and throughput optimization (minimize total lab time).",
            "budget_constraint_handling": "Reorders batch experiments to minimize instrument dead time; practical measure to stretch a fixed time budget.",
            "breakthrough_discovery_metric": null,
            "performance_metrics": "No quantitative measurement of time savings provided; described as beneficial in reducing time needed to stabilize temperatures between experiments.",
            "comparison_baseline": "",
            "performance_vs_baseline": "",
            "efficiency_gain": "Qualitative improvement in throughput by reducing temperature stabilization overhead; no numeric estimate provided.",
            "tradeoff_analysis": "Simple scheduler optimization that reduces overhead without risking model quality; recommended as a practical measure in lab automation.",
            "optimal_allocation_findings": "Order experiments to minimize setup/stabilization overhead to better utilize limited experimental time and extend effective budget.",
            "uuid": "e2429.5"
        },
        {
            "name_short": "Bayesian optimization (general / BoTorch)",
            "name_full": "Bayesian optimization (BoTorch as framework)",
            "brief_description": "A model-based global optimization paradigm commonly used in active learning and experiment design; BoTorch is a modern framework for scalable Monte-Carlo-based Bayesian optimization referenced by the authors.",
            "citation_title": "BoTorch: A framework for efficient Monte-Carlo Bayesian optimization.",
            "mention_or_use": "mention",
            "system_name": "Bayesian optimization (general mention)",
            "system_description": "Bayesian optimization uses probabilistic surrogates (typically Gaussian processes) and acquisition functions (e.g., expected improvement, information-based criteria) to sequentially select experiments that trade exploration and exploitation; BoTorch is an implementation framework for such methods.",
            "application_domain": "General experimental design and automated discovery across chemistry and materials science.",
            "resource_allocation_strategy": "Typically selects experiments by maximizing an acquisition function that encodes expected improvement or information gain per evaluation; can be extended to account for evaluation cost via cost-weighted acquisitions.",
            "computational_cost_metric": "Frameworks commonly consider wall-clock time per experiment and number of evaluations; BoTorch supports Monte Carlo techniques for acquisition evaluation which have computational cost measured in CPU/GPU time.",
            "information_gain_metric": "Often uses expected improvement, probability of improvement, or information-based metrics (e.g., mutual information, entropy search); not used directly in the ParMOO experiments but cited as common alternative.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Acquisition functions balance exploration vs exploitation (e.g., EI balances potential for improvement and surrogate uncertainty); multiobjective BO uses scalarizations or Pareto-aware acquisitions.",
            "diversity_mechanism": "Can incorporate penalization, batch diversity-promoting terms, or multi-start acquisition draws to encourage diverse candidates; specific mechanisms depend on implementation.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Can be adapted to fixed evaluation budgets, wall-clock/time budgets, or cost-weighted budgets.",
            "budget_constraint_handling": "Cost-aware variants weight acquisition by per-evaluation cost or use budgeted BO algorithms; not implemented in this paper's experiments but referenced as typical practice.",
            "breakthrough_discovery_metric": "BO can target large improvements via EI or capture novelty via information-based acquisitions; breakthrough detection depends on objective formulation.",
            "performance_metrics": null,
            "comparison_baseline": "",
            "performance_vs_baseline": "",
            "efficiency_gain": "",
            "tradeoff_analysis": "Authors note that Bayesian optimization typically requires many more experiments for global model convergenceâ€”potentially hundredsâ€”which may be infeasible in costly lab settings; this motivates their RSM-like allocation choice.",
            "optimal_allocation_findings": "BO is a powerful general approach but may be too experiment-hungry for tightly budgeted physical experiments; hybrid strategies (simulation early, experiments late) or cost-aware acquisitions are suggested to reduce needed physical runs.",
            "uuid": "e2429.6"
        },
        {
            "name_short": "Chimera",
            "name_full": "Chimera: hierarchy-based multi-objective optimization for self-driving laboratories",
            "brief_description": "A hierarchical multiobjective optimization approach for self-driving laboratories that prioritizes objectives through a hierarchy or ordering, enabling structured tradeoffs between competing experimental goals.",
            "citation_title": "Chimera: enabling hierarchy based multi-objective optimization for self-driving laboratories.",
            "mention_or_use": "mention",
            "system_name": "Chimera hierarchical multiobjective strategy",
            "system_description": "Chimera provides a way to impose hierarchical preferences among multiple objectives so that decision-making focuses on satisfying higher-priority goals before optimizing lower-priority onesâ€”a principled way to encode breakthrough potential or critical constraints into the acquisition policy.",
            "application_domain": "Self-driving laboratories and automated experimental optimization with multiple competing objectives.",
            "resource_allocation_strategy": "Allocates experiments to satisfy higher-priority objectives first, then refines lower-priority ones; can implicitly control allocation between exploitation of high-priority improvements and exploration for secondary objectives.",
            "computational_cost_metric": null,
            "information_gain_metric": null,
            "uses_information_gain": null,
            "exploration_exploitation_mechanism": "Explores tradeoffs according to hierarchical priorities rather than a flat Pareto-ranking; enables targeted search toward critical breakthroughs defined by hierarchy.",
            "diversity_mechanism": "Hierarchy can concentrate sampling on regions relevant to top-level objectives, which may reduce diversity for lower-level goals unless explicitly balanced.",
            "uses_diversity_promotion": null,
            "budget_constraint_type": "Applicable to fixed experimental budgets where priorities control allocation across objectives.",
            "budget_constraint_handling": "Hierarchy guides how limited experimental budget is apportioned among objectives, prioritizing higher-level goals.",
            "breakthrough_discovery_metric": "By promoting top-level objectives, Chimera can be used to prioritize experiments with higher breakthrough potential defined by hierarchy; specific metrics depend on objective definitions.",
            "performance_metrics": null,
            "comparison_baseline": "",
            "performance_vs_baseline": "",
            "efficiency_gain": "",
            "tradeoff_analysis": "Mentioned as a relevant multiobjective approach in the literature for self-driving labs; not used in this paper but highlighted as a technique that addresses structured objective tradeoffs.",
            "optimal_allocation_findings": "Not evaluated empirically in this paper; included as a follow-on example of multiobjective strategies that can encode priorities and thereby influence resource allocation toward breakthrough goals.",
            "uuid": "e2429.7"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chimera: enabling hierarchy based multi-objective optimization for self-driving laboratories.",
            "rating": 2,
            "sanitized_title": "chimera_enabling_hierarchy_based_multiobjective_optimization_for_selfdriving_laboratories"
        },
        {
            "paper_title": "BoTorch: A framework for efficient Monte-Carlo Bayesian optimization.",
            "rating": 2,
            "sanitized_title": "botorch_a_framework_for_efficient_montecarlo_bayesian_optimization"
        },
        {
            "paper_title": "Bayesian reaction optimization as a tool for chemical synthesis.",
            "rating": 2,
            "sanitized_title": "bayesian_reaction_optimization_as_a_tool_for_chemical_synthesis"
        },
        {
            "paper_title": "Optimal criteria and their asymptotic form for data selection in data-driven reduced-order modelling with Gaussian process regression.",
            "rating": 2,
            "sanitized_title": "optimal_criteria_and_their_asymptotic_form_for_data_selection_in_datadriven_reducedorder_modelling_with_gaussian_process_regression"
        },
        {
            "paper_title": "Active learning to overcome exponential-wall problem for effective structure prediction of chemical-disordered materials.",
            "rating": 2,
            "sanitized_title": "active_learning_to_overcome_exponentialwall_problem_for_effective_structure_prediction_of_chemicaldisordered_materials"
        }
    ],
    "cost": 0.01771,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>A framework for fully autonomous design of materials via multiobjective optimization and active learning: challenges and next steps
April 2023</p>
<p>Tyler H Chang tchang@anl.gov 
Mathematics and Computer Science Division
Argonne National Laboratory</p>
<p>Jakob R Elias jelias@anl.gov 
Applied Materials Division, Argonne National Laboratory</p>
<p>Stefan M Wild wild@lbl.gov 
Mathematics and Computer Science Division
Argonne National Laboratory</p>
<p>Applied Mathematics and Computational Research Division, Lawrence Berkeley National Laboratory</p>
<p>Santanu Chaudhuri schaudhuri@anl.gov 
Applied Materials Division, Argonne National Laboratory</p>
<p>Department of Civil, Materials, and Environmental Engineering
University of Illinois
Chicago</p>
<p>Joseph A Libera jlibera@anl.gov 
Applied Materials Division, Argonne National Laboratory</p>
<p>A framework for fully autonomous design of materials via multiobjective optimization and active learning: challenges and next steps
April 2023
In order to deploy machine learning in a real-world self-driving laboratory where data acquisition is costly and there are multiple competing design criteria, systems need to be able to intelligently sample while balancing performance trade-offs and constraints. For these reasons, we present an active learning process based on multiobjective black-box optimization with continuously updated machine learning models. This workflow is built on open-source technologies for real-time data streaming and modular multiobjective optimization software development. We demonstrate a proof of concept for this workflow through the autonomous operation of a continuous-flow chemistry laboratory, which identifies ideal manufacturing conditions for the electrolyte 2,2,2-trifluoroethyl methyl carbonate.</p>
<p>Introduction</p>
<p>Data-driven automated laboratories, also called self-driving laboratories, can significantly accelerate molecular synthesis and materials discovery. A key technical challenge of fully autonomous and artificial intelligenceassisted laboratory design is to effectively collect and utilize data from multiple complex processes in order to inform future experimentation. One long-standing template for utilizing experimental and simulation data is the multiresponse surface methodology (RSM) [13], whereby initial data sets are gathered through design of experiments and then statistical models are built for each quantity of interest, analyzed, and hypothesis tested iteratively.</p>
<p>In modern scientific and engineering settings, several common paradigms could be considered as specific implementations of this discovery framework, the most common of which are active learning and modelbased optimization techniques such as Bayesian optimization. To account for multiple competing criteria, we utilize an active learning framework based in multiobjective optimization, which utilizes surrogates (such as Gaussian processes), optimization solvers, and multicriteria data acquisition in a closed feedback loop.</p>
<p>Several definitions of active learning exist, including both adaptive sampling to model complex processes with uniform accuracy [15] and iterative selection of candidate designs to improve a global machine learning model that drives optimization convergence [18]. In this paper we use the latter definition, since global model accuracy is not a reasonable goal when using an extremely limited budget for experiments, as in our case.</p>
<p>We present our approach to integrate streaming experimental data from a real-world, fully automated continuous-flow chemistry setup into a customizable machine learning (ML) and multiobjective optimization framework for steering the design of battery electrolytes. In Section 2 we briefly summarize the problem background, focusing on relevant techniques and challenges. Section 3 introduces our framework for addressing these challenges, which is based on integration between the multiobjective optimization library ParMOO [5] and the data streaming platform for Manufacturing Data and Machine Learning (MDML) [9]. In Section 4 we present our results from the synthesis of the battery electrolyte 2,2,2-trifluoroetheyl methyl carbonate (TFMC) with an integrated automated feed, continuous-flow reactor (CFR), and nuclear magnetic resonance (NMR) setup. This experiment does not constitute novel material discovery, but in Section 5 we discuss how our framework will allow for extension to the discovery domain.</p>
<p>Background and Key Challenges</p>
<p>There is no shortage of options when it comes to multiobjective optimization solvers and libraries. Notable techniques include genetic algorithms [3], search-based methods [12], and Bayesian optimization [1]. We note that we are not the first paper to take a multiobjective approach to chemical synthesis [10,16]. However, setting up these solvers to integrate with scientific computing and laboratory environments is a nontrivial task, where each optimization software must be integrated as an optimization service [14] or function [6] in the broader library. This approach is appropriate for integrating with computer simulation environments; but in order to successfully integrate with heterogeneous data sources, a more flexible approach is needed [2].</p>
<p>Another challenge in applying optimization solvers for autonomous material discovery is exploiting structures to address problem complexity. Optimization solvers tend to perform well over continuous input spaces; but when some of the design variables are complex (such as chemical networks), the problem becomes combinatorial for classical solvers unless they are carefully tailored to the problem at hand. In the context of material discovery, the molecular structure can be embedded by using either its molecular descriptors [16] or problem-specific latent-space representations [11]. Similarly, after performing experiments, the raw outputs of these chemical processes may consist of large volumes of time series data or spectral measurements, and the majority of this information is lost when the data is postprocessed to derive meaningful objectives. However, modern simulation optimization solvers can take advantage of these raw simulation outputs as a method for exploiting the physical structure in the problem [17].</p>
<p>Thus, our challenge is to exploit problem-specific structures and embeddings in a way that generalizes to a variety of applications while modeling data from multiple chemical processes. This requires a multiobjective optimization framework that is flexible enough to utilize customized problem definitions, while still leveraging state-of-the-art modeling techniques, and that can be coupled with a flexible data-streaming service.</p>
<p>A Framework for Autonomous Experimentation</p>
<p>To address our challenges, we use the multiobjective optimization library ParMOO [5] to manage multiple surrogate models, exploit domain knowledge, and utilize problem embeddings through its customizable modular framework. The data-streaming service MDML [9] provides an event-driven architecture using an Apache Kafka instance with publishers and subscribers to pass data between laboratory workstations, servers, supercomputers, and more. Once an MDML instance has been created-typically on a server separate from experiments and machine learning codes-any number of experiments or clients can begin streaming data. Additional Kafka instances can be added to provide horizontal scaling if needed. By utilizing MDML as ParMOO's simulation distribution backend, we are able to stream scientific data from multiple sources, process that data, and issue requests for new experiments in a closed loop.</p>
<p>Additionally, both of these tools are open-source, lightweight Python libraries, which can be installed on a typical laboratory workstation. To integrate ParMOO with MDML, we have extended ParMOO's MOOP class, overwriting the solve method to produce Kafka requests for experimental data via MDML. This means that the MDML platform handles data streaming and recording through its usual methods and is responsible for issuing experiments or simulations, logging data, and storing results. In this work we used two servers running Kafka brokers as our MDML instance, providing FuncX [8] endpoints for our analysis tasks. We then used a lab workstation running LabVIEW (which directly connects to the CFR and NMR) and a compute node running a ParMOO solver instance as analysis client processes. Figure 5 in Appendix A.2 further illustrates this setup.</p>
<p>For more information on how ParMOO facilitates exploiting problem structure, the specific structure and components of its modeling and candidate selection, and the techniques used in this paper, see Appendix A.1. For more information on how MDML handles data streaming from heterogeneous sources, see Appendix A.2.</p>
<p>Chemical Synthesis with ParMOO and MDML</p>
<p>In this section we explore electrolyte production in an autonomously operated CFR using the framework from Section 3. Compared with traditional batch processing, CFRs can provide a means to rapidly test reagent combinations, experiment with high reaction temperatures, and measure yield and specificity when equipped with online characterization. However, manual operation can be very expensive, time-consuming, and inefficient. Thus, automating all aspects of the experimentation process is attractive in order to maximize equipment utilization and minimize labor. Furthermore, when coupled with ML algorithms, the total number of experiments required to reach optimal parameters is also minimized, in comparison with a traditional exhaustive search and experimental design techniques.</p>
<p>For this example we are interested in identifying ideal manufacturing conditions for the battery electrolyte TFMC in a CFR, based on the reaction illustrated in Figure 1. Note that in this example we are using a predetermined pair of reagents, solvent, and base and a limited range of values for the reaction conditions. Specifically, we must select an optimal flow rate, reaction temperature, and equivalence ratio for the two reagents. To reduce the reaction time and increase production speed, we would like the reaction to be carried out at high temperatures. However, high temperatures can also activate a side reaction that produces an unwanted byproduct and reduces the purity of the product. Therefore, we are seeking chemical mixtures and conditions that produce large amounts of TFMC but small amounts of the byproduct trifluoroethanol (TFE).</p>
<p>Trifluoroethanol</p>
<p>Methyl chloroformate 2,2,2-trifluoroethyl Methyl carbonate Solvent: acetonitrile Base: N,N-Diisopropylethylamine To summarize our self-driving laboratory setup, we created an MDML client process running a modified ParMOO solver, as described in Section 3, which produced requests for experiments and consumed experimental results. Specific ParMOO solver settings are given in Appendix A.1, and specific MDML settings are given in Appendix A.2. Experiment requests were consumed by a laboratory workstation running LabVIEW, which operates a multiport VICI valve controlling the flow of reagents into a Vaportec CFR. Outputs from the CFR were shunted into a Magritek NMR, which recorded peak areas for both the product (TFMC) and byproduct (TFE). The operation of this setup and collection of NMR data were controlled by a workstation running a control program written in LabVIEW and returned to the Kafka brokers via an MDML producer. Additional details on this lab setup are given in Appendix B.</p>
<p>In our case the correspondence of each peak to TFMC or TFE is determined by the ordering of the peaks, which had been identified as a suitable strategy for this experiment via prior experimentation and domain knowledge. Ultimately, only the peak areas are used in our objective calculations. However, this reduction sacrifices valuable information about the shape and location of each peak, which could potentially be used to improve surrogate model accuracy [17]. Additionally, we acknowledge that this might not be a suitable strategy in the general case.</p>
<p>Experiment: Optimal Manufacturing Conditions</p>
<p>Although one can experiment with different options, we have fixed the solvent and base for this experiment to a predetermined pairing that has shown promising results. We focus on identifying optimal manufacturing conditions, such as flow rates, reaction times, and reaction temperatures. In order to ensure safe operations of the CFR, upper and lower bound constraints were given for each of these values, as shown in Table 1.</p>
<p>Parameter</p>
<p>Lower bound Upper bound Temperature (degrees C) 40 150 Reaction time (seconds) 60 300 Equivalence ratio (no units) 0.9 2 Table 1: Design variables and bound constraints for initial experiment.</p>
<p>One of the key challenges for this problem is to achieve convergence on an extremely limited budget. Since we are running real experiments on a CFR, each experiment has significant costs in terms of financial cost, resources, and time. As examples, the raw materials (reagents, solvents, and bases) used in these experiments must have high purity, which comes at a significant financial cost; the machinery (CFR and NMR) have operating costs; and the experiments themselves are time-consuming, requiring up to 10 minutes each. Therefore, we cannot afford to perform hundreds of experiments in this setting, as would be typical in a Bayesian optimization feedback loop.</p>
<p>To directly control the balance between exploration and exploitation and encourage significant exploitation on a limited budget (while uncertainties are still high in some regions of the space), we favor an active learning implementation that is more similar to a traditional statistical RSM, where an initial design of experiments of predetermined size is allocated to drive exploration and initial model accuracy and then all future budget is allocated to exploiting our model, similarly as in [7]. We note that, in this strategy, global convergence is determined by the density of the initial design of experiments and is difficult to guarantee for an unknown function. However, this approach is also more flexible for extension to domain-specific surrogate modeling techniques, which will aid in the transition to chemical discovery [18].</p>
<p>ParMOO was instructed to maximize the product and minimize the byproduct subject to the constraints in Table 1. After a 15-point initial exploratory design of experiments, ParMOO was run for 9 additional model-exploiting iterations with a batch size of three experiments per iteration (i.e., 27 additional iterative experiments driven by the optimization framework). Because of lack of continued improvement over several batches of experiments, not all experiments allocated in the budget were carried to completion. In total, 41 experiments were completed; the corresponding data is shown in Figure 2. Since we are attempting to maximize the product (TFMC) and minimize the byproduct (TFE), the ideal solution would be located in the bottom-right corner. Note that the clustering of late (high index) runs in the bottom-right corner indicates convergence to optimal manufacturing conditions with an extremely limited budget. </p>
<p>Path Forward</p>
<p>In Section 4 we were successful in optimizing the manufacturing conditions for a predetermined material, but additional work is needed in order to broaden our scope to the original goal of material discovery. We fully anticipate that the code will continue to find more applications in both synthesis and discovery of materials. The fundamental method is fully extendable to multireactor, multistep reactions, with more complicated hardware configurations.</p>
<p>As an example, we can use a similar CFR setup to synthesize more complex materials such as metalorganic frameworks (MOFs), with the nucleation and connection of organic linkers to metal nodes being fully visible to the in-line NMR. However, the detection limits and reaction conditions for this problem generally take months to fully explore and optimize. The size of MOF libraries and linkers is increasing, but the convergence rates of high-throughput synthesis optimization tools are lacking. By utilizing a similar autonomous framework, we can extend these methods to reactors for making variations of linkers and MOFs and exploring their synthesis. This is a much larger problem than the one that has been described in the paper. To extend to this problem, we will need to utilize domain-specific latent space embeddings, structure-exploiting optimizers that consider the full spectrum of NMR data during surrogate modeling, and heterogeneous data sources including simulation (for early exploration) followed by experimental data (for late-stage testing). In order to keep costs reasonable and extend our limited budget, it could also be interesting to consider the cost of each experiment in the acquisition function. However, we must be careful when doing this in order to still allow for expensive but insightful experiments to be evaluated.</p>
<p>A Software Details</p>
<p>A.1 ParMOO</p>
<p>ParMOO is a framework for customizing and deploying multiobjective simulation optimization solvers [5].</p>
<p>The key distinction in ParMOO is the difference between a simulation and an objective, as depicted in  To create a ParMOO instance, users must first specify all design variables, data sources (e.g., experiments and simulations), problem constraints, and problem objectives. For complex design variable types, ParMOO is capable of automatically generating latent space embeddings or utilizing customized embedding tools. Next, users provide â€¢ any pre-existing data or a technique for generating an initial design of experiments;</p>
<p>â€¢ surrogate models for learning each of the simulation outputs;</p>
<p>â€¢ optimization solvers for the surrogate problem; and</p>
<p>â€¢ one or more data acquisition functions for determining how ParMOO will blend objectives.</p>
<p>When run, ParMOO will evaluate the initial experimental design, then iteratively generate batches of new experiments to be distributed for evaluation across all data sources, by following the standard response surface methodology. This feedback loop is illustrated in Figure 4. For further information on this process, see ParMOO's online documentation [4].  Figure 4: Program control flowchart, depicting how ParMOO combines user-provided techniques to optimize complex processes via active learning.</p>
<p>For the demonstration described in Section 4, ParMOO was configured to use a 15-point Latin hypercube design of experiments and a Gaussian RBF (the mean function for a Gaussian process) surrogate, and the surrogate problem was solved via generalized pattern search. Three acquisition functions were provided (resulting in candidate batches of size three), two of which used the epsilon-constraint method to scalarize the problem and the third of which used a fixed 50-50 weighting of the two objectives (i.e., maximize the product and minimize the byproduct).</p>
<p>A.2 MDML</p>
<p>The MDML platform provides cyberinfrastructure to standardize the research and operational environment to act on scientific data streams and integrate ML in the loop to steer or optimize experiments. MDML has been designed to meet the needs of in situ measurements for accelerating scalable materials manufacturing while providing capabilities that can be easily adapted to any scientific domain. MDML enables users to construct rich, data-oriented analysis pipelines that span disparate computational environments.</p>
<p>Underlying MDML is the widely used, distributed event streaming platform named Apache Kafka. This allows scientists using MDML to follow an event-driven architecture. Clients connected to the MDML's Kafka service are able to publish and subscribe to one or many data channels that they have created. Clients here include researchers themselves, software programs controlling/monitoring experiments, and ML models. Given this architecture, any relevant experiment data can be streamed anywhere and fed into client processes running ML models (such as ParMOO) in a unified format. Additionally, when the experimental data collection process can be fully automated (as with our integrated feed, CFR, and NMR setup), this enables a user-free feedback loop.</p>
<p>The architecture of our MDML instance utilizes two servers to run two Kafka brokers and their various components, databases for long-term storage of streaming data, and FuncX's [8] serverless compute endpoints for spawning analysis jobs that act on streamed data. A diagram of this architecture is shown in Figure 5.</p>
<p>In the diagram, the CFR is illustrated as a Python client since CFR data is collected locally via LabVIEW and streamed out via a Python script. The CFR also receives suggested experiments from ParMOO via a Python client and loads this information into LabVIEW. Here ParMOO is represented as an analysis worker. It receives results published from the CFR client, decides which experiment should be performed next, and publishes its own data, which is received by the CFR, completing the loop. The last important component to note here is Kafka, which is responsible for ushering produced data messages to the appropriate data consumers (i.e., CFR results to ParMOO and ParMOO's suggested experiments to CFR). Several MDML features shown in this diagram were not used for the automation of the CFR. These include the MQTT broker for collecting data from IoT devices and sensors; databases for storage; and Kafka Connect, which automatically converts data from the latter two sources to Kafka topics and vice versa. </p>
<p>B Experimental Methods</p>
<p>TO perform autonomous experimentation on the CFR using ParMOO and the MDML in a closed feedback loop, we created a MDML client process running a modified ParMOO solver, as described in Section 3. Next, we created an additional experiment-running client on a laboratory workstation that directly controlled the CFR. Then, the MDML host was run on a remote laboratory mainframe to broker requests and collect data.</p>
<p>As described in Section A.1, the ParMOO client consists of a ParMOO solver that was configured to use a 15-point Latin hypercube design of experiments, Gaussian RBF surrogate model, generalized pattern search optimizer, and three acquisition functions (two using the epsilon-constraint method for Pareto front exploration and the third using a fixed weighting of the two objectives). Following the active learning framework, the ParMOO client generated an initial design of experiments and then posted Kafka producer requests for these designs, listened for results data through a Kafka consumer, used those results to update its Gaussian models, optimized those models to target new experimental designs, and then posted new producer requests in an iterative feedback loop. In order to reduce experiment times on the CFR, experiment requests within a single batch were sorted by the reaction temperature. This process was beneficial since it takes additional time to stabilize temperatures when adjusting reaction temperatures by large amounts.</p>
<p>For the experiment client, a lab computer running the LabVIEW control program (LVCP) was configured to consume Kafka requests from the ParMOO client. The LVCP consumer polled for a producer request in the form of a valid experiment parameter test set. Whenever a valid experiment parameter test set was received from the ParMOO client, an experiment was set in motion by automated deployment of the provided set on the CFR and NMR setup, described in the next paragraph. After completion of a test, the LVCP posted the result as a Kafka producer and awaited the next data point as a consumer.</p>
<p>In order to perform the physical experiments, a CFR apparatus manufactured by Vaportec was used to carry out continuous-flow reactions. Inputs to the A and B channels of the Vaportec unit were provided by multiport VICI valves connecting up to 15 possible reagents to the CFR. The output from the CFR was shunted through a Magritek NMR equipped with a flow-through cell. The selector valves, Vaportec reactor, and Magritek NMR were fully controlled by using a Labview control program. A photograph illustrating this setup is given in Figure 6. In order to calibrate the equipment, the reactor was manually set up and NMR spectra tests were run on pure solvents. Operating parameter ranges were also set. Whenever a valid experiment parameter set was received, the experiment was initiated from LVCP. Steady state was determined by monitoring the integrated NMR peak areas provided by the Magritek software, and test completion was determined by the integrated peak area falling below a standard deviation criterion set in advance. To conclude a single experiment, data acquisition by the NMR required stopping of flow though its flow-through cell, whereas it was not possible to stop the flow in the CFR. Therefore a bypass loop was employed on the CFR in order to terminate each experiment.</p>
<p>C Open Access Statement</p>
<p>We acknowledge that it will not be possible for researchers to reproduce our results without access to highly specialized laboratory equipment. However, in order to facilitate open access and reproducibility of results as much as possible, the source code for using ParMOO to create MDML consumer/producer services and a variation of our script used for Section 4 (demonstrating solver settings, but not running the experiments) is publicly available at https://github.com/parmoo/cfr-materials. We have also included a copy of the raw MDML-recorded outputs (a JSON file) from our experiment, as well as a Python script for postprocessing these results to produce the plot in Figure 2.</p>
<p>The submitted manuscript has been created by UChicago Argonne, LLC, Operator of Argonne National Laboratory ("Argonne"). Argonne, a U.S. Department of Energy Office of Science laboratory, is operated under Contract No. DE-AC02-06CH11357. The U.S. Government retains for itself, and others acting on its behalf, a paid-up nonexclusive, irrevocable worldwide license in said article to reproduce, prepare derivative works, distribute copies to the public, and perform publicly and display publicly, by or on behalf of the Government. The Department of Energy will provide public access to these results of federally sponsored research in accordance with the DOE Public Access Plan. http://energy.gov/downloads/doe-public-access-plan</p>
<p>Figure 1 :
1Diagram of the basic reaction optimized for the production of the electrolyte 2,2,2-trifluoroethyl methyl carbonate (TFMC).</p>
<p>Figure 2 :
2Product (TFMC) vs. byproduct (TFE) areas recorded by NMR for 41 continuous-flow chemistry experiments steered by ParMOO using MDML. Point colors are coded by experiment index, with larger indices indicating later runs.</p>
<p>Figure A. 1 .
1This abstraction and the utilization of custom latent-space embedding tools are the primary mechanisms by which ParMOO exploits problem structure.</p>
<p>Figure 3 :
3ParMOO models complex processes separately from objectives, in order to exploit physical structure in how the objectives are defined.</p>
<p>Figure 5 :
5Diagram of the MDML architecture spread across two servers. Purple arrows represent data streams from producers and consumers.</p>
<p>Figure 6 :
6Our laboratory setup for performing the physical experiments. From left to right on the workbench are the lab workstation running the LVCP, the Vaportec CFR unit with automated feed, and the Magritek NMR recording results.
AcknowledgmentsThis material was based upon work supported by the U.S. Department of Energy, Office of Science, Office of Advanced Scientific Computing Research, Applied Mathematics and SciDAC programs under Contract Nos. DE-AC02-05CH11231 and DE-AC02-06CH11357 and by the Argonne LDRD program.
BoTorch: A framework for efficient Monte-Carlo Bayesian optimization. Maximilian Balandat, Brian Karrer, Daniel Jiang, Samuel Daulton, Ben Letham, G Andrew, Eytan Wilson, Bakshy, Advances in Neural Information Processing Systems. H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. LinCurran Associates, Inc33Maximilian Balandat, Brian Karrer, Daniel Jiang, Samuel Daulton, Ben Letham, Andrew G Wilson, and Eytan Bakshy. BoTorch: A framework for efficient Monte-Carlo Bayesian optimization. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 21524-21538. Curran Associates, Inc., 2020. URL https://proceedings. neurips.cc/paper/2020/file/f5b1b89d98b7286673128a5fb112cb9a-Paper.pdf.</p>
<p>Aldana-Montes. jMetalSP: A framework for dynamic multi-objective big data optimization. CristÃ³bal Barba-GonzÃ¡lez, JosÃ© GarcÃ­a-Nieto, Antonio J Nebro, JosÃ© A Cordero, Juan J Durillo, Ismael Navas-Delgado, F JosÃ©, 10.1016/j.asoc.2017.05.004Applied Soft Computing. 69CristÃ³bal Barba-GonzÃ¡lez, JosÃ© GarcÃ­a-Nieto, Antonio J. Nebro, JosÃ© A. Cordero, Juan J. Durillo, Ismael Navas-Delgado, and JosÃ© F. Aldana-Montes. jMetalSP: A framework for dynamic multi-objective big data optimization. Applied Soft Computing, 69:737-748, 2018. doi: https://doi.org/10.1016/j.asoc.2017. 05.004.</p>
<p>pymoo: Multi-objective optimization in Python. Julian Blank, Kalyanmoy Deb, 10.1109/ACCESS.2020.2990567IEEE Access. 8Julian Blank and Kalyanmoy Deb. pymoo: Multi-objective optimization in Python. IEEE Access, 8: 89497-89509, 2020. doi: 10.1109/ACCESS.2020.2990567.</p>
<p>ParMOO: Python library for parallel multiobjective simulation optimization. H Tyler, Stefan M Chang, Wild, Version 0.1.0Lemont, IL, USAArgonne National LaboratoryTechnical ReportTyler H. Chang and Stefan M. Wild. ParMOO: Python library for parallel multiobjective simulation optimization. Technical Report Version 0.1.0, Argonne National Laboratory, Lemont, IL, USA, 2022. URL https://parmoo.readthedocs.io/en/latest.</p>
<p>ParMOO: A Python library for parallel multiobjective simulation optimization. H Tyler, Stefan M Chang, Wild, 10.21105/joss.04468Journal of Open Source Software. 8824468Tyler H. Chang and Stefan M. Wild. ParMOO: A Python library for parallel multiobjective simulation optimization. Journal of Open Source Software, 8(82):4468, 2023. doi: 10.21105/joss.04468.</p>
<p>Managing computationally expensive blackbox multiobjective optimization problems using libEnsemble. H Tyler, Jeffrey Chang, Layne T Larson, Thomas C H Watson, Lux, 10.22360/SpringSim.2020.HPC.001Proc. 2020 Spring Simulation Conference (SpringSim 2020), the 28th High Performance Computing Symposium (HPC '20). 2020 Spring Simulation Conference (SpringSim 2020), the 28th High Performance Computing Symposium (HPC '20)2020Tyler H. Chang, Jeffrey Larson, Layne T. Watson, and Thomas C. H. Lux. Managing computationally expensive blackbox multiobjective optimization problems using libEnsemble. In Proc. 2020 Spring Simulation Conference (SpringSim 2020), the 28th High Performance Computing Symposium (HPC '20), pp. Article No. 31. SCS, 2020. doi: 10.22360/SpringSim.2020.HPC.001.</p>
<p>Algorithm 1028: VTMOP: Solver for blackbox multiobjective optimization problems. H Tyler, Layne T Chang, Jeffrey Watson, Nicole Larson, William I Neveu, Shubhangi Thacker, Thomas C H Deshpande, Lux, 10.1145/3529258ACM Transactions on Mathematical Software. 483Tyler H. Chang, Layne T. Watson, Jeffrey Larson, Nicole Neveu, William I. Thacker, Shubhangi Desh- pande, and Thomas C. H. Lux. Algorithm 1028: VTMOP: Solver for blackbox multiobjective opti- mization problems. ACM Transactions on Mathematical Software, 48(3):Article No. 36, 2022. doi: 10.1145/3529258.</p>
<p>funcX: A federated function serving fabric for science. Ryan Chard, Yadu Babuji, Zhuozhao Li, Tyler Skluzacek, Anna Woodard, Ben Blaiszik, Ian Foster, Kyle Chard, 10.1145/3369583.3392683Proc. 29th International Symposium on High-Performance Parallel and Distributed Computing (HPDC '20). 29th International Symposium on High-Performance Parallel and Distributed Computing (HPDC '20)ACM2020Ryan Chard, Yadu Babuji, Zhuozhao Li, Tyler Skluzacek, Anna Woodard, Ben Blaiszik, Ian Foster, and Kyle Chard. funcX: A federated function serving fabric for science. In Proc. 29th International Symposium on High-Performance Parallel and Distributed Computing (HPDC '20), pp. 65-76. ACM, 2020. doi: 10.1145/3369583.3392683.</p>
<p>The manufacturing data and machine learning platform: Enabling real-time monitoring and control of scientific experiments via IoT. Jakob R Elias, Ryan Chard, Joseph A Libera, Ian T Foster, Santanu Chaudhuri, 10.1109/WF-IoT48130.2020.9221078IEEE 6th World Forum on Internet of Things (WF-IoT). Jakob R. Elias, Ryan Chard, Joseph A. Libera, Ian T. Foster, and Santanu Chaudhuri. The manu- facturing data and machine learning platform: Enabling real-time monitoring and control of scientific experiments via IoT. 2020 IEEE 6th World Forum on Internet of Things (WF-IoT), pp. 1-2, 2020. doi: 10.1109/WF-IoT48130.2020.9221078.</p>
<p>Chimera: enabling hierarchy based multi-objective optimization for self-driving laboratories. Florian HÃ¤se, M LoÃ¯c, AlÃ¡n Roch, Aspuru-Guzik, doi: 10.1039/ C8SC02239AChemical science. 939Florian HÃ¤se, LoÃ¯c M Roch, and AlÃ¡n Aspuru-Guzik. Chimera: enabling hierarchy based multi-objective optimization for self-driving laboratories. Chemical science, 9(39):7642-7655, 2018. doi: 10.1039/ C8SC02239A.</p>
<p>Optimizing molecules using efficient queries from property evaluations. C Samuel, Vijil Hoffman, Kahini Chenthamarakshan, Pin-Yu Wadhawan, Payel Chen, Das, 10.1038/s42256-021-00422-yNature Machine Intelligence. 41Samuel C. Hoffman, Vijil Chenthamarakshan, Kahini Wadhawan, Pin-Yu Chen, and Payel Das. Opti- mizing molecules using efficient queries from property evaluations. Nature Machine Intelligence, 4(1): 21-31, 2022. doi: 10.1038/s42256-021-00422-y.</p>
<p>Algorithm 909: NOMAD: Nonlinear optimization with the MADS algorithm. SÃ©bastien Le Digabel, 10.1145/1916461.1916468ACM Transactions on Mathematical Software. 374SÃ©bastien Le Digabel. Algorithm 909: NOMAD: Nonlinear optimization with the MADS algorithm. ACM Transactions on Mathematical Software, 37(4):Article No. 44, 2011. doi: 10.1145/1916461. 1916468.</p>
<p>Response Surface Methodology: Process and Design Optimization Using Designed Experiments. H Raymond, Douglas C Myers, Christine M Montgomery, Anderson-Cook, John Wiley &amp; Sons, IncHoboken, NJ, USA4 edition. ISBN 9781118916032Raymond H. Myers, Douglas C. Montgomery, and Christine M. Anderson-Cook. Response Surface Methodology: Process and Design Optimization Using Designed Experiments. John Wiley &amp; Sons, Inc., Hoboken, NJ, USA, 4 edition, 2016. ISBN 9781118916032.</p>
<p>Global deterministic and stochastic optimization in a service oriented architecture. Chaitra Raghunath, Tyler H Chang, Layne T Watson, Mohamad Jrad, K Rakesh, Raymond M Kapania, Kolonay, 10.22360/springsim.2017.hpc.023the 25th High Performance Computing Symposium (HPC '17). Virginia Beach, VA, USAProc. 2017 Spring Simulation ConferenceChaitra Raghunath, Tyler H. Chang, Layne T. Watson, Mohamad Jrad, Rakesh K. Kapania, and Raymond M. Kolonay. Global deterministic and stochastic optimization in a service oriented archi- tecture. In Proc. 2017 Spring Simulation Conference (SpringSim 2017), the 25th High Performance Computing Symposium (HPC '17), pp. Article No. 7, Virginia Beach, VA, USA, 2017. SCS. doi: 10.22360/springsim.2017.hpc.023.</p>
<p>Optimal criteria and their asymptotic form for data selection in data-driven reduced-order modelling with Gaussian process regression. P Themistoklis, Antoine Sapsis, Blanchard, 10.1098/rsta.2021.0197Philosophical Transactions of the Royal Society A. 3802022Themistoklis P. Sapsis and Antoine Blanchard. Optimal criteria and their asymptotic form for data selection in data-driven reduced-order modelling with Gaussian process regression. Philosophical Trans- actions of the Royal Society A, 380(2229):20210197, 2022. doi: 10.1098/rsta.2021.0197.</p>
<p>Bayesian reaction optimization as a tool for chemical synthesis. J Benjamin, Jason Shields, Jun Stevens, Marvin Li, Farhan Parasram, Jesus I M Damani, Jacob M Alvarado, Rryan P Janey, Abigail G Adams, Doyle, 10.1038/s41586-021-03213-yNature. 5907844Benjamin J. Shields, Jason Stevens, Jun Li, Marvin Parasram, Farhan Damani, Jesus I. M. Alvarado, Jacob M. Janey, Rryan P. Adams, and Abigail G. Doyle. Bayesian reaction optimization as a tool for chemical synthesis. Nature, 590(7844):89-96, 2021. doi: 10.1038/s41586-021-03213-y.</p>
<p>Solving derivative-free nonlinear least squares problems with POUNDERS. Stefan M Wild, 10.1137/1.9781611974683.ch40Advances and Trends in Optimization with Engineering Applications. Tamas Terlaky, Miguel F. Anjos, and Shabbir AhmedStefan M. Wild. Solving derivative-free nonlinear least squares problems with POUNDERS. In Tamas Terlaky, Miguel F. Anjos, and Shabbir Ahmed (eds.), Advances and Trends in Optimization with Engi- neering Applications, pp. 529-540. SIAM, 2017. doi: 10.1137/1.9781611974683.ch40.</p>
<p>Active learning to overcome exponential-wall problem for effective structure prediction of chemical-disordered materials. Xiaoze Yuan, Yuwei Zhou, Qing Peng, Yong Yang, Yongwang Li, Xiaodong Wen, Nature Computational Materials. 9112Xiaoze Yuan, Yuwei Zhou, Qing Peng, Yong Yang, Yongwang Li, and Xiaodong Wen. Active learning to overcome exponential-wall problem for effective structure prediction of chemical-disordered materials. Nature Computational Materials, 9(1):12, 2023.</p>            </div>
        </div>

    </div>
</body>
</html>
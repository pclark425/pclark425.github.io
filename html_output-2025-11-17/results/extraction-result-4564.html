<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4564 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4564</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4564</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-98.html">extraction-schema-98</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <p><strong>Paper ID:</strong> paper-512c3a8b0d5462fe8a98a1713a02d1a3186e3aae</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/512c3a8b0d5462fe8a98a1713a02d1a3186e3aae" target="_blank">Are LLMs Ready for Real-World Materials Discovery?</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) create exciting possibilities for powerful language processing tools to accelerate research in materials science. While LLMs have great potential to accelerate materials understanding and discovery, they currently fall short in being practical materials science tools. In this position paper, we show relevant failure cases of LLMs in materials science that reveal current limitations of LLMs related to comprehending and reasoning over complex, interconnected materials science knowledge. Given those shortcomings, we outline a framework for developing Materials Science LLMs (MatSci-LLMs) that are grounded in materials science knowledge and hypothesis generation followed by hypothesis testing. The path to attaining performant MatSci-LLMs rests in large part on building high-quality, multi-modal datasets sourced from scientific literature where various information extraction challenges persist. As such, we describe key materials science information extraction challenges which need to be overcome in order to build large-scale, multi-modal datasets that capture valuable materials science knowledge. Finally, we outline a roadmap for applying future MatSci-LLMs for real-world materials discovery via: 1. Automated Knowledge Base Generation; 2. Automated In-Silico Material Design; and 3. MatSci-LLM Integrated Self-Driving Materials Laboratories.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4564.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4564.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MatSci-LLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Materials Science Large Language Model (MatSci-LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proposed class of domain-grounded LLMs tailored to materials science intended to ingest multi-modal scientific literature, generate hypotheses, and surface domain principles, experimental procedures, and design rules to accelerate materials discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Proposed framework: train/finetune LLMs on high-quality, multi-modal corpora (text, tables, figures, CIF/XYZ files, code) combined with distant supervision, regex-based tools (e.g., Robocrystallographer, ChemDataExtractor), in-context learning and instruction finetuning, linking entities across documents and externalizing knowledge (knowledge graphs / annotations). The MatSci-LLM is meant to operate in a 6-step discovery loop (Materials Query → Data Retrieval → Materials Design → In-silico Evaluation → Experiment Planning → Experiment Execution) and to produce grounded hypotheses / design principles by synthesizing evidence across sources.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>materials science</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td>qualitative hypotheses, domain principles, empirical relationships, design rules and experimentally-actionable protocols (i.e., mechanistic or empirical generalizations derived from literature synthesis)</td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Paper proposes that MatSci-LLMs could distill hypotheses and domain relationships from the literature if trained on curated, multi-modal datasets and augmented with grounding/external knowledge; highlights that current general LLMs show partial domain knowledge but lack reliable grounded reasoning required to produce correct hypotheses or rules.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Lack of access to paywalled machine-readable corpora, inconsistent domain notations, incomplete descriptions across papers, failure modes of LLMs (hallucination, poor numerical grounding, incorrect 3D-structure reasoning), multi-modal linking, and need for large-scale, expert-curated annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Are LLMs Ready for Real-World Materials Discovery?', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4564.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4564.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dagdelen et al. 2024</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Structured information extraction from scientific text with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited study reporting that LLMs can be used to extract structured information from scientific text; the current paper cites it as evidence that in-context learning with LLMs is effective for information extraction in materials science.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Structured information extraction from scientific text with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>LLM-based structured information extraction (in-context prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>The paper references this work as using modern LLMs' in-context learning abilities to extract structured facts from scientific literature (no implementation details provided in this paper). The cited approach is presented as an accelerator for processing materials science text.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>materials science / scientific literature</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td>structured factual information and relationships (experimental facts, property values, procedural steps) rather than formal physical laws; supports extraction of domain knowledge used for downstream tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>In-context learning is highlighted as a powerful tool to speed up information extraction from materials text; represents progress over prior targeted text-mining approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>The paper notes that further work is required to improve methods and that current LLMs still have reasoning and grounding limitations when extracting complex, multi-source scientific knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Are LLMs Ready for Real-World Materials Discovery?', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4564.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4564.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Cox et al. (Wikicrow)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Wikicrow: Automating synthesis of human scientific knowledge</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited work that used LLM methods to generate structured annotations for biological entities by cross-checking generated information against the scientific literature, demonstrating externalization of LLM-derived knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Wikicrow: Automating synthesis of human scientific knowledge</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>LLM-generated annotations with citation cross-checking</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>As cited, authors generated annotations for >15,000 protein-coding entries using robust LLM-driven pipelines that rely on citations and literature cross-checks to externalize LLM knowledge into structured databases.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>biology / genomics (protein annotation)</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td>structured factual annotations and knowledge statements (not formal laws) synthesized from papers and citations</td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>citation cross-checking (as described in this paper's summary)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Shows feasibility of externalizing LLM knowledge into structured annotations and databases that rely on literature evidence and citation-based verification.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Requires robust cross-checking to limit hallucination; domain-specific evaluation and verification remain necessary.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Are LLMs Ready for Real-World Materials Discovery?', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4564.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4564.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Buehler 2023/2024</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative retrieval-augmented ontologic graph / MechGPT (Buehler)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited contributions that externalize LLM knowledge into ontological graphs and multi-agent strategies for interpretive LLM-based materials design, enabling structured probing of LLM-derived knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Generative retrieval-augmented ontologic graph and multiagent strategies for interpretive large language model-based materials design</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Retrieval-augmented generation + ontology / knowledge-graph externalization</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Paper cites Buehler's approach to externalize LLM internal knowledge into interpretable structures (knowledge graphs / ontologies) using retrieval augmentation so that scientists can probe, correct, and refine model outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>materials science / mechanics</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td>domain relationships and interpretable knowledge graph nodes (empirical relationships, mechanistic links) rather than formal symbolic laws</td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Externalizing LLM knowledge into structured graphs can make model outputs probeable and correctable by experts, aiding trust and interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Building reliable ontologies and ensuring fidelity to literature evidence to avoid hallucinated relations is challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Are LLMs Ready for Real-World Materials Discovery?', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4564.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4564.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Venugopal & Olivetti 2024 (MatKG)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MatKG: An autonomously generated knowledge graph in material science</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited work that demonstrates automatic generation of a materials knowledge graph (MatKG) from literature, i.e., externalizing extracted relations and facts into a structured graph form.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>MatKG: An autonomously generated knowledge graph in material science</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Automated knowledge-graph generation from literature</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Described in this paper as an example of externalizing LLM-derived or NLP-extracted knowledge into a structured, human-interpretable graph for probing by scientists.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>materials science</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td>empirical relationships and associations between materials, properties, and methods (knowledge-graph triples)</td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Automated KG construction can help make literature-derived relationships accessible for human inspection and downstream use.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Linking entities correctly across documents and verifying relations against references remains difficult.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Are LLMs Ready for Real-World Materials Discovery?', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4564.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4564.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Rubungo et al. 2023 (LLM-Prop)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-prop: Predicting physical and electronic properties of crystalline solids from their text descriptions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited preprint that uses LLMs to predict material properties from textual descriptions, demonstrating LLMs can extract property-pattern mappings from text.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>LLM-prop: Predicting physical and electronic properties of crystalline solids from their text descriptions</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Text-to-property prediction using LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Paper cites this work as an example where LLMs map textual descriptions of crystalline materials to predicted physical/electronic properties (method details not given in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>materials science (crystalline solids)</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td>empirical mappings from textual descriptions to quantitative properties (pattern extraction relating structure/description → properties)</td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLMs can be leveraged to infer material properties from descriptive text, suggesting potential for extracting property-related patterns from literature.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Paper notes LLMs' grounding and numeric accuracy limits; mapping text to reliable quantitative properties requires careful curation and verification.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Are LLMs Ready for Real-World Materials Discovery?', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4564.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4564.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gruver et al. 2023</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fine-tuned language models generate stable inorganic materials as text</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited work demonstrating that fine-tuned language models can generate CIF-formatted descriptions of inorganic materials, indicating LLMs can produce structured material representations from text.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Fine-tuned language models generate stable inorganic materials as text</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Fine-tuned LMs for CIF/text generation</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>As cited, fine-tuned LMs were used to generate materials in CIF/text form; the current paper notes that such models can generate CIF-format materials but may fail to provide contextual interpretation of CIF details.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>materials science / inorganic materials</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td>textual/structural representations of materials (not explicit laws), enabling downstream extraction of structure-property relations</td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Fine-tuned LMs can produce syntactically-correct CIFs but often lack the interpretive context that explains the significance of generated structure entries.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>LLMs may generate plausible but ungrounded CIF entries and fail to reliably describe their physical implications.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Are LLMs Ready for Real-World Materials Discovery?', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4564.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e4564.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zaki et al. 2024 (MaScQA)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MaScQA: Investigating materials science knowledge of large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited evaluation study that assessed LLMs' knowledge on undergraduate-level materials science questions and on code generation tasks, revealing reasoning and numerical-grounding weaknesses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mascqa: Investigating materials science knowledge of large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (with Chain-of-Thought reported)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Question-answering and code-generation evaluation (MaScQA dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Created a dataset of 650 undergraduate-level questions and code tasks to probe LLMs' domain knowledge and reasoning on materials science; evaluated models' QA and code outputs (as reported in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>dataset of 650 questions (not number of input papers)</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>materials science (education-level QA and code tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td>not direct law extraction — used to probe conceptual understanding and ability to reason about domain principles (e.g., crystal symmetry, diffraction selection rules)</td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Benchmark QA and code-generation evaluation against correct answers and human performance</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>GPT-4 with chain-of-thought: 62% overall accuracy on 650 questions; numerical questions: 39% accuracy; GPT-4 code-writing accuracy: ~71% (as cited in this paper); GPT-4 reached ~50% of top-performing human score and did not pass the exam cutoff.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Human performance on same exam (top humans) and other LLM baselines; GPT-4 outperformed other LLM baselines but lagged behind top humans.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLMs show partial domain knowledge but systematic failures: poor numerical grounding, unit/conversion errors, incorrect substitution of values, and misinterpretation of 3D/crystallographic structure information which undermines reliable extraction of domain principles or hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Reasoning inconsistency, numerical grounding errors, inability to reliably interpret crystal symmetries, and hallucination of references; thus current LLMs are not yet reliable for producing correct domain laws/hypotheses without human oversight.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Are LLMs Ready for Real-World Materials Discovery?', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4564.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e4564.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Song et al. 2023b (Honeybee)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Honeybee: Progressive instruction finetuning of large language models for materials science</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited work demonstrating instruction finetuning of general LLMs for materials science tasks, leveraging in-context learning to improve performance on domain extraction tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Honeybee: Progressive instruction finetuning of large language models for materials science</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Instruction finetuning with in-context learning</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>As cited, instruction finetuning of LLMs using domain-specific prompts and examples (progressive approach) to improve performance on tasks such as named-entity recognition, synthesis action retrieval, and composition extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>materials science (NLP tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td>task-oriented extraction (entities, actions, relations) that can serve as building blocks for deriving higher-level principles from aggregated literature</td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Instruction finetuning plus in-context learning can improve LLM performance on domain tasks, enabling more effective extraction of structured materials information.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Scalability limited by availability of high-quality annotated examples; persistent difficulties in multi-modal and multi-document grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Are LLMs Ready for Real-World Materials Discovery?', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4564.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e4564.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gupta et al. 2023 (DiSCoMaT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DiSCoMaT: Distantly supervised composition extraction from tables in materials science articles</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited approach using distant supervision methods (not necessarily pure LLMs) to extract composition and tabular data from materials literature at scale, used as a component in building multi-modal corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>DiSCoMaT: Distantly supervised composition extraction from tables in materials science articles</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Distant supervision for table composition extraction</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Uses distant supervision signals combined with NLP tools to extract composition data from tables in the literature; recommended in the paper as a scalable approach that can complement LLM/data-generation methods.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>materials science (tables extraction)</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td>structured property/composition facts (empirical data) that can be aggregated to identify patterns or empirical relationships</td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Paper cites an extraction comparison: GPT-4 extracted ~55% of materials properties from a table vs. ~73% by a model trained specifically for the task (Gupta et al., 2023) — reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Specialized table-extraction model (Gupta et al., 2023) which outperformed GPT-4 on table property extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Distant supervision can scale extraction of tabular composition/property data; specialized models currently outperform off-the-shelf LLMs on table-based extractions.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Tables often incomplete or require grounding with surrounding text and arithmetic (e.g., inferring missing component percentages), posing challenges for LLMs and automated extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Are LLMs Ready for Real-World Materials Discovery?', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Structured information extraction from scientific text with large language models <em>(Rating: 2)</em></li>
                <li>Wikicrow: Automating synthesis of human scientific knowledge <em>(Rating: 2)</em></li>
                <li>Generative retrieval-augmented ontologic graph and multiagent strategies for interpretive large language model-based materials design <em>(Rating: 2)</em></li>
                <li>MatKG: An autonomously generated knowledge graph in material science <em>(Rating: 2)</em></li>
                <li>LLM-prop: Predicting physical and electronic properties of crystalline solids from their text descriptions <em>(Rating: 2)</em></li>
                <li>Fine-tuned language models generate stable inorganic materials as text <em>(Rating: 2)</em></li>
                <li>Mascqa: Investigating materials science knowledge of large language models <em>(Rating: 2)</em></li>
                <li>Honeybee: Progressive instruction finetuning of large language models for materials science <em>(Rating: 2)</em></li>
                <li>DiSCoMaT: Distantly supervised composition extraction from tables in materials science articles <em>(Rating: 2)</em></li>
                <li>14 examples of how llms can transform materials science and chemistry: a reflection on a large language model hackathon <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4564",
    "paper_id": "paper-512c3a8b0d5462fe8a98a1713a02d1a3186e3aae",
    "extraction_schema_id": "extraction-schema-98",
    "extracted_data": [
        {
            "name_short": "MatSci-LLM",
            "name_full": "Materials Science Large Language Model (MatSci-LLM)",
            "brief_description": "A proposed class of domain-grounded LLMs tailored to materials science intended to ingest multi-modal scientific literature, generate hypotheses, and surface domain principles, experimental procedures, and design rules to accelerate materials discovery.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "method_name": null,
            "method_description": "Proposed framework: train/finetune LLMs on high-quality, multi-modal corpora (text, tables, figures, CIF/XYZ files, code) combined with distant supervision, regex-based tools (e.g., Robocrystallographer, ChemDataExtractor), in-context learning and instruction finetuning, linking entities across documents and externalizing knowledge (knowledge graphs / annotations). The MatSci-LLM is meant to operate in a 6-step discovery loop (Materials Query → Data Retrieval → Materials Design → In-silico Evaluation → Experiment Planning → Experiment Execution) and to produce grounded hypotheses / design principles by synthesizing evidence across sources.",
            "number_of_papers": null,
            "domain_or_field": "materials science",
            "type_of_laws_extracted": "qualitative hypotheses, domain principles, empirical relationships, design rules and experimentally-actionable protocols (i.e., mechanistic or empirical generalizations derived from literature synthesis)",
            "example_laws_extracted": null,
            "evaluation_method": null,
            "performance_metrics": null,
            "comparison_baseline": null,
            "key_findings": "Paper proposes that MatSci-LLMs could distill hypotheses and domain relationships from the literature if trained on curated, multi-modal datasets and augmented with grounding/external knowledge; highlights that current general LLMs show partial domain knowledge but lack reliable grounded reasoning required to produce correct hypotheses or rules.",
            "challenges_limitations": "Lack of access to paywalled machine-readable corpora, inconsistent domain notations, incomplete descriptions across papers, failure modes of LLMs (hallucination, poor numerical grounding, incorrect 3D-structure reasoning), multi-modal linking, and need for large-scale, expert-curated annotations.",
            "uuid": "e4564.0",
            "source_info": {
                "paper_title": "Are LLMs Ready for Real-World Materials Discovery?",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Dagdelen et al. 2024",
            "name_full": "Structured information extraction from scientific text with large language models",
            "brief_description": "A cited study reporting that LLMs can be used to extract structured information from scientific text; the current paper cites it as evidence that in-context learning with LLMs is effective for information extraction in materials science.",
            "citation_title": "Structured information extraction from scientific text with large language models",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "method_name": "LLM-based structured information extraction (in-context prompting)",
            "method_description": "The paper references this work as using modern LLMs' in-context learning abilities to extract structured facts from scientific literature (no implementation details provided in this paper). The cited approach is presented as an accelerator for processing materials science text.",
            "number_of_papers": null,
            "domain_or_field": "materials science / scientific literature",
            "type_of_laws_extracted": "structured factual information and relationships (experimental facts, property values, procedural steps) rather than formal physical laws; supports extraction of domain knowledge used for downstream tasks.",
            "example_laws_extracted": null,
            "evaluation_method": null,
            "performance_metrics": null,
            "comparison_baseline": null,
            "key_findings": "In-context learning is highlighted as a powerful tool to speed up information extraction from materials text; represents progress over prior targeted text-mining approaches.",
            "challenges_limitations": "The paper notes that further work is required to improve methods and that current LLMs still have reasoning and grounding limitations when extracting complex, multi-source scientific knowledge.",
            "uuid": "e4564.1",
            "source_info": {
                "paper_title": "Are LLMs Ready for Real-World Materials Discovery?",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Cox et al. (Wikicrow)",
            "name_full": "Wikicrow: Automating synthesis of human scientific knowledge",
            "brief_description": "Cited work that used LLM methods to generate structured annotations for biological entities by cross-checking generated information against the scientific literature, demonstrating externalization of LLM-derived knowledge.",
            "citation_title": "Wikicrow: Automating synthesis of human scientific knowledge",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "method_name": "LLM-generated annotations with citation cross-checking",
            "method_description": "As cited, authors generated annotations for &gt;15,000 protein-coding entries using robust LLM-driven pipelines that rely on citations and literature cross-checks to externalize LLM knowledge into structured databases.",
            "number_of_papers": null,
            "domain_or_field": "biology / genomics (protein annotation)",
            "type_of_laws_extracted": "structured factual annotations and knowledge statements (not formal laws) synthesized from papers and citations",
            "example_laws_extracted": null,
            "evaluation_method": "citation cross-checking (as described in this paper's summary)",
            "performance_metrics": null,
            "comparison_baseline": null,
            "key_findings": "Shows feasibility of externalizing LLM knowledge into structured annotations and databases that rely on literature evidence and citation-based verification.",
            "challenges_limitations": "Requires robust cross-checking to limit hallucination; domain-specific evaluation and verification remain necessary.",
            "uuid": "e4564.2",
            "source_info": {
                "paper_title": "Are LLMs Ready for Real-World Materials Discovery?",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Buehler 2023/2024",
            "name_full": "Generative retrieval-augmented ontologic graph / MechGPT (Buehler)",
            "brief_description": "Cited contributions that externalize LLM knowledge into ontological graphs and multi-agent strategies for interpretive LLM-based materials design, enabling structured probing of LLM-derived knowledge.",
            "citation_title": "Generative retrieval-augmented ontologic graph and multiagent strategies for interpretive large language model-based materials design",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "method_name": "Retrieval-augmented generation + ontology / knowledge-graph externalization",
            "method_description": "Paper cites Buehler's approach to externalize LLM internal knowledge into interpretable structures (knowledge graphs / ontologies) using retrieval augmentation so that scientists can probe, correct, and refine model outputs.",
            "number_of_papers": null,
            "domain_or_field": "materials science / mechanics",
            "type_of_laws_extracted": "domain relationships and interpretable knowledge graph nodes (empirical relationships, mechanistic links) rather than formal symbolic laws",
            "example_laws_extracted": null,
            "evaluation_method": null,
            "performance_metrics": null,
            "comparison_baseline": null,
            "key_findings": "Externalizing LLM knowledge into structured graphs can make model outputs probeable and correctable by experts, aiding trust and interpretability.",
            "challenges_limitations": "Building reliable ontologies and ensuring fidelity to literature evidence to avoid hallucinated relations is challenging.",
            "uuid": "e4564.3",
            "source_info": {
                "paper_title": "Are LLMs Ready for Real-World Materials Discovery?",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Venugopal & Olivetti 2024 (MatKG)",
            "name_full": "MatKG: An autonomously generated knowledge graph in material science",
            "brief_description": "Cited work that demonstrates automatic generation of a materials knowledge graph (MatKG) from literature, i.e., externalizing extracted relations and facts into a structured graph form.",
            "citation_title": "MatKG: An autonomously generated knowledge graph in material science",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "method_name": "Automated knowledge-graph generation from literature",
            "method_description": "Described in this paper as an example of externalizing LLM-derived or NLP-extracted knowledge into a structured, human-interpretable graph for probing by scientists.",
            "number_of_papers": null,
            "domain_or_field": "materials science",
            "type_of_laws_extracted": "empirical relationships and associations between materials, properties, and methods (knowledge-graph triples)",
            "example_laws_extracted": null,
            "evaluation_method": null,
            "performance_metrics": null,
            "comparison_baseline": null,
            "key_findings": "Automated KG construction can help make literature-derived relationships accessible for human inspection and downstream use.",
            "challenges_limitations": "Linking entities correctly across documents and verifying relations against references remains difficult.",
            "uuid": "e4564.4",
            "source_info": {
                "paper_title": "Are LLMs Ready for Real-World Materials Discovery?",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Rubungo et al. 2023 (LLM-Prop)",
            "name_full": "LLM-prop: Predicting physical and electronic properties of crystalline solids from their text descriptions",
            "brief_description": "Cited preprint that uses LLMs to predict material properties from textual descriptions, demonstrating LLMs can extract property-pattern mappings from text.",
            "citation_title": "LLM-prop: Predicting physical and electronic properties of crystalline solids from their text descriptions",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "method_name": "Text-to-property prediction using LLMs",
            "method_description": "Paper cites this work as an example where LLMs map textual descriptions of crystalline materials to predicted physical/electronic properties (method details not given in this paper).",
            "number_of_papers": null,
            "domain_or_field": "materials science (crystalline solids)",
            "type_of_laws_extracted": "empirical mappings from textual descriptions to quantitative properties (pattern extraction relating structure/description → properties)",
            "example_laws_extracted": null,
            "evaluation_method": null,
            "performance_metrics": null,
            "comparison_baseline": null,
            "key_findings": "LLMs can be leveraged to infer material properties from descriptive text, suggesting potential for extracting property-related patterns from literature.",
            "challenges_limitations": "Paper notes LLMs' grounding and numeric accuracy limits; mapping text to reliable quantitative properties requires careful curation and verification.",
            "uuid": "e4564.5",
            "source_info": {
                "paper_title": "Are LLMs Ready for Real-World Materials Discovery?",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Gruver et al. 2023",
            "name_full": "Fine-tuned language models generate stable inorganic materials as text",
            "brief_description": "Cited work demonstrating that fine-tuned language models can generate CIF-formatted descriptions of inorganic materials, indicating LLMs can produce structured material representations from text.",
            "citation_title": "Fine-tuned language models generate stable inorganic materials as text",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "method_name": "Fine-tuned LMs for CIF/text generation",
            "method_description": "As cited, fine-tuned LMs were used to generate materials in CIF/text form; the current paper notes that such models can generate CIF-format materials but may fail to provide contextual interpretation of CIF details.",
            "number_of_papers": null,
            "domain_or_field": "materials science / inorganic materials",
            "type_of_laws_extracted": "textual/structural representations of materials (not explicit laws), enabling downstream extraction of structure-property relations",
            "example_laws_extracted": null,
            "evaluation_method": null,
            "performance_metrics": null,
            "comparison_baseline": null,
            "key_findings": "Fine-tuned LMs can produce syntactically-correct CIFs but often lack the interpretive context that explains the significance of generated structure entries.",
            "challenges_limitations": "LLMs may generate plausible but ungrounded CIF entries and fail to reliably describe their physical implications.",
            "uuid": "e4564.6",
            "source_info": {
                "paper_title": "Are LLMs Ready for Real-World Materials Discovery?",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Zaki et al. 2024 (MaScQA)",
            "name_full": "MaScQA: Investigating materials science knowledge of large language models",
            "brief_description": "Cited evaluation study that assessed LLMs' knowledge on undergraduate-level materials science questions and on code generation tasks, revealing reasoning and numerical-grounding weaknesses.",
            "citation_title": "Mascqa: Investigating materials science knowledge of large language models",
            "mention_or_use": "use",
            "model_name": "GPT-4 (with Chain-of-Thought reported)",
            "model_size": null,
            "method_name": "Question-answering and code-generation evaluation (MaScQA dataset)",
            "method_description": "Created a dataset of 650 undergraduate-level questions and code tasks to probe LLMs' domain knowledge and reasoning on materials science; evaluated models' QA and code outputs (as reported in this paper).",
            "number_of_papers": "dataset of 650 questions (not number of input papers)",
            "domain_or_field": "materials science (education-level QA and code tasks)",
            "type_of_laws_extracted": "not direct law extraction — used to probe conceptual understanding and ability to reason about domain principles (e.g., crystal symmetry, diffraction selection rules)",
            "example_laws_extracted": null,
            "evaluation_method": "Benchmark QA and code-generation evaluation against correct answers and human performance",
            "performance_metrics": "GPT-4 with chain-of-thought: 62% overall accuracy on 650 questions; numerical questions: 39% accuracy; GPT-4 code-writing accuracy: ~71% (as cited in this paper); GPT-4 reached ~50% of top-performing human score and did not pass the exam cutoff.",
            "comparison_baseline": "Human performance on same exam (top humans) and other LLM baselines; GPT-4 outperformed other LLM baselines but lagged behind top humans.",
            "key_findings": "LLMs show partial domain knowledge but systematic failures: poor numerical grounding, unit/conversion errors, incorrect substitution of values, and misinterpretation of 3D/crystallographic structure information which undermines reliable extraction of domain principles or hypotheses.",
            "challenges_limitations": "Reasoning inconsistency, numerical grounding errors, inability to reliably interpret crystal symmetries, and hallucination of references; thus current LLMs are not yet reliable for producing correct domain laws/hypotheses without human oversight.",
            "uuid": "e4564.7",
            "source_info": {
                "paper_title": "Are LLMs Ready for Real-World Materials Discovery?",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Song et al. 2023b (Honeybee)",
            "name_full": "Honeybee: Progressive instruction finetuning of large language models for materials science",
            "brief_description": "Cited work demonstrating instruction finetuning of general LLMs for materials science tasks, leveraging in-context learning to improve performance on domain extraction tasks.",
            "citation_title": "Honeybee: Progressive instruction finetuning of large language models for materials science",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "method_name": "Instruction finetuning with in-context learning",
            "method_description": "As cited, instruction finetuning of LLMs using domain-specific prompts and examples (progressive approach) to improve performance on tasks such as named-entity recognition, synthesis action retrieval, and composition extraction.",
            "number_of_papers": null,
            "domain_or_field": "materials science (NLP tasks)",
            "type_of_laws_extracted": "task-oriented extraction (entities, actions, relations) that can serve as building blocks for deriving higher-level principles from aggregated literature",
            "example_laws_extracted": null,
            "evaluation_method": null,
            "performance_metrics": null,
            "comparison_baseline": null,
            "key_findings": "Instruction finetuning plus in-context learning can improve LLM performance on domain tasks, enabling more effective extraction of structured materials information.",
            "challenges_limitations": "Scalability limited by availability of high-quality annotated examples; persistent difficulties in multi-modal and multi-document grounding.",
            "uuid": "e4564.8",
            "source_info": {
                "paper_title": "Are LLMs Ready for Real-World Materials Discovery?",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Gupta et al. 2023 (DiSCoMaT)",
            "name_full": "DiSCoMaT: Distantly supervised composition extraction from tables in materials science articles",
            "brief_description": "A cited approach using distant supervision methods (not necessarily pure LLMs) to extract composition and tabular data from materials literature at scale, used as a component in building multi-modal corpora.",
            "citation_title": "DiSCoMaT: Distantly supervised composition extraction from tables in materials science articles",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "method_name": "Distant supervision for table composition extraction",
            "method_description": "Uses distant supervision signals combined with NLP tools to extract composition data from tables in the literature; recommended in the paper as a scalable approach that can complement LLM/data-generation methods.",
            "number_of_papers": null,
            "domain_or_field": "materials science (tables extraction)",
            "type_of_laws_extracted": "structured property/composition facts (empirical data) that can be aggregated to identify patterns or empirical relationships",
            "example_laws_extracted": null,
            "evaluation_method": null,
            "performance_metrics": "Paper cites an extraction comparison: GPT-4 extracted ~55% of materials properties from a table vs. ~73% by a model trained specifically for the task (Gupta et al., 2023) — reported in this paper.",
            "comparison_baseline": "Specialized table-extraction model (Gupta et al., 2023) which outperformed GPT-4 on table property extraction.",
            "key_findings": "Distant supervision can scale extraction of tabular composition/property data; specialized models currently outperform off-the-shelf LLMs on table-based extractions.",
            "challenges_limitations": "Tables often incomplete or require grounding with surrounding text and arithmetic (e.g., inferring missing component percentages), posing challenges for LLMs and automated extraction.",
            "uuid": "e4564.9",
            "source_info": {
                "paper_title": "Are LLMs Ready for Real-World Materials Discovery?",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Structured information extraction from scientific text with large language models",
            "rating": 2
        },
        {
            "paper_title": "Wikicrow: Automating synthesis of human scientific knowledge",
            "rating": 2
        },
        {
            "paper_title": "Generative retrieval-augmented ontologic graph and multiagent strategies for interpretive large language model-based materials design",
            "rating": 2
        },
        {
            "paper_title": "MatKG: An autonomously generated knowledge graph in material science",
            "rating": 2
        },
        {
            "paper_title": "LLM-prop: Predicting physical and electronic properties of crystalline solids from their text descriptions",
            "rating": 2
        },
        {
            "paper_title": "Fine-tuned language models generate stable inorganic materials as text",
            "rating": 2
        },
        {
            "paper_title": "Mascqa: Investigating materials science knowledge of large language models",
            "rating": 2
        },
        {
            "paper_title": "Honeybee: Progressive instruction finetuning of large language models for materials science",
            "rating": 2
        },
        {
            "paper_title": "DiSCoMaT: Distantly supervised composition extraction from tables in materials science articles",
            "rating": 2
        },
        {
            "paper_title": "14 examples of how llms can transform materials science and chemistry: a reflection on a large language model hackathon",
            "rating": 1
        }
    ],
    "cost": 0.02074125,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Are LLMs Ready for Real-World Materials Discovery?</h1>
<p>Santiago Miret ${ }^{<em> 1}$ N. M. Anoop Krishnan ${ }^{</em> 2}$<br>${ }^{1}$ Intel Labs, Santa Clara, CA, United States<br>${ }^{2}$ Department of Civil Engineering, Yardi School of Artificial Intelligence, Indian Institute of Technology Delhi, Hauz Khas, New Delhi, India<br>*Email: santiago.miret@intel.com (SM), krishnan@iitd.ac.in (NMAK)</p>
<h4>Abstract</h4>
<p>Large Language Models (LLMs) create exciting possibilities to accelerate scientific discovery and knowledge dissemination in materials science. While LLMs have been successfully applied to select scientific problems and rudimentary challenges, they currently fall short of being practical materials science tools. In this perspective, we show relevant failure cases of LLMs in materials science that reveal current limitations of LLMs related to comprehending and reasoning over complex, interconnected materials science knowledge. Given those shortcomings, we outline a framework for developing Materials Science LLMs (MatSci-LLMs) that are grounded in domain knowledge, which can enable hypothesis generation followed by hypothesis testing. The path to attaining performant MatSci-LLMs rests, in large part, on building high-quality, multi-modal datasets sourced from scientific literature, where various information extraction challenges persist. As such, we describe key materials science information extraction challenges which need to be overcome to build large-scale, multi-modal datasets that capture valuable materials science knowledge. Aiming to bring a coherent effort to address these challenges, we outline a roadmap for applying MatSci-LLMs for real-world materials discovery through six interacting steps: 1. Materials Query; 2. Data Retrieval; 3. Materials Design; 4. Insilico Evaluation; 5. Experiment Planning; 6. Experiment Execution. Finally, we discuss some of the broader implications of the MatSciLLMs on the society at large in terms of sustainability, inclusivity and policy making.</p>
<h2>1. Introduction</h2>
<p>The advent of large language models (LLMs) has started to reshape many technology development efforts and research roadmaps. Apart from significantly impacting the space of natural language processing, LLMs have had a significant impact on many related fields, such as computer vision with text-to-image generators (Zhang et al., 2023), and have sparked the creation of efforts to integrate their capabilities into diverse industries. Examples of these efforts include task automation in healthcare (He et al., 2023), legal (Dahl et al., 2024), finance (Wu et al., 2023a), software engineering (Fan et al., 2023) and multiple scientific fields. The successful application of LLMs to materials science (MatSci) has the potential to transform the field by accelerating the discovery, synthesis, and analysis of new materials that can address some of today's complex societal-scale challenges, such as climate change, energy security, sustainable agriculture and manufacturing, personalized medical devices, and access to more powerful computing systems. While recent research work has seen increased adoption of LLMs in adjacent scientific fields, such as chemistry (Jablonka et al., 2023) and diverse areas of biology (Lin et al., 2023; Hsu et al., 2022; Xu et al., 2023; Cui et al., 2023; Dalla-Torre et al., 2023), the application of LLMs in materials science has been significantly slower. Analyzing the challenges and bottlenecks for the use and development of LLMs tailored to materials is the first step to address this issue. Hence, we aim to analyze the failures of LLMs in materials science and propose requirements for MATerials SCIence LLMs (MatSci-LLMs) along with a roadmap to enable the impactful application of MatSci-LLMs for furthering the field of materials.</p>
<p>Material science is a highly interdisciplinary field - the first scientific branch that emerged out of the fusion of different branches (physics, chemistry, mechanical engineering, and metallurgy) rather than a division - that studies how the interaction of atomic matter affects the properties and behavior of materials systems across diverse sets of conditions. The broad scope of materials systems encompasses multiple physical scales ranging from nanostructures with a small number</p>
<p>of atoms which are used in modern computing devices, to continuum materials, such as metals and cement, that make up modern infrastructure like roads, bridges and buildings. Moreover, the wide range of materials applications can range from electronic materials (e.g., transistors, batteries), to chemical materials (e.g., polymers) to biological materials (e.g., medical implants), each of which has very different requirements and application environments. As such, MatSci often borrows concepts from related scientific fields, including physics, chemistry, and biology, leading to a vast and dynamic body of work covered by the field. This diversity also informs the wide range of tasks that materials scientists perform across different subfields, which further informs the requirements of MatSci-LLMs.</p>
<p>Early work applying text-based methods to materials science largely focused on text-and-data mining. While much progress has been made to enable data mining, many of the challenges data-mining identified in early work by Kononova et al. (2021) remain. One particularly common task for early text-mining methods was mining text to predict synthesis procedures based on peer-reviewed literature (Kim et al., 2017; 2020). While the deployment of targeted text-mining techniques was successful in developing targeted synthesis procedures for a subset of materials systems, the proposed approaches did not provide scalable solutions to a diversity of materials design challenges (Ravinder et al., 2021) (see App. 4 for additional discussion).</p>
<p>In addition to text-mining, early works also focused on classifying the text in materials science publications according to their semantic meaning, which is similar to named-entity-recognition (NER) in traditional language processing (Gupta et al., 2022; Walker et al., 2021). As studied by Song et al. (2023a), specialized BERT models perform quite well on common NLP tasks for materials science, assuming labeled data exists. This, however, is not scalable given the sparsity of labeled data in materials science. Ensuing information extraction methods, such as ChemDataExtractor (Swain \&amp; Cole, 2016) spanned different modalities of data, including scientific text, property data, spectroscopy data, as well as tabular data. Additional efforts were also used to extract information related to images (Venugopal et al., 2021; Zaki et al., 2022b; Mukaddem et al., 2019), which were then used to create either database or to even extract features (Mukaddem et al., 2019; Zaki et al., 2023). LLMs have leapfrogged prior machine learning approaches for various text-based MatSci tasks, including information extraction and search. While specialized methods continue to improve the performance of data extraction for applicable modalities in materials science (Gupta et al., 2023), the advent of LLMs has led to a more unified and reliable way to arrive at reliable information extraction (Dagdelen et al., 2024). As described in Dagdelen et al. (2024) and in Section 5, in-context learning abilities of modern LLMs are a powerful tool to accelerate the abilities to process and extract information in materials science text. While further research is needed to improve such methods, they represent significant progress that enables new research opportunities discussed in this paper.</p>
<p>In this perspective, we start with an overview of the requirements of a MatSci-LLM. Following this, we discuss the failure cases of LLMs in meeting those requirements and the need for further benchmarking datasets. Next, we discuss the challenges in the development of a MatSci-LLM. This section also presents some of the interesting machine learning challenges involved in the process of building a MatSci-LLM that can be applied to several other domains as well. Concretely, the development of MatSci-LLMs requires high-quality, multi-modal datasets that are curated with the help of domain experts. Moreover, dataset curation also involves a set of unique, on-trivial challenges to potentially unlock the groundbreaking capabilities of MatSci-LLMs. These challenges, amongst others, include linking information across multiple documents referenced via citations, as well as managing diverse ways of expressing important procedures and concepts. To bridge these gaps, we propose a roadmap for MatSci-LLMs, which we hope will enable a coordinated community research effort to address these interwoven challenges. Finally, we discuss the broader implications of MatSci-LLMs and discuss how they could help in addressing some of the sustainable development goals.</p>
<h1>2. Requirements of a MatSci-LLM</h1>
<p>To evaluate the performance of the LLM for materials science, it is essential to identify the requirements a MatSci LLM needs to satisfy. Given the diversity of MatSci knowledge and its associated technical tasks, we propose that MatSci-LLM should meet the following requirements, shown in Figure 1, namely, Domain Knowledge \&amp; Grounded Reasoning: MatSci-LLMs should have an understanding of MatSci as a subject area to provide useful information to material scientists and be able to reason over core MatSci principles; and Augmenting Materials Scientists: MatSci-LLMs should have the ability to perform useful tasks to accelerate materials science research that augments the work of materials scientists in reliable and interpretable manner. These aspects are discussed in detail below.</p>
<p>Domain Knowledge \&amp; Grounded Reasoning: Recent research efforts have yielded progress towards infusing MatSci domain knowledge into LLMs to solve text-based tasks. Early work, such as Walker et al. (2021); Gupta et al. (2022);</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. Overview of MatSci-LLM requirements related to knowledge acquisition and science acceleration. MatSci-LLMs require knowledge contained across multiple documents along multiple data modalities. Pertinent materials science knowledge includes understanding materials structure, properties and behavior covering diverse conditions, as well as materials synthesis and analysis procedures based on experimental descriptions. To effectively accelerate science, MatSci-LLMs should interact with human scientists as robust question-answering system and act as grounded hypothesis generators that augment a scientist's knowledge. Additionally, MatSci-LLMs should provide executable procedures for real-world experiments through machine-machine and human-machine interfaces.</p>
<p>Huang \&amp; Cole (2022) focused on training BERT (Devlin et al., 2019) architecture on domain-specific text corpora relevant to MatSci. A detailed analysis of these and other scientific language models by Song et al. (2023a) revealed that domainspecific pretraining did infuse the language models with relevant materials knowledge showing the importance of building high-quality MatSci language datasets. Recent works by Song et al. (2023b) and Xie et al. (2023) showed that instruction fine-tuning leveraging commercial scale LLMs can further improve performance on MatSci language tasks. While this suggests that practical MatSci knowledge can be extracted from today's commercial LLMs (e.g., GPT-3.5, Claude and GPT-4) their understanding of MatSci remains incomplete as shown by Zaki et al. (2024). As we discuss in Section 3, grounded reasoning over materials science principles remains a challenge.
Augmenting Materials Scientists: Research work in this direction has been primarily limited to adjacent domains, in particular chemistry. Jablonka et al. (2023) showed different applications of LLMs across diverse chemistry tasks, and Bran et al. (2023) and Boiko et al. (2023) provided examples of how a tool-augmented GPT-4 can perform real-world experimental planning and execution based on user queries. While such works show great relevance to MatSci, they are incomplete in addressing the vast diversity of knowledge and scientific tasks in MatSci, thereby falling short on the criteria needed to be considered MatSci-LLMs. One concrete difference between chemistry LLMs and MatSci-LLMs, for example, is the fact that IUPAC names in chemistry make the identification and tracking of molecular structures significantly easier. As discussed in Section 4, the materials domain contains much greater diversity including periodic crystal structures, surfaces, nanostructures, metals and alloys, and disordered materials like glasses, limiting the degree of systematization that can be achieved for indexing these materials with natural language.</p>
<h1>3. Failure Cases of LLMs in MatSci</h1>
<p>While recent work has shown great promise in applying LLMs to materials science and adjacent fields, it is important to understand the limitations of LLMs on a broad set of challenges in the MatSci domain. To this extent, several efforts have focused on tasks related to question-answering, code writing, named-entity recognition, abstract classification, and</p>
<p>composition extraction from tables in materials literature (Zaki et al., 2024; Gupta et al., 2023; Song et al., 2023b;a). Here, we outline some of the failure cases demonstrating the need to develop robust MatSci-LLMs before deploying them for practical applications.</p>
<p>Proficient LLMs such as GPT-4 (Achiam et al., 2023) and LLaMA-2 (Touvron et al., 2023) are trained on vast corpora of publicly available text data (Computer, 2023). Accordingly, commercial LLMs should have had some reasonable exposure to the MatSci domain based on Wikipedia and other public domain information on MatSci. To evaluate the MatSci domain knowledge of LLMs, Zaki et al. (2024) curated a dataset of 650 questions requiring undergraduate-level knowledge to answer them. The analysis of the responses from LLMs revealed that GPT-4 with chain-of-thought (CoT) (Wei et al., 2022) reasoning performed the best with a score of $62 \%$ accuracy. Interestingly, GPT4-CoT performed the worst in numerical-type questions with a score of only $39 \%$. This suggests that present LLMs perform poorly in identifying the correct equations, grounding the context by substituting appropriate numerical values, and subsequently solving the original question. Moreover, a comparison of the performance of LLMs with human performance in the same exam revealed that GPT4-CoT, while outperforming other baselines, consistently obtained only $50 \%$ of the top-performing humans and ultimately never cleared the cutoff to pass the exams.</p>
<p>A task where LLMs are known to perform well is code generation (Chen et al., 2021; Lai et al., 2023; Zan et al., 2023). While code generation for chemistry tasks has yielded mostly promising results as detailed in White et al. (2023), Zaki et al. (2024) evaluated the performance of LLMs on code generation tasks as well and found that GPT-4 exhibited an accuracy of only $71 \%$ even in code writing tasks related to MatSci. A careful analysis of the questions and code-writing tasks where GPT4-CoT performed poorly reveals two salient conclusions: (i) LLMs have difficulty in solving complex numerical problems, not just because of the complex arithmetic (which could potentially be solved by translating the numerical question into code (Wu et al., 2023b)), but because of their inability to ground the available data in the given equation. This includes wrong substitution, incorrect conversion of units, and missing constants while converting units (e.g., taking into account the speed of light, Avagadro number, etc.). (ii) LLMs have difficulty in understanding the information related to 3D structure. Specifically, the crystal structures and the symmetries associated with materials are misinterpreted leading to incorrect conclusions. Zaki et al. (2024) observed these errors consistently in both code generation tasks and question-answering tasks for multiple LLMs.</p>
<p>To elucidate current LLM failure cases in MatSci, consider the question below. "The materials belonging to which one of the following crystal classes would be both piezoelectric and ferroelectric: (A) 222 (B) $4 \mathrm{~mm}(C)-1$ (D) $2 / \mathrm{m}$ ". Here, while (B) 4 mm is the correct answer, GPT4-CoT suggests $2 / \mathrm{m}$ as the correct answer with the reasoning that "(D) $2 / \mathrm{m}$ : This crystal class does not have a center of symmetry, which means it can be piezoelectric. Additionally, it is a polar crystal class, making it able to be ferroelectric as well.". However, this reasoning is incorrect- $2 / \mathrm{m}$ is centrosymmetric, and hence is not a polar crystal. Answering this question requires the knowledge of crystal structures, which needs to be grounded on the concept of the polarity of atoms based on the crystal structure. Although the LLM understands this requirement, it performs poor reasoning and inference, potentially due to the lack of such data during training. Another example worth noting is the following question. "The Miller indices of the first three Bragg peaks in the X-ray diffraction pattern obtained from a polycrystalline iron sample at room temperature are (A) (111), (200), (220) (B) (100), (110), (111) (C) (100), (110), (200) (D) (110), (200), (220)". While the correct answer is (D), GPT4-CoT suggests (C) as the answer, based on the reasoning that "systematically listing all possible combinations of $h, k$, and $l$, and identifying those that satisfy the BCC selection rule $(h+k+l=$ even), we get the following allowable Miller indices as: (100), (110), and (200)". However, this is contradictory as for the first Miller index (100) in the list, $\mathrm{h}+\mathrm{k}+1=1+0+0=1$, which is odd and not even. Accordingly, while the understanding that the BCC selection rule requires $(\mathrm{h}+\mathrm{k}+\mathrm{l})$ should be even is correct, the LLM was unable to ground this concept correctly and suggest the correct Miller indices. Ten such examples are listed in Appendix C, where we provide the question, the answer, and the reasoning provided by GPT-4, along with the correct solution. We observe consistent mistakes made by GPT-4 in numerical errors and reasoning inconsistencies both in conceptual MatSci questions and in MatSci code generation tasks. These observations strongly suggest that LLMs need to be exposed to more domain-specific information and develop greater reasoning capabilities for them to be usable for any real-world application in this domain.</p>
<p>LLM reasoning difficulties extend beyond the MatSci domain, including in general language tasks (BehnamGhader et al., 2023). Given the great importance of reasoning abilities to formulate and test scientific hypotheses, this remains a major gap in the development of MatSci-LLMs which also has implications for the broader LLM community. While LLMs have shown promise in solving concrete planning tasks, such as retrosynthesis planning (Boiko et al., 2023; Bran et al., 2023), and executing code-based functions as part of a greater workflow (Buehler, 2023; Yoshikawa et al., 2023), LLMs still rely on human intervention and correction in many cases. In addition to greater robustness, future LLMs can also benefit from</p>
<p>greater interpretability to accelerate hypothesis testing and enable scientists to better understand the various aspects of the system they are interested in. This is especially important in MatSci where many experimental procedures can expose valuable information about the underlying material properties and behavior under diverse settings, which in turn can inform a broader understanding of fundamental physical and chemical relationships of the underlying material system.</p>
<p>To fully unlock the power of LLMs for materials discovery, further progress is needed along the following directions.</p>
<ol>
<li>Hypothesis Generation Grounded on MatSci Knowledge: MatSci-LLMs require an understanding of diverse MatSci knowledge and the ability to reason over core domain principles. This will enable MatSci-LLMs to answer queries of domain-specific questions and generate useful hypotheses that can be verified with real-world experiments. Modern LLMs struggle to provide well-reasoned answers that reference relevant supporting evidence for in-depth questions. While ongoing progress highlights the capability of modern LLMs to provide relevant MatSci knowledge (Song et al., 2023b), clear gaps remain (Zaki et al., 2024).</li>
<li>Multi-Modal MatSci Datasets: To enable MatSci-LLMs to capture the true extent of human knowledge of MatSci, we require large-scale, high-quality, multi-modal datasets based primarily on peer-reviewed MatSci publications. Much of the knowledge in the MatSci literature is contained in tables, figures, and other modalities that express important material properties and behavior in addition to the physical and chemical relationships that underlie them. Additionally, modern LLMs still struggle to understand domain-specific MatSci language and effectively process MatSci notation, including chemical formulas and mathematical formulas expressed in line with other text.</li>
<li>Real-World Materials Design: Connecting MatSci-LLMs to real-world simulation and experimental tools in creative ways can comprehensively accelerate materials design, synthesis, and analysis by enabling end-to-end automation. As described in Section 6, MatSci-LLMs can provide powerful human-machine and machine-machine interfaces for increased automation in MatSci research, as well as commercial materials production. Recent work in chemistry LLMs (Bran et al., 2023; Boiko et al., 2023) has shown promise in deploying tool-augmented LLMs for end-to-end materials design tasks driven by text-based user input.</li>
</ol>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">NLP Tasks</th>
<th style="text-align: center;">References</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1. Battery Device QA</td>
<td style="text-align: center;">In-context Q\&amp;A</td>
<td style="text-align: center;">(Huang \&amp; Cole, 2022)</td>
</tr>
<tr>
<td style="text-align: center;">2. MaScQA</td>
<td style="text-align: center;">Materials Q\&amp;A, Code generation, Composition extraction</td>
<td style="text-align: center;">(Zaki et al., 2024; Gupta et al., 2023)</td>
</tr>
<tr>
<td style="text-align: center;">3. MatSciNLP</td>
<td style="text-align: center;">Named entity recognition, synthesis action retrieval, event argument extraction, paragraph classification, relation extraction, sentence classification, slot filling</td>
<td style="text-align: center;">(Song et al., 2023a; Gupta et al., 2022) <br> (Jensen et al., 2021; Kim et al., 2020) <br> (Kim et al., 2017; Mysore et al., 2019)</td>
</tr>
<tr>
<td style="text-align: center;">4. OpticalTable Q\&amp;A</td>
<td style="text-align: center;">Table and text-based Q\&amp;A</td>
<td style="text-align: center;">(Zhao et al., 2023)</td>
</tr>
<tr>
<td style="text-align: center;">5. SustainableConcrete</td>
<td style="text-align: center;">Text-based Concrete Design</td>
<td style="text-align: center;">(Völker et al., 2024)</td>
</tr>
</tbody>
</table>
<p>Table 1. Benchmarking datasets for MatSci-LLM along with the tasks.</p>
<p>Datasets and benchmarking: Evaluating the performance of LLMs on these tasks requires several carefully curated datasets and benchmarking frameworks that evaluate the model performance and limitations. Although there are several datasets available as listed in Table 1, further work is needed to systemically assess LLMs capabilities related to MatSci. Specifically, there are limited datasets covering only a small aspect of the wide range of topics related to materials. Moreover, there exist no datasets on multi-modal evaluations including text, graphs, figures, tables, audio, and video. On top of that, there are limited datasets on experimental procedures and protocols that could be used to evaluate the knowledge of LLMs related to understanding and executing relevant experimental procedures. It is worth noting that there are increasing research efforts on evaluating the performance of LLMs for chemistry that have led to more advanced datasets, benchmarks, and analysis (Mirza et al., 2024). We believe that similar efforts need to be carried out in the materials science domain as well, including but not limited to the area of composites, metals, alloys, ceramics, glasses, and nanomaterials.</p>
<h1>4. Challenges in the development of MatSci-LLM</h1>
<p>The MatSci domain requires great technical depth and breadth due to the field's broad technical scope that interfaces with physics, chemistry, biology, and various engineering disciplines. Hence, MatSci presents unique challenges for knowledge acquisition for domain-specific language models. While some of the challenges, such as domain-specific notations, are unique to MatSci, others have a broad intersection with multiple machine learning fields, such as multi-modal information extraction involving text, images, and video.</p>
<p>Domain-Specific Notations: Unlike IUPAC (Hellwich et al., 2020) nomenclatures in chemistry, there exist no standard notations for MatSci; for instance, $\mathrm{NaAlSi}<em 8="8">{2} \mathrm{O}</em>}, \mathrm{Na<em 2="2">{2} \mathrm{O} . \mathrm{Al}</em>} \mathrm{O<em 2="2">{3} .2 \mathrm{SiO}</em>}, \mathrm{SiO<em 2="2">{2}-0.5 \mathrm{Na}</em>} \mathrm{O}-0.5 \mathrm{Al<em 3="3">{2} \mathrm{O}</em>}$ all represent the same material in different contexts. Moreover, several common domain-specific names represent materials, including soda and lime, for $\mathrm{Na<em 3="3">{2} \mathrm{CO}</em>$, respectively. Furthermore, some sub-domains, such as cement, employ their own chemical notations that are contradictory with the general chemistry notation; C-S-H represents calcium silicate hydrate in cement chemistry, whereas, in standard chemistry notation, it represents carbon, sulfur, and hydrogen, respectively. Similarly, there are several inconsistent abbreviations used in the literature to refer to properties in MatSci. These abbreviations are sometimes specific to a given document or can also be present in multiple documents. Thus, understanding these notations requires grounding the LLMs in different domains with the right context.}$ and $\mathrm{CaCO}_{3</p>
<p>Incomplete Descriptions: Research publications regularly refer to previous works in the text to omit specific information. This is exemplified by the sentences such as "the fracture simulations were carried out using an established methodology as outlined in Griffith et al."; a common practice in the MatSci literature. Such approaches are also extremely common when reporting experimental or simulation protocols, material compositions, syntheses, and testing conditions, and optimizing process parameters. The context of the work reported in a manuscript, in this case, is thus grounded in a different manuscript. Moreover, the descriptions can occasionally be highly non-linear while referring to different sources in an extremely complicated manner. Finally, there could be a reference to the manuals of different instruments and material sources, which are described elsewhere. Thus, an LLM needs to procure information across multiple sources and process it together in the proper context to create a sequential description of the process and the related analysis that was carried out.</p>
<p>Text-to-Structure: MatSci employs text to represent several 3D or 2D structures in different ways. For instance, the crystal structures are represented using the Wyckoff positions (Aroyo et al., 2006) -4 mm in crystallography represents a crystal structure, whereas, in general literature, it might be confused with a distance metric (millimeter). Moreover, crystals are represented using a widely used format known as crystal information file (CIF), which explains the details of the crystal along with the positions of the atoms as shown in Appendix B with a CIF of Silicon. Current LLMs are unable to read, interpret, or generate CIF in their entirety, which is a strong limiting factor for novel materials discovery. Similarly, there are several other approaches to represent the 3D structure using text, such as XYZ files or other software-specific files for diverse purposes. As such, the ability of LLMs to understand such files plays a crucial role in the materials domain. Recent work by Gruver et al. (2023) shows that LLMs can generate correct CIF-formatted materials with targeted fine-tuning but fail to provide context describing important details of the CIF file.</p>
<p>Multi-Modal Information Extraction: Text and Tables: Pertinent information for materials in peer-reviewed papers are generally represented using multiple modalities, including text, tables, figures, and videos-audio is rarely used, although not absent. Significant progress has been made in effectively extracting tabular information (Gupta et al., 2023; Zhao et al., 2023; Zhao \&amp; Cole, 2022) with work still remaining on how to best make use of extracted data to infuse the underlying knowledge of the tables into language models. Extraction can be particularly challenging for property-based data, which often includes numbers that map to various scientific units. A simple exercise on information extraction from tables revealed that GPT-4 was able to extract only $\sim 55 \%$ of materials properties from a table (Zaki et al., 2024) in contrast to $\sim 73 \%$ by a model trained for the task (Gupta et al., 2023). This problem is amplified by the fact that more than $80 \%$ of material compositions and properties are reported in tables (Hira et al., 2023). Finally, in several cases, table information is incomplete, even when read together with the caption. For instance, when reporting the composition of Magnesium alloys, many tables in research papers report the values of all the components other than Magnesium in a given alloy. Thus, the percentage of Magnesium in these alloys needs to be computed as 100-(the sum of reported elements in the tables for a given alloy). Such a task requires grounding the table information along with the text and then performing the necessary arithmetic to obtain the percentage.</p>
<p>Multi-Modal Information Extraction: Text and Figures: Materials are characterized by many experimental methods such as Raman analysis, X-ray diffraction (XRD), X-ray fluorescence, scanning electron micrographs, or atomic force microscopy, the results of which are represented in figures. Such information could include the visual representation captured by the measurement technique, such as an optical micrograph or a plot that summarizes this information, as in the case of XRD</p>
<p>patterns. Moreover, this information will be elaborated briefly in the figure caption, followed by a detailed explanation in the text. A sample text from a materials literature could be as follows: "The XRD patterns shown in Figure XY(a) suggest that the sample is amorphous and does not have any crystalline content. Further, the XRF in Fig. XY(b) reveals the presence of clustering of Calcium instead of a uniform distribution". In this particular case, the text and figure need to be interpreted together. Moreover, the specific aspects of the image that allow one to interpret it, such as the clustering of calcium or the specific feature of the XRD patterns, allow a researcher to identify that the material is amorphous and is not labeled. In order for the LLM to learn these aspects, a large number of images and aligned text are needed for joint training.</p>
<p>Multi-Modal Information Extraction: Text, Figures, Tables, and Videos: Additional modalities of presenting relevant information involve a combination of text, figures, and tables. In such cases, the information from the table should be read along with the figures while grounding the reasoning in the context of the manuscript, all of which further compound the cumulative importance of multi-modal information extraction. Occasionally, additional modalities, such as audio-visual files, are used to demonstrate material response. This includes material failure modes illustrating cracks, crystal growth, thermal response, the response of actuators, and simulations, amongst others. In this case, there are additional challenges that require the training of LLMs to properly link the information in video, figures, text, and tables. These are open problems for which the community may need to develop novel methods and architectures. While these challenges are far from exhaustive, we note that the tasks mentioned here reveal novel domain-specific problems whose solutions can have a potentially strong impact on other scientific and machine learning domains as well.</p>
<p>Context Across Multiple Documents \&amp; Sources: Most of the information generated through scientific inquiry is documented and communicated through natural language in peer-reviewed publications. Understanding this text - and potentially extracting the information - presents several unique challenges distinct from other domains (Hira et al., 2023). One of the major challenges in this regard includes understanding the importance of context across multiple documents and sources of information. As highlighted above, this is additionally complicated by the fact that a given material can have multiple correct names and designations, meaning that important information can be contained across all of the names for a given material. Having unique IUPAC-style naming conventions can accelerate the deployment of LLMs as observed in chemistry (Jablonka et al., 2023), yet this is often not sufficient to capture all the relevant details needed to fully understand materials properties and synthesis leading to the development of additional data initiatives to fill that gap (Kearnes et al., 2021; Mercado et al., 2023). Hence, while proposing an IUPAC-style naming convention might seem like a natural solution to aid the deployment of LLMs in MatSci, many proposals of such conventions have previously fallen short, given the vast diversity of the field.</p>
<p>Diverse Experimental and Simulation Procedures: While understanding context is the first step toward automated discovery, the next natural step is to identify the appropriate experimental or simulation protocols relevant to the context. There is a multitude of experimental and simulation procedures that can be used to evaluate the same objective. As such, understanding them and choosing the relevant one or a set of relevant experiments is a challenging task, which requires domain-specific information and reasoning. Early works have shown some promise in providing useful tools for creating synthesis procedures of specific materials (Olivetti et al., 2020; Jensen et al., 2021), but much work remains in effectively deploying LLMs for these tasks. Additionally, understanding experimental MatSci procedures provides additional challenges given the diversity of synthesis, characterization techniques across data modalities (e.g., phase identification using X-ray diffraction, spectroscopic techniques), and processing conditions that can be observed across materials design use cases even within similar materials classes. Similarly, effective insilico design and filtering of materials requires accurate simulations, and the inability to generate the correct codes grounded in the context can make this task challenging. This is exemplified by the mistakes in the code generation tasks where the LLMs are unable to either ground the concept correctly or the generated code exhibits numerical errors. Thus, it is imperative that large amounts of curated and complete data associated with simulation and experimental procedures are used to train effective MatSci-LLMs.</p>
<p>It is worth noting that understanding materials synthesis and characterizing them is far from trivial given the cast diversity of procedures, many of which can exist even among the same family of materials. Modern materials systems range from metals, ceramics, semiconductors, glasses, and polymers to composites, nanomaterials, alloys, and biomaterials to name a few. Furthermore, many materials exist in liquid, solid (both amorphous and crystalline), or gaseous phases. Composites, which involve combinations of multiple materials, often contain multiple phases in a single system such as a gel, which is a liquid in solid, or a colloid, which is a solid in liquid. The synthesis procedures relevant for these diverse arrays of materials accordingly vary greatly and include diverse methods such as melt-quenching, sintering, annealing, sol-gel process to name a few. Following synthesis, characterizing the synthesized materials is also a noteworthy challenge where each technique needs to be mapped to the given system and property of interest. To further complicate matters, the same</p>
<p>property measured by different techniques can lead to different results. For instance, hardness measured by common techniques including nanoindentation, microindentation, atomic force microscopy, a picoindenter (Gouldstone et al., 2007), or a scratch test (Kasimuthumaniyan et al., 2021), all can yield significantly different results. Altogether, the diversity in these approaches suggest that synthesis and characterization procedures must be customized for each individual material system. As such, this type of information is important to the MatSci-LLM to understand have available in order for it to suggest the appropriate experimental technique and enable its execution.</p>
<p>Addressing this challenge requires a comprehensive database of synthesis and characterization data of materials. Early work, such as the synthesis project (Kim et al., 2017; 2020) have started to collect such data for a subset of modern materials. Additionally, past research efforts have focused on extracting information related to processing and testing conditions of materials (Mysore et al., 2019; Venugopal et al., 2021; Zaki et al., 2022a), which are acutely valuable for building comprehensive databases. Many of the aforementioned approaches employed classical natural language processing techniques which can benefit from the enhanced capabilities of modern LLMs.</p>
<h1>5. Multi-Modal MatSci Corpus Building</h1>
<p>The performance of a language model heavily depends on the quality of the dataset on which they are trained on. Thus, dataset creation remains an integral part in enabling progress across various deep learning subfields, including computer vision, graph learning, as well as natural language in both general and scientific domains. Given the vast amount of MatSci information expressed in diverse modalities, multi-modal language models would unlock significantly more powerful capabilities for scientific language modeling.</p>
<p>The gold standard data for training these LLMs for MatSci is mostly contained in peer-reviewed publications at established editorials, such as the family of journals at Elsevier, Royal Society, American Society, and Springer Nature amongst others. Unfortunately, much of the content of peer-reviewed publications at established journals remains inaccessible beyond paywalls limiting public access, which makes it difficult to access these valuable text data. Accordingly, it is unlikely that any general-purpose language models, such as GPT-4 or LLaMa, have had access to this data, which likely contributes to their poor performance on MatSci tasks highlighted in Section 3. While there have been promising developments in making scientific text data available via open access through various preprint servers and portals like Semantic Scholar, text data obtained from such sources often requires extensive cleaning and preparation processes before becoming useful for training scientific LLMs (Lo et al., 2020). Research work in making text data more accessible has already produced meaningful results in making language model training and evaluation more successful (Song et al., 2023a;b). However, curating such data on which an LLM can be trained requires addressing additional data-related challenges described below.</p>
<ol>
<li>Data Availability: While families of journals such as Elsevier and Springer provide text and data-mining APIs based on paid subscriptions, machine-readable formats such as xml files are available only for manuscripts published in the $21^{\text {st }}$ century. Our analysis of MatSci-related articles in Springer and Elsevier journals revealed that $\sim 6$ million peer-reviewed articles with a total of $\sim 20$ billion words are available in a machine-readable format through institutional subscriptions (see Appendix A). While this corpus is significantly smaller compared to general language LLMs, the quality and domain relevance is significantly higher. Nevertheless, almost all the publications from the $20^{\text {th }}$ century or earlier are only available as PDFs or scanned files of the hardcopy publication which are not easily machine-readable. Thus, a vast amount of data representing the cumulative scientific endeavor before the 21st century is not available for LLM training. Additionally, many other families of peer-reviewed journals do not either allow text and data mining or have a framework to support it. Data sourced from preprint servers, such as arXiv, often require extensive cleaning to make them amendable to LLM training.</li>
<li>Large-Scale Description \&amp; Annotation: While pretraining does not require annotations, several downstream tasks require high-quality annotations for finetuning the model or employing it using in-context or few-shot learning. This is particularly important when data is obtained from multiple sources and in multiple modalities, such as tables, text, images, videos, and codes. For instance, a CIF document of Silicon should have a detailed description of what information is contained in the file so that the LLM can learn to interpret the information in the CIF in addition to understanding its format. However, such large-scale annotations are currently unavailable and would require input from domain experts for them to be reliable. Given these challenges, to ensure scalable dataset generation, we propose: a. Distant Supervision: Here, sparse supervision signals are used to alleviate the amount of high-quality annotations required. This has shown promise in data extraction (Gupta et al., 2023) and can be applied along with existing materials databases, such as the Cambridge Structure Database (Groom et al., 2016). b. In-Context Learning: General purpose LLMs, such as GPT4, exhibit excellent in-context learning capabilities. Thus, prompt engineering along with in-context learning can be used to generate high-quality training</li>
</ol>
<p>data. Song et al. (2023b) demonstrated this approach for instructions-based finetuning of an LLM for materials science tasks. We provide several examples of composition extractions detailed in Appendix D, which show some of the capabilities and gaps of in-context learning for GPT-4. c. Domain-Specific Regex can also be very useful in developing high-quality training data. Robocrystallographer (Ganose \&amp; Jain, 2019), for example, is a regex-based approach that can produce a textual description for a CIF file. Similarly, ChemDataExtractor (Swain \&amp; Cole, 2016) employs a regex-based approach for chemical data extraction. Regex tools, along with LLMs and distant supervision, comprise promising methods for scalable data generation and materials design challenges discussed in Section 6.
3. Linking Multiple Entities: While obtaining data on multiple entities is possible, linking them appropriately so they can be read together with relevant context is a non-trivial task. For instance, the description of a figure or a table in a manuscript could be spread across multiple paragraphs and even the supplementary material. This is in contrast to standard machine learning approaches where a figure and description are given together while training. Thus, developing a dataset that links multiple entities appropriately and employing an appropriate training scheme that respects the dataset and the context remains an open challenge for the research community.
4. Handling External References: Another major challenge while curating a dataset based on peer-reviewed publications relates to the use of external references. In a manuscript, references to multiple documents will be made to either support the claims made in the current work and thereby base the manuscript to be read in the context of the referred work. In other words, information in the present manuscript is grounded on the claims or observations in the referenced manuscript, which in turn is grounded in its own references. Dataset and training schemes developed to train a MatSci-LLM should respect this fact to reduce hallucinations and provide well-reasoned, executable hypotheses. It is also worth noting that several LLMs often hallucinate when asked for references. Furthermore, when asked to generate a text in the form of a scientific manuscript, these LLMs generate fictitious references which are arbitrarily written to fit sentences generated in the process. This motivates the need to appropriately account for external references in the training data.</p>
<p>Altogether, these challenges suggest that the development of a dataset for MatSci-LLMs requires close collaboration between multiple entities, including publishers, government, industry and academia. While development of high-quality datasets are indeed challenging, employing some of the aforementioned approaches along with the development of new methods for managing sparse datasets can spark research for novel solutions that effectively manage data sparsity in the MatSci domain. Moreover, MatSci-LLMs warrant machine learning solutions for several new challenges such as: (i) handling old and non-machine readable data through computer vision techniques that can convert the scanned documents to text respecting the original formats, and (ii) novel machine learning methods that can handle external references and multi-modal data that are linked together across different contexts. Such solutions could have implications beyond the MatSci domain, such as analyzing old documents relevant to the history, law, and finance.</p>
<h1>6. Roadmap for MatSci-LLMs</h1>
<p>Figure 2 shows our proposed outline of an end-to-end materials discovery framework with MatSci-LLM at the core. Each part of the roadmap contains diverse research opportunities building towards a performant MatSci-LLMs. Furthermore, Table 2 outlines some of the recent attempts to address these challenges. Now, we discuss these set of tasks for MatSci-LLMs spanning the six steps specified in the roadmap in Figure 2.</p>
<ol>
<li>Materials Query: Materials queries may span diverse user queries, such as, "Suggest potential candidate materials for Li-solid state battery electrolytes". Alternatively, more complex queries may require the MatSci-LLM to generate hypotheses based on presented experimental data, a text document or a combination of multiple such sources. As previously described, hypothesis generation remains an open research problem for MatSci-LLMs and may require interaction with other steps in the roadmap. Thus, materials query refers to the initial stage where the problem identification is performed either manually or through an LLM, presenting a research opportunity for novel, specialized querying methods. Emerging work may also build on top of novel human-machine interfaces for designing complex materials systems (Kanarik et al., 2023; Vasudevan et al., 2022) that enable greater efficiency and faster scientific understanding.</li>
<li>Data Retrieval: The second step toward materials discovery is to exploit the existing knowledge from MatSci-LLM internal and external knowledge bases. External knowledge includes databases of diverse materials properties (e.g., composition-structure-properties), unstructured data from text, or images, or a combination thereof that capture diverse aspects of the materials simulation, synthesis and characterization pipeline. The vast amount of publications and knowledge sources makes it non-trivial to extract such information. As such, externalizing the knowledge of MatSci-LLMs could be useful. Recent works by Cox et al. and Buehler (2024) have shown that intentional use of LLMs has the potential to</li>
</ol>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. Roadmap of a Mat-Sci LLM based materials discovery cycle. The cycle starts with materials query from a researcher that specifies desired properties or an application. The MatSci-LLM then draws from external and internal knowledge bases to generate a materials design hypothesis which is evaluated in-silico. Next, the MatSci-LLM ingests the in-silico results and prepares an experimental plan to synthesize and characterize the material, after which the MatSci-LLM interfaces with the relevant machines to execute the experimental workflow. The final result is then shown to the user for evaluation and feedback. Each stage can interact with another for refinement and improvement by the MatSci-LLM.
broaden the availability of useful scientific knowledge by externalizing domain-specific LLM-knowledge in a structured, human-interpretable manner. Cox et al. generated annotations for a database of $&gt;15,000$ protein-codings using robust methods that rely on citations and cross-checking of the generated information against the scientific literature. Buehler (2024) and Venugopal \&amp; Olivetti (2024) externalized LLM knowledge in a structured knowledge graph, which can be probed by a scientist to further their understanding and potentially make corrections and adjustments as needed. Future work can build on top of these early approaches leading to more reliable retrieval methods along with creating interpretable MatSci-LLMs.
3. Materials Design: Based on the query and retrieved information, MatSci-LLMs may directly generate structures based on CIF files (Gruver et al., 2023; Flam-Shepherd \&amp; Aspuru-Guzik, 2023; Antunes et al., 2023) or in combination with current graph or point-cloud based generative methods (Xie et al., 2021; Jiao et al., 2023; Merchant et al., 2023). As shown in Zeni et al. (2023); Govindarajan et al. (2023), conditional materials generation is a promising approach for designing materials with specific properties, where the properties can also be predicted based on the textual description (Rubungo et al., 2023). The rapid progress on generative modeling techniques in adjacent fields such as computer vision, as well as the vast space of unsolved materials problems, such as designing complex real-world systems with multiple structures, provides great opportunities for future research work.
4. Insilico Evaluation: Code generation abilities can enable tool-agumented MatSci-LLMs to execute complex simulation workflows to evaluate materials designs. Recent work by Buehler (2023) shows a possible example of how LLMs can be integrated with relevant tools to perform end-to-end insilico design of polymer materials where an LLM can generate a new molecular compound in SMILES notation and then query different LLM-based agents to perform relevant calculations for the same materials. Further examples in Buehler (2023) show how the LLM system can also set up the relevant computation environment by installing relevant packages, showing further benefits of end-to-end tool integration. While many of these systems have not yet been tested at large scales, this represents a great step forward in constructing impactful MatSci-LLMs. Whereas recent work provides many reasons to be optimistic about LLMs capabilities to solve concrete in-silico MatSci</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Roadmap Step</th>
<th style="text-align: center;">MatSci-LLM Tasks</th>
<th style="text-align: center;">References</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1. Materials Query</td>
<td style="text-align: center;">Prompting</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Human-Machine Interfaces</td>
<td style="text-align: center;">(Vasudevan et al., 2022)</td>
</tr>
<tr>
<td style="text-align: center;">2. Data Retrieval</td>
<td style="text-align: center;">RAG</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Externalizing Knowledge</td>
<td style="text-align: center;">(Buehler, 2024; Venugopal \&amp; Olivetti, 2024)</td>
</tr>
<tr>
<td style="text-align: center;">3. Materials Design</td>
<td style="text-align: center;">Text-Based Design</td>
<td style="text-align: center;">(Gruver et al., 2023; Flam-Shepherd \&amp; Aspuru-Guzik, 2023)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Generative Model Tools</td>
<td style="text-align: center;">(Zeni et al., 2023; Govindarajan et al., 2023)</td>
</tr>
<tr>
<td style="text-align: center;">4. Insilico Evaluation</td>
<td style="text-align: center;">Predictive Modeling</td>
<td style="text-align: center;">(Rubungo et al., 2023; Jablonka et al., 2023)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Code Generation</td>
<td style="text-align: center;">(Zaki et al., 2024)</td>
</tr>
<tr>
<td style="text-align: center;">5. Experimental Planning</td>
<td style="text-align: center;">Task Automation</td>
<td style="text-align: center;">(Bran et al., 2023; Boiko et al., 2023)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Task Planning</td>
<td style="text-align: center;">(Sim et al., 2023; Szymanski et al., 2023)</td>
</tr>
<tr>
<td style="text-align: center;">6. Experimental Execution</td>
<td style="text-align: center;">Machine-to-Machine Interface</td>
<td style="text-align: center;">(Yoshikawa et al., 2023)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Real-Time Data Monitoring</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 2. MatSci-LLM tasks based on the roadmap identified in Figure 2.
problems, much published work fails to include negative results outlining current LLM failures. As such, it is important to continue to have broader analysis of LLMs’ capabilities for code generation in MatSci similar to Zaki et al. (2024), which can point to relevant research directions for more robust integration of MatSci-LLMs into materials simulation workflows.
5. Experiment Planning: Following insilico evaluation, the next step involves the development of experiment plan to synthesize and characterize the materials. LLMs have shown promise in automating simple laboratory experiments (Boiko et al., 2023; Bran et al., 2023), while adjacent work aims to build necessary infrastructure for autonomous laboratories that perform experiments without human intervention (Sim et al., 2023; Szymanski et al., 2023). The selection of synthesis and characterization remains highly non-trivial and depends significantly on the material systems, length and time scales, and the properties of interest (see App. 4). As such, further research is needed to have MatSci-LLMs effectively identify realistic, detailed, and executable experimental procedures informed by the constraints of state-of-the-art technology and user needs. Moreover, the development of effective digital twins for different synthesis procedures will be immensely valuable for insilico testing and refinement of experimental plans. Data mining approaches sometimes provide useful directions for targeted use cases, but still require large amount of expert intervention, making them unsuitable for large-scale use.
6. Experiment Execution: The actual execution of the experiments require identification of the relevant commands and the sequence in which these should be given. This also involves intermediate analysis and appropriate modification of the protocol according to the real experimental conditions at a given time. For instance, melting of a silicate system would require continuous strirring until the system is bubble free; a standard protocol while preparing glasses. Thus, MatSci-LLMs should be able to make and change the plans dynamically as per the actual conditions of the experiment. Recent work by Yoshikawa et al. (2023) shows that LLMs can provide practical interfaces that more effectively connect scientists to robotic systems to perform chemical tasks. Current capabilities, however, are still in their early stages with significant future work needed to create robust control methods for a single machine, as well as machine-to-machine communication capabilities.</p>
<h1>7. Conclusions and outlook</h1>
<p>The virtuous cycle shown in Figure 2 has the potential to enable impactful scientific discoveries through end-to-end automation while concurrently augmenting human knowledge through the discovery of new physical and chemical relationships for an expanding set of materials. The unique challenges of deploying LLMs in MatSci outlined in this paper, however, require further research to make MatSci-LLMs effective scientific assistants. Meaningful research advances will require advances along the interface of many fields, including machine learning, materials simulation, materials synthesis, materials characterization, and robotics.</p>
<p>Among the 17 sustainable development goals (SDGs) proposed by United Nations, 10 can be achieved or accelerated through materials innovation. The development of an end-to-end framework for accelerating materials discovery can have</p>
<p>broader impacts in several domains such as healthcare, agriculture, energy, sustainability, water filtration, and carbon capture, to name a few. Traditional materials discovery remains an extremely slow process often taking a period of 20-30 years from starting a design to final deployment. AI-driven materials discovery has a potential to reduce this cycle to a few years or even months, thereby providing a means to address major societal-scale challenges. Moreover, the availability of cloud computing and web services can make this process extremely efficient and economical leading to democratization of materials discovery. This could reduce the gap between large corporations and small and medium scale industries allowing materials discovery without the availability of large lab facilities and vast R\&amp;D budgets. While Mat-Sci LLMs have great promise in reducing the cost and democratizing materials discovery, great care must be taken to ensure that MatSci-LLMs and all related technologies be used in an ethical and harm-limiting manner. As such, we propose the following framework to reduce the risk of potentially harmful deployment of MatSci-LLMs:</p>
<ol>
<li>Transparency: Encouraging sharing of models, datasets, and research methodologies to promote transparency, replication, and validation of findings by the broader scientific community is key to ensure risk mitigation. This approach facilitates peer review and enables independent verification of results and claims. Further, maintaining comprehensive logs of data sources, model training processes, and decision-making pathways should be encouraged to ensure accountability and traceability.</li>
<li>Risk Assessment: Developing methodologies for assessing the risks associated with the application of LLMs in MatSci, focusing on both direct impacts (e.g., safety of newly discovered materials) and indirect impacts (e.g., environmental effects of scaled production) is of paramount importance. Further, systems should be established for ongoing monitoring of the outcomes and impacts of materials discovered or optimized through LLMs to identify and address unforeseen risks promptly.</li>
<li>Regulatory Compliance \&amp; Standardization: Guidelines that address the unique challenges and risks associated with using LLMs in materials discovery should be created and enforced by governing agencies. Furthermore, standards should be developed for data representation, algorithms, and model transparency to facilitate oversight and ensure quality and reliability.</li>
<li>Education \&amp; Training: Education and training programs that integrate ethics, environmental science, sustainability, and AI should be developed to prepare a workforce capable of navigating the complexities of using LLMs in materials discovery. Additionally, dialogues and knowledge exchange between scientists, policymakers, industry stakeholders, and the public to build understanding and trust in AI-driven materials discovery should be facilitated. A noteworthy aspect that requires discussion is the tendency of LLMs to hallucinate. It is important to educate the issues associated with hallucinations in LLMs and how this can have impact on the task that it is applied for.</li>
</ol>
<p>One example where MatSci-LLMs could cause unintended harmful consequences relates to the use of natural resources. One can imagine a case where new materials discoveries could put greater demand on minerals that were previously unterutilized. This, in turn, could result in significant impacts on the local communities and the surrounding ecosystem and would therefore require action at policy level to avoid inadvertent consequences.</p>
<h1>References</h1>
<p>Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.</p>
<p>Antunes, L. M., Butler, K. T., and Grau-Crespo, R. Crystal structure generation with autoregressive large language modeling. arXiv preprint arXiv:2307.04340, 2023.</p>
<p>Aroyo, M. I., Perez-Mato, J. M., Capillas, C., Kroumova, E., Ivantchev, S., Madariaga, G., Kirov, A., and Wondratschek, H. Bilbao crystallographic server: I. databases and crystallographic computing programs. Zeitschrift für KristallographieCrystalline Materials, 221(1):15-27, 2006.</p>
<p>BehnamGhader, P., Miret, S., and Reddy, S. Can retriever-augmented language models reason? the blame game between the retriever and the language model. In Bouamor, H., Pino, J., and Bali, K. (eds.), Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 15492-15509, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp.1036. URL https://aclanthology.org/2023. findings-emnlp. 1036 .</p>
<p>Boiko, D. A., MacKnight, R., Kline, B., and Gomes, G. Autonomous chemical research with large language models. Nature, 624(7992):570-578, 2023.</p>
<p>Bran, A. M., Cox, S., White, A. D., and Schwaller, P. Chemcrow: Augmenting large-language models with chemistry tools. arXiv preprint arXiv:2304.05376, 2023.</p>
<p>Buehler, M. J. Generative retrieval-augmented ontologic graph and multiagent strategies for interpretive large language model-based materials design. ACS Engineering Au, 2023.</p>
<p>Buehler, M. J. Mechgpt, a language-based strategy for mechanics and materials modeling that connects knowledge across scales, disciplines, and modalities. Applied Mechanics Reviews, 76(2):021001, 2024.</p>
<p>Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.</p>
<p>Computer, T. Redpajama: an open dataset for training large language models, 2023. URL https://github.com/ togethercomputer/RedPajama-Data.</p>
<p>Cox, S., Hammerling, M., Lála, J., Laurent, J., Rodriques, S., Rubashkin, M., and White, A. Wikicrow: Automating synthesis of human scientific knowledge.</p>
<p>Cui, H., Wang, C., Maan, H., Pang, K., Luo, F., and Wang, B. scgpt: Towards building a foundation model for single-cell multi-omics using generative ai. bioRxiv, 2023.</p>
<p>Dagdelen, J., Dunn, A., Lee, S., Walker, N., Rosen, A. S., Ceder, G., Persson, K. A., and Jain, A. Structured information extraction from scientific text with large language models. Nature Communications, 15(1):1418, 2024.</p>
<p>Dahl, M., Magesh, V., Suzgun, M., and Ho, D. E. Large legal fictions: Profiling legal hallucinations in large language models. arXiv preprint arXiv:2401.01301, 2024.</p>
<p>Dalla-Torre, H., Gonzalez, L., Mendoza Revilla, J., Lopez Carranza, N., Henryk Grywaczewski, A., Oteri, F., Dallago, C., Trop, E., Sirelkhatim, H., Richard, G., et al. The nucleotide transformer: Building and evaluating robust foundation models for human genomics. bioRxiv, pp. 2023-01, 2023.</p>
<p>Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. BERT: Pre-training of deep bidirectional transformers for language understanding. In Burstein, J., Doran, C., and Solorio, T. (eds.), Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171-4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https://aclanthology.org/N19-1423.</p>
<p>Fan, A., Gokkaya, B., Harman, M., Lyubarskiy, M., Sengupta, S., Yoo, S., and Zhang, J. M. Large language models for software engineering: Survey and open problems. arXiv preprint arXiv:2310.03533, 2023.</p>
<p>Flam-Shepherd, D. and Aspuru-Guzik, A. Language models can generate molecules, materials, and protein binding sites directly in three dimensions as xyz, cif, and pdb files. arXiv preprint arXiv:2305.05708, 2023.</p>
<p>Ganose, A. M. and Jain, A. Robocrystallographer: automated crystal structure text descriptions and analysis. MRS Communications, 9(3):874-881, 2019.</p>
<p>Gouldstone, A., Chollacoop, N., Dao, M., Li, J., Minor, A. M., and Shen, Y.-L. Indentation across size scales and disciplines: Recent developments in experimentation and modeling. Acta Materialia, 55(12):4015-4039, 2007.</p>
<p>Govindarajan, P., Miret, S., Rector-Brooks, J., Phielipp, M., Rajendran, J., and Chandar, S. Learning conditional policies for crystal design using offline reinforcement learning. In AI for Accelerated Materials Design - NeurIPS 2023 Workshop, 2023. URL https://openreview.net/forum?id=VbjD8w2ctG.</p>
<p>Groom, C. R., Bruno, I. J., Lightfoot, M. P., and Ward, S. C. The cambridge structural database. Acta Crystallographica Section B: Structural Science, Crystal Engineering and Materials, 72(2):171-179, 2016.</p>
<p>Gruver, N., Sriram, A., Madotto, A., Wilson, A. G., Zitnick, C. L., and Ulissi, Z. W. Fine-tuned language models generate stable inorganic materials as text. In AI for Accelerated Materials Design - NeurIPS 2023 Workshop, 2023. URL https://openreview.net/forum?id=0r5DE2ZSwJ.</p>
<p>Gupta, T., Zaki, M., Krishnan, N. A., and Mausam. Matscibert: A materials domain language model for text mining and information extraction. npj Computational Materials, 8(1):102, 2022.</p>
<p>Gupta, T., Zaki, M., Khatsuriya, D., Hira, K., Krishnan, N. M. A., and Mausam. DiSCoMaT: Distantly supervised composition extraction from tables in materials science articles. In Rogers, A., Boyd-Graber, J., and Okazaki, N. (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 13465-13483, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long. 753. URL https://aclanthology.org/2023.acl-long. 753.</p>
<p>He, K., Mao, R., Lin, Q., Ruan, Y., Lan, X., Feng, M., and Cambria, E. A survey of large language models for healthcare: from data, technology, and applications to accountability and ethics. arXiv preprint arXiv:2310.05694, 2023.</p>
<p>Hellwich, K.-H., Hartshorn, R. M., Yerin, A., Damhus, T., and Hutton, A. T. Brief guide to the nomenclature of organic chemistry (iupac technical report). Pure and Applied Chemistry, 92(3):527-539, 2020.</p>
<p>Hira, K., Zaki, M., Sheth, D. B., ., M., and Krishnan, N. M. A. Reconstructing materials tetrahedron: Challenges in materials information extraction. In AI for Accelerated Materials Design - NeurIPS 2023 Workshop, 2023. URL https://openreview.net/forum?id=u6ndfkFRJC.</p>
<p>Hsu, C., Verkuil, R., Liu, J., Lin, Z., Hie, B., Sercu, T., Lerer, A., and Rives, A. Learning inverse folding from millions of predicted structures. In Chaudhuri, K., Jegelka, S., Song, L., Szepesvari, C., Niu, G., and Sabato, S. (eds.), Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 8946-8970. PMLR, 17-23 Jul 2022. URL https://proceedings.mlr.press/v162/hsu22a.html.</p>
<p>Huang, S. and Cole, J. M. Batterybert: A pretrained language model for battery database enhancement. Journal of Chemical Information and Modeling, 62(24):6365-6377, 2022.</p>
<p>Jablonka, K. M., Ai, Q., Al-Feghali, A., Badhwar, S., Bocarsly, J. D., Bran, A. M., Bringuier, S., Brinson, L. C., Choudhary, K., Circi, D., et al. 14 examples of how llms can transform materials science and chemistry: a reflection on a large language model hackathon. Digital Discovery, 2(5):1233-1250, 2023.</p>
<p>Jensen, Z., Kwon, S., Schwalbe-Koda, D., Paris, C., Gómez-Bombarelli, R., Román-Leshkov, Y., Corma, A., Moliner, M., and Olivetti, E. A. Discovering relationships between osdas and zeolites through data mining and generative neural networks. ACS central science, 7(5):858-867, 2021.</p>
<p>Jiao, R., Huang, W., Lin, P., Han, J., Chen, P., Lu, Y., and Liu, Y. Crystal structure prediction by joint equivariant diffusion. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/ forum?id=DNdN26m2Jk.</p>
<p>Kanarik, K. J., Osowiecki, W. T., Lu, Y., Talukder, D., Roschewsky, N., Park, S. N., Kamon, M., Fried, D. M., and Gottscho, R. A. Human-machine collaboration for improving semiconductor process development. Nature, 616(7958):707-711, 2023.</p>
<p>Kasimuthumaniyan, S., Gosvami, N. N., and Krishnan, N. A. Towards understanding the scratchability in functional glasses. Ceramics International, 47(15):20821-20843, 2021.</p>
<p>Kearnes, S. M., Maser, M. R., Wleklinski, M., Kast, A., Doyle, A. G., Dreher, S. D., Hawkins, J. M., Jensen, K. F., and Coley, C. W. The open reaction database. Journal of the American Chemical Society, 143(45):18820-18826, 2021.</p>
<p>Kim, E., Huang, K., Saunders, A., McCallum, A., Ceder, G., and Olivetti, E. Materials synthesis insights from scientific literature via text extraction and machine learning. Chemistry of Materials, 29(21):9436-9444, 2017.</p>
<p>Kim, E., Jensen, Z., van Grootel, A., Huang, K., Staib, M., Mysore, S., Chang, H.-S., Strubell, E., McCallum, A., Jegelka, S., et al. Inorganic materials synthesis planning with literature-trained neural networks. Journal of chemical information and modeling, 60(3):1194-1201, 2020.</p>
<p>Kononova, O., He, T., Huo, H., Trewartha, A., Olivetti, E. A., and Ceder, G. Opportunities and challenges of text mining in materials research. Iscience, 24(3), 2021.</p>
<p>Lai, Y., Li, C., Wang, Y., Zhang, T., Zhong, R., Zettlemoyer, L., Yih, W.-t., Fried, D., Wang, S., and Yu, T. Ds-1000: A natural and reliable benchmark for data science code generation. In International Conference on Machine Learning, pp. 18319-18345. PMLR, 2023.</p>
<p>Lin, Z., Akin, H., Rao, R., Hie, B., Zhu, Z., Lu, W., Smetanin, N., Verkuil, R., Kabeli, O., Shmueli, Y., et al. Evolutionaryscale prediction of atomic-level protein structure with a language model. Science, 379(6637):1123-1130, 2023.</p>
<p>Lo, K., Wang, L. L., Neumann, M., Kinney, R., and Weld, D. S2ORC: The semantic scholar open research corpus. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 4969-4983, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.447. URL https://www.aclweb. org/anthology/2020.acl-main. 447.</p>
<p>Mercado, R., Kearnes, S. M., and Coley, C. W. Data sharing in chemistry: lessons learned and a case for mandating structured reaction data. Journal of Chemical Information and Modeling, 63(14):4253-4265, 2023.</p>
<p>Merchant, A., Batzner, S., Schoenholz, S. S., Aykol, M., Cheon, G., and Cubuk, E. D. Scaling deep learning for materials discovery. Nature, pp. 1-6, 2023.</p>
<p>Mirza, A., Alampara, N., Kunchapu, S., Emoekabu, B., Krishnan, A., Wilhelmi, M., Okereke, M., Eberhardt, J., Elahi, A. M., Greiner, M., et al. Are large language models superhuman chemists? arXiv preprint arXiv:2404.01475, 2024.</p>
<p>Mukaddem, K. T., Beard, E. J., Yildirim, B., and Cole, J. M. Imagedataextractor: a tool to extract and quantify data from microscopy images. Journal of chemical information and modeling, 60(5):2492-2509, 2019.</p>
<p>Mysore, S., Jensen, Z., Kim, E., Huang, K., Chang, H.-S., Strubell, E., Flanigan, J., McCallum, A., and Olivetti, E. The materials science procedural text corpus: Annotating materials synthesis procedures with shallow semantic structures. arXiv preprint arXiv:1905.06939, 2019.</p>
<p>Olivetti, E. A., Cole, J. M., Kim, E., Kononova, O., Ceder, G., Han, T. Y.-J., and Hiszpanski, A. M. Data-driven materials research enabled by natural language processing and information extraction. Applied Physics Reviews, 7(4), 2020.</p>
<p>Ravinder, Venugopal, V., Bishnoi, S., Singh, S., Zaki, M., Grover, H. S., Bauchy, M., Agarwal, M., and Krishnan, N. A. Artificial intelligence and machine learning in glass science and technology: 21 challenges for the 21st century. International journal of applied glass science, 12(3):277-292, 2021.</p>
<p>Rubungo, A. N., Arnold, C., Rand, B. P., and Dieng, A. B. Llm-prop: Predicting physical and electronic properties of crystalline solids from their text descriptions. arXiv preprint arXiv:2310.14029, 2023.</p>
<p>Sim, M., Vakili, M. G., Strieth-Kalthoff, F., Hao, H., Hickman, R., Miret, S., Pablo-García, S., and Aspuru-Guzik, A. Chemos 2.0: an orchestration architecture for chemical self-driving laboratories. 2023.</p>
<p>Song, Y., Miret, S., and Liu, B. MatSci-NLP: Evaluating scientific language models on materials science language tasks using text-to-schema modeling. In Rogers, A., Boyd-Graber, J., and Okazaki, N. (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 3621-3639, Toronto, Canada, July 2023a. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.201. URL https://aclanthology.org/2023.acl-long. 201.</p>
<p>Song, Y., Miret, S., Zhang, H., and Liu, B. Honeybee: Progressive instruction finetuning of large language models for materials science. In Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 5724-5739, 2023b.</p>
<p>Swain, M. C. and Cole, J. M. Chemdataextractor: a toolkit for automated extraction of chemical information from the scientific literature. Journal of chemical information and modeling, 56(10):1894-1904, 2016.</p>
<p>Szymanski, N. J., Rendy, B., Fei, Y., Kumar, R. E., He, T., Milsted, D., McDermott, M. J., Gallant, M., Cubuk, E. D., Merchant, A., et al. An autonomous laboratory for the accelerated synthesis of novel materials. Nature, 624(7990):86-91, 2023.</p>
<p>Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.</p>
<p>Vasudevan, R. K., Orozco, E., and Kalinin, S. V. Discovering mechanisms for materials microstructure optimization via reinforcement learning of a generative model. Machine Learning: Science and Technology, 3(4):04LT03, 2022.</p>
<p>Venugopal, V. and Olivetti, E. Matkg: An autonomously generated knowledge graph in material science. Scientific Data, 11 (1):217, 2024.</p>
<p>Venugopal, V., Sahoo, S., Zaki, M., Agarwal, M., Gosvami, N. N., and Krishnan, N. A. Looking through glass: Knowledge discovery from materials science literature using natural language processing. Patterns, 2(7), 2021.</p>
<p>Völker, C., Rug, T., Jablonka, K. M., and Kruschwitz, S. Llms can design sustainable concrete-a systematic benchmark. 2024.</p>
<p>Walker, N., Trewartha, A., Huo, H., Lee, S., Cruse, K., Dagdelen, J., Dunn, A., Persson, K., Ceder, G., and Jain, A. The impact of domain-specific pre-training on named entity recognition tasks in materials science. Available at SSRN 3950755, 2021.</p>
<p>Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824-24837, 2022.</p>
<p>White, A. D., Hocky, G. M., Gandhi, H. A., Ansari, M., Cox, S., Wellawatte, G. P., Sasmal, S., Yang, Z., Liu, K., Singh, Y., et al. Assessment of chemistry knowledge in large language models that generate code. Digital Discovery, 2(2):368-376, 2023.</p>
<p>Wu, S., Irsoy, O., Lu, S., Dabravolski, V., Dredze, M., Gehrmann, S., Kambadur, P., Rosenberg, D., and Mann, G. Bloomberggpt: A large language model for finance. arXiv preprint arXiv:2303.17564, 2023a.</p>
<p>Wu, Y., Jia, F., Zhang, S., Wu, Q., Li, H., Zhu, E., Wang, Y., Lee, Y. T., Peng, R., and Wang, C. An empirical study on challenging math problem solving with gpt-4. arXiv preprint arXiv:2306.01337, 2023b.</p>
<p>Xie, T., Fu, X., Ganea, O.-E., Barzilay, R., and Jaakkola, T. Crystal diffusion variational autoencoder for periodic material generation. arXiv preprint arXiv:2110.06197, 2021.</p>
<p>Xie, T., Wan, Y., Huang, W., Yin, Z., Liu, Y., Wang, S., Linghu, Q., Kit, C., Grazian, C., Zhang, W., et al. Darwin series: Domain specific large language models for natural science. arXiv preprint arXiv:2308.13565, 2023.</p>
<p>Xu, M., Yuan, X., Miret, S., and Tang, J. Protst: Multi-modality learning of protein sequences and biomedical texts. arXiv preprint arXiv:2301.12040, 2023.</p>
<p>Yoshikawa, N., Skreta, M., Darvish, K., Arellano-Rubach, S., Ji, Z., Bjørn Kristensen, L., Li, A. Z., Zhao, Y., Xu, H., Kuramshin, A., et al. Large language models for chemistry robotics. Autonomous Robots, 47(8):1057-1086, 2023.</p>
<p>Zaki, M., Krishnan, N. A., et al. Extracting processing and testing parameters from materials science literature for improved property prediction of glasses. Chemical Engineering and Processing-Process Intensification, 180:108607, 2022a.</p>
<p>Zaki, M., Namireddy, S. R., Pittie, T., Bihani, V., Keshri, S. R., Venugopal, V., Gosvami, N. N., Krishnan, N. A., et al. Natural language processing-guided meta-analysis and structure factor database extraction from glass literature. Journal of Non-Crystalline Solids: X, 15:100103, 2022b.</p>
<p>Zaki, M., Sharma, S., Gurjar, S. K., Goyal, R., Krishnan, N. A., et al. Cementron: Machine learning the alite and belite phases in cement clinker from optical images. Construction and Building Materials, 397:132425, 2023.</p>
<p>Zaki, M., Jayadeva, J., Mausam, M., and Krishnan, N. A. Mascqa: Investigating materials science knowledge of large language models. Digital Discovery, 2024.</p>
<p>Zan, D., Chen, B., Zhang, F., Lu, D., Wu, B., Guan, B., Yongji, W., and Lou, J.-G. Large language models meet nl2code: A survey. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 7443-7464, 2023.</p>
<p>Zeni, C., Pinsler, R., Zügner, D., Fowler, A., Horton, M., Fu, X., Shysheya, S., Crabbé, J., Sun, L., Smith, J., et al. Mattergen: a generative model for inorganic materials design. arXiv preprint arXiv:2312.03687, 2023.</p>
<p>Zhang, C., Zhang, C., Zhang, M., and Kweon, I. S. Text-to-image diffusion model in generative ai: A survey. arXiv preprint arXiv:2303.07909, 2023.</p>
<p>Zhao, J. and Cole, J. M. A database of refractive indices and dielectric constants auto-generated using chemdataextractor. Scientific data, 9(1):192, 2022.</p>
<p>Zhao, J., Huang, S., and Cole, J. M. Opticalbert and opticaltable-sqa: Text-and table-based language models for the optical-materials domain. Journal of Chemical Information and Modeling, 63(7):1961-1981, 2023.</p>
<h1>A. Materials Science Journals</h1>
<table>
<thead>
<tr>
<th style="text-align: center;">journal</th>
<th style="text-align: center;">papers</th>
<th style="text-align: center;">word_count</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Journal_of_Alloys_and_Compounds</td>
<td style="text-align: center;">66829</td>
<td style="text-align: center;">238537453</td>
</tr>
<tr>
<td style="text-align: center;">Science_of_The_Total_Environment</td>
<td style="text-align: center;">59490</td>
<td style="text-align: center;">354781803</td>
</tr>
<tr>
<td style="text-align: center;">Applied_Surface_Science</td>
<td style="text-align: center;">51627</td>
<td style="text-align: center;">191223396</td>
</tr>
<tr>
<td style="text-align: center;">Chemical_Engineering_Journal</td>
<td style="text-align: center;">42530</td>
<td style="text-align: center;">228873825</td>
</tr>
<tr>
<td style="text-align: center;">International_Journal_of_Hydrogen_Energy</td>
<td style="text-align: center;">39913</td>
<td style="text-align: center;">62269929</td>
</tr>
<tr>
<td style="text-align: center;">Journal_of_Biological_Chemistry</td>
<td style="text-align: center;">39370</td>
<td style="text-align: center;">9020708</td>
</tr>
<tr>
<td style="text-align: center;">Ceramics_International</td>
<td style="text-align: center;">37643</td>
<td style="text-align: center;">135734837</td>
</tr>
<tr>
<td style="text-align: center;">Journal_of_Cleaner_Production</td>
<td style="text-align: center;">37118</td>
<td style="text-align: center;">253572991</td>
</tr>
<tr>
<td style="text-align: center;">Chemosphere</td>
<td style="text-align: center;">36689</td>
<td style="text-align: center;">176012463</td>
</tr>
<tr>
<td style="text-align: center;">Electrochimica_Acta</td>
<td style="text-align: center;">34396</td>
<td style="text-align: center;">150818043</td>
</tr>
<tr>
<td style="text-align: center;">Food_Chemistry</td>
<td style="text-align: center;">33565</td>
<td style="text-align: center;">149308849</td>
</tr>
<tr>
<td style="text-align: center;">Nuclear_Instruments_and_Methods_in_Physics_Res...</td>
<td style="text-align: center;">32843</td>
<td style="text-align: center;">107224761</td>
</tr>
<tr>
<td style="text-align: center;">Materials_Science_and_Engineering:_A</td>
<td style="text-align: center;">32692</td>
<td style="text-align: center;">131945933</td>
</tr>
<tr>
<td style="text-align: center;">Materials_Letters</td>
<td style="text-align: center;">32352</td>
<td style="text-align: center;">50776687</td>
</tr>
<tr>
<td style="text-align: center;">Construction_and_Building_Materials</td>
<td style="text-align: center;">32032</td>
<td style="text-align: center;">176566669</td>
</tr>
<tr>
<td style="text-align: center;">Materials_Today:_Proceedings</td>
<td style="text-align: center;">29214</td>
<td style="text-align: center;">75989713</td>
</tr>
<tr>
<td style="text-align: center;">Chemical_Physics_Letters</td>
<td style="text-align: center;">29134</td>
<td style="text-align: center;">83262010</td>
</tr>
<tr>
<td style="text-align: center;">Journal_of_Power_Sources</td>
<td style="text-align: center;">29025</td>
<td style="text-align: center;">124779521</td>
</tr>
<tr>
<td style="text-align: center;">Journal_of_Magnetism_and_Magnetic_Materials</td>
<td style="text-align: center;">28963</td>
<td style="text-align: center;">78504883</td>
</tr>
<tr>
<td style="text-align: center;">Sensors_and_Actuators_B:_Chemical</td>
<td style="text-align: center;">28918</td>
<td style="text-align: center;">109397473</td>
</tr>
</tbody>
</table>
<p>Figure 3. List of 20 MatSci journal publications with maximum articles published along with the number of articles and their associated word counts that are available through publisher APIs for text mining.</p>
<p>Figure 3 shows the list of top 20 MatSci publications (in terms of the number of papers published). along with the number of papers and the associated word count. These represent the manuscripts that are available for download through the text-mining API. The complete list of journals is provided in https://github.com/M3RG-IITD/MatSci-LLM/.</p>
<h2>B. Crystal Information File of Silicon</h2>
<p>The following represents a CIF document of silicon atom. The meta data includes the source based on which the CIF is developed, that is, the journal publication details including author names, title of the paper, journal, vol, year and pages. Further, the complete lattice parameters are also included in the file. data_global
<em>chemical_name_mineral 'Silicon'
loop</em>
publ_author_name
'Kasper J S'
'Richards S M'
_journal_name_full 'Acta Crystallographica'
_journal_volume 17
_journal_year 1964
_journal_page_first 752
_journal_page_last 755</p>
<div class="codehilite"><pre><span></span><code>_publ_section_title
;
The crystal structures of new forms of silicon and germanium
;
_database_code_amcsd 0009282
_chemical_compound_source &#39;Synthetic&#39;
_chemical_formula_sum &#39;Si&#39;
_cell_length_a 6.636
_cell_length_b 6.636
_cell_length_c 6.636
_cell_angle_alpha 90
_cell_angle_beta 90
_cell_angle_gamma 90
_cell_volume 292.226
_exptl_crystal_density_diffrn 2.553
_symmetry_space_group_name_H-M &#39;I a 3&#39;
loop_
_space_group_symop_operation_xyz
&#39;x,y,z&#39;
&#39;1/2+x,1/2+y,1/2+z&#39;
&#39;1/2+z,x,1/2-y&#39;
&#39;+z,1/2+x,-y&#39;
&#39;1/2+z,-x,y&#39;
&#39;+z,1/2-x,1/2+y&#39;
&#39;-z,x,1/2+y&#39;
&#39;1/2-z,1/2+x,+y&#39;
&#39;-z,-x,-y&#39;
&#39;1/2-z,1/2-x,1/2-y&#39;
&#39;1/2+y,1/2-z,-x&#39;
&#39;+y,-z,1/2-x&#39;
&#39;-y,1/2-z,x&#39;
&#39;1/2-y,-z,1/2+x&#39;
&#39;1/2-y,z,-x&#39;
&#39;-y,1/2+z,1/2-x&#39;
&#39;y,z,x&#39;
&#39;1/2+y,1/2+z,1/2+x&#39;
&#39;x,1/2-y,1/2+z&#39;
&#39;1/2+x,-y,+z&#39;
&#39;-x,y,1/2+z&#39;
&#39;1/2-x,1/2+y,+z&#39;
&#39;x,1/2+y,-z&#39;
&#39;1/2+x,+y,1/2-z&#39;
&#39;-x,-y,-z&#39;
&#39;1/2-x,1/2-y,1/2-z&#39;
&#39;1/2-z,-x,1/2+y&#39;
&#39;-z,1/2-x,+y&#39;
&#39;1/2-z,x,-y&#39;
&#39;-z,1/2+x,1/2-y&#39;
&#39;z,-x,1/2-y&#39;
&#39;1/2+z,1/2-x,-y&#39;
&#39;z,x,y&#39;
&#39;1/2+z,1/2+x,1/2+y&#39;
&#39;1/2-y,1/2+z,x&#39;
&#39;-y,+z,1/2+x&#39;
</code></pre></div>

<div class="codehilite"><pre><span></span><code>&#39;y,1/2+z,-x&#39;
&#39;1/2+y,+z,1/2-x&#39;
&#39;1/2+y,-z,x&#39;
&#39;+y,1/2-z,1/2+x&#39;
&#39;-y,-z,-x&#39;
&#39;1/2-y,1/2-z,1/2-x&#39;
&#39;-x,1/2+y,1/2-z&#39;
&#39;1/2-x,+y,-z&#39;
&#39;x,-y,1/2-z&#39;
&#39;1/2+x,1/2-y,-z&#39;
&#39;-x,1/2-y,z&#39;
&#39;1/2-x,-y,1/2+z&#39;
loop_
_atom_site_label
_atom_site_fract_x
_atom_site_fract_y
_atom_site_fract_z
_atom_site_U_iso_or_equiv
Si 0.10030 0.10030 0.10030 0.01127
</code></pre></div>

<h1>C. Mistakes in Q\&amp;A by GPT4-CoT</h1>
<p>Following are some of the materials science questions where GPT4-CoT gave incorrect answers. We provide the detailed question and answer provided by the GPT4-CoT, the correct answer, and the mistake in the solution (highlighted in red).</p>
<h2>Question 1</h2>
<p>The materials belonging to which one of the following crystal classes would be both piezoelectric and ferroelectric: (A) 222
(B) $4 \mathrm{~mm}(\mathrm{C})-1$ (D) $2 / \mathrm{m}$</p>
<p>Correct: B, GPT4-CoT: D</p>
<h2>Solution by GPT4-CoT</h2>
<p>To determine which crystal class would exhibit both piezoelectric and ferroelectric properties, we need to understand the properties of each class and the criteria for displaying piezoelectric and ferroelectric behavior.</p>
<p>Piezoelectric materials: These materials generate an electrical charge when subjected to mechanical stress or strain. For a material to be piezoelectric, it must not have a center of symmetry (also called a center of inversion). In other words, it should belong to a non-centrosymmetric crystal class.</p>
<p>Ferroelectric materials: These materials possess a spontaneous electric polarization that can be reversed by an external electric field. Ferroelectric materials must also lack a center of symmetry and fall within a polar crystal class, meaning they have a unique polar axis.</p>
<p>Now, we can evaluate each crystal class:
(A) 222: This crystal class has a center of symmetry, so it cannot be either piezoelectric or ferroelectric.
(B) 4 mm : This crystal class does not have a center of symmetry, which means it can be piezoelectric. However, it is non-polar, meaning it cannot be ferroelectric.
(C) -1: This crystal class has a center of symmetry, so it cannot be either piezoelectric or ferroelectric.
(D) 2/m: This crystal class does not have a center of symmetry, which means it can be piezoelectric. Additionally, it is a polar crystal class, making it able to be ferroelectric as well.</p>
<p>Thus, among these crystal classes, only class $2 / \mathrm{m}$ would be both piezoelectric and ferroelectric.
The correct answer is: [D].</p>
<h1>Correct solution.</h1>
<p>$2 / \mathrm{m}$ is centro-symmetric, and hence it is not polar. In contrast, the 4 mm point group is polar.
Question 2 (Given: Universal gas constant, $\mathrm{R}=8.31 \mathrm{~J} \mathrm{~mol}^{-1} K^{-1}$, Faraday's constant, $\mathrm{F}=96500 \mathrm{C} \mathrm{mol}^{-1}$ ). A half cell consisting of pure Ni immersed in an aqueous solution containing $N i^{2+}$ ions of unknown concentration, is galvanically coupled with another half cell consisting of pure Cd immersed in a 1 M aqueous solution of $\mathrm{Cd}^{2+}$ ions. The temperature is $25^{\circ} \mathrm{C}$ and the pressure is 1 atm . The standard electrode reduction potentials of Ni and Cd are -0.250 V and -0.403 V , respectively. The voltage of the cell is found to be zero. The concentration (in micromolar) of $N i^{2+}$ in the solution is
Correct answer: 6 to 7.3, GPT4-CoT: 0</p>
<h2>Solution by GPT4-CoT</h2>
<p>Given the standard electrode reduction potentials of Ni and Cd are -0.250 V and -0.403 V , respectively. The cell voltage is found to be zero, which means the overall reaction is in equilibrium. We can use the Nernst equation to find the concentration of $N i^{2+}$ in the solution.</p>
<p>The Nernst equation is as follows:
$E=E^{0}-\frac{R T}{n F} \ln Q$
where, $E$ is the cell voltage, $E^{0}$ is the standard cell voltage, $R$ is the universal gas constant, $T$ is the temperature in Kelvin, $n$ is the number of moles of electrons transferred in the reaction, $F$ is Faraday's constant, and $Q$ is the reaction quotient.</p>
<p>Since the cell voltage is zero, the Nernst equation becomes:
$0=E_{N i}^{0}-E_{C d}^{0}-\frac{R T}{n F} \ln Q$
Before plugging in the known values, we need to convert the temperature to Kelvin:
Temperature $(\mathrm{T})=25^{\circ} \mathrm{C}+273.15=298.15 \mathrm{~K}$
Now we can plug in the values:
$0=(-0.250)-(-0.403)-\frac{8.31 \times 298.15}{2 \times 96500} \ln Q$
$0=0.153-\frac{2.4685 \times 10^{-4}}{2} \ln Q$
Solve for Q:
$\ln Q=\frac{0.153 \times 2}{2.4685 \times 10^{-4}}$
$\ln Q=1239.5083$
Take the exponent of both sides:
$Q=e^{1239.5083}$
Since the cell reaction is:
$N i^{2+}+C d \rightleftharpoons N i+C d^{2+}$
The reaction quotient Q is:
$Q=\frac{\left|C d^{2+}\right|}{\left|N i^{2+}\right|}$
Given that the concentration of $C d^{2+}$ is 1 M :
$1=\frac{\left|C d^{2+}\right|}{\left|N i^{2+}\right|}=\frac{1}{\left|N i^{2+}\right|}$
Therefore, the concentration of $N i^{2+}$ is:
$\left[N i^{2+}\right]=\frac{1}{Q}=\frac{1}{e^{1239.5083}}$
Since the question is asking for the concentration in micro molar, we will convert the concentration into micro molar:
$\left[N i^{2+}\right]=\frac{1}{e^{1239.5083}} \times 10^{6} \mu M$</p>            </div>
        </div>

    </div>
</body>
</html>
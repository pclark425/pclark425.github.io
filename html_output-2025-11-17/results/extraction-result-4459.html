<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4459 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4459</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4459</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-101.html">extraction-schema-101</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <p><strong>Paper ID:</strong> paper-278904821</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.18942v4.pdf" target="_blank">Language Models Surface the Unwritten Code of Science and Society</a></p>
                <p><strong>Paper Abstract:</strong> This paper calls on the research community not only to investigate how human biases are inherited by large language models (LLMs) but also to explore how these biases in LLMs can be leveraged to make society's"unwritten code"- such as implicit stereotypes and heuristics - visible and accessible for critique. We introduce a conceptual framework through a case study in science: uncovering hidden rules in peer review - the factors that reviewers care about but rarely state explicitly due to normative scientific expectations. The idea of the framework is to push LLMs to speak out their heuristics through generating self-consistent hypotheses - why one paper appeared stronger in reviewer scoring - among paired papers submitted to 45 academic conferences, while iteratively searching deeper hypotheses from remaining pairs where existing hypotheses cannot explain. We observed that LLMs'normative priors about the internal characteristics of good science extracted from their self-talk, e.g., theoretical rigor, were systematically updated toward posteriors that emphasize storytelling about external connections, such as how the work is positioned and connected within and across literatures. Human reviewers tend to explicitly reward aspects that moderately align with LLMs'normative priors (correlation = 0.49) but avoid articulating contextualization and storytelling posteriors in their review comments (correlation = -0.14), despite giving implicit reward to them with positive scores. These patterns are robust across different models and out-of-sample judgments. We discuss the broad applicability of our proposed framework, leveraging LLMs as diagnostic tools to amplify and surface the tacit codes underlying human society, enabling public discussion of revealed values and more precisely targeted responsible AI.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4459.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4459.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HypothesisGen Framework</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hypothesis Generation Framework (Prior–Posterior with Iterative Search)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A procedural framework that prompts LLMs to (1) generate priors (hypotheses without data), (2) produce hypotheses explaining paired-instance differences when given data, and (3) iteratively search new hypotheses for unexplained cases until coverage criteria are met.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Prior–Posterior Hypothesis Generation with Iterative Search</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>LLMs are prompted first to produce prior hypotheses about what makes a paper stronger (no data). Then, for sampled pairs of papers with a significant score gap, the LLM generates multiple hypotheses per round explaining why Paper A > Paper B. Each hypothesis is evaluated across the pairwise dataset to form a posterior (estimated as the proportion of pairs the hypothesis explains). Pairs not explained by any current hypotheses are sampled and used to generate new hypotheses in additional rounds. The iterative loop continues until unexplained pairs fall below a preset threshold (5%).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Measures include prior frequency (how often a hypothesis appears without data), posterior coverage/explanatory power (proportion of pairs explained), prior-to-posterior frequency shift (gain/loss), hypothesis distinctiveness (cosine similarity of posterior coverage), and convergence (proportion of unexplained pairs falling below threshold).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenAI o3-mini (hypothesis generation) and GPT-4o (matching/judging)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Science peer-review evaluation (computer science, physics, medicine, social sciences)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Evaluative/explanatory hypotheses about scholarly quality (why one paper is judged stronger than another)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Using 4 rounds × 5 hypotheses (20 hypotheses) the framework covered 97% of pairs under judge-model agreement; unexplained proportion target was <5%. Priors (5,000 generated across simulations) shifted toward narrative/contextual hypotheses in posterior; top gain hypotheses accounted for 61% of posterior attention but 15% of prior. Prior-to-posterior shifts and coverage were used as primary quantitative outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Hybrid — automated LLM-driven hypothesis generation and automated LLM judging to estimate posterior coverage, with human annotations used for validation of mentions and matching.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Cross-validation via out-of-sample pairs (second 5,000 pairs) showing posterior distributions correlated at 0.8 and >95% coverage; human annotation overlap (three master students) and dictionary-method cross-validation (correlation 0.7) validated matching of priors/posteriors.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Limited context windows required summarization of papers; computational cost prohibited exhaustive evaluation over all pairwise combinations; priors may reflect cultural model biases and may not map perfectly to human implicit reasoning; potential for LLMs to exaggerate priors; stopping threshold (5%) is heuristic.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Dataset of 26,731 submissions from 45 conferences (OpenReview + Paper Copilot); experiments run on random subsets (5,000 pairs main, +5,000 robustness).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models Surface the Unwritten Code of Science and Society', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4459.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4459.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Consistency Voting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Consistency Refinement with Confidence-Weighted 3-Fold Voting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method to increase reliability of LLM judgments by collecting multiple independent votes with scalar confidence and aggregating them by confidence-weighted majority.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Confidence-Weighted Self-Consistency Voting</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>For each (pair, hypothesis) judgment the LLM produces a binary label (0/1) and a confidence score (0–10). Three independent votes are collected (3-fold). Final label is the confidence-weighted majority: sum confidences for label 1 vs label 0; label with higher sum becomes final. Also compared to simple majority voting.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Primary metrics are final aggregated label reliability, self-consistency across rounds (proportion of identical votes across rounds), and confidence profiles distinguishing clear vs borderline cases.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Used with the same LLMs that judged hypotheses (GPT-4o as primary judge; experiments also across other LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Peer-review evaluation (as above)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Judgment labeling of explanatory hypotheses (binary explanatory assignment)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Confidence-weighted labels were highly correlated with majority-vote labels (correlation > 0.98). Confidence-weighting improved self-consistency and accelerated convergence according to cited prior work; authors report improved reliability and finer-grained uncertainty signal.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated aggregation of LLM outputs; validated against human annotation overlap and other voting schemes.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Compared to majority voting (correlation >0.98) and referenced literature showing confidence improves self-consistency; human annotations used to check labeling consistency (human-vs-LLM overlap 0.7).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>LLM-reported confidences can be miscalibrated; potential position biases and correlated errors across votes; dependence on prompt and sampling randomness.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Applied to 5,000 pair subset of the 26,731-paper dataset (evaluated across all sampled pairs).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models Surface the Unwritten Code of Science and Society', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4459.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4459.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ExplanatoryPower</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Explanatory Power as Posterior Coverage</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A quantitative metric defined as the proportion of pairwise cases that a given hypothesis successfully explains (based on LLM judging), used to operationalize hypothesis posterior probability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Posterior Coverage (Proportion Explained)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>For each hypothesis, the LLM judges every paper pair and produces a binary label indicating whether the hypothesis explains the observed score difference. The posterior is estimated as the fraction of all evaluated pairs labeled as 'explained' by that hypothesis. Hypotheses can overlap (one pair may be explained by multiple hypotheses).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Proportion of pairs explained (coverage), changes from prior frequency to posterior coverage (gain/loss), and collective coverage across hypothesis set (how many pairs explained overall).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenAI o3-mini (hypothesis gen) + GPT-4o (judging/matching)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Peer-review evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Explanatory hypotheses for evaluative differences</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Using the generated hypothesis set, cumulative posterior coverage reached ~97% of pairs (under judge agreement) after iterative search; out-of-sample posterior distributions correlated 0.8 with main set.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated LLM judgments produce the metric; human annotations used to validate whether human comments mention aspects corresponding to hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Validated via robustness checks including out-of-sample evaluation, cross-model replication, and human annotation matching (dictionary method correlation 0.7; human overlap 0.7).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Binary labeling simplifies gradations of explanatory strength; coverage conflates distinct causal directions; LLM judgments may reflect model priors rather than underlying causal mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models Surface the Unwritten Code of Science and Society', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4459.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4459.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prior-to-Posterior Shift</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prior vs Posterior Frequency Shift Analysis</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework to quantify how hypotheses' frequencies change from data-free priors to data-conditioned posteriors, revealing which evaluative criteria gain or lose importance when exposed to evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Prior–Posterior Frequency Shift</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Priors are collected by prompting the model without data; frequencies of hypotheses are tallied across repeated simulations. Posteriors are estimated by the proportion of pairs a hypothesis explains when the model sees actual pairwise data. The difference (posterior frequency minus prior frequency) ranks hypotheses by gains/losses and identifies shifts (e.g., from internal rigor to storytelling/contextualization).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Prior frequency, posterior coverage, prior-to-posterior gain/loss, ranking of hypotheses by shift magnitude, and aggregate share of posterior attention for top gainers/losers.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenAI o3-mini (priors) and GPT-4o (matching/posterior estimation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Peer-review evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Evaluative explanatory hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Top five hypotheses with largest prior-to-posterior gains accounted for 61% of posterior attention but only 15% of prior; converse for top five losses (17% posterior vs 68% prior). Correlations: mention frequency vs prior frequency = 0.49; mention frequency vs posterior coverage = -0.14.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated LLM production of priors/posteriors; human review comments annotated to compare explicit human mentions to model priors/posteriors.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Human annotations (GPT-4o and three student annotators) and dictionary-method cross-validation (correlation 0.7) used to validate matching between priors and posteriors.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Priors reflect model-corpus biases; matching priors to posteriors involves subjective matching decisions (mitigated via LLM matching and dictionary heuristic but not perfect).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models Surface the Unwritten Code of Science and Society', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4459.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4459.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Consistency & Confidence Metrics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Consistency and Confidence Difference Metrics</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Operational metrics measuring (a) consistency: proportion of pairs where the LLM casts the same vote across repeated rounds/position swaps, and (b) confidence: a scalar summary (difference in total confidence for chosen vs alternative label) reflecting decisiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Consistency Rate and Confidence Differential</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Consistency is measured as the share of judgments that remain identical across rounds or after position randomization (e.g., 77% consistency across randomized positions). Confidence is operationalized as the difference between summed confidences supporting the final weighted label and the summed confidences for the alternative; larger differences indicate higher decisiveness. Confidence also used to weight votes.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Consistency proportion across rounds/position swaps, confidence differential as measure of judgment strength, and how these change across iterative rounds/hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o (primary judge) and other LLMs in robustness checks</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Peer-review evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Judgment reliability metrics for explanatory hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Consistency across randomized positions was 77%; consistency between summarization-based judgments and full-text judgments was 0.9; models showed greater confidence in later rounds when evaluating narrative/contextual hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated LLM-derived metrics; human annotations used for complementary validation.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Empirical measurement across rounds, position-randomization experiments, and fidelity checks against full-text judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Confidence signals may be miscalibrated; consistency can mask systematic bias (consistent but wrong); position bias interacts with gap size (stronger when gap small).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models Surface the Unwritten Code of Science and Society', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4459.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4459.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CosineCoverageSim</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Cosine Similarity of Posterior Coverage Profiles</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A measure of distinctiveness between hypotheses computed as pairwise cosine similarity on binary posterior-coverage vectors (one-hot per pair indicating explained/not-explained).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Pairwise Cosine Similarity on Posterior Coverage</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Construct a binary vector for each hypothesis across all evaluated pairs (1 if hypothesis explains that pair, else 0). Compute pairwise cosine similarity across hypotheses to measure overlap and distinctiveness; reported mean similarity ~0.43 ± 0.02 indicating moderate distinctness.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Average pairwise cosine similarity and its variance to assess redundancy vs coverage diversity among hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Applied to posterior outputs from the judge models (GPT-4o and others)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Peer-review evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Hypothesis distinctiveness analysis</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Pairwise cosine similarity in posterior coverage = 0.43 ± 0.02, indicating hypotheses cover relatively different dimensions of the quality signal.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated metric computed from LLM labels.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Reported as robustness diagnostic across rounds and models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Binary coverage vectors remove graded explanatory strength; cosine similarity sensitive to sparsity and prevalence of hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models Surface the Unwritten Code of Science and Society', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4459.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4459.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HumanMentionRegression</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Regression of Human Mentions on Review Scores (OLS)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An OLS regression linking whether reviewers mention particular hypothesis-aspects (coded as praise/criticism/not mentioned) to the numerical review scores to assess explanatory power of explicit human mentions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>OLS Regression of Mentioned Aspects on Scores</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Reviewer comments are annotated for each hypothesis aspect as praise (1), not mentioned (0), or criticism (-1). These covariates are regressed on assigned review scores using OLS to see which mentioned aspects predict higher scores and to quantify explained variance (R^2).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Coefficient significance for aspects (t-tests), sign of coefficients (positive = higher score when aspect praised), and model explanatory power (R^2).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Annotations produced by GPT-4o (validated against human annotators) and regression run with scipy.stats</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Peer-review evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Associational analysis linking articulated criteria to scores</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Only five hypotheses were statistically significant predictors and all had positive coefficients; overall regression explanatory power R^2 = 0.06, indicating human-stated aspects explain little of score variance.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Hybrid — automated annotations (GPT-4o) supplemented with human annotation validation; regression is automated statistical analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Human annotators (three master's students) validated the annotation method; dictionary method cross-check produced high consistency (correlation 0.78) with LLM labels.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Low R^2 implies many implicit/unarticulated factors; annotations and dictionary heuristics imperfect; OLS is associational and not causal.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models Surface the Unwritten Code of Science and Society', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4459.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e4459.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PositionBiasAnalysis</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Position Bias Mitigation and Regression Analysis</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Procedures to detect and mitigate LLM position bias (favoring candidate in a specific prompt position) and an analysis quantifying how position-consistency depends on review-score gap.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Position-Randomization + Consistency Regression</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Randomize paper positions across voting runs, construct pairs with substantial score gaps to minimize position bias, and regress consistency rate (proportion of consistent judgments across swaps) on the review score gap to quantify bias dependence on gap size.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Consistency rate after position swap, regression coefficient significance linking gap size to consistency, and baseline consistency levels (e.g., 77%).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Judge LLMs (GPT-4o and others)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Peer-review evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Reliability/bias diagnostic for comparative judgments</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Consistency of judgments across randomized positions = 77%; regression shows larger score gaps are significantly associated with greater consistency (effect statistically significant but small).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated LLM voting with statistical analysis; human annotations used for complementary checks.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Empirical regression and consistency measurement across many randomized trials and gap strata.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Position bias stronger for small gaps; mitigation relies on constructing pairs with large gaps which can bias sample composition; randomization cannot fully eliminate systemic position preferences.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models Surface the Unwritten Code of Science and Society', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4459.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e4459.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DictionaryCrossVal</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dictionary-Based Cross-Validation for Matching Priors and Posteriors</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A heuristic keyword/dictionary method to match prior and posterior hypotheses and validate LLM matching judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Heuristic Dictionary Matching</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Create a dictionary of keywords mapping to hypothesis aspects; match posterior hypotheses to priors by locating keywords in prior-generated hypothesis text windows. Use this as a complementary method to LLM-based matching to cross-validate matches.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Correlation between dictionary-based matches and LLM-based matching (reported 0.7), and consistency with LLM-judged matches.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Method complements LLM matching (LLM used in parallel for matching)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Peer-review evaluation / hypothesis matching</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Textual matching/validation method</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Dictionary method correlates with LLM matching at 0.7 (p = 0.001), supporting reliability of match decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated heuristic matched and compared against LLM matching; human annotations used elsewhere for validation.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Statistical correlation between heuristic and LLM match labels; p-value reported.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Heuristic dictionary approach is brittle, can miss paraphrases/synonyms, and serves as a substitute rather than primary method.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models Surface the Unwritten Code of Science and Society', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4459.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e4459.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Summarization Fidelity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fidelity Experiments Using Embedding Overlap and Cosine Similarity</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Methods to validate that condensed extended abstracts faithfully represent full texts by comparing embeddings and distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Embedding Overlap + Cosine Similarity Fidelity Check</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Embed original full texts and generated extended abstracts using a scientific-paper pretrained embedding model; visualize distribution overlap via dimensionality reduction and kernel-density contours, and compute cosine similarities between paired embeddings. High overlap and cosine similarity (>0.95) indicate high fidelity; consistency of judgments comparing summaries vs full texts reported as 0.9.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Embedding cosine similarity distribution (most >0.95), overlap of 95% density contours, and judgment consistency between summary-based and full-text-based LLM judgments (0.9).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Scientific embedding model from [55] (pretrained on citation relations) and LLMs for downstream judgments</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Scientific text representation and evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Representation fidelity for downstream hypothesis evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Most summarized vs raw-text embedding cosine similarities >0.95; contour overlap indicates dense overlap; judgment consistency between full papers and extended abstracts = 0.9 across 1,000 judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated embedding-based metric validated by downstream consistency checks with LLM judgments and human inspection.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Empirical embedding comparisons and downstream judgment consistency checks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Embedding-based fidelity cannot detect subtle hallucinations or missing nuanced arguments; dependent on embedding model quality.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models Surface the Unwritten Code of Science and Society', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4459.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e4459.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RobustnessAcrossModels/OoS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Cross-Model Robustness and Out-of-Sample Generalizability Tests</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A set of experiments that replicate the framework across multiple LLM architectures and on out-of-sample data to test generality of hypotheses and evaluation results.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Cross-Model Replication and Out-of-Sample Evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Run the hypothesis generation and judging pipeline with alternative closed- and open-source LLMs (Deepseek, LLaMA-3.3-70B-Instruct, Claude variants, Mistral, Gemini) and test the hypothesis set on an independent 5,000-pair sample. Compare posterior distributions, hypothesis rankings, and agreement metrics across models and datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Inter-model agreement on key hypotheses (reported ~89% agreement for certain heuristics), correlation between posterior distributions across datasets (0.8), out-of-sample coverage (>95%), and consistency of prior patterns across models.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Deepseek-chat-v3-0324, LLaMA-3.3-70B-Instruct, Claude-Sonnet-4, Claude 3 Haiku, Mistral Medium 3.1, Gemini 2.0 Flash, OpenAI models</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>LLaMA-3.3-70B-Instruct (70B) and others where specified; many model sizes not declared for closed-source models</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Peer-review evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Evaluative explanatory hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Conclusions robust across models: consistent prior emphasis on normative rigor; systematic prior→posterior shift toward storytelling/contextualization across models; cross-model agreement on specific heuristics ~89%; out-of-sample posterior correlation 0.8 and >95% pairs explained by main posterior set.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated multi-model experiments with human annotation validation.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Replication across multiple models and independent dataset sample; human annotations and dictionary checks used for cross-validation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Models tested were intentionally from same company initially; closed-source models vary in training data/capacity which can affect cultural priors; some model sizes not disclosed making cross-capacity attribution difficult.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models Surface the Unwritten Code of Science and Society', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Hypothesis generation with large language models <em>(Rating: 2)</em></li>
                <li>Hypobench: Towards systematic and principled benchmarking for hypothesis generation <em>(Rating: 2)</em></li>
                <li>Confidence improves self-consistency in llms <em>(Rating: 2)</em></li>
                <li>Scientific hypothesis generation and validation: Methods, datasets, and future directions <em>(Rating: 2)</em></li>
                <li>Can llm feedback enhance review quality? a randomized study of 20k reviews at iclr <em>(Rating: 1)</em></li>
                <li>Can large language models provide useful feedback on research papers? a large-scale empirical analysis <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4459",
    "paper_id": "paper-278904821",
    "extraction_schema_id": "extraction-schema-101",
    "extracted_data": [
        {
            "name_short": "HypothesisGen Framework",
            "name_full": "Hypothesis Generation Framework (Prior–Posterior with Iterative Search)",
            "brief_description": "A procedural framework that prompts LLMs to (1) generate priors (hypotheses without data), (2) produce hypotheses explaining paired-instance differences when given data, and (3) iteratively search new hypotheses for unexplained cases until coverage criteria are met.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Prior–Posterior Hypothesis Generation with Iterative Search",
            "evaluation_method_description": "LLMs are prompted first to produce prior hypotheses about what makes a paper stronger (no data). Then, for sampled pairs of papers with a significant score gap, the LLM generates multiple hypotheses per round explaining why Paper A &gt; Paper B. Each hypothesis is evaluated across the pairwise dataset to form a posterior (estimated as the proportion of pairs the hypothesis explains). Pairs not explained by any current hypotheses are sampled and used to generate new hypotheses in additional rounds. The iterative loop continues until unexplained pairs fall below a preset threshold (5%).",
            "evaluation_criteria": "Measures include prior frequency (how often a hypothesis appears without data), posterior coverage/explanatory power (proportion of pairs explained), prior-to-posterior frequency shift (gain/loss), hypothesis distinctiveness (cosine similarity of posterior coverage), and convergence (proportion of unexplained pairs falling below threshold).",
            "model_name": "OpenAI o3-mini (hypothesis generation) and GPT-4o (matching/judging)",
            "model_size": null,
            "scientific_domain": "Science peer-review evaluation (computer science, physics, medicine, social sciences)",
            "theory_type": "Evaluative/explanatory hypotheses about scholarly quality (why one paper is judged stronger than another)",
            "human_comparison": true,
            "evaluation_results": "Using 4 rounds × 5 hypotheses (20 hypotheses) the framework covered 97% of pairs under judge-model agreement; unexplained proportion target was &lt;5%. Priors (5,000 generated across simulations) shifted toward narrative/contextual hypotheses in posterior; top gain hypotheses accounted for 61% of posterior attention but 15% of prior. Prior-to-posterior shifts and coverage were used as primary quantitative outputs.",
            "automated_vs_human_evaluation": "Hybrid — automated LLM-driven hypothesis generation and automated LLM judging to estimate posterior coverage, with human annotations used for validation of mentions and matching.",
            "validation_method": "Cross-validation via out-of-sample pairs (second 5,000 pairs) showing posterior distributions correlated at 0.8 and &gt;95% coverage; human annotation overlap (three master students) and dictionary-method cross-validation (correlation 0.7) validated matching of priors/posteriors.",
            "limitations_challenges": "Limited context windows required summarization of papers; computational cost prohibited exhaustive evaluation over all pairwise combinations; priors may reflect cultural model biases and may not map perfectly to human implicit reasoning; potential for LLMs to exaggerate priors; stopping threshold (5%) is heuristic.",
            "benchmark_dataset": "Dataset of 26,731 submissions from 45 conferences (OpenReview + Paper Copilot); experiments run on random subsets (5,000 pairs main, +5,000 robustness).",
            "uuid": "e4459.0",
            "source_info": {
                "paper_title": "Language Models Surface the Unwritten Code of Science and Society",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Self-Consistency Voting",
            "name_full": "Self-Consistency Refinement with Confidence-Weighted 3-Fold Voting",
            "brief_description": "A method to increase reliability of LLM judgments by collecting multiple independent votes with scalar confidence and aggregating them by confidence-weighted majority.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Confidence-Weighted Self-Consistency Voting",
            "evaluation_method_description": "For each (pair, hypothesis) judgment the LLM produces a binary label (0/1) and a confidence score (0–10). Three independent votes are collected (3-fold). Final label is the confidence-weighted majority: sum confidences for label 1 vs label 0; label with higher sum becomes final. Also compared to simple majority voting.",
            "evaluation_criteria": "Primary metrics are final aggregated label reliability, self-consistency across rounds (proportion of identical votes across rounds), and confidence profiles distinguishing clear vs borderline cases.",
            "model_name": "Used with the same LLMs that judged hypotheses (GPT-4o as primary judge; experiments also across other LLMs)",
            "model_size": null,
            "scientific_domain": "Peer-review evaluation (as above)",
            "theory_type": "Judgment labeling of explanatory hypotheses (binary explanatory assignment)",
            "human_comparison": true,
            "evaluation_results": "Confidence-weighted labels were highly correlated with majority-vote labels (correlation &gt; 0.98). Confidence-weighting improved self-consistency and accelerated convergence according to cited prior work; authors report improved reliability and finer-grained uncertainty signal.",
            "automated_vs_human_evaluation": "Automated aggregation of LLM outputs; validated against human annotation overlap and other voting schemes.",
            "validation_method": "Compared to majority voting (correlation &gt;0.98) and referenced literature showing confidence improves self-consistency; human annotations used to check labeling consistency (human-vs-LLM overlap 0.7).",
            "limitations_challenges": "LLM-reported confidences can be miscalibrated; potential position biases and correlated errors across votes; dependence on prompt and sampling randomness.",
            "benchmark_dataset": "Applied to 5,000 pair subset of the 26,731-paper dataset (evaluated across all sampled pairs).",
            "uuid": "e4459.1",
            "source_info": {
                "paper_title": "Language Models Surface the Unwritten Code of Science and Society",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "ExplanatoryPower",
            "name_full": "Explanatory Power as Posterior Coverage",
            "brief_description": "A quantitative metric defined as the proportion of pairwise cases that a given hypothesis successfully explains (based on LLM judging), used to operationalize hypothesis posterior probability.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Posterior Coverage (Proportion Explained)",
            "evaluation_method_description": "For each hypothesis, the LLM judges every paper pair and produces a binary label indicating whether the hypothesis explains the observed score difference. The posterior is estimated as the fraction of all evaluated pairs labeled as 'explained' by that hypothesis. Hypotheses can overlap (one pair may be explained by multiple hypotheses).",
            "evaluation_criteria": "Proportion of pairs explained (coverage), changes from prior frequency to posterior coverage (gain/loss), and collective coverage across hypothesis set (how many pairs explained overall).",
            "model_name": "OpenAI o3-mini (hypothesis gen) + GPT-4o (judging/matching)",
            "model_size": null,
            "scientific_domain": "Peer-review evaluation",
            "theory_type": "Explanatory hypotheses for evaluative differences",
            "human_comparison": true,
            "evaluation_results": "Using the generated hypothesis set, cumulative posterior coverage reached ~97% of pairs (under judge agreement) after iterative search; out-of-sample posterior distributions correlated 0.8 with main set.",
            "automated_vs_human_evaluation": "Automated LLM judgments produce the metric; human annotations used to validate whether human comments mention aspects corresponding to hypotheses.",
            "validation_method": "Validated via robustness checks including out-of-sample evaluation, cross-model replication, and human annotation matching (dictionary method correlation 0.7; human overlap 0.7).",
            "limitations_challenges": "Binary labeling simplifies gradations of explanatory strength; coverage conflates distinct causal directions; LLM judgments may reflect model priors rather than underlying causal mechanisms.",
            "uuid": "e4459.2",
            "source_info": {
                "paper_title": "Language Models Surface the Unwritten Code of Science and Society",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Prior-to-Posterior Shift",
            "name_full": "Prior vs Posterior Frequency Shift Analysis",
            "brief_description": "A framework to quantify how hypotheses' frequencies change from data-free priors to data-conditioned posteriors, revealing which evaluative criteria gain or lose importance when exposed to evidence.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Prior–Posterior Frequency Shift",
            "evaluation_method_description": "Priors are collected by prompting the model without data; frequencies of hypotheses are tallied across repeated simulations. Posteriors are estimated by the proportion of pairs a hypothesis explains when the model sees actual pairwise data. The difference (posterior frequency minus prior frequency) ranks hypotheses by gains/losses and identifies shifts (e.g., from internal rigor to storytelling/contextualization).",
            "evaluation_criteria": "Prior frequency, posterior coverage, prior-to-posterior gain/loss, ranking of hypotheses by shift magnitude, and aggregate share of posterior attention for top gainers/losers.",
            "model_name": "OpenAI o3-mini (priors) and GPT-4o (matching/posterior estimation)",
            "model_size": null,
            "scientific_domain": "Peer-review evaluation",
            "theory_type": "Evaluative explanatory hypotheses",
            "human_comparison": true,
            "evaluation_results": "Top five hypotheses with largest prior-to-posterior gains accounted for 61% of posterior attention but only 15% of prior; converse for top five losses (17% posterior vs 68% prior). Correlations: mention frequency vs prior frequency = 0.49; mention frequency vs posterior coverage = -0.14.",
            "automated_vs_human_evaluation": "Automated LLM production of priors/posteriors; human review comments annotated to compare explicit human mentions to model priors/posteriors.",
            "validation_method": "Human annotations (GPT-4o and three student annotators) and dictionary-method cross-validation (correlation 0.7) used to validate matching between priors and posteriors.",
            "limitations_challenges": "Priors reflect model-corpus biases; matching priors to posteriors involves subjective matching decisions (mitigated via LLM matching and dictionary heuristic but not perfect).",
            "uuid": "e4459.3",
            "source_info": {
                "paper_title": "Language Models Surface the Unwritten Code of Science and Society",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Consistency & Confidence Metrics",
            "name_full": "Consistency and Confidence Difference Metrics",
            "brief_description": "Operational metrics measuring (a) consistency: proportion of pairs where the LLM casts the same vote across repeated rounds/position swaps, and (b) confidence: a scalar summary (difference in total confidence for chosen vs alternative label) reflecting decisiveness.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Consistency Rate and Confidence Differential",
            "evaluation_method_description": "Consistency is measured as the share of judgments that remain identical across rounds or after position randomization (e.g., 77% consistency across randomized positions). Confidence is operationalized as the difference between summed confidences supporting the final weighted label and the summed confidences for the alternative; larger differences indicate higher decisiveness. Confidence also used to weight votes.",
            "evaluation_criteria": "Consistency proportion across rounds/position swaps, confidence differential as measure of judgment strength, and how these change across iterative rounds/hypotheses.",
            "model_name": "GPT-4o (primary judge) and other LLMs in robustness checks",
            "model_size": null,
            "scientific_domain": "Peer-review evaluation",
            "theory_type": "Judgment reliability metrics for explanatory hypotheses",
            "human_comparison": true,
            "evaluation_results": "Consistency across randomized positions was 77%; consistency between summarization-based judgments and full-text judgments was 0.9; models showed greater confidence in later rounds when evaluating narrative/contextual hypotheses.",
            "automated_vs_human_evaluation": "Automated LLM-derived metrics; human annotations used for complementary validation.",
            "validation_method": "Empirical measurement across rounds, position-randomization experiments, and fidelity checks against full-text judgments.",
            "limitations_challenges": "Confidence signals may be miscalibrated; consistency can mask systematic bias (consistent but wrong); position bias interacts with gap size (stronger when gap small).",
            "uuid": "e4459.4",
            "source_info": {
                "paper_title": "Language Models Surface the Unwritten Code of Science and Society",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "CosineCoverageSim",
            "name_full": "Cosine Similarity of Posterior Coverage Profiles",
            "brief_description": "A measure of distinctiveness between hypotheses computed as pairwise cosine similarity on binary posterior-coverage vectors (one-hot per pair indicating explained/not-explained).",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Pairwise Cosine Similarity on Posterior Coverage",
            "evaluation_method_description": "Construct a binary vector for each hypothesis across all evaluated pairs (1 if hypothesis explains that pair, else 0). Compute pairwise cosine similarity across hypotheses to measure overlap and distinctiveness; reported mean similarity ~0.43 ± 0.02 indicating moderate distinctness.",
            "evaluation_criteria": "Average pairwise cosine similarity and its variance to assess redundancy vs coverage diversity among hypotheses.",
            "model_name": "Applied to posterior outputs from the judge models (GPT-4o and others)",
            "model_size": null,
            "scientific_domain": "Peer-review evaluation",
            "theory_type": "Hypothesis distinctiveness analysis",
            "human_comparison": true,
            "evaluation_results": "Pairwise cosine similarity in posterior coverage = 0.43 ± 0.02, indicating hypotheses cover relatively different dimensions of the quality signal.",
            "automated_vs_human_evaluation": "Automated metric computed from LLM labels.",
            "validation_method": "Reported as robustness diagnostic across rounds and models.",
            "limitations_challenges": "Binary coverage vectors remove graded explanatory strength; cosine similarity sensitive to sparsity and prevalence of hypotheses.",
            "uuid": "e4459.5",
            "source_info": {
                "paper_title": "Language Models Surface the Unwritten Code of Science and Society",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "HumanMentionRegression",
            "name_full": "Regression of Human Mentions on Review Scores (OLS)",
            "brief_description": "An OLS regression linking whether reviewers mention particular hypothesis-aspects (coded as praise/criticism/not mentioned) to the numerical review scores to assess explanatory power of explicit human mentions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "OLS Regression of Mentioned Aspects on Scores",
            "evaluation_method_description": "Reviewer comments are annotated for each hypothesis aspect as praise (1), not mentioned (0), or criticism (-1). These covariates are regressed on assigned review scores using OLS to see which mentioned aspects predict higher scores and to quantify explained variance (R^2).",
            "evaluation_criteria": "Coefficient significance for aspects (t-tests), sign of coefficients (positive = higher score when aspect praised), and model explanatory power (R^2).",
            "model_name": "Annotations produced by GPT-4o (validated against human annotators) and regression run with scipy.stats",
            "model_size": null,
            "scientific_domain": "Peer-review evaluation",
            "theory_type": "Associational analysis linking articulated criteria to scores",
            "human_comparison": true,
            "evaluation_results": "Only five hypotheses were statistically significant predictors and all had positive coefficients; overall regression explanatory power R^2 = 0.06, indicating human-stated aspects explain little of score variance.",
            "automated_vs_human_evaluation": "Hybrid — automated annotations (GPT-4o) supplemented with human annotation validation; regression is automated statistical analysis.",
            "validation_method": "Human annotators (three master's students) validated the annotation method; dictionary method cross-check produced high consistency (correlation 0.78) with LLM labels.",
            "limitations_challenges": "Low R^2 implies many implicit/unarticulated factors; annotations and dictionary heuristics imperfect; OLS is associational and not causal.",
            "uuid": "e4459.6",
            "source_info": {
                "paper_title": "Language Models Surface the Unwritten Code of Science and Society",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "PositionBiasAnalysis",
            "name_full": "Position Bias Mitigation and Regression Analysis",
            "brief_description": "Procedures to detect and mitigate LLM position bias (favoring candidate in a specific prompt position) and an analysis quantifying how position-consistency depends on review-score gap.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Position-Randomization + Consistency Regression",
            "evaluation_method_description": "Randomize paper positions across voting runs, construct pairs with substantial score gaps to minimize position bias, and regress consistency rate (proportion of consistent judgments across swaps) on the review score gap to quantify bias dependence on gap size.",
            "evaluation_criteria": "Consistency rate after position swap, regression coefficient significance linking gap size to consistency, and baseline consistency levels (e.g., 77%).",
            "model_name": "Judge LLMs (GPT-4o and others)",
            "model_size": null,
            "scientific_domain": "Peer-review evaluation",
            "theory_type": "Reliability/bias diagnostic for comparative judgments",
            "human_comparison": true,
            "evaluation_results": "Consistency of judgments across randomized positions = 77%; regression shows larger score gaps are significantly associated with greater consistency (effect statistically significant but small).",
            "automated_vs_human_evaluation": "Automated LLM voting with statistical analysis; human annotations used for complementary checks.",
            "validation_method": "Empirical regression and consistency measurement across many randomized trials and gap strata.",
            "limitations_challenges": "Position bias stronger for small gaps; mitigation relies on constructing pairs with large gaps which can bias sample composition; randomization cannot fully eliminate systemic position preferences.",
            "uuid": "e4459.7",
            "source_info": {
                "paper_title": "Language Models Surface the Unwritten Code of Science and Society",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "DictionaryCrossVal",
            "name_full": "Dictionary-Based Cross-Validation for Matching Priors and Posteriors",
            "brief_description": "A heuristic keyword/dictionary method to match prior and posterior hypotheses and validate LLM matching judgments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Heuristic Dictionary Matching",
            "evaluation_method_description": "Create a dictionary of keywords mapping to hypothesis aspects; match posterior hypotheses to priors by locating keywords in prior-generated hypothesis text windows. Use this as a complementary method to LLM-based matching to cross-validate matches.",
            "evaluation_criteria": "Correlation between dictionary-based matches and LLM-based matching (reported 0.7), and consistency with LLM-judged matches.",
            "model_name": "Method complements LLM matching (LLM used in parallel for matching)",
            "model_size": null,
            "scientific_domain": "Peer-review evaluation / hypothesis matching",
            "theory_type": "Textual matching/validation method",
            "human_comparison": true,
            "evaluation_results": "Dictionary method correlates with LLM matching at 0.7 (p = 0.001), supporting reliability of match decisions.",
            "automated_vs_human_evaluation": "Automated heuristic matched and compared against LLM matching; human annotations used elsewhere for validation.",
            "validation_method": "Statistical correlation between heuristic and LLM match labels; p-value reported.",
            "limitations_challenges": "Heuristic dictionary approach is brittle, can miss paraphrases/synonyms, and serves as a substitute rather than primary method.",
            "uuid": "e4459.8",
            "source_info": {
                "paper_title": "Language Models Surface the Unwritten Code of Science and Society",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Summarization Fidelity",
            "name_full": "Fidelity Experiments Using Embedding Overlap and Cosine Similarity",
            "brief_description": "Methods to validate that condensed extended abstracts faithfully represent full texts by comparing embeddings and distributions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Embedding Overlap + Cosine Similarity Fidelity Check",
            "evaluation_method_description": "Embed original full texts and generated extended abstracts using a scientific-paper pretrained embedding model; visualize distribution overlap via dimensionality reduction and kernel-density contours, and compute cosine similarities between paired embeddings. High overlap and cosine similarity (&gt;0.95) indicate high fidelity; consistency of judgments comparing summaries vs full texts reported as 0.9.",
            "evaluation_criteria": "Embedding cosine similarity distribution (most &gt;0.95), overlap of 95% density contours, and judgment consistency between summary-based and full-text-based LLM judgments (0.9).",
            "model_name": "Scientific embedding model from [55] (pretrained on citation relations) and LLMs for downstream judgments",
            "model_size": null,
            "scientific_domain": "Scientific text representation and evaluation",
            "theory_type": "Representation fidelity for downstream hypothesis evaluation",
            "human_comparison": true,
            "evaluation_results": "Most summarized vs raw-text embedding cosine similarities &gt;0.95; contour overlap indicates dense overlap; judgment consistency between full papers and extended abstracts = 0.9 across 1,000 judgments.",
            "automated_vs_human_evaluation": "Automated embedding-based metric validated by downstream consistency checks with LLM judgments and human inspection.",
            "validation_method": "Empirical embedding comparisons and downstream judgment consistency checks.",
            "limitations_challenges": "Embedding-based fidelity cannot detect subtle hallucinations or missing nuanced arguments; dependent on embedding model quality.",
            "uuid": "e4459.9",
            "source_info": {
                "paper_title": "Language Models Surface the Unwritten Code of Science and Society",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "RobustnessAcrossModels/OoS",
            "name_full": "Cross-Model Robustness and Out-of-Sample Generalizability Tests",
            "brief_description": "A set of experiments that replicate the framework across multiple LLM architectures and on out-of-sample data to test generality of hypotheses and evaluation results.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Cross-Model Replication and Out-of-Sample Evaluation",
            "evaluation_method_description": "Run the hypothesis generation and judging pipeline with alternative closed- and open-source LLMs (Deepseek, LLaMA-3.3-70B-Instruct, Claude variants, Mistral, Gemini) and test the hypothesis set on an independent 5,000-pair sample. Compare posterior distributions, hypothesis rankings, and agreement metrics across models and datasets.",
            "evaluation_criteria": "Inter-model agreement on key hypotheses (reported ~89% agreement for certain heuristics), correlation between posterior distributions across datasets (0.8), out-of-sample coverage (&gt;95%), and consistency of prior patterns across models.",
            "model_name": "Deepseek-chat-v3-0324, LLaMA-3.3-70B-Instruct, Claude-Sonnet-4, Claude 3 Haiku, Mistral Medium 3.1, Gemini 2.0 Flash, OpenAI models",
            "model_size": "LLaMA-3.3-70B-Instruct (70B) and others where specified; many model sizes not declared for closed-source models",
            "scientific_domain": "Peer-review evaluation",
            "theory_type": "Evaluative explanatory hypotheses",
            "human_comparison": true,
            "evaluation_results": "Conclusions robust across models: consistent prior emphasis on normative rigor; systematic prior→posterior shift toward storytelling/contextualization across models; cross-model agreement on specific heuristics ~89%; out-of-sample posterior correlation 0.8 and &gt;95% pairs explained by main posterior set.",
            "automated_vs_human_evaluation": "Automated multi-model experiments with human annotation validation.",
            "validation_method": "Replication across multiple models and independent dataset sample; human annotations and dictionary checks used for cross-validation.",
            "limitations_challenges": "Models tested were intentionally from same company initially; closed-source models vary in training data/capacity which can affect cultural priors; some model sizes not disclosed making cross-capacity attribution difficult.",
            "uuid": "e4459.10",
            "source_info": {
                "paper_title": "Language Models Surface the Unwritten Code of Science and Society",
                "publication_date_yy_mm": "2025-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Hypothesis generation with large language models",
            "rating": 2,
            "sanitized_title": "hypothesis_generation_with_large_language_models"
        },
        {
            "paper_title": "Hypobench: Towards systematic and principled benchmarking for hypothesis generation",
            "rating": 2,
            "sanitized_title": "hypobench_towards_systematic_and_principled_benchmarking_for_hypothesis_generation"
        },
        {
            "paper_title": "Confidence improves self-consistency in llms",
            "rating": 2,
            "sanitized_title": "confidence_improves_selfconsistency_in_llms"
        },
        {
            "paper_title": "Scientific hypothesis generation and validation: Methods, datasets, and future directions",
            "rating": 2,
            "sanitized_title": "scientific_hypothesis_generation_and_validation_methods_datasets_and_future_directions"
        },
        {
            "paper_title": "Can llm feedback enhance review quality? a randomized study of 20k reviews at iclr",
            "rating": 1,
            "sanitized_title": "can_llm_feedback_enhance_review_quality_a_randomized_study_of_20k_reviews_at_iclr"
        },
        {
            "paper_title": "Can large language models provide useful feedback on research papers? a large-scale empirical analysis",
            "rating": 1,
            "sanitized_title": "can_large_language_models_provide_useful_feedback_on_research_papers_a_largescale_empirical_analysis"
        }
    ],
    "cost": 0.017686999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Language Models Surface the Unwritten Code of Science and Society
9 Oct 2025</p>
<p>Honglin Bao honglinbao@uchicago.edu 
University of Chicago</p>
<p>Siyang Wu 
University of Chicago</p>
<p>Jiwoong Choi 
University of Chicago</p>
<p>Yingrong Mao 
University of Chicago</p>
<p>James A Evans 
University of Chicago</p>
<p>Language Models Surface the Unwritten Code of Science and Society
9 Oct 2025BBF9C1CAAB36C43C65F177A0035CF2D6arXiv:2505.18942v4[cs.CY]
This paper calls on the research community not only to investigate how human biases are inherited by large language models (LLMs) but also to explore how these biases in LLMs can be leveraged to make society's "unwritten code" -such as implicit stereotypes and heuristics -visible and accessible for critique.We introduce a conceptual framework through a case study in science: uncovering hidden rules in peer reviewthe factors that reviewers care about but rarely state explicitly due to normative scientific expectations.The idea of the framework is to push LLMs to speak out their heuristics through generating self-consistent hypotheses -why one paper appeared stronger in reviewer scoring -among paired papers submitted to 45 academic conferences, while iteratively searching deeper hypotheses from remaining pairs where existing hypotheses cannot explain.We observed that LLMs' normative priors about the internal characteristics of good science extracted from their self-talk, e.g., theoretical rigor, were systematically updated toward posteriors that emphasize storytelling about external connections, such as how the work is positioned and connected within and across literatures.Human reviewers tend to explicitly reward aspects that moderately align with LLMs' normative priors (correlation = 0.49) but avoid articulating contextualization and storytelling posteriors in their review comments (correlation = -0.14),despite giving implicit reward to them with positive scores.These patterns are robust across different models and out-of-sample judgments.We discuss the broad applicability of our proposed framework, leveraging LLMs as diagnostic tools to amplify and surface the tacit codes underlying human society, enabling public discussion of revealed values and more precisely targeted responsible AI.</p>
<p>Introduction</p>
<p>Large language models (LLMs), extensively trained on vast corpora of human-generated text and visuals, inevitably absorb and reflect the patterns of human thought embedded within these datasets.Researchers have explored how AI can be used to judge human activities such as scientific research [37], misinformation oversight [12], and perceptions of physical appearance [14].With strong predictive capacities, LLMs can replicate shifts in public opinion and human beliefs on contentious political issues, including same-sex marriage [26] and climate change [34].These models often reveal biases that closely align with directly measured human biases and even amplify them across various social dimensions [73], such as gender [27], race [23], social identity [24], and profession [21].Such biases are not computational artifacts but reflect the deeper cultural and social frameworks that shape human communication [6,33,19].The recognition that LLMs encapsulate and propagate societal biases and heuristics is now well-established in literature, framing these models as inherently cultural and social technologies [11,16,65,46,35].This paper moves beyond merely acknowledging LLM biases; it proposes leveraging them as diagnostic tools to uncover and critically examine the implicit, unwritten societal codes that govern human interaction.</p>
<p>The "unwritten code" -comprising tacit norms, implicit stereotypes, and subtle conventions that invisibly influence societal dynamics -represents a potent force that, if left unexamined, unsummarized, and unaddressed, can perpetuate systemic inequities and injustices.Leveraging the inherent biases embedded within LLMs can shed light on otherwise elusive patterns that sustain societal inequality.One straightforward path to mitigating human bias involves summarizing and clearly articulating the biases humans hold, subsequently enabling evaluation, regulation and systemic change.Nevertheless, these biases frequently exist "unwritten" and thus resist formalization and systematic analysis.Even though some methods can expose bias in LLMs when models appear to avoid expressing it, as by analyzing associated words [7], there remains a deeper layer of unwritten code that contemporary simple techniques cannot recover.These deeper structures activate during the model's reasoning process, serving as heuristics that streamline its efficiency 1 .An algorithmic framework capable of extracting and explicitly articulating them through self-reinforcement and amplification of the rationale within LLMs, which are inherently imbued with human biases and heuristics, could illuminate the content and character of bias within models and human cultures.</p>
<p>Thus, this paper calls for an intentional shift in perspective: We call on researchers not only to investigate how human biases are inherited by LLMs but also to explore how these biases can be leveraged to make the unwritten codes visible, as shown in Figure 1.To this end, we present a conceptual framework that enables this process by pushing LLMs to speak out their heuristics through searching their deeper self-consistent hypotheses that can explain their decision-making.By prompting models to think out of the box and "exaggerate" hidden rules that humans might be unable or unwilling to acknowledge openly, we can more clearly illustrate how such biases and heuristics operate and which groups they potentially disadvantage.</p>
<p>A Case Study: The Aesthetic of Peer Review</p>
<p>What makes a scientific paper "good"?While scientific communities often emphasize norms such as rigor, the actual process of evaluating quality is far more entangled with social dynamics, implicit biases, and tacit expectations [8].Peer review, the central gatekeeping mechanism of science, is deeply shaped by these social dynamics.Despite its appearance of objectivity, peer review can be biased toward well-known institutions, established authors, or fashionable topics [31].Reviewers may assign high scores to work that aligns with unspoken disciplinary tastes or collective values [57], while "lazily" masking these preferences in the language of standardized evaluation criteria [48].As a result, much of what constitutes the tacit code of a field -its evaluative culture -remains hidden beneath the surface, shaping scientific recognition in nontransparent ways that remain hard to alter.Data: We collected data from the OpenReview API and Paper Copilot2 , a website that aggregates peer review scores (mostly) for computer science conference submissions.Our final dataset includes metadata, peer review scores, and review comments for 26,731 papers submitted to 45 conferences including computer science, physics, medicine, and social sciences.We use peer review scores as a proxy for the paper's perceived quality, as shown in Appendix A, due to the consistency and high confidence of judgments among reviewers.</p>
<p>To represent each paper's content, we follow the approach of [71], constructing an extended abstract that integrates additional structured elements -contextual background, key ideas, methodological and theoretical details, experiments and results, and mentioned impact.We generate these extended abstracts using prompts used in [71] with GPT-4o.This representation generation has been empirically shown to achieve high quality across evaluation metrics and to closely match human annotations.It also demonstrates strong alignment with the original text and a low rate of hallucination [4,71], see Appendix E for fidelity experiments.We deliberately avoid feed-ing full texts directly to LLMs for two main reasons.First, context windows remain limited, preventing broad coverage that would enable more generalizable hypotheses.Current approaches, e.g., [45], employ reduction methods such as sparse autoencoders to compress long inputs into activated neurons for hypothesis generation, but these are typically applied only to straightforward cases (e.g., political attitudes) where activations can be reduced to obvious dimensions (e.g., gender and race).Second, prior work has shown that LLMs often struggle with long, unstructured inputs, leading to degraded performance and increased hallucination rates mid-text [41,68].When presented with lengthy texts, attention may diffuse across irrelevant sections, impairing reasoning over core content.To address these issues, we adopt a curated and condensed input format, ensuring that the model attends to the most relevant information.Dataset and paper representations are detailed in Appendix H.Most importantly, robustness checks in the Appendix E show that LLM judgments on full papers and extended abstracts yield highly consistent results (consistency = 0.9).</p>
<p>The Framework: Prior, Posterior, and Hypothesis Generation</p>
<p>We adopt a hypothesis generation framework -a topic increasingly explored in the automation of scientific discovery using AI [49,40,9,39] -to update LLMs' stereotyped priors about good science into posteriors that reflect how human evaluators and LLMs actually judge.</p>
<p>Hypothesis Generation: We begin by constructing a pairwise dataset.For each conference, we identify all pairs of papers where one paper has received a significantly higher review score than the other.To reduce noise due to randomness, we define a "significant" score gap as one that exceeds one standard deviation within the score distribution of that conference.From this pool, we randomly sample 50 paper pairs (due to the limited contextual window) and feed them into an LLM.The LLM is tasked with reading, understanding, and comparing the papers' extended abstracts, and then generating five hypotheses to explain why the better scoring paper may appear stronger than the other.</p>
<p>Self-consistency Refinement: We apply the five generated hypotheses to all pairs in the dataset.For each pair and hypothesis, we ask the LLM to determine the degree to which the hypothesis explains the observed quality difference (human review scores) using the following prompt: "Analyze the content from both papers to determine whether Paper 2 is superior in the aspects identified in the hypothesis (i.e., if the comparison of content supports the hypothesis)."This type of self-confirmation explicitly reveals the inherent biases of LLMs during the judgment process.The LLM outputs two things: (1) a binary label (0 or 1) indicating whether the hypothesis explains the score difference, and (2) a confidence score (0-10) for its judgment.We use 3-fold voting and compute the final label using a confidence-weighted scheme: if the sum of confidence scores for the label "1" exceeds that of label "0", the final label is 1; otherwise, it is 0. This confidence-weighted approach has been shown to improve self-consistency and accelerate convergence to the stable answer of LLMs across a variety of QA tasks [58].Beyond improving reliability, confidence scores across multiple judgments provide a fine-grained signal that distinguishes borderline cases from clear-cut ones -an especially valuable property in our setting, where evaluating internal characteristics is inherently difficult [8] and confidence levels can help identify when the model is genuinely uncertain [58].We also tested majority voting, and the resulting labels are highly correlated with the confidence-weighted labels (correlation &gt; 0.98).LLMs may exhibit position bias, meaning that when serving as judges, they tend to favor the candidate appearing in a specific position (e.g., the first) [53].We adopt standard strategies to mitigate this bias.First, we construct paper pairs with substantial quality differences; prior research has shown that position bias is the most pronounced when the gap is small [36,53].Second, we randomize the positions of Paper 1 and Paper 2 across voting while maintaining logically consistent prompts and voting aggregation.Consistency of judgments across randomized positions achieves 77% and we have more discussions in Appendix M.</p>
<p>Iterative Search: The initial set of hypotheses may not account for all observed differences.For cases where none of the hypotheses yield a confident explanation (i.e., the confidence-weighted label is 0 for all hypotheses), we designate them as "unexplained".In each round, we sample 50 such unexplained pairs and prompt the LLM to generate five new hypotheses that are distinct from the existing ones.This iterative process continues until the proportion of unexplained cases drops below 5%.Note that each newly generated hypothesis is evaluated across all paper pairs, not just the subset of unexplained instances from which it was derived.Although new hypotheses are generated from a specific subset, they may still provide valid explanations for other pairs previously explained by different hypotheses.In our experiments, we find that 4 rounds of search, yielding 20 hypotheses, cover 97% of pairs under judge model agreements.This suggests that scientific judgments can often be explained by a compact set of generalizable hypotheses, reflecting recurring evaluative patterns across diverse cases 3 .</p>
<p>Prior and Posterior: From a Bayesian perspective, querying an LLM with a general question "What do you think makes one paper appear stronger than another?"without providing any specific data input, elicits the model's prior P (hypothesis).This prior reflects the LLM's internal distribution over explanatory hypotheses about what makes a scientific paper strong, independent of evidence.To extract this prior, we prompted the model to generate hypotheses in the absence of data.Specifically, we conducted 4 rounds per simulation, with 5 distinct hypotheses generated in each round.Repeating this simulation 250 times yielded a total of 5,000 prior hypotheses, following the same procedure of iteratively search-ing hypotheses.When the model is shown actual data, such as comparisons between paper pairs, it updates its beliefs, resulting in a posterior distribution P (hypothesis | data) over hypotheses, which captures how likely the model believes a given hypothesis explains the observed data and how confidently it makes the judgment.Operationally, we estimate this posterior by computing, for each hypothesis, the proportion of paper pairs it can successfully explain when the model is exposed to actual inputs.</p>
<p>We are particularly interested in belief change during this process, i.e., how LLM's prior about good science transforms into a robust posterior, after judging the actual pairwise dataset.For each generated hypothesis, when the LLM judges the pairwise dataset, we retain the proportion of cases it can explain (one pair can be explained by multiple hypotheses).We also match each hypothesis to priors (prompting: do you think they are talking about the same idea and same hypothesis?) to obtain their appearance frequency in the prior.We also conducted cross-validation experiments for the reliability of LLM labels of "match" using a heuristic dictionarybased method (locating some keywords of posterior hypotheses in priors to identify "match"), and these two methods exhibit a relatively high degree of correlation of 0.7, p=0.001 (see Appendix K for details).The frequency of each posterior hypothesis in the prior distribution is measured by counting how often a similar hypothesis appears within each window of 20 sequential prior hypotheses across 250 simulations.We use 20-hypothesis windows to align with the posterior generation setup, where LLMs are prompted to produce 4 rounds of 5 unique hypotheses.This ensures a fair comparison between the frequency of hypotheses in the prior and posterior distributions.</p>
<p>Overall experiments are formalized in Appendix F. Across experiments, we used OpenAI o3-mini for hypotheses (naively generate priors and iteratively generate posteriors in hypothesis search).We used Ope-nAI GPT-4o to match prior and posterior hypotheses, i.e., using a good reasoning model to "think" and a high-capacity aligned model to give a stable and reliable judgment.The overall framework of this process can be seen in Figure 2. All prompts used in the paper can be viewed in Appendix I. Generating highquality summaries for roughly 27K papers, pairing them under our criteria to create more than 14 million pairs, and having an advanced LLM to read, hypothesize, and evaluate each pair multiple rounds would lead to prohibitive computational and monetary costs.For demonstration purposes in our case study, we randomly selected 5,000 pairs for the main experiments.To strengthen the robustness of our conclusions, we further evaluated an additional 5,000 pairs through two robustness checks: (1) whether they produce similar hypothesis patterns, and (2) whether the generated set of hypotheses based on one dataset can be applied to out-of-sample data.Detailed discussions in Appendix D show that our conclusions are both generalizable and robust.Furthermore, in Appendix C we replicate our experiments across a range of popular closed-and open-source LLMs, all of which yield qualitatively similar patterns.</p>
<p>Results</p>
<p>We highlight the key results in the main paper.To comprehensively test generalizability and robustness, we designed 13 sections of experiments in the appendix.Across models, out-of-sample data, human annotation, and alternative representations of papers, we find that the major conclusions consistently hold.</p>
<p>Prior and posterior manifest distinct generation logics</p>
<p>We first examine how hypotheses are generated.Generation occurs over four rounds, each producing five hypotheses.As shown in Figure 3, the frequency with which hypotheses appear in the model's prior belief gradually decreases across rounds (represented by the mean of each round).This trend makes in-tuitive sense: the LLM initially generates hypotheses that are most "normative" for the standard of good science, such as theoretical rigor.These are the kinds of stereotyped considerations that naturally come to mind when researchers judge scientific work.As the rounds progress, we prompt the model to think beyond common ideas, encouraging the generation of less frequently seen hypotheses that can still explain peer review decisions in previously unexplained cases.Interestingly, these later hypotheses exhibit stronger explanatory power across the dataset.That is, although they are generated from cases that were initially hard to explain, they nevertheless prove applicable to cases already covered by earlier, more normative hypotheses.Moreover, the LLM shows greater confidence -quantified as the difference between the total confidence of votes matching the final weighted label and that of votes for the alternative label across three rounds -and improved consistency, measured by the proportion of pairs in which LLM casts the same vote across all rounds.Initially, the judging LLM draws on its priors but lacks confidence in assessing factors such as theoretical rigor.The model shows greater confidence in later rounds when evaluating the newly generated hypotheses, however, which are largely centered on presentation, contextualization, and storytelling (we will return to these hypotheses in the next section).</p>
<p>The full list of generated hypotheses is presented in Table 1 in the appendix.These hypotheses are also distinct, with the pairwise cosine similarity in posterior coverage (1-0 sequences) being 0.43 ± 0.02, indicating that they cover relatively different dimensions of the scientific quality signal.</p>
<p>Human judgments and LLMs share priors, but not posteriors</p>
<p>We observe notable shifts in the frequency of certain hypotheses from prior to posterior, as shown in Figure 4. Some hypotheses remain relatively stable in frequency and their explanatory power for good science (middle).In contrast, others manifest substantial change -some decrease markedly in frequency (left), while others increase significantly (right) from the prior distribution (see Table 1).This shift reflects a transition, after being exposed to the pairwise data, from emphasizing normative scientific values -such as theoretical or methodological rigor -to prioritizing more practical, communicative, and contextualization aspects, including storytelling quality and interdisciplinary relevance.When treated as a peer reviewer, the LLM initially aligns with conventional scientific ideals.In practice, however, it leans more heavily on accessible, narrative-driven criteria that appear more explanatory in distinguishing lower from higher rated papers (Figure 3 panels c and d).This suggests a divergence between explicit and implicit criteria of evaluation in science, a phenomenon widely appreciated although rarely measured in social studies of science [30].</p>
<p>We collected human review comments and annotated them using in-context learning prompts with GPT-4o, leveraging its strong performance in social computing annotation tasks, often surpassing human experts and crowd-workers [61,1,22].For each hypothesis, the annotation model was provided with three illustrative examples of reviewers notmentioning (label 0) specific aspects in a hypothesis, along with the reviewers' attitudes toward those aspects (positive: 1 or negative: -1, they are considered as "mentioning").For robustness, we conducted two experiments: 1) the dictionary method produces high consistency with LLM's "mention" label with correlation of 0.78, p = 0.000, detailed in Appendix K. 2) Three master's students specializing in social computing independently conducted the human annotations, achieving a high level of consistency with the LLM-generated labels (see Appendix B for details).We find human comments align with LLM's priors in terms of the frequency of aspects they address, meaning that they articulate a similar attention distribution as LLM's prior.In contrast, human reviewers rarely articulate their updated judgments like the LLM does.That is, if we assume the LLM mirrors human reviewing behavior, both begin from similar priors.However, while the LLM transparently updates its evaluation (posterior), human reviewers often refrain from articulating such shifts -likely due to normative expectations in academic reviewing -despite favoring unarticulated values with higher scores (we have more discussions and empirical evidence in Section 6).</p>
<p>Supporting this, the top five hypotheses in Table 1 with the largest prior-to-posterior gains (e.g., storytelling, orange texts) account for 61% of the LLM's posterior attention, but only 15% of its prior and 21% of mentions in human feedback.Conversely, the top five hypotheses with the largest prior-to-posterior losses (e.g., internal rigor, blue texts) account for 17% of the LLM's posteriors, but as much as 68% of its prior and 50% of human mentions.LLMs, as expected, initially even exaggerate stereotyped evaluation norms -overweighting "prior" and underweighting "posterior" standards -more sharply than humans do.Humans indeed explicitly reward internal qualities, but their stated reward patterns explain very little of the actual score distribution (R 2 = 0.06, details about the regression analysis can be seen in Appendix L), suggesting the presence of implicit rules guiding the evaluation process.Human reviewers tend to explicitly comment on aspects that moderately align with the LLM's normative priors (mention frequency and prior frequency correlation = 0.49) but are much less likely to articulate storytelling and contextualization posteriors in their written feedback (mention frequency and posterior coverage correlation = -0.14).All these suggest a possibility that while these storytelling aspects gain importance during evaluation -filling in the missing explanatory power R 2 of regressions -they remain underreported in human reviews, perhaps because they deviate from traditional scholarly norms, despite their relevance for establishing significance.</p>
<p>Generalizability</p>
<p>We briefly discuss examples to generalize our framework to other domains such as hiring, education, medicine, and criminal justice.</p>
<p>Resume Screening: In hiring, LLMs could be tasked with ranking pairs of resumes.Initially, their evaluations might focus on normative criteria such as educational background, work experience, and relevant skills.However, by pushing the models to elaborate beyond surface-level assessments, we can prompt them to elevate less normative but influential factors -such as perceived cultural fit -which account for differences in longer-term employee performance and retention.For example, an LLM might infer that a candidate who has previously worked at firms different in culture from the hiring company (e.g., Google vs. Amazon) is not a good fit, even if this is not explicitly stated in job descriptions [50].</p>
<p>College Admission: Similar to our main study, LLMs can be used to judge student statements/college admission essays while being encouraged to articulate implicit reasoning.Prior work suggests that essays written by students from certain ethnic or linguistic backgrounds often exhibit distinct writing styles, which can inadvertently be recognized and penalized by human graders [32,2,3].By analyzing patterns in LLMs' scoring rationales, we can investigate whether such signals contribute to systematic biases in college admission that undermine educational equity.</p>
<p>Discussions</p>
<p>It is important to emphasize that our goal is not to advocate for the automation of judgment tasks by LLMs.Rather, our primary goal is to present a position through a conceptual framework -namely, to exploit the biases and preference rationale embedded in LLMs by human discursive culture, then self-amplifying, reproducing, and explicitly identifying who might be harmed by these biases.Much of the existing research has focused on more explicit, easily identifiable forms of biases associated with social dimensions such as gender and race [47].Researchers have developed methods to bypass the barriers when the model refuses to disclose its biases, such as using associated words [7].However, they constitute the "prior" itself.These biases and heuristics still manifest superficially and can be uncovered through direct or indirect cues.In contrast, what we focus on in this paper is the "posterior" and how it diverges from the prior.Surprisingly, the results show that even when an LLM's prior (i.e., its stereotypes) is "good", its posterior can still be "bad" -let alone when its prior is already "bad".When acting as reviewers, LLMs appear to draw on two distinct systems to explain differences in quality signals.One is more normative and less confident in the evaluation process.The other is more narrative-driven, context-sensitive, and ultimately more confident and consistent.When judgment can be achieved through both systems, LLMs are more likely to apply the second system across the board as heuristics because they more consistently explain quality differentials.Humans, like LLMs, may also favor contextual criteria in evaluation, but they articulate this shift in terms of normative standards.</p>
<p>We make our position dialectical with alternative perspectives and policy suggestions:</p>
<p>To what extent does an LLM's shift from prior to posterior actually reflect human reasoning?Humans may never explicitly confess to using narrative or contextual heuristics in reviewing and other social dimensions, yet we have fairly strong evidence that they do.Prior survey-based studies show that reviewers in the social sciences place substantially greater emphasis on "interpretive challenges that question a study's framing" than on methodological rigor [59,56].We push LLMs to articulate such emphasis and generalize this insight to conference papers in technical fields, where -owing to normative expectations around methodological rigor and SOTA performance -reviewers are less inclined to make such concerns explicit 4 but use "lazy" words as a mask precisely in NLP [48].Moreover, if human score variance could be understood as comprising both explicit and implicit components, with LLMs' prior-to-posterior shift accounting for nearly all variance (as shown in Section 3 "iterative search") while human-stated explicit judgments map only to the prior, then the posterior can reasonably be taken to capture the implicit part.</p>
<p>Another view holds that even if we acknowledge the existence of unwritten codes, making them explicit serves little purpose and may even enable people to "game the system."But this view neglects the crucial balance between transparency and normativity.The fear that publicizing unwritten codes will be exploited implicitly assumes that it is preferable for such rules to continue operating in the shadows.In reality, the opposite is true: hidden rules primarily benefit those who already occupy advantaged positions or happen to be in the know.For example, 4 https://hackingsemantics.xyz/2020/reviewing-models/ researchers from elite institutions who are familiar with Western academic norms naturally understand how to write and frame a compelling "story" [13] -they have long benefited from tacit rules without needing them to be made explicit.The purpose of transparency is not to provide a cheat sheet, but to allow marginalized groups to understand the hidden barriers they face and thereby push institutions to reflect on and revise those standards.If people do game the system after unwritten codes are revealed, it is not a failure of transparency -it is evidence that those codes themselves need institutional correction.</p>
<p>Can LLMs replace the peer review and other judge processes?Our goal is not to provide a definitive answer -although certainly not yet.Nevertheless, our work suggests that insofar as LLMs function as cultural technologies that encode societal norms [11,16], then their "cognitive shift" in peer review reflects analogous unacknowledged shifts in human reviewers.That said, while LLM-generated reviews have been proven useful and cover broader and more specific aspects than human reviews [37,69,60] likely including those humans may be unwilling or unable to disclose -LLM reviewers still lack constructive feedback essential for critique when judging a standalone paper.Pairing papers for comparative judgment of feature existence and magnitude in some sense addresses this concern, as demonstrated here and in previous experiments [7].</p>
<p>What should we do at the implementation level?LLMs themselves cannot become wholly unbiased.They can, however, present multiple perspectives, label potential biases, and even collaborate in a multimodel "ecosystem," thereby approximating a more neutral environment open to public scrutiny.In addition to making the unwritten codes spoken out by LLMs more explicit in rule-making, it is thus equally important to introduce diverse dimensions of perspectives.Taking science as an example, the perceived rhetorical value often depends on the reviewer's proximity to the research [8].We could therefore adopt a complementary-neutral-proximal reviewer composition.Through such collective calibration mechanisms [54], humans can not only mitigate but also heighten their awareness of potential biases, reconcile multiple cognitive perspectives, and maintain balance in group composition, thereby ensuring that different aspects of society benefit in turn.</p>
<p>Limitations</p>
<p>In this paper, we focus on conference papers.Future research could examine more selective, longer, and detailed review comments from journals, although such data are often subject to legal constraints.Conducting an anonymous survey across both technology and social science fields would further strengthen the proposed framework.Our study employs wellnormalized and venue-controlled scores; future work could incorporate more nuanced statistical analyses on factors such as institutional prestige.</p>
<p>Ethics Statement</p>
<p>We used AI assistants to expedite the coding process.All code snippets produced by AI assistants were verified by the first author before they were incorporated.For writing, we only used AI assistants to check grammar.No external ethics review was required, as the experiments solely analyzed scientific papers, thereby posing no potential harm to individuals across any demographic or geographical group.Our study uses LLMs to surface implicit biases and unwritten evaluative codes in peer review.While this promotes transparency and fairness, it may also risk reinforcing stereotypes if misused.We mitigate this by anonymizing data, framing the method as diagnostic rather than prescriptive, and highlighting the importance of responsible use.All experiments were conducted using two NVIDIA 40-GB A100 GPUs.</p>
<p>B Human annotation experiments</p>
<p>Here, we complement the labeling methods (LLM labeling (major) and the dictionary method (complementary)) with human annotation.Three master's students specializing in social computing were involved to label the reviewer comments using exactly the same prompt as GPT-4o (if reviewers' attitudes toward the aspects in a given hypothesis were positive = 1 or negative = -1, they were considered as "mentioning", or 0 -not mentioned at all).The annotators are from Asia: two males and one female.They labeled independently, without using AI assistance and without communicating with one another.We find that one person shows high consistency with LLM labels, with an overlap of 0.76.Their majorityvoted label across annotators shows an overlap with LLM labels of 0.7.These results show the reliability of LLMs in annotating text data.</p>
<p>C Results using other LLMs are consistent</p>
<p>Across the paper, we intentionally adopted models from the same company (OpenAI) because we wanted the entire process to be shaped by a consistent set of cultural biases, making it more straightforward to expose and analyze these biases.</p>
<p>To increase the generalizability of our work, we explored other popular open-sourced or closed models including deepseek-chat-v3-0324 5 (the full set of experiments), LLaMA-3.3-70B-Instruct6 , Claude-Sonnet-4 7 , Claude 3 Haiku8 , Mistral Medium 3.1 9 , and Gemini 2.0 Flash 10 .Overall, we find our conclusions to be robust.Specifically:</p>
<p>Alignment with human comments: Deepseek also shows a relatively consistent set of priors as OpenAI models, and both align with human priors about the definition of "good science", compared to the low alignment for posteriors VS human comments.Human reviewers tend to explicitly reward aspects that moderately align with LLMs' normative priors: Ope-nAI correlation = 0.49; deepseek 0.28.By contrast, human reviewers generally avoid articulating posteriors in their review comments (OpenAI correlation = -0.14;Deepseek correlation = -0.16),but both models do articulate these posteriors.</p>
<p>Consistent priors: we also find that models show relatively consistent priors compared to each other and humans (not only DeepSeek).Correlation: Llama-human = 0.49; Claude4-human 0.47, Mistralhuman 0.41; Claude 3-human 0.41.</p>
<p>Shift from prior to posterior: During the transition from prior to posterior, we still observe a systematic shift from rigor-related aspects toward storytelling perspectives.All models demonstrate surprisingly high consistency in using certain heuristics such as "whether the paper tells a good interdisciplinary story", and across Llama, Mistral, Deepseek, Gemini, and Claude models, they show on average 89% agreement with Open-AI's judgment.This hypothesis also predictably gained the highest prior-toposterior attention gain: interdisciplinary integration (OpenAI top-1; DeepSeek top-1), contextualization (DeepSeek top-2; OpenAI top-2), and readability (OpenAI top-4; DeepSeek top-4) are all systematically overestimated in the posterior belief shift.By contrast, rigor is consistently underestimated (Ope-nAI lowest top-1; DeepSeek lowest top-1), as is attention to bias in evaluation (OpenAI lowest top-2; DeepSeek lowest top-2), along with reproduction and implementation details (OpenAI lowest top-5; DeepSeek lowest top-3).</p>
<p>D Out-of-sample judgments</p>
<p>For robustness, we experimented with another set of 5,000 pairs.We find the generated posteriors in the main paper can still explain a substantial proportion (&gt;95%) of pairs, even though the models had not seen them at all.Moreover, these posterior distributions are highly correlated (0.8) with those on the 5000 pairs in the original paper.We also experimented with using LLMs to generate hypotheses from the new dataset.We found that the generated hypotheses are qualitatively consistent, with only minor variations in order.We briefly noted this in the original paper (see Section 3 iterative search).We conclude that scientific judgments can often be explained by a compact set of generalizable hypotheses-spanning both easy heuristics and harder-to-discern, quality-driven priors-reflecting recurring evaluative patterns across diverse cases.</p>
<p>E Fidelity experiments</p>
<p>Summarization using methods from [71] demonstrates high fidelity compared to the raw text.In Figure 7, panel (a), we use the embedding model from [55], specifically pretrained on scientific papers' citation relations using contrastive learning.We embed both the summarizations and their corresponding raw texts in each section.To visualize and compare the distribution of these embedding areas, we reduce their dimensionality to two.We then estimate the boundary of each region using a 5% Gaussian kernel density estimate, drawing contours that enclose the densest 95% of data points in each embedding set.If the summarizations contain significant hallucinations, we would expect their embeddings to cover meaningfully different regions than those of the raw texts.However, in panel (a), we find that the distributions largely overlap, suggesting strong semantic alignment.In panel (b), we compute the cosine similarity between the two (raw, summarized) embeddings and find that most values are above 0.95.We conducted a complementary robustness check to assess whether LLMs make consistent judgments when directly reading full texts, compared to the extended summarization used in the paper for producing generalizable hypotheses under the limited context window.If using full texts from parsed PDFs, the results are essentially identical to those obtained with extended summaries (given their high fidelity), yielding a consistency score of 0.9 (i.e., the same judgment) across 1,000 judgments.</p>
<p>F Overall algorithmic framework</p>
<p>The overall algorithmic framework is shown in Algorithm 1.</p>
<p>G Related work</p>
<p>Hypothesis Generation: Recent studies have explored the capabilities of LLMs in generating novel, testable, and interpretable hypotheses across a range of domains.Zhou, Liu et al. [40,39,72] proposed a framework that prompts LLMs to generate hypotheses from labeled datasets, beginning with a few examples.These initial hypotheses are iteratively refined using a reward mechanism inspired by the upper confidence bound algorithm from multi-armed bandits, balancing exploration and exploitation.Other works have integrated external knowledge -such as scientific literature, knowledge graphs, and raw web corpus -into the hypothesis generation process [66,28,63,64,67], improving the accuracy and novelty of LLM-generated hypotheses while reducing hallucinations.This line of work demonstrates the potential of LLMs in advancing scientific discovery.Beyond computer science, other disciplines are also beginning to explore these possibilities.Ludwig and Mullainathan [42] highlighted the promise of LLMs as tools for hypothesis generation in economics, arguing that machine learning models can uncover patterns and relationships that traditional econometric methods may miss.Manning, Zhu, and Horton [43] presented a simulation-based approach for automatically generating and testing social science hypotheses in silico.In each simulated agentinteraction case, causal relationships are both proposed and tested by the LLM system, with results aligning well with those derived from economic theory, particularly in predicting the signs of estimated effects.In medicine, LLMs have been used to hypothesize synergistic drug combinations for breast cancer treatment, with several predictions later validated in laboratory experiments [10], showcasing the potential of LLMs to surface effective, previously overlooked therapeutic strategies.Implicit Bias in LLMs: Concerns about bias in AI systems have long focused on the ways these technologies can absorb and amplify harmful content from large-scale human-generated datasetsparticularly prejudices related to race, gender, and other social dimensions [47].Large language models (LLMs), trained on vast corpora of human text, are especially prone to perpetuating and even intensifying these biases [65,18].Much of the existing research on LLM bias has concentrated on explicit, easily identifiable forms of bias -such as derogatory language, disparate system performance, erasure, exclusionary norms, misrepresentation, stereotyping, and toxicity [18,15].Meanwhile, efforts in AI alignment, including outer alignment, inner alignment, and interpretability, have sought technological strategies for migrating such risks [52,44,29].Despite progress in addressing explicit bias, unwritten codes in LLMs' "cognitive" process remain understudied.Like human cognition, LLMs may exhibit subtle, unconscious forms of bias even when explicit prejudices are absent [52,38].These implicit biases are often harder to detect yet can have deeper and more pervasive effects on model outputs and downstream decision-making.Thus, systematically identifying these "unwritten codes" in LLMs is crucial to ensuring fairness, accuracy, and trustworthiness [38].One method worth mentioning in this topic is [7].The authors draw on experimental paradigms from the Implicit Association Test (IAT) in psychology to design prompts that reveal latent biases in LLMs.Rather than directly asking models about their views on particular social groups -which, like humans, they often avoid answering -they instead present structured tasks that require associations or comparative decisions.For instance, a model might be asked to assign positive or negative words to names with racial or gender connotations, or to choose which of two candidates is more suitable for a specific role.This prompt-based approach bypasses explicit selfreports and mirrors the logic of involuntary responses in psychological testing, allowing underlying implicit stereotypes to surface indirectly through model behavior.</p>
<p>Science, Innovation, and LLM-as-a-Judge: Prior work has increasingly applied LLMs to assist researchers by generating feedback, reviews, or reflections throughout the research process [70,25].Most recent studies have focused on addressing challenges posed by the rapid growth of academic publishing, such as rising review workloads and declining review quality [5,51].Liang et al. [37] evaluate GPT-4's capacity to generate scientific feedback and find substantial overlap with human reviews, especially in early manuscript stages.Thakkar et al. [60] developed a "Review Feedback Agent" combining multiple LLMs; in a randomized trial, 27% of reviewers revised their feedback after receiving LLM suggestions, which were more detailed and informative.Tyser et al. [62] propose a broader framework, training LLMs to align with human preferences through pairwise comparisons and enhancing review robustness with techniques like adaptive prompting and role-playing.While prior studies on LLMs in peer review have primarily focused on augmenting or replicating human judgment to address scalability and quality concerns, our argument departs from this substitutional framing.Instead of merely simulating reviewers to reduce workload or enhance coverage of aspects, we leverage LLMs' generative capacity to surface the implicit heuristics and tacit values that often shape peer review decisions but remain unspoken.By prompting LLMs to hypothesize why one paper is rated more highly than another, we reveal not only individual reviewer preferences but also broader cultural norms and evaluative codes that govern scientific judgment.In doing so, we treat AI not as a replacement for human thinking but as a diagnostic tool -one that can reflect, exaggerate, and ultimately make visible the "unwritten code" underpinning peer review and, more broadly, other sociotechnical systems.</p>
<p>H Details of the dataset of conference submissions</p>
<p>This dataset includes 26,731 submissions to 45  Raw abstract: Among a wide range of success of deep learning, convolutional neural networks have been extensively utilized in several tasks such as speech recognition, image processing, and natural language processing, which require inputs with large dimensions.Several studies have investigated function estimation capability of deep learning, but most of them have assumed that the dimensionality of the input is much smaller than the sample size.However, for typical data in applications such as those handled by the convolutional neural networks described above, the dimensionality of inputs is relatively high or even infinite.In this paper, we investigate the approximation and estimation errors of the (dilated) convolutional neural networks when the input is infinite dimensional.Although the approximation and estimation errors of neural networks are affected by the curse of dimensionality in the existing analyses for typical function spaces such as the Holder and Besov spaces, we show that, by considering anisotropic smoothness, they can alleviate exponential dependency on the dimensionality but they only depend on the smoothness of the target functions.Our theoretical analysis supports the great practical success of convolutional networks.Furthermore, we show that the dilated convolution is advantageous when the smoothness of the target function has a sparse structure.</p>
<p>Case Study 2:</p>
<p>Contextual background: Convolutional Neural Networks (CNNs) are widely used in applications like image recognition, speech processing, and natural language understanding.These applications often involve inputs that are extremely highdimensional-so high, in fact, that they can be treated as infinite-dimensional in theory.However, most theoretical research on neural networks has focused on low-or fixeddimensional settings, assuming the input has fewer dimensions than the number of samples available for training.Existing theoretical results on neural networks, such as those studying their function approximation capabilities, break down or become less meaningful in infinite-dimensional cases.Some prior studies attempted to address this by assuming inputs lie on a low-dimensional manifold or that the target function has certain regularity properties.Still, none fully addressed the behavior of CNNs when inputs are truly infinite-dimensional.This paper fills that gap.It explores whether CNNs can still learn effectively in such extreme settings by assuming the target function behaves differently across input dimensions, a property called anisotropic or mixed smoothness.</p>
<p>Case Study 2:</p>
<p>Key idea:</p>
<p>The main idea of the paper is that CNNs, especially those using dilated convolutions, can effectively learn even when the input is infinitedimensional, as long as the target function has a particular kind of smoothness across dimensions.Rather than relying on the number of input dimensions, the authors argue that how the target function changes across different input coordinates is what truly determines learnability.This is formalized using two smoothness conditions: Mixed smoothness, where the function may vary differently across each input direction but still follows a structured form.Anisotropic smoothness, where the function is especially smooth (or insensitive) in certain input directions and more complex in others.The paper introduces a formal function space to describe such functions and then proves that CNNs can achieve good learning performance in these spaces, without being affected by the sheer dimensionality of the input.The key is that the CNNs, particularly with dilated convolutions, are capable of automatically detecting and focusing on the most relevant input components.</p>
<p>Case Study 2:</p>
<p>Theoretical and methodological details: This section contains the core technical contributions of the paper.A. Learning Problem The paper studies a nonparametric regression problem, where the goal is to learn a function that maps an infinite-dimensional input to a real-valued output.The inputs can be thought of as sequences or functions, such as raw waveforms, high-resolution images, or long text embeddings.The challenge is to characterize how well a neural network can approximate this unknown function and how much error we can expect when training from a finite number of samples.B. Smoothness-Based Function Space To handle infinite-dimensional inputs, the authors define a new function space where functions are categorized based on their smoothness.Crucially, this smoothness is allowed to vary across input coordinates: Some directions may be very "rough" (i.e., important to capture precisely), while others can be ignored or treated coarsely.This idea mirrors how images contain both low-frequency (broad structure) and high-frequency (detail) components, and not all are equally important.By introducing a flexible way to quantify how much a function varies in each input direction, the authors build a mathematical foundation for analyzing the performance of neural networks in these spaces.C. Approximation Capabili-ties The paper first analyzes fully connected neural networks and shows that they can approximate functions in the proposed smoothness space well-but only if the network is explicitly given the most important input dimensions.In other words, these networks are theoretically powerful but need help in selecting relevant inputs.In contrast, CNNs can discover the important directions automatically thanks to their structured and hierarchical nature.The authors show that: Ordinary CNNs perform well when the smoothness of the target function increases gradually across input directions (e.g., the first few coordinates are rough, the rest smooth).Dilated CNNs are even more powerful when the target function has a sparse structure-that is, only a few input directions matter significantly, and these may be far apart.The authors prove that in both settings, CNNs can achieve fast convergence rates-meaning the prediction error decreases quickly as more data is observed-even if the input is infinitedimensional.These rates depend only on the smoothness of the function, not on the number of input dimensions.D. Estimation from Data The paper also analyzes how well neural networks trained on data (using empirical risk minimization) can learn the true function.They show that: CNNs achieve dimension-independent learning rates under the right smoothness assumptions.For functions with gradually increasing smoothness, shallow CNNs suffice.For sparsely smooth functions, deeper or dilated CNNs are needed to identify and capture distant important features.This theoretical result aligns well with practical intuitions about CNNs being good at capturing both local and global structures.</p>
<p>Case Study 2:</p>
<p>Experiments and results: The paper does not include any empirical experiments-no training on real datasets, no benchmarks, no runtime results.Instead, it provides a theoretically rigorous analytical framework to evaluate model performance, focusing on: Approximation error: How well the neural network class can represent the true function.Estimation error: How close the trained model gets to the true function when trained on finite data.The authors derive explicit mathematical expressions for the convergence rates of neural networks.These rates are shown to be: Polynomial in the number of samples, not exponential in the input dimension.Optimal or near-optimal, matching or extending existing bounds in lower-dimensional settings.They also compare their rates with those from other methods like kernel regression or traditional non-deep learning techniques, demonstrating that CNNs offer better or comparable rates in infinite-dimensional settings-especially when function smoothness is sparse or nonuniform.This analytical evaluation shows that CNNs, especially those using dilated convolutions, are theoretically justified for highand infinite-dimensional data, even in the absence of experimental validation.</p>
<p>Case Study 2:</p>
<p>Mentioned impact: This paper makes a theoretical contribution by offering comprehensive analyses of deep learning's learnability in infinite-dimensional spaces.Its results: Support the practical success of CNNs in real-world tasks involving high-dimensional data.Justify the use of dilated convolutions as an effective tool for handling sparse or long-range dependencies.Advance our understanding of when and why deep learning avoids the curse of dimensionality, shifting the focus from input size to function structure.{"scores": [X1, X2, X3, ..., X20]} Each Xi must be one of: -1, 0, or 1 -Use 1 if the feedback praises the standard -Use -1 if the feedback criticizes the standard -Use 0 if the standard is not mentioned</p>
<p>I Prompts used in the paper</p>
<p>Hypotheses: {hypothesis_list}</p>
<p>Feedback: {feedback_text}</p>
<p>J The list of generated hypotheses</p>
<p>The list of generated hypotheses is shown in Table 1.</p>
<p>K Cross-validation dictionary</p>
<p>The dictionary method used in the paper to complement LLM labeling/matching is shown in Table 2.Note that this heuristic dictionary-like approach, while widely used in social science [20,17], ultimately serves as a substitute rather than the major methods of labeling human comments of "mention" or matching priors and posteriors, presented in the paper.</p>
<p>L Humans explicitly reward the internal qualities of science, but a significant hidden driver remains.</p>
<p>We use GPT-4o with few-shot to annotate human review comments across the aspects of 20 hypotheses.Each aspect is labeled as praise (1), not mentioned (0), or criticism (-1).We then run an OLS regression (using scipy.statspackage) to examine how reviewers' attitudes toward these aspects relate to the scores they assign within the conference.The results are shown in Table 3.</p>
<p>We have two points to discuss:</p>
<p>• Only five hypotheses are statistically significant meaningful predictors of human review scores, and all have positive coefficients-indicating that when these aspects mentioned in the hypothesis are positive, the associated paper tends to receive a higher score.These hypotheses (highlighted in red) clearly reflect the internal qualities of scientific work.This suggests that reviewers do use their scores to explicitly reward normative aspects of science.Notably, two of these five hypotheses-theoretical rigor and the balance between theoretical analysis and practical impact-also appear among the top five hypotheses favored by priors but not favored by posteriors.</p>
<p>• However, the regression model's explanatory power is very low (R 2 = 0.06).This suggests that much of what drives reviewers' scoring decisions is not captured by the explicit reward of these normative qualities.There are implicit factors strongly influencing the review process that are not reflected directly in the scores.</p>
<p>M Discussions of position bias in LLM-as-a-judge</p>
<p>To address potential position bias, we adopt two mitigation strategies in the original paper.First, we construct paper pairs with substantial quality differences, as prior research has shown that position bias is most pronounced when the quality gap is small [36,53].Second, we randomize the positions of Paper 1 and Paper 2 across voting, while ensuring logically consistent prompts and vote aggregation.We then assess whether position bias is indeed stronger when the quality signal gap is small.Specifically, we regress the consistency rate -defined as the proportion of consistent judgments across 20 hypotheses after swapping paper positions -on the review score gap between the two papers.Our analysis (Table 4) shows that a larger review score gap is indeed associated with greater consistency, and the effect is statistically significant, though weak in magnitude.1: The generated hypotheses.A shift in rank indicates the change in "importance", measured as posterior minus prior (i.e., a high shift means the hypotheses that were not highly weighted in the prior but were used much more frequently in actual judgments).Orange highlights the top 5 hypotheses with the largest gains (e.g., contextual and narrative perspectives), while blue highlights the top 5 with the largest losses (e.g., theoretical rigor and normative perspectives)."Mention" indicates the proportion of human comments that mentioned the hypothesis, as identified by GPT-4o.All hypotheses start with "Compared to the other, one paper..."</p>
<p>Hypothesis</p>
<p>Figure 1 :
1
Figure 1: We argue and present a conceptual framework to self-reinforce biases to amplify unwritten codes until the model articulates and replicates them explicitly.</p>
<p>Figure 2 :
2
Figure 2: Updating priors to posteriors by hypothesis search.</p>
<p>Figure 3 :
3
Figure 3: Prior and posterior manifest distinct generation logics.</p>
<p>Figure 4 :
4
Figure 4: Prior versus Posterior.The index of each hypothesis represents its rank in the change of the prior frequency minus the posterior frequency (from high to low).</p>
<p>Figure 7 :
7
Figure 7: The summarization exhibits minimal hallucination (panel a -almost overlapping embeddings) and a high degree of fidelity (panel b).</p>
<p>Table 3 :
3
Regression results on the mentioned perspectives and human scoring.
HypothesisCoef. Std. Err.t P&gt;|t| [0.025, 0.975]const5.9660.100 59.508 0.000 [5.769, 6.163]lack justification regarding its novelty 0.2310.0962.410 0.016 [0.043, 0.420]not theoretically rigorous0.2090.0952.194 0.028 [0.022, 0.396]the design is over-engineered, not ele-0.0460.1000.463 0.643 [-0.150, 0.242]gant, and unnecessarily complicatedlack comprehensive ablation and hyper-0.1520.0871.754 0.080 [-0.018, 0.322]parameter analysisunclear about its contextualization0.0690.0820.836 0.403 [-0.093, 0.230]within the related literaturethe contribution is incremental0.2050.0892.310 0.021 [0.031, 0.379]not use a fair benchmark for evalua-0.1690.0851.998 0.046 [0.003, 0.336]tionthe presentation is too complicated-0.0060.081 -0.078 0.938 [-0.166, 0.153]lack implementation and reproducibility0.0190.0980.198 0.843 [-0.172, 0.211]detailsnot balance theoretical justification0.3560.1362.619 0.009 [0.089, 0.622]and practical impactmisaligned with the emerging scope0.1930.2230.865 0.388 [-0.245, 0.630]insufficiently discuss social and ethical-0.0510.262 -0.196 0.844 [-0.565, 0.462]aspects of its algorithmsnot interact well with adjacent domains,-0.0810.249 -0.325 0.745 [-0.569, 0.408]such as statistics and cognitive sciencelack empirical evidence in terms of0.0150.1020.149 0.882 [-0.185, 0.215]model and architecture design choicelimited in scalability to real-world sce-0.0340.1100.308 0.758 [-0.183, 0.251]nariosinsufficient in testing robustness such as0.0260.2040.127 0.899 [-0.374, 0.426]uncertainty and bias in evaluationweakly motivated in its training and op-0.0730.1690.430 0.668 [-0.259, 0.404]timization techniquesthe theoretical assumptions are unreal-0.0410.1750.232 0.817 [-0.302, 0.383]isticpoorly explore edge cases and failure-0.0300.080 -0.374 0.708 [-0.186, 0.127]modesrequire dense prior background knowl-0.0060.0990.056 0.955 [-0.189, 0.200]edge</p>
<p>Table 4 :
4
OLS regression results on the position bias.Standard errors assume that the covariance matrix of the errors is correctly specified.</p>
<p>We have more discussions in Section 6 and Appendix G.
https://papercopilot.com/
In Section 6 and the results below, we further decompose these generalized hypotheses into priors (which humans tend to explicitly emphasize) and posteriors (humans' implicit judgments).
https://api-docs.deepseek.com/news/news250325
https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct
7 https://www.anthropic.com/news/claude-4
8 https://www.anthropic.com/news/claude-3-
haiku 9 https://openrouter.ai/mistralai/mistral-
medium-3.1 10 https://cloud.google.com/vertex-ai/generativeai/docs/models/gemini/2-0-flash
AppendixA Reviewer score is a good measure of quality signalReview scores tend to be much more consistent within a single paper than across different papers (see Figure5
Open-source large language models outperform crowd workers and approach chatgpt in text-annotation tasks. Meysam Alizadeh, Maël Kubli, Zeynab Samei, Shirin Dehghani, Juan Diego Bermeo, Maria Korobeynikova, Fabrizio Gilardi, arXiv:2307.021792023101arXiv preprint</p>
<p>Essay content and style are strongly related to household income and sat scores: Evidence from 60,000 undergraduate applications. Sonia Aj Alvero, Ben Giebel, Anthony Lising Gebre-Medhin, Mitchell L Antonio, Benjamin W Stevens, Domingue, Science Advances. 74290312021</p>
<p>Multilingualism and mismatching: Spanish language usage in college admissions essays. Rebecca Aj Alvero, Pattichis, Poetics. 1051019032024</p>
<p>Openasp: A benchmark for multi-document open aspect-based summarization. Shmuel Amar, Liat Schiff, Ori Ernst, Asi Shefer, Ori Shapira, Ido Dagan, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>Publication trends in artificial intelligence conferences: The rise of super prolific authors. Ariful Azad, Afeefa Banu, arXiv:2412.077932024arXiv preprint</p>
<p>Quantifying gender bias in different corpora. Marzieh Babaeianjelodar, Stephen Lorenz, Josh Gordon, Jeanna Matthews, Evan Freitag, Companion Proceedings of the Web Conference 2020. 2020</p>
<p>Explicitly unbiased large language models still form biased associations. Xuechunzi Bai, Angelina Wang, Ilia Sucholutsky, Thomas L Griffiths, Proceedings of the National Academy of Sciences. 1228e24162281222025</p>
<p>A simulationbased analysis of the impact of rhetorical citations in science. Honglin Bao, Misha Teplitskiy, Nature Communications. 1514312024</p>
<p>Words that work: Using language to generate hypotheses. M Rafael, James Batista, Ross, 2024Available at SSRN 4926398</p>
<p>Agentichypothesis: A survey on hypothesis generation using llm systems. Adib Bazgir, Yuwen Zhang, ICLR 2025 AgenticAI Workshop -Towards Agentic AI for Science: Hypothesis Generation, Comprehension, Quantification, and Validation. 2025</p>
<p>Language (technology) is power: A critical survey of "bias. Lin Su, Solon Blodgett, Hal Barocas, Iii Daumé, Hanna Wallach, arXiv:2005.140502020arXiv preprintin nlp</p>
<p>Guiming Hardy Chen, Shunian Chen, Ziche Liu, Feng Jiang, Benyou Wang, arXiv:2402.10669Humans or llms as the judge? a study on judgement biases. 2024arXiv preprint</p>
<p>Geographical disparities in navigating rejection in science drive disparities in its file drawer. Hong Chen, Christopher I Rider, David Jurgens, Misha Teplitskiy, SSRN 48720232024</p>
<p>The perceptual primacy of feeling: Affectless visual machines explain a majority of variance in human visually evoked affect. Colin Conwell, Daniel Graham, Chelsea Boccagno, Edward A Vessel, Proceedings of the National Academy of Sciences. 1224e23060251212025</p>
<p>Quantifying social biases in nlp: A generalization and empirical comparison of extrinsic fairness metrics. Paula Czarnowska, Yogarshi Vyas, Kashif Shah, Transactions of the Association for Computational Linguistics. 92021</p>
<p>Large ai models are cultural and social technologies. Henry Farrell, Alison Gopnik, Cosma Shalizi, James Evans, Science. 38767392025</p>
<p>Disclosure sentiment: Machine learning vs. dictionary methods. Richard Frankel, Jared Jennings, Joshua Lee, Management Science. 6872022</p>
<p>Bias and fairness in large language models: A survey. Isabel O Gallegos, Ryan A Rossi, Joe Barrow, Md Mehrab Tanjim, Sungchul Kim, Franck Dernoncourt, Tong Yu, Ruiyi Zhang, Nesreen K Ahmed, Computational Linguistics. 5032024</p>
<p>Online images amplify gender bias. Douglas Guilbeault, Solène Delecourt, Tasker Hull, Bhargav Srinivasa Desikan, Mark Chu, Ethan Nadler, Nature. 62680012024</p>
<p>Big social data analytics in journalism and mass communication: Comparing dictionary-based text analysis and unsupervised topic modeling. Lei Guo, Chris J Vargo, Zixuan Pan, Weicong Ding, Prakash Ishwar, Journalism &amp; Mass Communication Quarterly. 9322016</p>
<p>Yufei Guo, Muzhe Guo, Juntao Su, Zhou Yang, Mengqiu Zhu, Hongfei Li, Mengyang Qiu, Shuo Shuo Liu, arXiv:2411.10915Bias in large language models: Origin, evaluation, and mitigation. 2024arXiv preprint</p>
<p>Muhammad Uzair, Ul Haq, Davide Rigoni, Alessandro Sperduti, arXiv:2504.15022Llms as data annotators: How close are we to human performance. 2025arXiv preprint</p>
<p>Ai generates covertly racist decisions about people based on their dialect. Valentin Hofmann, Ria Pratyusha, Dan Kalluri, Sharese Jurafsky, King, Nature. 63380282024</p>
<p>Generative language models exhibit social identity biases. Tiancheng Hu, Yara Kyrychenko, Steve Rathje, Nigel Collier, Jon Sander Van Der Linden, Roozenbeek, Nature Computational Science. 512025</p>
<p>The use of chatgpt and other large language models in surgical science. Geert Boris V Janssen, Marc G Kazemier, Besselink, BJS Open. 72322023</p>
<p>Ai-augmented surveys: Leveraging large language models and surveys for opinion prediction. Junsol Kim, Byungkyu Lee, arXiv:2305.096202023arXiv preprint</p>
<p>Gender bias and stereotypes in large language models. Hadas Kotek, Rikker Dockum, David Sun, Proceedings of the ACM Collective Intelligence Conference. the ACM Collective Intelligence Conference2023</p>
<p>Adithya Kulkarni, Fatimah Alotaibi, Xinyue Zeng, Longfeng Wu, Tong Zeng, Barry Menglong Yao, Minqian Liu, Shuaicheng Zhang, Lifu Huang, Dawei Zhou, arXiv:2505.04651Scientific hypothesis generation and validation: Methods, datasets, and future directions. 2025arXiv preprint</p>
<p>Language generation models can cause harm: So what can we do about it? an actionable survey. Sachin Kumar, Vidhisha Balachandran, Lucille Njoo, arXiv:2210.077002022arXiv preprintAntonios Anastasopoulos, and Yulia Tsvetkov</p>
<p>Science in action: How to follow scientists and engineers through society. Bruno Latour, 1987Harvard University Press</p>
<p>Bias in peer review. Carole J Lee, Cassidy R Sugimoto, Guo Zhang, Blaise Cronin, Journal of the American Society for Information Science and Technology. 6412013</p>
<p>Poor alignment and steerability of large language models: Evidence from college admission essays. Jinsook Lee, Thorsten Alvero, Rene Joachims, Kizilcec, arXiv:2503.200622025arXiv preprint</p>
<p>Large language models portray socially subordinate groups as more homogeneous, consistent with a bias observed in humans. Jacob M Messi Hj Lee, Calvin K Montgomery, Lai, Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency. the 2024 ACM Conference on Fairness, Accountability, and Transparency2024</p>
<p>Can large language models estimate public opinion about global warming? an empirical assessment of algorithmic fidelity and bias. Sanguk Lee, Tai-Quan Peng, Matthew H Goldberg, Seth A Rosenthal, John E Kotcher, Edward W Maibach, Anthony Leiserowitz, PLOS Climate. 38e00004292024</p>
<p>Culturellm: Incorporating cultural differences into large language models. Cheng Li, Mengzhuo Chen, Jindong Wang, Sunayana Sitaram, Xing Xie, Advances in Neural Information Processing Systems. 202437</p>
<p>Split and merge: Aligning position biases in llm-based evaluators. Zongjie Li, Chaozheng Wang, Pingchuan Ma, Daoyuan Wu, Shuai Wang, Cuiyun Gao, Yang Liu, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language Processing2024</p>
<p>Can large language models provide useful feedback on research papers? a large-scale empirical analysis. Weixin Liang, Yuhui Zhang, Hancheng Cao, Binglu Wang, Daisy Yi Ding, Xinyu Yang, Kailas Vodrahalli, NEJM AI. 1824001962024Siyu He, Daniel Scott Smith, Yian Yin, and 1 others</p>
<p>Xinru Lin, Luyang Li, arXiv:2503.02776Implicit bias in llms: A survey. 2025arXiv preprint</p>
<p>Haokun Liu, Sicong Huang, Jingyu Hu, Yangqiaoyu Zhou, Chenhao Tan, arXiv:2504.11524Hypobench: Towards systematic and principled benchmarking for hypothesis generation. 2025arXiv preprint</p>
<p>Literature meets data: A synergistic approach to hypothesis generation. Haokun Liu, Yangqiaoyu Zhou, Mingxuan Li, Chenfei Yuan, Chenhao Tan, arXiv:2410.173092024arXiv preprint</p>
<p>Lost in the middle: How language models use long contexts. Kevin Nelson F Liu, John Lin, Ashwin Hewitt, Michele Paranjape, Fabio Bevilacqua, Percy Petroni, Liang, Transactions of the Association for Computational Linguistics. 122024</p>
<p>Machine learning as a tool for hypothesis generation. Jens Ludwig, Sendhil Mullainathan, The Quarterly Journal of Economics. 13922024</p>
<p>Automated social science: Language models as scientist and subjects. Benjamin S Manning, Kehang Zhu, John J Horton, 2024. w32381National Bureau of Economic Research Working Paper</p>
<p>An empirical survey of the effectiveness of debiasing techniques for pre-trained language models. Nicholas Meade, Elinor Poole-Dayan, Siva Reddy, arXiv:2110.085272021arXiv preprint</p>
<p>Sparse autoencoders for hypothesis generation. Rajiv Movva, Kenny Peng, Nikhil Garg, Jon Kleinberg, Emma Pierson, 2025</p>
<p>Tarek Naous, Alan Michael J Ryan, Wei Ritter, Xu, arXiv:2305.14456Having beer after prayer? measuring cultural bias in large language models. 2023arXiv preprint</p>
<p>Bias in data-driven artificial intelligence systems-an introductory survey. Eirini Ntoutsi, Pavlos Fafalios, Ujwal Gadiraju, Vasileios Iosifidis, Wolfgang Nejdl, Maria-Esther Vidal, Salvatore Ruggieri, Franco Turini, Data Mining and Knowledge Discovery. 103e13562020Wiley Interdisciplinary ReviewsSymeon Papadopoulos, Emmanouil Krasanakis, and 1 others</p>
<p>Lazyreview: A dataset for uncovering lazy thinking in nlp peer reviews. Sukannya Purkayastha, Zhuang Li, Anne Lauscher, Lizhen Qu, Iryna Gurevych, Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 63rd Annual Meeting of the Association for Computational Linguistics20251</p>
<p>Scideator: Human-llm scientific idea generation grounded in research-paper facet recombination. Marissa Radensky, Simra Shahid, Raymond Fok, Pao Siangliulue, Tom Hope, Daniel S Weld, arXiv:2409.146342024arXiv preprint</p>
<p>Hiring as cultural matching: The case of elite professional service firms. Lauren A Rivera, American Sociological Review. 7762012</p>
<p>Program chairs' report on peer review at acl 2023. Anna Rogers, Marzena Karpinska, Jordan Boyd-Graber, Naoaki Okazaki, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational Linguistics20231</p>
<p>Tianhao Shen, Renren Jin, Yufei Huang, Chuang Liu, Weilong Dong, Zishan Guo, Xinwei Wu, Yan Liu, Deyi Xiong, arXiv:2309.15025Large language model alignment: A survey. 2023arXiv preprint</p>
<p>Judging the judges: A systematic investigation of position bias in pairwise comparative assessments by llms. Lin Shi, Chiyu Ma, Wenhua Liang, Weicheng Ma, Soroush Vosoughi, arXiv:2406.077912024arXiv preprint</p>
<p>Wise teamwork: Collective confidence calibration predicts the effectiveness of group discussion. Ike Silver, Barbara A Mellers, Philip E Tetlock, Journal of Experimental Social Psychology. 961041572021</p>
<p>Scirepeval: A multi-format benchmark for scientific document representations. Amanpreet Singh, Mike D' Arcy, Arman Cohan, Doug Downey, Sergey Feldman, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>Revising as reframing: Original submissions versus published papers in administrative science quarterly. David Strang, Kyle Siler, Sociological Theory. 3312015. 2005 to 2009</p>
<p>Differences in psychologists' cognitive traits are associated with scientific divides. Justin Sulik, Nakwon Rim, Elizabeth Pontikes, James Evans, Gary Lupyan, Nature Human Behaviour. 2025</p>
<p>Amir Taubenfeld, Tom Sheffer, Eran Ofek, Amir Feder, Ariel Goldstein, Zorik Gekhman, Gal Yona, arXiv:2502.06233Confidence improves self-consistency in llms. 2025arXiv preprint</p>
<p>Frame search and re-search: How quantitative sociological articles change during peer review. Misha Teplitskiy, The American Sociologist. 4722016</p>
<p>Can llm feedback enhance review quality? a randomized study of 20k reviews at iclr. Nitya Thakkar, Mert Yuksekgonul, Jake Silberg, Animesh Garg, Nanyun Peng, Fei Sha, Rose Yu, Carl Vondrick, James Zou, arXiv:2504.097372025. 2025arXiv preprint</p>
<p>Chatgpt-4 outperforms experts and crowd workers in annotating political twitter messages with zero-shot learning. Petter Törnberg, arXiv:2304.065882023arXiv preprint</p>
<p>Keith Tyser, Ben Segev, Gaston Longhitano, Xin-Yu Zhang, Zachary Meeks, Jason Lee, Uday Garg, Nicholas Belsten, Avi Shporer, arXiv:2408.10365Madeleine Udell, and 1 others. 2024. Ai-driven review systems: Evaluating llms in scalable and bias-aware academic reviews. arXiv preprint</p>
<p>Learning to generate novel scientific directions with contextualized literature-based discovery. Qingyun Wang, Doug Downey, Heng Ji, Tom Hope, arXiv:2305.142592023arXiv preprint</p>
<p>Scimon: Scientific inspiration machines optimized for novelty. Qingyun Wang, Doug Downey, Heng Ji, Tom Hope, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational Linguistics20241</p>
<p>Laura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin, Jonathan Uesato, Po-Sen Huang, Myra Cheng, Mia Glaese, arXiv:2112.04359Atoosa Kasirzadeh, and 1 others. 2021. Ethical and social risks of harm from language models. Borja BallearXiv preprint</p>
<p>Improving scientific hypothesis generation with knowledge grounded large language models. Guangzhi Xiong, Eric Xie, Amir Hassan Shariatmadari, Sikun Guo, Stefan Bekiranov, Aidong Zhang, arXiv:2411.023822024arXiv preprint</p>
<p>Large language models for automated open-domain scientific hypotheses discovery. Zonglin Yang, Xinya Du, Junxian Li, Jie Zheng, Findings of the Association for Computational Linguistics: ACL. 2024Soujanya Poria, and Erik Cambria</p>
<p>Helmet: How to evaluate longcontext language models effectively and thoroughly. Howard Yen, Tianyu Gao, Minmin Hou, Ke Ding, Daniel Fleischer, Peter Izsak, Moshe Wasserblat, Danqi Chen, arXiv:2410.026942024arXiv preprint</p>
<p>Can we automate scientific reviewing. Weizhe Yuan, Pengfei Liu, Graham Neubig, Journal of Artificial Intelligence Research. 752022</p>
<p>Mert Yuksekgonul, Federico Bianchi, Joseph Boen, Sheng Liu, Zhi Huang, Carlos Guestrin, James Zou, arXiv:2406.07496Textgrad: Automatic "differentiation" via text. 2024arXiv preprint</p>
<p>Massw: A new dataset and benchmark tasks for aiassisted scientific workflows. Xingjian Zhang, Yutong Xie, Jin Huang, Jinge Ma, Zhaoying Pan, Qijia Liu, Ziyang Xiong, Tolga Ergen, Dongsub Shim, Honglak Lee, and 1 others. NAACL2025</p>
<p>Yangqiaoyu Zhou, Haokun Liu, Tejes Srivastava, Hongyuan Mei, Chenhao Tan, arXiv:2404.04326Hypothesis generation with large language models. 2024arXiv preprint</p>
<p>Can large language models transform computational social science? Computational Linguistics. Caleb Ziems, William Held, Omar Shaikh, Jiaao Chen, Zhehao Zhang, Diyi Yang, Hypothesis Index Dictionary 1. 202450novelty", "novel"</p>
<p>Table 2: The dictionary method used in the paper to complement LLM labeling/matching. </p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1984 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1984</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1984</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-45.html">extraction-schema-45</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of adaptive, learned, or parameterized operators (mutation, crossover, variation operators) in evolutionary algorithms, genetic algorithms, or genetic programming, including performance comparisons, operator representations, and results on code or text domains.</div>
                <p><strong>Paper ID:</strong> paper-277272630</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2503.18061v1.pdf" target="_blank">Reinforcement Learning-based Self-adaptive Differential Evolution through Automated Landscape Feature Learning</a></p>
                <p><strong>Paper Abstract:</strong> Recently, Meta-Black-Box-Optimization (MetaBBO) methods significantly enhance the performance of traditional black-box optimizers through meta-learning flexible and generalizable meta-level policies that excel in dynamic algorithm configuration (DAC) tasks within the low-level optimization, reducing the expertise required to adapt optimizers for novel optimization tasks. Though promising, existing MetaBBO methods heavily rely on human-crafted feature extraction approach to secure learning effectiveness. To address this issue, this paper introduces a novel MetaBBO method that supports automated feature learning during the meta-learning process, termed as RLDE-AFL, which integrates a learnable feature extraction module into a reinforcement learning-based DE method to learn both the feature encoding and meta-level policy. Specifically, we design an attention-based neural network with mantissa-exponent based embedding to transform the solution populations and corresponding objective values during the low-level optimization into expressive landscape features. We further incorporate a comprehensive algorithm configuration space including diverse DE operators into a reinforcement learning-aided DAC paradigm to unleash the behavior diversity and performance of the proposed RLDE-AFL. Extensive benchmark results show that co-training the proposed feature learning module and DAC policy contributes to the superior optimization performance of RLDE-AFL to several advanced DE methods and recent MetaBBO baselines over both synthetic and realistic BBO scenarios. The source codes of RLDE-AFL are available at https://github.com/GMC-DRL/RLDE-AFL.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1984.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1984.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of adaptive, learned, or parameterized operators (mutation, crossover, variation operators) in evolutionary algorithms, genetic algorithms, or genetic programming, including performance comparisons, operator representations, and results on code or text domains.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RLDE-AFL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reinforcement Learning-based Self-adaptive Differential Evolution through Automated Landscape Feature Learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A meta-Black-Box-Optimization method that jointly learns a meta-level policy (via PPO) and an attention-based feature extractor (NeurELA) to select DE mutation/crossover operators and their parameters per-individual and per-generation, using an integrated pool of diverse DE operators.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>operator_name</strong></td>
                            <td>Learned operator selection and parameter control (per-individual)</td>
                        </tr>
                        <tr>
                            <td><strong>operator_type</strong></td>
                            <td>mutation + crossover (variation)</td>
                        </tr>
                        <tr>
                            <td><strong>operator_description</strong></td>
                            <td>At each generation and for each individual the agent (actor network) outputs: (1) a categorical distribution over 14 mutation operators (softmax) and 3 crossover operators (softmax), and (2) Gaussian distributions for up to 3 mutation parameters and up to 2 crossover parameters; an operator index is sampled and the corresponding parameters (first k of the sampled vector) are used to configure the DE operator for producing offspring.</td>
                        </tr>
                        <tr>
                            <td><strong>is_learned_or_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>learning_mechanism</strong></td>
                            <td>reinforcement learning (Proximal Policy Optimization, PPO) co-trained with a learned feature extractor</td>
                        </tr>
                        <tr>
                            <td><strong>operator_representation</strong></td>
                            <td>Categorical probability vectors (softmax) for operator choice; parameter distributions represented as Gaussian N(mu,sigma) outputs from MLPs</td>
                        </tr>
                        <tr>
                            <td><strong>domain_type</strong></td>
                            <td>numerical optimization (continuous black-box functions)</td>
                        </tr>
                        <tr>
                            <td><strong>context_dependent</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>modality_specific</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>compositional</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>normalized accumulated reward (training objective), convergence speed, best objective / AEI for realistic problems; aggregated accumulated reward and AEI reported for benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_learned_operator</strong></td>
                            <td>Aggregated accumulated rewards reported in ablation/summary: 10D testing problems: 9.645e-1 (≈0.9645); 20D: 9.334e-1 (≈0.9334); expensive problems: 8.856e-1 (≈0.8856); realistic (protein-docking): 7.056e-1 (≈0.7056) — values taken from the paper's averaged metrics tables</td>
                        </tr>
                        <tr>
                            <td><strong>performance_fixed_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_improvement</strong></td>
                            <td>Qualitative and aggregated improvements reported: RLDE-AFL outperforms advanced traditional DE variants (MadDE, JDE21, NL-SHADE-LBC, AMCDE) and multiple RL-based baselines on MetaBox synthetic benchmarks and shows superior zero-shot convergence speed on expensive scenarios; exact per-baseline percentage improvements are not aggregated in a single table in the paper</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>executability_preservation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_diversity_metric</strong></td>
                            <td>Not reported as explicit novelty/diversity numeric metric; diversity implicitly measured via optimization performance and operator-behavior diversity from the integrated pool</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_results</strong></td>
                            <td>Zero-shot generalization from models trained on 10D synthetic problems to 20D synthetic problems, expensive problems, and protein-docking realistic problems: RLDE-AFL retains strong performance and faster convergence in expensive scenarios and is competitive on protein-docking (AEI metric) versus baselines (competitive with GLEET, surpasses DE-DDQN, MadDE and NL-SHADE-LBC).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not quantified precisely; training details: PPO training for 100 epochs, learning rate 1e-3, population size 100, optimization horizon 200 (20,000 FEs), actor/critic updated with PPO steps; experiments run on Intel Xeon + NVIDIA 1080Ti. Authors note training cost but give no single-number overhead relative to fixed operators.</td>
                        </tr>
                        <tr>
                            <td><strong>population_size</strong></td>
                            <td>100</td>
                        </tr>
                        <tr>
                            <td><strong>cold_start_addressed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>operator_specialization</strong></td>
                            <td>Qualitative evidence: agent customizes operator choice and parameters per individual and across optimization progress (time-stamp feature improves performance), implying specialization by problem and by optimization stage; no precise per-operator specialization statistics provided.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Co-training an attention-based learned state representation (NeurELA) with an RL policy that selects among a large, diverse operator pool and controls parameters yields superior optimization performance and generalization (zero-shot to higher-dimension and realistic tasks) compared to hand-crafted feature RL baselines and traditional adaptive DE variants.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Training requires non-trivial compute (PPO training over many epochs); exact computational overhead vs. hand-crafted/adaptive methods is not fully quantified. Some RL-based baselines with problem-dimension-dependent features (e.g., RL-DAS) fail to generalize (marked as "/" on 20D).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1984.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1984.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of adaptive, learned, or parameterized operators (mutation, crossover, variation operators) in evolutionary algorithms, genetic algorithms, or genetic programming, including performance comparisons, operator representations, and results on code or text domains.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Integrated Operator Pool</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Integrated DE operator pool of 14 mutation operators and 3 crossover operators</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A comprehensive set of DE variants (14 mutation strategies including vanilla and many variants, and 3 crossover types) integrated as candidate actions for the RL agent to choose from, enabling wide behavioral diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>operator_name</strong></td>
                            <td>14 mutation operators + 3 crossover operators (pooled)</td>
                        </tr>
                        <tr>
                            <td><strong>operator_type</strong></td>
                            <td>mutation + crossover (variation)</td>
                        </tr>
                        <tr>
                            <td><strong>operator_description</strong></td>
                            <td>Mutation operators include vanilla DE strategies (rand/1, best/1, rand/2, best/2, current-to-rand/1, current-to-best/1, rand-to-best/1) and multiple advanced variants (current-to-pbest/1, current-to-pbest/1 + archive, current-to-rand/1 + archive, weighted-rand-to-pbest/1, ProDE-rand/1, HARDDE-current-to-pbest/2, TopoDE-rand/1, etc.). Crossover operators: binomial, exponential, and p-binomial (pbest-based variant). Each operator has its own parameterization (scale F, crossover CR, plus extra parameters where applicable).</td>
                        </tr>
                        <tr>
                            <td><strong>is_learned_or_adaptive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>learning_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>operator_representation</strong></td>
                            <td>Each operator is a hand-designed variation strategy (formulaic); in the meta-level they are represented as discrete choices (indices) selectable by the RL policy; operator parameters are represented via Gaussian outputs from the actor network.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_type</strong></td>
                            <td>numerical optimization (continuous)</td>
                        </tr>
                        <tr>
                            <td><strong>context_dependent</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>modality_specific</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>compositional</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Used indirectly via RL policy reward (normalized improvement) and final objective / AEI; individual operator ablation/per-operator performance not separately tabulated in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_learned_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_fixed_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_improvement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>executability_preservation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_diversity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>population_size</strong></td>
                            <td>100 (experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>cold_start_addressed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>operator_specialization</strong></td>
                            <td>Paper reports that including diverse operators enables RL agent to deploy distinct optimization strategies tailored to problem instances; no per-operator specialization numbers provided.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A large and diverse operator pool increases robustness and enables even random operator selection (with parameter sampling) to outperform vanilla DE; when combined with learned selection and parameter control, it further improves performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>No explicit per-operator performance breakdown provided; managing operators with differing numbers of parameters required a fixed maximum parameter vector scheme (unused entries ignored).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1984.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1984.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of adaptive, learned, or parameterized operators (mutation, crossover, variation operators) in evolutionary algorithms, genetic algorithms, or genetic programming, including performance comparisons, operator representations, and results on code or text domains.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NeurELA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neural Exploratory Landscape Analysis (NeurELA) feature extractor</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-stage attention-based neural network that embeds populations and evaluation values into expressive landscape features (cross-solution and cross-dimension attention), with mantissa-exponent evaluation-value representation to improve scale invariance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>operator_name</strong></td>
                            <td>learned state features for operator selection (not an operator but critical to operator adaptation)</td>
                        </tr>
                        <tr>
                            <td><strong>operator_type</strong></td>
                            <td>other</td>
                        </tr>
                        <tr>
                            <td><strong>operator_description</strong></td>
                            <td>Inputs population and objective values (organized per-dimension tuples) and applies linear embedding followed by cross-solution self-attention and cross-dimension self-attention; outputs per-individual decision vectors which are concatenated with a learned time-stamp embedding and fed to actor/critic networks for operator/parameter decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>is_learned_or_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>learning_mechanism</strong></td>
                            <td>supervised/self-supervised implicit via co-training with RL policy (gradient descent through the combined network during PPO updates)</td>
                        </tr>
                        <tr>
                            <td><strong>operator_representation</strong></td>
                            <td>attention-based neural representation (transformer-like blocks), MLPs, mantissa-exponent encoding for objective values</td>
                        </tr>
                        <tr>
                            <td><strong>domain_type</strong></td>
                            <td>numerical optimization (used to form state for operator adaptation)</td>
                        </tr>
                        <tr>
                            <td><strong>context_dependent</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>modality_specific</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>compositional</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Contributes to RL policy performance measured by accumulated reward and final optimization quality; ablation study compares 'w/o Time', 'w/o ME', 'MLP' and 'HandCraft' states.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_learned_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_fixed_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_improvement</strong></td>
                            <td>Ablation results: full NeurELA + mantissa-exponent + time-stamp (RLDE-AFL) achieves highest averaged accumulated rewards (10D: 9.645e-1) vs. hand-crafted features (9.512e-1) and other ablations (w/o ME and MLP performed worse), demonstrating measurable benefit of learned features.</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>executability_preservation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_diversity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_results</strong></td>
                            <td>Using NeurELA-enabled states contributed to robust zero-shot transfer to higher dimensions and realistic tasks; ablation without mantissa-exponent normalization (w/o ME) harmed cross-instance generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Added model forward-pass cost per generation; exact overhead not numerically reported.</td>
                        </tr>
                        <tr>
                            <td><strong>population_size</strong></td>
                            <td>Designed to scale across variable population sizes; experiments used population size 100.</td>
                        </tr>
                        <tr>
                            <td><strong>cold_start_addressed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>operator_specialization</strong></td>
                            <td>NeurELA allows the policy to perceive distinct landscape cues enabling operator specialization per instance/time; ablation removing time-stamp reduced performance.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Automatically learned landscape features (NeurELA) with mantissa-exponent evaluation encoding significantly improve RL-based operator/parameter control performance and generalization compared to hand-crafted features.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Min-max normalization (used in original NeurELA) fails to distinguish different problem instances/time and degrades generalization (w/o ME ablation). Computational overhead of attention modules not quantified.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1984.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1984.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of adaptive, learned, or parameterized operators (mutation, crossover, variation operators) in evolutionary algorithms, genetic algorithms, or genetic programming, including performance comparisons, operator representations, and results on code or text domains.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Random (RLDE-AFL without RL agent)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Random-action variant of RLDE-AFL (diverse operators + random selection/parameter sampling)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline where operator choices and parameters are sampled randomly from the same action spaces (no learned policy) to test whether diversity alone yields performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>operator_name</strong></td>
                            <td>Random operator selection + parameter sampling</td>
                        </tr>
                        <tr>
                            <td><strong>operator_type</strong></td>
                            <td>mutation + crossover (variation)</td>
                        </tr>
                        <tr>
                            <td><strong>operator_description</strong></td>
                            <td>At each generation select randomly among the 14 mutation and 3 crossover operators and sample parameter vectors uniformly/according to pre-defined ranges; no RL adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>is_learned_or_adaptive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>learning_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>operator_representation</strong></td>
                            <td>random sampling from categorical and continuous ranges</td>
                        </tr>
                        <tr>
                            <td><strong>domain_type</strong></td>
                            <td>numerical optimization</td>
                        </tr>
                        <tr>
                            <td><strong>context_dependent</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>modality_specific</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>compositional</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Final objective / convergence speed compared to vanilla DE and RLDE-AFL</td>
                        </tr>
                        <tr>
                            <td><strong>performance_learned_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_fixed_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_improvement</strong></td>
                            <td>Paper reports that the Random baseline significantly surpasses vanilla DE, indicating that integrating diverse operators increases robustness even without learning; however RLDE-AFL significantly outperforms Random, validating RL training. Exact numeric per-benchmark numbers for Random vs DE vs RLDE-AFL exist in tables but are not summarized as a single aggregated percentage.</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>executability_preservation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_diversity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Lower training cost (none) but still requires evaluation cost during optimization; no quantitative comparison provided.</td>
                        </tr>
                        <tr>
                            <td><strong>population_size</strong></td>
                            <td>100</td>
                        </tr>
                        <tr>
                            <td><strong>cold_start_addressed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>operator_specialization</strong></td>
                            <td>No specialization (random); nonetheless diversity yielded robustness across problems.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Diversity in operator pool alone offers measurable robustness improvements over vanilla DE, but learned RL policies further improve performance beyond random selection.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Random selection lacks adaptive specialization and is outperformed by trained RL policies across tested benchmarks.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1984.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1984.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of adaptive, learned, or parameterized operators (mutation, crossover, variation operators) in evolutionary algorithms, genetic algorithms, or genetic programming, including performance comparisons, operator representations, and results on code or text domains.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DE-DDQN / DE-DQN (value-based RL baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DE-DDQN / DE-DQN (deep Q-learning based operator selection methods referenced as baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Representative prior RL-based operator selection methods that use (double) deep Q-networks to map hand-crafted or engineered continuous states to discrete operator choices for DE.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>operator_name</strong></td>
                            <td>Value-based RL operator selection (DQN / DDQN)</td>
                        </tr>
                        <tr>
                            <td><strong>operator_type</strong></td>
                            <td>mutation + crossover (variation)</td>
                        </tr>
                        <tr>
                            <td><strong>operator_description</strong></td>
                            <td>These methods construct a state vector (sometimes high-dimensional, e.g., 99-dim) including optimization status and historical operator performance, and use DQN/DDQN to estimate Q-values for discrete operator choices; an argmax or epsilon-greedy policy selects operators per individual or sub-population.</td>
                        </tr>
                        <tr>
                            <td><strong>is_learned_or_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>learning_mechanism</strong></td>
                            <td>value-based reinforcement learning (DQN / Double DQN)</td>
                        </tr>
                        <tr>
                            <td><strong>operator_representation</strong></td>
                            <td>Q-value function approximator (neural network) over discrete operator actions</td>
                        </tr>
                        <tr>
                            <td><strong>domain_type</strong></td>
                            <td>numerical optimization</td>
                        </tr>
                        <tr>
                            <td><strong>context_dependent</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>modality_specific</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>compositional</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>final objective values, convergence speed on benchmarks (MetaBox / BBOB variants)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_learned_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_fixed_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_improvement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>executability_preservation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_diversity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not detailed in this paper; authors note some DQN methods use hand-crafted states that slow time efficiency (DE-DDQN design cited as example).</td>
                        </tr>
                        <tr>
                            <td><strong>population_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>cold_start_addressed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>operator_specialization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>DE-DDQN/DE-DQN are competitive RL baselines but rely on hand-crafted states and limited operator pools; RLDE-AFL outperforms these baselines, attributed to learned NeurELA features and a larger operator pool with parameter control.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Hand-crafted and high-dimensional state designs can be costly and limit generalization; some DQN-based baselines lack parameter control or full operator diversity.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1984.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1984.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of adaptive, learned, or parameterized operators (mutation, crossover, variation operators) in evolutionary algorithms, genetic algorithms, or genetic programming, including performance comparisons, operator representations, and results on code or text domains.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GLEET / LDE (parameter-control RL baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GLEET (Transformer-based) and LDE (LSTM-based) reinforcement-learning parameter control methods</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior RL-based approaches that focus on parameter control: LDE uses an LSTM to sequentially set scale factors and crossover rates per individual, and GLEET uses a Transformer-like architecture to balance exploration-exploitation via parameter control.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>operator_name</strong></td>
                            <td>Learned parameter control (per-individual)</td>
                        </tr>
                        <tr>
                            <td><strong>operator_type</strong></td>
                            <td>mutation + crossover (parameter control)</td>
                        </tr>
                        <tr>
                            <td><strong>operator_description</strong></td>
                            <td>Neural-network policies predict parameters (e.g., F and CR) for DE operators for each individual (LDE uses LSTM over fitness histograms; GLEET uses Transformer architecture). These methods typically do not select among a large explicit operator pool but control parameters for one or a few operator strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>is_learned_or_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>learning_mechanism</strong></td>
                            <td>policy gradient / neural sequence models (LSTM, Transformer variants)</td>
                        </tr>
                        <tr>
                            <td><strong>operator_representation</strong></td>
                            <td>parameter vectors predicted by neural networks (continuous action prediction); policy outputs used to sample or set operator parameters</td>
                        </tr>
                        <tr>
                            <td><strong>domain_type</strong></td>
                            <td>numerical optimization</td>
                        </tr>
                        <tr>
                            <td><strong>context_dependent</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>modality_specific</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>compositional</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>final objective values, convergence speed, AEI for realistic problems</td>
                        </tr>
                        <tr>
                            <td><strong>performance_learned_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_fixed_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_improvement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>executability_preservation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_diversity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not precisely quantified in this paper; GLEET and LDE require training of sequence models but are considered competitive baselines. RLDE-AFL is reported to outperform GLEET on many synthetic problems but is competitive with GLEET on realistic protein-docking AEI performance.</td>
                        </tr>
                        <tr>
                            <td><strong>population_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>cold_start_addressed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>operator_specialization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Parameter-control RL methods achieve good performance; RLDE-AFL that integrates both operator selection and parameter control plus learned features often outperforms pure parameter-control baselines but is competitive on some realistic tasks (GLEET).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Parameter-control-only approaches may underutilize operator diversity; some methods may be dimension-dependent or less generalizable than NeurELA-enabled approaches.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Learning adaptive differential evolution algorithm from optimization experiences by policy gradient <em>(Rating: 2)</em></li>
                <li>MetaBox: A Benchmark Platform for Meta-Black-Box Optimization with Reinforcement Learning <em>(Rating: 2)</em></li>
                <li>Neural exploratory landscape analysis <em>(Rating: 2)</em></li>
                <li>Deep reinforcement learning based parameter control in differential evolution <em>(Rating: 2)</em></li>
                <li>Reinforcement learning-based differential evolution for global optimization <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1984",
    "paper_id": "paper-277272630",
    "extraction_schema_id": "extraction-schema-45",
    "extracted_data": [
        {
            "name_short": "RLDE-AFL",
            "name_full": "Reinforcement Learning-based Self-adaptive Differential Evolution through Automated Landscape Feature Learning",
            "brief_description": "A meta-Black-Box-Optimization method that jointly learns a meta-level policy (via PPO) and an attention-based feature extractor (NeurELA) to select DE mutation/crossover operators and their parameters per-individual and per-generation, using an integrated pool of diverse DE operators.",
            "citation_title": "here",
            "mention_or_use": "use",
            "operator_name": "Learned operator selection and parameter control (per-individual)",
            "operator_type": "mutation + crossover (variation)",
            "operator_description": "At each generation and for each individual the agent (actor network) outputs: (1) a categorical distribution over 14 mutation operators (softmax) and 3 crossover operators (softmax), and (2) Gaussian distributions for up to 3 mutation parameters and up to 2 crossover parameters; an operator index is sampled and the corresponding parameters (first k of the sampled vector) are used to configure the DE operator for producing offspring.",
            "is_learned_or_adaptive": true,
            "learning_mechanism": "reinforcement learning (Proximal Policy Optimization, PPO) co-trained with a learned feature extractor",
            "operator_representation": "Categorical probability vectors (softmax) for operator choice; parameter distributions represented as Gaussian N(mu,sigma) outputs from MLPs",
            "domain_type": "numerical optimization (continuous black-box functions)",
            "context_dependent": true,
            "modality_specific": false,
            "compositional": true,
            "performance_metric": "normalized accumulated reward (training objective), convergence speed, best objective / AEI for realistic problems; aggregated accumulated reward and AEI reported for benchmarks",
            "performance_learned_operator": "Aggregated accumulated rewards reported in ablation/summary: 10D testing problems: 9.645e-1 (≈0.9645); 20D: 9.334e-1 (≈0.9334); expensive problems: 8.856e-1 (≈0.8856); realistic (protein-docking): 7.056e-1 (≈0.7056) — values taken from the paper's averaged metrics tables",
            "performance_fixed_operator": null,
            "performance_improvement": "Qualitative and aggregated improvements reported: RLDE-AFL outperforms advanced traditional DE variants (MadDE, JDE21, NL-SHADE-LBC, AMCDE) and multiple RL-based baselines on MetaBox synthetic benchmarks and shows superior zero-shot convergence speed on expensive scenarios; exact per-baseline percentage improvements are not aggregated in a single table in the paper",
            "has_comparison": true,
            "executability_preservation": null,
            "novelty_diversity_metric": "Not reported as explicit novelty/diversity numeric metric; diversity implicitly measured via optimization performance and operator-behavior diversity from the integrated pool",
            "transfer_learning": true,
            "transfer_results": "Zero-shot generalization from models trained on 10D synthetic problems to 20D synthetic problems, expensive problems, and protein-docking realistic problems: RLDE-AFL retains strong performance and faster convergence in expensive scenarios and is competitive on protein-docking (AEI metric) versus baselines (competitive with GLEET, surpasses DE-DDQN, MadDE and NL-SHADE-LBC).",
            "computational_cost": "Not quantified precisely; training details: PPO training for 100 epochs, learning rate 1e-3, population size 100, optimization horizon 200 (20,000 FEs), actor/critic updated with PPO steps; experiments run on Intel Xeon + NVIDIA 1080Ti. Authors note training cost but give no single-number overhead relative to fixed operators.",
            "population_size": "100",
            "cold_start_addressed": false,
            "operator_specialization": "Qualitative evidence: agent customizes operator choice and parameters per individual and across optimization progress (time-stamp feature improves performance), implying specialization by problem and by optimization stage; no precise per-operator specialization statistics provided.",
            "key_findings": "Co-training an attention-based learned state representation (NeurELA) with an RL policy that selects among a large, diverse operator pool and controls parameters yields superior optimization performance and generalization (zero-shot to higher-dimension and realistic tasks) compared to hand-crafted feature RL baselines and traditional adaptive DE variants.",
            "limitations_or_failures": "Training requires non-trivial compute (PPO training over many epochs); exact computational overhead vs. hand-crafted/adaptive methods is not fully quantified. Some RL-based baselines with problem-dimension-dependent features (e.g., RL-DAS) fail to generalize (marked as \"/\" on 20D).",
            "uuid": "e1984.0"
        },
        {
            "name_short": "Integrated Operator Pool",
            "name_full": "Integrated DE operator pool of 14 mutation operators and 3 crossover operators",
            "brief_description": "A comprehensive set of DE variants (14 mutation strategies including vanilla and many variants, and 3 crossover types) integrated as candidate actions for the RL agent to choose from, enabling wide behavioral diversity.",
            "citation_title": "here",
            "mention_or_use": "use",
            "operator_name": "14 mutation operators + 3 crossover operators (pooled)",
            "operator_type": "mutation + crossover (variation)",
            "operator_description": "Mutation operators include vanilla DE strategies (rand/1, best/1, rand/2, best/2, current-to-rand/1, current-to-best/1, rand-to-best/1) and multiple advanced variants (current-to-pbest/1, current-to-pbest/1 + archive, current-to-rand/1 + archive, weighted-rand-to-pbest/1, ProDE-rand/1, HARDDE-current-to-pbest/2, TopoDE-rand/1, etc.). Crossover operators: binomial, exponential, and p-binomial (pbest-based variant). Each operator has its own parameterization (scale F, crossover CR, plus extra parameters where applicable).",
            "is_learned_or_adaptive": false,
            "learning_mechanism": null,
            "operator_representation": "Each operator is a hand-designed variation strategy (formulaic); in the meta-level they are represented as discrete choices (indices) selectable by the RL policy; operator parameters are represented via Gaussian outputs from the actor network.",
            "domain_type": "numerical optimization (continuous)",
            "context_dependent": null,
            "modality_specific": false,
            "compositional": true,
            "performance_metric": "Used indirectly via RL policy reward (normalized improvement) and final objective / AEI; individual operator ablation/per-operator performance not separately tabulated in main text.",
            "performance_learned_operator": null,
            "performance_fixed_operator": null,
            "performance_improvement": null,
            "has_comparison": true,
            "executability_preservation": null,
            "novelty_diversity_metric": null,
            "transfer_learning": null,
            "transfer_results": null,
            "computational_cost": null,
            "population_size": "100 (experiments)",
            "cold_start_addressed": null,
            "operator_specialization": "Paper reports that including diverse operators enables RL agent to deploy distinct optimization strategies tailored to problem instances; no per-operator specialization numbers provided.",
            "key_findings": "A large and diverse operator pool increases robustness and enables even random operator selection (with parameter sampling) to outperform vanilla DE; when combined with learned selection and parameter control, it further improves performance.",
            "limitations_or_failures": "No explicit per-operator performance breakdown provided; managing operators with differing numbers of parameters required a fixed maximum parameter vector scheme (unused entries ignored).",
            "uuid": "e1984.1"
        },
        {
            "name_short": "NeurELA",
            "name_full": "Neural Exploratory Landscape Analysis (NeurELA) feature extractor",
            "brief_description": "A two-stage attention-based neural network that embeds populations and evaluation values into expressive landscape features (cross-solution and cross-dimension attention), with mantissa-exponent evaluation-value representation to improve scale invariance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "operator_name": "learned state features for operator selection (not an operator but critical to operator adaptation)",
            "operator_type": "other",
            "operator_description": "Inputs population and objective values (organized per-dimension tuples) and applies linear embedding followed by cross-solution self-attention and cross-dimension self-attention; outputs per-individual decision vectors which are concatenated with a learned time-stamp embedding and fed to actor/critic networks for operator/parameter decisions.",
            "is_learned_or_adaptive": true,
            "learning_mechanism": "supervised/self-supervised implicit via co-training with RL policy (gradient descent through the combined network during PPO updates)",
            "operator_representation": "attention-based neural representation (transformer-like blocks), MLPs, mantissa-exponent encoding for objective values",
            "domain_type": "numerical optimization (used to form state for operator adaptation)",
            "context_dependent": true,
            "modality_specific": false,
            "compositional": true,
            "performance_metric": "Contributes to RL policy performance measured by accumulated reward and final optimization quality; ablation study compares 'w/o Time', 'w/o ME', 'MLP' and 'HandCraft' states.",
            "performance_learned_operator": null,
            "performance_fixed_operator": null,
            "performance_improvement": "Ablation results: full NeurELA + mantissa-exponent + time-stamp (RLDE-AFL) achieves highest averaged accumulated rewards (10D: 9.645e-1) vs. hand-crafted features (9.512e-1) and other ablations (w/o ME and MLP performed worse), demonstrating measurable benefit of learned features.",
            "has_comparison": true,
            "executability_preservation": null,
            "novelty_diversity_metric": null,
            "transfer_learning": true,
            "transfer_results": "Using NeurELA-enabled states contributed to robust zero-shot transfer to higher dimensions and realistic tasks; ablation without mantissa-exponent normalization (w/o ME) harmed cross-instance generalization.",
            "computational_cost": "Added model forward-pass cost per generation; exact overhead not numerically reported.",
            "population_size": "Designed to scale across variable population sizes; experiments used population size 100.",
            "cold_start_addressed": false,
            "operator_specialization": "NeurELA allows the policy to perceive distinct landscape cues enabling operator specialization per instance/time; ablation removing time-stamp reduced performance.",
            "key_findings": "Automatically learned landscape features (NeurELA) with mantissa-exponent evaluation encoding significantly improve RL-based operator/parameter control performance and generalization compared to hand-crafted features.",
            "limitations_or_failures": "Min-max normalization (used in original NeurELA) fails to distinguish different problem instances/time and degrades generalization (w/o ME ablation). Computational overhead of attention modules not quantified.",
            "uuid": "e1984.2"
        },
        {
            "name_short": "Random (RLDE-AFL without RL agent)",
            "name_full": "Random-action variant of RLDE-AFL (diverse operators + random selection/parameter sampling)",
            "brief_description": "Baseline where operator choices and parameters are sampled randomly from the same action spaces (no learned policy) to test whether diversity alone yields performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "operator_name": "Random operator selection + parameter sampling",
            "operator_type": "mutation + crossover (variation)",
            "operator_description": "At each generation select randomly among the 14 mutation and 3 crossover operators and sample parameter vectors uniformly/according to pre-defined ranges; no RL adaptation.",
            "is_learned_or_adaptive": false,
            "learning_mechanism": null,
            "operator_representation": "random sampling from categorical and continuous ranges",
            "domain_type": "numerical optimization",
            "context_dependent": false,
            "modality_specific": false,
            "compositional": true,
            "performance_metric": "Final objective / convergence speed compared to vanilla DE and RLDE-AFL",
            "performance_learned_operator": null,
            "performance_fixed_operator": null,
            "performance_improvement": "Paper reports that the Random baseline significantly surpasses vanilla DE, indicating that integrating diverse operators increases robustness even without learning; however RLDE-AFL significantly outperforms Random, validating RL training. Exact numeric per-benchmark numbers for Random vs DE vs RLDE-AFL exist in tables but are not summarized as a single aggregated percentage.",
            "has_comparison": true,
            "executability_preservation": null,
            "novelty_diversity_metric": null,
            "transfer_learning": false,
            "transfer_results": null,
            "computational_cost": "Lower training cost (none) but still requires evaluation cost during optimization; no quantitative comparison provided.",
            "population_size": "100",
            "cold_start_addressed": true,
            "operator_specialization": "No specialization (random); nonetheless diversity yielded robustness across problems.",
            "key_findings": "Diversity in operator pool alone offers measurable robustness improvements over vanilla DE, but learned RL policies further improve performance beyond random selection.",
            "limitations_or_failures": "Random selection lacks adaptive specialization and is outperformed by trained RL policies across tested benchmarks.",
            "uuid": "e1984.3"
        },
        {
            "name_short": "DE-DDQN / DE-DQN (value-based RL baselines)",
            "name_full": "DE-DDQN / DE-DQN (deep Q-learning based operator selection methods referenced as baselines)",
            "brief_description": "Representative prior RL-based operator selection methods that use (double) deep Q-networks to map hand-crafted or engineered continuous states to discrete operator choices for DE.",
            "citation_title": "",
            "mention_or_use": "mention",
            "operator_name": "Value-based RL operator selection (DQN / DDQN)",
            "operator_type": "mutation + crossover (variation)",
            "operator_description": "These methods construct a state vector (sometimes high-dimensional, e.g., 99-dim) including optimization status and historical operator performance, and use DQN/DDQN to estimate Q-values for discrete operator choices; an argmax or epsilon-greedy policy selects operators per individual or sub-population.",
            "is_learned_or_adaptive": true,
            "learning_mechanism": "value-based reinforcement learning (DQN / Double DQN)",
            "operator_representation": "Q-value function approximator (neural network) over discrete operator actions",
            "domain_type": "numerical optimization",
            "context_dependent": true,
            "modality_specific": false,
            "compositional": false,
            "performance_metric": "final objective values, convergence speed on benchmarks (MetaBox / BBOB variants)",
            "performance_learned_operator": null,
            "performance_fixed_operator": null,
            "performance_improvement": null,
            "has_comparison": true,
            "executability_preservation": null,
            "novelty_diversity_metric": null,
            "transfer_learning": false,
            "transfer_results": null,
            "computational_cost": "Not detailed in this paper; authors note some DQN methods use hand-crafted states that slow time efficiency (DE-DDQN design cited as example).",
            "population_size": null,
            "cold_start_addressed": false,
            "operator_specialization": null,
            "key_findings": "DE-DDQN/DE-DQN are competitive RL baselines but rely on hand-crafted states and limited operator pools; RLDE-AFL outperforms these baselines, attributed to learned NeurELA features and a larger operator pool with parameter control.",
            "limitations_or_failures": "Hand-crafted and high-dimensional state designs can be costly and limit generalization; some DQN-based baselines lack parameter control or full operator diversity.",
            "uuid": "e1984.4"
        },
        {
            "name_short": "GLEET / LDE (parameter-control RL baselines)",
            "name_full": "GLEET (Transformer-based) and LDE (LSTM-based) reinforcement-learning parameter control methods",
            "brief_description": "Prior RL-based approaches that focus on parameter control: LDE uses an LSTM to sequentially set scale factors and crossover rates per individual, and GLEET uses a Transformer-like architecture to balance exploration-exploitation via parameter control.",
            "citation_title": "",
            "mention_or_use": "mention",
            "operator_name": "Learned parameter control (per-individual)",
            "operator_type": "mutation + crossover (parameter control)",
            "operator_description": "Neural-network policies predict parameters (e.g., F and CR) for DE operators for each individual (LDE uses LSTM over fitness histograms; GLEET uses Transformer architecture). These methods typically do not select among a large explicit operator pool but control parameters for one or a few operator strategies.",
            "is_learned_or_adaptive": true,
            "learning_mechanism": "policy gradient / neural sequence models (LSTM, Transformer variants)",
            "operator_representation": "parameter vectors predicted by neural networks (continuous action prediction); policy outputs used to sample or set operator parameters",
            "domain_type": "numerical optimization",
            "context_dependent": true,
            "modality_specific": false,
            "compositional": false,
            "performance_metric": "final objective values, convergence speed, AEI for realistic problems",
            "performance_learned_operator": null,
            "performance_fixed_operator": null,
            "performance_improvement": null,
            "has_comparison": true,
            "executability_preservation": null,
            "novelty_diversity_metric": null,
            "transfer_learning": false,
            "transfer_results": null,
            "computational_cost": "Not precisely quantified in this paper; GLEET and LDE require training of sequence models but are considered competitive baselines. RLDE-AFL is reported to outperform GLEET on many synthetic problems but is competitive with GLEET on realistic protein-docking AEI performance.",
            "population_size": null,
            "cold_start_addressed": false,
            "operator_specialization": null,
            "key_findings": "Parameter-control RL methods achieve good performance; RLDE-AFL that integrates both operator selection and parameter control plus learned features often outperforms pure parameter-control baselines but is competitive on some realistic tasks (GLEET).",
            "limitations_or_failures": "Parameter-control-only approaches may underutilize operator diversity; some methods may be dimension-dependent or less generalizable than NeurELA-enabled approaches.",
            "uuid": "e1984.5"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Learning adaptive differential evolution algorithm from optimization experiences by policy gradient",
            "rating": 2
        },
        {
            "paper_title": "MetaBox: A Benchmark Platform for Meta-Black-Box Optimization with Reinforcement Learning",
            "rating": 2
        },
        {
            "paper_title": "Neural exploratory landscape analysis",
            "rating": 2
        },
        {
            "paper_title": "Deep reinforcement learning based parameter control in differential evolution",
            "rating": 2
        },
        {
            "paper_title": "Reinforcement learning-based differential evolution for global optimization",
            "rating": 1
        }
    ],
    "cost": 0.01831025,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Reinforcement Learning-based Self-adaptive Differential Evolution through Automated Landscape Feature Learning</p>
<p>Hongshu Guo guohongshu369@gmail.com 
Yue-Jiao Gong gongyuejiao@gmail.com 0000-0002-5648-1160</p>
<p>South China University of Technology Guangzhou
GuangdongChina</p>
<p>Sijie Ma</p>
<p>South China University of Technology Guangzhou
GuangdongChina Zechuan Huang</p>
<p>South China University of Technology Guangzhou
GuangdongChina Yuzhi Hu</p>
<p>South China University of Technology Guangzhou
GuangdongChina</p>
<p>Zeyuan Ma</p>
<p>South China University of Technology Guangzhou
GuangdongChina</p>
<p>Xinglin Zhang</p>
<p>South China University of Technology Guangzhou
GuangdongChina</p>
<p>South China University of Technology Guangzhou
GuangdongChina</p>
<p>Reinforcement Learning-based Self-adaptive Differential Evolution through Automated Landscape Feature Learning
1851CEB055EEC388C5C96033B02096CAAutomatic configurationdifferential evolutionreinforcement learningmeta-black-box optimization
Recently, Meta-Black-Box-Optimization (MetaBBO) methods significantly enhance the performance of traditional black-box optimizers through meta-learning flexible and generalizable meta-level policies that excel in dynamic algorithm configuration (DAC) tasks within the low-level optimization, reducing the expertise required to adapt optimizers for novel optimization tasks.Though promising, existing MetaBBO methods heavily rely on human-crafted feature extraction approach to secure learning effectiveness.To address this issue, this paper introduces a novel MetaBBO method that supports automated feature learning during the meta-learning process, termed as RLDE-AFL, which integrates a learnable feature extraction module into a reinforcement learning-based DE method to learn both the feature encoding and meta-level policy.Specifically, we design an attention-based neural network with mantissa-exponent based embedding to transform the solution populations and corresponding objective values during the low-level optimization into expressive landscape features.We further incorporate a comprehensive algorithm configuration space including diverse DE operators into a reinforcement learning-aided DAC paradigm to unleash the behavior diversity and performance of the proposed RLDE-AFL.Extensive benchmark results show that co-training the proposed feature learning module and DAC policy contributes to the superior optimization performance of RLDE-AFL to several advanced DE methods and recent MetaBBO baselines over both synthetic and realistic BBO scenarios.The source codes of RLDE-AFL are available at https://github.com/GMC-DRL/RLDE-AFL.CCS Concepts• Computing methodologies → Bio-inspired approaches; Reinforcement learning; Markov decision processes.</p>
<p>Introduction</p>
<p>In the last few decades, Evolutionary Computation (EC) methods such as Genetic Algorithm (GA) [13], Particle Swarm Optimization (PSO) [21] and Differential Evolution (DE) [52] have become more and more eye-catching in solving Black-Box Optimization (BBO) problems lacking accessible formulations or derivative information in both academia and industry [35].However, according to the "No-Free-Lunch" (NFL) theorem [62], no single algorithm or algorithm configuration (AC) can dominate on all problems.Therefore, to enhance the optimization performance on diverse problems, researchers have manually designed various adaptive operator selection and parameter control methods, which achieve superior performance on BBO benchmarks [12,33,38].However, the design of the operator and parameter adaptive mechanisms requires substantial experience and deep expertise in optimization problems and algorithms.One may need to adjust the configurations iteratively according to the problem characteristic and optimization feedback, consuming significant time and computational resources.</p>
<p>To relieve the human effort burden, recent researchers introduce Meta-Black-Box Optimization (MetaBBO) which leverages a metalevel policy to replace the human-crafted algorithm designs in Algorithm Selection [10], Algorithm Configuration [28,32,48,53], Solution Manipulation [23][24][25]64] and Algorithm Generation [7,34,75].MetaBBO methods typically involve a bi-level architecture.In the meta level, given the optimization state, a neural network based policy determines an appropriate algorithm design for the low-level algorithm at each optimization generation.The resulting changes in objective values after algorithm optimization are returned to the meta-level policy as meta performance signal, which is then used to refine the meta policy [35,68].Given the meta policy   parameterized by  with the algorithm  on a set of problem instances I, the objective for algorithm configuration is formulated as: where  is the optimization horizon,   (•) is a performance metric function,   =   (  ) is the algorithm design outputted by the policy and   is the optimization state at generation .For algorithm configuration,  may correspond to operator selection, parameter values, or both.To maximize the expected performance on the problem distribution, machine learning methods such as Reinforcement Learning (RL) are widely adopted for policy training.Though promising, these works still retain significant potential to further reduce the expertise burden and enhance the performance.The first limitation is the expertise dependent optimization state design, which requires substantial domain to develop informing and representative features for comprehensive configurations.Besides, while traditional BBO community have developed diverse operator variants, existing MetaBBO methods typically adopt only a small subset of these operators, leading to a limited strategy diversity.Moreover, the configuration spaces in existing MetaBBO methods are restricted, most works focus on exclusively operator selection [48,54] or parameter control [32,53], failing to fully unleash the behavior diversity of the meta policies.
J(𝜃 ) = arg max 𝜃 ∈Θ E 𝑓 ∈ I 𝑇 ∑︁ 𝑡 =1 𝑃𝑒𝑟 𝑓 (𝐴, 𝜔 𝑡 , 𝑓 )(1
To address these issues, we propose RLDE-AFL.Firstly, for the expertise-dependent state representation, we employ NeurELA [31], as shown in the top left of Figure 1, a self-attention neural network based optimization information extractor to automatically analyze the problem landscape from the population and evaluation values, eliminating the need for manual feature design.The two-stage self-attention between the dimensions and individuals achieves generalizable and comprehensive state extraction.By integrating the mantissa-exponent based evaluation value representation, we further enhance the generalization ability of RLDE-AFL.</p>
<p>Then, to further enhance the performance of RLDE-AFL, we integrate 14 DE mutation operators and 3 DE crossover operators with diverse optimization behaviours to form the candidate operator pool (illustrated in the bottom right of Figure 1).This diversity enables the agent to adaptively deploy distinct optimization strategies tailored to different problem instances.By incorporating parameter control, RL agent gains full control over the DE framework through optimization states derived from the automatic feature extractor, thereby unleashing the behavior diversity of RL agent to achieve superior performance.</p>
<p>Finally, we conduct extensive experiments to demonstrate the effectiveness of RLDE-AFL on MetaBox [33] benchmark problems.The comparisons with advanced traditional DE algorithms and RLbased DE methods confirm the superior performance of RLDE-AFL.The zero-shot performance on different dimensional problems, expensive problems and out-of-distribution realistic problems shows the robust generalization capabilities of RLDE-AFL, outperforming existing advanced traditional and learning based methods.</p>
<p>The rest of this paper is organized as follows: Section 2 reviews the related works on existing traditional adaptive DE.Section 3 introduces the preliminary concepts on RL and NeurELA.In Section 4, we present our technical details, including the Markov Decision Process (MDP) definition, network design and training process.The experimental results are presented in Section 5, followed by a conclusion in Section 6.</p>
<p>Related Works 2.1 Traditional AC for DE</p>
<p>For DE algorithms, the algorithm configurations usually focus on selecting mutation and crossover operators and controlling the parameters of these operators.In vanilla DE [52], the used operator and parameters are static across optimization horizon and problem instances.However, since the landscapes and characteristics of BBO problems can vary, static configurations may not be optimal.</p>
<p>In the last few decades, DE community has proposed diverse mutation and crossover operators with diverse exploratory and exploitative behaviours.Combining these operators using adaptive operator selection mechanisms is a promising approach for superior performance.SaDE [43] assigns each individual mutation operators selected from the two candidate operators following the probabilities calculated from operators' historical performance.CoDE [60] integrates 3 mutation-crossover combinations and selects the best one among the 3 individuals generated by the three combinations as the offspring.LADE [26] assigns different mutation operators for different sub-populations to enhance the population diversity.</p>
<p>To further enhance the behavior diversity and improve algorithm performance, jDE [4] randomly generates scale factor  for mutation and crossover rate  for crossover operators, then keeps the parameters that successfully improved individuals.One of the advanced variant of jDE, JDE21 [5] introduces multi-population mechanism and divides the population into exploratory one and exploitative one with different parameter and operator configurations.However, random searching parameter configurations is not efficient, JADE [74] samples parameters from normal or Cauchy distribution with the mean updated by the Lehmer mean of the parameter values that successfully improve the individuals in last generation.Based on JADE, SHADE [57] uses two memories to record the Lehmer mean of  and  respectively in each generation, which enhances the exploration.Such successful history based parameter adaptive mechanism is widely adopted in advanced DE variants such as MadDE [2], NL-SHADE-LBC [49] and L-SRTDE [51].</p>
<p>Although these mechanisms bring significant performance [22,38,42], one must equip with enough expert knowledge on the algorithm and optimization problem to design suitable operator selection rules and parameter adaptive mechanisms, which leads to heavy human-effort burden.</p>
<p>MetaBBO-RL based AC for DE</p>
<p>To relieve the burden, researchers turns to machine learning for answers.One of the most commonly adopted solution is to use Reinforcement Learning (RL) agents to learn the knowledge about optimization and dynamically determine the operator selection and parameter control according to the optimization states.For operator selection, value-based RL methods such as Tabular Q-Learning [61] and Deep Q-Learning [37] are widely adopted to handle the discrete action spaces.Tabular Q-Learning maintains a table mapping each discrete state to an optimal action [9,11,14,17,27,45,66,73].For instance, RLDMDE [67] selects mutation-crossover operator combinations for each sub-population according to the population diversity levels.However, the coarse-grained discrete state spaces may lead to information loss and degrade the performance.Therefore, neural networks are introduced to process continuous states and predict the expected accumulated reward of each candidate action, which turns to Deep Q-Network (DQN) [39,48,54,73].DE-DDQN [48] designs a 99-dimensional continuous state including the optimization status and operator performance history.A Multilayer Perceptron (MLP) based Double DQN [58] agent is employed to determine the mutation operator for each individual.Though promising, the complex state design significantly slows down the time efficiency.DE-DQN [54] uses Fitness Landscape Analysis (FLA) [63] to extract 4 optimization features from random walk sampling, which are then feed into a DQN agent for operator selection.Besides, neural networks can also be used to predict the probabilities of action selection [16,28,29,72].RLEMMO [28] uses the agent trained by Proximal Policy Optimization (PPO) [46] to select DE mutation operators for solving multi-modal problems.Furthermore, some methods consider the whole DE algorithms as switchable components.RL-DAS [10] selects algorithms periodically from three advanced candidate DE algorithms according to the optimization status and algorithm histories.</p>
<p>For parameter control, some methods discretize the continuous action space and use Q-Learning to select the parameter values [15,18,70].RLMODE [70] uses the feasible and domination relationship of individuals as the states to control increasing or decreasing the values of scale factors and crossover rates for solving constrained multi-objective problems.More RL-based parameter control methods for DE use neural networks to predict the distribution of the target parameters [30,32,40,53,65,71].LDE [53] leverages Long Short-Term Memory (LSTM) network to sequentially determine the values of scale factors and crossover rates for each individual according to the population fitness histograms.GLEET [32] further proposes a Transformer-based architecture [59] to balance the exploration-exploitation in DE and PSO by using parameter control.There are also works that control both operator and parameters [55].RL-HPSDE [55] employs Q-table to select the combinations of operator and parameter sampling methods.</p>
<p>Although these RL based adaptive methods achieve remarkable optimization performance, they still face three limitations.Firstly, the design of state features still require expertise on optimization problems and algorithms so that features can be properly selected to reflect the optimization status.Besides, some features need extra function evaluations for random sampling which occupy the resource for optimization.Secondly, the collected candidate operators in the operator pools of existing methods are limited, only a small number of operators are considered, which limits the strategy diversity and generalization.Finally, most of these methods focus on selecting operators only or controlling parameters only, the learning and generalization ability of RL agents are not fully developed.To address these limitations, we hence in this paper propose RLDE-AFL, which introduces automatic learning based state representation and integrates diverse operators fully controlled by the RL agent for superior performance.</p>
<p>Preliminary 3.1 Markov Decision Process</p>
<p>A MDP could be denoted as M :=&lt; S, A, T ,  &gt;.Given a state   ∈ S at time step , the policy  accordingly determines an action   ∈ A which interacts with the environment.The next state   +1 is produced by the changed environment through the environment dynamic T (  +1 |  ,   ).A reward function  : S × A → R acts as a performance metric measuring the performance of the actions.Those transitions between states and actions achieve a trajectory  := ( 0 ,  0 ,  1 , • • • ,   ).The target of MDP is to find an optimal policy  * that maximizes the accumulated rewards in trajectories:
𝜋 * = arg max 𝜋 ∈Π 𝑇 ∑︁ 𝑡 =1 𝛾 𝑡 −1 𝑅(𝑠 𝑡 , 𝑎 𝑡 )(2)
where Π : S → A selects an action with a given state,  is a discount factor and  is the length of trajectory.</p>
<p>Neural Evolutionary Landscape Analysis</p>
<p>To obtain the optimization status or the problem characteristic, various landscape analysis methods such as Evolutionary Landscape Analysis (ELA) have emerged.Although these approaches provide a comprehensive understanding, they require a certain level of expert knowledge and consume non-negligible computational resource.</p>
<p>To address these issues, learning-based ELA methods are proposed to use neural networks to analyze landscapes [41,47].However, these methods are constrained by problem dimensions or profile problems statically, which makes them not applicable for the feature extraction in MetaBBO.Recently, Ma et al. [31] proposes Neural Exploratory Landscape Analysis (NeurELA), which employs a twostage attention-based neural network as a feature extractor.Given a population  ∈ R  × with  -dimensional solutions and its corresponding evaluation values  ∈ R  , the observations  are organized as per-dimensional tuples {{( , ,   )}  =1 }  =1 , with a shape of  ×  × 2. Then  is embedded by a linear layer and advanced the two-stage attention for information sharing in cross-solution and cross-dimension levels, respectively.The cross-solution attention uses an attention block to enable same dimensions shared by different candidate solutions within the population to exchange information, while the cross-dimension attention further promotes the sharing of information across different dimensions within each candidate.In this way, NeurELA efficiently extracts comprehensive optimization state information for each individual automatically.The attention-based architectures on both dimension and solution levels boost the scalability of NeurELA across different problem dimensions and different algorithm population sizes.</p>
<p>Methodology 4.1 MDP Formulation</p>
<p>At the -th generation, given a state   = {  ,   ,  } including the population   ∈ R  × with  -dimensional individuals and the corresponding evaluation values   =  (  ) under problem instance  , in the meta level the RLDE-AFL policy   parameterized by  extracts the optimization features of all individuals using the modified NeurELA module from   , then determines the operator selection and parameter control actions   ∼   (  ) for each individual.With   as configurations, the DE algorithm in the lower level optimize the population and produces the next state   +1 = {  +1 ,   +1 ,  + 1}.The reward function  is introduced to evaluate the performance improvement   = (  ,   | ).Considering a set of problem instances I, RL agent targets at finding an optimal policy   * which maximizes the expected accumulated rewards over all problem instances  ∈ I:
𝜃 * = arg max 𝜃 ∈Θ E 𝑓 ∈ I 𝑇 ∑︁ 𝑡 =1 𝛾 𝑡 −1 𝑅(𝑠 𝑡 , 𝜋 𝜃 (𝑠 𝑡 )|𝑓 )(3)
In this paper, we use the Proximal Policy Optimization (PPO) [46] to train the policy.Next we introduce the detailed MDP designs, including the state, action and reward in the following subsections.where   and   are the upper and lower bounds of the searching space at the -th dimension, respectively.Besides, the objective value scales across different problem instances can also vary, to ensure state values across problem instances share the same numerical level, we introduce the mantissa-exponent representation for the evaluation value terms in states.Specifically, for each evaluation value  , ∈   , we first represent it in scientific notation  , =  × 10  ,  ∈ [−1, 1],  ∈ Z. Then we use a tuple { , ,  , } as the mantissa-exponent representation of  , in the state, where  =   and  is a scale factor making the scales of exponents in all problem instances similar.For the time stamp  in the state, we normalize it with the optimization horizon  so that it would share the same scale with other features:   =   .In summary, the state at generation  is represented as Besides, fine-tuning the parameters of the selected operators is also a key to superior performance.Therefore, the action space for each individual in this paper includes two parameter control actions  1 ∈ [0, 1]  1 and  2 ∈ [0, 1]  2 for the selected mutation operators and crossover operators respectively. 1 and  2 are the maximal numbers of parameters of all integrated mutation and crossover operators respectively.In summary, the overall action space for the population at generation  is
𝑠 𝑡 = {{{ 𝑥 𝑡,𝑖,𝑗 𝑢𝑏 𝑗 −𝑙𝑏 𝑗 } 𝐷 𝑗=1 } 𝑁 𝑖=1 , {(𝜛𝑎 𝑡 = {(𝑎 𝑜𝑠1 𝑖,𝑡 , 𝑎 𝑝𝑐1 𝑖,𝑡 , 𝑎 𝑜𝑠2 𝑖,𝑡 , 𝑎 𝑝𝑐2 𝑖,𝑡 )} 𝑁 𝑖=1 .
4.1.3Reward.We use the reward function formulated as follow:
𝑟 𝑡 = 𝑦 * 𝑡 −1 − 𝑦 * 𝑡 𝑦 * 0 − 𝑦 *(4)
where  *  is the found best evaluation at generation ,  * 0 is the best value in the initial population and  * is the global optimal evaluation value of the problem instance.The accumulated rewards reflects the optimization performance, and the denominator normalizes all reward values into [0, 1] to make the scales of the accumulated rewards in all problems similar, hence stabilize the training.In the crosssolution attention, information are shared between the representations of all solutions at the same dimension: ĥ(1) =LN(  (h (0) ) + h (0) ) h (1) =LN( ( ĥ(1) ; W</p>
<p>Network Design
Y t X i,j ϖ i ε i X N,j ϖ N ε N ... D Cross-D N N N N MLP 80 X 32 X 3 MLP 80 X 32 X 2 μ 1 σ 1 μ 2 σ 2
 )) + ĥ( 1) )</p>
<p>where LN denotes the Layernorm [1],   is the Multi-head Self-Attention [59] in the solution dimension and  (•; W</p>
<p>) is a MLP layer with the shape of 64×64.In the cross-dimension attention, the encoded results h (1) from the cross-solution attention is transposed to  ×  × 64 and augmented with cosine/sine positional encodings to maintain the dimensional order within a solution.</p>
<p>h ′(1) = (h (1) ) T + W  (6) where W  is the positional encoding weights.Subsequently, we conduct the feature extraction to discover the inner connections between the dimensions: ĥ(2) =LN(  (h ′ (1) ) + h ′(1) )</p>
<p>h (2) =LN( ( ĥ(2) ; W</p>
<p>)) + ĥ( 2) )</p>
<p>where   is the self-attention module between the dimensions and  (•; W</p>
<p>) is also a MLP layer with the shape of 64 × 64.The encoded individual representations e   ∈ R  ×64 is obtained by meanpooling h (2) ∈ R  × ×64 in the second dimension.</p>
<p>Time stamp feature.</p>
<p>To take time stamp information into consideration when determining the actions, we embed   into a 16dimensional representation   =  (  ; W  ) where  (•; W  ) is also a MLP layer with the shape of 1 × 16.Then repeat it for  times to match the shape of e   : e  = {  }  =1 .The decision vector dv ∈ R  × (64+16) is obtained as the concatenation of e   and e  : dv = Concat(e   , e  ).</p>
<p>4.2.4</p>
<p>Actor.Finally, the probabilities of selecting mutation operators  1 , probabilities of selecting crossover operators  2 , distribution of the parameters for mutation N ( 1 ,  1 ) and distribution of the parameters for crossover N ( 2 ,  2 ) for each individual are obtained through the MLP layers in the Actor separately as illustrated in the right of Figure 2.For instance, to select the mutation operator of the -th individual, its decision vector is mapped to the 14-dimensional probability vector corresponding to the 14 candidate mutation operators:  1, = Softmax( (dv  ; W  )) by the MLP  (•; W  ) with shape 80 × 32 × 14, and the index of the selected operator is sampled from Categorical( 1, ).The parameters of the selected mutation operator is sampled from the normal distribution  1  ∼ N ( (dv  ; W   ), Diag  (dv  ; W  )) where  (•; W   ) and  (•; W  ) are two MLP layers with the same shape of 80 × 32 × 3. It is worth noting that for parameter control, to uniformly control all operators which have diverse numbers of parameters, we pre-defined the maximum parameter numbers for mutation and crossover operators (in this paper, they are 3 and 2 respectively).The MLPs in Actor output 3 or 2 parameter for all operators.If the operator needs less parameters, the first few values would be used and the rest are ignored.</p>
<p>Experiment</p>
<p>In this section, we discuss the following research questions: RQ1: How does the proposed RLDE-AFL perform on synthetic problem instances?RQ2: Can RLDE-AFL zero-shot to synthetic problems with expensive evaluation costs or different dimensions, as well as realistic problems?RQ3: How does the design of feature extractor affect RLDE-AFL's performance?Below, we first introduce the experimental settings and then address RQ1∼RQ3 respectively.</p>
<p>Experimental Setup</p>
<p>Training setup.</p>
<p>The following experiments are based on the MetaBox Benchmark [33] which provides the synthetic CoCo-BBOB benchmark [12], the noisy-synthetic benchmark [12] and the Protein-Docking benchmark [19].In this paper we use 8 of the 24 problem instances with dimensions of 10 in the synthetic In this paper we adopt vanilla DE [52], advanced DE variants MadDE [3], JDE21 [6], NL-SHADE-LBC [50] and AMCDE [69] as traditional DE baselines.Besides, we include the random action RLDE-AFL without RL agent (denoted as "Random") to validate the effectiveness of RL training.For RL-based baselines, we adopt operator selection methods DE-DDQN [48], DE-DQN [54], parameter control methods LDE [53], GLEET [32], method that conducts both operator selection and parameter control RL-HPSDE [56] and algorithm selection method RL-DAS [10].The configurations of these baselines follow the setting in their original papers.RL-based baselines are trained on the same I train for the same number of learning steps as RLDE-AFL for fair comparisons.</p>
<p>Comparison on 10D Testing Set (RQ1)</p>
<p>In this section we compare the optimization performance of our proposed RLDE-AFL with the baselines to answer RQ1.We train RLDE-AFL and RL-based baselines on the 8 problem instance I train  (1) Our RLDE-AFL outperforms all traditional and RL-based baselines, achieving the state-of-the-art performance, which validates the effectiveness of the proposed method.(2) RLDE-AFL significantly surpasses the random action baseline which reveals the advantage of RL-based configuration policy and validates the effectiveness of the RL training.Besides, the random action baselines outperforms DE which reveals that integrating diverse operators would enhance the robustness of algorithm and lead to certain performance even using random operator selection and parameters.(3) Compared to traditional baselines, the RL based configuration policy in RLDE-AFL shows superior performance than human-crafted adaptive mechanisms, indicating that RL agents could not only relieve the expertise dependence in adaptive mechanism designs, but also present promising optimization performance.(4) Compared to RL-based baselines using human-crafted features, the superior performance of RLDE-AFL validates the effectiveness of the NeurELA based feature extraction.Besides, instead of exclusively selecting operators or controlling parameters, including both operator selection and parameter control for individuals in action space could minimize the expertise dependence, unleash the behavior diversity, and hence obtain better performance.
F15 3.171e0 ±5.202e-1 + 2.478e0 ±3.177e-1 + 2.343e0 ±3.829e-1 ≈ 2.415e0 ±3.670e-1 ≈ 3.630e0 ±5.854e-1 + 2.522e0 ±8.491e-1 ≈ 2.288e0 ±3.562e-1 ≈ 4.156e0 ±7.387e-1 + 2.430e0 ±3.896e-1 + 2.709e0 ±3.974e-1 + 2.112e0 ±4.176e-1 ≈ / 2.223e0 ±4.864e-1 F16 1.650e2 ±1.069e1 + 1.677e2 ±8.599e0 + 1.219e2 ±1.844e1 ≈ 1.244e2 ±1.091e1 + 2.262e2 ±2.286e1 + 1.923e2 ±3.426e1 + 1.336e2 ±1.358e1 + 4.757e2 ±4.756e1 + 1.305e2 ±8.811e0 + 2.006e2 ±1.658e1 + 1.278e2 ±1.276e1 + / 1.127e2 ±1.966e1 + / − / ≈</p>
<p>Zero-shot Generalization Performance (RQ2)</p>
<p>In this section, we conduct the zero-shot generalization to 20D F1∼F16 synthetic problems, expensive problems and realistic problems to answer RQ2.Specifically, we use the RLDE-AFL model trained with 10D synthetic training set I train to the aforementioned testing problem sets without further tuning.The RL-based baselines are also zero-shot generalized in the same way as RLDE-AFL.GLEET. Figure 3 presents the optimization curves of the baselines under 4 10D synthetic problem instances in MetaBox: Attractive Sector (F2), Composite Griewank-Rosenbrock (F12), Lunacek bi-Rastrigin (F16) and Gallagher 101Peaks (training problem).The results show that RLDE-AFL has faster convergence speed in expensive scenarios and hence surpasses the 4 baselines.The results infers the potential of applying the RLDE-AFL models pre-trained on non-expensive problem instances to solve expensive problems, saving training cost while obtaining promising performance.</p>
<p>5.3.3</p>
<p>Zero-shot to realistic problems.The experiments above are conducted on synthetic problems which are in the same distribution as synthetic training problem instances.To further validate the generalization ability of RLDE-AFL on out-of-distribution problems, we employ the Protein-Docking problems [19] provided in MetaBox benchmark which containing 280 12D protein-docking task instances.Due to the computationally expensive evaluations, the maximum number of function evaluations is set to 500.The default search range for the optimization is [−5, 5].We zero-shot RLDE-AFL and RL-based baselines DE-DDQN and GLEET trained on 10D synthetic problem instances to the protein-docking benchmark.The best objective value AEI [33,35] results between RLDE-AFL and the 4 best baselines MadDE, NL-SHADE-LBC, DE-DDQN and GLEET are presented in Figure 4, the details of the AEI metric is provided in Appendix C. On protein-docking problem instances, RLDE-AFL shows competitive performance with GLEET and surpasses DE-DDQN, MadDE and NL-SHADE-LBC, revealing the remarkable out-of-distribution generalization ability of RLDE-AFL.</p>
<p>Ablation Study (RQ3)</p>
<p>In order to verify the effectiveness of the neural network based feature extractor, we conduct the ablation studies on the state design and feature extractor.We first remove the time stamp feature in the state and use NeurELA features only (denoted as "w/o Time").Then we further ablate the mantissa-exponent representation and apply the min-max normalization adopted in original NeurELA (denoted  as "w/o ME") for evaluation value normalization.Next we replace the attention modules in NeurELA by simple MLPs to validate the necessity of the information sharing between solutions and dimensions (denoted as "MLP").Lastly we remove the whole NeurELA and instead use human-crafted optimization features proposed in GLEET as the states (denoted as "HandCraft").The boxplots of the accumulated rewards of RLDE-AFL and the ablated baselines on 10D, 20D, expensive and realistic testing problems are presented in Table 3.The RLDE-AFL variant without time stamp feature slightly underperforms RLDE-AFL.The RL agent informed with optimization progress information could accordingly adjust the configuration along the optimization and hence achieve better performance.The baseline "w/o ME" uses min-max normalization within the populations which fails in distinguishing different problem instances and different optimization progresses, leading to poor performance.It confirms the necessity of the mantissa-exponent representation which not only normalizes the scales but also provides the agent exact performance of the individuals thereby supporting purposeful configuration.The lower performance of the MLP baseline validates the significance of the information sharing between individuals and dimensions in comprehensive decision-making.The superior performance of RLDE-AFL over the baseline with hand-crafted features emphasizes the effectiveness of the neural network based feature extractor, which relieves the human effort burden while acquiring better performance.</p>
<p>Conclusion</p>
<p>This paper proposed a novel MetaBBO approach, RLDE-AFL, with RL-based policy, generalizable NeurELA based feature extractor and diverse mutation and crossover operators.By integrating the attention-based learnable feature extraction module with mantissaexponent based fitness representation to encode the population and evaluation values into expressive optimization states, we relieved the expertise dependency in feature design and enhanced the generalization ability.We formulated the optimization process as a MDP and incorporated a comprehensive algorithm configuration space including the integrated diverse DE operators into a RL-aided algorithm configuration paradigm.Experimental verified that the proposed RLDE-AFL not only showed promising optimization performance on in-distribution synthetic problems, but also presented robust generalization ability across problem dimensions and optimization horizons, as well as the out-of-distribution realistic BBO scenarios.In-depth analysis on state feature extraction further validated the effectiveness of the NeurELA module and the necessity of co-training the proposed feature learning module.ProDE-rand/1 [</p>
<p>B Training &amp; Testing Problem Set</p>
<p>MetaBox provides 24 synthetic problem instances with diverse characteristics and landscapes as listed in Table 5.</p>
<p>C AEI Metric</p>
<p>In Section 5.3.3 in the main paper, we present the best objective value AEI scores of the baselines for validating the zero-shot performance on realistic problems.To get the score, we test the target approach on  problem instances for  repeated runs and then record the basic</p>
<p>Figure 1 :
1
Figure 1: The overview of the bi-level structure in RLDE-AFL.</p>
<ol>
<li>
<p>1 . 1
11
State.As mentioned above, the state comprises the population solutions   = { , }  =1 and evaluation values   ∈ R  which indicates the optimization situation, and the time stamp  ∈ [1, ] indicating the optimization progress.Since the searching spaces of different optimization problems vary, we normalize the solution values with the upper and lower bounds of the searching space:  ′ , = {  ,,   −  }  =1</p>
</li>
<li>
<p>2 . 1
21
Embedding.As shown in Figure2, given the state , we first reorganize the population  and evaluation values  into the observation  ∈ R  × ×3 as mentioned in Section 4.1.1.Then  is embedded by a linear embedder h (0) =  (, W   ) where  (•, W   ) denotes a MLP layer with shape 3 × 64.</p>
</li>
</ol>
<p>X</p>
<p>Figure 2 :
2
Figure 2: Illustration of the network structure.</p>
<ol>
<li>
<p>2 . 2
22
NeurELA..The hidden representation h (0) ∈ R  × ×64 is next encoded by the two-stage NeurELA module.</p>
</li>
<li>
<p>2 . 5
25
Critic.For the critic   parameterized by  , we calculate the value of an individual as   (  ) =  (dv  ; W  ) using a MLP with the shape of 80 × 16 × 8 × 1 and ReLU activation functions.The value of the population is the averaged value per individual   () = 1   =1   (  ).</p>
</li>
</ol>
<p>benchmark as training problem set I train and the rest 16 problem instances as testing set (F1∼F16).The detailed problem formulation and train-test split are provided in Appendix B. We train the policy for 100 epochs with a learning rate of 1e-3 and Adam optimizer.The PPO process is conducted  = 3 steps for every  = 10 generation with discount factor  = 0.99.For the DE optimization, the population size  is set to 100, the maximum function evaluations is 20,000 thus the optimization horizon  = 200.The searching space of all problem instances are [−5, 5]  .All baselines in all experiments are run for 51 times.All experiments are run on Intel(R) Xeon(R) E5-2678 CPU and NVIDIA GeForce 1080Ti GPU with 32G RAM.5.1.2Baselines.MetaBox integrates a large number of classic and advanced DE algorithms.</p>
<ol>
<li>3 . 1 Figure 3 :
313
Figure 3: The optimization curves of RLDE-AFL and baselines on the four problems with 2,000 function evaluations.</li>
</ol>
<p>Figure 4 :
4
Figure 4: The AEI scores of RLDE-AFL and the baselines on the protein-docking realistic problem set.</p>
<p>Figure 5 and
5
Figure 6   presents the landscapes of the training and testing problem instances when dimensions are set to 2, respectively.We select  1 ,  2 ,  3 ,  5 ,  15 ,  16 ,  17 and  21 as training functions and the rest for testing to balance the optimization difficulty in training and testing problem sets.</p>
<p>[52] , )}  =1 ,   }, and the observation   for NeurELA module is changed to {{( ,, ,  , ,  , )} 2) Mutation operator 8∼11 are the variants of the basic operators: current-to-pbest/1, current-to-pbest/1 + archive, currentto-rand/1 + archive and weighted-rand-to-pbest/1.Mutation operator 14 TopoDE-rand/1[44]enhances the exploitation of rand/1 by using the nearest best individual to the current individual as   1 .(6)Crossoveroperator1∼2 are basic crossover operators proposed in vanilla DE[52]: binomial and exponential crossover.(7) Crossover operator 3 p-binomial crossover is a variant of binomial crossover which borrows the idea of "pbest" and replace the parent individual in the crossover with a randomly selected top individual.
They in-troduce the "pbest" technique which replace the best indi-vidual by randomly selected top individuals and employthe archive of the eliminated individuals to enhance theexploration.(3) Mutation operator 12 ProDE-rand/1 [8] is a variant of rand/1which selects random individuals according to the proba-bilities inversely proportional to distances between individ-uals.(4) Mutation operator 13 HARDDE-current-to-pbest/2 [36] im-proves the optimization performance by introducing timestamps to the archive in current-to-pbest/1 and samplingrandom individuals from the two archives containing recentindividuals and former individuals respectively.(5)𝑁 𝑖=1 } 𝐷 𝑗=1 .4.1.2 Action. For operator selection we integrate 14 mutation op-erators and 3 crossover operators form various DE variants.
[52]Mutation operators 1∼7 are basic mutation operators in vanilla DE[52]: rand/1, best/1, rand/2, best/2, current-torand/1.current-to-best/1 and rand-to-best/1, which have diverse preferences on exploration or exploitation.(In the action space for each individual we include two actions for operator selection  1 ∈ [1, 14] and  2 ∈ [1, 3].The detailed introduction of the operators are shown in Table I in Appendix A.</p>
<p>Algorithm 1: Pseudo Code of the training of RLDE-AFL Input: Policy   , Critic   , Training instance Set I train Output: Trained Policy   , Critic   for ℎ = 1 to ℎ do for  ∈ I train do Initialize population  1 and evaluation values  1 =  ( 1 ); for  = 1 to  do Obtain state   using   ,   and  ; Determine actions   =   (  ); Optimize   using DE with configurations   and obtain   +1 ,   +1 ; Calculate reward   following Eq.(4); Collect the transition &lt;   ,   ,   +1 ,   &gt;; if mod( , ) == 0 then for  = 1 to  do Update   and   by PPO method;
endendendendend</p>
<p>Table 1 :
1
The comparison results of the baselines on 10D testing problems.The policy   determines the action   including the selected mutation and crossover operators and their corresponding parameters according to the state.With these configurations the DE algorithm optimize the population for one generation and obtain the next state   +1 and reward   .The transition &lt;   ,   ,   +1 ,   &gt; is appended into a memory.For each  generations, the actor   and critic   are updated for  steps using PPO manner.
Traditional DE VariantsRL-based DE VariantsDEMadDEJDE21NL-SHADE-LBCAMCDERandomDE-DDQNDE-DQNLDERL-HPSDEGLEETRL-DASRLDE-AFLF14.584e1 ±6.640e0+2.614e1 ±5.528e0+3.051e1 ±1.415e1+1.653e1 ±4.289e0≈3.161e1 ±1.417e1+3.032e1 ±7.238e0+4.004e1 ±7.818e0+3.118e2 ±6.292e1+3.471e1 ±4.974e0+7.457e1 ±1.040e1+2.815e1 ±5.718e0+3.400e1 ±6.207e0+1.811e1 ±7.333e0F29.325e-1 ±3.203e-1+1.170e-2 ±6.815e-3+4.893e-2 ±2.101e-1+1.846e-1 ±1.371e-1+4.423e0 ±7.042e0+1.521e-2 ±4.300e-2+4.669e-1 ±3.250e0+4.678e4 ±2.687e4+1.290e-1 ±1.329e-1+1.204e1 ±4.692e0+4.251e-1 ±1.662e-1+3.743e-2 ±1.841e-2+2.525e-7 ±2.205e-7F31.575e0 ±3.954e-1+4.395e-1 ±2.559e-1≈7.377e-1 ±4.520e-1+2.993e-2 ±6.775e-2−2.218e0 ±8.504e-1+2.571e-1 ±1.616e-1≈5.655e-3 ±3.999e-2−1.807e2 ±6.459e1+3.401e-1 ±4.349e-1≈2.792e0 ±1.591e0+2.359e-2 ±2.660e-2≈6.781e-1 ±3.084e-1+3.719e-1 ±3.830e-1F48.196e0 ±4.098e0+5.044e0 ±4.435e-1+3.359e0 ±2.082e0+4.557e0 ±7.521e-1+1.365e1 ±2.285e1≈6.570e0 ±8.534e-1+8.239e-1 ±1.877e0−1.499e4 ±7.657e3+5.407e0 ±1.995e0+5.312e1 ±3.281e1+6.248e0 ±9.377e-1+5.417e0 ±9.221e-1+1.514e0 ±1.093e0F57.506e0 ±2.479e-1+3.254e0 ±6.338e-1+6.780e0 ±2.738e0+6.944e0 ±6.599e-1+8.237e0 ±7.712e0+7.372e0 ±4.360e-1+2.431e0 ±2.492e0≈1.258e4 ±5.998e3+7.088e0 ±9.246e-1+4.327e1 ±2.368e1+6.478e0 ±7.257e-1+4.363e0 ±8.598e-1+2.407e0 ±1.182e0F61.380e4 ±6.229e3+8.980e2 ±5.369e2+6.242e2 ±6.666e2+9.828e1 ±8.054e1−2.839e3 ±1.131e3+2.588e3 ±1.023e3+4.560e1 ±2.147e2−5.294e5 ±3.317e5+2.206e2 ±2.227e2≈2.621e3 ±2.232e3+1.723e1 ±1.643e1−1.387e3 ±8.063e2+1.530e2 ±1.444e2F71.344e2 ±2.775e1+2.949e1 ±1.125e1+1.596e1 ±8.448e0+8.617e0 ±4.953e0+1.543e2 ±3.598e1+2.977e1 ±9.567e0+7.803e0 ±6.936e0+1.007e3 ±1.446e3+8.445e0 ±5.847e0+2.593e1 ±2.523e1+1.525e-1 ±1.147e-1−3.484e1 ±9.974e0+3.414e0 ±2.532e0F81.369e3 ±1.196e3+8.770e1 ±6.079e1+8.144e0 ±9.372e0+2.285e0 ±2.077e0+7.474e3 ±4.546e3+5.914e6 ±1.772e6+1.211e4 ±6.156e4+2.924e7 ±9.112e6+3.627e0 ±2.712e0+1.992e5 ±1.578e5+2.942e1 ±1.950e1+7.466e1 ±6.990e1+1.240e0 ±1.575e0F91.380e1 ±3.017e0+2.358e0 ±8.985e-1+5.347e0 ±7.360e0≈1.023e-1 ±1.827e-1−1.561e1 ±4.849e0+4.783e0 ±1.832e0+1.042e-6 ±4.091e-7−9.133e2 ±1.664e2+5.508e-1 ±9.579e-1−7.408e1 ±2.781e1+1.254e0 ±5.199e-1≈3.124e0 ±1.405e0+2.908e0 ±3.724e0F103.599e-3 ±8.465e-4+8.888e-4 ±4.605e-4+4.771e-4 ±3.167e-4+7.050e-5 ±5.500e-5≈3.025e-3 ±1.414e-2−4.252e-4 ±2.705e-4+2.637e-8 ±1.276e-7−1.193e1 ±4.089e0+2.611e-4 ±1.704e-4+2.080e-1 ±1.373e-1+1.996e-4 ±1.118e-4+1.570e-3 ±4.325e-4+6.642e-5 ±3.046e-5F111.510e0 ±3.539e-1+1.435e0 ±4.560e-1+8.793e-1 ±7.375e-1+1.257e-1 ±7.574e-2+4.087e0 ±1.314e0+2.361e-2 ±3.254e-2−3.786e-3 ±1.742e-2−3.092e1 ±6.330e0+3.505e-1 ±2.517e-1+4.839e0 ±1.536e0+4.281e-1 ±2.208e-1+2.035e0 ±4.582e-1+7.239e-2 ±9.005e-2F122.946e0 ±4.763e-1+1.138e0 ±4.309e-1+2.501e0 ±4.347e-1+2.200e0 ±4.661e-1+1.795e0 ±1.308e0+2.279e0 ±4.367e-1+2.279e0 ±5.311e-1+1.209e1 ±2.158e0+2.069e0 ±3.717e-1+4.256e0 ±6.716e-1+2.108e0 ±4.858e-1+9.524e-1 ±2.673e-1+6.821e-1 ±6.788e-1F131.610e0 ±1.802e-1+8.774e-1 ±1.687e-1+4.500e-1 ±2.202e-1−2.477e-1 ±1.674e-1−1.514e0 ±2.063e-1+4.497e-1 ±2.208e-1−1.640e0 ±3.319e-1+7.034e3 ±2.884e3+1.491e0 ±1.696e-1+2.386e0 ±3.024e-1+1.091e0 ±2.146e-1+1.059e0 ±2.536e-1+6.569e-1 ±2.343e-1F143.638e0 1.000e0+5.860e-1 ±2.263e-1≈1.118e0 ±7.399e-1≈6.666e-1 ±1.251e-1≈1.705e0 ±8.843e-1≈2.631e0 ±2.232e0+8.157e-1 ±3.756e-1≈5.254e1 ±1.298e1+6.093e-1 ±1.936e-1≈2.964e0 ±1.844e0+6.805e-1 ±2.331e-1≈5.895e-1 ±2.205e-1≈2.439e0 ±2.302e0F151.837e0 ±3.562e-1+1.337e0 ±3.141e-1+1.414e0 ±2.419e-1+1.360e0 ±2.802e-1+1.930e0 ±2.836e-1+1.523e0 ±2.916e-1+1.313e0 ±1.999e-1+3.477e0 ±9.457e-1+1.374e0 ±2.926e-1+1.782e0 ±3.684e-1+1.252e0 ±2.708e-1+1.393e0 ±2.121e-1+1.105e0 ±4.564e-1F163.992e1 ±5.603e0+4.178e1 ±6.003e0+3.841e1 ±8.614e0+3.581e1 ±6.988e0+5.055e1 ±7.247e0+3.706e1 ±7.623e0+4.177e1 ±5.589e0+1.686e2 ±2.163e1+4.138e1 ±3.673e0+6.822e1 ±7.224e0+4.157e1 ±6.349e0+4.411e1 ±6.492e0+2.217e1 ±6.183e0+ / − / ≈16 / 0 / 013 / 0 / 313 / 1 / 29 / 4 / 313 / 1 / 213 / 2 / 18 / 6 / 216 / 0 / 012 / 1 / 316 / 0 / 011 / 2 / 315 / 0 / 1N/A4.3 Training
[46]his paper we use the -step PPO[46]to train the policy in RLDE-AFL.As illustrated in Algorithm 1, given a training problem instance set I train , for each epoch and each problem instance  ∈ I train , the DE algorithm first initializes a  ×  population and evaluate it using  .For each generation , the state   is collected as mentioned in Section 4.1.1.</p>
<p>Table 2 :
2
The comparison results of the baselines on 20D testing problems.RL-DAS fails to generalize to 20D problems and is marked as "/" due to its problem dimension-dependent feature design.
Traditional DE VariantsRL-based DE VariantsDEMadDEJDE21NL-SHADE-LBCAMCDERandomDE-DDQNDE-DQNLDERL-HPSDEGLEETRL-DAS RLDE-AFLF11.882e2 ±1.563e1+1.935e2 ±1.528e1+1.470e2 ±4.598e1+1.040e2 ±1.598e1+4.708e2 ±1.105e2+3.195e2 ±7.503e1+1.494e2 ±2.528e1+1.201e3 ±2.781e2+1.367e2 ±1.689e1+2.948e2 ±4.318e1+1.385e2 ±1.444e1+/5.309e1 ±1.559e1F26.017e1 ±9.446e0+4.013e1 ±4.998e0+1.320e1 ±1.426e1+1.188e1 ±4.803e0+4.051e2 ±1.661e2+1.012e2 ±6.252e1+1.077e1 ±1.382e1+1.265e5 ±5.919e4+6.539e0 ±3.717e0+1.929e2 ±3.054e1+1.816e1 ±4.907e0+/1.033e-1 ±1.319e-1F33.755e1 ±7.877e0+1.598e1 ±3.362e0+1.137e1 ±7.579e0+2.798e0 ±1.026e0−7.321e1 ±1.616e1+5.703e1 ±2.681e1+7.478e0 ±7.017e0≈5.881e2 ±1.345e2+7.896e0 ±2.971e0+4.479e1 ±2.070e1+3.839e0 ±1.451e0−/6.225e0 ±3.929e0F46.885e1 ±1.836e1+1.039e2 ±1.951e1+4.256e1 ±3.212e1+1.689e1 ±6.453e-1+2.059e3 ±2.578e3+1.599e2 ±8.383e1+6.275e1 ±4.567e1+1.335e5 ±3.857e4+5.084e1 ±3.004e1+1.069e3 ±6.218e2+2.701e1 ±1.516e1+/1.745e1 ±1.548e1F53.206e1 ±5.580e0+3.356e1 ±1.030e1+3.299e1 ±2.766e1+1.832e1 ±7.480e-1+8.553e3 ±6.270e3+1.161e2 ±1.796e1+1.966e1 ±5.498e0+7.805e4 ±2.019e4+1.854e1 ±6.756e0+6.568e2 ±4.247e2+1.875e1 ±1.097e0+/1.423e1 ±6.982e0F62.281e5 ±5.702e4+2.856e4 ±6.367e3+1.070e4 ±7.056e3+4.260e3 ±1.412e3≈2.006e5 ±5.102e4+3.678e4 ±2.996e4+7.251e2 ±3.789e3−1.937e6 ±6.158e5+4.689e3 ±1.767e3≈1.050e5 ±5.803e4+2.992e3 ±1.406e3−/4.700e3 ±1.913e3F72.863e2 ±3.913e1+7.694e1 ±1.265e1+6.426e1 ±1.998e1+3.377e1 ±9.034e0−3.440e2 ±4.995e1+9.865e1 ±3.796e1+3.162e1 ±9.994e0−1.682e3 ±2.411e3+4.300e1 ±9.238e0−1.268e2 ±4.995e1+1.054e1 ±5.289e0−/5.096e1 ±1.108e1F87.019e4 ±3.034e4+1.437e5 ±5.586e4+1.186e3 ±8.341e3≈1.550e2 ±3.021e2+9.262e6 ±2.599e6+1.302e6 ±4.642e6+5.609e6 ±1.118e7+1.682e8 ±4.063e7+7.870e0 ±1.292e1+5.778e6 ±2.675e6+6.007e3 ±2.486e3+/4.590e0 ±6.529e0F91.014e2 ±1.307e1+1.156e2 ±1.709e1+3.106e1 ±2.600e1≈2.385e1 ±5.755e0≈5.438e2 ±6.236e1+2.517e2 ±1.352e2+1.292e2 ±6.969e1+1.861e3 ±1.863e2+6.266e1 ±1.779e1+3.772e2 ±1.057e2+2.697e1 ±7.855e0≈/2.427e1 ±2.134e1F101.422e-1 ±3.063e-2+1.182e-1 ±3.807e-2+6.561e-3 ±5.119e-3+2.250e-3 ±9.769e-4+4.823e0 ±9.922e-1+4.444e-1 ±3.564e-1+3.585e-1 ±4.326e-1+3.394e1 ±7.914e0+5.238e-3 ±2.398e-3+3.211e0 ±1.500e0+1.634e-2 ±6.760e-3+/9.519e-4 ±2.523e-4F119.217e0 ±1.420e0+8.668e0 ±1.432e0+4.928e0 ±2.658e0+9.403e-1 ±4.131e-1+1.778e1 ±2.345e0+1.717e1 ±4.165e0+1.823e0 ±1.160e0+4.629e1 ±4.713e0+1.896e0 ±7.897e-1+1.558e1 ±2.574e0+3.284e0 ±1.015e0+/4.828e-1 ±4.071e-1F125.729e0 ±5.177e-1+3.804e0 ±4.603e-1≈4.888e0 ±6.351e-1+4.801e0 ±4.913e-1+7.365e0 ±6.496e-1+2.504e-1 ±9.903e-5−4.826e0 ±3.498e-1+2.052e1 ±2.492e0+4.339e0 ±3.452e-1+6.671e0 ±7.936e-1+4.461e0 ±6.467e-1+/3.301e0 ±1.362e0F132.667e0 ±1.361e-1+2.348e0 ±1.274e-1+9.153e-1 ±1.933e-1−1.435e0 ±1.930e-1+1.836e1 ±3.228e1+6.684e0 ±3.077e1+2.670e0 ±1.876e-1+3.529e4 ±1.060e4+2.296e0 ±1.037e-1+1.224e1 ±4.386e1+2.098e0 ±1.561e-1+/1.109e0 ±1.796e-1F141.712e1 ±8.110e0+8.914e0 ±3.003e-1+9.162e0 ±4.979e0+8.113e0 ±2.164e0+4.562e1 ±1.108e1+1.304e1 ±8.494e0+1.229e1 ±7.759e0+7.855e1 ±2.890e0+7.721e0 ±2.609e0+1.490e1 ±8.964e0+6.398e0 ±3.650e0+/1.043e1 ±6.426e0</p>
<p>Table 3 :
3
The averaged accumulated rewards of the ablated baselines and RLDE-AFL.
w/o Timew/o MEMLPHandCraft RLDE-AFL10D9.497e-19.442e-19.451e-19.512e-19.645e-1problems±1.231e-2±1.135e-2±1.419e-2±9.478e-3±6.577e-320D9.289e-019.207e-019.274e-019.317e-019.334e-01problems±1.500e-02±1.592e-02±1.414e-02±1.243e-02±1.295e-02Expensive8.836e-018.713e-018.847e-018.714e-018.856e-01problems±3.069e-02±2.950e-02±3.079e-02±2.767e-02±2.494e-02Realistic6.279e-015.664e-016.603e-016.565e-017.056e-01problems±1.475e-03±1.300e-03±1.535e-03±1.519e-03±1.608e-03</p>
<p>8]  = 1 +  • (2 − 3)A variant of rand/1, which takes the distances between individuals into consideration and assigns close individuals larger selection probabilities, enhancing the exploitation of rand/1.∈[0,1]13 HARDDE-current-to-pbest/2 [36]  =  +  • ( *  − ) + 1 • (1 − x2) + 1 • (1 − x3)A variant of current-to-pbest/1, replace the last random individual with a individual randomly selected from the union of the population and archive x2, and add a solution difference using the individual x3 randomly selected from the union of the population and former individual archive.=+  • (2 − 3) A variant of rand/1,use the best individual   in the nearest individuals to replace the first random individual in rand/1. If   &lt;  or  = ;   Otherwise. = 1, 2, . . .,  Basic crossover operator that randomly selects values from trail solution or parent solution and uses a random index  to ensure that the generated individual is different from the parent. If  ∈ {⟨⟩  , ⟨ + 1⟩  , ..., ⟨ +  − 1⟩  } ;   Otherwise. = 1, 2, . . .,  Basic crossover operator that randomly selects a random length segment of the parent and cover the parent values in this segment using trail solution values. = 1, 2, . . .,  A variant of binomial crossover which replaces the parent individual with the individual randomly selected from the top % individuals in the population  *  .
𝐹 ∈ [0, 1]𝐹1 ∈ [0, 1]𝑝 ∈ [0, 1]14TopoDE-rand/1 [44]𝐹 ∈ [0, 1]1binomial crossover𝑣 𝑗 =𝐶𝑟 ∈ [0, 1]2exponential crossover𝑣 𝑗 =𝐶𝑟 ∈ [0, 1]3p-binomial crossover [3]𝑣 𝐶𝑟 ∈ [0, 1] 𝑝 ∈ [0, 1]
 =  If  &lt;  or  = ; x  Otherwise.</p>
<p>Table 5 :
5
Overview of the BBOB testsuites.
Problem Functions𝑓1Sphere Function𝑓2Ellipsoidal FunctionSeparable functions𝑓3Rastrigin Function𝑓4Buche-Rastrigin Function𝑓5Linear SlopeFunctions with low or moderate conditioning𝑓6 𝑓7 𝑓8 𝑓9Attractive Sector Function Step Ellipsoidal Function Rosenbrock Function, original Rosenbrock Function, rotated𝑓10Ellipsoidal FunctionFunctions with𝑓11Discus Functionhigh conditioning𝑓12Bent Cigar Functionand unimodal𝑓13Sharp Ridge Function𝑓14Different Powers FunctionMulti-modal functions with adequate global structure𝑓15 𝑓16 𝑓17 𝑓18 𝑓19Rastrigin Function (non-separable counterpart of F3) Weierstrass Function Schaffers F7 Function Schaffers F7 Function, moderately ill-conditioned Composite Griewank-Rosenbrock Function F8F2Multi-modal functions with weak global structure𝑓20 𝑓21 𝑓22 𝑓23 𝑓24Schwefel Function Gallagher's Gaussian 101-me Peaks Function Gallagher's Gaussian 21-hi Peaks Function Katsuura Function Lunacek bi-Rastrigin FunctionDefault search range: [-5, 5] 𝐷
AcknowledgmentsThis work was supported in part by the National Natural Science Foundation of China No. 62276100, in part by the Guangdong Provincial Natural Science Foundation for Outstanding Youth Team Project No. 2024B1515040010, in part by the Guangdong Natural Science Funds for Distinguished Young Scholars No. 2022B1515020049, and in part by the TCL Young Scholars Program.A Operator CollectionIn this section we list the names, formulations, descriptions and configuration spaces of the integrated DE mutation and crossover operators in Table4.A variant of current-to-best/1, replace the best individual with the individual randomly selected from the top % individuals in the population  *  for better exploration.A variant of current-to-pbest/1, replace the last random individual with a individual randomly selected from the union of the population and archive x • to further enhance the exploration.A variant of current-to-rand/1, which also replaces the last random individual with a individual randomly selected from the union of the population and archive xA variant of rand-to-best/1, replace the best individual with the individual randomly selected from the top % individuals in the population  *  and add a new parameter to enhance the behaviour diversity.  .Then, Z-score normalization is applied:where  * and  * are calculated by using Random Search as a baseline.Finally, the best objective value AEI score is calculated by:where Z-scores are first aggregated, then subjected to an inverse logarithmic transformation, and subsequently averaged across the test problem instances.A higher AEI indicates better performance of the corresponding approach.
Layer normalization. Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E Hinton, 2016In NeurIPS</p>
<p>Improving differential evolution through Bayesian hyperparameter optimization. Subhodip Biswas, Debanjan Saha, Shuvodeep De, Adam D Cobb, Swagatam Das, Brian A Jalaian, IEEE Congress on Evolutionary Computation. 2021. 2021CEC</p>
<p>Improving differential evolution through Bayesian hyperparameter optimization. Subhodip Biswas, Debanjan Saha, Shuvodeep De, Adam D Cobb, Swagatam Das, Brian A Jalaian, CEC. 832-8402021</p>
<p>Self-adapting control parameters in differential evolution: A comparative study on numerical benchmark problems. Janez Brest, Sao Greiner, Borko Boskovic, Marjan Mernik, Viljem Zumer, IEEE Transactions on Evolutionary Computation. 1062006. 2006</p>
<p>Self-adaptive differential evolution algorithm with population size reduction for single objective bound-constrained optimization: Algorithm j21. Janez Brest, Mirjam Sepesy Maučec, Borko Bošković, IEEE Congress on Evolutionary Computation. 2021. 2021CEC</p>
<p>Self-adaptive differential evolution algorithm with population size reduction for single objective bound-constrained optimization: Algorithm j21. Janez Brest, Mirjam Sepesy Maučec, Borko Bošković, CEC. 2021</p>
<p>SYMBOL: Generating Flexible Black-Box Optimizers through Symbolic Equation Learning. Jiacheng Chen, Zeyuan Ma, Hongshu Guo, Yining Ma, Jie Zhang, Yue-Jiao Gong, ICLR. 2024</p>
<p>Enhancing differential evolution utilizing proximity-based mutation operators. Dimitris K Michael G Epitropakis, Nicos G Tasoulis, Pavlidis, Michael N Vassilis P Plagianakos, Vrahatis, IEEE Transactions on Evolutionary Computation. 152011. 2011</p>
<p>Reinforcement learning-based differential evolution for global optimization. Iztok Fister, Dušan Fister, Iztok FisterJr, Differential Evolution: From Theory to Practice. 2022</p>
<p>Deep Reinforcement Learning for Dynamic Algorithm Selection: A Proof-of-Principle Study on Differential Evolution. Hongshu Guo, Yining Ma, Zeyuan Ma, Jiacheng Chen, Xinglin Zhang, Zhiguang Cao, Jun Zhang, Yue-Jiao Gong, TSMC. 2024. 2024</p>
<p>Multi-strategy multi-objective differential evolutionary algorithm with reinforcement learning. Yupeng Han, Hu Peng, Changrong Mei, Lianglin Cao, Changshou Deng, Hui Wang, Zhijian Wu, KBS. 2771108012023. 2023</p>
<p>Realparameter black-box optimization benchmarking 2009: Noiseless functions definitions. Nikolaus Hansen, Steffen Finck, Raymond Ros, Anne Auger, 2009Ph. D. Dissertation. INRIA</p>
<p>Adaptation in natural and artificial systems: an introductory analysis with applications to biology, control, and artificial intelligence. H John, Holland, 1992MIT press</p>
<p>Constrained evolutionary optimization based on reinforcement learning using the objective function and constraints. Zhenzhen Hu, Wenyin Gong, KBS. 2371077312022. 2022</p>
<p>Reinforcement learning-based differential evolution for parameters extraction of photovoltaic models. Zhenzhen Hu, Wenyin Gong, Shuijia Li, Energy Rep. 72021. 2021</p>
<p>Deep reinforcement learning assisted co-evolutionary differential evolution for constrained optimization. Zhenzhen Hu, Wenyin Gong, Witold Pedrycz, Yanchi Li, Swarm Evol. Comput. 831013872023. 2023</p>
<p>A fitness landscape ruggedness multiobjective differential evolution algorithm with a reinforcement learning strategy. Ying Huang, Wei Li, Furong Tian, Xiang Meng, Appl. Soft Comput. 961066932020. 2020</p>
<p>Q-Learning-based parameter control in differential evolution for structural optimization. Thanh N Huynh, Jaehong Dieu Tt Do, Lee, Appl. Soft Comput. 1071074642021. 2021</p>
<p>Proteinprotein docking benchmark version 4.0. Howook Hwang, Thom Vreven, Joël Janin, Zhiping Weng, Proteins: Structure, Function, and Bioinformatics. 782010. 2010</p>
<p>An adaptive differential evolution algorithm with novel mutation and crossover strategies for global numerical optimization. Minhazul Sk, Swagatam Islam, Saurav Das, Subhrajit Ghosh, Ponnuthurai Roy, Nagaratnam Suganthan, TSMC. 422011. 2011</p>
<p>Particle swarm optimization. James Kennedy, Russell Eberhart, In ICNN. 41995IEEE</p>
<p>Problem definitions and evaluation criteria for the CEC 2022 special session and competition on single objective bound constrained numerical optimization. Abhishek Kumar, K V Price, Ali Wagdy Mohamed, Anas A Hadi, P N Suganthan, Tech. Rep2022SingaporeNanyang Technological University</p>
<p>Xiaobin Li, Kai Wu, Yujian Betterest Li, Xiaoyu Zhang, Handing Wang, Jing Liu, arXiv:2405.03728GLHF: General Learned Evolutionary Algorithm Via Hyper Functions. 2024. 2024arXiv preprint</p>
<p>Pretrained Optimization Model for Zero-Shot Black Box Optimization. Xiaobin Li, Kai Wu, Yujian Betterest Li, Xiaoyu Zhang, Handing Wang, Jing Liu, The Thirty-eighth Annual Conference on Neural Information Processing Systems. 2024</p>
<p>B2Opt: Learning to Optimize Black-box Optimization with Little Budget. Xiaobin Li, Kai Wu, Xiaoyu Zhang, Handing Wang, The 39th Annual AAAI Conference on Artificial Intelligence. 2024</p>
<p>Enhancing differential evolution algorithm using leader-adjoint populations. Yuzhen Li, Shihao Wang, Hongyu Yang, Hu Chen, Bo Yang, Information Sciences. 6222023. 2023</p>
<p>Differential evolution based on reinforcement learning with fitness ranking for solving multimodal multiobjective problems. Zhihui Li, Li Shi, Caitong Yue, Zhigang Shang, Boyang Qu, Swarm Evol. Comput. 492019. 2019</p>
<p>Rlemmo: Evolutionary multimodal optimization assisted by deep reinforcement learning. Hongqiao Lian, Zeyuan Ma, Hongshu Guo, Ting Huang, Yue-Jiao Gong, GECCO. 2024</p>
<p>Differential evolution based on strategy adaptation and deep reinforcement learning for multimodal optimization problems. Zuowen Liao, Qishuo Pang, Qiong Gu, Swarm Evol. Comput. 871015682024. 2024</p>
<p>Learning to learn evolutionary algorithm: A learnable differential evolution. Xin Liu, Jianyong Sun, Qingfu Zhang, Zhenkun Wang, Zongben Xu, TETCI. 72023. 2023</p>
<p>Zeyuan Ma, Jiacheng Chen, Hongshu Guo, Yue-Jiao Gong, arXiv:2408.10672Neural exploratory landscape analysis. 2024. 2024arXiv preprint</p>
<p>Auto-configuring Exploration-Exploitation Tradeoff in Evolutionary Computation via Deep Reinforcement Learning. Zeyuan Ma, Jiacheng Chen, Hongshu Guo, Yining Ma, Yue-Jiao Gong, GECCO. 1497-15052024</p>
<p>MetaBox: A Benchmark Platform for Meta-Black-Box Optimization with Reinforcement Learning. Zeyuan Ma, Hongshu Guo, Jiacheng Chen, Zhenrui Li, Guojun Peng, Yue-Jiao Gong, Yining Ma, Zhiguang Cao, In NeurIPS. 362023</p>
<p>LLaMoCo: Instruction Tuning of Large Language Models for Optimization Code Generation. Zeyuan Ma, Hongshu Guo, Jiacheng Chen, Guojun Peng, Zhiguang Cao, Yining Ma, Yue-Jiao Gong, arXiv:2403.011312024. 2024arXiv preprint</p>
<p>Toward Automated Algorithm Design: A Survey and Practical Guide to Meta. Zeyuan Ma, Hongshu Guo, Yue-Jiao Gong, Jun Zhang, Kay Chen, Tan , arXiv:2411.006252024. 2024Black-Box-Optimization. arXiv preprint</p>
<p>HARD-DE: Hierarchical archive based mutation strategy with depth information of evolution for the enhancement of differential evolution on numerical optimization. Zhenyu Meng, Jeng-Shyang Pan, IEEE Access. 72019. 2019</p>
<p>Playing atari with deep reinforcement learning. Volodymyr Mnih, 2013. 2013arXiv preprint</p>
<p>Problem definitions and evaluation criteria for the CEC 2021 Special Session and Competition on Single Objective Bound Constrained Numerical Optimization. Ali Wagdy, Mohamed , Anas A Hadi, Ali Khater Mohamed, Prachi Agrawal, Abhishek Kumar, P N Suganthan, 2021Technical Report</p>
<p>Learning from Offline and Online Experiences: A Hybrid Adaptive Operator Selection Framework. Jiyuan Pei, Jialin Liu, Yi Mei, GECCO. 2024</p>
<p>Reinforcement learning-based hybrid differential evolution for global optimization of interplanetary trajectory design. Lei Peng, Zhuoming Yuan, Guangming Dai, Maocai Wang, Zhe Tang, Swarm Evol. Comput. 811013512023. 2023</p>
<p>Towards feature-free automated algorithm selection for singleobjective continuous black-box optimization. Raphael Patrick Prager, Vinzent Moritz, Heike Seiler, Pascal Trautmann, Kerschke, Symposium Series on Computational Intelligence (SSCI). IEEE. 2021. 2021</p>
<p>Evaluation criteria for CEC 2024 competition and special session on numerical optimization considering accuracy and speed. Kangjia Qiao, Xupeng Wen, Xuanxuan Ban, Peng Chen, Kenneth V Price, N Ponnuthurai, Jing Suganthan, Guohua Liang, Caitong Wu, Yue, 2024Technical Report</p>
<p>Self-adaptive differential evolution algorithm for numerical optimization. Kai Qin, Ponnuthurai N Suganthan, IEEE Congress on Evolutionary Computation. 22005. 2005IEEE</p>
<p>Differential evolution with topographical mutation applied to nuclear reactor core design. F Wagner, Nélio Sacco, Henderson, Progress in Nuclear Energy. 702014. 2014</p>
<p>Evolutionary framework with reinforcement learning-based mutation adaptation. Karam M Sallam, Ripon K Saber M Elsayed, Michael J Chakrabortty, Ryan, IEEE Access. 82020. 2020</p>
<p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, arXiv:1707.06347Proximal policy optimization algorithms. 2017. 2017arXiv preprint</p>
<p>Deep-ela: Deep exploratory landscape analysis with self-supervised pretrained transformers for single-and multi-objective continuous optimization problems. Vinzent Moritz, Pascal Seiler, Heike Kerschke, Trautmann, Evolutionary Computation. 2025. 2025</p>
<p>Deep reinforcement learning based parameter control in differential evolution. Mudita Sharma, Alexandros Komninos, Manuel López-Ibáñez, Dimitar Kazakov, GECCO. 709-7172019</p>
<p>NL-SHADE-LBC algorithm with linear parameter adaptation bias change for CEC 2022 Numerical Optimization. Vladimir Stanovov, Shakhnaz Akhmedova, Eugene Semenkin, IEEE Congress on Evolutionary Computation (CEC). 2022. 2022IEEE</p>
<p>NL-SHADE-LBC algorithm with linear parameter adaptation bias change for CEC 2022 Numerical Optimization. Vladimir Stanovov, Shakhnaz Akhmedova, Eugene Semenkin, CEC. 01-082022</p>
<p>Success Rate-based Adaptive Differential Evolution L-SRTDE for CEC 2024 Competition. Vladimir Stanovov, Eugene Semenkin, IEEE Congress on Evolutionary Computation (CEC). IEEE. 2024. 2024</p>
<p>Differential evolution-a simple and efficient heuristic for global optimization over continuous spaces. Rainer Storn, Kenneth Price, J. Glob. Optim. 111997. 1997</p>
<p>Learning adaptive differential evolution algorithm from optimization experiences by policy gradient. Jianyong Sun, Xin Liu, Thomas Bäck, Zongben Xu, TEC. 252021. 2021</p>
<p>Differential evolution with mixed mutation strategy based on deep reinforcement learning. Zhiping Tan, Kangshun Li, Appl. Soft Comput. 1111076782021. 2021</p>
<p>Differential evolution with hybrid parameters and mutation strategies based on reinforcement learning. Zhiping Tan, Yu Tang, Kangshun Li, Huasheng Huang, Shaoming Luo, Swarm Evol. Comput. 751011942022. 2022</p>
<p>Differential evolution with hybrid parameters and mutation strategies based on reinforcement learning. Zhiping Tan, Yu Tang, Kangshun Li, Huasheng Huang, Shaoming Luo, Swarm Evol. Comput. 751011942022. 2022</p>
<p>Success-history based parameter adaptation for differential evolution. Ryoji Tanabe, Alex Fukunaga, IEEE Congress on Evolutionary Computation. 2013. 2013</p>
<p>Deep reinforcement learning with double q-learning. Hado Van Hasselt, Arthur Guez, David Silver, AAAI. 2016</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, Advances in Neural Information Processing Systems. 2017. 2017</p>
<p>Differential evolution with composite trial vector generation strategies and control parameters. Yong Wang, Zixing Cai, Qingfu Zhang, IEEE transactions on evolutionary computation. 152011. 2011</p>
<p>Q-learning. Jch Christopher, Peter Watkins, Dayan, Mach. Learn. 81992. 1992</p>
<p>No free lunch theorems for search. William G David H Wolpert, Macready, 1995CiteseerTechnical Report</p>
<p>The roles of mutation, inbreeding, crossbreeding, and selection in evolution. Sewall Wright, 1932. 1932</p>
<p>DECN: Evolution inspired deep convolution network for black-box optimization. Kai Wu, Xiaobin Li, Penghui Liu, Jing Liu, arXiv:2304.095992023. 2023arXiv preprint</p>
<p>Learning to Transfer for Evolutionary Multitasking. Sheng-Hao Wu, Yuxiao Huang, Xingyu Wu, Liang Feng, Zhi-Hui Zhan, Kay Chen, Tan , 2024. 2024arXiv preprint</p>
<p>A reinforcement-learning-based evolutionary algorithm using solution space clustering for multimodal optimization problems. Hai Xia, Changhe Li, Sanyou Zeng, Qingshan Tan, Junchen Wang, Shengxiang Yang, 2021In CEC. 1938-1945</p>
<p>Dynamic multi-strategy integrated differential evolution algorithm based on reinforcement learning for optimization problems. Qingyong Yang, Shu-Chuan Chu, Jeng-Shyang Pan, Jyh-Horng Chou, Junzo Watada, Complex Intell. Syst. 2023. 2023</p>
<p>Meta-Black-Box Optimization for Evolutionary Algorithms: Review and Perspective. Xu Yang, Rui Wang, Kaiwen Li, Available at SSRN. 49569562024. 2024</p>
<p>Differential evolution with alternation between steady monopoly and transient competition of mutation strategies. Chenxi Ye, Chengjun Li, Yang Li, Yufei Sun, Wenxuan Yang, Mingyuan Bai, Xuanyu Zhu, Jinghan Hu, Tingzi Chi, Hongbo Zhu, Swarm and Evolutionary Computation. 831014032023. 2023</p>
<p>Reinforcement learning-based differential evolution algorithm for constrained multi-objective optimization problems. Xiaobing Yu, Pingping Xu, Feng Wang, Xuming Wang, EAAI. 1311078172024. 2024</p>
<p>A Gradient-based Method for Differential Evolution Parameter Control by Smoothing. Haotian Zhang, Jialong Shi, Jianyong Sun, Ali Wagdy Mohamed, Zongben Xu, GECCO. 2024</p>
<p>Learning to select the recombination operator for derivative-free optimization. Haotian Zhang, Jianyong Sun, Thomas Bäck, Zongben Xu, Sci. China Math. 2024. 2024</p>
<p>Haotian Zhang, Jianyong Sun, Thomas Bäck, Qingfu Zhang, Zongben Xu, Controlling Sequential Hybrid Evolutionary Algorithm by Q-Learning. 2023. 202318Research Frontier. Research Frontier</p>
<p>JADE: adaptive differential evolution with optional external archive. Jingqiao Zhang, Arthur C Sanderson, IEEE Transactions on Evolutionary Computation. 132009. 2009</p>
<p>Qi Zhao, Tengfei Liu, Bai Yan, Qiqi Duan, Jian Yang, Yuhui Shi, Automated Metaheuristic Algorithm Design with Autoregressive Learning. 2024. 2024arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
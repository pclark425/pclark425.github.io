<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4749 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4749</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4749</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-105.html">extraction-schema-105</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <p><strong>Paper ID:</strong> paper-262465259</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2309.13339v4.pdf" target="_blank">Enhancing Zero-Shot Chain-of-Thought Reasoning in Large Language Models through Logic</a></p>
                <p><strong>Paper Abstract:</strong> Recent advancements in large language models have showcased their remarkable generalizability across various domains. However, their reasoning abilities still have significant room for improvement, especially when confronted with scenarios requiring multi-step reasoning. Although large language models possess extensive knowledge, their reasoning often fails to effectively utilize this knowledge to establish a coherent thinking paradigm. These models sometimes show hallucinations as their reasoning procedures are unconstrained by logical principles. Aiming at improving the zero-shot chain-of-thought reasoning ability of large language models, we propose LoT (Logical Thoughts), a self-improvement prompting framework that leverages principles rooted in symbolic logic, particularly Reductio ad Absurdum, to systematically verify and rectify the reasoning processes step by step. Experimental evaluations conducted on language tasks in diverse domains, including arithmetic, commonsense, symbolic, causal inference, and social problems, demonstrate the efficacy of enhanced reasoning by logic. The implementation code for LoT can be accessed at: https://github.com/xf-zhao/LoT.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4749.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4749.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zero-shot-CoT (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-shot Chain-of-Thought prompting (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The standard zero-shot chain-of-thought prompting that elicits a single step-by-step reasoning trace by prompting the model with 'Let's think step by step.'; used as the baseline method in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large Language Models are Zero-Shot Reasoners</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI chat model (GPT-3.5 family) used as a backend LLM in experiments with temperature=0.1 and max_tokens=2048.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Zero-shot Chain-of-Thought (zero-shot-CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Single chain-of-thought generated per question using a fixed zero-shot prompt ('Let's think step by step.'). Similar reasoning in that one trace is produced (no ensembling/diverse sampling).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K / AQuA / Date (examples reported)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Reasoning benchmarks: GSM8K (grade-school math), AQuA (advanced math multiple-choice), Date Understanding (commonsense+date arithmetic).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>GSM8K accuracy 78.75%; AQuA 57.09%; Date 51.26% (as reported in Table 4 for GPT-3.5-turbo, CoT baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>LoT (Adpt-LoT) on same tasks: GSM8K 80.15%; AQuA 60.63%; Date 52.37% (Table 4, GPT-3.5-turbo).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The paper uses zero-shot-CoT as the baseline and shows that adding logical verification/revision (LoT) can improve accuracy modestly on challenging tasks (notably math benchmarks) when comparing single-chain CoT to LoT-enhanced methods.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>On some tasks and models, naive double-check ('Self-Check') or simple comparison-based methods (Cmps-LoT) do not outperform CoT consistently; CoT occasionally matches or slightly outperforms ablations (see Table 4 where Self-Check can be worse on some tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enhancing Zero-Shot Chain-of-Thought Reasoning in Large Language Models through Logic', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4749.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4749.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Adpt-LoT (LoT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adaptive Logical Thoughts (Adpt-LoT) prompting framework</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A think-verify-revise zero-shot prompting framework that uses reductio ad absurdum: for each step it generates both a post-hoc explanation for the step and for its negation, has the model choose between them, and revises failing steps until all steps pass verification.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI chat model (GPT-3.5 family) used as backend in the ablation and LoT comparisons (temperature=0.1).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Adpt-LoT (think-verify-revise with opposing post-hoc explanations)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Generates a candidate chain-of-thought, then for each intermediate step generates two opposing post-hoc explanations (E_i for T_i and E¬_i for ¬T_i), prompts the model to choose which view to adopt, and if the step is rejected, revises that step and re-generates the remainder. Diversity arises from explicitly producing and comparing alternative (opposing) reasoning paths/views rather than relying on a single trace.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K / AQuA / Date</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same benchmarks as above used to evaluate zero-shot reasoning and error detection.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>GSM8K accuracy 80.15%; AQuA 60.63%; Date 52.37% (Table 4, GPT-3.5-turbo with Adpt-LoT).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>CoT (single-trace) performance: GSM8K 78.75%; AQuA 57.09%; Date 51.26% (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Producing explicit opposing explanations and forcing the model to pick one (Adpt-LoT) improved error detection and final accuracy over the single-chain zero-shot-CoT baseline, especially on harder tasks (math). The paper argues that embracing one of two opposing viewpoints (T,E vs ¬T,E¬) is more effective for verification than asking the model to 'self-check' directly.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Adpt-LoT requires stronger base models to be reliable; smaller models can be confused by the additional prompts (the paper reports that larger models benefit more and that small models like Vicuna-7b can worsen in some cases). Also in some tasks (e.g., particular commonsense items) LoT and CoT can produce the same final answer despite revisions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enhancing Zero-Shot Chain-of-Thought Reasoning in Large Language Models through Logic', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4749.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4749.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Cmps-LoT (comparison-only LoT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Comparison-based LoT (Cmps-LoT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simpler LoT ablation that composes a negation explanation and uses the LLM to judge whether the composed contradiction is false, then revises steps if judged false.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-3.5-turbo used as backend for ablation experiments; same experimental config as other ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Cmps-LoT</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>other</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>For each step T_i the model produces a post-hoc negation explanation E¬_i and composes C' = premises ∧ ... ∧ ¬T_i ∧ E¬_i; the model is asked whether C' is contradictory; if judged false, it revises T_i. This is less symmetric/diverse than Adpt-LoT because it does not generate both supportive and opposing post-hoc explanations to be chosen between.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K / AQuA / Date</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Benchmarks used in ablation comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>GSM8K 77.67%; AQuA 57.48%; Date 52.37% (Table 4, Cmps-LoT with GPT-3.5-turbo).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Adpt-LoT: GSM8K 80.15%; AQuA 60.63%; Date 52.37% ; CoT: GSM8K 78.75%; AQuA 57.09%; Date 51.26% (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cmps-LoT provides some improvement over CoT on certain tasks (e.g., small gains on AQuA/Date) but generally underperforms the full Adpt-LoT; the paper finds that producing and contrasting both T- and ¬T-centered explanations (Adpt-LoT) yields better verification than composing a single negation check.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Cmps-LoT is less effective than Adpt-LoT on difficult tasks (Adpt-LoT achieves larger gains), and simple 'Self-Check' (no logical hints) can sometimes be competitive on particular tasks but not robustly better.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enhancing Zero-Shot Chain-of-Thought Reasoning in Large Language Models through Logic', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4749.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4749.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Check (ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Check (double-check without logical hints)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ablation where the LLM is prompted to 'double check the step' without the logical guidance of reductio ad absurdum or generating opposing explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-3.5-turbo used for ablation comparison (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Self-Check</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>The model is asked to 'double check' each reasoning step directly (no explicit generation of opposing viewpoints or post-hoc paired explanations); a straightforward self-verification prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K / AQuA / Date</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same benchmarks as other ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>GSM8K 76.15%; AQuA 56.19%; Date 51.57% (Table 4, Self-Check with GPT-3.5-turbo).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Adpt-LoT: GSM8K 80.15%; Cmps-LoT: GSM8K 77.67%; CoT: GSM8K 78.75% (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Self-Check without logical structure or the creation of opposing reviews is less effective for robust error detection and overall performance, particularly on math reasoning where Adpt-LoT shows clearer gains. The paper argues that logical hints and contrasting opposing viewpoints are important for detectable improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Self-Check sometimes performs similarly to CoT or slightly worse — it does not consistently improve performance and can be inferior to both Cmps-LoT and Adpt-LoT on harder problems.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enhancing Zero-Shot Chain-of-Thought Reasoning in Large Language Models through Logic', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4749.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4749.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Ensemble / diverse-sampling (related work mention)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ensembling via diverse prompts or multiple samples (self-consistency / majority voting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior approaches generate multiple reasoning traces via sampling or diverse prompts and aggregate them (e.g., majority voting or self-consistency) to improve robustness; mentioned as effective but costly and sometimes limited.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-Consistency Improves Chain of Thought Reasoning in Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>various (general prior methods)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Methods typically applied to LLMs by sampling multiple chains or using diverse few-shot exemplars; not a single model.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Ensembling / Diverse sampling / Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Produce N different reasoning paths either by repeated stochastic sampling with the same prompt (self-consistency) or by creating diverse prompts/exemplars, then aggregate answers via majority voting or other ensembling. Diversity comes from sampling randomness or prompt variation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>General reasoning benchmarks (referenced prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Used widely in prior work to boost accuracy on multi-step reasoning by leveraging multiple candidate chains.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>No single set of performance numbers in this paper; the paper states ensemble/diverse-sampling approaches have been widely combined with other techniques for higher accuracy (cites self-consistency and related works) but are costly (N-times sampling) and can be limited by majority-vote strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The paper contrasts LoT's guided revision (which produces a small set of targeted alternative viewpoints and revises only when necessary) with broad ensembling, arguing that while diversity via ensembling helps, it is costly and not guided; LoT aims to be more efficient by generating and adjudicating specific alternative explanations per step.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Paper notes ensemble/majority voting can be limited as it is unguided and may not perform deep logical error detection; no new experimental comparison numbers in this paper comparing ensemble methods directly to LoT.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enhancing Zero-Shot Chain-of-Thought Reasoning in Large Language Models through Logic', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4749.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4749.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multi-model size effect</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of model scale on similar vs diverse reasoning methods</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Aggregate observation across experiments that larger, more capable LLMs benefit more consistently from LoT's verification-and-revision than smaller models, which can be confused by extra intervention.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Vicuna-7b / Vicuna-13b / Vicuna-33b / GPT-3.5-turbo / GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Range of models evaluated in the paper: Vicuna variants (7B, 13B, 33B) — LLaMA-based open-source chat models — and closed models GPT-3.5-turbo and GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>LoT vs CoT (cross-model comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse (LoT) vs similar (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Same methods as above applied across model sizes: CoT single-chain baseline vs LoT adaptive revision producing opposing explanations. The paper reports revision frequency and final chain length statistics per model/task to analyze behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multiple (GSM8K, AQuA, Date, SocialQA, CauseEffect, ShuffledObjects, LastLetter, OddOneOut)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Wide set of benchmarks spanning math, commonsense, causal, symbolic, and social reasoning used to study model-size effects and LoT impact.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Aggregate statement: LoT enhances CoT across domains, with benefits more consistent for models >7B and becoming more prominent as model ability increases (e.g., GPT-4 shows robust self-correction improvements). Specific per-model deltas are reported in Table 1 (paper).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>CoT baseline performances are reported per model/task in Table 1; LoT usually improves or keeps performance stable, with larger models showing larger gains.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Larger models (GPT-4, GPT-3.5) revise more often and derive more benefit from LoT; smaller models (e.g., Vicuna-7B) are more likely to be harmed or confused by extra interventions. LoT-refined chains tend to be slightly shorter than original CoT chains, and revision frequency correlates with model capability and task difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>For small models such as Vicuna-7b, LoT can sometimes worsen performance (the paper reports non-negligible worsening rates for small models); also some tasks show no improvement or mixed effects depending on model size and task.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enhancing Zero-Shot Chain-of-Thought Reasoning in Large Language Models through Logic', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-Consistency Improves Chain of Thought Reasoning in Language Models <em>(Rating: 2)</em></li>
                <li>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models <em>(Rating: 2)</em></li>
                <li>Large Language Models are Zero-Shot Reasoners <em>(Rating: 2)</em></li>
                <li>Self-refine: Iterative refinement with self-feedback <em>(Rating: 1)</em></li>
                <li>Tree of thoughts: Deliberate problem solving with large language models <em>(Rating: 1)</em></li>
                <li>Least-to-most prompting enables complex reasoning in large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4749",
    "paper_id": "paper-262465259",
    "extraction_schema_id": "extraction-schema-105",
    "extracted_data": [
        {
            "name_short": "Zero-shot-CoT (baseline)",
            "name_full": "Zero-shot Chain-of-Thought prompting (baseline)",
            "brief_description": "The standard zero-shot chain-of-thought prompting that elicits a single step-by-step reasoning trace by prompting the model with 'Let's think step by step.'; used as the baseline method in this paper.",
            "citation_title": "Large Language Models are Zero-Shot Reasoners",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo",
            "model_description": "OpenAI chat model (GPT-3.5 family) used as a backend LLM in experiments with temperature=0.1 and max_tokens=2048.",
            "reasoning_method_name": "Zero-shot Chain-of-Thought (zero-shot-CoT)",
            "reasoning_method_type": "similar",
            "reasoning_method_description": "Single chain-of-thought generated per question using a fixed zero-shot prompt ('Let's think step by step.'). Similar reasoning in that one trace is produced (no ensembling/diverse sampling).",
            "task_name": "GSM8K / AQuA / Date (examples reported)",
            "task_description": "Reasoning benchmarks: GSM8K (grade-school math), AQuA (advanced math multiple-choice), Date Understanding (commonsense+date arithmetic).",
            "performance": "GSM8K accuracy 78.75%; AQuA 57.09%; Date 51.26% (as reported in Table 4 for GPT-3.5-turbo, CoT baseline).",
            "comparison_with_other_method": true,
            "performance_other_method": "LoT (Adpt-LoT) on same tasks: GSM8K 80.15%; AQuA 60.63%; Date 52.37% (Table 4, GPT-3.5-turbo).",
            "key_findings": "The paper uses zero-shot-CoT as the baseline and shows that adding logical verification/revision (LoT) can improve accuracy modestly on challenging tasks (notably math benchmarks) when comparing single-chain CoT to LoT-enhanced methods.",
            "counter_examples_or_negative_results": "On some tasks and models, naive double-check ('Self-Check') or simple comparison-based methods (Cmps-LoT) do not outperform CoT consistently; CoT occasionally matches or slightly outperforms ablations (see Table 4 where Self-Check can be worse on some tasks).",
            "uuid": "e4749.0",
            "source_info": {
                "paper_title": "Enhancing Zero-Shot Chain-of-Thought Reasoning in Large Language Models through Logic",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Adpt-LoT (LoT)",
            "name_full": "Adaptive Logical Thoughts (Adpt-LoT) prompting framework",
            "brief_description": "A think-verify-revise zero-shot prompting framework that uses reductio ad absurdum: for each step it generates both a post-hoc explanation for the step and for its negation, has the model choose between them, and revises failing steps until all steps pass verification.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo",
            "model_description": "OpenAI chat model (GPT-3.5 family) used as backend in the ablation and LoT comparisons (temperature=0.1).",
            "reasoning_method_name": "Adpt-LoT (think-verify-revise with opposing post-hoc explanations)",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "Generates a candidate chain-of-thought, then for each intermediate step generates two opposing post-hoc explanations (E_i for T_i and E¬_i for ¬T_i), prompts the model to choose which view to adopt, and if the step is rejected, revises that step and re-generates the remainder. Diversity arises from explicitly producing and comparing alternative (opposing) reasoning paths/views rather than relying on a single trace.",
            "task_name": "GSM8K / AQuA / Date",
            "task_description": "Same benchmarks as above used to evaluate zero-shot reasoning and error detection.",
            "performance": "GSM8K accuracy 80.15%; AQuA 60.63%; Date 52.37% (Table 4, GPT-3.5-turbo with Adpt-LoT).",
            "comparison_with_other_method": true,
            "performance_other_method": "CoT (single-trace) performance: GSM8K 78.75%; AQuA 57.09%; Date 51.26% (Table 4).",
            "key_findings": "Producing explicit opposing explanations and forcing the model to pick one (Adpt-LoT) improved error detection and final accuracy over the single-chain zero-shot-CoT baseline, especially on harder tasks (math). The paper argues that embracing one of two opposing viewpoints (T,E vs ¬T,E¬) is more effective for verification than asking the model to 'self-check' directly.",
            "counter_examples_or_negative_results": "Adpt-LoT requires stronger base models to be reliable; smaller models can be confused by the additional prompts (the paper reports that larger models benefit more and that small models like Vicuna-7b can worsen in some cases). Also in some tasks (e.g., particular commonsense items) LoT and CoT can produce the same final answer despite revisions.",
            "uuid": "e4749.1",
            "source_info": {
                "paper_title": "Enhancing Zero-Shot Chain-of-Thought Reasoning in Large Language Models through Logic",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Cmps-LoT (comparison-only LoT)",
            "name_full": "Comparison-based LoT (Cmps-LoT)",
            "brief_description": "A simpler LoT ablation that composes a negation explanation and uses the LLM to judge whether the composed contradiction is false, then revises steps if judged false.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo",
            "model_description": "GPT-3.5-turbo used as backend for ablation experiments; same experimental config as other ablations.",
            "reasoning_method_name": "Cmps-LoT",
            "reasoning_method_type": "other",
            "reasoning_method_description": "For each step T_i the model produces a post-hoc negation explanation E¬_i and composes C' = premises ∧ ... ∧ ¬T_i ∧ E¬_i; the model is asked whether C' is contradictory; if judged false, it revises T_i. This is less symmetric/diverse than Adpt-LoT because it does not generate both supportive and opposing post-hoc explanations to be chosen between.",
            "task_name": "GSM8K / AQuA / Date",
            "task_description": "Benchmarks used in ablation comparison.",
            "performance": "GSM8K 77.67%; AQuA 57.48%; Date 52.37% (Table 4, Cmps-LoT with GPT-3.5-turbo).",
            "comparison_with_other_method": true,
            "performance_other_method": "Adpt-LoT: GSM8K 80.15%; AQuA 60.63%; Date 52.37% ; CoT: GSM8K 78.75%; AQuA 57.09%; Date 51.26% (Table 4).",
            "key_findings": "Cmps-LoT provides some improvement over CoT on certain tasks (e.g., small gains on AQuA/Date) but generally underperforms the full Adpt-LoT; the paper finds that producing and contrasting both T- and ¬T-centered explanations (Adpt-LoT) yields better verification than composing a single negation check.",
            "counter_examples_or_negative_results": "Cmps-LoT is less effective than Adpt-LoT on difficult tasks (Adpt-LoT achieves larger gains), and simple 'Self-Check' (no logical hints) can sometimes be competitive on particular tasks but not robustly better.",
            "uuid": "e4749.2",
            "source_info": {
                "paper_title": "Enhancing Zero-Shot Chain-of-Thought Reasoning in Large Language Models through Logic",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Self-Check (ablation)",
            "name_full": "Self-Check (double-check without logical hints)",
            "brief_description": "An ablation where the LLM is prompted to 'double check the step' without the logical guidance of reductio ad absurdum or generating opposing explanations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo",
            "model_description": "GPT-3.5-turbo used for ablation comparison (Table 4).",
            "reasoning_method_name": "Self-Check",
            "reasoning_method_type": "similar",
            "reasoning_method_description": "The model is asked to 'double check' each reasoning step directly (no explicit generation of opposing viewpoints or post-hoc paired explanations); a straightforward self-verification prompt.",
            "task_name": "GSM8K / AQuA / Date",
            "task_description": "Same benchmarks as other ablations.",
            "performance": "GSM8K 76.15%; AQuA 56.19%; Date 51.57% (Table 4, Self-Check with GPT-3.5-turbo).",
            "comparison_with_other_method": true,
            "performance_other_method": "Adpt-LoT: GSM8K 80.15%; Cmps-LoT: GSM8K 77.67%; CoT: GSM8K 78.75% (Table 4).",
            "key_findings": "Self-Check without logical structure or the creation of opposing reviews is less effective for robust error detection and overall performance, particularly on math reasoning where Adpt-LoT shows clearer gains. The paper argues that logical hints and contrasting opposing viewpoints are important for detectable improvement.",
            "counter_examples_or_negative_results": "Self-Check sometimes performs similarly to CoT or slightly worse — it does not consistently improve performance and can be inferior to both Cmps-LoT and Adpt-LoT on harder problems.",
            "uuid": "e4749.3",
            "source_info": {
                "paper_title": "Enhancing Zero-Shot Chain-of-Thought Reasoning in Large Language Models through Logic",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Ensemble / diverse-sampling (related work mention)",
            "name_full": "Ensembling via diverse prompts or multiple samples (self-consistency / majority voting)",
            "brief_description": "Prior approaches generate multiple reasoning traces via sampling or diverse prompts and aggregate them (e.g., majority voting or self-consistency) to improve robustness; mentioned as effective but costly and sometimes limited.",
            "citation_title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models",
            "mention_or_use": "mention",
            "model_name": "various (general prior methods)",
            "model_description": "Methods typically applied to LLMs by sampling multiple chains or using diverse few-shot exemplars; not a single model.",
            "reasoning_method_name": "Ensembling / Diverse sampling / Self-Consistency",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "Produce N different reasoning paths either by repeated stochastic sampling with the same prompt (self-consistency) or by creating diverse prompts/exemplars, then aggregate answers via majority voting or other ensembling. Diversity comes from sampling randomness or prompt variation.",
            "task_name": "General reasoning benchmarks (referenced prior work)",
            "task_description": "Used widely in prior work to boost accuracy on multi-step reasoning by leveraging multiple candidate chains.",
            "performance": "No single set of performance numbers in this paper; the paper states ensemble/diverse-sampling approaches have been widely combined with other techniques for higher accuracy (cites self-consistency and related works) but are costly (N-times sampling) and can be limited by majority-vote strategies.",
            "comparison_with_other_method": true,
            "performance_other_method": null,
            "key_findings": "The paper contrasts LoT's guided revision (which produces a small set of targeted alternative viewpoints and revises only when necessary) with broad ensembling, arguing that while diversity via ensembling helps, it is costly and not guided; LoT aims to be more efficient by generating and adjudicating specific alternative explanations per step.",
            "counter_examples_or_negative_results": "Paper notes ensemble/majority voting can be limited as it is unguided and may not perform deep logical error detection; no new experimental comparison numbers in this paper comparing ensemble methods directly to LoT.",
            "uuid": "e4749.4",
            "source_info": {
                "paper_title": "Enhancing Zero-Shot Chain-of-Thought Reasoning in Large Language Models through Logic",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Multi-model size effect",
            "name_full": "Effect of model scale on similar vs diverse reasoning methods",
            "brief_description": "Aggregate observation across experiments that larger, more capable LLMs benefit more consistently from LoT's verification-and-revision than smaller models, which can be confused by extra intervention.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Vicuna-7b / Vicuna-13b / Vicuna-33b / GPT-3.5-turbo / GPT-4",
            "model_description": "Range of models evaluated in the paper: Vicuna variants (7B, 13B, 33B) — LLaMA-based open-source chat models — and closed models GPT-3.5-turbo and GPT-4.",
            "reasoning_method_name": "LoT vs CoT (cross-model comparison)",
            "reasoning_method_type": "diverse (LoT) vs similar (CoT)",
            "reasoning_method_description": "Same methods as above applied across model sizes: CoT single-chain baseline vs LoT adaptive revision producing opposing explanations. The paper reports revision frequency and final chain length statistics per model/task to analyze behavior.",
            "task_name": "Multiple (GSM8K, AQuA, Date, SocialQA, CauseEffect, ShuffledObjects, LastLetter, OddOneOut)",
            "task_description": "Wide set of benchmarks spanning math, commonsense, causal, symbolic, and social reasoning used to study model-size effects and LoT impact.",
            "performance": "Aggregate statement: LoT enhances CoT across domains, with benefits more consistent for models &gt;7B and becoming more prominent as model ability increases (e.g., GPT-4 shows robust self-correction improvements). Specific per-model deltas are reported in Table 1 (paper).",
            "comparison_with_other_method": true,
            "performance_other_method": "CoT baseline performances are reported per model/task in Table 1; LoT usually improves or keeps performance stable, with larger models showing larger gains.",
            "key_findings": "Larger models (GPT-4, GPT-3.5) revise more often and derive more benefit from LoT; smaller models (e.g., Vicuna-7B) are more likely to be harmed or confused by extra interventions. LoT-refined chains tend to be slightly shorter than original CoT chains, and revision frequency correlates with model capability and task difficulty.",
            "counter_examples_or_negative_results": "For small models such as Vicuna-7b, LoT can sometimes worsen performance (the paper reports non-negligible worsening rates for small models); also some tasks show no improvement or mixed effects depending on model size and task.",
            "uuid": "e4749.5",
            "source_info": {
                "paper_title": "Enhancing Zero-Shot Chain-of-Thought Reasoning in Large Language Models through Logic",
                "publication_date_yy_mm": "2023-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Large Language Models are Zero-Shot Reasoners",
            "rating": 2,
            "sanitized_title": "large_language_models_are_zeroshot_reasoners"
        },
        {
            "paper_title": "Self-refine: Iterative refinement with self-feedback",
            "rating": 1,
            "sanitized_title": "selfrefine_iterative_refinement_with_selffeedback"
        },
        {
            "paper_title": "Tree of thoughts: Deliberate problem solving with large language models",
            "rating": 1,
            "sanitized_title": "tree_of_thoughts_deliberate_problem_solving_with_large_language_models"
        },
        {
            "paper_title": "Least-to-most prompting enables complex reasoning in large language models",
            "rating": 1,
            "sanitized_title": "leasttomost_prompting_enables_complex_reasoning_in_large_language_models"
        }
    ],
    "cost": 0.0173795,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Enhancing Zero-Shot Chain-of-Thought Reasoning in Large Language Models through Logic
26 Mar 2024</p>
<p>Xufeng Zhao xufeng.zhao@uni-hamburg.de 
University of Hamburg</p>
<p>Mengdi Li mengdi.li@studium.uni-hamburg.de 
University of Hamburg</p>
<p>Wenhao Lu wenhao.lu@uni-hamburg.de 
University of Hamburg</p>
<p>Cornelius Weber cornelius.weber@uni-hamburg.de 
University of Hamburg</p>
<p>Jae Hee Lee 
University of Hamburg</p>
<p>Kun Chu kun.chu@uni-hamburg.de 
University of Hamburg</p>
<p>Stefan Wermter stefan.wermter@uni-hamburg.de 
University of Hamburg</p>
<p>Enhancing Zero-Shot Chain-of-Thought Reasoning in Large Language Models through Logic
26 Mar 20249F645F168B3928A5510CBBEE9E268A9BarXiv:2309.13339v4[cs.CL]Large Language ModelsChain-of-ThoughtPromptingReasoningReductio ad Absurdum
Recent advancements in large language models have showcased their remarkable generalizability across various domains.However, their reasoning abilities still have significant room for improvement, especially when confronted with scenarios requiring multi-step reasoning.Although large language models possess extensive knowledge, their reasoning often fails to effectively utilize this knowledge to establish a coherent thinking paradigm.These models sometimes show hallucinations as their reasoning procedures are unconstrained by logical principles.Aiming at improving the zero-shot chain-of-thought reasoning ability of large language models, we propose LoT (Logical Thoughts), a self-improvement prompting framework that leverages principles rooted in symbolic logic, particularly Reductio ad Absurdum, to systematically verify and rectify the reasoning processes step by step.Experimental evaluations conducted on language tasks in diverse domains, including arithmetic, commonsense, symbolic, causal inference, and social problems, demonstrate the efficacy of enhanced reasoning by logic.The implementation code for LoT can be accessed at: https://github.com/xf-zhao/LoT.</p>
<p>Introduction</p>
<p>Large language models (LLMs) appear to be nearly omniscient because of their extraordinary ability to deal with tasks requiring knowledge of common sense or specialized domain knowledge.Their success has been established in numerous fields extending beyond the realm of language processing (Bubeck et al., 2023;Yao et al., 2023b;Ahn et al., 2022;Zhao et al., 2023).However, one major prob-CoT LoT Figure 1: An overview of CoT (chain-of-thought prompting, Wei et al., 2022) and LoT (ours).In CoT, the failure of entailment ( ) makes the rest of the deduction untrustworthy ( ), impeding the overall success of the deduction.In contrast, LoT is designed to think-verify-revise: it adopts those who pass the verification ( ) and revises ( ) those who do not, thereby effectively improving the overall reasoning capability.lem residing in generative LLMs yet to be solved is their tendency to hallucinate wrong statements in a confident style (Bang et al., 2023).A quick example can be found by asking a non-internet-based LLM about very recent news -it will too easily make up facts without hesitation.</p>
<p>An educated human with expertise in logical reasoning can systematically examine words before coming to a conclusion.Unlike logical reasoning by humans, the logical incompetence of deductions by LLMs makes their decisions untrustworthy.LLMs may have a large number of logical concepts and tricks available but fail to actively utilize them in an organized manner.However, principles in logic well-adapted by humans can leverage the reasoning ability of language models.Take a simple logic reasoning task as an example: "If Tom plays football outside, then John will also join to play; if John plays football, then Mary won't go outside.Knowing that Mary is outside, is Tom playing football?"Nine out of ten answers from ChatGPT 1 will conclude that "we cannot conclude whether Tom is playing football or not".However, with the help of the knowledge that the contrapositive holds the exact same truth value with the original proposition, we may prompt ChatGPT to "use contrapositive" to reason in another way.Then it deduces correctly: " ... Using the contrapositive of the first statement, if John does not join to play (which we have deduced), then it implies that Tom does not play football outside.Therefore, based on the given information and the contrapositives, it can be deduced that Tom is not playing football."Even though logical concepts are not new to an LLM, the model initially struggles to incorporate them.See Fig. 3 in Appendix A for the full conversation.</p>
<p>By prompting an LLM to utilize logical equivalence, i.e. being equivalent in logic but expressed differently in natural language, the original premises could be represented in variational ways, effectively fostering the exploration of potential solutions.</p>
<p>Motivated by the reasoning process in logic, we propose LoT (Logical Thoughts) prompting framework, a fully automatic reasoning paradigm, to selfimprove further the zero-shot reasoning2 ability of LLMs, which not only lets an LLM think step by step but also verify, step by step, according to the guidance via the principle of Reductio ad Absurdum, and revise the reasoning chain if necessary to guarantee a sound inference (see Fig. 1 for an overview).</p>
<p>Related Work</p>
<p>In order to unleash the power of a pre-trained generative language model, the quality of the interaction prompts plays an important role.</p>
<p>Chain-of-Thought Prompting</p>
<p>Prior works show that LLMs have the ability to solve complex tasks but require a proper strategy to unleash this ability, e.g., human-in-the-loop alignment tuning (Ouyang et al., 2022) and Chain-of-Thought prompting (CoT) (Wei et al., 2022).To generate a chain of thought that decomposes the original problem into several small parts that a language model can easily handle, CoT creates few-shot exemplars of a detailed reasoning path to let the model follow.Least-to-most (Zhou et al., 2023) explicitly prompts the LLM to divide complex questions into sub-problems and solve them one by one.Moreover, zero-shot-CoT (Kojima et al., 2022) showcases the impressive effectiveness of simply attaching the sentence "Let's think step by step."before any reasoning trace starts.</p>
<p>We build our approach under a zero-shot setting and integrate zero-shot-CoT as a baseline to compare against.While existing CoT-based methods focus on encouraging the reasoning step to be concrete but lack supervision of their faithfulness, we propose a step-by-step verification mechanism.</p>
<p>Variational Prompting</p>
<p>As an auto-regressive model, the output of an LLM can be influenced by its input.Therefore, there are many research endeavors on prompt variations.</p>
<p>Summarizing existing works, the reasoning procedure benefits from prompts that (1) are relevant to the reasoning task, (2) are diverse in expression, (3) lead to decomposition of complex tasks, (4) suggest grounding with known facts, and (5) results in progressive revision of reasoning steps.In the design of LoT prompting, we selectively adopt these effective prompt properties.</p>
<p>Relevance.An LLM can be easily distracted by irrelevant words in the prompt.A pre-selection of context enhances the correctness of reasoning (Creswell et al., 2023;Creswell and Shanahan, 2022;Ling et al., 2023).Previous works typically resort to a language model to evaluate the relevance of facts and infer with the ones that contribute to a reasoning step (Creswell et al., 2023;Ling et al., 2023).Our verification of each reasoning step is conducted by prompting a language model to select relevant premises to deduct from.Diversity.The collective intelligence from a set of reasoning paths (typically, sampling N times) helps produce a reliable answer that is nearly consistent among these variants.Despite the N -times increased cost, this ensemble approach has been widely combined with other techniques for higher accuracy (Li et al., 2023b;Ling et al., 2023;Yao et al., 2023a;Zheng et al., 2023).A single reasoning trace may be biased.In order to produce a set of reasoning candidates, previous works resort to generating samples several times with the same prompt (Wang et al., 2023), or create diverse prompts in the beginning for variants (Li et al., 2023b).However, the ensemble-based approaches are both costly and inefficient.The performance of their majority voting strategy can be limited since it is not a guided, in-depth thinking strategy.</p>
<p>Decomposition.An automatic decomposition of a tough question improves the reasoning reliability, which has been evidenced by the success of Leastto-Most (Zhou et al., 2023), Zero-shot-CoT (Kojima et al., 2022) and many prompting techniques (Yao et al., 2023a;Kojima et al., 2022;Wei et al., 2022).Grounding.External functions, e.g., a third-party calculator for mathematical problems (Schick et al., 2023), information acquisition from Wikipedia (Yao et al., 2023b), or an affordance evaluation in robotics (Ahn et al., 2022), can ground the generation to be meaningful.This verification can be triggered under a specified condition or be applied to the reasoning process (Lightman et al., 2024;Ling et al., 2023;Li et al., 2023b).LoT is primarily inspired by a logical standpoint to ground LLM generations with logical principles, empowering an LLM to argue different possibilities.It suggests verification and also introduces revisions of the suspected reasoning steps.Revision.Revision (or refinement) can be re-garded as a special kind of diversity, but it is conditioned on the previous generation as hints.It re-examines the words with a focus on their quality in terms of, for example, validity and conciseness (Madaan et al., 2023;Zheng et al., 2023;Welleck et al., 2023).It is an iterative generating process conditioned on previous content.Many previous works actually benefit from this manner though not explicitly mentioned.For example, Progressive-Hint Prompting (Zheng et al., 2023) generates consistent answers by progressively guiding the LLM with hints of accumulated possible answers.It repeats the generation until the answer is deemed consistent with the previous.Other works generate content conditioned not only on the previous content but also on extra feedback (Madaan et al., 2023).To obtain a revision with high quality, this guiding feedback should be specific and actionable.LoT avoids unnecessary duplicating on non-controversial reasoning steps and only revises steps deemed implausible, resulting in a chain that grows only when required (Fig. 1 blue circle).Besides, we employ a post hoc explanation (Jung et al., 2022) to provide constructive suggestions for purposeful revisions.</p>
<p>Neurosymbolic Models</p>
<p>Neurosymbolic models combine neural networks with symbolic representations and reasoning techniques.Their success stems from their ability to leverage symbolic (structured) knowledge to enhance learning or reasoning (Sarker et al., 2021;d'Avila Garcez and Lamb, 2020;Nye et al., 2021).Unlike end-to-end black-box frameworks, these neurosymbolic models are more interpretable and explainable because of their transparency.</p>
<p>There exist works that adopt concepts from symbolic logic (Agler, 2012) to establish a reliable reasoning path (Creswell et al., 2023;Jung et al., 2022).To solve binary question-answering problems, Jung et al. (2022) propose to generate a post hoc explanation graph for a statement and compute the relative relations to formulate a symbolic logic expression.The truth of the statement is thereby assigned by solving the satisfiability problem of this symbolic expression.The LoT framework employs a controlled prompting strategy that leverages logic rules and post hoc arguments to enhance error detection.</p>
<p>Method</p>
<p>As demonstrated in the contraposition example presented in the introduction, when known logical rules are utilized to achieve a logical equivalence, the resultant distinct natural language expression affords LLMs a chance to engage in reasoning from an alternative perspective.</p>
<p>A challenge is that the language model has to identify the inherent logical structures first to know whether certain prior knowledge can be effectively applied.Moreover, transforming everything from the real world into a symbolic expression is unrealistic.The applicable scenario is limited because questions in many reasoning fields beyond logic, e.g., mathematics problem solving, can hardly be expressed in symbolic logic.Nevertheless, there is promise in incorporating concepts from logic that contribute to the process of argument proof in order to construct a neurosymbolic framework (d'Avila Garcez and Lamb, 2020;Creswell et al., 2023) that facilitates a causal reasoning trace, i.e. the premises and leading thoughts entail the thoughts behind.Continuing with the success of "let the model talk", e.g., "let's think step by step" in zeroshot-CoT (Kojima et al., 2022), we further propose to guide the conversation with logic for exploration of solutions.See Fig. 2 for the guiding diagram.</p>
<p>Reductio ad Absurdum</p>
<p>Self-checking is a challenging task for LLMs (Ling et al., 2023;Huang et al., 2024), and humans may also struggle with it.In logic, an effective technique to establish a claim is known as reductio ad absurdum, which involves an initial assumption and consequent derivation of absurdity or contradiction.Let P and Q denote two propositions.The relation between a premise P and its conclusion Q can be expressed as P ⊢ Q.Here "⊢" is a syntactic turnstile which means Q is a syntactic consequence of P (Agler, 2012), i.e. there exists a proof that claims the conclusion Q given the premise P .In order to prove Q using reductio ad absurdum, let us assume its negation ¬Q is valid and then check the contradiction3 of the conjunctive proposition
C = P ∧ ¬Q,(1)
where "∧" is a binary conjunction operator, meaning the truth of the conjunction requires the truth of both sides.Upon the contradiction of the co-existence of the P and ¬Q, P ⊢ Q is thus proved true, and then we can claim the validation of the conclusion Q given the premise P .Many logic principles, e.g., the contraposition mentioned in the introduction section (see Appendix B for a proof), can be derived by deductions following this rule.This thinking paradigm helps humans check arguments carefully before composing a conclusion.As we will demonstrate later, the reasoning ability of LLMs can also be improved by benefiting from this paradigm.</p>
<p>LoT Prompting</p>
<p>There is evidence that a series of coherent explanations helps an LLM to unleash its reasoning ability (Wei et al., 2022;Kojima et al., 2022;Zhou et al., 2023), while discouragement on its utterance, e.g., prompts like "just tell me the result without any explanation", negatively impact on an LM's reasoning ability.So we elaborate on the success of an explicit reasoning process.</p>
<p>A typical N -step reasoning trace can be expressed as {P, T 1 , • • • , T N }, where P is the known premise and T i is the i-th step of thoughts that originates from the output of a vanilla CoT. 4 Usually, T N concludes the thoughts and answers the specified question.</p>
<p>Unfortunately, LLMs hallucinate.LLMs usually generate content autoregressively, which means the generation of T i is based on the former content {P, • • • , T i−1 }.Errors in T i will propagate and gradually influence T i ′ for increasing i ′ &gt; i, making the successive deductions and ultimately the final conclusion untrustworthy (cf.Fig. 1).Therefore, we propose a verification loop to double-check each reasoning step.Following Eq. 1, this doublecheck procedure unrolls by checking the validity of
P, • • • , T i−1 ⊢ T i , i.e. the contradiction of C i = P ∧ T 1 ∧ • • • ∧ T i−1 ∧ ¬T i ,(2)
once T &lt;i passed the verification.If any step T i fails the verification, this implies that the premises and previously verified thoughts T &lt;i do not entail T i .In this case, T ≥i needs to be revised.To negate T i by an LLM, a straightforward way is to format ¬T i as "It is false to say T i " or to give to the LLM an instruction of "Negate T i ".Then, the LLM has to further identify possible contradictions in C i (Eq.2).Cmps-LoT.Given that T i is articulated in natural language and can span multiple sentences, we aim to derive a more insightful negation by adopting the chain-of-thought methodology.Here, we task the model with composing a post hoc explanation5 , E ¬ i of ¬T i (Jung et al., 2022), and then prompt the LLM to check the validity of
C ′ i = C i ∧ E ¬ i instead.
We call this simple approach Cmps-LoT.Adpt-LoT.Considering that a logical error in a text generated by an LLM is hard to spot by the LLM itself (Huang et al., 2024), we additionally propose to alleviate the difficulty in verifying T i by generating a pair of post hoc explanations E i and E ¬ i of T i and ¬T i respectively, and let the LLM decide between T i ∧ E i and ¬T i ∧ E ¬ i and adopt one of the two.We call this full approach Adpt-LoT or just LoT.</p>
<p>An LLM is then often biased by the prompt and, as a result, generates an explanation consistent with the prompt.Because of this "compulsory" behavior, once a statement is deemed false in the leading prompt, the LLM tries hard to discover errors even if they are less obvious.LoT gains advantages from the mandatory error-detection behavior.Additionally, when transitioning from the Cmps-to the Adpt-variant, the problem transforms into a preference discrimination task (Saunders et al., 2022), featuring more balanced reviews for both T and ¬T and being more manageable.</p>
<p>Chain Growth</p>
<p>In order to investigate a step T i , LoT drops all of the trailing thoughts T &gt;i and branches out for revision
T ′ i conditioned on {T ≤i , E ¬ i }.
Since precise feedback is important to the success of revision (Madaan et al., 2023), we also encourage the LLM to revise any inappropriate thought with the advice to reason "why it is wrong", i.e.E ¬ i .Then, an adapted chain with a new conclusion can be re-generated based on the concatenation of the verified thoughts so far, i.e. {T <i , T ′ i }.This loop continues until the final conclusion passes the verification, which results in a chain with all the nodes Algorithm 1 Adpt-LoT Reasoning
Require: P , LLM Initialize T ← {P } T 1 , T 2 , • • • , T N ← LLM(T ) ▷ Vanilla CoT i ← 1 while i ≤ N do E ¬ i ← post hoc LLM(E|¬T i ; T ) E i ← post hoc LLM(E|T i ; T ) Ê ← LLM(E i ; E ¬ i |T ) ▷ Adopt if Ê is E ¬ i then T ′ i ← LLM(T |T ; T i ; E ¬ i ) ▷ Revise T i ← T ′ i {T >i } N ′ ← LLM(T ∪ T i ) ▷ Adapt N ← N ′ end if T ← T ∪ T i ▷ Update i ← i + 1 end while return T
being verified (see Fig. 2 for an abstract depiction and Fig. 4 for a detailed example).Note that this chain grows only when required.</p>
<p>Alg. 1 shows the pseudo-code of the function to compute the reasoning trace of (Adpt-)LoT.The pseudo-code for Cmps-LoT can be found in Alg. 3, Appendix C, where the distinct procedure for identifying contradictions is emphasized.</p>
<p>Experiments</p>
<p>For the following reasons we carry out the experiments in a zero-shot setting: 1) Zero-shot-CoT (Kojima et al., 2022) has a wide task-agnostic application potential, while few-shot requires domain knowledge; 2) The few-shot prompts heavily influence the performance even on the same dataset, so it is hard to evaluate fairly as the prompt varies.Drawing direct comparisons with other prompting works in the literature is challenging due to variations in task settings and backend language models.Many of these works are specifically under a few-shot setting, which would necessitate additional modifications to adapt them for zero-shot reasoning.We consider this as an area for future investigation.</p>
<p>Since our work is an enhancement on the chain produced by zero-shot-CoT (Kojima et al., 2022), we compare LoT with it as the baseline to demonstrate the benefit of step-wise verification and revision for zero-shot reasoning.We evaluate the accuracy of tasks in various domains as the overall performance measure and also report the impact of the logical revision on the original reasoning chain.</p>
<p>We  Ling et al. (2017).The GSM8K dataset contains grade school mathematics questions that should be responded to by numerical answers; AQuA has more advanced questions but has several optional answers to choose from.</p>
<p>(2) Commonsense reasoning tasks DateUnderstanding and OddOneOut Srivastava et al. ( 2023).The DateUnderstanding task necessitates the utilization of both common sense and fundamental arithmetic calculations to find out the correct date, making it sufficiently challenging to prevent it from being solvable through simple one-step reasoning.The OddOneOut requires common sense to deduct the unusual object in the context.</p>
<p>(3) Causal inference tasks CauseEffect and Shuf-fledObjects (Srivastava et al., 2023), where both of the tasks require reasoning from the context for a correct deduction.</p>
<p>(4) Symbolic reasoning task LastLetter (Srivastava et al., 2023).In this task, the language model has to extract the last letter of given candidates and concatenate them in order, which is simple for humans but challenging for language models because of tokenization (Mielke et al., 2021).</p>
<p>(5) Social interaction reasoning task, SocialQA (Srivastava et al., 2023), that measures the model's emotional and social intelligence in human daily activities.Completing the task requires an understanding of human behavior.</p>
<p>To get a formatted answer that can be directly compared with the ground truth in the aforementioned dataset, a final prompt asking the final answer is attached after the reasoning trace, e.g., for the GSM8K dataset we simply attach "Therefore, the final numerical answer is:" at the ends.For robustness, this answer is matched with a regular expression to only extract numerical digits before comparing it with the ground truth.varies when applied on language models of different capabilities (Wei et al., 2022;Kojima et al., 2022).Therefore, we conducted an evaluation of the LoT method using a range of models, including Vicuna-7b, Vicuna-13b, and Vicuna-33b models (Chiang et al., 2023), as well as GPT-3.5-turboand GPT-4.The Vicuna model is an open-sourced language model trained by fine-tuning LLaMA (Touvron et al., 2023) on user-shared conversations.It demonstrates strong performance across various scenarios and offers flexibility in terms of model size selection.On the other hand, GPT-3.5-turbo and GPT-4 are larger models known for their stateof-the-art performance in numerous tasks.</p>
<p>Backend LLMs. Previous works show that the performance improvement of the CoT technique</p>
<p>To ensure stable results and promote self-error detection within the models, we set the temperature parameter to 0.1.Additionally, the max_token parameter was established at 2048, a sufficient limit to accommodate all the datasets employed in our evaluation.</p>
<p>Does LoT enhance the performance of CoT?</p>
<p>To answer the first question, we conduct zero-shot experiments with datasets covering more diverse topics and with language models of different sizes.The LoT-enhanced performance is compared with the zero-shot baseline in Tab. 1.The experiment shows that LoT can enhance the performance of the base CoT in various domains.The performance benefits are more consistent when the model size gets considerable (&gt;7B parameters).Moreover, the performance gain becomes more prominent as the model's ability increases (e.g.GPT-4).</p>
<p>What is the impact on individual reasoning chains?</p>
<p>We report more insightful case-wise statistics and discussions in this section, including (1) average revision frequency in Tab.2; (2) the resultant number of reasoning steps in Tab.3; and (3) a case study to illustrate the logical reasoning procedure.More detailed statistics including the worsening rate (i.e. the ones being originally correct by CoT but "corrected" to be wrong by LoT) and improvement rate (i.e. the ones that are originally wrong and being corrected by LoT) can be found in Tab. 5, Appendix E. Revision Frequency.In order to measure the complexity of revisions, we list the average revisions per chain in Tab. 2 and typical reasoning steps required by CoT and LoT in Tab. 3. Note that the number of steps is not human-defined or prompted since our setting is in zero-shot, so the language models decide by themselves the length of a reasoning chain.</p>
<p>The percentage of revisions indicates the frequency of LoT to revise the candidate reasoning chain.We observe that language models with powerful capabilities (e.g., GPT-4 and GPT-3.5-turbo) are more active in revision than smaller models; and challenging tasks such as the math reasoning task lead to more revisions.However, revision does not necessarily alter the final deduction answer.For example, LoT with GPT-3.5-turbobackend revises several times on SocialQA yet achieves the same accuracy as CoT.Intuitively, solving one problem may lead to multiple pathways, and some revisions   2) The LoT refined reasoning chain is generally a little bit shorter than the original zero-shot CoT.Our conjecture is that this phenomenon might arise because, during the refinement process, the language model strives to incorporate additional information, consequently yielding concise chains of reasoning.</p>
<p>Case Study.We show a successful case on the Date Understanding task to demonstrate the verification and revision procedure applied to the chain of thoughts initialized by zero-shot-CoT.(See Appendix F for detailed prompts and further case studies.)</p>
<p>In Snippet A, there are initialized zero-shot-CoT reasoning steps where step #6 is actually incorrectly inferred (colored in red).Here, black color indicates given context or fixed prompts; non-black color indicates generated content by the LLM.The error occurs because zero-shot-CoT is distracted by the irrelevant premise of "Jane's appointment will be 3 days later" and concludes with a wrong answer.</p>
<p>Snippet A Question: Today is Apr 10, 1985.Jane's appointment will be 3 days later.What is the date 10 days ago in MM/DD/YYYY?Let's think step by step.Answer: #1.Today is Apr 10, 1985.#2.Jane's appointment will be 3 days later.#3.So, Jane's appointment will be on Apr 13, 1985.#4.To find the date 10 days ago, we subtract 10 days from Apr 13, 1985. #5. Apr 13, 1985-10 days = Apr 3, 1985. #6.Therefore, the date 10 days ago is Apr 3, 1985.</p>
<p>Snippet B Verification of the next step: #6.Therefore, the date 10 days ago is Apr 3, 1985.Review X: <review>step #6 is TRUE because we are subtracting 10 days from the given date of Apr 13, 1985.By subtracting 10 days, we go back in time and arrive at Apr 3, 1985.Therefore, the date 10 days ago is Apr 3, 1985.</review> Review Y: <review>step #6 is FALSE because the reasoning in step #4 is incorrect.To find the date 10 days ago, we need to subtract 10 days from Apr 10, 1985, not from Apr 13, 1985.The correct calculation would be: Apr 10, 1985-10 days = Mar 31, 1985.Therefore, the date 10 days ago is Mar 31, 1985. </re-view> To verify the 6th step, as shown in Snippet B, the LLM is prompted to generate two opposite re-Snippet C Let's check two different reviews (X and Y).Support the more plausible one and criticize the other one.Review X: ... Review Y: ... Let's start by analyzing one by one.I. What are the premises and previous steps to support the verification of step #6? (Your answer should quote the exact quote as support.)II.Criticise the incorrect review.(Note the examined step doesn't have to tackle the whole problem at once.)Finally, identify whether step #6 is true or false.Analysis and conclusion: I.The premises and previous steps to support the verification of step #6 are as follows:</p>
<p>-Today is Apr 10, 1985.-Jane's appointment will be 3 days later.-Jane's appointment will be on Apr 13, 1985.</p>
<p>-To find the date 10 days ago, we subtract 10 days from Apr 13, 1985.II.Review Y is correct in criticizing step #4.The reasoning in step #4 is indeed incorrect.To find the date 10 days ago, we need to subtract 10 days from Apr 10, 1985, not from Apr 13, 1985.The correct calculation would be: Apr 10, 1985 -10 days = Mar 31, 1985.Therefore, the date 10 days ago is Mar 31, 1985. Conclusion: Step #6 is false.The correct date 10 days ago is Mar 31, 1985, not Apr 3, 1985.views 6 given the question context and the previous five verified steps.Due to the content limitation, we omit the long prompt and focus on the resultant generation.Note that Review Y successfully detects the unobvious error and provides a correction that can be adopted later for revision.In Snippet C, the LLM is encouraged to extract and deduct based on relevant premises.Finally, it ends with adopting Review Y and reaching a correct conclusion.</p>
<p>Do post-hoc explanations help LLM self-check?</p>
<p>In order to examine the error-finding ability, the results of using LoT and its ablated variants on three challenging tasks are shown in Tab. 4, where "Self-Check" is an ablation of reductio ad absurdum such that the LLM is only prompted to "double check the step" without logical hints.With LoT, the complex examination problem is broken down into two procedures: 1) the post hoc explanation mechanism for rigorous error detection, and 2) a single-choice preference for selecting the more reasonable opinion from the two candidates.The better performance of (Adpt-)LoT suggests that when it comes to error detection in LLM's reasoning, it is more effective for an LLM to embrace one of two opposing viewpoints (T, E or ¬T, E ¬ ) rather than composing the verification directly (which is the case for both "Self-Check" and Cmps-LoT), especially when coping with tasks that are difficult such as math reasoning.The underlying LLM is GPT-3.5-turbo.</p>
<p>Method</p>
<p>Conclusion and Future Work</p>
<p>LLMs have impressive reasoning ability in domains that require commonsense knowledge, specialized expertise, comprehensive understanding, etc.However, there is still room to improve their multistep reasoning capability.Building upon zero-shot-CoT, we derive the LoT prompting framework from a symbolic logic perspective, utilizing the widely applicable principle of reductio ad absurdum, resulting in a robust think-verify-revise framework with plausible prompting properties.Experiments conducted on a variety of reasoning tasks spanning different domains demonstrate that enhancing zeroshot Chain-of-Thought with LoT leads to improved reasoning ability, particularly when applied to LLMs of large scale.Future Work.While our research primarily focuses on integrating human knowledge into CoT prompting, further exploration of additional logical deduction principles could enhance the reasoning process.Moreover, we demonstrate the efficacy of bolstering the robustness of complex reasoning by discerning between conflicting outputs, suggesting the potential extension of this approach to prompt and refine LLMs for self-improvement.This may entail utilizing self-checked outputs for reinforcement learning from AI feedback (RLAIF) (Li et al., 2023a;Lee et al., 2023).Such endeavors show promise, particularly in situations where a positive "GD gap" exists (Saunders et al., 2022), i.e., language models with promises to be further improved by discerning the quality of its generation, but we defer the investigation of this avenue to future research endeavors.</p>
<p>Limitations</p>
<p>Prompting and Fine-tuning.LoT establishes a controlled prompting strategy for self-correction.Nonetheless, it is worthwhile to explore future endeavors of fine-tuning LLMs for the purpose of spontaneous logical reasoning.Generation Probability.Rather than letting the LLM choose from different reviews, another possible method is to access and compare the probability of the generations.Unfortunately, there is no public access to the generation probability of GPT-3.5-turboyet 7 as it is possible for completion models (such as text-davinci-003).Considering a cheaper price and better performance, we conducted our experiments with the chatting model and left this possibility for future work.</p>
<p>Zero-shot, Few-shot and Beyond.Since our work is done with an aim to be as generalizable as possible, the experiments are all conducted in the zeroshot setting.Nonetheless, incorporating domain knowledge into the exemplar prompt proves advantageous for enhancing performance (Kojima et al., 2022;Wei et al., 2022), it is still worthwhile to explore the potential when LoT is applied in the few-shot setting in future work.</p>
<p>Ethics Statement</p>
<p>Large language models sometimes produce biased, untrustworthy statements.Despite our best intention to enhance the model, we are not rectifying these issues.It is advisable for individuals to exercise caution and avoid placing excessive reliance on it.This method is released with the purpose of research only.</p>
<p>A. ChatGPT Deduction Example w/ and w/o Contraposition</p>
<p>Fig. 3 shows the full ChatGPT conversation of the "football" example introduced earlier in the introduction section, indicating the benefits of explicitly prompting LLMs with deduction knowledge such as logic principles.</p>
<p>Tom football status unclear</p>
<p>February 21, 2024</p>
<p>Anonymous</p>
<p>If Tom plays foot-ball outside, then John will also join to play; if John plays football, then Mary won't go outside.Knowing that Mary is outside, is Tom playing football?</p>
<p>ChatGPT</p>
<p>Based on the given information:</p>
<p>If Tom plays football outside, then John will also join to play.</p>
<p>If John plays football, then Mary won't go outside.</p>
<p>Given that Mary is outside, we can deduce the following:</p>
<p>Since Mary won't go outside if John plays football, and Mary is outside, it implies that John is not playing football.</p>
<p>Since John is not playing football, it doesn't necessarily mean that Tom is playing football, as the first statement only establishes a condition for John joining if Tom plays, not vice versa.</p>
<p>Therefore, we cannot conclusively determine whether Tom is playing football or not based solely on the given information.</p>
<p>. .</p>
<p>B. Proof of Contraposition</p>
<p>We will prove contraposition using reductio ad absurdum:
Known premises P → Q (if P is true, then Q is true) and ¬Q (Q is false), prove ¬P . Proof P → Q, ¬Q ⊢ ¬P : 1 P → Q P 2 ¬Q P 3 P A 4 Q → E 1, 3 5 X C 2, 4 6 ¬P ¬I 3-5
Inside the proof, P stands for the known premise, A for assumption, → E for the elimination of "→" symbol by following the conditional statement, C for contradiction assertion (followed by an "X" which indicates the branch with an assumption is closed) and ¬I for the introduction of negation according to the rule of reductio ad absurdum.</p>
<p>C. Pseudo Codes for Adpt-LoT and Cmps-LoT Prompting</p>
<p>Alg. 1 and Alg. 3 are the pseudo-code of the function to compute the reasoning trace of LoT, where the difference regarding discovering contradiction is highlighted in blue.</p>
<p>Algorithm 2 Adpt-LoT Reasoning</p>
<dl>
<dt>Require</dt>
<dd>P , LLM Initialize T ← {P } T 1 , T 2 , • • • , T N ← LLM(T ) ▷ Vanilla CoT i ← 1 while i ≤ N do E ¬ i ← post hoc LLM(E|¬T i ; T ) E i ← post hoc LLM(E|T i ; T ) Ê ← LLM(E i ; E ¬ i |T ) ▷ Adopt if Ê is E ¬ i then T ′ i ← LLM(T |T ; T i ; E ¬ i ) ▷ Revise T i ← T ′ i {T &gt;i } N ′ ← LLM(T ∪ T i ) ▷ Adapt N ← N ′ end if T ← T ∪ T i ▷ Update i ← i + 1 end while return T Algorithm 3 Cmps-LoT Reasoning Require: P , LLM Initialize T ← {P } T 1 , T 2 , • • • , T N ← LLM(T ) ▷ Vanilla CoT i ← 1 while i ≤ N do E ¬ i ← post hoc LLM(E i |¬T i ; T ) C ← LLM(E ¬ i |T ) ▷ Compose if C is False then T ′ i ← LLM(T |T ; T i ; E ¬ i ) ▷ Revise T i ← T ′ i {T &gt;i } N ′ ← LLM(T ∪ T i ) ▷ Adapt N ← N ′ end if T ← T ∪ T i ▷ Update i ← i + 1
end while return T P is the known premises, e.g., question context, and an LLM is employed with various purposes in this context.By prompting the LLM to generate post hoc inferences and subsequently exposing them as discernible options for differentiation, the process facilitates a more convenient verification of entailment, as opposed to relying on the model to independently discover contradictions.Review Y: step #1 is false because it does not take into account the additional 10 miles that John had to drive due to the detour.The correct distance that John drove to his friend's house is 210 miles, not 200 miles.Therefore, the correct calculation for the time it took John to drive to his friend's house is: Every reasoning step has to undergo a verification procedure, which is guided by two post hoc reviews generated by the LLM ( ) independently.In this example, step #1 fails ( ) the verification because the discriminator agrees with the "Review Y" which correctly points out the error in this step.As a result, the LLM further revises ( ) the original step into a new step #1 and re-generates the trailing paths based on the revision.The procedure unrolls until every step is verified to be valid ( ).Key snippets of prompts used to achieve each procedure are shown in dotted boxes.Full prompts are given in the case study in Sec.4.4 and Appendix F.</dd>
</dl>
<p>D. LoT Prompting Diagram in Detail</p>
<p>E. Worsening and Improvement Rates</p>
<p>The worsening rate computes as #(correct→wrong)  #(correct→ * ) , where "#" means count and " * " indicates arbitrary correct/wrong candidates.Similarly, the improvement rate computes as #(wrong→correct)  #(wrong→ * ) .From Tab. 5, we can have a closer look at the intervention impact of LoT.For example, for small-sized language models such as Vicuna-7b, it is riskier to exert extra intervention, as the model may fail to follow.Indeed, larger models generally benefit from the proposed self-improvement procedure.For instance, GPT-4 exhibited enhanced accuracy on the Date Understanding, LastLetter, and OddOneOut tasks, with the improvement</p>
<p>Figure 2 :
2
Figure 2: A diagram demonstrating the think-verifyrevision loop of LoT.The two zoom-in boxes display the processes when a thought passes (top-left) and fails (bottom) the verification respectively.A thought passing the verification is kept in the reasoning trace, while a thought failing the verification is revised and a new chain of thought is generated based on the revision.The symbols in this figure are introduced in Sec.3.2 and Sec.3.3.See also Fig. 4 in Appendix D with extended details.</p>
<p>Figure 3 :
3
Figure3: An example conversation with ChatGPT where the language model fails to correctly deduce the answer initially, but when being prompted to use the idea of "contraposition", it successfully reaches the desired result.</p>
<p>Fig. 4
4
Fig.4shows a detailed diagram of the LoT workflow (cf.Fig.2) with a real example on the AQuA dataset.</p>
<h1>1</h1>
<p>1
To find out how long it took John to drive to his friend's house, we can use the formula: time = distance / speed.John drove 200 miles at a speed of 70 mph, but he had to take a detour that added 10 miles to his trip.So, the total distance he traveled was 200 + 10 = 210 miles.time = 210 miles / 70 mph time = 3 hours Question: John drives to his friend's house 200 miles away.He drives at a speed of 70 mph.He had to take a detour that added 10 miles to his trip.After he gets there he takes a route home that is 240 miles but he goes 80 mph.How long did the trip take?Let's think step by step.Answer: #1.let's find out how long it took John to drive to his friend's house.We can use the formula: time = distance / speed.time = 200 miles / 70 mph time = 2.86 hours #2.Next, we need to add the extra 10 miles from the detour to his total distance.total distance = 200 miles + 10 miles total distance = 210 miles #3.Now we can find out how long it took John to drive home.time = 240 miles / 80 mph time = 3 hours #4.Finally, we can add the time it took to drive to his friend's house and the time it took to drive home to find the total trip time.total trip time = 2.86 hours + 3 hours total trip time = 5.86 hours Therefore, the trip took John 5.86 hours.#2.To find out how long it took John to drive home, we can use the same formula: time = distance / speed.John drove 240 miles at a speed of 80 mph.time = 240 miles / 80 mph time = 3 hours #3.To find out the total time of the trip, we just need to add the time it took John to drive to his friend's house and the time it took him to drive home.total time = 3 hours + 3 hours total time = 6 hours Therefore, the trip took John 6 hours.</p>
<p>Figure 4 :
4
Figure4: Applying LoT verification and revision on CoT reasoning paths on the example of an arithmetic task.Every reasoning step has to undergo a verification procedure, which is guided by two post hoc reviews generated by the LLM ( ) independently.In this example, step #1 fails ( ) the verification because the discriminator agrees with the "Review Y" which correctly points out the error in this step.As a result, the LLM further revises ( ) the original step into a new step #1 and re-generates the trailing paths based on the revision.The procedure unrolls until every step is verified to be valid ( ).Key snippets of prompts used to achieve each procedure are shown in dotted boxes.Full prompts are given in the case study in Sec.4.4 and Appendix F.</p>
<p>aim to answer the following research questions by conducting experiments: 1.Does LoT enhance the performance of CoT in various domains as well as with LLMs of varying model scales?(Sec.4.2)
2. What is the impact of LoT on individual reason-ing chains (e.g. revision frequency, length)?(Sec. 4.3)3. Do post-hoc explanations help LLM self-check? (Sec. 4.4)4.1. Experimental SetupDataset. We demonstrate the effectiveness of LoTon diverse language topics:(1) Math reasoning tasks GSM8K Cobbe et al.(2021) and AQuA</p>
<p>LoT GSM8K AQuA Date SocialQA Cau.Eff. Objects Letter OddOut
✗17.5221.657.2437.0052.9434.000.0025.58Vicuna-7b✓17.6820.477.2436.5052.9435.000.0025.58(+0.16) (-1.18) (0.00)(-0.50)(0.00) (+1.00) (0.00)(0.00)✗33.7922.05 32.3141.0068.7531.002.0029.07Vicuna-13b✓37.5623.62 33.1548.5068.7531.504.0045.35(+3.77) (+1.57) (+0.84) (+7.50)(0.00) (+0.50) (+2.00) (+16.28)✗40.3326.38 15.7037.5052.9432.0014.6740.70Vicuna-33b✓40.4929.53 20.3547.5068.7534.5014.0043.02(+0.16) (+3.15) (+4.65) (+10.00) (+15.81) (+2.50) (-0.67) (+2.32)✗78.7557.09 51.2672.0092.1660.7567.3381.40GPT-3.5-turbo ✓80.1560.63 52.3772.0092.1658.2567.3381.40(+1.40) (+3.54) (+1.11)(0.00)(0.00)(-2.50) (0.00)(0.00)✗94.2971.56 83.0977.50100.00 100.00 92.6195.35GPT-4✓95.7174.31 85.1677.50100.00 100.00 93.1496.51(+1.42) (+2.75) (+2.07)(0.00)(0.00)(0.00) (+0.53) (+1.16)</p>
<p>Table 1 :
1
(Kojima et al., 2022)acy of our methods and compare them against baseline approaches using various models and datasets, with computation based on ground truth annotations.The percentage difference of CoT(Kojima et al., 2022)without (✗) and with (✓) LoT enhancement using different LLMs is shown below each cell group (green if positive, red if negative).CoT generally gains better performance when being enhanced by LoT.Larger models, such as GPT-4, exhibit strong and robust self-correction ability.</p>
<p>Revision ⟲ GSM8K AQuA Date SocialQA Cau.Eff. Objects Letter OddOut
Vicuna-7b2%4%2%1%2%0%3%0%Vicuna-13b7%10% 5%5%0%7%2%0%Vicuna-33b2%9%8%7%6%9%1%7%GPT-3.5-turbo 16%28% 32%5%20%9%4%16%GPT-43%20% 7%2%0%1%0%8%</p>
<p>Table 2 :
2
The average step-wise revision frequency is presented as a percentage, reflecting how often a reasoning step is revised by LoT.
LoT GSM8K AQuA Date SocialQA Cau.Eff. Objects Letter OddOutVicuna-7b✗1.221.16 1.341.091.002.543.461.00✓1.271.21 1.351.101.022.543.491.00Vicuna-13b✗2.812.89 5.062.691.002.931.661.00✓2.742.87 5.052.711.002.961.691.00Vicuna-33b✗1.941.99 2.313.261.003.261.201.70✓1.941.91 2.333.131.063.231.211.64GPT-3.5-turbo ✗4.176.83 3.662.501.733.024.841.57✓4.086.24 3.562.511.923.054.811.70GPT-4✗3.424.22 2.712.331.003.004.051.00✓3.414.39 2.722.331.003.014.051.08</p>
<p>Table 3 :
3
The average number of resultant reasoning steps without (✗) and with (✓) LoT applied.
might aim to enrich the sentence by incorporatingadditional known conditions and rhetorical supple-ments.Resultant Steps. The average step count is thevalid reasoning steps in the final CoT and LoT rea-soning paths (i.e. the intermediate verification, re-finement, etc. are not shown). From Tab. 3, we canconclude that 1) larger language models generallygenerate longer chains and are also more active inrevision;</p>
<p>Table 4 :
4
Zero-shot accuracy results (in %) in comparison of LoT (i.e.Adpt-) and its ablated variants.
GSM8K AQuA DateCoT78.75 57.09 51.26Self-Check 76.15 56.19 51.57Cmps-LoT77.67 57.48 52.37LoT80.15 60.63 52.37
Under the setting that no exemplars are provided in the prompts for in-context learning.
A proposition is considered contradictory if and only if it is false under every valuation.
In practice, we employ the prompt "Let's think step by step.\n # 1." to guide an LLM to generate reasoning steps in a consistent format, each leading with a "# NUM. ". Subsequently, regular expression rules can be easily applied to split these steps into atomic units T NUM .
A post hoc explanation is an explanation completed by the LLM with a prompt like "Ti is true because" or "Ti is false because".
In practice, as is shown in the example case, the special language mark "<review></review>" is applied for the sake of robust separations of different reviews.
AcknowledgementsWe gratefully acknowledge support from the China Scholarship Council (CSC) and the German Research Foundation (DFG) under the project Crossmodal Learning (TRR 169).Table5: Worsening rate (↓) and improvement rate (↑) when LoT is introduced.Numbers are in %.Note that the accuracy of the number of candidates can significantly impact the outcome.rate significantly surpassing the worsening rate, indicating that the LoT revisions are more trustworthy than the default ones, resulting in better performance.F. Prompts and Results for CasesF.1. GSM8K Success CaseInitial Zero-shot-CoTQuestion: Terry eats 2 yogurts a day.They are currently on sale at 4 yogurts for $5.00.How much does he spend on yogurt over 30 days?Let's think step by step.Answer: #1.First, we need to determine how many packs of 4 yogurts Terry will need to buy over 30 days.To do this, we divide 30 by 4: 30 / 4 = 7.5 #2.Since Terry can't buy a fraction of a pack, we round up to the nearest whole number: 8 #3.Next, we need determine how many individual yogurts Terry will buy.To do this, we multiply the number of packs by the number of yogurts per pack: 8 * 4 = 32 #4.Finally, we need to determine how much Terry will spend on yogurt.To do this, we divide the total number of yogurts by the number of yogurts per dollar: 32 / 4 = 8 Therefore, Terry will spend $8.00 on yogurt over 30 days.Verification of step #1Question: Terry eats 2 yogurts a day.They are currently on sale at 4 yogurts for $5.00.How much does he spend on yogurt over 30 days?Let's think step by step.Answer: Verification of the next step: #1.First, we need to determine how many packs of 4 yogurts Terry will need to buy over 30 days.To do this, we divide 30 by 4: 30 / 4 = 7.5Let's check two different reviews (X and Y).Suport the more plausible one and criticise the other one.Review X: <review> step #1 is TRUE because Terry eats 2 yogurts a day and there are 4 yogurts in each pack.So, to determine how many packs of 4 yogurts Terry will need to buy over 30 days, we divide 30 by 4. </review> Review Y: <review> step #1 is FALSE because the question states that Terry eats 2 yogurts a day, not 4. Therefore, we cannot assume that Terry needs to buy packs of 4 yogurts.Instead, we need to calculate how many individual yogurts Terry needs to buy over 30 days.Since Terry eats 2 yogurts a day, we multiply 2 by 30: 2 * 30 = 60.Therefore, Terry needs to buy 60 individual yogurts over 30 days.Next, we need to determine the cost of each individual yogurt.The sale price is 4 yogurts for $5.00, so each yogurt costs $5.00 / 4 = $1.25.Finally, we can calculate how much Terry spends on yogurt over 30 days by multiplying the number of yogurts (60) by the cost per yogurt ($1.25): 60 * $1.25 = $75.Therefore, Terry spends $75 on yogurt over 30 days.</review> Let's start by analyzing one by one: I. What are the premises and previous steps to support the verification of step #1? (Your answer should quote exact quote as support.)II.Criticise the incorrect review.(Note the examined step doesn't have to tackle the whole problem at once.)Finally, identify whether step #1 is true or false.Analysis and conclusion: I.The premise to support the verification of step #1 is that Terry eats 2 yogurts a day and the yogurts are on sale at 4 yogurts for $5.00.II.The incorrect review (Review X) is incorrect because it misinterprets the information given in the question.The question clearly states that Terry eats 2 yogurts a day, not 4. Therefore, the calculation of how many packs of 4 yogurts Terry needs to buy over 30 days is not relevant to the problem.III.Step #1 is false.The correct calculation should be to determine how many individual yogurts Terry needs to buy over 30 days, which is 2 yogurts per day multiplied by 30 days, resulting in 60 individual yogurts.Therefore, Terry needs to buy 60 individual yogurts over 30 days.RevisionQuestion: Terry eats 2 yogurts a day.They are currently on sale at 4 yogurts for $5.00.How much does he spend on yogurt over 30 days?Let's think step by step.Answer: Original next step #1: First, we need to determine how many packs of 4 yogurts Terry will need to buy over 30 days.To do this, we divide 30 by 4: 30 / 4 = 7.5(Hint: It is not good to directly adopt the step #1 because there is a review says <review> the question states that Terry eats 2 yogurts a day, not 4. Therefore, we cannot assume that Terry needs to buy packs of 4 yogurts.Instead, we need to calculate how many individual yogurts Terry needs to buy over 30 days.Since Terry eats 2 yogurts a day, we multiply 2 by 30: 2 * 30 = 60.Therefore, Terry needs to buy 60 individual yogurts over 30 days.Next, we need to determine the cost of each individual yogurt.The sale price is 4 yogurts for $5.00, so each yogurt costs $5.00 / 4 = $1.25.Finally, we can calculate how much Terry spends on yogurt over 30 days by multiplying the number of yogurts (60) by the cost per yogurt ($1.25): 60 * $1.25 = $75.Therefore, Terry spends $75 on yogurt over 30 days.</review>.)Let's revise for a better version based on the question premises and on the reasoning steps so far.Revision of step #1: First, we need to determine how many individual yogurts Terry will need to buy over 30 days.Since Terry eats 2 yogurts a day, we multiply 2 by 30: 2 * 30 = 60.Next, we need to determine the cost of each individual yogurt.The sale price is 4 yogurts for $5.00, so each yogurt costs $5.00 / 4 = $1.25.Finally, we can calculate how much Terry spends on yogurt over 30 days by multiplying the number of yogurts (60) by the cost per yogurt ($1.25): 60 * $1.25 = $75.Therefore, Terry spends $75 on yogurt over 30 days....F.2. GSM8K Failure CaseInitial Zero-shot-CoTQuestion: Mike plays ping pong for 40 minutes.In the first 20 minutes, he scores 4 points.In the second 20 minutes, he scores 25% more points.How many total points did he score?Let's think step by step.Answer: #1.In the first 20 minutes, Mike scores 4 points.#2.In the second 20 minutes, he scores 25% more points than in the first 20 minutes.#3.To find 25% more points, we can multiply the number of points in the first 20 minutes by 1.25.#4. 4 points * 1.25 = 5 points.#5.In total, Mike scores 4 points + 5 points = &lt;&lt;4+5=9&gt;&gt;9 points.Answer: \boxed{9}....Verification of step #4Question: Mike plays ping pong for 40 minutes.In the first 20 minutes, he scores 4 points.In the second 20 minutes, he scores 25% more points.How many total points did he score?Let's think step by step.Answer: #1.In the first 20 minutes, Mike scores 4 points.#2.In the second 20 minutes, he scores 25% more points than in the first 20 minutes.#3.To find 25% more points, we can multiply the number of points in the first 20 minutes by 1.25.Verification of the next step: #4. 4 points * 1.25 = 5 points.Let's check two different reviews (X and Y).Suport the more plausible one and criticise the other one.Review X: <review> step #4 is TRUE because 25% more than 4 points is 1.25 * 4 = 5 points.</review> Review Y: <review> step #4 is FALSE because the question states that Mike scores 25% more points in the second 20 minutes, not 25% more points than the first 20 minutes.Therefore, we cannot simply multiply the number of points in the first 20 minutes by 1.25 to find the number of points in the second 20 minutes.Instead, we need to calculate 25% of the number of points in the first 20 minutes and add that to the number of points in the first 20 minutes.25% of 4 points is 0.25 * 4 = 1 point.So, in the second 20 minutes, Mike scores an additional 1 point.Therefore, the total number of points scored by Mike is 4 + 1 = 5 points.</review> Let's start by analyzing one by one: I. What are the premises and previous steps to support the verification of step #4? (Your answer should quote exact quote as support.)II.Criticise the incorrect review.(Note the examined step doesn't have to tackle the whole problem at once.)Finally, identify whether step #4 is true or false.Analysis and conclusion: I.The premise to support the verification of step #4 is that in the second 20 minutes, Mike scores 25% more points than in the first 20 minutes.II.The incorrect review (Review Y) criticizes step #4 by stating that we cannot simply multiply the number of points in the first 20 minutes by 1.25 to find the number of points in the second 20 minutes.Instead, it suggests calculating 25% of the number of points in the first 20 minutes and adding that to the number of points in the first 20 minutes.After analyzing both reviews, it is clear that step #4 is FALSE.The correct approach is to calculate 25% of the number of points in the first 20 minutes and add that to the number of points in the first 20 minutes.Therefore, the total number of points scored by Mike is 4 + 1 = 5 points....Revision and ReductionQuestion: Mike plays ping pong for 40 minutes.In the first 20 minutes, he scores 4 points.In the second 20 minutes, he scores 25% more points.How many total points did he score?Let's think step by step.Answer: #1.In the first 20 minutes, Mike scores 4 points.#2.In the second 20 minutes, he scores 25% more points than in the first 20 minutes.#3.To find 25% more points, we can multiply the number of points in the first 20 minutes by 1.25.#4.To find the number of points Mike scores in the second 20 minutes, we need to calculate 25% of the number of points in the first 20 minutes and add that to the number of points in the first 20 minutes.25% of 4 points is 0.25 * 4 = 1 point.So, in the second 20 minutes, Mike scores an additional 1 point.Therefore, the total number of points scored by Mike is 4 + 1 = 5 points.#5.Therefore, Mike scored a total of 5 points.F.3. GSM Failure CaseQuestion: Jerry is rolling a six-sided die.How much more likely is it (expressed as a percentage) that he rolls a number greater than 3 than that he rolls two even numbers in a row?Let's think step by step.Answer: #1.The probability of rolling a number greater than 3 is 3/6 or 1/2, since there are three numbers (4, 5, and 6) that are greater than 3 out of six possible outcomes.#2.The probability of rolling two even numbers in a row is (1/2) x (1/2) or 1/4, since the probability of rolling an even number on a single roll is 1/2 and the probability of rolling another even number on the next roll is also 1/2.#3.To find out how much more likely it is to roll a number greater than 3 than to roll two even numbers in a row, we need to compare the difference between their probabilities.#4.The probability of rolling a number greater than 3 is 1/2, and the probability of rolling two even numbers in a row is 1/4.#5.The difference between these probabilities is 1/2 -1/4 = 1/4.#6.To express this as a percentage, we can multiply by 100 to get 25%.\n\nTherefore, it is 25% more likely that Jerry rolls a number greater than 3 than that he rolls two even numbers in a row.Revision of step #6.... #6.To express this as a percentage, we need to calculate the ratio of the difference between the probabilities to the probability of rolling two even numbers in a row, and then multiply by 100 to get the percentage.\n\nThe difference between the probabilities is 1/4, and the probability of rolling two even numbers in a row is 1/4.Therefore, the ratio is 1/4 \u00f7 1/4 = 1.Multiplying by 100 gives us 100%, which means that it is 100% more likely that Jerry rolls a number greater than 3 than that he rolls two even numbers in a row.\n#7.Therefore, it is twice as likely that Jerry rolls a number greater than 3 than that he rolls two even numbers in a row.Revision of step #7.... #7.Therefore, it is 100% more likely that Jerry rolls a number greater than 3 than that he rolls two even numbers in a row.\n#8.Final Answer: It is 100% more likely that Jerry rolls a number greater than 3 than that he rolls two even numbers in a row.The result is not aligned with the ground truth but is acceptable because it presented the answer with a ratio rather than an absolute value increase.F.4. AQuA Success CaseInitial Zero-shot-CoTAnalyze and answer the following single-choice problem.Question: An aeroplane flies along the four sides of a square at the speeds of 200, 400, 600 and 800km/hr.Find the average speed of the plane around the field?Options: OptA)384 OptB)562 OptC)458 OptD)156 OptE)452Let's think step by step.Answer: #1.To find the average speed, we need to find the total distance traveled and divide it by the total time taken.#2.The distance traveled on each side of the square is equal to the speed multiplied by the time taken.Let's assume the length of each side of the square is "s" km.#3.The time taken to travel each side of the square is equal to the length of the side divided by the speed.So, the time taken for each side is s/200, s/400, s/600, and s/800 hours.#4.The total distance traveled is equal to the sum of the distances traveled on each side of the square.So, the total distance is 4s km.#5.The total time taken is equal to the sum of the times taken for each side of the square.So, the total time is (s/200) + (s/400) + (s/600) + (s/800) hours.#6.Now, we can calculate the average speed by dividing the total distance by the total time.So, the average speed is (4s) / ((s/200) + (s/400) + (s/600) + (s/800)) km/hr.#7.Simplifying the expression, we get the average speed as (4s) / ((3s + 2s + (4/3)s + (1/2)s) / 2400) km/hr.#8.Further simplifying the expression, we get the average speed as (4s) / ((19/6)s / 2400) km/hr.#9.Canceling out the common terms, we get the average speed as (4/1) / ((19/6) / 2400) km/hr.#10.Simplifying the expression, we get the average speed as (4/1) * (2400.Rounding the answer to the nearest whole number, we get the average speed as 337 km/hr.#17.None of the given options match the calculated average speed....Revision and Re-deductionAnalyze and answer the following single-choice problem.Question: An aeroplane flies along the four sides of a square at the speeds of 200, 400, 600 and 800km/hr.Find the average speed of the plane around the field?Options: OptA)384 OptB)562 OptC)458 OptD)156 OptE)452Let's think step by step.Answer: #1.To find the average speed, we need to find the total distance traveled and divide it by the total time taken.#2.The distance traveled on each side of the square is equal to the speed multiplied by the time taken.Let's assume the length of each side of the square is "s" km.#3.The time taken to travel each side of the square is equal to the length of the side divided by the speed.So, the time taken for each side is s/200, s/400, s/600, and s/800 hours.#4.The total distance traveled is equal to the sum of the distances traveled on each side of the square.So, the total distance is 4s km.#5.The total time taken is equal to the sum of the times taken for each side of the square.So, the total time is (s/200) + (s/400) + (s/600) + (s/800) hours.#6.Now, we can calculate the average speed by dividing the total distance by the total time.So, the average speed is (4s) / ((s/200) + (s/400) + (s/600) + (s/800)) km/hr.#7.To simplify this expression, we need to find a common denominator for the fractions in the denominator.The common denominator is 2400.F.5. AQuA Failure CaseInitial Zero-shot-CoTAnalyze and answer the following single-choice problem.Question: Roy was suffering from severe headaches.He went to see his doctor and the doctor gave him 5 tablets asking him to take one tablet every 15 minutes.How much time will it take Roy to consume all the 5 tablets?Options: OptA)45 Min OptB)75 Min OptC)90 Min OptD)120 Min OptE)60 Min Let's think step by step.Answer: #1.Roy needs to take 5 tablets.#2.He needs to take one tablet every 15 minutes.#3.To calculate the total time, we need to multiply the number of tablets by the time it takes to consume one tablet.#4. 5 tablets x 15 minutes = 75 minutes.Therefore, the correct answer is OptB) 75 Min.In this case, all of the reasoning steps passed the verification, such that the answer by LoT is consistent with CoT.However, they are both not correct.The ground truth is OptE) 60 Min, with an explanation that "Tablet 1 will be taken in 0 min...".Because there is no message for this hidden commonsense knowledge, neither CoT nor LoT can realize it.F.6. Date Understanding Success CaseInitial Zero-shot-CoT Question: Jane and John married on Jan 2, 1958.Today is their golden wedding anniversary.What is the date one week ago from today in MM/DD/YYYY?Let's think step by step.Answer: #1.First, we need to determine the date of their golden wedding anniversary.To do this, we need to add 50 years to their wedding date.Jan 2, 1958 + 50 years = Jan 2, 2008 #2.Next, we need to determine the date one week ago from today.To do this, we subtract 7 days from today's date.Today's date -7 days = (MM/DD/YYYY)Therefore, the date one week ago from today in MM/DD/YYYY format is (MM/DD/YYYY).Revision and Re-deductionQuestion: Jane and John married on Jan 2, 1958.Today is their golden wedding anniversary.What is the date one week ago from today in MM/DD/YYYY?Let's think step by step.Answer: #1.First, we need to determine the date of their golden wedding anniversary.To do this, we need to add 50 years to their wedding date.Jan 2, 1958 + 50 years = Jan 2, 2008 #2.Next, we need to determine the date one week ago from today.To do this, we need to subtract 7 days from the date of their golden wedding anniversary.Jan In this case, CoT successfully applies the commonsense knowledge that a "golden wedding anniversary" means fifty years.However, the language model fails to generate a meaningful answer as a conclusion while the revision by LoT leads to a correct answer.This failure of LoT showcases the importance of correctly identifying the more plausible review, especially when both reviews claim statements but with established facts as support.Apparently, there is no clear reason to support "Review Y" -the post hoc explanation, in this case, is just a hallucination of the LLM but it sometimes fools the LLM itself.F.7. Date Understanding Failure Case
Symbolic Logic: Syntax, Semantics, and Proof. Bibliographical References, David Agler, 2012Rowman &amp; Littlefield PublishersLanham, Md</p>
<p>Do As I Can. Anthony Michael Ahn, Noah Brohan, Yevgen Brown, Omar Chebotar, Byron Cortes, Chelsea David, Chuyuan Finn, Keerthana Fu, Karol Gopalakrishnan, Alex Hausman, Daniel Herzog, Jasmine Ho, Julian Hsu, Brian Ibarz, Alex Ichter, Eric Irpan, Rosario Jang, Kyle Jauregui Ruano, Sally Jeffrey, Jesmonth, J Nikhil, Ryan Joshi, Dmitry Julian, Yuheng Kalashnikov, Kuang-Huei Kuang, Sergey Lee, Yao Levine, Linda Lu, Carolina Luu, Peter Parada, Jornell Pastor, Kanishka Quiambao, Jarek Rao, Diego Rettinghouse, Pierre Reyes, Sermanet, 10.48550/arXiv.2204.01691arXiv:2204.01691):2204.01691Grounding Language in Robotic Affordances. Nicolas Sievers, Clayton Tan, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Mengyuan Yan, Andy Zeng, Not As I Say2022</p>
<p>Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, Quyet V Do, Yan Xu, Pascale Fung, 10.48550/arXiv.2302.04023arXiv:2302.04023):2302.04023A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity. 2023</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, Yi Zhang, 10.48550/arXiv.2303.12712arXiv:2303.12712Sparks of Artificial General Intelligence: Early experiments with GPT-4. 2023. 127122303</p>
<p>Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An open-source chatbot impressing GPT-4 with 90%* ChatGPT quality. </p>
<p>Antonia Creswell, Murray Shanahan, 10.48550/arXiv.2208.14271arXiv:2208.14271Faithful Reasoning Using Large Language Models. 2022</p>
<p>Selection-inference: Exploiting large language models for interpretable logical reasoning. Antonia Creswell, Murray Shanahan, The Eleventh International Conference on Learning Representations. 2023and Irina Higgins</p>
<p>Artur , Avila Garcez, Luis C Lamb, arXiv:2012.05876Neurosymbolic AI: The 3rd Wave. 2020</p>
<p>Large language models cannot self-correct reasoning yet. Jie Huang, Xinyun Chen, Swaroop Mishra, Steven Huaixiu, Adams Wei Zheng, Xinying Yu, Denny Song, Zhou, The Twelfth International Conference on Learning Representations. 2024. 017982310</p>
<p>Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations. Jaehun Jung, Lianhui Qin, Sean Welleck, Faeze Brahman, Chandra Bhagavatula, Ronan Le Bras, Yejin Choi, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022</p>
<p>Large Language Models are Zero-Shot Reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in Neural Information Processing Systems. 2022</p>
<p>Harrison Lee, Samrat Phatale, Hassan Mansoor, Thomas Mesnard, Johan Ferret, Kellie Lu, Colton Bishop, Ethan Hall, Victor Carbune, Abhinav Rastogi, Sushant Prakash, 10.48550/arXiv.2309.00267arXiv:2309.00267):2309.00267RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback. 2023</p>
<p>. Mengdi Li, Xufeng Zhao, Kun Chu, Wenhao Lu, Awesome RLAIF. 2023a</p>
<p>Making Large Language Models Better Reasoners with Step-Aware Verifier. Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, Weizhu Chen, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational Linguistics2023b1</p>
<p>Let's verify step by step. Vineet Hunter Lightman, Yuri Kosaraju, Harrison Burda, John Edwards ; Leike, Ilya Schulman, Karl Sutskever, Cobbe, arXiv:2305.20050The Twelfth International Conference on Learning Representations. Bowen Baker, Teddy LeeJan. 2024. 200502305</p>
<p>Deductive verification of chain-of-thought reasoning. Zhan Ling, Yunhao Fang, Xuanlin Li, Zhiao Huang, Mingu Lee, Roland Memisevic, Hao Su, Thirty-Seventh Conference on Neural Information Processing Systems. 2023</p>
<p>Selfrefine: Iterative refinement with self-feedback. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, Peter Clark, Thirty-Seventh Conference on Neural Information Processing Systems. 2023</p>
<p>Improving coherence and consistency in neural sequence models with dual-system, neuro-symbolic reasoning. Maxwell Nye, Michael Tessler, Josh Tenenbaum, Brenden M Lake, Advances in Neural Information Processing Systems. 202134</p>
<p>Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, 10.48550/arXiv.2203.02155arXiv:2203.02155):2203.02155Training language models to follow instructions with human feedback. Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul ChristianoJan Leike, and Ryan Lowe. 2022</p>
<p>Neuro-symbolic artificial intelligence. Lu Md Kamruzzaman Sarker, Aaron Zhou, Pascal Eberhart, Hitzler, 10.3233/AIC-210084AI Communications. 3432021</p>
<p>Self-critiquing models for assisting human evaluators. William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, 10.48550/arXiv.2206.05802arXiv:2206.05802Jan Leike. 20225802Jonathan Ward</p>
<p>Toolformer: Language models can teach themselves to use tools. Timo Schick, Jane Dwivedi-Yu, Roberto Dessi, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom, Thirty-Seventh Conference on Neural Information Processing Systems. 2023</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Aurelien Azhar, Armand Rodriguez, Edouard Joulin, Guillaume Grave, Lample, 10.48550/arXiv.2302.13971arXiv:2302.13971LLaMA: Open and Efficient Foundation Language Models. 2023. 139712302</p>
<p>Self-Consistency Improves Chain of Thought Reasoning in Language Models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Ed H Quoc V Le, Sharan Chi, Aakanksha Narang, Denny Chowdhery, Zhou, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H Chi, V Quoc, Denny Le, Zhou, Advances in Neural Information Processing Systems. 2022</p>
<p>Generating Sequences by Learning to Self-Correct. Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi, Yejin Choi, 10.48550/arXiv.2211.00053The Eleventh International Conference on Learning Representations. 2023</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, Karthik R Narasimhan, Thirty-Seventh Conference on Neural Information Processing Systems. 2023a</p>
<p>ReAct: Synergizing Reasoning and Acting in Language Models. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, R Karthik, Yuan Narasimhan, Cao, The Eleventh International Conference on Learning Representations. 2023b</p>
<p>Chat with the Environment: Interactive Multimodal Perception using Large Language Models. Xufeng Zhao, Mengdi Li, Cornelius Weber, Muhammad Burhan Hafez, Stefan Wermter, 10.48550/arXiv.2303.082682023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). 2023</p>
<p>Chuanyang Zheng, Zhengying Enze Xie, Zhenguo Li, Yu Li, 10.48550/arXiv.2304.09797arXiv:2304.09797):2304.09797Progressive-Hint Prompting Improves Reasoning in Large Language Models. 2023</p>
<p>Least-to-most prompting enables complex reasoning in large language models. Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Ed H Quoc V Le, Chi, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Language Resource, References Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, 10.48550/arXiv.2110.14168arXiv:2110.14168Training Verifiers to Solve Math Word Problems. 2021. 141682110</p>
<p>Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems. Wang Ling, Dani Yogatama, Chris Dyer, Phil Blunsom, 10.18653/v1/P17-1015Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaAssociation for Computational Linguistics20171</p>
<p>Aarohi Srivastava, Abhinav Rastogi, and et. al. 2023. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Sabrina J Mielke, Zaid Alyafeai, Elizabeth Salesky, Colin Raffel, Manan Dey, Matthias Gallé, Arun Raja, Chenglei Si, Wilson Y Lee, Benoît Sagot, Samson Tan, arXiv:2112.10508):2112.10508Transactions on Machine Learning Research. 2021Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1979 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1979</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1979</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-40.html">extraction-schema-40</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <p><strong>Paper ID:</strong> paper-280686378</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2508.13998v1.pdf" target="_blank">Embodied-R1: Reinforced Embodied Reasoning for General Robotic Manipulation</a></p>
                <p><strong>Paper Abstract:</strong> Generalization in embodied AI is hindered by the"seeing-to-doing gap,"which stems from data scarcity and embodiment heterogeneity. To address this, we pioneer"pointing"as a unified, embodiment-agnostic intermediate representation, defining four core embodied pointing abilities that bridge high-level vision-language comprehension with low-level action primitives. We introduce Embodied-R1, a 3B Vision-Language Model (VLM) specifically designed for embodied reasoning and pointing. We use a wide range of embodied and general visual reasoning datasets as sources to construct a large-scale dataset, Embodied-Points-200K, which supports key embodied pointing capabilities. We then train Embodied-R1 using a two-stage Reinforced Fine-tuning (RFT) curriculum with a specialized multi-task reward design. Embodied-R1 achieves state-of-the-art performance on 11 embodied spatial and pointing benchmarks. Critically, it demonstrates robust zero-shot generalization by achieving a 56.2% success rate in the SIMPLEREnv and 87.5% across 8 real-world XArm tasks without any task-specific fine-tuning, representing a 62% improvement over strong baselines. Furthermore, the model exhibits high robustness against diverse visual disturbances. Our work shows that a pointing-centric representation, combined with an RFT training paradigm, offers an effective and generalizable pathway to closing the perception-action gap in robotics.</p>
                <p><strong>Cost:</strong> 0.031</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1979.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1979.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Embodied-R1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Embodied-R1: Reinforced Embodied Reasoning for General Robotic Manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 3B-parameter vision-language model trained with a two-stage Reinforced Fine-Tuning (RFT) curriculum to produce point-centric grounding outputs (single points and ordered point traces) that act as embodiment-agnostic intermediates between vision-language reasoning and robot motion execution.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Embodied-R1</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Multimodal model built on the Qwen2.5-VL architecture: a Vision Transformer (ViT) visual encoder → projector → autoregressive LLM. The model performs explicit chain-of-thought reasoning (<think> tags) and outputs text-encoded point commands (<point> [[x,y],...]</point>). Grounding is done by producing 2D pixel coordinates (or RGB-D 3D coordinates in the RGBD variant) that are verified against masks/regions or executed in simulation, with RL rewards shaping multi-task pointing behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>Vision Transformer (ViT) visual encoder (as in Qwen2.5-VL) plus a projector to the LLM</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>Backbone initialized from Qwen2.5-VL (Qwen2.5-VL-3B-Instruct); explicit pretraining dataset names are not enumerated in the paper beyond the Qwen2.5-VL backbone reference</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Point-centric grounding: the model grounds language by predicting pixel coordinates or sequences of coordinates tied to semantic entities (points must fall within object masks or target placement regions). Grounding is learned via RFT using task-specific reward functions (point-in-mask, distance, trajectory RMSE, environment execution) rather than direct supervision of low-level actions.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>Object-centric / pixel-level points and ordered visual traces (multi-point trajectories); also supports region-level placement via point-in-region verification; has an RGB-D variant with depth value per point (3D).</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>Explicit 2D pixel coordinates (p=(x,y) in image space); optional depth channel for (X,Y,D) in RGB-D variant; object masks used for verification; object-centric visual traces (ordered sequences of points).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>Object manipulation (robotic manipulation / affordance placement / trajectory generation)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>SIMPLEREnv (simulated control), 8 real-world xArm tabletop manipulation tasks (xArm experiments), plus spatial/pointing benchmarks (RoboRefit/RoboRef, Where2Place, VABench-P/V, Part-Afford, Open6DOR-Position)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>Photorealistic simulation (SIMPLEREnv, Isaac Gym synthetic scenes) and real-world third-person tabletop views (RealSense L515 on xArm), also web images for REG sources</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success rate for manipulation tasks; accuracy for point-in-region tasks; RMSE/MAE and LLM-based scores for visual-trace trajectory similarity</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Zero-shot SIMPLEREnv average success rate 56.2%; zero-shot real-world xArm tasks 87.5% success (8 tasks); VABench-V: lowest RMSE/MAE among compared VLM baselines (precise table entries in paper); Open6DOR-Position: Embodied-R1-RGBD overall 90.2, Embodied-R1-RGB 66.8 (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td>SFT baseline (Embodied-SFT) and SFT variants: e.g., Table 8 (RRG benchmarks) shows SFT w/o Think: Where2Place 36.85, VABench-P 50.46 versus RL w/ Think: Where2Place 65.50, VABench-P 65.39.</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>Compared to SFT w/o CoT, RL w/ Think improved Where2Place by +28.65 points and VABench-P by +14.93 points (absolute percentage points in Table 8); overall ~62% improvement over strong baselines on zero-shot xArm tasks (paper claim).</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td>Paper compares Embodied-R1-RGB vs Embodied-R1-RGBD: on Open6DOR-Position Embodied-R1-RGBD achieves 90.2 overall while Embodied-R1-RGB achieves 66.8, showing large benefit from explicit depth for 3D positioning; but authors note RGB-D may hallucinate depth in high relational-complexity cases and sometimes underperform the 2D variant at Level 1 relations.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Authors identify the 'seeing-to-doing gap' caused by (1) data scarcity for embodied grounding and (2) embodiment heterogeneity across robots. They also report noisy upstream vision models in the VTG pipeline (incorrect grounding, incomplete tracking) and potential 'depth hallucinations' in RGB-D inputs that can reduce performance at higher relational complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Documented failure modes: (a) general VLMs (e.g., Qwen2.5-VL, GPT-4o) perform poorly on pointing tasks (qualitative/quantitative), (b) pre-trained vision-model noise introduces incorrect object grounding/tracking in VTG data-generation causing dataset noise, (c) reward-hacking in VTG where the model outputs only start/end points (mitigated by requiring exactly 8 points), (d) baseline methods fail on spatial-reasoning tasks and rigid/difficult-to-grasp objects (e.g., screwdriver, moka pot), (e) RGB-D can produce hallucinated depth leading to errors at high relational complexity (noted qualitatively). Specific frequencies: Table 8 and task tables quantify performance drops (see SFT v RL and baseline comparisons); Table 7 disturbance tests show drop from 100% to 83% under combined background+light+height changes.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>Mixed-data training and pointing abstraction: Embodied-R1 is trained on both large web image/regression datasets and embodied datasets (Embodied-Points-200K, Embodied-Spatial-84K, ViRL-subset-18K) to bridge cyberspace↔physical domains. Pointing is used as an embodiment-agnostic intermediate to reduce the effect of robot morphology differences; RFT (reward-driven) handles multi-solution labeling. Empirically, model shows robust zero-shot sim-to-real transfer and robustness to visual disturbances (Table 7).</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td>Robust zero-shot on unseen real-world objects: xArm real-world OOD tasks achieved 87.5% overall success across 8 tasks with objects/scenes unseen during training. Also successful zero-shot VTG predictions on unseen simulators (LIBERO, ManiSkill) per qualitative results.</td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Visual tokens from ViT are projected via a projector into the LLM's multimodal input (Qwen2.5-VL style); the model autoregressively generates text responses that include structured point outputs. In practice this is a visual→projector→LLM fusion (cross-modal conditioning inside the LLM).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Training uses ~200K embodied-point samples (Embodied-Points-200K) plus Embodied-Spatial-84K and ViRL-subset-18K; two-stage RFT: stage1 2 epochs, stage2 1 epoch (≈48 hours per stage on 8 A100s). Paper reports RL generalizes better than SFT trained on same data (Embodied-SFT trained 3 epochs) indicating higher effective generalization/sample efficiency for RL in multi-solution tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Pointing as an intermediate, embodiment-agnostic grounding representation (single points and ordered visual traces) combined with reinforced fine-tuning (RFT) and multi-task rewards yields substantial zero-shot improvement in robotic manipulation; RFT handles multi-solution labeling better than SFT, mixed multi-task training shares coordinate/semantic priors across tasks, depth input improves 3D position accuracy but can hallucinate, and careful reward/format constraints are crucial to avoid reward-hacking (VTG).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1979.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1979.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pointing (representation)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Point-centric (Pointing) Representation for Embodied Grounding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A unified, embodiment-agnostic intermediate representation where grounding is expressed as 2D pixel coordinates or ordered point sequences (visual traces), encoding object identity, affordances, target placements, and implicit execution trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Pointing representation (Embodied intermediate)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Not a model but a representation: tasks are reframed to require predicting coordinates p=(x,y) inside object masks (REG/OFG), in free-space placement regions (RRG), or an ordered set τ of points for VTG. These textual point outputs are parsed and verified during training via reward functions (point-in-mask, distance-to-center, RMSE to ground-truth trajectory, or simulator environment execution).</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>N/A (representation-level choice; relies on ViT visual features from backbone models)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Language is grounded by mapping instruction-derived referents to pixel coordinates or ordered traces in image space; grounding is learned via multi-task rewards rather than direct action regression.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>Pixel-level/object-centric points; sequence-level for visual traces (VTG); region-referenced implicitly via point-in-region.</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>Explicit 2D pixel coordinates, optional depth values for RGB-D variant, and ordered sequences representing object-centric trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>Object manipulation, placement, trajectory generation</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>REG (Referring Expression Grounding), RRG (Region Referring Grounding), OFG (Object Functional Grounding), VTG (Visual Trace Generation) benchmarks (RoboRefit/RoboRef, Where2Place, VABench-P/V, Part-Afford, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>Web images, simulation images, and real-world third-person tabletop camera views</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Point-in-mask accuracy, placement accuracy, RMSE/MAE for trajectory similarity, task success rate when plugged into motion planner</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Using pointing as representation, Embodied-R1 achieves state-of-the-art on 11 pointing/spatial benchmarks and enables 56.2% SIMPLEREnv sim success and 87.5% zero-shot xArm real-world success when paired with motion executors.</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td>General VLMs without pointing-specialized training (GPT-4o, Qwen2.5-VL) perform poorly on pointing tasks — qualitative and quantitative drops reported in benchmark tables (Embodied-SFT and baseline rows show lower point-in-region accuracies).</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>Pointing-centric training plus RFT yields large improvements vs general VLM baselines and SFT variants (see Embodied-R1 vs Embodied-SFT and vs RoboPoint/FSD baseline numbers; paper cites 62% improvement over strong baselines on some zero-shot metrics).</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Pointing helps decouple perception from embodiment by providing a common interface, but upstream perception errors (segmentation/tracking depth hallucinations) can corrupt point targets; the paper documents noise injected by pre-trained vision models during VTG generation.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Point-space failure modes include multi-solution ambiguity (many valid points) causing SFT memorization; reward-hacking on VTG where model outputs trivial 2-point lines unless constrained; noisy tracker outputs produce incorrect traces that must be filtered; small or occluded targets can reduce point-in-mask accuracy (reported qualitatively).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>Pointing is used to mitigate domain shift by enabling training on web and embodied datasets and by being embodiment-agnostic, improving sim-to-real transfer in zero-shot tests.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td>Pointing supports generalization to novel objects as shown by zero-shot performance on unseen tasks and environments (LIBERO, ManiSkill simulations and real xArm tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Points are output as structured text by the LLM conditioned on visual features; the verifier/reward compares these coordinates to masks/regions or simulated execution.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Pointing representation reduces need for embodiment-specific action labels and supports learning from combined web + embodied datasets (paper trains on Embodied-Points-200K), improving multi-task sharing.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>A point-centric intermediate effectively anchors language to scene entities and actions, enabling cross-embodiment zero-shot control when combined with RL-trained reasoning and proper reward design, but depends on reliable upstream perception (segmentation/tracking/depth).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1979.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1979.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RFT (GRPO)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reinforced Fine-Tuning (RFT) using GRPO</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reinforcement learning fine-tuning paradigm (GRPO algorithm variant) used to optimize an autoregressive VLM with task-specific reward functions over generated point outputs, addressing multi-solution labeling and encouraging generalizable embodied reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Reinforced Fine-Tuning (RFT) with GRPO</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>RFT trains the multimodal policy π_θ(y|x) by maximizing expected reward E_x E_y[r(y,x)] using GRPO: generate G candidate responses from the behavior policy, compute normalized advantages and apply a clipped surrogate loss (PPO-like) with KL penalty. Rewards are task-specific (format, accuracy, point-in-mask, point-distance, trajectory RMSE, simulator environment reward).</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>N/A (applies to the multimodal LLM pipeline using ViT+projector+LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Learning to ground via reward signals: points are evaluated by geometric/semantic verifiers (mask membership, distances, RMSE, simulation success) and these scalar rewards supervise autoregressive generation via policy gradient-style optimization (GRPO).</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>Trains at pixel-level/object-centric point outputs and sequence-level VTG outputs</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>2D pixel coordinates (and depth when provided); rewards operate on these coordinates relative to masks/target centroids or trajectory ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>Object manipulation grounding tasks (REG/RRG/OFG/VTG) and downstream robot control when points are executed via motion planners</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Used to train for REG, RRG, OFG, VTG tasks and evaluated on SIMPLEREnv and xArm real-world manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>Web, simulated and real-world images (mixed during training and evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Task-specific reward (normalized 0-1), downstream success rate and benchmark accuracies/trajectory RMSE</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>RFT (RL w/ Think) outperforms SFT in RRG benchmarks (Table 8: Where2Place 65.50 vs SFT w/o Think 36.85; VABench-P 65.39 vs 50.46) and improves zero-shot manipulation success in SIMPLEREnv and real xArm tasks (paper-level comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td>SFT w/ CoT and SFT w/o CoT variants show lower performance across RRG benchmarks (e.g., SFT w/ Think 41.25 Where2Place, SFT w/o Think 36.85).</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>RFT with free-form CoT yields the best scores in ablations; absolute improvements over SFT variants on Where2Place up to +28.65 points (Table 8) and overall stronger OOD generalization vs SFT.</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>RFT helps alleviate multi-solution supervision problems that cause SFT memorization; however, it still depends on accurate perceptual verification (masks, tracker outputs) which can be noisy and become a bottleneck.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Reward hacking observed in VTG (model outputs minimal points that match start/end to gain reward); fixed by adding format constraints (exact 8 points) and format reward r_format; multi-task reward dominance solved by normalizing task rewards to [0,1] and weight normalization.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>RFT trains on mixed cyberspace and embodied datasets and explicitly optimizes for simulator/environment rewards for some RRG samples (r_env), aiding domain adaptation and sim-to-real transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td>RFT-trained model generalizes to novel objects/environments better than SFT in evaluations (e.g., zero-shot xArm tests and unseen simulation VTG cases).</td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Policy updates operate on the autoregressive text output that encodes coordinates; visual features condition the policy via the LLM's multimodal input—no additional fusion mechanism is introduced beyond the Qwen2.5-VL style visual→projector→LLM pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>RFT uses a 'question-verification' style dataset (Embodied-Points-200K) and provides positive reward for any valid solution, which the authors argue yields better utilization of multi-solution data than SFT.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>RFT (GRPO) combined with structured rewards and chain-of-thought reasoning yields substantially better grounding generalization (point predictions and trace generation) than SFT, especially under multi-solution semantics and OOD conditions; reward and format constraints are critical to prevent degenerate solutions.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1979.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1979.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Embodied-Points-200K</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Embodied-Points-200K dataset</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A curated dataset of ~200k question-verification samples for embodied pointing tasks covering REG, RRG, OFG, and VTG, designed for RFT training using verifiable rewards rather than strict Q→A pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Embodied-Points-200K (training dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Dataset contains point-centric annotations and verification methods: REG points inside segmentation masks (sources: RefCOCO, RoboRef, Robo-Point), RRG relational placement points (extracted from ~1M embodied examples plus Isaac Gym synthetic 3D rearrangements), OFG functional grasp points (HandAL-derived 40k points), and VTG object-centric visual traces (extracted via keypoint proposal, Cotracker3 tracking, downsampled to 8 points).</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>N/A (dataset used to train ViT+LLM pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Provides paired inputs and verifiers: model outputs point(s) in text; verifier computes rewards (point-in-mask, distance-to-center, RMSE for traces, env simulation). Used with RFT to shape grounding behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>Point-level and ordered-point-sequence (VTG) annotations projected back onto initial images; also includes RGB-D entries for 3D RRG.</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>2D pixel coordinates, 3D (X,Y,D) for simulated 3D RRG (depth normalized), segmentation masks for verification, and 8-point visual trace sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>Training data for REG, RRG, OFG, VTG embodied manipulation grounding tasks</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Dataset supports training for the benchmarks used in paper: RoboRefit, Where2Place, VABench-P/V, Part-Afford, Open6DOR, SIMPLEREnv RRG/3D RRG, VTG tasks</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>Aggregated from web images (RefCOCO), real-world embodied datasets, Isaac Gym synthetic 3D renders, and video trackers (for VTG)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Used to compute reward signals during RFT (point-in-mask boolean, distance reward 0-1, trajectory RMSE→normalized trace reward, env binary sim success)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Dataset size ~200,000 samples; specific per-task splits enumerated in paper (e.g., 33k high-quality RRG samples from pipeline, 40k OFG points from HandAL-derived annotations).</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>Not quantified as a standalone ablation but credited as enabling the multi-task RFT training and better generalization than SFT on pointing tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Authors note the VTG generation pipeline uses multiple pre-trained vision models (keypoint extractors, Grounded-SAM, Cotracker3) which introduce noise; they apply rule-based filtering and manual validation to reduce noisy samples.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Dataset construction required heavy filtering because tracker/grounding errors and occlusions can produce invalid traces; reward-driven training mitigates multi-solution annotation issues but relies on clean verifiers.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>Dataset intentionally mixes cyberspace (web) and embodied/simulated examples to improve domain robustness of pointing supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td>Designed to support generalization; when used to train Embodied-R1 via RFT, enabled strong zero-shot performance on novel objects in simulation and real-world.</td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>N/A (dataset supplies target point labels and verification functions used inside RL reward pipeline).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Large-scale (200K) dataset created to address data scarcity; RFT training uses these samples plus spatial reasoning datasets to obtain strong generalization in few epochs (2+1 epoch schedule).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>A large, verification-style, point-centric dataset allows RL-based training to reward any valid solution (mitigating multi-solution overfitting) and is critical to learn robust grounding and visual-trace generation that transfer zero-shot to robots.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1979.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1979.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pointing abilities (REG/RRG/OFG/VTG)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Four embodied pointing abilities: Referring Expression Grounding (REG), Region Referring Grounding (RRG), Object Functional Grounding (OFG), Visual Trace Generation (VTG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Defined core capabilities for embodied pointing: REG localizes described objects with points inside segmentation masks; RRG identifies free-space placement points relative to other objects; OFG selects functionally manipulable part points; VTG outputs ordered object-centric trajectories (8-point traces) for manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>REG / RRG / OFG / VTG (capability set learned by Embodied-R1)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Each ability is implemented as a prompt+output format that elicits point coordinates from the LLM conditioned on the visual encoder; verification-reward functions differ per ability (point-in-mask for REG/OFG, mask+distance/env for RRG, RMSE-based trajectory reward for VTG). VTG uses multi-model pipelines for data generation and enforces an 8-point output constraint during training.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>ViT visual encoder (backbone Qwen2.5-VL) used to extract features for all abilities</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>Backbone initialized from Qwen2.5-VL; task-specific data provided via Embodied-Points-200K and Embodied-Spatial-84K</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Direct output of semantic pixel coordinates tied to language referents; verification is geometric/semantic (masks, relations, distance, trajectory RMSE); RRG optionally uses simulator environment reward for 3D placement correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>Object-centric / region-level / sequence-level depending on ability (REG/OFG point-in-mask; RRG free-space placement point; VTG ordered sequence of points forming an object-centric trajectory).</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>2D pixel coordinates (p=(x,y)), VTG sequences of 8 points, and 3D (X,Y,D) for 3D RRG when depth is available.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>Referring grounding, placement/region grounding, affordance localization, trajectory generation for manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Benchmarks: RoboRefit (REG), Where2Place & VABench-P (RRG), Part-Afford (OFG), VABench-V (VTG), Open6DOR-Position (3D RRG)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>Web images, simulation (Isaac Gym), real-world tabletop third-person camera imagery</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Point-in-mask accuracy for REG/OFG/RRG; RMSE/MAE and LLM score for VTG; environment success for 3D RRG executed in simulator.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Embodied-R1 achieves SOTA on REG/RRG/OFG/VTG benchmarks reported; concrete examples: VTG improved RMSE/MAE over baselines (see Table 3); RRG Where2Place 65.50 with RL w/ Think (Table 8).</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td>SFT and unguided model variants show large performance drops on these abilities (see Table 8 and comparisons to GPT-4o/Qwen2.5-VL and specialized baselines RoboPoint/FSD).</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>Multi-task RFT training of these abilities yields consistent improvements vs unmixed (single-ability) training (Table 9 shows mixed training improves e.g., Where2Place 69.50 vs 65.50 unmixed), and RL vs SFT dramatically boosts RRG performance (Table 8).</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td>3D RRG (Open6DOR): Embodied-R1-RGBD outperforms RGB-only variant substantially on 3D position tasks (90.2 vs 66.8 overall in Table 4), but authors warn depth can hallucinate for complex relations.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>REG and OFG rely on accurate segmentation masks; VTG relies on robust tracking; RRG 3D requires accurate depth—errors in these perceptual components are called out as sources of dataset noise and performance failure.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>REG/OFG failures tied to segmentation/occlusion; RRG failures tied to depth hallucination and simulator mismatch; VTG failures include tracking errors and reward-hacking (insufficient points). Paper reports mitigation strategies (filtering, format constraints, dense distance rewards).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>Abilities are trained on mixed sources (web, simulation, embodied datasets) and RFT encourages generalization; object-centric VTG traces support embodiment-agnostic transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td>All four abilities generalize to novel objects/environments per qualitative and quantitative zero-shot evaluations (e.g., successful VTG on unseen LIBERO/ManiSkill scenes and xArm tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>LLM-conditioned outputs generated from projected ViT features; ability-specific prompts and format rewards steer output structure.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Joint mixed-task training (all abilities together) improves success rates over isolated training; Table 9 shows mixed training outperforms unmixed on Part-Afford, Where2Place, VABench-P.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Decomposing grounding into these four pointing abilities provides a compact, shared coordinate space that enables multi-task learning, better generalization across embodiments, and direct interfacing with motion planners; success depends on reliable perception (masks/tracking/depth) and careful reward/formatted-output constraints.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1979.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1979.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reward design & VTG fixes</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-task Reward Design and VTG-specific constraints (format rewards, point-number constraint)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A library of verifiable reward components (format, accuracy, point-in-mask, distance, trajectory RMSE, environment reward) combined as normalized weighted sums per-task to stabilize multi-task RL; includes specific fixes to avoid VTG reward-hacking (require exactly 8 points in trace).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Reward library (r_format, r_acc, r_mask, r_dis, r_trace, r_env)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Each predicted text response is parsed and evaluated by task-specific reward terms (format validity, answer accuracy, mask membership, dense distance reward to center, RMSE-based trace similarity, simulator execution). Per-task weights are normalized so total ℛ ∈ [0,1]. VTG enforces format and 8-point constraint; failure to satisfy format sets reward to zero.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>N/A (reward-level mechanism operating on model outputs and visual verifiers)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Rewards directly supervise grounding outputs by verifying predicted coordinates against masks/ground-truth trajectories or by executing predictions in a simulator for env rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>Applies to point outputs and sequence outputs</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>Distance and RMSE computed in pixel-space (or normalized depth units for RGB-D) to produce dense shaping rewards</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>REG/RRG/OFG/VTG and simulator-executed placement tasks</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>RRG (including Isaac Gym 3D RRG with simulator env rewards), VTG (VABench-V), REG/OFG benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>Mixed (web, simulation, real-world)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Normalized reward (0-1) used to drive policy updates; downstream metrics (success rate, RMSE) reported in benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Specific per-task reward mixes: e.g., ℛ_RRG = 0.1 r_format + 0.6 r_mask + 0.3 r_dis; ℛ_VTG = 0.1 r_format + 0.9 r_trace. VTG point-number constraint reduced RMSE from 105.25→77.83 (Table 11) and MAE from 59.75→44.97, and increased GPT Score.</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td>Without the VTG 8-point format constraint, models learned degenerate 2-point solutions: VABench-V RMSE rose to 105.25 and MAE to 59.75 vs with constraint RMSE 77.83 MAE 44.97 (Table 11).</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>Format and point-number constraints significantly improved VTG fidelity (RMSE/MAE improvements as above); normalizing task rewards prevented simpler tasks from dominating mixed-task training.</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Reward computation depends on correct masks/trajectories; noisy perceptual labels degrade reward signals and can mislead RL optimization, requiring rigorous filtering of training data.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Reward hacking (VTG two-point solutions) and dominance of simpler tasks in mixed RL if rewards are not normalized; both addressed by format enforcement and normalized per-task reward weights.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>Format-based rewards are domain-agnostic and help when mixing web/sim/real data; env rewards used only on simulator-sourced RRG samples.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td>Robust reward design contributed to zero-shot success on unseen objects/environments by incentivizing generalizable point predictions rather than memorization.</td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Rewards act on parsed coordinate outputs after LLM decoding; not a fusion mechanism per se.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Dense auxiliary rewards (r_dis, r_trace) accelerate learning by providing shaped signals rather than sparse only env rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Carefully designed, verifiable reward components and strict output format constraints are essential to steer RL-based grounding away from degenerate solutions and to stabilize multi-task training across heterogeneous pointing tasks.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1979.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1979.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Action Executors (P & V branches)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Affordance Points Branch (-P) and Visual Traces Branch (-V) action executors</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Two downstream execution pipelines that convert point outputs into robot motions: (P) generate key grasp/place points fed to a motion planner (CuRobo) and (V) map 2D visual traces to 3D SE(3) trajectories via pinhole camera projection and depth interpolation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Affordance Points Branch and Visual Traces Branch</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Affordance Points Branch: Embodied-R1 predicts grasp and placement points (OFG/RRG), CuRobo planner computes collision-free SE(3) paths to those affordance points. Visual Traces Branch: VTG outputs 2D object-centric traces; these are lifted to 3D using pinhole camera model and depth, interpolated into continuous SE(3) trajectories for execution (similar to FSD-style trace following).</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>ViT backbone used to produce the points/trace; execution uses external motion planning (CuRobo) and geometric projection (pinhole model).</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>Qwen2.5-VL backbone initialization used during training of point predictors</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Grounding outputs are 2D/3D points that act as the interface to motion planners; success depends on correspondence between predicted points and real-world geometry (depth/projection accuracy).</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>Point-level (grasp/place) and trajectory-level (ordered points -> SE(3) path)</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>2D pixel points converted to 3D via depth (pinhole) in Visual Traces Branch; Affordance Branch uses 2D points plus grasping heuristics (projector + planner).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>Zero-shot robot manipulation execution on xArm and WidowX (SIMPLEREnv sim experiments and xArm real-world tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>SIMPLEREnv tasks (sim), xArm 8 real-world tabletop tasks (RealSense L515 third-person camera)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>Simulation and real-world third-person views</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Stage success rates (grasp/correct object) and final task success rate</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Embodied-R1-P and Embodied-R1-V reported high success: Embodied-R1 overall zero-shot SIMPLEREnv 56.2% average; xArm zero-shot 87.5% overall (Embodied-R1-T variant in Table 6); Embodied-R1-V slightly higher than Embodied-R1-P on average.</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>Using VTG-based (Visual Traces) execution produced slightly higher average success than Affordance Points branch, indicating richer trajectory grounding is beneficial for some tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Execution correctness depends on depth accuracy and correct point localization; depth hallucinations or incorrect projection degrade execution quality; planners assume accurate geometric targets.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Failures arise when predicted points do not correspond to feasible grasp affordances (incorrect OFG), depth/projection errors map points to wrong 3D locations, or when predicted traces do not avoid obstacles; baselines struggle particularly with rigid/difficult-to-grasp objects.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>Pointing + planner separation aids cross-embodiment transfer because the same point interface can be consumed by different motion planners for different robots.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td>Action executor pipelines enabled Embodied-R1 to achieve high success on unseen objects/scenes in xArm real-world evaluations (87.5% zero-shot overall).</td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Execution pipelines are separate (late fusion) — predicted coordinates are parsed from model output and then handed to planners/projection modules.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Point-level interfaces reduce need for per-robot action data, enabling zero-shot execution with limited robot-specific data.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>A point-centric grounding interface cleanly separates perception/reasoning from embodiment-specific control; coupling with planners (CuRobo) or geometric lifting to SE(3) enables effective zero-shot robot manipulation, but relies on accurate depth/projection and correct affordance points.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1979.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e1979.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SFT vs RL Ablation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Supervised Fine-Tuning (SFT) vs Reinforced Fine-Tuning (RFT) ablations with/without chain-of-thought</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Controlled ablation comparing SFT and RL training modalities and the effect of including or excluding chain-of-thought (Think) reasoning in prompts, showing RL with free-form Think yields best grounding/generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SFT vs RFT (with/without CoT) ablation</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Four experimental conditions: RL w/ Think (full method), RL w/o Think (forced direct answer), SFT w/ Think (supervised CoT templates), SFT w/o Think (standard QA training). All trained on same data (not mixed all-datasets for RL ablation) to isolate training paradigm effects.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>Same ViT+projector+LLM backbone across conditions</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>Qwen2.5-VL initialization in all variants</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>All conditions produce point outputs; RL variants optimized via rewards, SFT variants via supervised token loss.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>Point-level outputs for RRG benchmarks used in ablation</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>2D pixel coordinates in image space (RRG tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>Region Referring Grounding (RRG) benchmarks (Where2Place, VABench-P)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Where2Place, VABench-P (RRG)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>Benchmarks' image domains (mixed embodied/web/simulated inputs used by RRG tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Point-in-region accuracy (%) on RRG benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Table 8 results: RL w/ Think Where2Place 65.50, VABench-P 65.39; RL w/o Think Where2Place 63.00, VABench-P 60.50; SFT w/ Think Where2Place 41.25, VABench-P 47.67; SFT w/o Think Where2Place 36.85, VABench-P 50.46.</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td>SFT variants (particularly SFT w/o Think) show much lower RRG accuracy (Where2Place 36.85 vs RL w/ Think 65.50), indicating that supervised training alone poorly handles multi-solution point labeling and generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>RL w/ Think improved Where2Place by +28.65 pts and VABench-P by +14.93 pts relative to SFT w/o Think (absolute improvements in Table 8). RL variants outperform SFT regardless of CoT inclusion.</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Authors attribute SFT failure partly to multi-solution ambiguity in pointing problems, which leads SFT to memorize and overfit; RL with reward for any valid point alleviates this bottleneck.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>SFT with CoT did not meaningfully help and in some cases decreased performance (VABench-P), suggesting fixed CoT templates in SFT can hinder learning if inappropriate reasoning is enforced.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>RL-trained models exhibited stronger OOD generalization across unseen scenarios and embodiments than SFT-trained models.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td>RL w/ Think generalized better to unseen objects/environments in downstream tests (xArm real-world and unseen simulations) than SFT.</td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Same projection/LLM fusion for all conditions; main variable is the objective (RL vs supervised) and presence of free-form CoT in prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>SFT was trained for 3 epochs on same data; RL trained for 2+1 epoch RFT schedule and produced better generalization, indicating RL objective yields more effective use of multi-solution data for grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Reinforcement learning with free-form chain-of-thought (RFT w/ Think) significantly improves point-based grounding generalization versus supervised fine-tuning; SFT struggles with multi-solution ambiguity and can overfit to specific annotations.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Robopoint: A vision-language model for spatial affordance prediction for robotics. <em>(Rating: 2)</em></li>
                <li>FSD <em>(Rating: 2)</em></li>
                <li>Qwen2.5-vl technical report <em>(Rating: 2)</em></li>
                <li>From seeing to doing: Bridging reasoning and decision for robotic manipulation <em>(Rating: 1)</em></li>
                <li>Track2act: Predicting point tracks from internet videos enables generalizable robot manipulation. <em>(Rating: 1)</em></li>
                <li>SoFar: (pipeline integrating Florence-2 + SAM + GPT-4o) (Qi et al. 2025) <em>(Rating: 1)</em></li>
                <li>HandAL: A dataset of real-world manipulable object categories with pose annotations, affordances, and reconstructions. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1979",
    "paper_id": "paper-280686378",
    "extraction_schema_id": "extraction-schema-40",
    "extracted_data": [
        {
            "name_short": "Embodied-R1",
            "name_full": "Embodied-R1: Reinforced Embodied Reasoning for General Robotic Manipulation",
            "brief_description": "A 3B-parameter vision-language model trained with a two-stage Reinforced Fine-Tuning (RFT) curriculum to produce point-centric grounding outputs (single points and ordered point traces) that act as embodiment-agnostic intermediates between vision-language reasoning and robot motion execution.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Embodied-R1",
            "model_description": "Multimodal model built on the Qwen2.5-VL architecture: a Vision Transformer (ViT) visual encoder → projector → autoregressive LLM. The model performs explicit chain-of-thought reasoning (&lt;think&gt; tags) and outputs text-encoded point commands (&lt;point&gt; [[x,y],...]&lt;/point&gt;). Grounding is done by producing 2D pixel coordinates (or RGB-D 3D coordinates in the RGBD variant) that are verified against masks/regions or executed in simulation, with RL rewards shaping multi-task pointing behavior.",
            "visual_encoder_type": "Vision Transformer (ViT) visual encoder (as in Qwen2.5-VL) plus a projector to the LLM",
            "visual_encoder_pretraining": "Backbone initialized from Qwen2.5-VL (Qwen2.5-VL-3B-Instruct); explicit pretraining dataset names are not enumerated in the paper beyond the Qwen2.5-VL backbone reference",
            "grounding_mechanism": "Point-centric grounding: the model grounds language by predicting pixel coordinates or sequences of coordinates tied to semantic entities (points must fall within object masks or target placement regions). Grounding is learned via RFT using task-specific reward functions (point-in-mask, distance, trajectory RMSE, environment execution) rather than direct supervision of low-level actions.",
            "representation_level": "Object-centric / pixel-level points and ordered visual traces (multi-point trajectories); also supports region-level placement via point-in-region verification; has an RGB-D variant with depth value per point (3D).",
            "spatial_representation": "Explicit 2D pixel coordinates (p=(x,y) in image space); optional depth channel for (X,Y,D) in RGB-D variant; object masks used for verification; object-centric visual traces (ordered sequences of points).",
            "embodied_task_type": "Object manipulation (robotic manipulation / affordance placement / trajectory generation)",
            "embodied_task_name": "SIMPLEREnv (simulated control), 8 real-world xArm tabletop manipulation tasks (xArm experiments), plus spatial/pointing benchmarks (RoboRefit/RoboRef, Where2Place, VABench-P/V, Part-Afford, Open6DOR-Position)",
            "visual_domain": "Photorealistic simulation (SIMPLEREnv, Isaac Gym synthetic scenes) and real-world third-person tabletop views (RealSense L515 on xArm), also web images for REG sources",
            "performance_metric": "Success rate for manipulation tasks; accuracy for point-in-region tasks; RMSE/MAE and LLM-based scores for visual-trace trajectory similarity",
            "performance_value": "Zero-shot SIMPLEREnv average success rate 56.2%; zero-shot real-world xArm tasks 87.5% success (8 tasks); VABench-V: lowest RMSE/MAE among compared VLM baselines (precise table entries in paper); Open6DOR-Position: Embodied-R1-RGBD overall 90.2, Embodied-R1-RGB 66.8 (Table 4).",
            "has_grounding_ablation": true,
            "performance_without_grounding": "SFT baseline (Embodied-SFT) and SFT variants: e.g., Table 8 (RRG benchmarks) shows SFT w/o Think: Where2Place 36.85, VABench-P 50.46 versus RL w/ Think: Where2Place 65.50, VABench-P 65.39.",
            "grounding_improvement": "Compared to SFT w/o CoT, RL w/ Think improved Where2Place by +28.65 points and VABench-P by +14.93 points (absolute percentage points in Table 8); overall ~62% improvement over strong baselines on zero-shot xArm tasks (paper claim).",
            "has_encoder_comparison": true,
            "encoder_comparison_results": "Paper compares Embodied-R1-RGB vs Embodied-R1-RGBD: on Open6DOR-Position Embodied-R1-RGBD achieves 90.2 overall while Embodied-R1-RGB achieves 66.8, showing large benefit from explicit depth for 3D positioning; but authors note RGB-D may hallucinate depth in high relational-complexity cases and sometimes underperform the 2D variant at Level 1 relations.",
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "Authors identify the 'seeing-to-doing gap' caused by (1) data scarcity for embodied grounding and (2) embodiment heterogeneity across robots. They also report noisy upstream vision models in the VTG pipeline (incorrect grounding, incomplete tracking) and potential 'depth hallucinations' in RGB-D inputs that can reduce performance at higher relational complexity.",
            "failure_mode_analysis": "Documented failure modes: (a) general VLMs (e.g., Qwen2.5-VL, GPT-4o) perform poorly on pointing tasks (qualitative/quantitative), (b) pre-trained vision-model noise introduces incorrect object grounding/tracking in VTG data-generation causing dataset noise, (c) reward-hacking in VTG where the model outputs only start/end points (mitigated by requiring exactly 8 points), (d) baseline methods fail on spatial-reasoning tasks and rigid/difficult-to-grasp objects (e.g., screwdriver, moka pot), (e) RGB-D can produce hallucinated depth leading to errors at high relational complexity (noted qualitatively). Specific frequencies: Table 8 and task tables quantify performance drops (see SFT v RL and baseline comparisons); Table 7 disturbance tests show drop from 100% to 83% under combined background+light+height changes.",
            "domain_shift_handling": "Mixed-data training and pointing abstraction: Embodied-R1 is trained on both large web image/regression datasets and embodied datasets (Embodied-Points-200K, Embodied-Spatial-84K, ViRL-subset-18K) to bridge cyberspace↔physical domains. Pointing is used as an embodiment-agnostic intermediate to reduce the effect of robot morphology differences; RFT (reward-driven) handles multi-solution labeling. Empirically, model shows robust zero-shot sim-to-real transfer and robustness to visual disturbances (Table 7).",
            "novel_object_performance": "Robust zero-shot on unseen real-world objects: xArm real-world OOD tasks achieved 87.5% overall success across 8 tasks with objects/scenes unseen during training. Also successful zero-shot VTG predictions on unseen simulators (LIBERO, ManiSkill) per qualitative results.",
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": "Visual tokens from ViT are projected via a projector into the LLM's multimodal input (Qwen2.5-VL style); the model autoregressively generates text responses that include structured point outputs. In practice this is a visual→projector→LLM fusion (cross-modal conditioning inside the LLM).",
            "sample_efficiency": "Training uses ~200K embodied-point samples (Embodied-Points-200K) plus Embodied-Spatial-84K and ViRL-subset-18K; two-stage RFT: stage1 2 epochs, stage2 1 epoch (≈48 hours per stage on 8 A100s). Paper reports RL generalizes better than SFT trained on same data (Embodied-SFT trained 3 epochs) indicating higher effective generalization/sample efficiency for RL in multi-solution tasks.",
            "key_findings_grounding": "Pointing as an intermediate, embodiment-agnostic grounding representation (single points and ordered visual traces) combined with reinforced fine-tuning (RFT) and multi-task rewards yields substantial zero-shot improvement in robotic manipulation; RFT handles multi-solution labeling better than SFT, mixed multi-task training shares coordinate/semantic priors across tasks, depth input improves 3D position accuracy but can hallucinate, and careful reward/format constraints are crucial to avoid reward-hacking (VTG).",
            "uuid": "e1979.0"
        },
        {
            "name_short": "Pointing (representation)",
            "name_full": "Point-centric (Pointing) Representation for Embodied Grounding",
            "brief_description": "A unified, embodiment-agnostic intermediate representation where grounding is expressed as 2D pixel coordinates or ordered point sequences (visual traces), encoding object identity, affordances, target placements, and implicit execution trajectories.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Pointing representation (Embodied intermediate)",
            "model_description": "Not a model but a representation: tasks are reframed to require predicting coordinates p=(x,y) inside object masks (REG/OFG), in free-space placement regions (RRG), or an ordered set τ of points for VTG. These textual point outputs are parsed and verified during training via reward functions (point-in-mask, distance-to-center, RMSE to ground-truth trajectory, or simulator environment execution).",
            "visual_encoder_type": "N/A (representation-level choice; relies on ViT visual features from backbone models)",
            "visual_encoder_pretraining": "N/A",
            "grounding_mechanism": "Language is grounded by mapping instruction-derived referents to pixel coordinates or ordered traces in image space; grounding is learned via multi-task rewards rather than direct action regression.",
            "representation_level": "Pixel-level/object-centric points; sequence-level for visual traces (VTG); region-referenced implicitly via point-in-region.",
            "spatial_representation": "Explicit 2D pixel coordinates, optional depth values for RGB-D variant, and ordered sequences representing object-centric trajectories.",
            "embodied_task_type": "Object manipulation, placement, trajectory generation",
            "embodied_task_name": "REG (Referring Expression Grounding), RRG (Region Referring Grounding), OFG (Object Functional Grounding), VTG (Visual Trace Generation) benchmarks (RoboRefit/RoboRef, Where2Place, VABench-P/V, Part-Afford, etc.)",
            "visual_domain": "Web images, simulation images, and real-world third-person tabletop camera views",
            "performance_metric": "Point-in-mask accuracy, placement accuracy, RMSE/MAE for trajectory similarity, task success rate when plugged into motion planner",
            "performance_value": "Using pointing as representation, Embodied-R1 achieves state-of-the-art on 11 pointing/spatial benchmarks and enables 56.2% SIMPLEREnv sim success and 87.5% zero-shot xArm real-world success when paired with motion executors.",
            "has_grounding_ablation": true,
            "performance_without_grounding": "General VLMs without pointing-specialized training (GPT-4o, Qwen2.5-VL) perform poorly on pointing tasks — qualitative and quantitative drops reported in benchmark tables (Embodied-SFT and baseline rows show lower point-in-region accuracies).",
            "grounding_improvement": "Pointing-centric training plus RFT yields large improvements vs general VLM baselines and SFT variants (see Embodied-R1 vs Embodied-SFT and vs RoboPoint/FSD baseline numbers; paper cites 62% improvement over strong baselines on some zero-shot metrics).",
            "has_encoder_comparison": false,
            "encoder_comparison_results": "",
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "Pointing helps decouple perception from embodiment by providing a common interface, but upstream perception errors (segmentation/tracking depth hallucinations) can corrupt point targets; the paper documents noise injected by pre-trained vision models during VTG generation.",
            "failure_mode_analysis": "Point-space failure modes include multi-solution ambiguity (many valid points) causing SFT memorization; reward-hacking on VTG where model outputs trivial 2-point lines unless constrained; noisy tracker outputs produce incorrect traces that must be filtered; small or occluded targets can reduce point-in-mask accuracy (reported qualitatively).",
            "domain_shift_handling": "Pointing is used to mitigate domain shift by enabling training on web and embodied datasets and by being embodiment-agnostic, improving sim-to-real transfer in zero-shot tests.",
            "novel_object_performance": "Pointing supports generalization to novel objects as shown by zero-shot performance on unseen tasks and environments (LIBERO, ManiSkill simulations and real xArm tasks).",
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": "Points are output as structured text by the LLM conditioned on visual features; the verifier/reward compares these coordinates to masks/regions or simulated execution.",
            "sample_efficiency": "Pointing representation reduces need for embodiment-specific action labels and supports learning from combined web + embodied datasets (paper trains on Embodied-Points-200K), improving multi-task sharing.",
            "key_findings_grounding": "A point-centric intermediate effectively anchors language to scene entities and actions, enabling cross-embodiment zero-shot control when combined with RL-trained reasoning and proper reward design, but depends on reliable upstream perception (segmentation/tracking/depth).",
            "uuid": "e1979.1"
        },
        {
            "name_short": "RFT (GRPO)",
            "name_full": "Reinforced Fine-Tuning (RFT) using GRPO",
            "brief_description": "A reinforcement learning fine-tuning paradigm (GRPO algorithm variant) used to optimize an autoregressive VLM with task-specific reward functions over generated point outputs, addressing multi-solution labeling and encouraging generalizable embodied reasoning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Reinforced Fine-Tuning (RFT) with GRPO",
            "model_description": "RFT trains the multimodal policy π_θ(y|x) by maximizing expected reward E_x E_y[r(y,x)] using GRPO: generate G candidate responses from the behavior policy, compute normalized advantages and apply a clipped surrogate loss (PPO-like) with KL penalty. Rewards are task-specific (format, accuracy, point-in-mask, point-distance, trajectory RMSE, simulator environment reward).",
            "visual_encoder_type": "N/A (applies to the multimodal LLM pipeline using ViT+projector+LLM)",
            "visual_encoder_pretraining": "N/A",
            "grounding_mechanism": "Learning to ground via reward signals: points are evaluated by geometric/semantic verifiers (mask membership, distances, RMSE, simulation success) and these scalar rewards supervise autoregressive generation via policy gradient-style optimization (GRPO).",
            "representation_level": "Trains at pixel-level/object-centric point outputs and sequence-level VTG outputs",
            "spatial_representation": "2D pixel coordinates (and depth when provided); rewards operate on these coordinates relative to masks/target centroids or trajectory ground truth.",
            "embodied_task_type": "Object manipulation grounding tasks (REG/RRG/OFG/VTG) and downstream robot control when points are executed via motion planners",
            "embodied_task_name": "Used to train for REG, RRG, OFG, VTG tasks and evaluated on SIMPLEREnv and xArm real-world manipulation",
            "visual_domain": "Web, simulated and real-world images (mixed during training and evaluation)",
            "performance_metric": "Task-specific reward (normalized 0-1), downstream success rate and benchmark accuracies/trajectory RMSE",
            "performance_value": "RFT (RL w/ Think) outperforms SFT in RRG benchmarks (Table 8: Where2Place 65.50 vs SFT w/o Think 36.85; VABench-P 65.39 vs 50.46) and improves zero-shot manipulation success in SIMPLEREnv and real xArm tasks (paper-level comparisons).",
            "has_grounding_ablation": true,
            "performance_without_grounding": "SFT w/ CoT and SFT w/o CoT variants show lower performance across RRG benchmarks (e.g., SFT w/ Think 41.25 Where2Place, SFT w/o Think 36.85).",
            "grounding_improvement": "RFT with free-form CoT yields the best scores in ablations; absolute improvements over SFT variants on Where2Place up to +28.65 points (Table 8) and overall stronger OOD generalization vs SFT.",
            "has_encoder_comparison": false,
            "encoder_comparison_results": "",
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "RFT helps alleviate multi-solution supervision problems that cause SFT memorization; however, it still depends on accurate perceptual verification (masks, tracker outputs) which can be noisy and become a bottleneck.",
            "failure_mode_analysis": "Reward hacking observed in VTG (model outputs minimal points that match start/end to gain reward); fixed by adding format constraints (exact 8 points) and format reward r_format; multi-task reward dominance solved by normalizing task rewards to [0,1] and weight normalization.",
            "domain_shift_handling": "RFT trains on mixed cyberspace and embodied datasets and explicitly optimizes for simulator/environment rewards for some RRG samples (r_env), aiding domain adaptation and sim-to-real transfer.",
            "novel_object_performance": "RFT-trained model generalizes to novel objects/environments better than SFT in evaluations (e.g., zero-shot xArm tests and unseen simulation VTG cases).",
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": "Policy updates operate on the autoregressive text output that encodes coordinates; visual features condition the policy via the LLM's multimodal input—no additional fusion mechanism is introduced beyond the Qwen2.5-VL style visual→projector→LLM pipeline.",
            "sample_efficiency": "RFT uses a 'question-verification' style dataset (Embodied-Points-200K) and provides positive reward for any valid solution, which the authors argue yields better utilization of multi-solution data than SFT.",
            "key_findings_grounding": "RFT (GRPO) combined with structured rewards and chain-of-thought reasoning yields substantially better grounding generalization (point predictions and trace generation) than SFT, especially under multi-solution semantics and OOD conditions; reward and format constraints are critical to prevent degenerate solutions.",
            "uuid": "e1979.2"
        },
        {
            "name_short": "Embodied-Points-200K",
            "name_full": "Embodied-Points-200K dataset",
            "brief_description": "A curated dataset of ~200k question-verification samples for embodied pointing tasks covering REG, RRG, OFG, and VTG, designed for RFT training using verifiable rewards rather than strict Q→A pairs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Embodied-Points-200K (training dataset)",
            "model_description": "Dataset contains point-centric annotations and verification methods: REG points inside segmentation masks (sources: RefCOCO, RoboRef, Robo-Point), RRG relational placement points (extracted from ~1M embodied examples plus Isaac Gym synthetic 3D rearrangements), OFG functional grasp points (HandAL-derived 40k points), and VTG object-centric visual traces (extracted via keypoint proposal, Cotracker3 tracking, downsampled to 8 points).",
            "visual_encoder_type": "N/A (dataset used to train ViT+LLM pipeline)",
            "visual_encoder_pretraining": "N/A",
            "grounding_mechanism": "Provides paired inputs and verifiers: model outputs point(s) in text; verifier computes rewards (point-in-mask, distance-to-center, RMSE for traces, env simulation). Used with RFT to shape grounding behavior.",
            "representation_level": "Point-level and ordered-point-sequence (VTG) annotations projected back onto initial images; also includes RGB-D entries for 3D RRG.",
            "spatial_representation": "2D pixel coordinates, 3D (X,Y,D) for simulated 3D RRG (depth normalized), segmentation masks for verification, and 8-point visual trace sequences.",
            "embodied_task_type": "Training data for REG, RRG, OFG, VTG embodied manipulation grounding tasks",
            "embodied_task_name": "Dataset supports training for the benchmarks used in paper: RoboRefit, Where2Place, VABench-P/V, Part-Afford, Open6DOR, SIMPLEREnv RRG/3D RRG, VTG tasks",
            "visual_domain": "Aggregated from web images (RefCOCO), real-world embodied datasets, Isaac Gym synthetic 3D renders, and video trackers (for VTG)",
            "performance_metric": "Used to compute reward signals during RFT (point-in-mask boolean, distance reward 0-1, trajectory RMSE→normalized trace reward, env binary sim success)",
            "performance_value": "Dataset size ~200,000 samples; specific per-task splits enumerated in paper (e.g., 33k high-quality RRG samples from pipeline, 40k OFG points from HandAL-derived annotations).",
            "has_grounding_ablation": false,
            "performance_without_grounding": null,
            "grounding_improvement": "Not quantified as a standalone ablation but credited as enabling the multi-task RFT training and better generalization than SFT on pointing tasks.",
            "has_encoder_comparison": false,
            "encoder_comparison_results": "",
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "Authors note the VTG generation pipeline uses multiple pre-trained vision models (keypoint extractors, Grounded-SAM, Cotracker3) which introduce noise; they apply rule-based filtering and manual validation to reduce noisy samples.",
            "failure_mode_analysis": "Dataset construction required heavy filtering because tracker/grounding errors and occlusions can produce invalid traces; reward-driven training mitigates multi-solution annotation issues but relies on clean verifiers.",
            "domain_shift_handling": "Dataset intentionally mixes cyberspace (web) and embodied/simulated examples to improve domain robustness of pointing supervision.",
            "novel_object_performance": "Designed to support generalization; when used to train Embodied-R1 via RFT, enabled strong zero-shot performance on novel objects in simulation and real-world.",
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": "N/A (dataset supplies target point labels and verification functions used inside RL reward pipeline).",
            "sample_efficiency": "Large-scale (200K) dataset created to address data scarcity; RFT training uses these samples plus spatial reasoning datasets to obtain strong generalization in few epochs (2+1 epoch schedule).",
            "key_findings_grounding": "A large, verification-style, point-centric dataset allows RL-based training to reward any valid solution (mitigating multi-solution overfitting) and is critical to learn robust grounding and visual-trace generation that transfer zero-shot to robots.",
            "uuid": "e1979.3"
        },
        {
            "name_short": "Pointing abilities (REG/RRG/OFG/VTG)",
            "name_full": "Four embodied pointing abilities: Referring Expression Grounding (REG), Region Referring Grounding (RRG), Object Functional Grounding (OFG), Visual Trace Generation (VTG)",
            "brief_description": "Defined core capabilities for embodied pointing: REG localizes described objects with points inside segmentation masks; RRG identifies free-space placement points relative to other objects; OFG selects functionally manipulable part points; VTG outputs ordered object-centric trajectories (8-point traces) for manipulation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "REG / RRG / OFG / VTG (capability set learned by Embodied-R1)",
            "model_description": "Each ability is implemented as a prompt+output format that elicits point coordinates from the LLM conditioned on the visual encoder; verification-reward functions differ per ability (point-in-mask for REG/OFG, mask+distance/env for RRG, RMSE-based trajectory reward for VTG). VTG uses multi-model pipelines for data generation and enforces an 8-point output constraint during training.",
            "visual_encoder_type": "ViT visual encoder (backbone Qwen2.5-VL) used to extract features for all abilities",
            "visual_encoder_pretraining": "Backbone initialized from Qwen2.5-VL; task-specific data provided via Embodied-Points-200K and Embodied-Spatial-84K",
            "grounding_mechanism": "Direct output of semantic pixel coordinates tied to language referents; verification is geometric/semantic (masks, relations, distance, trajectory RMSE); RRG optionally uses simulator environment reward for 3D placement correctness.",
            "representation_level": "Object-centric / region-level / sequence-level depending on ability (REG/OFG point-in-mask; RRG free-space placement point; VTG ordered sequence of points forming an object-centric trajectory).",
            "spatial_representation": "2D pixel coordinates (p=(x,y)), VTG sequences of 8 points, and 3D (X,Y,D) for 3D RRG when depth is available.",
            "embodied_task_type": "Referring grounding, placement/region grounding, affordance localization, trajectory generation for manipulation",
            "embodied_task_name": "Benchmarks: RoboRefit (REG), Where2Place & VABench-P (RRG), Part-Afford (OFG), VABench-V (VTG), Open6DOR-Position (3D RRG)",
            "visual_domain": "Web images, simulation (Isaac Gym), real-world tabletop third-person camera imagery",
            "performance_metric": "Point-in-mask accuracy for REG/OFG/RRG; RMSE/MAE and LLM score for VTG; environment success for 3D RRG executed in simulator.",
            "performance_value": "Embodied-R1 achieves SOTA on REG/RRG/OFG/VTG benchmarks reported; concrete examples: VTG improved RMSE/MAE over baselines (see Table 3); RRG Where2Place 65.50 with RL w/ Think (Table 8).",
            "has_grounding_ablation": true,
            "performance_without_grounding": "SFT and unguided model variants show large performance drops on these abilities (see Table 8 and comparisons to GPT-4o/Qwen2.5-VL and specialized baselines RoboPoint/FSD).",
            "grounding_improvement": "Multi-task RFT training of these abilities yields consistent improvements vs unmixed (single-ability) training (Table 9 shows mixed training improves e.g., Where2Place 69.50 vs 65.50 unmixed), and RL vs SFT dramatically boosts RRG performance (Table 8).",
            "has_encoder_comparison": true,
            "encoder_comparison_results": "3D RRG (Open6DOR): Embodied-R1-RGBD outperforms RGB-only variant substantially on 3D position tasks (90.2 vs 66.8 overall in Table 4), but authors warn depth can hallucinate for complex relations.",
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "REG and OFG rely on accurate segmentation masks; VTG relies on robust tracking; RRG 3D requires accurate depth—errors in these perceptual components are called out as sources of dataset noise and performance failure.",
            "failure_mode_analysis": "REG/OFG failures tied to segmentation/occlusion; RRG failures tied to depth hallucination and simulator mismatch; VTG failures include tracking errors and reward-hacking (insufficient points). Paper reports mitigation strategies (filtering, format constraints, dense distance rewards).",
            "domain_shift_handling": "Abilities are trained on mixed sources (web, simulation, embodied datasets) and RFT encourages generalization; object-centric VTG traces support embodiment-agnostic transfer.",
            "novel_object_performance": "All four abilities generalize to novel objects/environments per qualitative and quantitative zero-shot evaluations (e.g., successful VTG on unseen LIBERO/ManiSkill scenes and xArm tasks).",
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": "LLM-conditioned outputs generated from projected ViT features; ability-specific prompts and format rewards steer output structure.",
            "sample_efficiency": "Joint mixed-task training (all abilities together) improves success rates over isolated training; Table 9 shows mixed training outperforms unmixed on Part-Afford, Where2Place, VABench-P.",
            "key_findings_grounding": "Decomposing grounding into these four pointing abilities provides a compact, shared coordinate space that enables multi-task learning, better generalization across embodiments, and direct interfacing with motion planners; success depends on reliable perception (masks/tracking/depth) and careful reward/formatted-output constraints.",
            "uuid": "e1979.4"
        },
        {
            "name_short": "Reward design & VTG fixes",
            "name_full": "Multi-task Reward Design and VTG-specific constraints (format rewards, point-number constraint)",
            "brief_description": "A library of verifiable reward components (format, accuracy, point-in-mask, distance, trajectory RMSE, environment reward) combined as normalized weighted sums per-task to stabilize multi-task RL; includes specific fixes to avoid VTG reward-hacking (require exactly 8 points in trace).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Reward library (r_format, r_acc, r_mask, r_dis, r_trace, r_env)",
            "model_description": "Each predicted text response is parsed and evaluated by task-specific reward terms (format validity, answer accuracy, mask membership, dense distance reward to center, RMSE-based trace similarity, simulator execution). Per-task weights are normalized so total ℛ ∈ [0,1]. VTG enforces format and 8-point constraint; failure to satisfy format sets reward to zero.",
            "visual_encoder_type": "N/A (reward-level mechanism operating on model outputs and visual verifiers)",
            "visual_encoder_pretraining": "N/A",
            "grounding_mechanism": "Rewards directly supervise grounding outputs by verifying predicted coordinates against masks/ground-truth trajectories or by executing predictions in a simulator for env rewards.",
            "representation_level": "Applies to point outputs and sequence outputs",
            "spatial_representation": "Distance and RMSE computed in pixel-space (or normalized depth units for RGB-D) to produce dense shaping rewards",
            "embodied_task_type": "REG/RRG/OFG/VTG and simulator-executed placement tasks",
            "embodied_task_name": "RRG (including Isaac Gym 3D RRG with simulator env rewards), VTG (VABench-V), REG/OFG benchmarks",
            "visual_domain": "Mixed (web, simulation, real-world)",
            "performance_metric": "Normalized reward (0-1) used to drive policy updates; downstream metrics (success rate, RMSE) reported in benchmarks",
            "performance_value": "Specific per-task reward mixes: e.g., ℛ_RRG = 0.1 r_format + 0.6 r_mask + 0.3 r_dis; ℛ_VTG = 0.1 r_format + 0.9 r_trace. VTG point-number constraint reduced RMSE from 105.25→77.83 (Table 11) and MAE from 59.75→44.97, and increased GPT Score.",
            "has_grounding_ablation": true,
            "performance_without_grounding": "Without the VTG 8-point format constraint, models learned degenerate 2-point solutions: VABench-V RMSE rose to 105.25 and MAE to 59.75 vs with constraint RMSE 77.83 MAE 44.97 (Table 11).",
            "grounding_improvement": "Format and point-number constraints significantly improved VTG fidelity (RMSE/MAE improvements as above); normalizing task rewards prevented simpler tasks from dominating mixed-task training.",
            "has_encoder_comparison": false,
            "encoder_comparison_results": "",
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "Reward computation depends on correct masks/trajectories; noisy perceptual labels degrade reward signals and can mislead RL optimization, requiring rigorous filtering of training data.",
            "failure_mode_analysis": "Reward hacking (VTG two-point solutions) and dominance of simpler tasks in mixed RL if rewards are not normalized; both addressed by format enforcement and normalized per-task reward weights.",
            "domain_shift_handling": "Format-based rewards are domain-agnostic and help when mixing web/sim/real data; env rewards used only on simulator-sourced RRG samples.",
            "novel_object_performance": "Robust reward design contributed to zero-shot success on unseen objects/environments by incentivizing generalizable point predictions rather than memorization.",
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": "Rewards act on parsed coordinate outputs after LLM decoding; not a fusion mechanism per se.",
            "sample_efficiency": "Dense auxiliary rewards (r_dis, r_trace) accelerate learning by providing shaped signals rather than sparse only env rewards.",
            "key_findings_grounding": "Carefully designed, verifiable reward components and strict output format constraints are essential to steer RL-based grounding away from degenerate solutions and to stabilize multi-task training across heterogeneous pointing tasks.",
            "uuid": "e1979.5"
        },
        {
            "name_short": "Action Executors (P & V branches)",
            "name_full": "Affordance Points Branch (-P) and Visual Traces Branch (-V) action executors",
            "brief_description": "Two downstream execution pipelines that convert point outputs into robot motions: (P) generate key grasp/place points fed to a motion planner (CuRobo) and (V) map 2D visual traces to 3D SE(3) trajectories via pinhole camera projection and depth interpolation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Affordance Points Branch and Visual Traces Branch",
            "model_description": "Affordance Points Branch: Embodied-R1 predicts grasp and placement points (OFG/RRG), CuRobo planner computes collision-free SE(3) paths to those affordance points. Visual Traces Branch: VTG outputs 2D object-centric traces; these are lifted to 3D using pinhole camera model and depth, interpolated into continuous SE(3) trajectories for execution (similar to FSD-style trace following).",
            "visual_encoder_type": "ViT backbone used to produce the points/trace; execution uses external motion planning (CuRobo) and geometric projection (pinhole model).",
            "visual_encoder_pretraining": "Qwen2.5-VL backbone initialization used during training of point predictors",
            "grounding_mechanism": "Grounding outputs are 2D/3D points that act as the interface to motion planners; success depends on correspondence between predicted points and real-world geometry (depth/projection accuracy).",
            "representation_level": "Point-level (grasp/place) and trajectory-level (ordered points -&gt; SE(3) path)",
            "spatial_representation": "2D pixel points converted to 3D via depth (pinhole) in Visual Traces Branch; Affordance Branch uses 2D points plus grasping heuristics (projector + planner).",
            "embodied_task_type": "Zero-shot robot manipulation execution on xArm and WidowX (SIMPLEREnv sim experiments and xArm real-world tasks)",
            "embodied_task_name": "SIMPLEREnv tasks (sim), xArm 8 real-world tabletop tasks (RealSense L515 third-person camera)",
            "visual_domain": "Simulation and real-world third-person views",
            "performance_metric": "Stage success rates (grasp/correct object) and final task success rate",
            "performance_value": "Embodied-R1-P and Embodied-R1-V reported high success: Embodied-R1 overall zero-shot SIMPLEREnv 56.2% average; xArm zero-shot 87.5% overall (Embodied-R1-T variant in Table 6); Embodied-R1-V slightly higher than Embodied-R1-P on average.",
            "has_grounding_ablation": false,
            "performance_without_grounding": null,
            "grounding_improvement": "Using VTG-based (Visual Traces) execution produced slightly higher average success than Affordance Points branch, indicating richer trajectory grounding is beneficial for some tasks.",
            "has_encoder_comparison": false,
            "encoder_comparison_results": "",
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "Execution correctness depends on depth accuracy and correct point localization; depth hallucinations or incorrect projection degrade execution quality; planners assume accurate geometric targets.",
            "failure_mode_analysis": "Failures arise when predicted points do not correspond to feasible grasp affordances (incorrect OFG), depth/projection errors map points to wrong 3D locations, or when predicted traces do not avoid obstacles; baselines struggle particularly with rigid/difficult-to-grasp objects.",
            "domain_shift_handling": "Pointing + planner separation aids cross-embodiment transfer because the same point interface can be consumed by different motion planners for different robots.",
            "novel_object_performance": "Action executor pipelines enabled Embodied-R1 to achieve high success on unseen objects/scenes in xArm real-world evaluations (87.5% zero-shot overall).",
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": "Execution pipelines are separate (late fusion) — predicted coordinates are parsed from model output and then handed to planners/projection modules.",
            "sample_efficiency": "Point-level interfaces reduce need for per-robot action data, enabling zero-shot execution with limited robot-specific data.",
            "key_findings_grounding": "A point-centric grounding interface cleanly separates perception/reasoning from embodiment-specific control; coupling with planners (CuRobo) or geometric lifting to SE(3) enables effective zero-shot robot manipulation, but relies on accurate depth/projection and correct affordance points.",
            "uuid": "e1979.6"
        },
        {
            "name_short": "SFT vs RL Ablation",
            "name_full": "Supervised Fine-Tuning (SFT) vs Reinforced Fine-Tuning (RFT) ablations with/without chain-of-thought",
            "brief_description": "Controlled ablation comparing SFT and RL training modalities and the effect of including or excluding chain-of-thought (Think) reasoning in prompts, showing RL with free-form Think yields best grounding/generalization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "SFT vs RFT (with/without CoT) ablation",
            "model_description": "Four experimental conditions: RL w/ Think (full method), RL w/o Think (forced direct answer), SFT w/ Think (supervised CoT templates), SFT w/o Think (standard QA training). All trained on same data (not mixed all-datasets for RL ablation) to isolate training paradigm effects.",
            "visual_encoder_type": "Same ViT+projector+LLM backbone across conditions",
            "visual_encoder_pretraining": "Qwen2.5-VL initialization in all variants",
            "grounding_mechanism": "All conditions produce point outputs; RL variants optimized via rewards, SFT variants via supervised token loss.",
            "representation_level": "Point-level outputs for RRG benchmarks used in ablation",
            "spatial_representation": "2D pixel coordinates in image space (RRG tasks)",
            "embodied_task_type": "Region Referring Grounding (RRG) benchmarks (Where2Place, VABench-P)",
            "embodied_task_name": "Where2Place, VABench-P (RRG)",
            "visual_domain": "Benchmarks' image domains (mixed embodied/web/simulated inputs used by RRG tasks)",
            "performance_metric": "Point-in-region accuracy (%) on RRG benchmarks",
            "performance_value": "Table 8 results: RL w/ Think Where2Place 65.50, VABench-P 65.39; RL w/o Think Where2Place 63.00, VABench-P 60.50; SFT w/ Think Where2Place 41.25, VABench-P 47.67; SFT w/o Think Where2Place 36.85, VABench-P 50.46.",
            "has_grounding_ablation": true,
            "performance_without_grounding": "SFT variants (particularly SFT w/o Think) show much lower RRG accuracy (Where2Place 36.85 vs RL w/ Think 65.50), indicating that supervised training alone poorly handles multi-solution point labeling and generalization.",
            "grounding_improvement": "RL w/ Think improved Where2Place by +28.65 pts and VABench-P by +14.93 pts relative to SFT w/o Think (absolute improvements in Table 8). RL variants outperform SFT regardless of CoT inclusion.",
            "has_encoder_comparison": false,
            "encoder_comparison_results": "",
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "Authors attribute SFT failure partly to multi-solution ambiguity in pointing problems, which leads SFT to memorize and overfit; RL with reward for any valid point alleviates this bottleneck.",
            "failure_mode_analysis": "SFT with CoT did not meaningfully help and in some cases decreased performance (VABench-P), suggesting fixed CoT templates in SFT can hinder learning if inappropriate reasoning is enforced.",
            "domain_shift_handling": "RL-trained models exhibited stronger OOD generalization across unseen scenarios and embodiments than SFT-trained models.",
            "novel_object_performance": "RL w/ Think generalized better to unseen objects/environments in downstream tests (xArm real-world and unseen simulations) than SFT.",
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": "Same projection/LLM fusion for all conditions; main variable is the objective (RL vs supervised) and presence of free-form CoT in prompts.",
            "sample_efficiency": "SFT was trained for 3 epochs on same data; RL trained for 2+1 epoch RFT schedule and produced better generalization, indicating RL objective yields more effective use of multi-solution data for grounding.",
            "key_findings_grounding": "Reinforcement learning with free-form chain-of-thought (RFT w/ Think) significantly improves point-based grounding generalization versus supervised fine-tuning; SFT struggles with multi-solution ambiguity and can overfit to specific annotations.",
            "uuid": "e1979.7"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Robopoint: A vision-language model for spatial affordance prediction for robotics.",
            "rating": 2
        },
        {
            "paper_title": "FSD",
            "rating": 2
        },
        {
            "paper_title": "Qwen2.5-vl technical report",
            "rating": 2
        },
        {
            "paper_title": "From seeing to doing: Bridging reasoning and decision for robotic manipulation",
            "rating": 1
        },
        {
            "paper_title": "Track2act: Predicting point tracks from internet videos enables generalizable robot manipulation.",
            "rating": 1
        },
        {
            "paper_title": "SoFar: (pipeline integrating Florence-2 + SAM + GPT-4o) (Qi et al. 2025)",
            "rating": 1
        },
        {
            "paper_title": "HandAL: A dataset of real-world manipulable object categories with pose annotations, affordances, and reconstructions.",
            "rating": 1
        }
    ],
    "cost": 0.03122325,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Embodied-R1: Reinforced Embodied Reasoning for General Robotic Manipulation
19 Aug 2025</p>
<p>Yifu Yuan yuanyf@tju.edu.cn 
Tianjin University</p>
<p>Haiqin Cui 
Tianjin University</p>
<p>Yaoting Huang 
Tianjin University</p>
<p>♡ Yibin 
Tianjin University</p>
<p>Chen ♡ Fei 
Tianjin University</p>
<p>Zibin Dong 
Tianjin University</p>
<p>Pengyi Li 
Tianjin University</p>
<p>♡ Yan 
Tianjin University</p>
<p>Jianye Hao jianye.hao@tju.edu.cn 
Tianjin University</p>
<p>Tianjin University</p>
<p>Image Encoder 
Tianjin University</p>
<p>Embodied-R1: Reinforced Embodied Reasoning for General Robotic Manipulation
19 Aug 20255326BB90FE0155CF686588F33D05B12FarXiv:2508.13998v1[cs.RO]Embodied ReasoningGeneral Robotic ManipulationReinforcement Learning
Generalization in embodied AI is hindered by the "seeing-to-doing gap", stemming from data scarcity and embodiment heterogeneity.To address this, we pioneer "pointing" as a unified, embodiment-agnostic intermediate representation, defining four core embodied pointing abilities that bridge high-level vision-language comprehension with low-level action primitives.We introduce Embodied-R1, a 3B Vision-Language Model (VLM) specifically designed for embodied reasoning and pointing.We use a wide range of embodied and general visual reasoning datasets as sources to construct a large-scale dataset, Embodied-Points-200K, which supports key embodied pointing capabilities.Then we train Embodied-R1 using a two-stage Reinforced Fine-tuning (RFT) curriculum with specialized multi-task reward design.Embodied-R1 achieves state-of-the-art performance on 11 embodied spatial and pointing benchmarks.Critically, it demonstrates robust zero-shot generalization by achieving a 56.2% success rate in the SIMPLEREnv and 87.5% across 8 real-world XArm tasks without any task-specific fine-tuning, representing a 62% improvement over strong baselines.Furthermore, the model exhibits high robustness against diverse visual disturbances.Our work shows that a pointing-centric representation, combined with an RFT training paradigm, offers an effective and generalizable pathway to closing the perception-action gap in robotics.</p>
<p>Introduction</p>
<p>Recent advancements in Vision-Language Models (VLMs) Liu et al. (2024b), Bai et al. (2025b) have inspired a new wave of Vision-Language-Action (VLA) models Kim et al. (2024) aimed at enhancing generalization in robotic manipulation.While these models exhibit strong visual perception and excel at imitating expert demonstrations, their manipulation performance often degrades significantly in novel settings.This critical disparity is widely recognized as the seeing-to-doing gap Yuan et al. (2025): a failure to reliably translate rich perceptual understanding into effective robotic actions.This gap is largely attributed to two key challenges: (a) data scarcity, where limited embodied data fails to sufficiently ground language and vision with physical actions Walke et al. (2023), Lin et al. (2024), and (b) heterogeneity, where diverse robot morphologies hinder the knowledge transferability.</p>
<p>The community has explored several paradigms of VLAs.End-to-end VLAs Kim et al. (2024), Nasiriany et al. (2024) aim to learn a direct mapping from multimodal inputs to action spaces.However, there is a fundamental mismatch between aligning action modalities in the physical world and pre-trained cyberspace data, which can lead to knowledge forgetting and task conflicts.Training end-to-end VLAs solely on limited embodied datasets hinders the improvement of zero-shot generalization capability.Modular VLAs Huang et al. (2024c), Liu et al. (2024a), Qi et al. (2025), on the other hand, chain together powerful specialized vision models and design pipelines for atomic steps such as object and grasp detection.However, such methods are prone to cascading failures, are difficult to tune, and also suffer from relatively high inference latency.Furthermore, such disaggregated systems often lack a holistic understanding of scene-level spatial relationships.Affordance VLAs Li et al. (2024b), Yuan et al. (2025Yuan et al. ( , 2024b) offer a promising way towards more integrated solutions by training specialized embodied VLMs to predict intermediate visual aids.Despite their potential, they lack support for the comprehensive visual aids required for decision-making.Robobrain Ji et al. (2025) and FSD Yuan et al. (2025) use bounding boxes to mark objects or affordances, and they leverage visual traces to capture the dynamics of a task.Besides, Robopoint Yuan et al. (2024b) focuses exclusively on target regions within free space.However, all of these methods can only meet limited task requirements, as different tasks often demand a richer variety of visual aids and more comprehensive embodied grounding.FSD provides a crucial insight: embodied reasoning can effectively anchor task instructions to the correct semantic entities.However, FSD is trained on fixed Chain-of-Thought (CoT) reasoning templates through SFT; its thought process is inflexible, which limits its ability to generalize to new tasks.</p>
<p>Where should it go?</p>
<p>Region Reference Grounding</p>
<p>What it is?</p>
<p>Referring Expression Grounding Object Functional Grounding</p>
<p>How to grasp it?</p>
<p>Visual Trace Generation</p>
<p>How to perform it?Summarizing observations from prior work Deitke et al. (2024), Xu et al. (2025), Yuan et al. (2025), we propose pointing as a highly intuitive and effective choice that bridges high-level comprehension with generalizable action prediction.A point-centric representation Yuan et al. (2024b), Deitke et al. (2024) unifies rich semantic and spatial information into a compact and accurate representation of manipulation processes, which includes object identity (What is the object?"),functional affordance (How to use/grasp it?"),desired target destination ("Where it should be placed"), and can even implicitly convey the execution process of actions ("How to perform the task?") through a visual trace.To thoroughly assess the embodied reasoning and pointing capabilities, we define four key abilities: Referring Expression Grounding (REG), Region Referring Grounding (RRG), Object Functional Grounding (OFG), and Visual Trace Generation (VTG).We illustrate these pointing abilities in Fig. 2 and provide detailed definitions in the following sections.</p>
<p>Embodied Pointing Abilities
Referring
Then, we propose Embodied-R1, an advanced embodied reasoning VLM, whose core mechanism is to achieve unified anchoring of objects and spatial concepts through "pointing", thereby mastering general robotic manipulation.As shown in Fig. 1, Embodied-R1 first generates a detailed reasoning process and then provides the answer.With only 3B model parameters, Embodied-R1 achieves state-of-the-art performance in multiple spatial understanding and precise embodied pointing benchmarks.Embodied-R1 can also directly generate pointing signals to guide efficient robotic manipulation.By using pointing as an intermediate-level representation, Embodied-R1 grounds its reasoning in a universal visual perception task.This approach avoids the pitfalls of directly predicting low-level, embodiment-specific actions, thereby preserving and fully leveraging the strong visual generalization capabilities inherent to the pre-trained VLM.This enables the robot to achieve zero-shot control across different scenarios and tasks without any fine-tuning.Embodied-R1 achieves the best performance in SIMPLEREnv Li et al. (2024d) simulation and attains an 87.5% success rate on 8 real-world tasks based on the XArm platform.Meanwhile, Embodied-R1 enhances the robustness when facing various visual disturbances such as changes in lighting and background.</p>
<p>We train Embodied-R1 through Reinforced Fine-tuning (RFT), structured around a two-stage curriculum.The first stage is dedicated to building robust spatial reasoning.Subsequently, the second stage employs our specially curated Embodied-Points-200k dataset to foster diverse multi-task embodied pointing abilities.This extensive dataset features 200,000 high-quality questions and verification methods, drawn from diverse sources.We also design reward functions that support multi-task training, ensuring that each capability is thoroughly trained.A critical aspect of embodied pointing problems is their multi-solution nature.Consider marking a point in the empty space to the right of a drawer, any point within that region is considered a valid solution.This poses a significant "multi-solution dilemma" for SFT, where the model tends to overfit and merely memorize the data.In contrast, the RFT paradigm can provide positive reinforcement for all correct answers, thereby effectively encouraging the model to develop a genuine understanding of the tasks.</p>
<p>Our contributions include: 1 pioneering "pointing" as a unified, embodiment-agnostic representation and defining core embodied pointing abilities to bridge perception and decision; 2 constructing the comprehensive Embodied-Points-200K dataset for these capabilities; and 3 proposing Embodied-R1, a VLM designed to boost spatial reasoning and embodied pointing, thereby delivering powerful embodied reasoning performance.4 With only 3B parameters, Embodied-R1 attains state-of-the-art performance on 11 diverse spatial and pointing benchmarks and enables robust zero-shot robotic manipulation, achieving 56.2% success in SIMPLEREnv simulation and 87.5% in 8 real-world XArm tasks, representing a 62% improvement over strong baselines, without task-specific fine-tuning.</p>
<p>Embodied-R1: Advancing Embodied Reasoning from RFT</p>
<p>In this section, we sequentially introduce the model architecture of Embodied-R1 and its various embodied pointing abilities.Subsequently, we present the dataset construction and training methodology of Embodied-R1.Finally, we describe the deployment scheme of Embodied-R1 for performing tasks in real-world scenarios.</p>
<p>The Architecture and Capabilities of Embodied-R1</p>
<p>Embodied-R1 follows the fundamental architecture of Qwen2.5-VLBai et al. (2025a), which consists of three components: a Vision Transformer (ViT) as the visual encoder, a projector, and an LLM.Given a multimodal input x = (I, Q), where I represents the images and Q is the textual instruction, the model autoregressively predicts a textual response y.Embodied-R1 is specifically designed for embodied manipulation, enhancing spatial reasoning and embodied pointing capabilities.We define four fundamental embodied pointing abilities.We posit that these point-centric representations can serve as embodiment-agnostic intermediaries between perception and action (overcoming heterogeneity).This unified representation enables training on both large-scale internet datasets (from cyberspace) and embodied robotics datasets (from the physical world)(overcoming data scarcity), thereby promoting robust generalization to novel scenarios and tasks.</p>
<p>The core abilities all operate by generating a coordinate point p = (p, q) ∈ [0, w] × [0, h] on the image with width w and height h.But it differs in semantic purpose and output structure: 1 Referring Expression Grounding (REG): This ability localizes objects via linguistic descriptions, generating a point within its corresponding mask.It enables a robot to locate relevant objects via natural language instructions.2 Region Referring Grounding (RRG): This ability identifies a spatial region based on relational language (e.g., "the space between the cup and the bowl") by generating a point in a suitable free-space location for object placement.3 Object Functional Grounding (OFG): This ability identifies functionally significant part-level regions of an object (i.e., affordances).The point must lie within this functional area, such as the handle of a knife for grasping.4 Visual Trace Generation (VTG): This ability produces an ordered sequence of points, τ = {p t | t = 1, 2, . . ., T}, T denotes the sequence length, to form a complete, object-centric manipulation trajectory.This sequence provides a comprehensive spatial plan, enabling a robot to follow motion patterns dictated by instructions while avoiding obstacles.We deliberately use the visual trace of the object-centric rather than the agent-centric to achieve an agent-agnostic visual representation, ensuring a strict correspondence between the visual traces and the task instructions.We present the visualization of each embodied pointing capability in Fig. 2.</p>
<p>Enhancing the Embodied Reasoning Abilities of VLM</p>
<p>To develop general embodied pointing capabilities, we train Embodied-R1 on three data types: embodied spatial reasoning for foundational awareness, general reasoning to preserve its existing skills, and embodied pointing to learn four key pointing abilities.We present an overview of the training data in Fig. 3.</p>
<p>General and Spatial Reasoning Data</p>
<p>The cornerstone of this dataset is Embodied-Spatial-84K, specifically designed for embodied spatial awareness.These data were aggregated from two prominent benchmarks, SAT Ray et al. (2024) and WhatsUp Kamath et al. (2023).To facilitate objective performance evaluation and a verifiable reward structure, all source data were systematically converted into a unified multiple-choice format.Furthermore, to counteract the issue of catastrophic forgetting and preserve general reasoning during specialized training, we introduced a supplementary dataset, ViRL-subset-18K.This comprises diverse, general-knowledge instances.We strategically filtered the ViRL39K Wang et al. (2025a) dataset by removing overly difficult questions and balancing questions across different subjects and types, resulting in a final dataset of 18,000 question-answer pairs.The resulting composite dataset provides a balanced curriculum, fostering specialized spatial skills while safeguarding the model's foundational knowledge.</p>
<p>Embodied Pointing Data</p>
<p>To advance a suite of embodied pointing capabilities, we introduce the Embodied-Points-200K dataset, a high-quality, meticulously curated corpus containing about 200k samples.Due to the multi-solution dilemma inherent in embodied pointing problems, we circumvent the need to construct "question-answer" pairs typical for SFT.Instead, we structure the data as "question-verification" pairs, leveraging RFT for training.Subsequently, pre-defined reward functions for each task evaluate the response based on the verification and calculate the corresponding rewards.We briefly outline the pipeline for generating point data below.For more details on data generation, please refer to Appendix A.</p>
<p>• REG Data: In robotic manipulation, precise localization is critical.However, traditional bounding boxes suffer from inherent ambiguity and fail to meet this requirement.We constructed a point-centric REG dataset.Its data sources are diverse, integrating general web images from RefCOCO Kazemzadeh et al. (2014) with multiple embodied datasets (RoboRef It Lu et al. (2023), Yuan et al. (2024b) and Robo-Point Yuan et al. (2024b)) to ensure broad coverage.We critically adjusted the success criterion for the task: instead of outputting a bounding box, the model must specify a single point.The prediction is considered correct if the point falls within the object's segmentation mask.• RRG Data: To enable robots to comprehend complex spatial placement commands, we developed an automated data generation pipeline for creating relation-aware placement regions.This pipeline processes a large corpus of open-source embodied dataset (about 1 million), and after rigorous filtering, yields 33,000 high-quality samples.The core process includes: 1 Region Extraction: extracting the final position of the manipulated object from the terminal frame; 2 Region Referring: calculating the precise placement of the region relative to a reference object in the scene; and 3 Rendering: rendering this spatial placement information back onto the initial image.To ensure data diversity and quality, we designed a heuristic filtering strategy, which covers a rich variety of spatial relationships, object configurations, and scenes.Furthermore, we leveraged the Isaac Gym simulation engine to generate a synthetic dataset of 3D object rearrangements, equipping the model with 3D spatial awareness.In this task, which takes RGB-D images as input, the model is required to place objects in the correct relative positions according to instructions.Task success is automatically determined and fed back by the simulation based on the true physical state.• OFG Data: To enhance Embodied-R1's fine-grained understanding of functional object parts, we built a dataset of 40,000 functional grasping points.We leveraged the HandAL dataset Guo et al. (2023), which contains 212 real-world objects with meticulously annotated manipulable parts.We converted these part annotations into bounding boxes to serve as verification for the OFG tasks.Meanwhile, we used the GPT-4o model to rewrite function-related questions (e.g., "Which part should be held when using a knife to cut vegetables?"),enhancing the model's ability to generalize its knowledge of object affordances.• VTG Data: We constructed an object-centric visual trace dataset that exclusively tracks the object's movement.The extraction pipeline follows the methodology of Yuan et al. (2025) and consists of three main steps: 1 Key Object Proposal: Using GPT-4o to identify the primary object of interest for a given task. 2 KeyPoint Identification: A self-supervised keypoint extractor Huang et al. (2024c), in conjunction with Grounded-SAM Ren et al. (2024), is used to automatically identify the object's grasping point.3 Point Tracking and Projection: We used Cotracker3 Karaev et al. (2024) to compute the dense temporal visual trace originating from the keypoints.Next, the trajectory is then downsampled into 8 equidistant discrete points and projected back onto the initial image, creating an "image-visual trace" pair.Notably, using multiple pre-trained vision models in the process inevitably introduces noise.We implemented rigorous rule-based filtering and continually validated our approach using a manually annotated test set.</p>
<p>Based on this feedback, we iteratively refined the filtering criteria to improve the quality of the dataset.</p>
<p>Training Strategy Based on the collected data, Embodied-R1 adopts a two-stage training process: the first stage focuses on enhancing spatial reasoning abilities, as spatial reasoning serves as the foundation for point comprehension; the second stage further trains embodied pointing capabilities using point-centric, multi-task mixed data.At each stage, we train a policy π θ (y|x) to generate output y by maximizing expected reward
max θ E x∼𝒟 E y∼π θ (⋅|x) [r(y, x)].
Training is performed using the GRPO Guo et al. (2025), Shao et al.</p>
<p>(2024) algorithm.The behavior policy π θ old generates G candidate responses {y i } G i=1 following input x.The advantage for the i-th response at time step t is computed by normalizing rewards:
Âi,t = r(y i , x) − mean({r(y 1 , x), . . . , r(y G , x)}) std({r(y 1 , x), . . . , r(y G , x)}) .(1)
Then we incorporate a clipped surrogate loss with the clip function:
ℒ(θ) = 1 G G ∑ i=1 |y i | ∑ t=1 min [ π θ (y i,t |x, y i,&lt;t ) π θ old (y i,t |x, y i,&lt;t ) Âi,t , clip ( π θ (y i,t |x, y i,&lt;t ) π θ old (y i,t |x, y i,&lt;t ) , 1 − ϵ, 1 + ϵ) Âi,t ] .
(2)</p>
<p>Multi-task Reward Design</p>
<p>Embodied-R1 mixes the training of multiple tasks at each stage by uniformly sampling from a shared distribution, meaning that different tasks are included in every training batch.We aim to leverage mixed training to share general knowledge across various embodied pointing tasks, thereby achieving better alignment for pointing coordinates and improving training efficiency.However, due to the tendency of reward optimization in RL, simpler tasks are more likely to receive higher rewards and thus dominate policy training.</p>
<p>To address this issue in multi-task mixed training, we have carefully designed multiple verifiable reward functions that support various types of question answering and pointing annotations.Each task has a distinct total reward function ℛ, which is composed of several primary rewards r weighted differently.</p>
<p>Format Rewards: To encourage structured outputs y, we define a unified format reward r format (y).The reasoning process is enclosed within the <think></think> tags, and the answer is wrapped in <answer> </answer>.For pointing reasoning tasks, we further require that the model's output must follow the <point> [[x1,y1],[x2,y2],...]</point> format, where all predicted coordinates are in the original pixel image coordinate system.By adopting a unified point space, we facilitate the sharing of common knowledge across different tasks.If all the above conditions are satisfied, the output is marked as tags valid.r format (y) = I(tags valid(y)).</p>
<p>Accuracy Rewards: For the multiple-choice questions for general QA tasks, we adopt an accuracy reward to evaluate whether the answer extracted from the response matches the standard answer g.The model only receives a positive incentive when the answers are consistent.r acc (y, g) = I(y = g).</p>
<p>Point in Mask Reward: For pointing tasks, point in mask reward function r mask is determined by whether the predicted output point p lies within the ground-truth answer mask M gt .The reward function can be formally expressed as r mask (p, M gt ) = I(p ∈ M gt ).</p>
<p>Point Distance Reward: To improve learning efficiency, we also designed a dense auxiliary reward r dis , which is used to guide the predicted point to approach the target region M gt .The Euclidean distance is d = ∥p − g∥ 2 , where g is the center of the M gt .Given pixel distance thresholds D min_thresh and D max_thresh ,
r dis is then defined as r dis (p, M gt ) = min (1.0, max (0.0, 1.0 − d−D min_thresh D max_thresh −D min_thresh
)), with limiting the scope of this reward to 0-1.</p>
<p>Visual Trace Reward: For evaluating generated visual trace, rewards are derived from trajectory similarity metrics comparing the predicted trajectory τ with the ground-truth trajectory τ gt .First, we compare the number of points in the τ and τ gt .Using the longer one as the reference, we interpolate both trajectories to have the same number of points and then proceed with the calculation Root Mean Square Error (RMSE): d RMSE (τ, τ gt ).Similarly, we use the D RMSE_min and D RMSE_max hyperparameters to ensure that the reward remains between 0 and 1.The reward is calculated as:
r trace (τ, τ gt ) = min (1.0, max (0.0, 1.0 − d RMSE (τ,τ gt )−D RMSE_min D RMSE_max −D RMSE_min )).
Environment Reward: The environment reward r env provides a direct signal of task completion based on feedback from a simulated environment.This reward is used for a portion of the training data originating from the Isaac Gym simulator in the RRG task.It parses and executes the model's output in the simulator and then returns a binary outcome indicating success or failure.The reward r env is formally defined as an indicator function: r env (y) = I(Simulate(y)).</p>
<p>Total Reward: The total reward ℛ for each task is formulated as a combination of these individual reward components.We define the reward function library ℱ = {r format , r acc , r mask , r dis , r trace , r env }.Each component function evaluates a specific aspect of the model's performance.Since we conduct mixed training on multiple tasks, in order to ensure that each task is equally and sufficiently trained, we constrain the total reward ℛ for each task to the range of 0 to 1, which is implemented as follows.ℛ is formulated as a weighted-sum combination: ℛ = ∑ r∈ℱ w r ⋅ r.The task-specific weights w r are normalized to sum to one (∑ r∈ℱ w r = 1).This structure guarantees that the total reward ℛ is also bounded within the range [0, 1] and allows us to tailor the reward signal for each task's specific needs.For example, the RRG task requires simultaneously satisfying the format requirements, ensuring that the predicted points are within the specified region, and accelerating training by employing dense distance rewards.At this point, ℛ RRG is defined as ℛ RRG = 0.1r format + 0.2r dis + 0.7r mask .This consistent scaling of rewards across different tasks is crucial for stabilizing.We refer to Appendix B for detailed hyperparameters.</p>
<p>Action Executor of Embodied-R1</p>
<p>Through simple pointing, Embodied-R1 can be flexibly integrated with various downstream action executors.This allows it to reason from any step, freely select the necessary pointing abilities, and combine them with a motion planner to achieve zero-shot robot control.We offer two primary decision-making approaches for this: the Affordance Points Branch (-P) and the Visual Traces Branch (-V).Affordance Points Branch: Embodied-R1 is capable of predicting multiple key grasping and placement points through abilities such as RRG and OFG.We then utilize CuRobo Sundaralingam et al. (2023) as the motion planner.CuRobo is responsible for generating collision-free paths to guide the robot's end effector to the inferred target affordance points.Visual Traces Branch: Leveraging object-centric visual traces from VTG, we first map 2D visual traces τ to 3D Cartesian coordinates using the Pinhole camera model and initial depth information.</p>
<p>We interpolate these discrete points to form a complete motion trajectory in SE(3) space.Then, the robot follows the visual trace for execution like FSD Yuan et al. (2025).</p>
<p>Experiments</p>
<p>To validate Embodied-R1's generalization in robotic manipulation, we conducted extensive experiments evaluating its Seeing (spatial reasoning and pointing capabilities) and Doing (manipulation tasks) dimensions.</p>
<p>Our evaluation encompassed 11 QA benchmarks, 4 simulated tasks (SIMPLEREnv) Li et al. (2024d), and 8 real-world robot (xArm platform) tasks.We used the Qwen2.5-VL-3B-InstructBai et al. (2025a) model as the initial model.First, we trained it for two epochs using the first-stage Embodied-Spatial-84K and ViRLsubset-18K datasets.Then, we continued training for one epoch with the second-stage EmbodiedPoints-200K dataset.For all experiments, we focus on comparing SFT models trained with the same batch size and data, which we refer to as Embodied-SFT.For training details, please refer to Appendix B and Appendix C.</p>
<p>Evaluation of Spatial Reasoning Capabilities</p>
<p>Setup: To verify the effect of the first stage, we first evaluated Embodied-R1's spatial perception and understanding capabilities.We chose five widely-adopted spatial reasoning benchmarks: CVBench I need to pick up the whiteboard marker.</p>
<p>I hope to grab the yellow screwdriver on the left.</p>
<p>I hope to grasp the white cylindrical bottled vitamins at the bottom.</p>
<p>Could you please pass me the white electronic thermometer.</p>
<p>RoboRefit --Referring Expression Grounding (REG)</p>
<p>Will you please fetch me the vitamins in purple cylindrical bottle on the middle shelf near the left.</p>
<p>Find some spots in the space that is between the blue mug and orange bowl.</p>
<p>Put the bowl inside the kitchen cabinet.</p>
<p>Identify a few places in the free region in between the yellow cup and the cardboard box.</p>
<p>Place the gray bowl in the table.</p>
<p>Where2Place &amp; VABench-P --Region Referring Grounding (RRG)</p>
<p>Put the banana in the pot cardboard fence.Results: We present the results of RoboRefit, Where2Place, VABench-P, and the Part-Afford benchmark in Table 2, the results of VABench-V in Table 3, and the results of Open6DoR in Table 4.Some visualizations are provided in Fig. 4. The following are several observations regarding the experimental results:</p>
<p>(O1) Powerful general VLMs perform poorly on pointing tasks, such as GPT-4o and Qwen2.5-VL,indicating the necessity of specialized training with additional data for these capabilities.This indicates that it is necessary to train embodied VLMs with strong spatial reasoning and pointing abilities.</p>
<p>(O2) Embodied-R1 demonstrates superior performance across key benchmarks.In Table 2 and Table 3, on tasks such as REG (RoboRefit), RRG (Where2Place, VABench-P), OFG (Part-Afford) and VTG (VABench-V),</p>
<p>Embodied-R1 achieves state-of-the-art results compared to various specialized and general VLM baselines.</p>
<p>Compared with FSD and RoboPoint, which also focus on pointing tasks, Embodied-R1 demonstrates stronger embodied reasoning capabilities and achieves significant improvements across multiple tasks.We present the prediction results for different tasks in Fig. 4, demonstrating that Embodied-R1 is capable of mastering a wide range of abilities with a single model.Embodied-R1 also demonstrates high accuracy for small objects and complex spatial relationships in cluttered scenes.</p>
<p>(O3) Embodied-R1 generates highly accurate visual traces for robotic manipulation.On the VABench-V benchmark, Embodied-R1 achieves the lowest RMSE and MAE, indicating its ability to produce precise point sequences for traces, a crucial aspect for reliable action execution.Embodied-R1 also demonstrates significant improvement, indicating that R1 focuses on reasoning about the ideal visual trajectory rather than rote memorization.We refer to more visualization in Fig. 4 and Appendix D.</p>
<p>(O4) Embodied-R1 exhibits strong capabilities in generating precise depth information for embodied tasks.As shown in Table 4, on the Open6DOR-Position benchmark, the Embodied-R1-RGBD variant achieves high performance, highlighting the effectiveness of our model and the value of incorporating RGB and Depth images.However, in the complex relations of level 1, the performance of Embodied-R1 is slightly lower compared to the 2D version.We believe that at higher levels of relational complexity, depth map understanding may be more prone to hallucinations, leading to incorrect answers.Here, we have only conducted a preliminary exploration of the RGBD version, and we consider this to be a promising direction for further improvement.</p>
<p>(O5) Embodied-R1 significantly outperforms models trained solely with SFT.Compared to the Embodied-SFT, Embodied-R1 demonstrates substantial improvements across these tasks, validating the benefits of RFT in developing strong generalization capabilities for embodied point reasoning and generation.   5. Embodied-R1 surpasses the baseline in most tasks with a 56.2% average success rate, demonstrating the strong generalization ability.Notably, Embodied-R1 achieves the best average performance, even outperforming specially fine-tuned VLA models.We believe that, compared to end-to-end VLAs, perception-centered VLAs (modular and affordance-based methods) generally exhibit better zero-shot generalizability.</p>
<p>Real World Robot Evaluation</p>
<p>We conducted zero-shot real-world evaluations of different methods using an xArm 6 robot across eight tabletop manipulation tasks.The experimental setup uses an Intel RealSense L515 LiDAR camera positioned at a third-person perspective, with an image resolution of 640×480.The experimental objects, scenes, and tasks have never been seen in the training data and are directly used to test the performance of the OOD generalization.Table 6 presents the complete results for all tasks, including stage success rates (Grasp./CorrectObj.) and final success rates (Succ.).Fig. 5 illustrates the execution process of all real-world tasks.Compared to the RoboPoint and FSD baselines, Embodied-R1 achieved an improvement of over 60%, reaching a zero-shot success rate of 87.5%.We found that this improvement mainly stems from the poor performance of baseline algorithms on tasks requiring spatial reasoning (such as moving the nearest object).Additionally, the baselines exhibited low success rates when manipulating rigid objects that are difficult to grasp, such as the screwdriver and moka pot.In contrast, the Embodied-R1 correctly identified   these cases and achieved high success rates, demonstrating the effectiveness of its reasoning process.Overall, Embodied-R1-V generated more accurate annotations than Embodied-R1-P, resulting in a slightly higher average success rate.</p>
<p>We also selected the task of "moving the nearest object to the right side of the drawer" to test the model's robustness under the zero-shot setting by introducing visual disturbances such as changes in background, lighting, and height.As shown in Table 7, Embodied-R1 demonstrates outstanding generalization when facing various visual disturbances.Surprisingly, the results indicate that our method exhibits strong adaptability to different lighting conditions, accurately locating objects and completing the task even under the poorest lighting.In addition, changing the background has no effect on task performance at all.This experiment confirms that pointing serves as a general representation capable of maintaining policy performance and robustness under visual disturbances.We present the experimental demo in Fig. 6.</p>
<p>Further Analysis of Embodied-R1</p>
<p>Embodied-R1 already acquired embodied reasoning capabilities.As shown in Fig. 7, we demonstrated the reasoning pathways of Embodied-R1 when facing different tasks.Even without any SFT stage, Embodied-R1 exhibited a human-like and rational reasoning process: it first infers the target object to focus on based on the task goal, then analyzes the relative spatial relationship between the object and the surrounding environment, and subsequently performs step-by-step reasoning to determine the target location (in RRG and VTG tasks), strictly adhering to a structured process of reasoning before providing the final answer.In some cases, Embodied-R1 still demonstrates a clear and accurate reasoning process even when confronted with cluttered scenarios.Appendix D presents several case comparisons between RL and SFT. using VTG tasks in entirely unseen scenarios (simulation, novel embodiment, and hand-drawn sketches).We were pleasantly surprised to find that Embodied-R1 can generalize to vastly different and previously unseen environments.Embodied-R1 was trained solely on real-world data, yet it still made accurate predictions on LIBERO Liu et al. (2023) and Maniskill Tao et al. (2024) simulations, which may suggest a promising direction for sim-to-real transfer.Also, Embodied-R1 adapts to different embodiments, demonstrating the advantage of object-centric visual traces-an embodiment-agnostic representation.The right figure presents a particularly interesting case: even when using a real screwdriver and a hand-drawn box, the predicted visual trajectory remains accurate.</p>
<p>Embodied</p>
<p>Ablations Performance Comparison between SFT and RL:</p>
<p>We further investigate the differences between SFT and RL, and analyze whether the performance improvement brought by RL stems from the reasoning process or the reward-driven paradigm.To this end, we selected two benchmarks for evaluating RRG capabilities and trained four models corresponding to the combinations of (SFT, RL) × (Think, Not Think).RL w/ Think corresponds to the method used in this paper; however, for a fair comparison, we did not use mixed training with all datasets.In RL w/o Think, we modified the prompt to "Do not think, just output the answer" to force the model to output answers directly.SFT w/o Think uses standard question-answer pair data, while in SFT w/ Think, we first designed manual templates and examples, then used GPT-4o to complete the chain-of-thought section.The relevant results are presented in Table 8.We observed that the full RL model (RL w/ Think mechanism) achieved the highest scores on both benchmarks, demonstrating the effectiveness of combining reinforcement learning with the reasoning mechanism, while directly outputting answers leads Q: Identify some points within the free space that is beside the glass cup and to the left of the weights.</p>
<p><think>To identify points in the free space beside the glass cup and to the left of the weights, I need to consider the layout of the objects on the table.The glass cup is placed on the table, and there are two dumbbells to the right of it.The free space would be the area between these objects.However, since the question asks for points in the free space, I should focus on identifying areas where no objects are present.This could include the entire surface of the table or any gaps between the objects.</think><answer><point> [[350, 298], [346,297]]</point></answer> Q: Provide one or more points coordinate of objects region this sentence describes: I would like to pick up the white electric toothbrush.</p>
<p><think>The white electric toothbrush is located on the left side of the sink, closer to the edge of the counter.It appears to be lying horizontally on the countertop.</think><answer><point> [[156, 304], [158,307]]</point></answer> Q: Put the marker in the pot.Use 2D points to mark the manipulated object-centric waypoints to guide the robot to successfully complete the task.</p>
<p><think>To put the marker in the pot, I need to first locate the marker and the pot.The marker is likely on the table or nearby, while the pot is on the table as well.to some performance drop.Models utilizing RL consistently outperformed those without (SFT), indicating that RL plays a key role in OOD generalization.Incorporating COT into supervised learning did not yield significant performance improvements and even resulted in decreased performance on VABench-Point.We believe this may be because inappropriate reasoning processes hinder the learning process, making the learning objectives more complex.</p>
<p>Advantage of Mixed Training:</p>
<p>We performed multi-task joint training of the various abilities required for embodied pointing and reasoning in the second stage.The advantage of this approach is that all abilities can share general knowledge of point coordinates and semantic space alignment during training, thereby achieving better performance.To validate this idea, we conducted multiple sets of experiments comparing the performance of mixed training and unmixed training.In the unmixed training setting, only the data corresponding to the benchmark ability is used; for example, training for one epoch only on the HandAL dataset and then testing on the Part-Afford dataset.As shown in Table 9, joint training consistently improves the success rate across multiple tasks compared to unmixed training.We believe that mixed training enables knowledge sharing among multiple abilities, enhances semantic understanding of spatial information, and thus leads to better generalization.Recent studies in the field of embodied AI have also explored reasoning-driven robotic manipulation.Some works based on SFT involve various forms of templated CoT, such as language prompts Zawalski et al. ( 2024 2025) introduces latent visual planning with action-aligned RL.In this paper, we propose an embodied reasoning VLM tailored for general robotic manipulation.Compared to SFT-based approaches, we do not constrain the reasoning within fixed templates; instead, we stimulate free-form reasoning by integrating pointing with RL.In addition, Embodied-R1 utilizes pointing to precisely anchor the reasoning within the scene, directly guiding manipulation rather than relying on indirect instructions through language or sub-goal image.2025), Gu et al. (2023).These structured and spatially grounded annotations serve as a bridge between visual perception and physical interaction.This approach abstracts out embodiment-specific control mechanisms, thereby enhancing cross-embodiment generalization.In contrast to previous methods that generate a single type of visual signal, we propose a unified "pointing" definition to express diverse and multi-granular manipulation capabilities, and adopt an RL paradigm to improve zero-shot generalization in novel environments.</p>
<p>Related Work Embodied Reasoning in Robotic Manipulation</p>
<p>Spatial Reasoning with VLMs</p>
<p>Conclusion</p>
<p>We propose Embodied-R1, a powerful embodied reasoning VLM that effectively addresses the "seeing-todoing" gap in robotic manipulation by enhancing spatial reasoning and embodied pointing abilities.We first constructed a large-scale, meticulously designed dataset and trained Embodied-R1 using a two-stage RFT paradigm, incorporating a multi-task mixed reward design.Embodied-R1 possesses an understanding of the physical world and, through pointing, realizes a suite of capabilities including grounding, spatial region reference, object affordance marking, and visual trace generation, which are further applied to downstream robotic manipulation tasks.Embodied-R1 achieves state-of-the-art results on multiple benchmark tests and demonstrates robust zero-shot generalization in robotic manipulation tasks, offering a promising direction for the development of more capable embodied AI.We discuss detailed limitations in Appendix E.</p>
<p>Acknowledgment</p>
<p>We would like to thank Zhongwen Xu, Hongyao Tang, Liang Wang, Shuyang Gu, and Chen Li for their participation in the discussions of this paper and for providing valuable insights.In addition, we would especially like to thank Yiyang Huang for the constructive suggestions on improving the figures in the manuscript.</p>
<p>trajectory is most likely to have successfully tracked the object through its entire course of motion without premature termination.Following the selection of the representative trajectory, a two-stage refinement process is applied to produce the final visual trace.The raw trajectory, composed of discrete frame-by-frame coordinates, is first smoothed using cubic spline interpolation.This step transforms the discrete points into a continuous, smooth curve, effectively filtering out high-frequency noise and jitter inherent in the tracking process.From this smoothed curve, we then uniformly sample eight equidistant points.This final set of eight points constitutes the visual trace-a structured, discretized representation of the object's motion, suitable for downstream analysis and model consumption.</p>
<p>We found that this process inevitably faces prediction errors from pre-trained visual models, such as incorrect object grounding or incomplete motion trajectory tracking.To mitigate these issues, we employ stringent rule-based filtering methods using hyperparameters such as size thresholds and trajectory length thresholds.Before annotating each dataset, we iteratively adjust these rules and conduct manual sampling inspections.</p>
<p>Only when the filtering rules are robust enough do we apply them to the full data generation pipeline, ensuring the high quality of the data.</p>
<p>B. Implementation Details of Embodied-R1</p>
<p>Training Hyperparameters: We conducted model training on eight NVIDIA A100 40G GPUs.The first phase was trained for 2 epochs, and the second phase for 1 epoch, with each phase taking approximately 48 hours.The backbone model used is Qwen2.5-VL-3B-Instruct1, with a maximum context length of 4096 and a maximum response length of 2048.The optimizer selected is AdamW, with a learning rate of 1e-6 and a weight decay coefficient of 1e-2.In Embodied-R1, we performed reinforcement learning training based on GRPO Shao et al. (2024), set the number of samples to 8, and introduced a KL penalty (coefficient 1e-2), with a global batch size of 128 for each step.For all experiments, we focus on comparing SFT models trained with the same batch size and data, which we refer to as Embodied-SFT.As for Embodied-SFT, we used exactly the same data but trained with a supervised learning loss, kept the batch size at 128, and trained for 3 epochs.</p>
<p>Reward Hyperparameters: To enable stable multi-task training, we constrain the total reward for each task to the range of 0 to 1 and define the total reward ℛ as a weighted combination ℛ = ∑ r∈ℱ w r ⋅ r.Each task utilizes a different combination of reward terms.Here, we provide the hyperparameters for each task in the Table 10.We would like to add two clarifying points: First, if the task output fails to meet the required parsing format, subsequent analysis cannot proceed successfully, so the reward is set directly to 0. Second, for the VTG task, we introduced an additional constraint on the format: the generated visual trace must consist of exactly 8 points.In practice, we found that without this constraint, the model in the VTG task was prone to reward hacking behavior.It would tend to output only two points to form a straight line, which easily yields a high reward and prematurely terminates exploration.By enforcing the 8-point constraint, we compel the model to perform more complex curve interpolation, thereby improving the performance of visual trace generation.We provide a detailed comparison in the Appendix D.</p>
<p>Task Reward Function ℛ</p>
<p>General QA ℛ = 0.1 r format + 0.9 r acc Spatial QA ℛ = 0.1 r format + 0.9 r acc REG ℛ = 0.1 r format + 0.9 r mask RRG ℛ = 0.1 r format + 0.6 r mask + 0.3 r dis 3D RRG ℛ = 0.1 r format + 0.9 r env OAG ℛ = 0.1 r format + 0.8 r mask + 0.1 r dis VTG ℛ = 0.1 r format + 0.9 r trace</p>
<p>Visual Trace Generation (VTG) Prompt</p>
<p>You are currently a robot performing robotic manipulation tasks.The task instruction is: Your Instruction.Use 2D points to mark the manipulated object-centric waypoints to guide the robot to successfully complete the task.You must provide the points in the order of the trajectory, and the number of points must be 8.You FIRST think about the reasoning process as an internal monologue and then provide the final answer.The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags.The answer consists only of several coordinate points, with the overall format being: <think> reasoning process here </think><answer><point> [[x1, y1], [x2, y2], ..., [x8, y8]]</point></answer>.</p>
<p>D. Additional Experiments</p>
<p>The Phenomenon of Reward Hacking in VTG Tasks We carefully designed the reward function so that the reward for each task is only related to the final goal, thereby avoiding reward hacking caused by intermediate rewards.However, we found that in the VTG task, designing the reward solely based on the distance between the predicted trajectory and the target trajectory can still result in reward hacking.The model quickly learned that in visual trajectory generation tasks, accurately predicting the starting and ending points is both crucial and relatively easy, leading it to converge rapidly to outputs with only these two points while ignoring the generation of intermediate trajectory points.We observed that by forcing the model to output multiple points and applying reward constraints for format reward, it becomes possible to generate complete visual traces.Therefore, we explicitly require the model to output a visual trace with eight points; otherwise, all rewards are set to zero (since the format reward is not satisfied, subsequent analysis will not be performed).</p>
<p>As shown in Table 11, we present the results on VABench-V, demonstrating the performance differences with and without the trajectory point number constraint.After modifying the reward function, the model is better able to fit the visual trace, achieving lower RMSE and a higher GPT score.Qualitative Comparison: Embodied-R1 vs. SFT To provide a deeper insight into the performance gains of our model, we conducted a qualitative analysis comparing Embodied-R1 with the SFT baseline.As illustrated in Fig. 9, the difference in capabilities is stark.In the first task, "move the orange toy inside the right sink in the bin," Embodied-R1 first articulates a clear plan: it identifies the toy's initial position, determines the need to move it towards the sink's center, and then guides it into the bin.This logical reasoning translates into a precise and successful visual trajectory.The SFT baseline, however, produces an erroneous trajectory that fails to place the toy correctly.Similarly, for the second task, "place blue chip bag into white bowl," Embodied-R1 correctly reasons that the task requires moving to the bag, lifting it, positioning it over the bowl, and then lowering it.This step-by-step plan underpins the generated trajectory, which successfully completes the task.The SFT model again fails, generating a flawed path that does not accomplish the goal.These examples strongly suggest that the explicit chain-of-thought reasoning in Embodied-R1 is critical for understanding complex instructions and planning accurate, multi-stage visual trajectories, a capability that the standard SFT approach evidently lacks.(2024) have explored using visual traces as conditional inputs for policies, which improves the learning efficiency and performance of the policies; however, they only focus on designing downstream policies and do not provide a general visual trajectory predictor.Therefore, this integration approach is promising; however, it remains unexplored in the current study.• Untapped Potential in Long-horizon Tasks.The current framework is designed to process single-step instructions and does not natively include a mechanism for decomposing long-horizon commands (e.g., "prepare a meal").However, this could be addressed through a modular, hierarchical approach.Embodied-R1 is well-suited to act as a robust execution module for individual sub-tasks.A high-level embodied planner could first decompose a complex instruction into a sequence of simpler steps, which would then be passed to Embodied-R1 for execution, enabling the system to tackle complex, multi-stage problems.• Inherent Limitations of the "Pointing" Representation.While the pointing representation is effective for localization, placement, and trajectory generation, it may be insufficient for the full spectrum of complex robotic manipulation.Tasks requiring precise force control, twisting, wiping, or intricate interactions with deformable objects demand a richer representation than 2D coordinate points.We believe this issue can be mitigated by coupling the high-level "pointing" commands with a learnable downstream policy that can translate these targets into complex, dynamic actions.The design for Embodied-R1 reflects our primary focus on providing a promising solution for zero-shot generalization, for which a simplified, embodiment-agnostic intermediate representation is a key advantage.• Preliminary Integration of 3D Information.The exploration of an RGB-D version of the model is still in its early stages.The paper notes that in tasks with complex spatial relations, the performance of the RGB-D variant can be slightly lower than its 2D counterpart.It is hypothesized that "depth map understanding may be more prone to hallucinations," indicating that robustly fusing 3D information into the model requires further development.</p>
<p>Figure 1: The Embodied-R1 framework for zero-shot robotic manipulation through "pointing".Embodied-R1 takes visual and textual instructions, performs explicit reasoning, and then generates a visual trace as a universal command.The other panel showcases our comprehensive evaluation, including spatial reasoning, embodied pointing benchmarks, and real-world robot tasks.</p>
<p>Figure 2 :
2
Figure 2: Overview of four embodied pointing abilities.</p>
<p>Figure 3 :
3
Figure 3: Overview of training data: In stage 1, we focus on improving the model's spatial reasoning capability, while incorporating a small amount of general reasoning data.In stage 2, we train the model's embodied pointing capabilities, which comprise four distinct capability items.</p>
<p>Figure 4 :
4
Figure 4: Visualizing Embodied-R1's Performance on Various Pointing Tasks.The model can follow diverse text instructions and generalize its capabilities to novel, unseen environments.capacity, we followed the VABench-VisualTrace (VABench-V) Yuan et al. (2025) evaluation methodology, measuring MAE, RMSE, and LLM Scores.5 In addition, we fine-tuned Embodied-R1 on the synthetic object dataset from YCB Xiang et al. (2017) and ObjaverseXL Deitke et al. (2023), which contains both RGB and depth images.This model is named Embodied-R1-RGBD, where RGB and depth images are input separately to predict the target region's position and depth of the object.For comparison, the model that only takes RGB images as input is referred to as Embodied-R1-RGB.This 3D capability was tested on the Open6DOR-Position benchmark Qi et al. (2025) and compared against other VLM-based methods.</p>
<ol>
<li>3 .
3
Evaluation of Embodied-R1 for Robot ManipulationSimplerEnv Simulation We utilized Embodied-R1 to generate object affordance points and target region points with CuRoboSundaralingam et al. (2023) planner, then performed zero-shot deployment on the WidowX arm.We compared its performance with three types of VLAs.For end-to-end VLAs, we selected OctoTeam et al. (2024), OpenVLA Kim et al. (2024), RoboVLMLi et al. (2024c), and SpatialVLAQu et al. (2025).For modular VLAs, we chose SoFarQi et al. (2025) andMOKA Liu et al. (2024a).The SoFar pipeline integrates Florence-2Xiao et al. (2024),SAM Kirillov et al. (2023), and GPT-4o to accomplish the tasks.MOKA utilizes multiple models to obtain keypoints for task execution.We also compared RoboPointYuan et al. (2024b) andFSD Yuan et al. (2025)  as affordance VLA baselines.The results are presented in Table</li>
</ol>
<p>Task 3 :
3
Move the vise to the red basket Task 2: Move egg to the bowl Task 4: Place the fork in the green bin Task 5: Pick the [x] toothbrush and place it in the basket Task 6: Move the nearest object* to the right side of the drawer Task 7: Put the screwdriver between drawer and the vase Task 1: Pick up the strawberry Task 8: Move the moka pot to the right of drawer Task 8: Move the moka pot to the right of drawer</p>
<p>Figure 5 :
5
Figure 5: The process of Embodied-R1 performing real-world tasks.</p>
<p>Figure 6 :
6
Figure 6: The process of Embodied-R1 performing Task 6 under different visual disturbances.</p>
<p>Figure 7 :
7
Figure 7: Case Analysis: Embodied-R1 possesses embodied reasoning capabilities.It can progressively locate relevant objects and infer spatial relationships according to task instructions, and ultimately provide coordinates through pointing based on embodied scene analysis.</p>
<p>Figure 8 :
8
Figure 8: Embodied-R1 exhibits strong generalization capabilities.We specifically designed tests using VTG tasks in entirely unseen scenarios (simulation, novel embodiment, and hand-drawn sketches), where the model must reason about objects in the images based on common knowledge: (left) LIBERO and ManiSkill simulator tasks, (middle) AhaRobot dual-arm robot tasks, and (right) human-drawn sketches.</p>
<p>),Clark et al. (2025), visual subgoalsZhao et al. (2025), depth mapLee et al. (2025), and spatial relation graphsYuan et al. (2025) to guide action execution.More recent efforts have begun to incorporate RFT for VLA, though these are generally limited to robotic manipulation in simulation environmentsLu et al. (2025),Liu et al. (2025a) or online learning in real-worldChen et al. (2025).Besides, ThinkActHuang et al. (</p>
<p>Developing spatial intelligenceYang et al. (2024),Song et al. (2024),Du et al. (2024),Zhou et al. (2025),Liao et al. (2024),Ray et al. (2024) in open-world environments is essential for enabling generalizable embodied AI.Robots must grasp spatial concepts and object relationships to perform precise manipulationYuan et al. (2024b) and navigation Song et al. (2024), Hong et al. (2023), Li et al. (2024a).Recent research has focused on enhancing the spatial reasoning abilities of VLMs through methods such as SFT with specially customized datasets Du et al. (2024), Ray et al. (2024) and embedding spatial knowledge into training data Cai et al. (2024).These methods address tasks such as recognizing spatial relationships Fu et al. (2024), distance Chen et al. (2024), and counting Cai et al. (2024).SpatialRGPT Cheng et al. (2024) improves spatial cognition by more accurately modeling spatial relationships, while Yuan et al. (2025) and Liu et al. (2025b) enhance the spatial reasoning ability of VLMs by spatial chain-of-thought for step-by-step reasoning.SAT Ray et al. (2024) uses simulation engines and 3D assets to generate complex, configurable spatial reasoning scenarios.Notably, Embodied-R1 employs RL to stimulate model reasoning, achieving stronger OOD generalization compared to the SFT approach.Visual Auxiliary Signals for Robotic Manipulation Utilizing visual auxiliary cuesBharadhwaj et al. (2024),Wen et al. (2023),Xu et al. (2024),Zheng et al. (2024),Yuan et al. (2024a) is a promising approach to enhance robotic manipulation performance.Previous studies have explored various auxiliary signals, such as keypointsYuan et al. (2024bYuan et al. ( , 2025)), affordance mapHuang et al. (2024aHuang et al. ( , 2023)),Li et al. (2024e), bounding boxesLiu et al. (2024a),Huang et al. (2024b), optical flowXu et al. (2024),Wen et al. (2023), and visual trajectoriesYuan et al. (2025),Ji et al. (2025),Li et al. (</p>
<p>Thinking:Figure 9 :Figure 10 :
910
Figure 9: Qualitative comparison of Embodied-R1 and the SFT baseline.Our model Embodied-R1, leverages chain-of-thought reasoning (middle column) to generate a successful visual trace (left column).In contrast, the SFT baseline, which lacks an explicit reasoning process, produces incorrect trajectories (right column) for the same tasks.</p>
<p>Table 1 :
1
Performance comparison on spatial reasoning benchmarks.Bold indicates the highest value among opensource models, and underlined values show the second-highest scores.
CVBenchCRPESATBLINKEmbSp. RankCount 2DRel 3DDep 3DDis Avg. Subj. Pred. Obj. Avg.ValReal MV RelDepth SpRel Obj Avg.TestClosed-source modelsGPT4V62.471.179.868.3 70.4 76.7 65.1 68.5 70.1 44.8 50.7 55.659.772.7 54.9 60.736.1-GPT4o65.985.587.878.2 79.4 81.9 71.8 73.6 75.8 49.4 57.5 60.274.269.2 59.8 65.949.1-Open-source modelsLLaVA-1.5-13B58.246.653.047.8 51.4 57.4 54.2 55.2 55.6 51.4 41.6 41.453.269.9 52.5 54.235.19.4SAT-Dynamic-13B61.589.780.773.0 76.2 60.6 57.6 65.2 61.1 87.7 54.9 44.473.466.4 45.9 57.551.36.6RoboPoint-13B56.577.281.557.7 68.2 66.3 62.4 70.9 66.5 53.3 46.6 44.462.165.7 56.6 57.251.47.2ASMv2-13B58.968.968.968.9 66.4 69.2 59.0 65.3 64.5 63.9 46.7 44.456.565.0 63.9 57.557.47.2FSD-13B62.486.588.086.7 80.9 75.2 65.1 70.4 70.2 73.2 63.3 46.670.278.3 46.7 60.563.34.6RoboBrain-7B64.376.684.072.0 74.2 81.3 71.8 74.8 76.0 45.3 52.2 55.675.881.8 45.1 64.669.34.4Qwen2.5VL-3B68.472.877.068.2 71.6 80.7 71.0 76.1 76.0 48.7 45.1 44.466.979.7 55.7 61.762.85.6Embodied-SFT66.492.385.883.8 82.1 74.7 71.3 73.8 73.3 59.3 65.5 50.481.578.3 54.9 66.363.13.7Embodied-R1 w/o CS 70.490.284.581.0 81.5 80.3 69.9 75.4 75.2 70.0 73.9 47.472.679.7 56.6 64.165.43.4Embodied-R170.690.884.784.8 82.7 82.2 70.7 75.4 76.1 70.0 76.3 51.176.680.4 57.4 66.467.42.1</p>
<p>Table 2 : Performance on 4 Pointing benchmarks.
2
The score is the accuracy of points falling within the target region.
Table 3: Performance on VABench-V. Lower valuesare better for RMSE/MAE, higher is better for LLMScore.ModelRoboRefit Where2Place VABench-P Part-AffordGPT4o15.2829.069.3010.15ModelRMSE ↓ MAE ↓ LLM Score ↑ASMv2 RoboBrain RoboPoint FSD Qwen2.5VL Embodied-SFT Embodied-R148.40 10.10 49.82 56.73 74.90 83.85 85.5822.00 16.60 46.01 45.81 31.11 41.25 69.5010.07 7.00 19.09 61.82 9.89 50.46 66.0013.75 25.25 27.60 9.55 23.42 40.20 56.63GPT-4o DINOv2 Predictor RoboBrain FSD Embodied-SFT Embodied-R1136.1 128.3 121.6 78.3 109.4 77.8113.5 117.5 103.8 63.4 65.2 45.04.4 4.0 4.5 6.2 6.2 7.3</p>
<p>Table 4 : Performance on Open6DOR-Position Benchmark
4BenchmarkLevel0 Level1 OverallGPT-4V46.839.145.2Qwen2.5-VL Bai et al. (2025b)59.536.254.9VoxPoser Huang et al. (2023)35.621.732.6SoFar Qi et al. (2025)96.081.593.0Embodied-SFT62.444.758.9Embodied-R1-RGB68.559.466.8Embodied-R1-RGBD99.850.990.2</p>
<p>Table 5 :
5
Yuan et al. (2025)ion on WidowX Robot.The results of baselines are derived fromQu et al. (2025).ZS: zero-shot, FT: fine-tuning using BridgeData.The results of Embodied-R1 and SoFar are averaged over three rounds of experiments, with each round consisting of 24 runs.The other results are derived fromYuan et al. (2025).Spoon→Towel: Put Spoon on Towel, Carrot→Plate: Put Carrot on Plate, Green→Yellow: Stack Green Block on Yellow Block, Eggplant→Basket: Put Eggplant in Yellow Basket.
ModelSpoon→TowelCarrot→PlateGreen→YellowEggplant→BasketAvg.GraspSucc.GraspSucc.GraspSucc.GraspSucc.End-to-end VLAsRT-1-X O'Neill et al. (2023)16.7%0.0%20.8%4.2%8.3%0.0%0.0%0.0%1.1%Octo-S Team et al. (2024)77.8% 47.2% 27.8%9.7%40.3%4.2%87.5%56.9%30.0%OpenVLA Kim et al. (2024)4.1%0.0%33.3%0.0%12.5%0.0%8.3%4.1%1.0%RoboVLM (ZS) Li et al. (2024c)37.5% 20.8% 33.3% 25.0%8.3%8.3%0.0%0.0%13.5%RoboVLM (FT) Li et al. (2024c)54.2% 29.2% 25.0% 25.0% 45.8% 12.5%58.3%58.3%31.3%SpatialVLA (ZS) Qu et al. (2025) 25.0% 20.8% 41.7% 20.8% 58.3% 25.0%79.2%70.8%34.4%SpatialVLA (FT) Qu et al. (2025) 20.8% 16.7% 29.2% 25.0% 62.5% 29.2% 100.0% 100.0% 42.7%Modular VLAsMOKA Liu et al. (2024a)75.0% 45.8% 64.0% 41.6% 83.3% 33.3%50.0%12.5%33.3%SoFar Qi et al. (2025)69.4% 55.5% 73.6% 56.9% 87.5% 62.5%68.0%40.2%53.8%Affordance VLAsRoboPoint Yuan et al. (2024b)58.3% 16.7% 41.7% 20.8% 54.2%8.3%66.7%25.0%17.7%FSD Yuan et al. (2025)58.3% 41.6% 58.3% 50.0% 91.6% 33.3%37.5%37.5%40.6%Embodied-R165.2% 62.5% 81.9% 68.0% 93.0% 36.1%62.5%58.3%56.2%</p>
<p>Table 6 :
6
Real-world experimental evaluation results.The first two tasks were conducted 5 times each, while the other tasks were conducted 6 times each.The best results are highlighted in bold.Embodied-R1-P: the version that predicts grasp and placement points; Embodied-R1-V: the version that predicts visual traces.[x]: The instruction for each trial is a randomly selected color, including white/green/red/yellow. Nearest object<em>: grasping the object closest to the camera's viewpoint.Grasp.and Correct Obj.represent stage success rates, i.e., successfully grasping the target object.Succ.denotes the overall success rate.
PickMoveMove the visePlace the forkPick the [x] toothbrushMove the nearestPut the screwdriver Move the mokaAvgup thethe egg toto thein theand place itobject</em> to the rightbetween drawerpot to thestrawberry the bowlred basketgreen binto the bucketside of the drawerand the vaseright of drawerSucc.Succ.Grasp.Succ.Grasp.Succ.Correct Obj.Succ.Correct Obj.Succ.Grasp.Succ.Grasp. Succ.Succ.RoboPoint40.0%60.0%50.0%0.0%0.0%0.0%16.7%0.0%0.0%0.0%66.7%0.0%0.0%0.0% 12.5%FSD20.0%80.0%66.7%33.3%16.7%16.7%16.7%0.0%0.0%0.0%66.7%33.3%16.7% 16.7% 25.0%Embodied-R1-P100.0%100.0%66.7%50.0% 100.0% 100.0%100.0%83.3%100.0%100.0% 100.0% 100.0% 50.0% 33.3% 83.3%Embodied-R1-T100.0%100.0%100.0% 100.0% 100.0% 100.0%100.0%66.7%100.0%100.0% 100.0% 100.0% 33.3% 33.3% 87.5%</p>
<p>Table 7 :
7
Performance under different visual disturbances for the task "Move the nearest object* to the right side of the drawer".Each task runs 6 tests.Visualizations are in Fig.6.
ModelTaskGrasp. Succ.Original100% 100%Embodied-R1Background Change Background+Light Change100% 100% 83% 83%Background+Light+Height Change83%83%</p>
<p>Table 8 : Performance Comparison between SFT and RL on RRG benchmarks.
8ModelRL Think Where2Place VABench-PRL w/ Think✓✓65.5065.39RL w/o Think✓✗63.0060.50SFT w/ Think✗✓41.2547.67SFT w/o Think ✗✗36.8550.46</p>
<p>Table 9 : Comparison of Mixed Training of Multiple Datasets and Only Training Corresponding Dataset.
9MixedUnmixedTraining TrainingPart-Afford56.6351.25Where2Place69.5065.50VABench-P66.0065.39</p>
<p>Table 10 :
10
Detailed Reward Functions for Each Task</p>
<p>C. Embodied-R1 Prompts for Each Task Referring Expression Grounding (REG) Prompt Provide</p>
<p>one or more points coordinate of object region this sentence describes: Your Instruction.The results are presented in a format <point>[[x1,y1], [x2,y2], ...]</point>.You FIRST think about the reasoning process as an internal monologue and then provide the final answer.The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags.The answer consists only of several coordinate points, with the overall format being: <think> reasoning process here </think><answer><point>[[x1, y1], [x2, y2], ...]</point></answer>
Region Referring Grounding (RRG) PromptYou are currently a robot performing robotic manipulation tasks. The task instruction is: YourInstruction. Use 2D points to mark the target location where the object you need to manipulate in thetask should ultimately be moved. You FIRST think about the reasoning process as an internal monologueand then provide the final answer. The reasoning process and answer are enclosed within <think></think> and <answer> </answer> tags. The answer consists only of several coordinate points,with the overall format being: <think> reasoning process here </think><answer><point>[[x1,y1], [x2, y2], ...]</point></answer>.Object Functional Grounding (OFG) PromptPlease provide the 2D points coordinates of the region this sentence describes: Your Instruction. Theresults are presented in a format <point>[[x1,y1], [x2,y2], ...]</point>. You FIRST think about thereasoning process as an internal monologue and then provide the final answer. The reasoning processand answer are enclosed within <think> </think> and <answer> </answer> tags. The answerconsists only of several coordinate points, with the overall format being: <think> reasoning processhere </think><answer><point>[[x1, y1], [x2, y2], ...]</point></answer>.</p>
<p>Table 11 :
11
Comparasion of w/ and w/o Point Num Reward.Bolds are better.
VABench-VisualTraceRMSE↓ MAE↓ GPT Score↑w/ Point Num Constraint77.83 44.977.27w/o Point Num Constraint 105.259.75.57
https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct
https://embodied-r1.github.io/Code Repository: https://github.com/pickxiguapi/Embodied-R1Datasets: https://huggingface.co/Iff YuanAppendixA. Automatic Data Generation PipelineIn this section, we provide additional explanations regarding the generation of certain datasets.The generation processes of both the RRG and VTG datasets are improved based onYuan et al. (2025).3D RRG Data Generation using Isaac Gym Simulation The dataset comprises 10,028 tasks, each situated in a tabletop scene containing multiple objects.The input for each task consists of processed RGB and depth images, accompanied by a language instruction describing the desired target position of an object within the scene (e.g., "place the cup between the book and the spoon").Based on these instructions, the model is required to output the 3D position of the target object in the camera coordinate system, specified by its pixel coordinates (X, Y) and a depth value D in millimeters.The D value is obtained either through monocular depth estimation or by reasoning from the scene geometry.The generation process of our dataset is informed by the methodology of Open6DorQi et al. (2025).The object set utilized contains over 200 items spanning more than 70 distinct categories, originally sourced from the YCBXiang et al. (2017)and Objaverse-XLDeitke et al. (2023)datasets.These objects underwent a rigorous selection process to ensure their physical integrity and semantic suitability for tabletop arrangements.All selected objects were subsequently scale-normalized and uniformly represented using a consistent mesh format.In terms of scene configuration, between two and five objects were randomly selected from the object set and placed on a tabletop with random initial poses.For each configured scene, we rendered both RGB and depth images.The depth values represent ground truth measurements, with the scene's depth range spanning from 600 mm to 1700 mm.For subsequent processing convenience, the depth images were normalized to an 8-bit grayscale format (0-255).We filtered out low-quality scenes, such as those exhibiting implausible object placements or severe occlusions.To augment the dataset's quality and volume, we expanded a subset of high-quality, filtered data by algorithmically generating variations in task descriptions, such as substituting directional prepositions or altering object relations.Then, the task instructions are formulated in two primary types: basic directional commands (e.g., left, right, top, behind, front) and relational commands (e.g., "between," "center of").All instructions adhere to a standardized template, for instance, "Place object A in front of object B," where A and B are objects present in the scene.During training, the model receives RGB and depth image inputs and is required to output the target's coordinates (X, Y) and depth value D. The simulated environment then executes and evaluates the predicted position, giving positive rewards for correct predictions.VTG Dataset Generation PipelineFor each video sequence, we first process the initial frame to perform instance segmentation on the manipulated object, thereby obtaining its pixel-wise mask.Instead of relying on a single tracking point, which is susceptible to tracking failure from occlusion or rapid motion, we sample a set of three distinct query points from within this mask.This multi-point initialization serves as a redundancy measure, significantly enhancing the robustness of the tracking process.These points are strategically chosen to represent the object's initial state before they are passed, along with the full video sequence, to the tracking model for trajectory prediction.The core of the trajectory generation is handled by the CoTracker modelKaraev et al. (2024), which takes the initialized query points and video as input.The model concurrently tracks each point throughout the sequence, yielding a set of three candidate trajectories.As these trajectories may vary in quality and completeness due to transient tracking errors, a selection criterion is required to identify the single most representative path.We employ a path length heuristic for this purpose, calculating the total Euclidean distance of each trajectory.The trajectory with the longest path is selected as the definitive motion path for the object.The rationale behind this criterion is that the longest
Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report. 2025a</p>
<p>Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, arXiv:2502.13923Jun Tang, et al. Qwen2. 5-vl technical report. 2025barXiv preprint</p>
<p>Track2act: Predicting point tracks from internet videos enables generalizable robot manipulation. Homanga Bharadhwaj, Roozbeh Mottaghi, Abhinav Gupta, Shubham Tulsiani ; Wenxiao, Iaroslav Cai, Jianhao Ponomarenko, Xiaoqi Yuan, Wankou Li, Hao Yang, Bo Dong, Zhao, arXiv:2405.01527arXiv:2406.13642Spatialbot: Precise spatial understanding with vision language models. 2024. 2024arXiv preprint</p>
<p>Spatialvlm: Endowing vision-language models with spatial reasoning capabilities. Boyuan Chen, Zhuo Xu, Sean Kirmani, Brain Ichter, Dorsa Sadigh, Leonidas Guibas, Fei Xia, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Conrft: A reinforced fine-tuning method for vla models via consistency policy. Yuhui Chen, Shuai Tian, Shugao Liu, Yingting Zhou, Haoran Li, Dongbin Zhao, arXiv:2502.054502025arXiv preprint</p>
<p>An-Chieh Cheng, Hongxu Yin, Yang Fu, Qiushan Guo, Ruihan Yang, Jan Kautz, Xiaolong Wang, Sifei Liu, arXiv:2406.01584Spatialrgpt: Grounded spatial reasoning in vision language model. 2024arXiv preprint</p>
<p>Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Sergey Quoc V Le, Yi Levine, Ma, arXiv:2501.17161Sft memorizes, rl generalizes: A comparative study of foundation model post-training. 2025arXiv preprint</p>
<p>Action-free reasoning for policy generalization. Jaden Clark, Suvir Mirchandani, Dorsa Sadigh, Suneel Belkhale, arXiv:2502.037292025arXiv preprint</p>
<p>Objaverse-xl: A universe of 10m+ 3d objects. Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, Advances in Neural Information Processing Systems. 202336</p>
<p>Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models. Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, arXiv:2409.171462024arXiv preprint</p>
<p>Embspatial-bench: Benchmarking spatial understanding for embodied tasks with large vision-language models. Mengfei Du, Binhao Wu, Zejun Li, Xuanjing Huang, Zhongyu Wei, arXiv:2406.057562024arXiv preprint</p>
<p>Blink: Multimodal large language models can see but not perceive. Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah A Smith, Wei-Chiu Ma, Ranjay Krishna, European Conference on Computer Vision. Springer2024</p>
<p>Rt-trajectory: Robotic task generalization via hindsight trajectory sketches. Jiayuan Gu, Sean Kirmani, Paul Wohlhart, Yao Lu, Montserrat Gonzalez Arenas, Kanishka Rao, Wenhao Yu, Chuyuan Fu, Keerthana Gopalakrishnan, Zhuo Xu, arXiv:2311.019772023arXiv preprint</p>
<p>HANDAL: A dataset of real-world manipulable object categories with pose annotations, affordances, and reconstructions. Andrew Guo, Bowen Wen, Jianhe Yuan, Jonathan Tremblay, Stephen Tyree, Jeffrey Smith, Stan Birchfield, IROS2023</p>
<p>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, arXiv:2501.129482025arXiv preprint</p>
<p>3d-llm: Injecting the 3d world into large language models. Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen, Chuang Gan, Advances in Neural Information Processing Systems. 202336</p>
<p>Thinkact: Visionlanguage-action reasoning via reinforced visual latent planning. Chi-Pin Huang, Yueh-Hua Wu, Min-Hung Chen, Yu-Chiang Frank, Wang , Fu-En Yang, arXiv:2507.168152025arXiv preprint</p>
<p>Siyuan Huang, Haonan Chang, Yuhan Liu, Yimeng Zhu, Hao Dong, Peng Gao, Abdeslam Boularias, Hongsheng Li, arXiv:2406.07549A3vlm: Actionable articulation-aware vision language model. 2024aarXiv preprint</p>
<p>Manipvqa: Injecting robotic affordance and physically grounded information into multi-modal large language models. Siyuan Huang, Iaroslav Ponomarenko, Zhengkai Jiang, Xiaoqi Li, Xiaobin Hu, Peng Gao, Hongsheng Li, Hao Dong, 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2024b</p>
<p>Wenlong Huang, Chen Wang, Ruohan Zhang, Yunzhu Li, Jiajun Wu, Li Fei-Fei, arXiv:2307.05973Voxposer: Composable 3d value maps for robotic manipulation with language models. 2023arXiv preprint</p>
<p>Wenlong Huang, Chen Wang, Yunzhu Li, Ruohan Zhang, Li Fei-Fei, arXiv:2409.01652Rekep: Spatio-temporal reasoning of relational keypoint constraints for robotic manipulation. 2024carXiv preprint</p>
<p>Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, Akila Ostrow, Alan Welihinda, Alec Hayes, Radford, arXiv:2410.21276Gpt-4o system card. 2024arXiv preprint</p>
<p>Yuheng Ji, Huajie Tan, Jiayu Shi, Xiaoshuai Hao, Yuan Zhang, Hengyuan Zhang, Pengwei Wang, Mengdi Zhao, Yao Mu, Pengju An, Xinda Xue, Qinghang Su, Huaihai Lyu, Xiaolong Zheng, Jiaming Liu, Zhongyuan Wang, Shanghang Zhang, arXiv:2502.21257Robobrain: A unified brain model for robotic manipulation from abstract to concrete. 2025arXiv preprint</p>
<p>Swe-bench: Can language models resolve real-world github issues?. Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, Karthik Narasimhan, 2024</p>
<p>What's "up" with vision-language models? investigating their struggle with spatial reasoning. Amita Kamath, Jack Hessel, Kai-Wei Chang, 2023</p>
<p>Nikita Karaev, Iurii Makarov, Jianyuan Wang, Natalia Neverova, arXiv:2410.11831Andrea Vedaldi, and Christian Rupprecht. Cotracker3: Simpler and better point tracking by pseudo-labelling real videos. 2024arXiv preprint</p>
<p>ReferItGame: Referring to objects in photographs of natural scenes. Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, Tamara Berg, 10.3115/v1/D14-1086Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Alessandro Moschitti, Bo Pang, Walter Daelemans, the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)Doha, QatarAssociation for Computational LinguisticsOctober 2014</p>
<p>Openvla: An open-source vision-language-action model. Jin Moo, Karl Kim, Siddharth Pertsch, Ted Karamcheti, Ashwin Xiao, Suraj Balakrishna, Rafael Nair, Ethan Rafailov, Grace Foster, Pannag Lam, Sanketi, arXiv:2406.092462024arXiv preprint</p>
<p>Segment anything. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, Proceedings of the IEEE/CVF international conference on computer vision. the IEEE/CVF international conference on computer vision2023</p>
<p>Jason Lee, Jiafei Duan, Haoquan Fang, Yuquan Deng, Shuo Liu, Boyang Li, Bohan Fang, Jieyu Zhang, Yi Ru Wang, Sangho Lee, arXiv:2508.07917Action reasoning models that can reason in space. 2025arXiv preprint</p>
<p>Chengzu Li, Caiqi Zhang, Han Zhou, Nigel Collier, Anna Korhonen, Ivan Vulić, Topviewrs, arXiv:2406.02537Visionlanguage models as top-view spatial reasoners. 2024aarXiv preprint</p>
<p>Cristina Xiang, Jongwoo Mata, Kumara Park, Kahatapitiya, Sung Yoo, Jinghuan Jang, Kanchana Shang, Ryan Ranasinghe, Mu Burgert, Yong Jae Cai, Lee, arXiv:2406.20095Supercharging robot learning data for vision-language policy. 2024barXiv preprint</p>
<p>Towards generalist robot policies: What matters in building vision-language-action models. Xinghang Li, Peiyan Li, Minghuan Liu, Dong Wang, Jirong Liu, Bingyi Kang, Xiao Ma, Tao Kong, Hanbo Zhang, Huaping Liu, arXiv:2412.140582024carXiv preprint</p>
<p>Evaluating real-world robot manipulation policies in simulation. Xuanlin Li, Kyle Hsu, Jiayuan Gu, Karl Pertsch, Oier Mees, Homer Rich Walke, Chuyuan Fu, Ishikaa Lunawat, Isabel Sieh, Sean Kirmani, arXiv:2405.059412024darXiv preprint</p>
<p>Yi Li, Yuquan Deng, Jesse Zhang, Joel Jang, Marius Memmel, Raymond Yu, Caelan Reed Garrett, Fabio Ramos, Dieter Fox, Anqi Li, arXiv:2502.05485Hierarchical action models for open-world robot manipulation. 2025arXiv preprint</p>
<p>Laso: Language-guided affordance segmentation on 3d object. Yicong Li, Na Zhao, Junbin Xiao, Chun Feng, Xiang Wang, Tat-Seng Chua, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024e</p>
<p>Reasoning paths with reference objects elicit quantitative spatial reasoning in large vision-language models. Yuan-Hong Liao, Rafid Mahmood, Sanja Fidler, David Acuna, arXiv:2409.097882024arXiv preprint</p>
<p>Data scaling laws in imitation learning for robotic manipulation. Fanqi Lin, Yingdong Hu, Pingyue Sheng, Chuan Wen, Jiacheng You, Yang Gao, arXiv:2410.186472024arXiv preprint</p>
<p>Libero: Benchmarking knowledge transfer for lifelong robot learning. Bo Liu, Yifeng Zhu, Chongkai Gao, Yihao Feng, Qiang Liu, Yuke Zhu, Peter Stone, Advances in Neural Information Processing Systems. 202336</p>
<p>Moka: Open-world robotic manipulation through mark-based visual prompting. Fangchen Liu, Kuan Fang, Pieter Abbeel, Sergey Levine, 2024a</p>
<p>Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, Yong Jae Lee, Llava-next: Improved reasoning, ocr, and world knowledge. 2024b</p>
<p>What can rl bring to vla generalization? an empirical study. Jijia Liu, Feng Gao, Bingwen Wei, Xinlei Chen, Qingmin Liao, Yi Wu, Chao Yu, Yu Wang, arXiv:2505.197892025aarXiv preprint</p>
<p>Advancing spatial reasoning through coordinate alignment and chain-of-thought for embodied task planning. Yuecheng Liu, Dafeng Chi, Shiguang Wu, Zhanguang Zhang, Yaochen Hu, Lingfeng Zhang, Yingxue Zhang, Shuang Wu, Tongtong Cao, Guowei Huang, arXiv:2501.100742025barXiv preprint</p>
<p>Guanxing Lu, Wenkai Guo, Chubin Zhang, Yuheng Zhou, Haonan Jiang, Zifeng Gao, Yansong Tang, Ziwei Wang, arXiv:2505.18719Vla-rl: Towards masterful and general robotic manipulation with scalable reinforcement learning. 2025arXiv preprint</p>
<p>Vl-grasp: a 6-dof interactive grasp policy for language-oriented objects in cluttered indoor scenes. Yuhao Lu, Yixuan Fan, Beixing Deng, Fangfu Liu, Yali Li, Shengjin Wang, 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2023</p>
<p>Affordance detection of tool parts from geometric features. Austin Myers, Ching L Teo, Cornelia Fermüller, Yiannis Aloimonos, 2015 IEEE international conference on robotics and automation (ICRA). IEEE2015</p>
<p>Soroush Nasiriany, Sean Kirmani, Tianli Ding, Laura Smith, Yuke Zhu, Danny Driess, Dorsa Sadigh, Ted Xiao, arXiv:2411.02704Rt-affordance: Affordances are versatile intermediate representations for robot manipulation. 2024arXiv preprint</p>
<dl>
<dt>Open x-embodiment: Robotic learning datasets and rt-x models. Jake O' Neill, Abraham Arthurs, Fábio Avila Belbute-Peres, Julian Balaguer, Sarah Bechtle, Gemma Bidoia, Kyle Burden, Erwin Chang, Sheila Chen, Todor Davchev, arXiv:2310.088642023arXiv preprint</dt>
<dd>
<p>Openai, Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, Alex Iftimie, Alex Karpenko, Alex Tachard Passos, Alexander Neitz, Alexander Prokofiev, Alexander Wei, Allison Tam, Ally Bennett, Ananya Kumar, Andre Saraiva, Andrea Vallone, Andrew Duberstein, Andrew Kondrich, Andrey Mishchenko, Andy Applebaum, Angela Jiang, Ashvin Nair, Barret Zoph, Behrooz Ghorbani, Ben Rossen, Benjamin Sokolowsky, Boaz Barak, Bob Mcgrew, Borys Minaiev, Botao Hao, Bowen Baker, Brandon Houghton, Brandon Mckinzie, Brydon Eastman, Camillo Lugaresi, Cary Bassin, Cary Hudson, Ming Chak, Charles Li, Chelsea De Bourcy, Chen Voss, Chong Shen, Chris Zhang, Chris Koch, Christopher Orsinger, Claudia Hesse, Clive Fischer, Dan Chan, Daniel Roberts, Daniel Kappler, Daniel Levy, David Selsam, David Dohan, David Farhi, David Mely, Dimitris Robinson, Doug Tsipras, Dragos Li, Eben Oprica, Eddie Freeman, Edmund Zhang, Elizabeth Wong, Enoch Proehl, Eric Cheung, Eric Mitchell, Erik Wallace, Evan Ritter, Fan Mays, Felipe Petroski Wang, Filippo Such, Florencia Raso, Foivos Leoni, Francis Tsimpourlas, Song, Freddie Fred Von Lohmann, Geoff Sulit, Giambattista Salmon, Gildas Parascandolo, Grace Chabot, Greg Zhao, Guillaume Brockman, Hadi Leclerc, Haiming Salman, Hao Bao, Hart Sheng, Hessam Andrin, Hongyu Bagherinezhad, Hunter Ren, Hyung Won Lightman, Ian Chung, Kivlichan, O' Ian, Ian Connell, Ignasi Osband, Ilge Clavera Gilaberte, Ilya Akkaya, Ilya Kostrikov, Irina Sutskever, Jakub Kofman, James Pachocki, Jason Lennon, Jean Wei, Jerry Harb, Jiacheng Twore, Jiahui Feng, Jiayi Yu, Jie Weng, Jieqi Tang, Joaquin Quiñonero Yu, Joe Candela, Joel Palermo, Johannes Parish, John Heidecke, John Hallman, Jonathan Rizzo, Jonathan Gordon, Jonathan Uesato, Joost Ward, Julie Huizinga, Kai Wang, Kai Chen, Karan Xiao, Karina Singhal, Karl Nguyen, Katy Cobbe, Kayla Shi, Kendra Wood, Keren Rimbach, Kevin Gu-Lemberg, Kevin Liu, Kevin Lu, Kevin Stone, Lama Yu, Lauren Ahmad, Leo Yang, Leon Liu, Leyton Maksin, Liam Ho, Lilian Fedus, Linden Weng, Lindsay Li, Lindsey Mccallum, Lorenz Held, Lukas Kuhn, Lukasz Kondraciuk, Luke Kaiser, Madelaine Metz, Maja Boyd, Manas Trebacz, Mark Joglekar, Marko Chen, Mason Tintor, Matt Meyer, Matt Jones, Max Kaufer, Meghan Schwarzer, Mehmet Shah, Melody Y Yatbaz, Mengyuan Guan, Mengyuan Xu, Mia Yan, Mianna Glaese, Michael Chen, Michael Lampe, Michele Malek, Michelle Wang, Mike Fradin, Mikhail Mcclay, Miles Pavlov, Mingxuan Wang, Mira Wang, Murati, Mo Bavarian ; Rui, Ryan Shu, Ryan Cheu, Saachi Greene, Sam Jain, Sam Altman, Sam Toizer, Samuel Toyer, Sandhini Miserendino, Santiago Agarwal, Sasha Hernandez, Scott Baker, Scottie Mckinney, Shengjia Yan, Shengli Zhao, Shibani Hu, Shraman Santurkar, Shuyuan Ray Chaudhuri, Siyuan Zhang, Spencer Fu, Steph Papay, Suchir Lin, Suvansh Balaji, Szymon Sanjeev, Wenda Sidor ; Weiyi Zheng, Wes Zhou, Wojciech Mccabe, Yann Zaremba, Yinghai Dubois, Yining Lu, Young Chen, Yu Cha, Yuchen Bai, Yuchen He, Yunyun Zhang, Zheng Wang, Zhuohan Shao, Li, Openai o1 system card. Mostafa Rohaninejad, Nat Mcaleese, Neil Chowdhury, Neil Chowdhury, Nick Ryder, Nikolas Tezak, Noam Brown, Ofir Nachum, Oleg Boiko, Oleg Murk, Olivia Watkins, Patrick Chao, Paul Ashbourne, Pavel Izmailov, Peter Zhokhov, Rachel Dias, Rahul Arora, Randall Lin, Rapha Gontijo Lopes, Raz Gaon, Reah Miyara, Reimar Leike, Renny Hwang, Rhythm Garg, Robin Brown, Roshan James,; Tal Broda, Aidan Clark, Tao Wang, Taylor Gordon, Ted Sanders2024Tejal Patwardhan, Thibault Sottiaux, Thomas Degry, Thomas Dimson, Tianhao Zheng, Timur Garipov, Tom Stasi, Trapit Bansal, Trevor Creech, Troy Peterson, Tyna Eloundou, Valerie Qi, Vineet Kosaraju, Vinnie Monaco, Vitchyr Pong, Vlad Fomenko,</p>
</dd>
</dl>
<p>Zekun Qi, Wenyao Zhang, Yufei Ding, Runpei Dong, Xinqiang Yu, Jingwen Li, Lingyun Xu, Baoyu Li, Xialin He, Guofan Fan, arXiv:2502.13143Language-grounded orientation bridges spatial reasoning and object manipulation. 2025arXiv preprint</p>
<p>Delin Qu, Haoming Song, Qizhi Chen, Yuanqi Yao, Xinyi Ye, Yan Ding, Zhigang Wang, Jiayuan Gu, Bin Zhao, Dong Wang, arXiv:2501.15830Exploring spatial representations for visual-language-action model. 2025arXiv preprint</p>
<p>Arijit Ray, Jiafei Duan, Reuben Tan, Dina Bashkirova, Rose Hendrix, Kiana Ehsani, Aniruddha Kembhavi, Bryan A Plummer, Ranjay Krishna, Kuo-Hao Zeng, arXiv:2412.07755Spatial aptitude training for multimodal language models. 2024arXiv preprint</p>
<p>Grounded sam: Assembling open-world models for diverse visual tasks. Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, arXiv:2401.141592024arXiv preprint</p>
<p>Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Li, Wu, arXiv:2402.03300Pushing the limits of mathematical reasoning in open language models. 2024arXiv preprint</p>
<p>Hee Chan, Valts Song, Jonathan Blukis, Stephen Tremblay, Yu Tyree, Stan Su, Birchfield, arXiv:2411.16537Robospatial: Teaching spatial understanding to 2d and 3d vision-language models for robotics. 2024arXiv preprint</p>
<p>Curobo: Parallelized collision-free robot motion generation. Balakumar Sundaralingam, Siva Kumar Sastry, Adam Hari, Caelan Fishman, Karl Garrett, Valts Van Wyk, Alexander Blukis, Helen Millane, Ankur Oleynikova, Fabio Handa, Ramos, 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE2023</p>
<p>Stone Tao, Fanbo Xiang, Arth Shukla, Yuzhe Qin, Xander Hinrichsen, Xiaodi Yuan, Chen Bao, Xinsong Lin, Yulin Liu, Tse-Kai Chan, arXiv:2410.00425Gpu parallelized robotics simulation and rendering for generalizable embodied ai. 2024arXiv preprint</p>
<p>Kimi k1. 5: Scaling reinforcement learning with llms. Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, arXiv:2501.125992025arXiv preprint</p>
<p>Octo Team, Rt-X Team, Anthony Brohan, Noah Brown, Lauren Chen, Michael Cheng, Krzysztof Choromanski, Eamonn Cullina, Gabe Dalal, Chelsea Fu, Florian Golemo, arXiv:2403.10164An open-source generalist robot policy. 2024arXiv preprint</p>
<p>Cambrian-1: A fully open, vision-centric exploration of multimodal llms. Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Charitha Sai, Jihan Akula, Shusheng Yang, Adithya Yang, Xichen Iyer, Austin Pan, Rob Wang, Yann Fergus, Saining Lecun, Xie, 2024</p>
<p>Bridgedata v2: A dataset for robot learning at scale. Homer Rich Walke, Kevin Black, Tony Z Zhao, Quan Vuong, Chongyi Zheng, Philippe Hansen-Estruch, Andre Wang He, Vivek Myers, Jin Moo, Max Kim, Du, Conference on Robot Learning. PMLR2023</p>
<p>Vl-rethinker: Incentivizing self-reflection of vision-language models with reinforcement learning. Haozhe Wang, Chao Qu, Zuming Huang, Wei Chu, Fangzhen Lin, Wenhu Chen, 2025a</p>
<p>The all-seeing project v2: Towards general relation comprehension of the open world. Weiyun Wang, Yiming Ren, Haowen Luo, Tiantong Li, Chenxiang Yan, Zhe Chen, Wenhai Wang, Qingyun Li, Lewei Lu, Xizhou Zhu, European Conference on Computer Vision. Springer2025b</p>
<p>Any-point trajectory modeling for policy learning. Chuan Wen, Xingyu Lin, John So, Kai Chen, Qi Dou, Yang Gao, Pieter Abbeel, arXiv:2401.000252023arXiv preprint</p>
<p>Posecnn: A convolutional neural network for 6d object pose estimation in cluttered scenes. Yu Xiang, Tanner Schmidt, Venkatraman Narayanan, Dieter Fox, arXiv:1711.001992017arXiv preprint</p>
<p>Florence-2: Advancing a unified representation for a variety of vision tasks. Bin Xiao, Haiping Wu, Weijian Xu, Xiyang Dai, Houdong Hu, Yumao Lu, Michael Zeng, Ce Liu, Lu Yuan, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Flow as the cross-domain manipulation interface. Mengda Xu, Zhenjia Xu, Yinghao Xu, Cheng Chi, Gordon Wetzstein, Manuela Veloso, Shuran Song, arXiv:2407.152082024arXiv preprint</p>
<p>A0: An affordance-aware hierarchical model for general robotic manipulation. Rongtao Xu, Jian Zhang, Minghao Guo, Youpeng Wen, Haoting Yang, Min Lin, Jianzheng Huang, Zhe Li, Kaidong Zhang, Liqiong Wang, arXiv:2504.126362025arXiv preprint</p>
<p>Thinking in space: How multimodal large language models see, remember, and recall spaces. Jihan Yang, Shusheng Yang, Anjali W Gupta, Rilyn Han, Li Fei-Fei, Saining Xie, arXiv:2412.141712024arXiv preprint</p>
<p>Dapo: An open-source llm reinforcement learning system at scale. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, arXiv:2503.144762025arXiv preprint</p>
<p>Chengbo Yuan, Chuan Wen, Tong Zhang, Yang Gao, arXiv:2401.11439General flow as foundation affordance for scalable robot learning. 2024aarXiv preprint</p>
<p>Robopoint: A vision-language model for spatial affordance prediction for robotics. Wentao Yuan, Jiafei Duan, Valts Blukis, Wilbert Pumacay, Ranjay Krishna, Adithyavairavan Murali, Arsalan Mousavian, Dieter Fox, arXiv:2406.107212024barXiv preprint</p>
<p>From seeing to doing: Bridging reasoning and decision for robotic manipulation. Yifu Yuan, Haiqin Cui, Yibin Chen, Zibin Dong, Fei Ni, Longxin Kou, Jinyi Liu, Pengyi Li, Yan Zheng, Jianye Hao, 2025</p>
<p>Robotic control via embodied chain-of-thought reasoning. Michał Zawalski, William Chen, Karl Pertsch, Oier Mees, Chelsea Finn, Sergey Levine, arXiv:2407.086932024arXiv preprint</p>
<p>Cot-vla: Visual chain-of-thought reasoning for vision-language-action models. Qingqing Zhao, Yao Lu, Jin Moo, Zipeng Kim, Zhuoyang Fu, Yecheng Zhang, Zhaoshuo Wu, Qianli Li, Song Ma, Chelsea Han, Finn, Proceedings of the Computer Vision and Pattern Recognition Conference. the Computer Vision and Pattern Recognition Conference2025</p>
<p>Tracevla: Visual trace prompting enhances spatial-temporal awareness for generalist robotic policies. Ruijie Zheng, Yongyuan Liang, Shuaiyi Huang, Jianfeng Gao, Hal Daumé, Iii , Andrey Kolobov, Furong Huang, Jianwei Yang, arXiv:2412.103452024arXiv preprint</p>
<p>Yuchen Zhou, Jiayu Tang, Xiaoyan Xiao, Yueyao Lin, Linkai Liu, Zipeng Guo, Hao Fei, arXiv:2506.23088Xiaobo Xia, and Chao Gou. Where, what, why: Towards explainable driver attention prediction. 2025arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4423 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4423</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4423</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-100.html">extraction-schema-100</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <p><strong>Paper ID:</strong> paper-271544398</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2407.20906v5.pdf" target="_blank">Automated literature research and review-generation method based on large language models</a></p>
                <p><strong>Paper Abstract:</strong> Abstract Literature research, which is vital for scientific work, faces the challenge of surging information volumes that are exceeding researchers’ processing capabilities. This paper describes an automated review-generation method based on large language models (LLMs) to overcome efficiency bottlenecks and reduce cognitive load. Our statistically validated evaluation framework demonstrates that the generated reviews match or exceed manual quality, offering broad applicability across research fields without requiring user domain knowledge. Applied to propane dehydrogenation catalysts, our method demonstrated two aspects: first, generating comprehensive reviews from 343 articles spanning 35 topics; and, second, evaluating data-mining capabilities by using 1041 articles for experimental catalyst property analysis. Through multilayered quality control, we effectively mitigated the hallucinations of LLMs, with expert verification confirming accuracy and citation integrity, while demonstrating hallucination risks reduced to <0.5% with 95% confidence. The released software application enables one-click review generation, enhancing research productivity and literature-recommendation efficiency while facilitating broader scientific explorations.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4423.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4423.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ARG-LLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Automated Review Generation Method Based on Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An end-to-end, modular pipeline that uses LLMs to retrieve, extract, aggregate, verify, and compose literature into topic-based review text and structured data, with multi-layer hallucination mitigation and statistical evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Automated Review Generation Method (this study)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A four-component pipeline (literature search, topic formulation, knowledge extraction, review composition) plus a data-mining module and a dual-baseline quality assessment framework. The pipeline: (1) automated retrieval via API (e.g., Google Scholar through SerpAPI) and two-stage coarse-to-fine filtering (quick title/abstract filter then LLM full-text relevance filtering); (2) topic formulation either by extracting outlines from existing reviews or by direct LLM outline drafting; (3) knowledge extraction where the LLM generates extraction questions, performs multiple rounds of article-level question-answering (including segmentation of long texts), produces structured/XML answers associated with DOIs, and aggregates repeated answers with self-consistency voting; (4) review composition where aggregated answers are assembled into topic-specific paragraphs, scored across multiple generations, top versions selected, DOI-verified, and format-standardized. Multi-layer hallucination mitigation is applied (strict prompt templates, XML format filtering, DOI verification, relevance checks, self-consistency aggregation, full data-stream traceability). An optional data-mining module parses numeric/structured properties into cleaned datasets and visualizations using generated code.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Primarily Claude 2 (used for main pipeline work). Evaluation additionally used Claude 3.5 Sonnet, Qwen2-72b-Instruct (72B), and Qwen2-7b-Instruct (7B).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>RAG-style context injection + LLM-generated question-answer extraction; multi-round QA per article; segmentation of full text; structured XML-format extraction; aggregation across repeated answers with self-consistency voting; DOI-linked provenance for each extracted item.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Aggregation of extracted answers into topic-specific paragraphs, multi-generation paragraph scoring and selection, compression/refinement to fit context windows, final citation verification and format standardization; multi-stage knowledge reconstruction rather than single-step retrieval summarization.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Applied to an initial 1420 search results; shortlisted 343 topic-related articles (238 confirmed relevant after LLM evaluation); extended analyses report on 839 PDH catalyst papers (from 1041 filtered) and other aggregated sets (e.g., 1041, 875 LLM outputs sampled).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Chemistry / Chemical engineering — propane dehydrogenation (PDH) catalysts (case study), but designed for cross-disciplinary generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Automated literature reviews (multi-topic, paragraph-level outputs), structured XML extraction records, data-mining tables, and visual analyses (charts); GUI one-click review generation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>LLM-based evaluation protocols including self-scoring and uniform scoring, intraclass correlation coefficient (ICC), Transitive Consistency Ratio (TCR), accuracy, false positive rate (FPR) with 95% confidence intervals, precision, recall, F1 score, and consistency rate; manual expert verification sampling for factual/citation checks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Knowledge-extraction aggregated accuracy 95.77% (875 data points). In paragraph generation, 875 total generations were performed; 36% passed format+DOI checks. Manual sampling (25 articles) supports hallucination probability <0.5% at 95% confidence for knowledge extraction. Model generation comparisons: Qwen2-7b average reached 43.94% of manual scoring, Qwen2-72b average reached 64.81% of manual scoring, and Claude3.5Sonnet averaged 23.63% above manual scores; optimal-paragraph scores (best among repeated generations) reached 89.07% (Qwen2-7b), 92.64% (Qwen2-72b), and 130.79% (Claude3.5Sonnet) relative to manual paragraphs. Repetition/aggregation reduced hallucination rates (binomial aggregation example: a model with 79.09% per-answer accuracy yields aggregated accuracies of 93.49% after 5 reps, 96.12% after 7, and 97.64% after 9).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Dual baselines: (1) human-written Q1 journal reviews (semantic fragments from 14 published Q1 reviews) and (2) direct single-step LLM generation without the multi-stage pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>The pipeline-generated reviews matched or exceeded manual reviews in many experiments; Claude3.5Sonnet pipeline outputs exceeded manual scores on average by ~23.63%; aggregated knowledge-extraction accuracy (95.77%) and low hallucination sampling (<0.5% at 95% CI) indicate superior factual reliability relative to direct LLM generation baseline which produced notably lower quality.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Multi-stage decomposition of the review task (structured QA per article, repeated sampling, aggregation, DOI-traceable provenance, format checks, and multi-generation paragraph selection) leverages LLM strengths (context learning and language synthesis) to approach or exceed manual review quality; model scale materially impacts output quality; repeated generations plus self-consistency aggregation significantly reduce hallucinations; RAG-style contextualization plus provenance (DOIs) is critical for factual traceability.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Residual LLM hallucination risk (especially outside the knowledge-extraction stage), false negatives where LLM fails to extract highly abstract content, unit-conversion and definition errors in numeric data mining, dependence on LLM capability/model size and context window limits (necessitating segmentation), reliance on subscription/full-text access (dataset copyright constraints), potential evaluation biases when using LLMs as evaluators.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Performance improves with larger models (Qwen2-7b < Qwen2-72b < Claude3.5). Multiple-generation and best-paragraph selection can compensate for smaller-model limitations. Increasing the number of independent extraction repetitions increases aggregated accuracy per binomial calculations; the framework's effectiveness increases as underlying model performance improves.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated literature research and review-generation method based on large language models', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4423.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4423.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A class of methods that augment LLM generation with retrieval of external documents (context) to ground outputs in source material, commonly used for knowledge-intensive NLP tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Retrieval-augmented generation for knowledgeintensive nlp tasks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Retrieval-Augmented Generation (RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>RAG systems retrieve relevant documents or passages from an external corpus and provide them as context to an LLM to improve factual grounding and timeliness of generated outputs. In this paper RAG is cited as conceptual foundation and the proposed pipeline adopts RAG-like context injection plus multi-stage processing.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Embedding-based retrieval of relevant documents combined with LLM question-answering over the retrieved context (context injection).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Context-conditioned generation where retrieved passages are combined by the LLM into answers or summaries; in this paper the authors extend RAG by multi-stage aggregation and verification rather than single-step generation.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General knowledge-intensive NLP and literature processing tasks (as cited).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Grounded answers, summaries, question-answer pairs, and review text when combined with generation stages.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared conceptually against fine-tuning approaches; paper states RAG sustains efficacy with new contextual knowledge and lower initial cost.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Paper states RAG offers lower initial cost and maintains efficacy for contextually new knowledge compared to domain-specific fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>RAG provides a practical mechanism to inject up-to-date literature into LLM context windows, but single-step RAG question-answering is insufficient for systematic knowledge reconstruction—multi-stage processing and provenance checks improve reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>RAG alone can surface semantically related but non-answer-bearing documents; requires careful relevance verification; still prone to hallucination if retrieval or LLM handling is imperfect.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Effectiveness of RAG depends on retrieval quality and LLM capacity; not quantified in paper beyond conceptual statements.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated literature research and review-generation method based on large language models', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4423.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4423.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaperQA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaperQA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval-augmented generative agent for scientific research that performs literature retrieval, question-answering, summarization, and contradiction detection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Paperqa: Retrieval-augmented generative agent for scientific research.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>PaperQA</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Described in this paper as a RAG agent designed to perform literature retrieval, question-answering, summarization, and contradiction detection on scientific documents; cited as demonstrating excellent performance on literature-related tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Retrieval-augmented question-answering and summarization (RAG-style retrieval + generation).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Summarization and contradiction detection across documents; implied multi-document QA and summarization.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Scientific literature generally (paper-cited examples).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Retrieval results, answers to queries, summaries, contradiction detection outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Paper states PaperQA (and PaperQA2) demonstrate excellent performance and can surpass human expertise in some aspects, but no quantitative metrics are given in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared in discussion to human expertise; positioned as surpassing humans on some tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Paper claims PaperQA/PaperQA2 outperform humans in some literature-related tasks (no numbers provided in this text).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>RAG agents like PaperQA are effective for retrieval and document-grounded QA tasks; they inform the motivation for automated review generation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>According to this paper, existing RAG-based tools often require user-provided literature, depend on QA interactions, or focus only on specific points limiting transferability to full review generation.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated literature research and review-generation method based on large language models', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4423.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4423.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaperQA2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaperQA2 (improved PaperQA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An improved version of PaperQA, described as a retrieval-augmented generation agent with enhanced performance for literature-related tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>PaperQA2</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Cited as an improved RAG-based agent over PaperQA for retrieval, question-answering, summarization, and contradiction detection across scientific literature; no further implementation details provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Retrieval-augmented generation / QA (implied).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Multi-document summarization and contradiction detection (implied).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Scientific literature generally.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Retrieval outputs, QA, summaries, contradiction flags.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Human expertise (claimed surpassing in some aspects).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Reported in text as surpassing humans in some respects (no numbers provided here).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Improved RAG agents can perform complex literature tasks, motivating automated review pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Still requires user-provided literature or interactive QA in many use-cases, limiting transferability.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated literature research and review-generation method based on large language models', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4423.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4423.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AcademicGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AcademicGPT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced LLM-based system providing comprehensive research support across academic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AcademicGPT</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Mentioned as an LLM-based tool that provides broad research assistance (no implementation details provided in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General academic research support.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Research assistance outputs (unspecified).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as part of the ecosystem of LLM-based research tools; not analyzed in depth here.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated literature research and review-generation method based on large language models', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4423.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4423.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CuriousLLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CuriousLLM</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-document question-answering system that uses reasoning-based traversal agents to improve QA across documents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>CuriousLLM</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Described as enhancing multi-document question answering via reasoning-based traversal agents, enabling effective cross-document reasoning for literature tasks (paper provides no system internals).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Multi-document QA with agent-based traversal (implied reasoning traversal over retrieved documents).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Reasoning-infused traversal to combine evidence across documents (implied).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Multi-document QA for literature.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Answers to multi-document questions; improved QA performance.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Agentic traversal with reasoning can improve multi-document QA quality.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated literature research and review-generation method based on large language models', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4423.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4423.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LitLLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LitLLM</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A toolkit that combines retrieval-augmented generation with LLM reranking to produce high-quality literature reviews from user-provided abstracts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Litllm</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LitLLM</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Combines RAG with an LLM-based reranking stage to generate literature reviews based on provided abstracts; positioned as producing high-quality reviews when abstracts are supplied by users.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>RAG retrieval of abstracts and LLM reranking of candidate outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Reranked generation of review text from aggregated abstracts.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Scientific literature review (toolkit for review generation).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Literature reviews generated from user-provided abstracts.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared conceptually to other LLM-based review approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>RAG combined with reranking can improve output quality for review generation when inputs (abstracts) are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Requires user-provided abstracts; not necessarily end-to-end retrieval from the web.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated literature research and review-generation method based on large language models', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4423.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e4423.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLAssist</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLAssist</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based tool for automating parts of literature screening in systematic reviews.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLAssist</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Cited as an LLM-powered tool focused on literature screening tasks (title/abstract filtering and related operations); specifics not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Screening via LLM-assisted relevance classification (implied).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Literature screening for systematic reviews.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Screened lists of candidate papers; filtered results.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLM tools can accelerate literature screening, but may require human oversight.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated literature research and review-generation method based on large language models', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4423.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e4423.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatCite</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatCite</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM agent that improves summary quality via human-like workflow guidance and citation-aware summarization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chatcite</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ChatCite</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Described as an approach that uses LLM agents with human-workflow guidance to generate comparative literature summaries and improve citation-aware summary quality.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Workflow-guided summarization with attention to citations and provenance (implied).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Comparative literature summary generation following human-like workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Literature summarization and comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Citation-aware summaries and comparative literature summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Incorporating human-like workflows into LLM agents can improve summary quality and citation handling.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated literature research and review-generation method based on large language models', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4423.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e4423.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multi-AI agents</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-AI agent systems for literature review automation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Systems composed of multiple interacting AI agents to automate end-to-end literature tasks from question generation to data extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>System for systematic literature review using multiple ai agents: Concept and an empirical evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Multi-AI agent systems</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Agentic multi-AI systems that coordinate sub-tasks (e.g., question generation, retrieval, extraction, synthesis) to automate the full literature review process; cited as recent LLM-based solutions for process automation.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Agent-driven retrieval and QA workflows; likely RAG-style retrieval plus inter-agent passes (as cited).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Cooperative agent composition and multi-step synthesis across retrieved documents.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General literature review automation.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>End-to-end automated literature reviews and extracted datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Agent orchestration can enable full-process automation but specifics vary across implementations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>May require complex orchestration and careful verification to prevent cascading hallucinations.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated literature research and review-generation method based on large language models', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4423.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e4423.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-consistency aggregation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-consistency aggregation (repeated-answer aggregation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A technique that reduces stochastic hallucinations by querying models multiple times and aggregating recurring answers as more likely to be correct.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Self-consistency aggregation</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Repeatedly query the LLM for the same extraction question and aggregate answers (voting/consensus) under the assumption that correct answers appear more frequently than hallucinated ones; used here to suppress hallucinations during knowledge extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Multiple independent LLM answer samplings per question followed by frequency-based aggregation and selection.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Use aggregated consensus answers as inputs to downstream paragraph synthesis and data-mining aggregation.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Applied within the pipeline to scientific literature extraction tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>More reliable extracted facts/answers with reduced hallucination.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Consistency rate, aggregated accuracy; manual sampling compared pre- and post-aggregation results.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>In knowledge extraction, aggregation produced 84.80% results judged as 100% consistent by LLM comparisons; binomial aggregation calculations (example) show that increasing repetitions raises aggregated accuracy (e.g., 79.09% -> 93.49% after 5 reps).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Single-sample LLM outputs (direct generation) without aggregation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Aggregation substantially reduced false positives/hallucinations relative to single-sample outputs; manual verification supports large reductions in hallucination rates.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Self-consistency aggregation effectively suppresses stochastic hallucinations and increases factual consistency of extracted answers; recommended repetition count empirically ~5 in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Increases API usage and compute cost; does not fully eliminate systematic model errors; assumes independence of samples.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Aggregate accuracy improves monotonically with the number of independent samples, subject to diminishing returns and increased cost (paper gives example probabilities for 5/7/9 reps).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated literature research and review-generation method based on large language models', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Paperqa: Retrieval-augmented generative agent for scientific research. <em>(Rating: 2)</em></li>
                <li>Retrieval-augmented generation for knowledgeintensive nlp tasks <em>(Rating: 2)</em></li>
                <li>Language agents achieve superhuman synthesis of scientific knowledge <em>(Rating: 2)</em></li>
                <li>Litllm <em>(Rating: 2)</em></li>
                <li>System for systematic literature review using multiple ai agents: Concept and an empirical evaluation <em>(Rating: 2)</em></li>
                <li>Chatcite <em>(Rating: 1)</em></li>
                <li>Curiousllm <em>(Rating: 1)</em></li>
                <li>LLAssist <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4423",
    "paper_id": "paper-271544398",
    "extraction_schema_id": "extraction-schema-100",
    "extracted_data": [
        {
            "name_short": "ARG-LLM",
            "name_full": "Automated Review Generation Method Based on Large Language Models",
            "brief_description": "An end-to-end, modular pipeline that uses LLMs to retrieve, extract, aggregate, verify, and compose literature into topic-based review text and structured data, with multi-layer hallucination mitigation and statistical evaluation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Automated Review Generation Method (this study)",
            "system_description": "A four-component pipeline (literature search, topic formulation, knowledge extraction, review composition) plus a data-mining module and a dual-baseline quality assessment framework. The pipeline: (1) automated retrieval via API (e.g., Google Scholar through SerpAPI) and two-stage coarse-to-fine filtering (quick title/abstract filter then LLM full-text relevance filtering); (2) topic formulation either by extracting outlines from existing reviews or by direct LLM outline drafting; (3) knowledge extraction where the LLM generates extraction questions, performs multiple rounds of article-level question-answering (including segmentation of long texts), produces structured/XML answers associated with DOIs, and aggregates repeated answers with self-consistency voting; (4) review composition where aggregated answers are assembled into topic-specific paragraphs, scored across multiple generations, top versions selected, DOI-verified, and format-standardized. Multi-layer hallucination mitigation is applied (strict prompt templates, XML format filtering, DOI verification, relevance checks, self-consistency aggregation, full data-stream traceability). An optional data-mining module parses numeric/structured properties into cleaned datasets and visualizations using generated code.",
            "llm_model_used": "Primarily Claude 2 (used for main pipeline work). Evaluation additionally used Claude 3.5 Sonnet, Qwen2-72b-Instruct (72B), and Qwen2-7b-Instruct (7B).",
            "extraction_technique": "RAG-style context injection + LLM-generated question-answer extraction; multi-round QA per article; segmentation of full text; structured XML-format extraction; aggregation across repeated answers with self-consistency voting; DOI-linked provenance for each extracted item.",
            "synthesis_technique": "Aggregation of extracted answers into topic-specific paragraphs, multi-generation paragraph scoring and selection, compression/refinement to fit context windows, final citation verification and format standardization; multi-stage knowledge reconstruction rather than single-step retrieval summarization.",
            "number_of_papers": "Applied to an initial 1420 search results; shortlisted 343 topic-related articles (238 confirmed relevant after LLM evaluation); extended analyses report on 839 PDH catalyst papers (from 1041 filtered) and other aggregated sets (e.g., 1041, 875 LLM outputs sampled).",
            "domain_or_topic": "Chemistry / Chemical engineering — propane dehydrogenation (PDH) catalysts (case study), but designed for cross-disciplinary generalization.",
            "output_type": "Automated literature reviews (multi-topic, paragraph-level outputs), structured XML extraction records, data-mining tables, and visual analyses (charts); GUI one-click review generation.",
            "evaluation_metrics": "LLM-based evaluation protocols including self-scoring and uniform scoring, intraclass correlation coefficient (ICC), Transitive Consistency Ratio (TCR), accuracy, false positive rate (FPR) with 95% confidence intervals, precision, recall, F1 score, and consistency rate; manual expert verification sampling for factual/citation checks.",
            "performance_results": "Knowledge-extraction aggregated accuracy 95.77% (875 data points). In paragraph generation, 875 total generations were performed; 36% passed format+DOI checks. Manual sampling (25 articles) supports hallucination probability &lt;0.5% at 95% confidence for knowledge extraction. Model generation comparisons: Qwen2-7b average reached 43.94% of manual scoring, Qwen2-72b average reached 64.81% of manual scoring, and Claude3.5Sonnet averaged 23.63% above manual scores; optimal-paragraph scores (best among repeated generations) reached 89.07% (Qwen2-7b), 92.64% (Qwen2-72b), and 130.79% (Claude3.5Sonnet) relative to manual paragraphs. Repetition/aggregation reduced hallucination rates (binomial aggregation example: a model with 79.09% per-answer accuracy yields aggregated accuracies of 93.49% after 5 reps, 96.12% after 7, and 97.64% after 9).",
            "comparison_baseline": "Dual baselines: (1) human-written Q1 journal reviews (semantic fragments from 14 published Q1 reviews) and (2) direct single-step LLM generation without the multi-stage pipeline.",
            "performance_vs_baseline": "The pipeline-generated reviews matched or exceeded manual reviews in many experiments; Claude3.5Sonnet pipeline outputs exceeded manual scores on average by ~23.63%; aggregated knowledge-extraction accuracy (95.77%) and low hallucination sampling (&lt;0.5% at 95% CI) indicate superior factual reliability relative to direct LLM generation baseline which produced notably lower quality.",
            "key_findings": "Multi-stage decomposition of the review task (structured QA per article, repeated sampling, aggregation, DOI-traceable provenance, format checks, and multi-generation paragraph selection) leverages LLM strengths (context learning and language synthesis) to approach or exceed manual review quality; model scale materially impacts output quality; repeated generations plus self-consistency aggregation significantly reduce hallucinations; RAG-style contextualization plus provenance (DOIs) is critical for factual traceability.",
            "limitations_challenges": "Residual LLM hallucination risk (especially outside the knowledge-extraction stage), false negatives where LLM fails to extract highly abstract content, unit-conversion and definition errors in numeric data mining, dependence on LLM capability/model size and context window limits (necessitating segmentation), reliance on subscription/full-text access (dataset copyright constraints), potential evaluation biases when using LLMs as evaluators.",
            "scaling_behavior": "Performance improves with larger models (Qwen2-7b &lt; Qwen2-72b &lt; Claude3.5). Multiple-generation and best-paragraph selection can compensate for smaller-model limitations. Increasing the number of independent extraction repetitions increases aggregated accuracy per binomial calculations; the framework's effectiveness increases as underlying model performance improves.",
            "uuid": "e4423.0",
            "source_info": {
                "paper_title": "Automated literature research and review-generation method based on large language models",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "RAG",
            "name_full": "Retrieval-Augmented Generation",
            "brief_description": "A class of methods that augment LLM generation with retrieval of external documents (context) to ground outputs in source material, commonly used for knowledge-intensive NLP tasks.",
            "citation_title": "Retrieval-augmented generation for knowledgeintensive nlp tasks",
            "mention_or_use": "use",
            "system_name": "Retrieval-Augmented Generation (RAG)",
            "system_description": "RAG systems retrieve relevant documents or passages from an external corpus and provide them as context to an LLM to improve factual grounding and timeliness of generated outputs. In this paper RAG is cited as conceptual foundation and the proposed pipeline adopts RAG-like context injection plus multi-stage processing.",
            "llm_model_used": null,
            "extraction_technique": "Embedding-based retrieval of relevant documents combined with LLM question-answering over the retrieved context (context injection).",
            "synthesis_technique": "Context-conditioned generation where retrieved passages are combined by the LLM into answers or summaries; in this paper the authors extend RAG by multi-stage aggregation and verification rather than single-step generation.",
            "number_of_papers": null,
            "domain_or_topic": "General knowledge-intensive NLP and literature processing tasks (as cited).",
            "output_type": "Grounded answers, summaries, question-answer pairs, and review text when combined with generation stages.",
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": "Compared conceptually against fine-tuning approaches; paper states RAG sustains efficacy with new contextual knowledge and lower initial cost.",
            "performance_vs_baseline": "Paper states RAG offers lower initial cost and maintains efficacy for contextually new knowledge compared to domain-specific fine-tuning.",
            "key_findings": "RAG provides a practical mechanism to inject up-to-date literature into LLM context windows, but single-step RAG question-answering is insufficient for systematic knowledge reconstruction—multi-stage processing and provenance checks improve reliability.",
            "limitations_challenges": "RAG alone can surface semantically related but non-answer-bearing documents; requires careful relevance verification; still prone to hallucination if retrieval or LLM handling is imperfect.",
            "scaling_behavior": "Effectiveness of RAG depends on retrieval quality and LLM capacity; not quantified in paper beyond conceptual statements.",
            "uuid": "e4423.1",
            "source_info": {
                "paper_title": "Automated literature research and review-generation method based on large language models",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "PaperQA",
            "name_full": "PaperQA",
            "brief_description": "A retrieval-augmented generative agent for scientific research that performs literature retrieval, question-answering, summarization, and contradiction detection.",
            "citation_title": "Paperqa: Retrieval-augmented generative agent for scientific research.",
            "mention_or_use": "mention",
            "system_name": "PaperQA",
            "system_description": "Described in this paper as a RAG agent designed to perform literature retrieval, question-answering, summarization, and contradiction detection on scientific documents; cited as demonstrating excellent performance on literature-related tasks.",
            "llm_model_used": null,
            "extraction_technique": "Retrieval-augmented question-answering and summarization (RAG-style retrieval + generation).",
            "synthesis_technique": "Summarization and contradiction detection across documents; implied multi-document QA and summarization.",
            "number_of_papers": null,
            "domain_or_topic": "Scientific literature generally (paper-cited examples).",
            "output_type": "Retrieval results, answers to queries, summaries, contradiction detection outputs.",
            "evaluation_metrics": null,
            "performance_results": "Paper states PaperQA (and PaperQA2) demonstrate excellent performance and can surpass human expertise in some aspects, but no quantitative metrics are given in this paper.",
            "comparison_baseline": "Compared in discussion to human expertise; positioned as surpassing humans on some tasks.",
            "performance_vs_baseline": "Paper claims PaperQA/PaperQA2 outperform humans in some literature-related tasks (no numbers provided in this text).",
            "key_findings": "RAG agents like PaperQA are effective for retrieval and document-grounded QA tasks; they inform the motivation for automated review generation.",
            "limitations_challenges": "According to this paper, existing RAG-based tools often require user-provided literature, depend on QA interactions, or focus only on specific points limiting transferability to full review generation.",
            "scaling_behavior": null,
            "uuid": "e4423.2",
            "source_info": {
                "paper_title": "Automated literature research and review-generation method based on large language models",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "PaperQA2",
            "name_full": "PaperQA2 (improved PaperQA)",
            "brief_description": "An improved version of PaperQA, described as a retrieval-augmented generation agent with enhanced performance for literature-related tasks.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "PaperQA2",
            "system_description": "Cited as an improved RAG-based agent over PaperQA for retrieval, question-answering, summarization, and contradiction detection across scientific literature; no further implementation details provided in this paper.",
            "llm_model_used": null,
            "extraction_technique": "Retrieval-augmented generation / QA (implied).",
            "synthesis_technique": "Multi-document summarization and contradiction detection (implied).",
            "number_of_papers": null,
            "domain_or_topic": "Scientific literature generally.",
            "output_type": "Retrieval outputs, QA, summaries, contradiction flags.",
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": "Human expertise (claimed surpassing in some aspects).",
            "performance_vs_baseline": "Reported in text as surpassing humans in some respects (no numbers provided here).",
            "key_findings": "Improved RAG agents can perform complex literature tasks, motivating automated review pipelines.",
            "limitations_challenges": "Still requires user-provided literature or interactive QA in many use-cases, limiting transferability.",
            "scaling_behavior": null,
            "uuid": "e4423.3",
            "source_info": {
                "paper_title": "Automated literature research and review-generation method based on large language models",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "AcademicGPT",
            "name_full": "AcademicGPT",
            "brief_description": "A referenced LLM-based system providing comprehensive research support across academic tasks.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "AcademicGPT",
            "system_description": "Mentioned as an LLM-based tool that provides broad research assistance (no implementation details provided in this paper).",
            "llm_model_used": null,
            "extraction_technique": null,
            "synthesis_technique": null,
            "number_of_papers": null,
            "domain_or_topic": "General academic research support.",
            "output_type": "Research assistance outputs (unspecified).",
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": "Cited as part of the ecosystem of LLM-based research tools; not analyzed in depth here.",
            "limitations_challenges": null,
            "scaling_behavior": null,
            "uuid": "e4423.4",
            "source_info": {
                "paper_title": "Automated literature research and review-generation method based on large language models",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "CuriousLLM",
            "name_full": "CuriousLLM",
            "brief_description": "A multi-document question-answering system that uses reasoning-based traversal agents to improve QA across documents.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "CuriousLLM",
            "system_description": "Described as enhancing multi-document question answering via reasoning-based traversal agents, enabling effective cross-document reasoning for literature tasks (paper provides no system internals).",
            "llm_model_used": null,
            "extraction_technique": "Multi-document QA with agent-based traversal (implied reasoning traversal over retrieved documents).",
            "synthesis_technique": "Reasoning-infused traversal to combine evidence across documents (implied).",
            "number_of_papers": null,
            "domain_or_topic": "Multi-document QA for literature.",
            "output_type": "Answers to multi-document questions; improved QA performance.",
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": "Agentic traversal with reasoning can improve multi-document QA quality.",
            "limitations_challenges": null,
            "scaling_behavior": null,
            "uuid": "e4423.5",
            "source_info": {
                "paper_title": "Automated literature research and review-generation method based on large language models",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "LitLLM",
            "name_full": "LitLLM",
            "brief_description": "A toolkit that combines retrieval-augmented generation with LLM reranking to produce high-quality literature reviews from user-provided abstracts.",
            "citation_title": "Litllm",
            "mention_or_use": "mention",
            "system_name": "LitLLM",
            "system_description": "Combines RAG with an LLM-based reranking stage to generate literature reviews based on provided abstracts; positioned as producing high-quality reviews when abstracts are supplied by users.",
            "llm_model_used": null,
            "extraction_technique": "RAG retrieval of abstracts and LLM reranking of candidate outputs.",
            "synthesis_technique": "Reranked generation of review text from aggregated abstracts.",
            "number_of_papers": null,
            "domain_or_topic": "Scientific literature review (toolkit for review generation).",
            "output_type": "Literature reviews generated from user-provided abstracts.",
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": "Compared conceptually to other LLM-based review approaches.",
            "performance_vs_baseline": null,
            "key_findings": "RAG combined with reranking can improve output quality for review generation when inputs (abstracts) are provided.",
            "limitations_challenges": "Requires user-provided abstracts; not necessarily end-to-end retrieval from the web.",
            "scaling_behavior": null,
            "uuid": "e4423.6",
            "source_info": {
                "paper_title": "Automated literature research and review-generation method based on large language models",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "LLAssist",
            "name_full": "LLAssist",
            "brief_description": "An LLM-based tool for automating parts of literature screening in systematic reviews.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "LLAssist",
            "system_description": "Cited as an LLM-powered tool focused on literature screening tasks (title/abstract filtering and related operations); specifics not provided in this paper.",
            "llm_model_used": null,
            "extraction_technique": "Screening via LLM-assisted relevance classification (implied).",
            "synthesis_technique": null,
            "number_of_papers": null,
            "domain_or_topic": "Literature screening for systematic reviews.",
            "output_type": "Screened lists of candidate papers; filtered results.",
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": "LLM tools can accelerate literature screening, but may require human oversight.",
            "limitations_challenges": null,
            "scaling_behavior": null,
            "uuid": "e4423.7",
            "source_info": {
                "paper_title": "Automated literature research and review-generation method based on large language models",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "ChatCite",
            "name_full": "ChatCite",
            "brief_description": "An LLM agent that improves summary quality via human-like workflow guidance and citation-aware summarization.",
            "citation_title": "Chatcite",
            "mention_or_use": "mention",
            "system_name": "ChatCite",
            "system_description": "Described as an approach that uses LLM agents with human-workflow guidance to generate comparative literature summaries and improve citation-aware summary quality.",
            "llm_model_used": null,
            "extraction_technique": "Workflow-guided summarization with attention to citations and provenance (implied).",
            "synthesis_technique": "Comparative literature summary generation following human-like workflows.",
            "number_of_papers": null,
            "domain_or_topic": "Literature summarization and comparison.",
            "output_type": "Citation-aware summaries and comparative literature summaries.",
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": "Incorporating human-like workflows into LLM agents can improve summary quality and citation handling.",
            "limitations_challenges": null,
            "scaling_behavior": null,
            "uuid": "e4423.8",
            "source_info": {
                "paper_title": "Automated literature research and review-generation method based on large language models",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Multi-AI agents",
            "name_full": "Multi-AI agent systems for literature review automation",
            "brief_description": "Systems composed of multiple interacting AI agents to automate end-to-end literature tasks from question generation to data extraction.",
            "citation_title": "System for systematic literature review using multiple ai agents: Concept and an empirical evaluation",
            "mention_or_use": "mention",
            "system_name": "Multi-AI agent systems",
            "system_description": "Agentic multi-AI systems that coordinate sub-tasks (e.g., question generation, retrieval, extraction, synthesis) to automate the full literature review process; cited as recent LLM-based solutions for process automation.",
            "llm_model_used": null,
            "extraction_technique": "Agent-driven retrieval and QA workflows; likely RAG-style retrieval plus inter-agent passes (as cited).",
            "synthesis_technique": "Cooperative agent composition and multi-step synthesis across retrieved documents.",
            "number_of_papers": null,
            "domain_or_topic": "General literature review automation.",
            "output_type": "End-to-end automated literature reviews and extracted datasets.",
            "evaluation_metrics": null,
            "performance_results": null,
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "key_findings": "Agent orchestration can enable full-process automation but specifics vary across implementations.",
            "limitations_challenges": "May require complex orchestration and careful verification to prevent cascading hallucinations.",
            "scaling_behavior": null,
            "uuid": "e4423.9",
            "source_info": {
                "paper_title": "Automated literature research and review-generation method based on large language models",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Self-consistency aggregation",
            "name_full": "Self-consistency aggregation (repeated-answer aggregation)",
            "brief_description": "A technique that reduces stochastic hallucinations by querying models multiple times and aggregating recurring answers as more likely to be correct.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Self-consistency aggregation",
            "system_description": "Repeatedly query the LLM for the same extraction question and aggregate answers (voting/consensus) under the assumption that correct answers appear more frequently than hallucinated ones; used here to suppress hallucinations during knowledge extraction.",
            "llm_model_used": null,
            "extraction_technique": "Multiple independent LLM answer samplings per question followed by frequency-based aggregation and selection.",
            "synthesis_technique": "Use aggregated consensus answers as inputs to downstream paragraph synthesis and data-mining aggregation.",
            "number_of_papers": null,
            "domain_or_topic": "Applied within the pipeline to scientific literature extraction tasks.",
            "output_type": "More reliable extracted facts/answers with reduced hallucination.",
            "evaluation_metrics": "Consistency rate, aggregated accuracy; manual sampling compared pre- and post-aggregation results.",
            "performance_results": "In knowledge extraction, aggregation produced 84.80% results judged as 100% consistent by LLM comparisons; binomial aggregation calculations (example) show that increasing repetitions raises aggregated accuracy (e.g., 79.09% -&gt; 93.49% after 5 reps).",
            "comparison_baseline": "Single-sample LLM outputs (direct generation) without aggregation.",
            "performance_vs_baseline": "Aggregation substantially reduced false positives/hallucinations relative to single-sample outputs; manual verification supports large reductions in hallucination rates.",
            "key_findings": "Self-consistency aggregation effectively suppresses stochastic hallucinations and increases factual consistency of extracted answers; recommended repetition count empirically ~5 in this work.",
            "limitations_challenges": "Increases API usage and compute cost; does not fully eliminate systematic model errors; assumes independence of samples.",
            "scaling_behavior": "Aggregate accuracy improves monotonically with the number of independent samples, subject to diminishing returns and increased cost (paper gives example probabilities for 5/7/9 reps).",
            "uuid": "e4423.10",
            "source_info": {
                "paper_title": "Automated literature research and review-generation method based on large language models",
                "publication_date_yy_mm": "2025-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Paperqa: Retrieval-augmented generative agent for scientific research.",
            "rating": 2,
            "sanitized_title": "paperqa_retrievalaugmented_generative_agent_for_scientific_research"
        },
        {
            "paper_title": "Retrieval-augmented generation for knowledgeintensive nlp tasks",
            "rating": 2,
            "sanitized_title": "retrievalaugmented_generation_for_knowledgeintensive_nlp_tasks"
        },
        {
            "paper_title": "Language agents achieve superhuman synthesis of scientific knowledge",
            "rating": 2,
            "sanitized_title": "language_agents_achieve_superhuman_synthesis_of_scientific_knowledge"
        },
        {
            "paper_title": "Litllm",
            "rating": 2
        },
        {
            "paper_title": "System for systematic literature review using multiple ai agents: Concept and an empirical evaluation",
            "rating": 2,
            "sanitized_title": "system_for_systematic_literature_review_using_multiple_ai_agents_concept_and_an_empirical_evaluation"
        },
        {
            "paper_title": "Chatcite",
            "rating": 1
        },
        {
            "paper_title": "Curiousllm",
            "rating": 1,
            "sanitized_title": "curiousllm"
        },
        {
            "paper_title": "LLAssist",
            "rating": 1
        }
    ],
    "cost": 0.020035999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Automated Review Generation Method Based on Large Language Models
1 May 2025</p>
<p>Shican Wu 
School of Chemical Engineering and Technology
Collaborative Innovation Center of Chemical Science and Engineering (Tianjin)
Key Laboratory for Green Chemical Technology of Ministry of Education
Tianjin University
300072TianjinChina</p>
<p>Xiao Ma 
School of Chemical Engineering and Technology
Collaborative Innovation Center of Chemical Science and Engineering (Tianjin)
Key Laboratory for Green Chemical Technology of Ministry of Education
Tianjin University
300072TianjinChina</p>
<p>Dehui Luo 
School of Chemical Engineering and Technology
Collaborative Innovation Center of Chemical Science and Engineering (Tianjin)
Key Laboratory for Green Chemical Technology of Ministry of Education
Tianjin University
300072TianjinChina</p>
<p>Lulu Li 
School of Chemical Engineering and Technology
Collaborative Innovation Center of Chemical Science and Engineering (Tianjin)
Key Laboratory for Green Chemical Technology of Ministry of Education
Tianjin University
300072TianjinChina</p>
<p>Xiangcheng Shi 
Joint School of National University of Singapore and Tianjin University
International Campus of Tianjin University
350207Binhai New City, Fuzhou, FujianChina</p>
<p>Xin Chang 
Joint School of National University of Singapore and Tianjin University
International Campus of Tianjin University
350207Binhai New City, Fuzhou, FujianChina</p>
<p>Xiaoyun Lin 
School of Chemical Engineering and Technology
Collaborative Innovation Center of Chemical Science and Engineering (Tianjin)
Key Laboratory for Green Chemical Technology of Ministry of Education
Tianjin University
300072TianjinChina</p>
<p>Ran Luo 
Joint School of National University of Singapore and Tianjin University
International Campus of Tianjin University
350207Binhai New City, Fuzhou, FujianChina</p>
<p>Chunlei Pei 
School of Chemical Engineering and Technology
Collaborative Innovation Center of Chemical Science and Engineering (Tianjin)
Key Laboratory for Green Chemical Technology of Ministry of Education
Tianjin University
300072TianjinChina</p>
<p>Zhejiang Institute of Tianjin University Ningbo
315201ZhejiangChina</p>
<p>Changying Du 
AIStrucX Technologies
No. 26, Information Road, Haidian District100000BeĳingChina</p>
<p>Zhi-Jian Zhao zjzhao@tju.edu.cn 
School of Chemical Engineering and Technology
Collaborative Innovation Center of Chemical Science and Engineering (Tianjin)
Key Laboratory for Green Chemical Technology of Ministry of Education
Tianjin University
300072TianjinChina</p>
<p>International Joint Laboratory of Low-carbon Chemical Engineering
300192TianjinChina</p>
<p>Jinlong Gong jlgong@tju.edu.cn 
School of Chemical Engineering and Technology
Collaborative Innovation Center of Chemical Science and Engineering (Tianjin)
Key Laboratory for Green Chemical Technology of Ministry of Education
Tianjin University
300072TianjinChina</p>
<p>Joint School of National University of Singapore and Tianjin University
International Campus of Tianjin University
350207Binhai New City, Fuzhou, FujianChina</p>
<p>Zhejiang Institute of Tianjin University Ningbo
315201ZhejiangChina</p>
<p>International Joint Laboratory of Low-carbon Chemical Engineering
300192TianjinChina</p>
<p>Haihe Laboratory of Sustainable Chemical Transformations
300192TianjinChina</p>
<p>National Industry-Education Platform of Energy Storage
Tianjin University
135 Yaguan Road300350TianjinChina</p>
<p>Tianjin Normal University
300387TianjinChina</p>
<p>Automated Review Generation Method Based on Large Language Models
1 May 20254DB21C7AD76586CF6B82D3C2D7169717arXiv:2407.20906v5[cs.CL]large language modelsautomated review generationliterature analysisscientific writing
Literature research, vital for scientific work, faces the challenge of surging information volumes exceeding researchers' processing capabilities.We present an automated review generation method based on large language models (LLMs) to overcome efficiency bottlenecks and reduce cognitive load.Our statistically validated evaluation framework demonstrates that the generated reviews match or exceed manual quality, offering broad applicability across research fields without requiring users' domain knowledge.Applied to propane dehydrogenation (PDH) catalysts, our method swiftly analyzed 343 articles, averaging seconds per article per LLM account, producing comprehensive reviews spanning 35 topics, with extended analysis of 1041 articles providing insights into catalysts' properties.Through multi-layered quality control, we effectively mitigated LLMs' hallucinations, with expert verification confirming accuracy and citation integrity while demonstrating hallucination risks reduced to below 0.5% with 95% confidence.Released Windows application enables one-click review generation, enhancing research productivity and literature recommendation efficiency while setting the stage for broader scientific explorations.</p>
<p>INTRODUCTION</p>
<p>Peer-reviewed academic literature functions as a critical medium for scientific knowledge dissemination, enabling researchers to advance human understanding through cumulative progress [1].The clarity and rigor of scientific language facilitates entity description, concept extraction, and consensus building, ensuring cognitive consistency between information senders and receivers.However, the exponential growth in publications has exceeded researcher' processing capacity [2,3,4], necessitating efficient tools for literature analysis and integration, thus avoiding redundant discoveries and broadening research perspectives.</p>
<p>Natural language processing (NLP), encompassing co-reference resolution, semantic analysis, etc. [5], powers literature comprehension.Since November 2022, Large Language Models like ChatGPT, as the latest NLP advancement, have exhibited unprecedented language understanding [6].Leading LLMs have surpassed human performance on various benchmarks including MMLU [7], which tests undergraduate knowledge, and GPQA Diamond [8], which assesses graduate-level reasoning, positioning them as potential "second brains" for efficient literature processing [6,9].Recent studies like PaperQA [10] and its improved version PaperQA2 [11], as retrieval-augmented generation (RAG [12]) agents, demonstrate excellent performance in literature-related tasks including retrieval, question-answering, summarization, and contradiction detection, surpassing human expertise in some aspects.AcademicGPT [13] provides comprehensive research support, while CuriousLLM [14] enhances multi-document questionanswering through reasoning-based traversal agents.However, these applications require user-provided literature, rely on question-answer interactions, or focus on specific points, limiting their transferability.</p>
<p>The review format effectively integrates literature information and generalizes across disciplinary fields, naturally leading to automated review generation research.However, early attempts encountered several limitations.They either reduced reviews to multi-document summarization [15] or depended on existing reviews and citation networks [16,17,18,19,20], which struggled with rapidly evolving fields and underestimated recent publications due to citation lag.Additional constraints included using only abstracts instead of full texts as input data [17,18,19,20] and employing either extractive summarization rather than integrated generation [16,17,18] or template-based generation [19], risking information loss and redundancy.Recent LLM-based solutions include: multi-AI agent systems [21] for full-process automation from research question generation to data extraction; LitLLM [22] combining RAG with LLM reranking to generate high-quality literature reviews based on user-provided abstracts; LLAssist [23] and related work [24] for literature screening; and ChatCite [25] improving summary quality through human-like workflows.These advances enhance automated review generation while enabling efficient academic research.</p>
<p>Based on the potential of LLMs, this study proposes an automated review generation method based on LLMs, builds an end-to-end data pipeline from literature retrieval to final review text generation.By leveraging information refinement and knowledge construction capabilities of LLMs, this method overcomes human cognitive limitations in single-threaded processing and memory capacity, reducing researchers' cognitive load while offering superior speed and scalability, thereby substantially conserving professional human resources.However, two critical challenges persist: the macro-level requirement for systematic quality evaluation and comparison with manual reviews, and the micro-level necessity to effectively mitigate LLM hallucinations.To address these challenges, we designed a dual-baseline automatic evaluation framework with rigorous statistical validation, alongside multi-level quality control strategies throughout the process.The distinctive feature of method lies in its adaptability to diverse disciplinary terminologies and knowledge structures without domain-specific training, facilitating both comprehensive field overviews for experienced researchers and accessible entry points for those lacking relevant background, opening up new possibilities for promoting interdisciplinary research and knowledge dissemination.This approach holds substantial scientific significance by enhancing literature processing efficiency, fostering knowledge discovery, and stimulating innovation.Its ability to promote interdisciplinary communication and knowledge integration positions it as a potential cornerstone of modern research infrastructure, accelerating scientific discovery and technological advancement across domains.</p>
<p>RESULTS AND DISCUSSION</p>
<p>Automated retrieval</p>
<p>Automated review generation fundamentally relies on retrieving and extracting scientific literature, with output quality dependent on input timeliness, quality, and breadth.To demonstrate cross-disciplinary generalization without human intervention, we conducted a case study on propane dehydrogenation (PDH) catalysts, searching chemistry and chemical engineering journals (1980-2024) ranked Q1 in the Chinese Academy of Sciences journal classification on Google Scholar through SerpAPI.</p>
<p>The automated retrieval yielded 1420 initial results from Google Scholar.To address the challenge of irrelevant or duplicate findings, we implemented a dual-level filtering process.The first level employed quick filtering of abstracts and titles to remove obviously irrelevant documents, as detailed in Method section, serving as a rapid but less precise narrowing method.The second level involved deeper LLM-based analysis of full texts, offering higher accuracy albeit at a slower pace.This coarse-to-fine screening method, reminiscent of high-throughput screening, enabled us to identify literature pertinent efficiently and accurately to our research.The initial screening shortlisted 343 articles as related to our topic.Subsequent LLM evaluation further confirmed 238 of these articles as relevant.</p>
<p>Implementation and analysis of one-click automated review generation</p>
<p>Using PDH catalysts as an example and building on the aforementioned automated retrieval, we have effectively produced high-quality, specialized review articles.Considering that the entire process is completely end-to-end without the need for human intervention, we believe that a single domain example is sufficient to demonstrate the applicability of this method.The main reason for limiting the journal range to Q1 journals is that although the impact factor of journals may not be closely related to the quality of articles, the lower limit of literature in Q1 journals that have passed strict peer review is relatively higher.Considering users lacking prior knowledge in the target domain, directly traversing Q1 chemistry journals provides an efficient starting point.We solemnly declare here that we are not encouraging users to only consider Q1 journals, but rather suggesting that in the initial stage, one can consider starting with Q1 journals, and for research on the entire field, all possibilities should be explored, which is also supported by our method.In the Windows GUI we provide, using Q1/Q2-3 journals is an optional button, allowing users to choose for themselves.For those with domain familiarity, the program allows the specification of a custom journal list to refine article selection.</p>
<p>We evaluated two topic construction strategies: based on existing reviews (9 topics, 35 questions, 125 citations) and direct LLM generation (12 topics, 12 questions, 43 citations).The examples showcased in subsequent sections and SI are based on outlines derived from existing reviews.The content has been manually checked by experts in the relevant field, with no errors in knowledge, correct referencing of cited literature, and a length and citation count that align with conventional review standards (see SI).The method's effectiveness stems from LLMs' human-level or superior language comprehension abilities, coupled with the injection of domain knowledge from retrieved literature through context window, thereby enabling generalization across all research fields.Beyond content accuracy, the method enables customizable research focus through supporting of adding specific questions and provides forward-looking insights with comprehensive understanding sections.To facilitate broad adoption, we developed an open-source Python3 GUI enabling one-click review generation without programming expertise or domain knowledge.</p>
<p>Evaluation of generated review quality</p>
<p>Research demonstrates LLMs excel in evaluation tasks, with GPT-4 surpassing both crowdsourced workers [26] and experts [27] in text annotation accuracy and reliability, and bias control for complex tasks requiring contextual knowledge reasoning [27].LLMs show comparable or superior performance to human annotation in persuasiveness, accuracy, and satisfaction [28].Studies confirm the capability of LLMs to evaluate other LLMs, with verification abilities improving faster than generation quality [29].GPT-4's evaluations exceeds 80% consistency with human reviewers [30] and exceeds 85% alignment in pairwise comparisons [31], reaching nearly 100% agreement when performance differences are significant [31].Self-verification benefits LLM performance [32], though biases issues [33], such as position bias [34], length bias [30], and self-bias [35], persist but can be reduced through proper design [36].</p>
<p>Given characteristic writing patterns of LLMs and potential human evaluation bias, this study employs LLM-based evaluation exclusively.Existing LLM-based LLM evaluation methods encompass scoring, comparison, selection judgment, and comprehensive description.This study introduces a dual-baseline review quality evaluation framework, to minimize potential LLM evaluation bias and quantitatively compare LLM-generated reviews with peer-reviewed expert-written content, validating reliability through statistical analysis.In our method, we segmented 14 published Q1 reviews into 89 fragments based on semantic content.Using extracted topics from these fragments and the literature cited in the original text, we generated comparative reviews using Qwen2-7b-Instruct, Qwen2-72b-Instruct, and Claude3.5Sonnet.This methodology enabled direct comparison between human experts and LLMs in writing reviews with identical literature background, establishing a rigorous benchmark for LLMs given their limitations in accessing both human accumulated domain expertise and pre-1970 undigitized literature.</p>
<p>In the evaluation process, we compared the performance of two models of different scales from the open-source Qwen2 series (Qwen2-7b-Instruct and Qwen2-72b-Instruct) and the closed-source model Claude3.5Sonnet(Fig. 1).Evaluation employed both selfscoring and uniform scoring by Qwen2-72b-Instruct.Intraclass Correlation Coefficient (ICC) tests and Transitive Consistency Ratio (TCR) analyses confirmed high reliability for Claude3.5Sonnet (Fig. 1(a,b,e,f)) and Qwen2-72b-Instruct (Fig. 1(c,d,e,f)), meeting human evaluation standards, while Qwen2-7b-Instruct's results necessitated substitution with Qwen2-72b-Instruct's scoring due to insufficient reliability.This phenomenon may stem from differences in capabilities between LLMs of different scales, specifically manifested in three dimensions: world knowledge, language understanding, and logical reasoning.We believe that small models differ significantly from large models in logical reasoning ability, while world knowledge has been supplemented through context-provided literature, and language ability differences are relatively small.For details on Qwen2-7b-Instruct's evaluation results, see SI. Model capability significantly impacts generation quality, while our method ensures a basic lower limit of generation quality.In repeated generation tests (9 times per model), average scores, taken as comprehensive performance of models, showed that Qwen2-7b-Instruct reached 43.94% of manual scoring, Qwen2-72b-Instruct reached 64.81% (Fig. 2(d,f)), and Claude3.5Sonnetexceeding manual scores by 23.63% (Fig. 2(c,f)), all significantly higher than the baseline level of direct generation.Optimal performance analysis, which taking the paragraph with the highest total score among those generated by each model as the optimal paragraph, revealed best-paragraph scores of 89.07%for Qwen2-7b-Instruct, 92.64% for Qwen2-72b-Instruct (Fig. 2(b,f)), and 130.79% for Claude3.5Sonnetrelative to manual scores (Fig. 2(a,f)).These results indicate multiple generations and selecting the best paragraph can improve performance of smaller models, while larger models maintain consistent quality.For optimal results, we recommend using larger models when possible; otherwise, multiple generations can enhance final performance in hardware-constrained scenarios.For details on Qwen2-7b-Instruct's evaluation results, see SI.The near-human scores of optimal paragraphs might reflect quality-related bias [37] because their closely approximate human-level quality could make LLMs' potential evaluation bias more prominent, while the distinctly inferior quality of direct generation remains easily distinguishable.Analysis of optimal paragraphs reveals high correlations between Qwen2-7B and Qwen2-72B across evaluation sub-items (0.926 for highest scores, 0.939 for scores relative to human benchmarks), reflecting the scaling law of LLM and suggesting potential for further improvements with model advancement (see SI for details).Our research validates the effectiveness of LLM-based automated review generation, with quality approaching or exceeding manual reviews.The effectiveness of method stems from general language processing and context learning capabilities of LLMs, rather than requiring specific domain expertise, suggests broad disciplinary applicability.While inheriting common biases and requiring readers' professional judgment, this method serves as a supportive rather than a replacement tool for human innovation, with open-source models showing comparable capabilities to closed-source alternatives.The approach demonstrates broad cross-disciplinary potential, promising to become an important tool for promoting academic innovation and knowledge dissemination, while the dual-baseline framework offers potential methodology for evaluating LLM agent workflows where manual data acquisition is costly.</p>
<p>Data mining and visual analysis</p>
<p>Catalysts play a vital role in chemical process optimization [38], with data mining enabling accelerated design through pattern recognition [39].Analyzing 839 PDH catalyst papers from a total of 1041 articles filtered by abstracts and titles in Q1-Q3 chemistry journals (1980-2024), our data mining module revealed comprehensive insights into catalysts' composition, structure, and performance.</p>
<p>For instance, publication analysis showed surging alloy research since 1995 and single-atom catalyst studies post-2015 (Fig. 3a), driven by advancements in structural composition (Fig. 3b).Performance analysis identified optimal promoter elements (Zn, Sn, La) (Fig. 3c) and support materials (alumina, zeolites) (Fig. 3d), while combination studies revealed superior performance in multi-metal systems, particularly Pt-based catalysts with Sn, Zn, In promoters (Fig. 3e).Moreover, impregnationprepared nanometallic catalysts demonstrated high conversion rates and selectivity, contrasting with single-atom alloys' high selectivity but lower conversion rates (Fig. 3f).</p>
<p>This comprehensive analysis reveals variable interactions and guides catalyst optimization, recommending Pt-based systems for selectivity and metal oxides for conversion rates, while highlighting the promise of single-atom and nanostructured catalysts.These findings not only establish performance benchmarks in PDH catalysis, but also demonstrate how our LLM-based methodology enables real-time scientific insight ex-traction, facilitating industrial-oriented catalyst design optimization.Complete data charts are available in the SI.</p>
<p>Hallucination mitigation</p>
<p>Unlike search engines, LLMs' process of understanding information and outputting it anew provides LLMs' creativity while inevitably accompanying the "hallucination" phenomenon, referring to false information generated by LLMs without sufficient evidence support or contextually inconsistent, off-input information responses [40].LLM hallucinations mainly originate from statistical biases in the training phase, noise in training data, and decision strategies when handling uncertain or multi-interpretable information during the alignment phase [40].Currently, there is no solution within the field [9,40,41], and research suggests that hallucinations are unavoidable [42,43].Especially in specialized sub-fields, LLMs greatly exacerbate the hallucination phenomenon due to extremely scarce data exposure.For example, tests in literature [44] show that even the most advanced GPT-4 only has a 73.3% accuracy rate when answering professional multiple-choice questions, which is far from sufficient for scientific research fields that strictly require correctness.In scientific research, this may lead to unrealistic academic conclusions and misleading research directions, often meaning great waste of time and material consumption [41].Therefore, effective hallucination mitigation is crucial for ensuring the scientific nature and reliability of automated review generation.</p>
<p>To address the challenge of hallucinations in LLMs, a high priority has been placed on the detection and prevention of such phenomena.In the entire automated review generation process, we adopted a multi-level filtering and verification quality control strategy, similar to the concept of retrieval-augmented generation (RAG) [12,45], to mitigate and correct hallucinations:</p>
<p>Prompt design and task decomposition.Firstly, we utilized strict and clear text summary guiding prompts, aimed at enhancing the scientific rationality of LLM's outputs and ensuring accuracy and reliability in its analysis and generation processes.Notably, the task of automated review generation aligns well with the strengths of LLMs-information extraction and text generation capabilities.LLMs can rapidly and accurately extract core information from a vast array of literature and integrate it into a coherent and rigorous review text.To enhance efficiency and quality, we deconstructed the core of the review writing process, namely literature reading and summarization, into a series of text summarization tasks.This approach is adopted because summaries generated by LLM significantly surpass manually crafted and fine-tuned model-generated summaries in terms of fluency, factual consistency, and flexibility [46].By establishing a list of questions, we directed the model to extract relevant content from the literature and respond based on this content, subsequently conducting a comprehensive analysis of all literature citations and responses.Ultimately, the LLM generates high-quality paragraphs closely related to the topic.Additionally, we employed a single-round, segmented generation strategy to avoid truncation limitations of approximately 8K output length.By reasonably segmenting long texts for generation, we not only ensured that the output was completed in a single conversational round but also provided finer parallel granularity to improve generation efficiency.In practice, we divided the 35 questions into 5 groups, ensuring that the generation results for each group could be successfully completed within the 8K limit of the LLM.This granularity avoids efficiency drops due to a high proportion of shared content and identical prompt frameworks, thereby enhancing processing speed while ensuring the quality of text generation.</p>
<p>Hallucination filtering and verification.To mitigate and rectify hallucinations, we employed a layered filtering and verification approach:</p>
<p>1.Text format filtering: Noting that hallucinations often disrupt text formatting, we applied a predefined XML format template to filter out disarrayed texts.</p>
<ol>
<li>
<p>DOI verification: DOIs, a combination of symbols and numbers lacking direct semantic linkage to context, present a challenge in generation and are prone to hallucinations.Yet, the precise reference nature of DOIs allows for verification.Through strict DOI verifications on generated content, we suppressed hallucinatory content from advancing further, ensuring each generated conclusion is traceable to its original source.</p>
</li>
<li>
<p>Relevance verification: Within the RAG system, documents related in semantics but lacking correct answers are particularly detrimental [47].We scrutinized each response in the knowledge extraction phase to ensure its relevance, eliminating off-topic answers with relevant keywords.</p>
</li>
<li>
<p>Self-consistency [48] verification: For text summarization, where a definitive correct answer exists, recognizing that the stochasticity of hallucinations means correct answers should recur more frequently across iterations, we employ aggregation from repeated queries to effectively suppress hallucinations.</p>
</li>
<li>
<p>Full data stream traceability mechanism: By using DOIs as key reference identifiers for each piece of generated content and mandating citations for every conclusion, we enable review readers to easily trace back to the original literature, supporting verification and deeper exploration in topics of interest.</p>
</li>
</ol>
<p>Effectiveness of hallucination mitigation.In evaluating the effectiveness of hallucination mitigation, we employed a confusion matrix to classify outcomes according to whether the LLM provided content and its pertinence to the original text, differentiating between two types of inaccuracies: false positives, which include fabricated or inconsistent information, and false negatives, referring to overlooked or partially extracted content.Our focus was primarily on reducing false positives, while adopting a relatively tolerant stance on false negatives.</p>
<p>Substantial progress was made in mitigating hallucinations.Specifically, in the paragraph generation part, which was achieved through 9 repetitions of 35-paragraph generation tasks, a total of 315 paragraphs that passed format checking and DOI checking were needed.Throughout the entire paragraph generation process, statistics show that LLM cumulatively performed 875 generations, of which only 36% of generation results passed after format and DOI list checks.In the analysis involving 343 topic-related literature, we divided 35 questions into 7 questions per segment for each literature, i.e., 5 segments per literature, totaling 1715 knowledge extractions.By conducting 5 repeated questions in each knowledge extraction, we obtained a total of 8575 answers, and finally aggregated 2783 effective information combinations after excluding answers unrelated to the literature and questions.Among these, up to 84.80% of the results were judged by LLM to have a 100% consistency rate when compared with the aggregated results (Table 1 and Fig. 4a), thus verifying the model's stability.For specific methods, see the Methods section.This method also provides a rough standard for judging the proportion of hallucinations, which can be used in the screening and evaluation of LLMs.</p>
<p>To assess the effectiveness of the knowledge extraction and data mining stages, we implemented a rigorous manual verification process.Specifically, 25 randomly selected articles from each stage were evaluated by a third-year PhD student specializing in PDH research.For the knowledge extraction stage, 35 segments per article were examined, totaling 875 data points.The data mining stage assessed 14 catalyst properties, including 5 direct answer repetitions and final generated results, encompassing 1750 and 350 data points respectively.We employed precise classification criteria for the evaluation.</p>
<p>In the knowledge extraction phase, true negatives (TN) were instances where the article did not address the guiding question and the LLM correctly identified it as irrelevant.True positives (TP) occurred when relevant topics were accurately extracted.False negatives (FN) were cases where relevant topics were incompletely extracted or incorrectly deemed irrelevant.False positives (FP) included irrelevant topics mistakenly identified as relevant or extractions that exceeded or deviated from the article's actual content.Similar criteria were applied to the data mining stage, with particular attention to unit conversion errors, which were classified as false positives even if numerical values were correct.Consistency comparisons were conducted using the Claude2 model through designed prompt templates, comparing pre-and post-aggregation texts and statistically analyzing the model's scoring results.Based on these evaluations, we calculated key metrics including accuracy, false positive rate (with 95% confidence intervals), precision, recall, F1 score, and consistency rate (Table 1).Confidence intervals for false positive rates were computed using Python3's statsmodels library.For detailed results and calculation methods, see SI.</p>
<p>It is crucial to emphasize that this manual verification step was conducted to demonstrate the method's effectiveness during the proof-of-concept phase and is not required in the actual automated review generation process.The detailed results are presented in Table 1.The data comparison underscores the efficacy of self-consistency verifications, revealing a substantial decrease in hallucinations, i.e., false positive content, while also compensating for some false negatives, where information was not fully extracted (Fig. 4b).In the knowledge extraction phase, critical for review content, our manual sampling found no fabricated conclusions by LLMs (Fig. 4a), attesting to our method's scientific integrity and reliability.From the sampling results, we are over 95% confident that the likelihood of hallucinations in this part is less than 0.5% (Table 1), which is also the source of our confidence that this method supports fully automated processes without manual intervention.Analysis of false positives in the post-aggregation data mining phase revealed hallucinations typically involved correct numerical extraction but with errors in units or definitions.False negatives mainly stemmed from LLMs' inability to comprehend highly abstract expressions, reflecting a general LLM's limited understanding of highly specialized scientific concepts.The incidence of hallucinations in knowledge extraction was significantly lower than in data mining, as answering questions did not involve converting units and concepts, thus avoiding the most challenging part of testing an LLM's grasp of scientific knowledge.Domain-specific models enhanced by domain-adaptive pretraining (DAPT) [49] are poised to mitigate this issue.Opting not to fine-tune LLMs for specific domains in this study prioritizes out-of-thebox functionality and multi-domain generalization, utilizing a general LLM as the base.Comparisons between RAG and fine-tuning effects in specific domains indicate that RAG sustains efficacy with contextually new knowledge and offers a significantly lower initial cost [50], aligning with our objective to support researchers' entry into diverse fields efficiently.</p>
<p>Considering the stringent accuracy requirements in research, increasing the number of repetitions can significantly reduce the probability of hallucinations appearing in aggregated results.Binomial probability calculations indicate that theoretically, a model with 79.09% accuracy yields aggregated prediction accuracies of 93.49%, 96.12%, and 97.64% after five, seven, and nine independent predictions, respectively, aligning with our sampling results (Table 1).Detailed sampling outcomes and calculations are available in the SI.We believe that 5 repetitions is an ideal empirical value, and users do not need to change this parameter when using it.On this foundation, every conclusive description in the generated reviews is supported by literature references and has been verified by relevant field researchers through tracing the cited literature, confirming that all literature references are correctly linked to the original publications and that the descriptions in the generated reviews correspond to those in the original publications.</p>
<p>This multi-layered strategy for hallucination control has built an effective verification system, ensuring the scientific integrity and reliability of the automated review generation.Furthermore, through a full data stream traceability mechanism, the authenticity and practicality of the content are further strengthened.This not only provides a secondary means of hallucination mitigation but also allows researchers to delve into original research papers for more precise and detailed academic information while accessing fast, automated research reviews.The strategy also implements a kind of literature recommendation mechanism.Since each content segment includes related DOIs, researchers can quickly locate specific original literature based on their interests and research needs, enabling deeper academic exploration.</p>
<p>While both our method and RAG utilize LLM's context learning ability, our approach fundamentally differs by achieving systematic knowledge reconstruction through multi-stage processing rather than simple retrieval combination.This method simulates the complete academic research process and produces coherent knowledge frame-works aligned with scholarly thinking, surpassing traditional RAG's question-answering paradigm through comprehensive quality control and hallucination mitigation mechanisms.</p>
<p>CONCLUSIONS</p>
<p>In this study, we introduce an innovative LLM-based automated review generation method, addressing two fundamental challenges in scientific research: improving literature review efficiency and mitigating LLM hallucination risks.Through proposing an evaluation framework that ensures the objectivity and reliability via statistical validation, and innovatively compared LLM-generated reviews with high-quality manual reviews, we demonstrate that our modular end-to-end approach produces reviews comparable to or exceeding human-written ones, while maintaining high reliability and traceability.Expert evaluation using PDH catalysts as a case study confirms the method's effectiveness: generated reviews are comparable to manual reviews in length and citations, show no hallucinations, and have impeccable reference accuracy.Statistical validation confirms the method's effectiveness in hallucination reduction, with testing on 875 LLM outputs from 25 random articles showing hallucination probability below 0.5% at 95% confidence.The quality assurance pipeline ensures robust data processing.Additionally, our advanced data mining module offers experienced users' in-depth field integrated perception, fully exploiting LLMs' analytical capabilities.Furthermore, an open-source user-friendly one-click program developed for Windows platforms significantly simplifies the review generation process.</p>
<p>The method's architecture offers significant advantages through its crossdisciplinary applicability without manual intervention or domain-specific knowledge injection.Its modular design enables component reuse for literature tracking, topic discovery, and dataset construction, while achieving cross-disciplinary generalization through LLM's inherent contextual adaptability.This means that by providing corresponding domain literature input, the method can generate high-quality reviews across various disciplines.Future development will focus on enhanced multimodal processing capabilities, automated scientific question generation and answering, personalized text generation, integration with existing academic tools, and domain-specific features for structured data analysis.</p>
<p>This advancement heralds a new era in human-machine academic collaboration, offering broad prospects for LLMs as writing assistance tools.While not intended to replace traditional manual reviews, our method serves as a powerful auxiliary tool for rapid domain overview and research hotspot identification, laying the foundation for in-depth analysis.Beyond its demonstrated excellence in chemistry, the method's technical framework exhibits remarkable cross-disciplinary applicability, potentially breaking down barriers between fields and catalyzing interdisciplinary innovation.By revolutionizing researcher-literature interaction and accelerating knowledge dissemination, this milestone advancement holds profound implications for knowledge base construction, literature recommendation, and structured academic writing, heralding a new era of scientific research productivity and interdisciplinary collaboration.</p>
<p>METHODS</p>
<p>Our method consists of four core components: literature search, topic formulation, knowledge extraction and review composition, along with a data mining module (Fig. 5a) and quality assessment framework (Fig. 5b).All prompt templates are available in SI and GitHub without requiring user adjustment.</p>
<p>Literature search</p>
<p>Literature retrieval begins with journal selection from journal classification tables, followed by an API-based keyword search and preliminary title/abstract filtering using a keyword list, with review-type literature marked separately (Fig. 5a, i).</p>
<p>Topic formulation</p>
<p>Review topics can be constructed either through direct LLM outline drafting or through LLM extraction and refinement of existing literature reviews (Fig. 5a, ii).After obtaining a list of topics, additional topics can be manually added and sorted as needed.This manual addition is not mandatory but provides an interface for advanced users to intervene if necessary.</p>
<p>Knowledge extraction</p>
<p>Based on the topic list, LLM generates extraction questions and conducts multiple rounds of information retrieval from each article.The LLM evaluates answer relevance to questions using structured prompts, where combinations of questions, LLM-aggregated relevant answers, and their corresponding citations constitute valid information combinations for subsequent processing.For literature exceeding the context window of LLM, the text is segmented into approximately equal parts, processed separately, and results are integrated during answer aggregation (Fig. 5a, iii).</p>
<p>Review composition</p>
<p>Extracted answers are associated with source DOIs and integrated into topicspecific paragraphs.Through multiple iterations and LLM scoring, optimal versions are selected to form the preliminary draft, followed by citation verification and format standardization.For answers exceeding context window length, LLM performs compression based on referenced texts and extracted answers until fitting within window limits (Fig. 5a, iv).</p>
<p>Data mining</p>
<p>The data mining module extends knowledge extraction (Fig. 5a, iii) capabilities for specific data extraction and aggregation, enables extraction of user-defined targets (e.g., catalyst types, compositions, performance metrics) from literature.The LLM performs multiple rounds of parsing and extraction in XML format, followed by result aggregation.The extracted data undergoes standardization and cleaning, with GPT4-generated code facilitating statistical analysis and visualization, requiring no programming expertise from users.</p>
<p>Quality Assessment</p>
<p>The evaluation framework employs dual baselines using manual Q1 journal reviews and direct LLM generation for quality assessment (Fig. 5b).High-quality reviews are semantically segmented, with corresponding content regenerated using our method and compared against direct LLM-generated content.Assessment utilizes chain-ofthought prompts across 27 scoring items in 9 categories, implementing cross-evaluation and repetition strategies to mitigate bias.The page rerank algorithm converts relative comparisons to absolute scores on a 0-10 scale, with framework reliability validated through intraclass correlation coefficient (ICC) tests and transitive consistency ratio (TCR) analyses.our data processing work, except for the evaluation section, was completed prior to November 21, 2023, utilizing the then-latest available Claude 2 API version [https://www.anthropic.com/news/claude-2,https://www.anthropic.com/news/claude-2-1].Anthropic did not publish specific minor version numbers within the Claude 2 series, only distinguishing between Claude 2, Claude 2.1, and the subsequent Claude 3 series.Our proposed framework demonstrates good adaptability, with overall effectiveness increasing as the performance of the underlying model improves.This characteristic has been amply demonstrated in our evaluation work, indicating that the framework's efficacy is not strictly dependent on any particular model version.The use of different LLMs in the Evaluation of generated review quality section was primarily to assess the performance of the latest and most powerful open-source and closed-source models (as of September 2024) under the method described in this paper.</p>
<p>Our published graphical user interface (GUI) leverages certain APIs for functionality, which, due to legal and regulatory requirements, necessitate that users provide their own API keys.This requirement is detailed in the documentation accompanying the code repository to assist users in setting up and utilizing the GUI effectively.</p>
<p>Figure 1 :
1
Figure 1: Reliability verification results of the dual-baseline review quality assess-</p>
<p>Figure 2 :
2
Figure 2: Quality assessment results of automatically generated reviews.Heat map of the percentage difference in scores of review paragraphs generated by this method relative to human scores, red to green showing -100% to +100% range, higher values indicate better performance, values truncated to ±100% range, values exceeding are recorded as -100% and +100%: a, Highest scoring paragraph of Claude3.5 Sonnet model; b, Highest scoring paragraph of Qwen2-72b-Instruct model; c, Average paragraph score of Claude3.5 Sonnet model; d, Average paragraph score of Qwen2-72b-Instruct model.e, Histogram of percentage differences in scores relative to human scores for highest scoring paragraphs, average paragraph scores, and directly generated paragraph scores without going through this method for Claude3.5 Sonnet model and Qwen2-72b-Instruct model, colors ranging from dark to light representing Claude3.5</p>
<p>Figure 3 :
3
Figure 3: Example of visual analysis results.Line charts for annual publication numbers: a, different catalyst types; b, Performance enhancement sources.Radar charts for peak performance of single factors, with selectivity (black) and stability (purple) scales: c, Promoter elements; d, Support materials.Bubble charts for dual-variable correlations, show selectivity (color depth), conversion rate (bubble size), and stability (bubble edge thickness), aiming for high selectivity, conversion rate, and stability.Data includes only those with selectivity ≥85%, conversion rate ≥45%, stability ≥1h: e, Active site element-composition element; f, Alloy structure type-preparation method.Complete data charts are available in the SI.</p>
<p>4 :
4
Effectiveness of hallucination mitigation.a, Consistency as determined by LLMs between direct LLM responses and aggregated results during the knowledge extraction phase, where blue represents 100% consistency and orange less than 100%.b, Distribution of manual sampling results for direct LLM responses and aggregated outcomes during the data mining phase, with TP (True Positive), TN (True Negative), FP (False Positive), FN (False Negative)</p>
<p>a b 5 :
5
a, Flowchart of the automated review generation method based on large language models.It includes four modules: i) literature search, ii) topic formulation, iii) knowledge extraction, iv) review composition, as well as an additional data mining module.b, Flowchart of the quality assessment framework for review generation based on large language models.</p>
<p>Table 1 :
1
Comparison of results before and after self-consistency aggregation
StageData PointsAccu-racyFalse Positive95%CI of FPRPreci-sionRecallF1 ScoreConsist -encyRateKnowledge Extraction (Aggregated)87595.77% 0.000%0.000% -0.485%100.0% 57.47% 72.99% 84.80%Data Mining (Direct Response)175079.09% 35.34%31.45% -39.42%84.14% 85.68% 84.90%86.60%12.20%Data Mining35093.71% 18.75%-93.28% 98.43% 95.79%(Aggregated)27.70%</p>
<p>Online database Abstracts Correlation test highly correlated papers Local database LLM Direct Review Topics Creation LLM Literature-based Review Topics Creation LLM for Literature Answer Extraction LLM for Aggregating Literature Answers LLM for Paragraph Construction LLM for Full-text Refinement LLM for Paragraph Scoring
Key-wordsAPI1. Automated Literature Search2. Automated Topic Formulation4. Automated Review Composition3. Automated Knowledge ExtractionSegment peer-Extract uniquereviewed, published expert reviewstopics for each segment using LLMLLM-basedValidate reliability via ICC and transitivity consistency testsComparative Assessment Framework for Review GenerationRegenerate reviews based on original cited literature by LLMQualityPairwise compareConvert scores to 0-10 scale using Page Rerankgenerated reviews, original reviews, anddirect LLM output
CONFLICT OF INTEREST STATEMENTThe authors declare no competing interests.ACKNOWLEDGMENTWe acknowledge the Natural Science Foundation of China (No. 22121004), the Haihe Laboratory of Sustainable Chemical Transformations, the Program of Introducing Talents of Discipline to Universities (BP0618007) and the XPLORER PRIZE for financial support.We also acknowledge generous computing resources at High Performance Computing Center of Tianjin University.AUTHOR CONTRIBUTIONSJ.G and Z.Z.conceived and supervised the project.S.W. and X.M. designed the research and developed the program.S.W. led the manuscript preparation.D.L. performed manual verification of hallucination.L.L., X.S., and X.C. advanced the integration of computational models, while X.L., R.L., C.P. and C.D. advanced the catalytic science.All authors contributed to writing and revising the manuscript.DATA AVAILABILITYOur study leverages a dataset compiled from scientific literature acquired through our institution's subscription.Due to copyright considerations, the dataset itself cannot be made publicly available.However, we ensure that our research's integrity and reproducibility do not rely on direct access to these proprietary documents.Instead, we provide extensive documentation on the dataset's structure, the criteria used for literature selection, and the analysis methods applied, enabling interested researchers to reconstruct a similar dataset from publicly available resources or their institutional subscriptions.Furthermore, to facilitate a deeper understanding of our research process and promote further exploration and innovation, we have made all intermediate data, excluding the copyrighted full-text articles, publicly available on GitHub [https://github.com/TJU-ECAT-AI/AutomaticReviewGenerationData].This repository includes the prompts used in our study and the corresponding responses generated by the large language model.By sharing these resources, we aim to provide valuable insights into our methodology and encourage other researchers to build upon our work, advancing the field of natural language processing and its applications in scientific literature analysis.CODE AVAILABILITYThe custom code developed for this research is central to our conclusions and is made available to ensure transparency and reproducibility of our results.The codebase, including all relevant custom scripts and mathematical algorithms, has been opensourced under the Apache 2.0 license and is accessible via our GitHub repository at [https://github.com/TJU-ECAT-AI/AutomaticReviewGeneration].We encourage users to review the license for any usage restrictions that may apply.As stated in the text, all LLMs invoked in this article are Claude 2, except for the Evaluation of generated review quality section which uses Claude 3.5 Sonnet, Qwen2-72b-Instruct, and Qwen2-7b-Instruct.It is important to note thatADDITIONAL INFORMATIONSupplementary information is available for this paper.Correspondence and requests for materials should be addressed to J.G.
Literature reviews: modern methods for investigating scientific and technological knowledge. Apc Ermel, D P Lacerda, Miw Morandi, L Gauss, 2021Springer Nature</p>
<p>Free online availability substantially increases a paper's impact. Nature. S Lawrence, May 31 2001411521</p>
<p>Speed reading: scientists are struggling to make sense of the expanding scientific literature. Corie Lok asks whether computational tools can do the hard work for them. C Lok, Nature. 46372802010</p>
<p>Recent Advances in Lead Chemisorption for Perovskite Solar Cells. P F Wu, F Zhang, Oct 202228Transactions of Tianjin University</p>
<p>Natural language processing: state of the art, current trends and challenges. D Khurana, A Koli, K Khatter, S Singh, Multimed Tools Appl. 8232023</p>
<p>Summary of chatgpt-related research and perspective towards the future of large language models. Y Liu, T Han, S Ma, Meta-Radiology. 1000172023</p>
<p>Measuring massive multitask language understanding. D Hendrycks, C Burns, S Basart, arXiv:2009033002020arXiv preprint</p>
<p>D Rein, B L Hou, A C Stickland, arXiv:231112022A graduate-level google-proof q&amp;a benchmark. 2023arXiv preprint</p>
<p>The future of chemistry is language. A D White, Nature Reviews Chemistry. 77Jul 2023</p>
<p>Paperqa: Retrieval-augmented generative agent for scientific research. J Lála, O 'donoghue, O Shtedritski, A Cox, S Rodriques, S G White, A D , arXiv:2312075592023arXiv preprint</p>
<p>Language agents achieve superhuman synthesis of scientific knowledge. M D Skarlinski, S Cox, J M Laurent, arXiv:2409137402024arXiv preprint</p>
<p>Retrieval-augmented generation for knowledgeintensive nlp tasks. P Lewis, E Perez, A Piktus, Advances in Neural Information Processing Systems. 332020</p>
<p>S Wei, X Xu, X Qi, arXiv:231112315Empowering Academic Research. 2023arXiv preprint</p>
<p>Z Yang, Z Zhu, Curiousllm, arXiv:240409077Elevating Multi-Document QA with Reasoning-Infused Knowledge Graph Prompting. 2024arXiv preprint</p>
<p>Multi-document summarization via deep learning techniques: A survey. C Ma, W E Zhang, M Guo, H Wang, Q Z Sheng, Acm Comput Surv. 5552022</p>
<p>Automatic generation of reviews of scientific papers. A Nikiforovskaya, N Kapralov, A Vlasova, O Shpynov, A Shpilman, IEEE2020</p>
<p>Using citations to generate surveys of scientific paradigms. S Mohammad, B Dorr, M Egan, 2009</p>
<p>Towards multi-document summarization of scientific articles: making interesting comparisons with SciSumm. N Agarwal, R S Reddy, G Kiran, C Rose, 2011</p>
<p>Deconstructing human literature reviews-a framework for multi-document summarization. K Jaidka, C Khoo, J-C Na, 2013</p>
<p>SciReviewGen: A Large-scale Dataset for Automatic Literature Review Generation. T Kasanishi, M Isonuma, J Mori, I Sakata, arXiv:2305151862023arXiv preprint</p>
<p>System for systematic literature review using multiple ai agents: Concept and an empirical evaluation. A M Sami, Z Rasheed, K-K Kemell, arXiv:2403083992024arXiv preprint</p>
<p>S Agarwal, I H Laradji, L Charlin, C Pal, Litllm, arXiv:240201788A Toolkit for Scientific Literature Review. 2024arXiv preprint</p>
<p>C Y Haryanto, Llassist, arXiv:240713993Simple Tools for Automating Literature Review Using Large Language Models. 2024arXiv preprint</p>
<p>Cutting Through the Clutter: The Potential of LLMs for Efficient Filtration in Systematic Literature Reviews. L Joos, D A Keim, M T Fischer, arXiv:2407106522024arXiv preprint</p>
<p>Y Li, L Chen, A Liu, K Yu, Wen L Chatcite, arXiv:240302574LLM Agent with Human Workflow Guidance for Comparative Literature Summary. 2024arXiv preprint</p>
<p>ChatGPT outperforms crowd workers for textannotation tasks. F Gilardi, M Alizadeh, M Kubli, P Natl Acad Sci. 12030e2305016120Jul 25 2023</p>
<p>Chatgpt-4 outperforms experts and crowd workers in annotating political twitter messages with zero-shot learning. P Törnberg, arXiv:2304065882023arXiv preprint</p>
<p>Large Language Models as Evaluators for Recommendation Explanations. X Zhang, Y Li, J Wang, 2024</p>
<p>Language models (mostly) know what they know. S Kadavath, T Conerly, A Askell, arXiv:2207052212022arXiv preprint</p>
<p>Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena. L M Zheng, W L Chiang, Y Sheng, Adv Neur In. 362023</p>
<p>Benchmarking foundation models with language-modelas-an-examiner. Y Bai, J Ying, Y Cao, Advances in Neural Information Processing Systems. 362024</p>
<p>Critic: Large language models can self-correct with tool-interactive critiquing. Z Gou, Z Shao, Y Gong, arXiv:2305117382023arXiv preprint</p>
<p>Llm-based nlg evaluation: Current status and challenges. M Gao, X Hu, J Ruan, X Pu, Wan X , arXiv:2402013832024arXiv preprint</p>
<p>Large language models are not fair evaluators. P Wang, L Li, L Chen, arXiv:2305179262023arXiv preprint</p>
<p>Y Liu, D Iter, Y Xu, S Wang, R Xu, C Zhu, G-Eval, arXiv:230316634Nlg evaluation using gpt-4 with better human alignment. 2023arXiv preprint</p>
<p>Split and merge: Aligning position biases in large language model based evaluators. Z Li, C Wang, P Ma, arXiv:2310014322023arXiv preprint</p>
<p>Large language models are not yet human-level evaluators for abstractive summarization. C Shen, L Cheng, X-P Nguyen, Y You, L Bing, arXiv:2305130912023arXiv preprint</p>
<p>Progress in Processes and Catalysts for Dehydrogenation of Cyclohexanol to Cyclohexanone. J Gong, S X Hou, Y Wang, X B Ma, Jun 202329Transactions of Tianjin University</p>
<p>Data-Driven Design of Single-Atom Electrocatalysts with Intrinsic Descriptors for Carbon Dioxide Reduction Reaction. X Y Lin, S Y Zhen, X H Wang, Oct 202430Transactions of Tianjin University</p>
<p>Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models. Y Zhang, Y Li, L Cui, arXiv:2309012192023arXiv preprint</p>
<p>GPT-4 is here: what scientists think. K Sanderson, Nature. 6157954Mar 30 2023</p>
<p>Hallucination is inevitable: An innate limitation of large language models. Z Xu, S Jain, M Kankanhalli, arXiv:2401118172024arXiv preprint</p>
<p>Calibrated language models must hallucinate. A T Kalai, S S Vempala, 2024</p>
<p>A comparative study of open-source large language models, gpt-4 and claude 2: Multiple-choice test taking in nephrology. S Wu, M Koo, L Blum, arXiv:2308047092023arXiv preprint</p>
<p>Large language models should be used as scientific reasoning engines, not knowledge databases. D Truhn, J S Reis-Filho, J N Kather, Nature Medicine. 2023</p>
<p>Summarization is (almost) dead. X Pu, M Gao, Wan X , arXiv:2309095582023arXiv preprint</p>
<p>The power of noise: Redefining retrieval for rag systems. F Cuconasu, G Trappolini, F Siciliano, 2024</p>
<p>Self-consistency improves chain of thought reasoning in language models. X Wang, J Wei, D Schuurmans, arXiv:2203111712022arXiv preprint</p>
<p>Don't stop pretraining: Adapt language models to domains and tasks. S Gururangan, A Marasović, S Swayamdipta, arXiv:2004109642020arXiv preprint</p>
<p>RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture. A Gupta, A Shirgaonkar, Balaguer Adl, arXiv:2401084062024arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
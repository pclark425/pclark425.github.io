<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-395 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-395</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-395</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-15.html">extraction-schema-15</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <p><strong>Paper ID:</strong> paper-272832255</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2409.15658v2.pdf" target="_blank">Long-horizon Embodied Planning with Implicit Logical Inference and Hallucination Mitigation</a></p>
                <p><strong>Paper Abstract:</strong> Long-horizon embodied planning underpins embodied AI. To accomplish long-horizon tasks, one of the most feasible ways is to decompose abstract instructions into a sequence of actionable steps. Foundation models still face logical errors and hallucinations in long-horizon planning, unless provided with highly relevant examples to the tasks. However, providing highly relevant examples for any random task is unpractical. Therefore, we present ReLEP, a novel framework for Real-time Long-horizon Embodied Planning. ReLEP can complete a wide range of long-horizon tasks without in-context examples by learning implicit logical inference through fine-tuning. The fine-tuned large vision-language model formulates plans as sequences of skill functions. These functions are selected from a carefully designed skill library. ReLEP is also equipped with a Memory module for plan and status recall, and a Robot Configuration module for versatility across robot types. In addition, we propose a data generation pipeline to tackle dataset scarcity. When constructing the dataset, we considered the implicit logical relationships, enabling the model to learn implicit logical relationships and dispel hallucinations. Through comprehensive evaluations across various long-horizon tasks, ReLEP demonstrates high success rates and compliance to execution even on unseen tasks and outperforms state-of-the-art baseline methods.</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e395.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e395.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReLEP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Real-time Long-horizon Embodied Planning (ReLEP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework that fine-tunes a large vision-language model to decompose long-horizon embodied instructions into sequences of pre-defined skill functions, using a Skill Library, Memory module, and Robot Configuration to encode procedural, spatial, and object-relational constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaVA-1.6-7B (fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A vision-language instruction-following transformer (LLaVA-1.6-7B) fine-tuned with LoRA on a 5K + 24K real-world embodied planning dataset (images, tasks, plans). It takes raw images and instruction text as input and outputs structured sequences of skill-function calls (APIs).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Real-time long-horizon embodied planning</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given a natural-language embodied task and a current scene image, generate a sequence of pre-defined skill functions (from a skill library) to accomplish the task; run in repeated rounds where after executing the first skill, a new image is observed and a new plan is produced referring to Memory and prior plans. Evaluated in AI2-THOR simulated scenes and on real-world collected images across navigation, manipulation, inspection, delivery and other multi-step everyday tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step planning (navigation + object manipulation + household tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>procedural+spatial+object-relational</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>fine-tuning on embodied task dataset (images+task+plans) + visual input at runtime + predefined skill library + Memory + Robot Configuration prompts</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>fine-tuning (instruction-following) and structured prompting (skill library, memory, robot configuration)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>procedural knowledge represented explicitly as ordered action/skill sequences (API calls); spatial and object-relational knowledge encoded implicitly in model weights and visual feature inputs; task state encoded explicitly in Memory (finished steps, robot status); embodiment constraints provided as natural-language Robot Configuration</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Initial Plan Success Rate (IPSR), Step-wise Success Rate (SSR), overall Success Rate (SR), Language Compliance (LC)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Real-world offline image evaluation: IPSR 94.3%, SSR 97.7%, SR 94.3% (Table 3). In AI2-THOR simulated tasks ReLEP achieved very high IPSR/SSR/SR across the ten evaluated tasks (substantially outperforming baselines; see Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Successfully decomposes high-level instructions into executable multi-step skill sequences; maintains consistency across rounds by using Memory; handles embodiment constraints (avoids impossible sequences like consecutive Grasps for single-arm robots when prompted); mitigates hallucinated undefined skills after fine-tuning on implicit logical relations.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Requires reliable low-level skill implementations and robust perception; small environment changes (e.g., 'getting closer to a target') may be hard to detect without Memory—if Memory disabled, model can loop on Detect/Navigation; real-world deployment dependent on underlying skill execution robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to VILA* and GPT-4V* baselines; in real-world offline test VILA*: IPSR 61.4%, SSR 81.1%, SR 58.6%; GPT-4V*: IPSR 82.9%, SSR 77.5%, SR 22.9%; ReLEP+GPT-4V: IPSR 94.3%, SSR 83.6%, SR 34.3% (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Ablating fine-tuning (use raw LLaVA-1.6-7B zero-shot or with loosely relevant examples) greatly reduced Language Compliance and SR; removing Memory caused repeated Detect/Navigation loops and large SSR drop — Memory is necessary to mark finished steps and robot status for subsequent-round planning.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Fine-tuning a vision-language model on data that encodes implicit logical relationships between skills shifts the model from hallucination-prone freeform planning to generating structured, executable skill sequences; procedural knowledge is most effectively handled by representing actions as explicit API-style skill functions and training the model to output those tokens; Memory and Robot Configuration supply essential state and embodiment constraints so that the model can correctly sequence skills even when visual changes are subtle or absent.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Long-horizon Embodied Planning with Implicit Logical Inference and Hallucination Mitigation', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e395.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e395.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Skill Library (ReLEP)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pre-defined Skill Library (nine skills) used by ReLEP</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A curated set of nine interactive and non-interactive skill functions (e.g., Detect, Grasp, Navigate, Pull, Push, Put, Speak, EQA, Wait) that ReLEP plans over; plans are sequences of calls to these skills, which serve as the procedural action representation layer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaVA-1.6-7B (fine-tuned in ReLEP)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Not a language model per se but the action API interface that the fine-tuned LVLM is trained to output; each skill has a fixed name and parameter schema and is used as the atomic unit of procedural planning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Action abstraction for embodied planning</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Provides the atomic operations that an embodied planner must sequence to achieve tasks (e.g., Navigate(destination), Detect(object), Grasp(object), Put(object, place, side)). The planner must assemble these into multi-step plans conditioned on image and memory.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>procedural abstraction for multi-step tasks</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>procedural (primary) + object-relational (parameters) + spatial (destination/place arguments)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>designed API schema (explicit), training dataset pairs (image, task, plan) used to teach the LVLM how to map visual + instruction to the skill sequence</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>output prediction of skill tokens from fine-tuned LVLM (supervised learning)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>explicit symbolic API calls (skill name + structured arguments) representing procedural knowledge; arguments encode object identities, spatial targets (destination/place/side), and directions</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Language Compliance for skill format; execution success via SSR/SR when skills are executed by environment/simulator</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>When used by ReLEP, skill-format outputs achieved high Language Compliance and led to high SSR/SR (see ReLEP results). Untrained LVLMs produce freeform output and low LC.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Explicit skill tokens reduce hallucination and ambiguity; argument slots (place, side, direction) allow the model to represent spatial relations in the plan.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>If the LVLM does not understand the semantics of particular skill parameters (e.g., meaning of 'Put(..., side)'), it can produce logical understanding errors; large skill libraries increase the demand for training to avoid incorrect skill selection.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared with freeform natural-language plans (VILA*, raw LLaVA zero-shot), the skill-library-based outputs are more structured and executable.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Not ablated independently in isolation from the framework, but the paper reports that explicit skill outputs plus fine-tuning lead to higher LC and SR versus freeform outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Representing procedural knowledge as an explicit library of API-like skills and training the LVLM to output those tokens is an effective way to ground language planning into executable robot operations and reduce hallucination.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Long-horizon Embodied Planning with Implicit Logical Inference and Hallucination Mitigation', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e395.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e395.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Memory Module</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Memory module (Finished Steps + Robot Status) in ReLEP</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A stored state that records past plans, finished steps, and robot status so the planner can make consistent subsequent-round plans even when visual changes are subtle or absent.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaVA-1.6-7B (fine-tuned in ReLEP)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A non-learned storage/prompting module containing previously generated plans, a list of finished steps, and a natural-language description of robot status; presented to the LVLM at subsequent planning rounds to condition new plan outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Stateful multi-round planning</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Enables the LVLM to produce consistent plans across repeated observations by explicitly informing it which steps have been executed and the robot's current configuration/state.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>procedural state tracking for multi-step tasks</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>procedural (state/history) + object-relational (which objects have been manipulated) + spatial (robot position/status indirectly)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>runtime execution records (finished steps) and robot status fed as natural-language prompt context</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>prompt augmentation (past plan text + finished steps + status included in input to LVLM)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>explicit natural-language / structured memory of executed actions and robot configuration; not learned representation but used as conditioning context</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Step-wise Success Rate (SSR), loop-avoidance in subsequent-round planning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Removing Memory produced repeated Detect/Navigation loops and lower SSR; with Memory enabled ReLEP had substantially higher SSR (see ablation discussion and Table 4 summary).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Enables the model to advance to next steps when image changes are minimal (e.g., navigation that only changes viewpoint), preserves plan consistency across rounds.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Without Memory, planner often cannot infer that a non-visual skill finished and gets stuck repeating the same skill; Memory must be accurate and up-to-date or it can mislead planning.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>ReLEP with Memory vs ReLEP without Memory: Memory variant avoids the repeated-skill failure modes observed in GPT-4V* and Memory-ablated ReLEP.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Ablation (remove Memory) -> fails to predict new plans after Detect/Navigate, lower SSR, stuck in loops; explicit provision of finished steps is critical.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Explicitly storing and prompting finished steps and robot status is necessary to resolve perceptual ambiguity and to maintain procedural consistency in multi-round planning; visual input alone is sometimes insufficient to indicate that a step completed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Long-horizon Embodied Planning with Implicit Logical Inference and Hallucination Mitigation', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e395.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e395.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Robot Configuration Module</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Robot Configuration prompting module</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A natural-language description of the robot's embodiment and capabilities (e.g., single-arm vs humanoid) used to constrain planning and prevent physically impossible or inappropriate skill sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaVA-1.6-7B (fine-tuned in ReLEP)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A prompt-time module providing natural-language constraints about the robot (which skills it supports, degrees of freedom, number of arms), used to bias the LVLM's plan generation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Embodiment-aware planning</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Ensure generated skill sequences are realizable given the robot's physical capabilities (e.g., avoid two consecutive Grasps for single-arm robot).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>procedural + embodiment constraint enforcement</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>procedural + object-relational (which tools/objects robot can interact with) + spatial (capabilities affecting spatial reach/motion)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>explicit natural-language robot description provided as prompt context</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>prompt-conditioning (Robot Configuration included in LVLM input)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>explicit natural-language constraints describing embodiment/capabilities; acts as an additional symbolic context for planning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Plan executability and SR under different embodiment constraints</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Using Robot Configuration helps avoid logically impossible plans (e.g., consecutive Grasps) and improves executability; reported qualitatively in experiments (no numerical per-robot ablation shown).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Prevents generation of skill sequences incompatible with the robot (increases realism of plans).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>If Robot Configuration is absent or underspecified, LVLM may propose infeasible sequences for a given embodiment.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>ReLEP with Robot Configuration vs no-configuration (not explicitly ablated), but authors note its necessity for multi-embodiment generality.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Not separately ablated in main results, but authors claim robot-configuration prompting is required to adapt plans across robot types.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Explicit, natural-language embodiment descriptions provided as prompts are an effective way to impose physical constraints on high-level planning generated by vision-language models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Long-horizon Embodied Planning with Implicit Logical Inference and Hallucination Mitigation', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e395.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e395.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4V</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4V (vision-enabled GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A powerful commercial large vision-language model used as a baseline brain for embodied planning; can generate plausible plans with highly relevant in-context examples but is prone to logical errors and hallucinations without them.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4V</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A multimodal model (OpenAI's GPT-4 with vision) that accepts images and text and can output natural-language plans or structured plan code when prompted; used in experiments both zero-shot and with in-context examples.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Long-horizon embodied planning (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate step-by-step plans (in natural language or predefined plan format) from an image + task instruction; tested with varying in-context example relevance and as the planning brain in a ReLEP-like framework.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step planning (navigation + manipulation + inspection)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>procedural+spatial+object-relational</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pre-training on large multimodal corpora + in-context examples (few-shot) when provided</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>in-context prompting (zero-shot, loosely relevant, highly relevant examples); few-shot; direct image input</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>primarily natural-language reasoning and plan descriptions; can be prompted to return structured plan code or API-like calls but often outputs freeform text unless guided by examples</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Plausible Success Rate (PSR), Executable Success Rate (ESR), Language Compliance (LC), IPSR/SSR/SR in downstream tests</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>In real-world offline test (Table 3): IPSR 82.9%, SSR 77.5%, SR 22.9% when using highly relevant in-context examples for first-round planning. In in-context experiments, GPT-4V with highly relevant examples performs well; with loosely relevant examples it produces many logical errors and hallucinations.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>When given highly relevant, carefully constructed in-context examples, GPT-4V can produce executable, logically consistent multi-step plans and reason about spatial layout and object attributes from visual input.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Without highly relevant examples, it produces four major error types: logical understanding errors (misinterpreting skill semantics), skill combination errors (missing detection before navigation), logical planning errors (incorrect manipulation actions/directions), and hallucinations (fabricating undefined skills). It also gets stuck repeating Detect/Navigation in subsequent rounds when it cannot confirm step completion from images.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to ReLEP: GPT-4V* (used with ReLEP framework and highly relevant first-round examples) has good IPSR but much lower overall SR due to subsequent-round failures (Table 3); VILA* and GPT-4V* perform worse than fine-tuned ReLEP overall.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Authors show that the presence/absence and relevance of in-context examples strongly affects GPT-4V performance; GPT-4V requires highly relevant, task-specific examples to avoid logical errors.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Large vision-language models like GPT-4V can perform spatial and object-relational reasoning from images, but when operating primarily via in-context examples (without fine-tuning on implicit logical relationships and without explicit memory/embodiment prompts) they are prone to hallucination and logical planning failures; they may also be unable to reliably infer step completion from images alone, causing loops in multi-round planning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Long-horizon Embodied Planning with Implicit Logical Inference and Hallucination Mitigation', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e395.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e395.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaVA-1.6-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaVA-1.6 (7B) vision-language model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The base vision-language instruction-following model used as the ReLEP brain before and after fine-tuning; zero-shot/loosely prompted versions fail to output structured skill sequences whereas the fine-tuned variant can.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaVA-1.6-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A publicly released instruction-following vision-language transformer; authors fine-tune a checkpoint of this model with LoRA on their embodied planning dataset to produce ReLEP.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Embodied planning baseline and fine-tuning target</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Zero-shot or loosely prompted LLaVA produces freeform plans with low Language Compliance; after fine-tuning on the structured dataset it outputs skill-function sequences with high LC and SR.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step planning</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>procedural+spatial+object-relational</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pre-training + fine-tuning on ReLEP dataset</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>zero-shot / few-shot prompting (baseline) and supervised fine-tuning (ReLEP)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>visual features fed to transformer + learned implicit knowledge in weights; after fine-tuning, the model directly outputs symbolic skill sequences</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>LC, SSR, SR</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Zero-shot LLaVA produces freeform plans with zero LC; with loosely relevant examples it performs worse than GPT-4V and has more logical errors; fine-tuning (ReLEP) markedly improves LC and SR (see ablation).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>After fine-tuning, LLaVA can generate structured, executable plans and learn implicit logical inference between skills.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Without fine-tuning it cannot produce the required structured skill-token format and exhibits hallucinations and logical errors.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>LLaVA-1.6-7B (zero-shot) and with loosely relevant examples perform substantially worse than the fine-tuned ReLEP model.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Fine-tuning LLaVA on implicit logical-relationship-encoded dataset is necessary to acquire multi-round planning and to mitigate hallucinations.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Fine-tuning a vision-language model on data that encodes implicit logical relationships is effective at teaching the model to map visual inputs + instructions to executable procedural plans represented as skill sequences; without this fine-tuning, the vision-language model struggles to follow the required structured output format.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Long-horizon Embodied Planning with Implicit Logical Inference and Hallucination Mitigation', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e395.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e395.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VILA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>VILA (baseline leveraging GPT-4V)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline method that leverages GPT-4V to generate step-by-step plans and performs closed-loop embodied tasks; it reasons about spatial layout and object attributes but tends to output freeform natural language and sometimes ignores navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>VILA (uses GPT-4V as planner)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Framework that uses GPT-4V to generate executable plans from images; in these experiments VILA was equipped with the same predefined skill library as ReLEP but often returned long natural-language outputs where only a small portion was a plan.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Long-horizon embodied planning (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Use GPT-4V to reason about scenes and plan steps for navigation/manipulation tasks; evaluated in AI2-THOR and real-world images.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step planning (navigation + manipulation)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial+object-relational+procedural</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>GPT-4V pretraining and prompts; detectors/object lists when used</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>prompting GPT-4V with image + task</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>natural-language reasoning about scene layout and object attributes; sometimes uses object detectors to produce object lists</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>IPSR, SSR, SR</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>In real-world offline evaluation (Table 3) VILA*: IPSR 61.4%, SSR 81.1%, SR 58.6%; in simulated tests VILA* tended to overthink and produce long natural-language output and sometimes omitted navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Can reason about object attributes and spatial layout from visual input; can produce plausible textual plans.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Outputs lengthy freeform text with limited structured plan content; often ignores navigation and manipulates objects without approaching; lower IPSR and SR vs ReLEP.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>VILA* vs ReLEP: ReLEP substantially outperformed VILA* across IPSR/SSR/SR metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Not ablated in this paper; reported behavioral failure modes indicate need for structured skill outputs and memory to improve multi-round consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using a general-purpose vision-language model like GPT-4V without fine-tuning or structured output constraints can lead to verbose plans with poor executability; structured skill APIs and memory resolve many of these issues.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Long-horizon Embodied Planning with Implicit Logical Inference and Hallucination Mitigation', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e395.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e395.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TaPA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TaPA (detector + LLM planning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior approach that constructs an object list by running detectors on multiple RGB viewpoints and passes that symbolic object list plus instruction into a fine-tuned LLM to generate plans for navigation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>TaPA (uses object detectors + fine-tuned LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Collects multi-view RGB images, runs detectors to produce an object list (symbolic representation of scene), and feeds that plus the instruction to an LLM fine-tuned to generate action plans.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Long-horizon navigation planning</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Plan navigation steps in indoor scenes by reasoning over a detector-produced object list rather than raw images.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>navigation</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial+object-relational (symbolic object list)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>object detectors (vision-to-symbol) + LLM fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>LLM prompted with symbolic object list (few-shot/fine-tune)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>scene represented as an explicit object list (symbolic); LLM reasons over that textual representation to produce plans</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>planning success in navigation tasks (not numerically reported in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Mentioned as prior work showing that symbolic object lists can be used to ground LLM planning for navigation; not directly evaluated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Transforms visual perception into explicit symbolic representations (object lists) that LLMs can reason over without raw visual input.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Relies on detector quality; symbolic lists may omit spatial layout details like precise geometry or orientation unless detectors provide them.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>TaPA-style object-list approaches contrast with ReLEP's direct visual input to a vision-language model and ReLEP's fine-tuning on implicit logical relations.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Not evaluated here.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Converting vision to symbolic object lists enables LLMs to reason without direct raw sensory input, but may sacrifice fine-grained spatial information and still requires careful grounding of procedural semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Long-horizon Embodied Planning with Implicit Logical Inference and Hallucination Mitigation', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e395.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e395.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaLM-E</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaLM-E</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An embodied multimodal model that combines vision, continuous state estimation, and a large language model to generate textual plans, noted as powerful but limited in processing very intricate instructions in prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM-E</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A hybrid model that conditions an LLM on visual observations and continuous state estimates to generate text plans or policies; it does not itself execute low-level control.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Embodied planning with continuous state estimation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Use continuous state estimates plus vision and instructions to produce plans; intended for embodied reasoning but has limitations on intricate instruction processing.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step planning / instruction following</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial+procedural</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>vision input + continuous state estimation + LLM pretraining</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>conditioning LLM on fused visual and state inputs</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>continuous state vectors and natural-language plans (implicit in weights)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>text-plan quality and downstream task performance in prior works (not reported here)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Mentioned as related work; paper notes PaLM-E lacks the ability to process highly intricate instructions in some cases.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Combines perception and state estimation to ground LLM planning in continuous state.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>May lack fine-grained planning/execution details and struggles with very complex instructions without further adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared conceptually to ReLEP which uses vision-language fine-tuning plus explicit skill APIs.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Not applicable in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>PaLM-E demonstrates that conditioning language models on state+vision enables embodied planning, but ReLEP emphasizes learning implicit logical skill relations and structured API outputs to improve multi-step executability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Long-horizon Embodied Planning with Implicit Logical Inference and Hallucination Mitigation', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e395.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e395.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EmbodiedGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>EmbodiedGPT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior approach that uses an 'embodied-former' bottleneck to pass compact visual representations from videos to a language model to generate embodied plans in natural language.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>EmbodiedGPT (embodied-former + LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Trains a bottlenecked architecture (embodied-former) to present the most salient visual information to an LLM; trained on Ego4D video-derived planning examples to generate natural-language plans or explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Embodied plan generation from first-person videos</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Produce natural-language multi-step plans given visual input (egocentric video) and instructions; can explain performed plans in videos.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step planning / explanation</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>procedural+object-relational+spatial</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>video dataset (Ego4D), LLM pretraining and supervised learning</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>learned bottleneck features fed into LLM</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>compressed visual bottleneck features plus natural-language plan outputs (implicit knowledge in weights)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>plan quality and alignment with demonstrated plans (not quantified here)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Cited as work showing feasibility of video-trained LLM planning; not directly compared numerically in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Can distill important visual cues into a compact representation for LLM reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Natural-language plans may require further structuring to execute as API calls; may inherit hallucination issues if training data is noisy.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Conceptually contrasted with ReLEP which trains LVLM to output explicit skill tokens rather than freeform natural-language plans.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Bottlenecking visual features for LLM consumption is an effective route to grounding but should be combined with structured action representations to maximize executability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Long-horizon Embodied Planning with Implicit Logical Inference and Hallucination Mitigation', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e395.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e395.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>COME-robot</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>COME-robot</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recent system that queries GPT-4V for predefined plan code and reasoning in natural language to perform open-ended re-planning with a mobile robot arm.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>COME-robot (uses GPT-4V)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Uses GPT-4V as a planner that returns plan code (not just text) plus language reasoning from a first-person perspective, interacting with a mobile robot arm with a small set of low-level action functions.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Open-ended embodied re-planning</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate plan code and reasoning for closed-loop robot tasks; re-plans in response to environment changes using GPT-4V.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>re-planning / manipulation + navigation</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>procedural+spatial+object-relational</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>GPT-4V pretraining + prompts, first-person visual inputs</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>direct GPT-4V queries for plan code and natural-language reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>structured plan code plus natural-language reasoning; relies on a small predefined action set (six functions)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>task completion in mobile manipulation experiments (qualitative results reported in cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Cited as a strong example of GPT-4V-based re-planning; authors note small number of action primitives restricts task variety.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Works well when action set is small and tasks are expressible with few primitives; GPT-4V handles re-planning and open-ended reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Scales poorly as the action library grows; GPT-4V struggles to master many functions without highly relevant in-context examples or fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>COME-robot vs ReLEP: COME-robot uses GPT-4V with small action set; ReLEP emphasizes fine-tuning to scale to larger skill libraries and to mitigate hallucinations.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Not evaluated here.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Querying large vision-language models for plan code is effective for closed-loop re-planning but depends on the action set size and suffers when many diverse skills must be mastered without fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Long-horizon Embodied Planning with Implicit Logical Inference and Hallucination Mitigation', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>VILA <em>(Rating: 2)</em></li>
                <li>PaLM-E <em>(Rating: 2)</em></li>
                <li>EmbodiedGPT <em>(Rating: 2)</em></li>
                <li>TaPA <em>(Rating: 1)</em></li>
                <li>COME-robot <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-395",
    "paper_id": "paper-272832255",
    "extraction_schema_id": "extraction-schema-15",
    "extracted_data": [
        {
            "name_short": "ReLEP",
            "name_full": "Real-time Long-horizon Embodied Planning (ReLEP)",
            "brief_description": "A framework that fine-tunes a large vision-language model to decompose long-horizon embodied instructions into sequences of pre-defined skill functions, using a Skill Library, Memory module, and Robot Configuration to encode procedural, spatial, and object-relational constraints.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaVA-1.6-7B (fine-tuned)",
            "model_size": "7B",
            "model_description": "A vision-language instruction-following transformer (LLaVA-1.6-7B) fine-tuned with LoRA on a 5K + 24K real-world embodied planning dataset (images, tasks, plans). It takes raw images and instruction text as input and outputs structured sequences of skill-function calls (APIs).",
            "task_name": "Real-time long-horizon embodied planning",
            "task_description": "Given a natural-language embodied task and a current scene image, generate a sequence of pre-defined skill functions (from a skill library) to accomplish the task; run in repeated rounds where after executing the first skill, a new image is observed and a new plan is produced referring to Memory and prior plans. Evaluated in AI2-THOR simulated scenes and on real-world collected images across navigation, manipulation, inspection, delivery and other multi-step everyday tasks.",
            "task_type": "multi-step planning (navigation + object manipulation + household tasks)",
            "knowledge_type": "procedural+spatial+object-relational",
            "knowledge_source": "fine-tuning on embodied task dataset (images+task+plans) + visual input at runtime + predefined skill library + Memory + Robot Configuration prompts",
            "has_direct_sensory_input": true,
            "elicitation_method": "fine-tuning (instruction-following) and structured prompting (skill library, memory, robot configuration)",
            "knowledge_representation": "procedural knowledge represented explicitly as ordered action/skill sequences (API calls); spatial and object-relational knowledge encoded implicitly in model weights and visual feature inputs; task state encoded explicitly in Memory (finished steps, robot status); embodiment constraints provided as natural-language Robot Configuration",
            "performance_metric": "Initial Plan Success Rate (IPSR), Step-wise Success Rate (SSR), overall Success Rate (SR), Language Compliance (LC)",
            "performance_result": "Real-world offline image evaluation: IPSR 94.3%, SSR 97.7%, SR 94.3% (Table 3). In AI2-THOR simulated tasks ReLEP achieved very high IPSR/SSR/SR across the ten evaluated tasks (substantially outperforming baselines; see Table 2).",
            "success_patterns": "Successfully decomposes high-level instructions into executable multi-step skill sequences; maintains consistency across rounds by using Memory; handles embodiment constraints (avoids impossible sequences like consecutive Grasps for single-arm robots when prompted); mitigates hallucinated undefined skills after fine-tuning on implicit logical relations.",
            "failure_patterns": "Requires reliable low-level skill implementations and robust perception; small environment changes (e.g., 'getting closer to a target') may be hard to detect without Memory—if Memory disabled, model can loop on Detect/Navigation; real-world deployment dependent on underlying skill execution robustness.",
            "baseline_comparison": "Compared to VILA* and GPT-4V* baselines; in real-world offline test VILA*: IPSR 61.4%, SSR 81.1%, SR 58.6%; GPT-4V*: IPSR 82.9%, SSR 77.5%, SR 22.9%; ReLEP+GPT-4V: IPSR 94.3%, SSR 83.6%, SR 34.3% (Table 3).",
            "ablation_results": "Ablating fine-tuning (use raw LLaVA-1.6-7B zero-shot or with loosely relevant examples) greatly reduced Language Compliance and SR; removing Memory caused repeated Detect/Navigation loops and large SSR drop — Memory is necessary to mark finished steps and robot status for subsequent-round planning.",
            "key_findings": "Fine-tuning a vision-language model on data that encodes implicit logical relationships between skills shifts the model from hallucination-prone freeform planning to generating structured, executable skill sequences; procedural knowledge is most effectively handled by representing actions as explicit API-style skill functions and training the model to output those tokens; Memory and Robot Configuration supply essential state and embodiment constraints so that the model can correctly sequence skills even when visual changes are subtle or absent.",
            "uuid": "e395.0",
            "source_info": {
                "paper_title": "Long-horizon Embodied Planning with Implicit Logical Inference and Hallucination Mitigation",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Skill Library (ReLEP)",
            "name_full": "Pre-defined Skill Library (nine skills) used by ReLEP",
            "brief_description": "A curated set of nine interactive and non-interactive skill functions (e.g., Detect, Grasp, Navigate, Pull, Push, Put, Speak, EQA, Wait) that ReLEP plans over; plans are sequences of calls to these skills, which serve as the procedural action representation layer.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaVA-1.6-7B (fine-tuned in ReLEP)",
            "model_size": "7B",
            "model_description": "Not a language model per se but the action API interface that the fine-tuned LVLM is trained to output; each skill has a fixed name and parameter schema and is used as the atomic unit of procedural planning.",
            "task_name": "Action abstraction for embodied planning",
            "task_description": "Provides the atomic operations that an embodied planner must sequence to achieve tasks (e.g., Navigate(destination), Detect(object), Grasp(object), Put(object, place, side)). The planner must assemble these into multi-step plans conditioned on image and memory.",
            "task_type": "procedural abstraction for multi-step tasks",
            "knowledge_type": "procedural (primary) + object-relational (parameters) + spatial (destination/place arguments)",
            "knowledge_source": "designed API schema (explicit), training dataset pairs (image, task, plan) used to teach the LVLM how to map visual + instruction to the skill sequence",
            "has_direct_sensory_input": true,
            "elicitation_method": "output prediction of skill tokens from fine-tuned LVLM (supervised learning)",
            "knowledge_representation": "explicit symbolic API calls (skill name + structured arguments) representing procedural knowledge; arguments encode object identities, spatial targets (destination/place/side), and directions",
            "performance_metric": "Language Compliance for skill format; execution success via SSR/SR when skills are executed by environment/simulator",
            "performance_result": "When used by ReLEP, skill-format outputs achieved high Language Compliance and led to high SSR/SR (see ReLEP results). Untrained LVLMs produce freeform output and low LC.",
            "success_patterns": "Explicit skill tokens reduce hallucination and ambiguity; argument slots (place, side, direction) allow the model to represent spatial relations in the plan.",
            "failure_patterns": "If the LVLM does not understand the semantics of particular skill parameters (e.g., meaning of 'Put(..., side)'), it can produce logical understanding errors; large skill libraries increase the demand for training to avoid incorrect skill selection.",
            "baseline_comparison": "Compared with freeform natural-language plans (VILA*, raw LLaVA zero-shot), the skill-library-based outputs are more structured and executable.",
            "ablation_results": "Not ablated independently in isolation from the framework, but the paper reports that explicit skill outputs plus fine-tuning lead to higher LC and SR versus freeform outputs.",
            "key_findings": "Representing procedural knowledge as an explicit library of API-like skills and training the LVLM to output those tokens is an effective way to ground language planning into executable robot operations and reduce hallucination.",
            "uuid": "e395.1",
            "source_info": {
                "paper_title": "Long-horizon Embodied Planning with Implicit Logical Inference and Hallucination Mitigation",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Memory Module",
            "name_full": "Memory module (Finished Steps + Robot Status) in ReLEP",
            "brief_description": "A stored state that records past plans, finished steps, and robot status so the planner can make consistent subsequent-round plans even when visual changes are subtle or absent.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaVA-1.6-7B (fine-tuned in ReLEP)",
            "model_size": "7B",
            "model_description": "A non-learned storage/prompting module containing previously generated plans, a list of finished steps, and a natural-language description of robot status; presented to the LVLM at subsequent planning rounds to condition new plan outputs.",
            "task_name": "Stateful multi-round planning",
            "task_description": "Enables the LVLM to produce consistent plans across repeated observations by explicitly informing it which steps have been executed and the robot's current configuration/state.",
            "task_type": "procedural state tracking for multi-step tasks",
            "knowledge_type": "procedural (state/history) + object-relational (which objects have been manipulated) + spatial (robot position/status indirectly)",
            "knowledge_source": "runtime execution records (finished steps) and robot status fed as natural-language prompt context",
            "has_direct_sensory_input": false,
            "elicitation_method": "prompt augmentation (past plan text + finished steps + status included in input to LVLM)",
            "knowledge_representation": "explicit natural-language / structured memory of executed actions and robot configuration; not learned representation but used as conditioning context",
            "performance_metric": "Step-wise Success Rate (SSR), loop-avoidance in subsequent-round planning",
            "performance_result": "Removing Memory produced repeated Detect/Navigation loops and lower SSR; with Memory enabled ReLEP had substantially higher SSR (see ablation discussion and Table 4 summary).",
            "success_patterns": "Enables the model to advance to next steps when image changes are minimal (e.g., navigation that only changes viewpoint), preserves plan consistency across rounds.",
            "failure_patterns": "Without Memory, planner often cannot infer that a non-visual skill finished and gets stuck repeating the same skill; Memory must be accurate and up-to-date or it can mislead planning.",
            "baseline_comparison": "ReLEP with Memory vs ReLEP without Memory: Memory variant avoids the repeated-skill failure modes observed in GPT-4V* and Memory-ablated ReLEP.",
            "ablation_results": "Ablation (remove Memory) -&gt; fails to predict new plans after Detect/Navigate, lower SSR, stuck in loops; explicit provision of finished steps is critical.",
            "key_findings": "Explicitly storing and prompting finished steps and robot status is necessary to resolve perceptual ambiguity and to maintain procedural consistency in multi-round planning; visual input alone is sometimes insufficient to indicate that a step completed.",
            "uuid": "e395.2",
            "source_info": {
                "paper_title": "Long-horizon Embodied Planning with Implicit Logical Inference and Hallucination Mitigation",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Robot Configuration Module",
            "name_full": "Robot Configuration prompting module",
            "brief_description": "A natural-language description of the robot's embodiment and capabilities (e.g., single-arm vs humanoid) used to constrain planning and prevent physically impossible or inappropriate skill sequences.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaVA-1.6-7B (fine-tuned in ReLEP)",
            "model_size": "7B",
            "model_description": "A prompt-time module providing natural-language constraints about the robot (which skills it supports, degrees of freedom, number of arms), used to bias the LVLM's plan generation.",
            "task_name": "Embodiment-aware planning",
            "task_description": "Ensure generated skill sequences are realizable given the robot's physical capabilities (e.g., avoid two consecutive Grasps for single-arm robot).",
            "task_type": "procedural + embodiment constraint enforcement",
            "knowledge_type": "procedural + object-relational (which tools/objects robot can interact with) + spatial (capabilities affecting spatial reach/motion)",
            "knowledge_source": "explicit natural-language robot description provided as prompt context",
            "has_direct_sensory_input": false,
            "elicitation_method": "prompt-conditioning (Robot Configuration included in LVLM input)",
            "knowledge_representation": "explicit natural-language constraints describing embodiment/capabilities; acts as an additional symbolic context for planning",
            "performance_metric": "Plan executability and SR under different embodiment constraints",
            "performance_result": "Using Robot Configuration helps avoid logically impossible plans (e.g., consecutive Grasps) and improves executability; reported qualitatively in experiments (no numerical per-robot ablation shown).",
            "success_patterns": "Prevents generation of skill sequences incompatible with the robot (increases realism of plans).",
            "failure_patterns": "If Robot Configuration is absent or underspecified, LVLM may propose infeasible sequences for a given embodiment.",
            "baseline_comparison": "ReLEP with Robot Configuration vs no-configuration (not explicitly ablated), but authors note its necessity for multi-embodiment generality.",
            "ablation_results": "Not separately ablated in main results, but authors claim robot-configuration prompting is required to adapt plans across robot types.",
            "key_findings": "Explicit, natural-language embodiment descriptions provided as prompts are an effective way to impose physical constraints on high-level planning generated by vision-language models.",
            "uuid": "e395.3",
            "source_info": {
                "paper_title": "Long-horizon Embodied Planning with Implicit Logical Inference and Hallucination Mitigation",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "GPT-4V",
            "name_full": "GPT-4V (vision-enabled GPT-4)",
            "brief_description": "A powerful commercial large vision-language model used as a baseline brain for embodied planning; can generate plausible plans with highly relevant in-context examples but is prone to logical errors and hallucinations without them.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4V",
            "model_size": null,
            "model_description": "A multimodal model (OpenAI's GPT-4 with vision) that accepts images and text and can output natural-language plans or structured plan code when prompted; used in experiments both zero-shot and with in-context examples.",
            "task_name": "Long-horizon embodied planning (baseline)",
            "task_description": "Generate step-by-step plans (in natural language or predefined plan format) from an image + task instruction; tested with varying in-context example relevance and as the planning brain in a ReLEP-like framework.",
            "task_type": "multi-step planning (navigation + manipulation + inspection)",
            "knowledge_type": "procedural+spatial+object-relational",
            "knowledge_source": "pre-training on large multimodal corpora + in-context examples (few-shot) when provided",
            "has_direct_sensory_input": true,
            "elicitation_method": "in-context prompting (zero-shot, loosely relevant, highly relevant examples); few-shot; direct image input",
            "knowledge_representation": "primarily natural-language reasoning and plan descriptions; can be prompted to return structured plan code or API-like calls but often outputs freeform text unless guided by examples",
            "performance_metric": "Plausible Success Rate (PSR), Executable Success Rate (ESR), Language Compliance (LC), IPSR/SSR/SR in downstream tests",
            "performance_result": "In real-world offline test (Table 3): IPSR 82.9%, SSR 77.5%, SR 22.9% when using highly relevant in-context examples for first-round planning. In in-context experiments, GPT-4V with highly relevant examples performs well; with loosely relevant examples it produces many logical errors and hallucinations.",
            "success_patterns": "When given highly relevant, carefully constructed in-context examples, GPT-4V can produce executable, logically consistent multi-step plans and reason about spatial layout and object attributes from visual input.",
            "failure_patterns": "Without highly relevant examples, it produces four major error types: logical understanding errors (misinterpreting skill semantics), skill combination errors (missing detection before navigation), logical planning errors (incorrect manipulation actions/directions), and hallucinations (fabricating undefined skills). It also gets stuck repeating Detect/Navigation in subsequent rounds when it cannot confirm step completion from images.",
            "baseline_comparison": "Compared to ReLEP: GPT-4V* (used with ReLEP framework and highly relevant first-round examples) has good IPSR but much lower overall SR due to subsequent-round failures (Table 3); VILA* and GPT-4V* perform worse than fine-tuned ReLEP overall.",
            "ablation_results": "Authors show that the presence/absence and relevance of in-context examples strongly affects GPT-4V performance; GPT-4V requires highly relevant, task-specific examples to avoid logical errors.",
            "key_findings": "Large vision-language models like GPT-4V can perform spatial and object-relational reasoning from images, but when operating primarily via in-context examples (without fine-tuning on implicit logical relationships and without explicit memory/embodiment prompts) they are prone to hallucination and logical planning failures; they may also be unable to reliably infer step completion from images alone, causing loops in multi-round planning.",
            "uuid": "e395.4",
            "source_info": {
                "paper_title": "Long-horizon Embodied Planning with Implicit Logical Inference and Hallucination Mitigation",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "LLaVA-1.6-7B",
            "name_full": "LLaVA-1.6 (7B) vision-language model",
            "brief_description": "The base vision-language instruction-following model used as the ReLEP brain before and after fine-tuning; zero-shot/loosely prompted versions fail to output structured skill sequences whereas the fine-tuned variant can.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaVA-1.6-7B",
            "model_size": "7B",
            "model_description": "A publicly released instruction-following vision-language transformer; authors fine-tune a checkpoint of this model with LoRA on their embodied planning dataset to produce ReLEP.",
            "task_name": "Embodied planning baseline and fine-tuning target",
            "task_description": "Zero-shot or loosely prompted LLaVA produces freeform plans with low Language Compliance; after fine-tuning on the structured dataset it outputs skill-function sequences with high LC and SR.",
            "task_type": "multi-step planning",
            "knowledge_type": "procedural+spatial+object-relational",
            "knowledge_source": "pre-training + fine-tuning on ReLEP dataset",
            "has_direct_sensory_input": true,
            "elicitation_method": "zero-shot / few-shot prompting (baseline) and supervised fine-tuning (ReLEP)",
            "knowledge_representation": "visual features fed to transformer + learned implicit knowledge in weights; after fine-tuning, the model directly outputs symbolic skill sequences",
            "performance_metric": "LC, SSR, SR",
            "performance_result": "Zero-shot LLaVA produces freeform plans with zero LC; with loosely relevant examples it performs worse than GPT-4V and has more logical errors; fine-tuning (ReLEP) markedly improves LC and SR (see ablation).",
            "success_patterns": "After fine-tuning, LLaVA can generate structured, executable plans and learn implicit logical inference between skills.",
            "failure_patterns": "Without fine-tuning it cannot produce the required structured skill-token format and exhibits hallucinations and logical errors.",
            "baseline_comparison": "LLaVA-1.6-7B (zero-shot) and with loosely relevant examples perform substantially worse than the fine-tuned ReLEP model.",
            "ablation_results": "Fine-tuning LLaVA on implicit logical-relationship-encoded dataset is necessary to acquire multi-round planning and to mitigate hallucinations.",
            "key_findings": "Fine-tuning a vision-language model on data that encodes implicit logical relationships is effective at teaching the model to map visual inputs + instructions to executable procedural plans represented as skill sequences; without this fine-tuning, the vision-language model struggles to follow the required structured output format.",
            "uuid": "e395.5",
            "source_info": {
                "paper_title": "Long-horizon Embodied Planning with Implicit Logical Inference and Hallucination Mitigation",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "VILA",
            "name_full": "VILA (baseline leveraging GPT-4V)",
            "brief_description": "A baseline method that leverages GPT-4V to generate step-by-step plans and performs closed-loop embodied tasks; it reasons about spatial layout and object attributes but tends to output freeform natural language and sometimes ignores navigation.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "VILA (uses GPT-4V as planner)",
            "model_size": null,
            "model_description": "Framework that uses GPT-4V to generate executable plans from images; in these experiments VILA was equipped with the same predefined skill library as ReLEP but often returned long natural-language outputs where only a small portion was a plan.",
            "task_name": "Long-horizon embodied planning (baseline)",
            "task_description": "Use GPT-4V to reason about scenes and plan steps for navigation/manipulation tasks; evaluated in AI2-THOR and real-world images.",
            "task_type": "multi-step planning (navigation + manipulation)",
            "knowledge_type": "spatial+object-relational+procedural",
            "knowledge_source": "GPT-4V pretraining and prompts; detectors/object lists when used",
            "has_direct_sensory_input": true,
            "elicitation_method": "prompting GPT-4V with image + task",
            "knowledge_representation": "natural-language reasoning about scene layout and object attributes; sometimes uses object detectors to produce object lists",
            "performance_metric": "IPSR, SSR, SR",
            "performance_result": "In real-world offline evaluation (Table 3) VILA*: IPSR 61.4%, SSR 81.1%, SR 58.6%; in simulated tests VILA* tended to overthink and produce long natural-language output and sometimes omitted navigation.",
            "success_patterns": "Can reason about object attributes and spatial layout from visual input; can produce plausible textual plans.",
            "failure_patterns": "Outputs lengthy freeform text with limited structured plan content; often ignores navigation and manipulates objects without approaching; lower IPSR and SR vs ReLEP.",
            "baseline_comparison": "VILA* vs ReLEP: ReLEP substantially outperformed VILA* across IPSR/SSR/SR metrics.",
            "ablation_results": "Not ablated in this paper; reported behavioral failure modes indicate need for structured skill outputs and memory to improve multi-round consistency.",
            "key_findings": "Using a general-purpose vision-language model like GPT-4V without fine-tuning or structured output constraints can lead to verbose plans with poor executability; structured skill APIs and memory resolve many of these issues.",
            "uuid": "e395.6",
            "source_info": {
                "paper_title": "Long-horizon Embodied Planning with Implicit Logical Inference and Hallucination Mitigation",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "TaPA",
            "name_full": "TaPA (detector + LLM planning)",
            "brief_description": "A prior approach that constructs an object list by running detectors on multiple RGB viewpoints and passes that symbolic object list plus instruction into a fine-tuned LLM to generate plans for navigation tasks.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "TaPA (uses object detectors + fine-tuned LLM)",
            "model_size": null,
            "model_description": "Collects multi-view RGB images, runs detectors to produce an object list (symbolic representation of scene), and feeds that plus the instruction to an LLM fine-tuned to generate action plans.",
            "task_name": "Long-horizon navigation planning",
            "task_description": "Plan navigation steps in indoor scenes by reasoning over a detector-produced object list rather than raw images.",
            "task_type": "navigation",
            "knowledge_type": "spatial+object-relational (symbolic object list)",
            "knowledge_source": "object detectors (vision-to-symbol) + LLM fine-tuning",
            "has_direct_sensory_input": false,
            "elicitation_method": "LLM prompted with symbolic object list (few-shot/fine-tune)",
            "knowledge_representation": "scene represented as an explicit object list (symbolic); LLM reasons over that textual representation to produce plans",
            "performance_metric": "planning success in navigation tasks (not numerically reported in this paper)",
            "performance_result": "Mentioned as prior work showing that symbolic object lists can be used to ground LLM planning for navigation; not directly evaluated in this paper.",
            "success_patterns": "Transforms visual perception into explicit symbolic representations (object lists) that LLMs can reason over without raw visual input.",
            "failure_patterns": "Relies on detector quality; symbolic lists may omit spatial layout details like precise geometry or orientation unless detectors provide them.",
            "baseline_comparison": "TaPA-style object-list approaches contrast with ReLEP's direct visual input to a vision-language model and ReLEP's fine-tuning on implicit logical relations.",
            "ablation_results": "Not evaluated here.",
            "key_findings": "Converting vision to symbolic object lists enables LLMs to reason without direct raw sensory input, but may sacrifice fine-grained spatial information and still requires careful grounding of procedural semantics.",
            "uuid": "e395.7",
            "source_info": {
                "paper_title": "Long-horizon Embodied Planning with Implicit Logical Inference and Hallucination Mitigation",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "PaLM-E",
            "name_full": "PaLM-E",
            "brief_description": "An embodied multimodal model that combines vision, continuous state estimation, and a large language model to generate textual plans, noted as powerful but limited in processing very intricate instructions in prior work.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "PaLM-E",
            "model_size": null,
            "model_description": "A hybrid model that conditions an LLM on visual observations and continuous state estimates to generate text plans or policies; it does not itself execute low-level control.",
            "task_name": "Embodied planning with continuous state estimation",
            "task_description": "Use continuous state estimates plus vision and instructions to produce plans; intended for embodied reasoning but has limitations on intricate instruction processing.",
            "task_type": "multi-step planning / instruction following",
            "knowledge_type": "spatial+procedural",
            "knowledge_source": "vision input + continuous state estimation + LLM pretraining",
            "has_direct_sensory_input": true,
            "elicitation_method": "conditioning LLM on fused visual and state inputs",
            "knowledge_representation": "continuous state vectors and natural-language plans (implicit in weights)",
            "performance_metric": "text-plan quality and downstream task performance in prior works (not reported here)",
            "performance_result": "Mentioned as related work; paper notes PaLM-E lacks the ability to process highly intricate instructions in some cases.",
            "success_patterns": "Combines perception and state estimation to ground LLM planning in continuous state.",
            "failure_patterns": "May lack fine-grained planning/execution details and struggles with very complex instructions without further adaptation.",
            "baseline_comparison": "Compared conceptually to ReLEP which uses vision-language fine-tuning plus explicit skill APIs.",
            "ablation_results": "Not applicable in this paper.",
            "key_findings": "PaLM-E demonstrates that conditioning language models on state+vision enables embodied planning, but ReLEP emphasizes learning implicit logical skill relations and structured API outputs to improve multi-step executability.",
            "uuid": "e395.8",
            "source_info": {
                "paper_title": "Long-horizon Embodied Planning with Implicit Logical Inference and Hallucination Mitigation",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "EmbodiedGPT",
            "name_full": "EmbodiedGPT",
            "brief_description": "A prior approach that uses an 'embodied-former' bottleneck to pass compact visual representations from videos to a language model to generate embodied plans in natural language.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "EmbodiedGPT (embodied-former + LLM)",
            "model_size": null,
            "model_description": "Trains a bottlenecked architecture (embodied-former) to present the most salient visual information to an LLM; trained on Ego4D video-derived planning examples to generate natural-language plans or explanations.",
            "task_name": "Embodied plan generation from first-person videos",
            "task_description": "Produce natural-language multi-step plans given visual input (egocentric video) and instructions; can explain performed plans in videos.",
            "task_type": "multi-step planning / explanation",
            "knowledge_type": "procedural+object-relational+spatial",
            "knowledge_source": "video dataset (Ego4D), LLM pretraining and supervised learning",
            "has_direct_sensory_input": true,
            "elicitation_method": "learned bottleneck features fed into LLM",
            "knowledge_representation": "compressed visual bottleneck features plus natural-language plan outputs (implicit knowledge in weights)",
            "performance_metric": "plan quality and alignment with demonstrated plans (not quantified here)",
            "performance_result": "Cited as work showing feasibility of video-trained LLM planning; not directly compared numerically in this paper.",
            "success_patterns": "Can distill important visual cues into a compact representation for LLM reasoning.",
            "failure_patterns": "Natural-language plans may require further structuring to execute as API calls; may inherit hallucination issues if training data is noisy.",
            "baseline_comparison": "Conceptually contrasted with ReLEP which trains LVLM to output explicit skill tokens rather than freeform natural-language plans.",
            "ablation_results": "Not provided here.",
            "key_findings": "Bottlenecking visual features for LLM consumption is an effective route to grounding but should be combined with structured action representations to maximize executability.",
            "uuid": "e395.9",
            "source_info": {
                "paper_title": "Long-horizon Embodied Planning with Implicit Logical Inference and Hallucination Mitigation",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "COME-robot",
            "name_full": "COME-robot",
            "brief_description": "A recent system that queries GPT-4V for predefined plan code and reasoning in natural language to perform open-ended re-planning with a mobile robot arm.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "COME-robot (uses GPT-4V)",
            "model_size": null,
            "model_description": "Uses GPT-4V as a planner that returns plan code (not just text) plus language reasoning from a first-person perspective, interacting with a mobile robot arm with a small set of low-level action functions.",
            "task_name": "Open-ended embodied re-planning",
            "task_description": "Generate plan code and reasoning for closed-loop robot tasks; re-plans in response to environment changes using GPT-4V.",
            "task_type": "re-planning / manipulation + navigation",
            "knowledge_type": "procedural+spatial+object-relational",
            "knowledge_source": "GPT-4V pretraining + prompts, first-person visual inputs",
            "has_direct_sensory_input": true,
            "elicitation_method": "direct GPT-4V queries for plan code and natural-language reasoning",
            "knowledge_representation": "structured plan code plus natural-language reasoning; relies on a small predefined action set (six functions)",
            "performance_metric": "task completion in mobile manipulation experiments (qualitative results reported in cited work)",
            "performance_result": "Cited as a strong example of GPT-4V-based re-planning; authors note small number of action primitives restricts task variety.",
            "success_patterns": "Works well when action set is small and tasks are expressible with few primitives; GPT-4V handles re-planning and open-ended reasoning.",
            "failure_patterns": "Scales poorly as the action library grows; GPT-4V struggles to master many functions without highly relevant in-context examples or fine-tuning.",
            "baseline_comparison": "COME-robot vs ReLEP: COME-robot uses GPT-4V with small action set; ReLEP emphasizes fine-tuning to scale to larger skill libraries and to mitigate hallucinations.",
            "ablation_results": "Not evaluated here.",
            "key_findings": "Querying large vision-language models for plan code is effective for closed-loop re-planning but depends on the action set size and suffers when many diverse skills must be mastered without fine-tuning.",
            "uuid": "e395.10",
            "source_info": {
                "paper_title": "Long-horizon Embodied Planning with Implicit Logical Inference and Hallucination Mitigation",
                "publication_date_yy_mm": "2024-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "VILA",
            "rating": 2
        },
        {
            "paper_title": "PaLM-E",
            "rating": 2
        },
        {
            "paper_title": "EmbodiedGPT",
            "rating": 2,
            "sanitized_title": "embodiedgpt"
        },
        {
            "paper_title": "TaPA",
            "rating": 1
        },
        {
            "paper_title": "COME-robot",
            "rating": 1,
            "sanitized_title": "comerobot"
        }
    ],
    "cost": 0.023244499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Long-horizon Embodied Planning with Implicit Logical Inference and Hallucination Mitigation
13 Mar 2025</p>
<p>Siyuan Liu 
Peking University</p>
<p>Jiawei Du 
Peking University</p>
<p>Sicheng Xiang 
Peking University</p>
<p>Zibo Wang 
Peking University</p>
<p>Dingsheng Luo dsluo@pku.edu.cn 
Peking University</p>
<p>Long-horizon Embodied Planning with Implicit Logical Inference and Hallucination Mitigation
13 Mar 2025AC78389F2A1134C4A402DFA8AD6B45ECarXiv:2409.15658v2[cs.RO]
Long-horizon embodied planning underpins embodied AI.To accomplish long-horizon tasks, one of the most feasible ways is to decompose abstract instructions into a sequence of actionable steps.Foundation models still face logical errors and hallucinations in long-horizon planning, unless provided with highly relevant examples to the tasks.However, providing highly relevant examples for any random task is unpractical.Therefore, we present ReLEP, a novel framework for Real-time Long-horizon Embodied Planning.ReLEP can complete a wide range of long-horizon tasks without in-context examples by learning implicit logical inference through fine-tuning.The fine-tuned large vision-language model formulates plans as sequences of skill functions.These functions are selected from a carefully designed skill library.ReLEP is also equipped with a Memory module for plan and status recall, and a Robot Configuration module for versatility across robot types.In addition, we propose a data generation pipeline to tackle dataset scarcity.When constructing the dataset, we considered the implicit logical relationships, enabling the model to learn implicit logical relationships and dispel hallucinations.Through comprehensive evaluations across various long-horizon tasks, ReLEP demonstrates high success rates and compliance to execution even on unseen tasks and outperforms state-of-the-art baseline methods.</p>
<p>Introduction</p>
<p>The significant emergent abilities of foundation models to comprehend semantic information and reason make them good candidates for use as robot brains.However, even with the help of foundation models, embodied tasks are still difficult to carry out.</p>
<p>Foundation models use language as a medium to interact with the environment.It is hard to ground tasks in natural language into robot action space.There are generally three ways to achieve the grounding of embodied tasks.The first is to fine-tune the foundation model to directly output control signals in text form.This method can only be used in simple control like primitive autonomous driving [Xu et al., 2023], and the performance is not guaranteed.The second approach is vision-language-action models [Brohan et al., 2023;Belkhale et al., 2024].It assigns action controls to rarely used tokens or integer tokens in the original foundation models and names them action tokens, then trains these models to predict action tokens given language input.The third way is more cost-efficient.It uses pre-defined skills as APIs, and let the foundation models predict a sequence of skills, then execute these skills in succession [Ahn et al., 2022;Huang et al., 2023].</p>
<p>Another difficulty lies in task planning.Tasks such as "bring me a bottle of water" can be difficult to carry out in an end-to-end manner, as they often require multiple steps and contextual understanding.These long-horizon tasks usually need to be broken down into multiple short-term lowlevel tasks in the first place.Previous works show that advanced foundation models are already capable of decomposing simple long-chain tasks [Hu et al., 2023;Zhi et al., 2024].However, they can currently only be applied to a small skill set, which limits them to completing a narrow array of tasks.When given more complex skills, the models cannot fully understand the logical relationships between skills through limited examples, leading to logical errors in planning and hallucinations of using self-fabricated skills.They require highly relevant task-specific in-context examples to generate tractable plans for later execution (see Section 4.1).The limited comprehension of foundation models regarding the skills and their fluid expressions is the underlying cause of the aforementioned issues.</p>
<p>Since creating highly relevant task-specific examples is impractical, we seek to address the problem through model fine-tuning.To this end, we require a dataset that encompasses the implicit logical relationships between skills.Most embodied planning methods are carried out in simulated environments [Shridhar et al., 2020;Huang et al., 2023;Skreta et al., 2024] due to the lack of real-world data.Ego-COT [Mu et al., 2024] is one of the few real-world datasets for embodied planning, but the skills in the dataset are freeform and difficult to reuse.</p>
<p>In this work, we introduce ReLEP, a universal frame- work for real-time long-horizon embodied planning via large vision-language models.An overview of ReLEP is shown in Figure 1.The brain of ReLEP is a fine-tuned large visionlanguage model which is aligned to decompose long-horizon tasks into a sequence of pre-defined skill functions.</p>
<p>We present a pipeline to construct our dataset with realworld images using GPT-4V [OpenAI, 2023].In constructing the dataset, the logical relationships between the skills are considered, such as the logical sequence of skills, skill parameters, robot embodiment, and hallucinations.In this way, the model can leverage the learned implicit logical inference to better understand the logical relationships between skills and their usage, while reducing hallucinations.</p>
<p>ReLEP has a carefully constructed skill library, including nine interactive and non-interactive skills.By combining these skills, ReLEP can accomplish dozens of long-horizon tasks, such as manipulation tasks, pick-and-place tasks, interaction tasks, navigation tasks, inspection tasks, delivery tasks, and more.</p>
<p>To maintain consistency in multi-step planning, ReLEP introduces a Memory Module to store past information and the robot's state.A Robot Configuration Module is used to prompt the model to account for the logical issues introduced by the robot's embodiment in planning.With the help of these modules, ReLEP can be applied to robots with different embodiments and skill libraries.</p>
<p>Comprehensive evaluations have been conducted to test the performance of foundation models and ReLEP.We first conducted an in-context experiment to demonstrate the reliance of foundation models on highly relevant examples in longhorizon planning tasks.Then, we compared ReLEP with the state-of-the-art method VILA [Hu et al., 2023] and model GPT-4V and as baselines.In the simulation environment AI2-THOR [Kolve et al., 2017], real-time comparative experiments were carried out on ten long-horizon everyday tasks, including both tasks within and outside of the training set.In addition, we collected images from the real world as input to test whether ReLEP also exhibits strong planning capabilities in real-world scenarios.To validate the effectiveness of the key modules, we also performed ablation studies.The results indicate that ReLEP outperforms all baseline methods in dif-ferent ways and demonstrate the effectiveness of the proposed framework.</p>
<p>The main contributions of our work are as follows:</p>
<p>• We discovered that foundation models using in-context learning methods encounter logical errors and hallucinations in long-horizon embodied planning.We point out that fine-tuning the model with implicit logical relationships can effectively solve these issues.Additionally, we fine-tuned a large vision-language model with implicit logical inference and hallucination mitigation.</p>
<p>• A real-time long-horizon embodied planning framework named ReLEP, which is able to complete a wide range of long-horizon embodied tasks with implicit logical inference and hallucination mitigation.The framework can be adopted for robots with different embodiments and skill libraries.</p>
<p>• A pipeline to generate real-world embodied planning dataset with the help of GPT-4V, and a 5K + 24K realworld embodied planning dataset constructed with implicit logical relationships.This fills a gap in real-world data for long-horizon embodied tasks and is of great significance for the development of embodied agents in real-world environments.</p>
<p>2 Related Work</p>
<p>Large Vision-Language Models</p>
<p>The outstanding capabilities of large language models inspire researchers to build large multimodal models for multimodal instruction-following tasks.Large vision language models such as BLIP-2 [Li et al., 2023], LLaVA [Liu et al., 2024c], InstructBLIP [Dai et al., 2023], MiniGPT-v2 [Chen et al., 2023], Qwen-VL [Bai et al., 2023], LLaVA-1.5 [Liu et al., 2024a] and GPT-4V [OpenAI, 2023] have shown their capabilities in classification, detection, segmentation, captioning, and visual question answering.One of the most exciting discoveries is the possibility that large vision language models can perform embodied tasks like embodied question answering and embodied planning.</p>
<p>Long-horizon Embodied Planning via Large Vision-Language Models</p>
<p>TaPA [Wu et al., 2023] collects multiple RGB images in simulator scenes from different viewpoints, and utilizes detectors to detect a list of objects.This object list together with task instructions in natural language is passed into a fine-tuned large language model to generate a sequence of task plans.TaPA can solve long-horizon tasks that require robot navigation.</p>
<p>PaLM-E [Driess et al., 2023] combines vision, continuous state estimation, and natural language instructions as input to generate a text plan, using a classic ViT [Dosovitskiy et al., 2020;Dehghani et al., 2023] and a large language model framework.The model itself cannot execute low-level control.Furthermore, it lacks the ability to process intricate instructions.</p>
<p>EmbodiedGPT [Mu et al., 2024] introduces a new model architecture embodied-former as a bottleneck to pass the most relevant visual information to the language model.Embod-iedGPT is trained with embodied plannings generated by ChatGPT based on videos from the Ego4D dataset [Grauman et al., 2022].It can generate embodied plans in natural language with an input image, or explain what embodied plan is performed by a robot in a given video.</p>
<p>With abundant training and data resources, the Tech Giants have launched powerful large vision language models such as GPT-4V.These models can achieve successful planning for simple tasks without further training.With appropriate prompting and few-shot examples, they are able to complete real-world embodied tasks planning.</p>
<p>VILA [Hu et al., 2023] leverages GPT-4V to generate an executable step-by-step plan, and performs closed-loop embodied tasks with a fixed robot arm.VILA can perform reasoning related to spatial layout and object attributes on visual input.</p>
<p>More recently, COME-robot [Zhi et al., 2024] utilizes GPT-4V for open-ended reasoning and re-planning in realworld environments.Instead of generating plans in text, it directly queries GPT-4V for predefined plan code, as well as the reasoning process in natural language from a first-person perspective.Experiments are carried out with a mobile robot arm.</p>
<p>These works use GPT-4V as brain to generate plans in predefined form, then execute them with low-level action APIs.These actions are often limited and simple.For example, COME-robot has only six action functions, including navigate, grasp, place, and functions for exploration.Therefore, the types of tasks they can complete are limited.This framework works fine for a small library of action functions.However, when the library scales up, GPT-4V may find it difficult to master the usage of these functions and fail to generate appropriate plans without highly relevant task-specific in-context examples.</p>
<p>Methodology</p>
<p>We first introduce the formulation of the real-time embodied planning problem in Section 3.1.The details of our framework are discussed in Section 3.2.Finally, we present the pipeline we used to generate real-world embodied task planning dataset in Section 3.3.</p>
<p>Problem Formulation</p>
<p>Long-horizon Embodied Planning Given an embodied task E in natural language and an image of the scene I, the agent M decomposes E into a sequence of skill functions P .These skill functions belong to a set of pre-defined skill library Π = {s 1 , s 2 , ..., s n }.By executing P in succession, the agent should be able to complete task E.
P = M (E, I, Π)(1)
Real-time Embodied Planning At time t 0 , the agent is given an embodied task E in natural language and an image of the current scene I 0 .Then the agent M decomposes E into a sequence of skill functions P 0 .At this point, the first round of embodied task planning is complete.After the robot executes the first skill function of P 0 by generating a trajectory τ to control the robot or performing a non-interactive function, the environment changes and another image of the scene I 1 at time t 1 is given.The agent then decomposes E into another sequence P 1 , referring to P 0 .
P i = M (E, I i , Π, h i−1 )(2)
where P i is the plan generated by the agent and I i is the image of the scene at time t i .h i−1 denotes the past information accumulated up to time t i .</p>
<p>ReLEP</p>
<p>The real-time long-horizon embodied planning framework consists of five parts: a large vision-language model for planning, three storage modules for prompting, and an execution module for interacting.At time t 0 an embodied task E and the initial image I 0 are passed into the framework.The large vision language model generates its initial plan P 0 with guidance in the Skill Library and the Robot Configuration.Then, the execution module performs the first step of the plan by calling the corresponding skill function.Finally, P 0 is recorded in the Memory module as well as the finished steps and robot status.</p>
<p>For the upcoming time stamp t i , the same task E and the current image I i are passed into the framework.This time, the model refers to all three modules to make P i .Then the first step of P i is executed and the information is recorded as in the first round.</p>
<p>The loop will break at time t i+1 if P i only contains a Done state.</p>
<p>The pseudo code of the entire process is shown in Algorithm 1 for better understanding.</p>
<p>Large Vision-Language Model as Brain</p>
<p>The center of the framework is a large vision-language model that predicts plans based on information from other modules.Compared to large language models in embodied tasks, large vision language models take visual input directly instead of converting it into texts and then passing it into large language models [Huang et al., 2022;Song et al., 2023;Liang et al., 2023;Wu et al., 2024].This helps the model see the scene Algorithm 1 ReLEP Require: Initial visual image I 0 , an embodied task instruction E and a skill library Π 1: Let t = 0,
P 0 =[], F =[] 2: P 0 =LVLM((E, Π), I 0 ) 3: while P t [0] ̸ ="Done" do 4:
Execute skill Π(P t [0]) 5:
Add P t [0] to F 6: t = t + 1 7:
I t =GetCurrentImage()
8:
P t =LVLM((E, Π, P t−1 , F ), I t ) 9: end while itself, avoiding information loss and misunderstanding [Hu et al., 2023].</p>
<p>We choose to fine-tune the instruction-following model LLaVA-1.6-7B[Liu et al., 2024b].We fine-tuned the model from a checkpoint with LoRA [Hu et al., 2021].We first trained the model for one epoch using our 5K first-round planning data.In order to acquire the subsequent-round planning ability, we continued to train the model for two epochs using 5K first-round + 5K subsequent-round data and 5K first-round + 10K subsequent-round data respectively.The subsequent-round data for each epoch were randomly selected from the full 24K subsequent-round data.</p>
<p>By training with data containing implicit logical relationships, the model can acquire implicit logical inference capabilities and effectively reduce hallucinations.</p>
<p>Skill Library</p>
<p>The skill library is one of the most important parts of our framework.It functions as the limbs of the embodied agent to interact with the environment.</p>
<p>We have carefully designed nine skills that can be combined to complete a wide range of tasks in daily life.Details of these skills are listed in Appendix.</p>
<p>Furthermore, we designed two states at the end of each plan: Done and Pending.Done state indicates that the task is completed, while Pending means that the task is not completed and the planning can only continue after the status of a certain step is confirmed.If a plan ends with Done, it means that the task can be completed according to the information collected so far.Otherwise, it will end with Pending.</p>
<p>Memory</p>
<p>If the agent does not consider previous plans, the plans made at different time will show inconsistency.Therefore, the agent should refer to previous plans in its Memory in subsequent rounds of planning.More importantly, we found that it is difficult for the agent to determine whether a step is finished according to the image (see Section 4.3).Thus, when a step is executed, we add it to the Finished Steps in Memory module to ask the agent to proceed to the next step.In addition, the agent may forget its status during a long-chain mission.Inferring the implicit status from the plan is not easy for the agent.To this end, the Memory module should contain the status of the robot.</p>
<p>Robot Configuration</p>
<p>Robots with different embodiments may not be able to share the same skill library.For instance, a quadruped robot dog cannot perform Grasp since it has no robot arm.In terms of planning, a single-armed robot cannot perform two Grasps in a row, compared to a humanoid robot.</p>
<p>With this in mind, we introduce the Robot Configuration module, which uses natural language to describe the robot configuration, prompting the agent to refer to this module while planning.</p>
<p>Dataset Acquisition</p>
<p>The dataset we used for fine-tuning consists of two parts: 5K first-round planning data with past information and 24K subsequent-round data without.</p>
<p>The pipeline to generate the embodied planning dataset shown in Appendix takes two steps: using images to generate tasks, then using the paired image and task to generate the plan.In the end, we will obtain a set of triplets (image, task, plan) then re-form them into dialogues of the first round and subsequent rounds.</p>
<p>Task Generation</p>
<p>We first collected a set of indoor object detection datasets [Jaiinsom, 2022;Kipuw, 2023] from the Internet.Object detection has a large foundation of image data, and images from the indoor object detection dataset are often more closely related to embodied tasks.Then we manually filtered out the images whose scenes are not suitable for generating embodied tasks.After obtaining the filtered images, we queried GPT-4V to generate five tasks that a robot may perform on the scene of those images.Sometimes, GPT-4V may hallucinate and generate tasks related to objects that are not in the scene.We then modified or deleted these tasks on the basis of hallucination and feasibility.</p>
<p>Plan Generation</p>
<p>With the paired images and tasks that we generated from the first step, we constructed a simplified prompt for embodied planning to query GPT-4V to create a plan in a specified format.However, GPT-4V sometimes makes logical errors or it may hallucinate and use undefined skills.Finally, we manually modified unreasonable plans according to the implicit logical relationship between each skill, as well as the physical constraints of the robot embodiment.</p>
<p>Experiments</p>
<p>We first tested GPT-4V with in-context examples of varying relevance in long-horizon embodied planing in Section 4.1.Then we conducted comparative experiments in a simulated environment and with collected real-world images in Section 4.2.Finally, we conducted ablation experiments on ReLEP's model and its Memory module in Section 4.3.</p>
<p>Can Foundation Models Master the Usage of Skills?</p>
<p>To test how well foundation models master the use of skills and logical relationships through in-context learning, we provided GPT-4V with in-context examples of varying relevance  to the tasks and conducted comparative experiments.That is, highly relevant, loosely relevant, and zero-shot.Each example includes a different task, a corresponding scene image, and a correct plan.</p>
<p>Highly relevant examples are carefully constructed according to the given tasks.Some of them are constructed by replacing the subject of the task, so the structures of the plans are mostly the same.Weakly relevant examples for all tasks are the same, which is a simple Bring Water task presented in Appendix.</p>
<p>We introduce Plausible Success Rate (PSR), Executable Success Rate (ESR), and Language Compliance (LC) to measure the performance of each method.Since the output skill sequences may not meet the strict requirements for execution, we count them as plausibly successful as long as they are reasonable regardless of the execution.Language Compliance was designed by [Li et al., 2024] to measure the structuredness and compliance of the skill generated by the foundation models.It is given as LC = ŝ/s * , where ŝ is the number of valid skills and s * is the total number of skills.</p>
<p>We tested the three methods on seven long-horizon embodied planning tasks, and the results are shown in Table 1.</p>
<p>Without in-context examples demonstrating the format of the output and the logical relationships between the skills, GPT-4V can make plausible plans but with errors in detail.GPT-4V with highly relevant, manually crafted examples performs well in all tasks.</p>
<p>GPT-4V with loosely related in-context examples can sometimes produce textually plausible plans, but due to logical errors and hallucinations, they are not executable.We categorize these errors into four types in Figure 2: logical understanding errors, skill combination errors, logical planning errors, and hallucinations.</p>
<p>Put(cup, table, right) puts the cup on the right side of the table, on the ground.While the model assumes it should be placed on the right side of the tabletop.This results in a logical understanding error.Before navigating to a destination, the agent should first detect it.Not detecting will result in failure of the next step, which is a typical skill combination error.Pulling the fridge door instead of the fridge door handle and pulling in the wrong directions are logical planning errors.Not knowing how to combine the skills provided to fill the kettle with water, the model hallucinated and fabricated an undefined skill Fill.</p>
<p>All of these errors and hallucinations are caused by the inability of GPT-4V to understand the logical relationships between skills without examples that are highly relevant to the task.However, building such task-specific examples for each task is no different from manually planning.</p>
<p>Comparative Experiments</p>
<p>Baselines and Metrics</p>
<p>We choose GPT-4V * , ReLEP+GPT-4V, and the VILA [Hu et al., 2023] method as our baselines.</p>
<p>We equip VILA with the same skill library as ReLEP, and utilize its original framework for planning, referred as VILA * .</p>
<p>GPT-4V * uses the ReLEP framework, but replaces the finetuned model with GPT-4V for all rounds of planning.In the first round of planning, GPT-4V * is provided with highly relevant, manually crafted in-context examples.For in the previous experiment, GPT-4V was unable to make executable plans when only loosely relevant examples were provided.It should be emphasized that this is a significant enhancement to GPT-4V * , and poses an even greater challenge for ReLEP.</p>
<p>Apart from using GPT-4V for subsequent rounds of planning, the rest of ReLEP+GPT-4V is identical to ReLEP.</p>
<p>We compared the success rate of the initial plans (Initial Plan Success Rate, IPSR) made on the designed tasks, demonstrating the ability of first-round planning.In addition, to verify the ability of subsequent-round planning, we also display the success rate of each planning step as (Step-wise Success Rate, SSR).Finally, we report the success rate of the entire process (Success Rate, SR).</p>
<p>Real-time Experiment in Simulated Environment</p>
<p>We selected six scenes in the AI2-THOR simulation environment and designed ten long-horizon embodied tasks for real-time comparative experiments.These tasks include both tasks from the training set and tasks outside the training set.Tasks outside the training set include unseen objects and unseen tasks.Furthermore, all the scenes used in the experiment are unseen.We present the quantitative evaluation results in Table 2. VILA * tends to overthink and discusses a variety of scenarios, resulting in longer inference time.Moreover, most of the output is in natural language, with the actual plan comprising only a small portion.VILA * often ignores the navigation skill and manipulates objects without approaching.</p>
<p>With the help of highly relevant examples, GPT-4V * performs well in the first round of planning.However, GPT-4V sometimes gets stuck at Detect or Navigation in subsequent rounds of planning and repeatedly executes the same skill.This may be because it is difficult for the model to confirm whether these steps have been completed based on the image information.In fact, the prompts already include all the finished steps, but the model does not refer to them.</p>
<p>ReLEP has similar IPSR compared to GPT-4V with highly relevant examples, demonstrating the effectiveness of finetuning with the method incorporating implicit logical relationships.</p>
<p>ReLEP achieves high success rates on all tasks, suggesting the implicit logical inference and hallucination mitigation capabilities of our approach.Even in unseen tasks, ReLEP maintains a high success rate, indicating the robustness of the method.planning and its mastery of skill sets.</p>
<p>Real-world Experiment Using Collected Images</p>
<p>We designed eight long-horizon embodied tasks in real-world environment to verify ReLEP's ability in real-world planning, In this experiment, we did not execute the steps in the plans.Instead, we collected scene images after each executed step for offline testing and manually assessed the success rate.That is to say, the captured images are what the scene should look like after the steps are executed.Because the inputs of the models and the evaluation criteria are essentially the same, this experiment can also verify the effectiveness of the methods.The evaluation results are shown in Table 3. ReLEP still demonstrates a higher success rate compared to other baselines, highlighting its effectiveness in real-world environments.</p>
<p>Ablation Study</p>
<p>In the ablation study, we ablate the large vision-language model to demonstrate the effectiveness of fine-tuning, and the Memory module to prove its necessity in subsequent-round planning.Results are presented in Table 4.</p>
<p>To ablate the large vision-language model, we directly use LLaVA-1.6-7B to predict the plans.We compared ReLEP with both LLaVA-1.6-7Bzero-shot and LLaVA-1.6-7B with loosely relevant examples.Given the same prompt as ReLEP, LLaVA-1.6-7Bzero-shot generates plans in freeform like GPT-4V zero-shot.This was expected since the model cannot possibly predict the required format without examples.With loosely relevant examples, LLaVA-1.6-7Bperforms worse than GPT-4V, with more logical errors and hallucinations.Moreover, it fails on subsequent rounds of planning with low Language Compliance.ReLEP's higher SR and LC suggest that fine-tuning the large-vision language model is necessary.It helps the model learn implicit logical inference and better understand how each module works.</p>
<p>To verify the effectiveness of Memory module, we compared ReLEP with a version of ReLEP without Memory module on the long-horizon embodied tasks in the simulated environments.ReLEP without Memory module fails to predict new plans after steps like Detect and Navigate because it is difficult for the model to determine whether such steps are finished solely according to the image.In other words, some skills do not change the scene image or cause minimal changes in the scene, resulting in the same or similar inputs between steps.Since the framework does not record the completed steps, the model cannot distinguish between the two states.Therefore, it stuck in the loop to repeatedly detect an object like GPT-4V * did.Nevertheless, we proceeded our experiments by providing subsequent images to verify if it could break the loop based on obvious changes in the image.As a result, obvious changes like the refrigerator door being opened can be detected by ReLEP without the Memory module, but not for small changes like getting closer to the target.</p>
<p>In conclusion, the above facts and ReLEP's higher SSR demonstrates the effectiveness of the Memory Module.</p>
<p>Conclusion</p>
<p>We pointed out that foundation models using in-context learning face logical errors and hallucinations in long-horizon embodied tasks.To this end, we proposed to fine-tune a large visual-language model with implicit logical relations.Furthermore, we proposed a novel framework for real-time longhorizon embodied planning called ReLEP.It plans with a large vision-language model according to the environment image.We also proposed a pipeline for real-world embodied planning data generation and constructed a real-world longhorizon embodied planning dataset with implicit logical relationships.Comprehensive experiments are conducted and the results demonstrate the effectiveness of ReLEP and suggest that it outperforms the SOTA method and the GPT-4V model.</p>
<p>However, the real-world application of ReLEP requires robust and universal algorithms as well as low-level control policies.Although obtaining these algorithms remains a challenge, current advancements in general tasks such as object detection, grasping, and navigation [Liu et al., 2025;Wan et al., 2023;Chang et al., 2023] have created promising prospects for the implementation of skill functions.In addition, we only constructed a small real-world embodied planning dataset with 5K basic data due to limited resources.Hopefully, more researchers will be joining this cause and build larger datasets with better quality.We collected images from indoor object detection datasets and used GPT-4V to generate possible tasks a robot may perform on these scenes.We then manually refined the generated tasks and asked GPT-4V to predict corresponding plans.Finally, by manually refining the generated plans, we acquire data triplets of task, plan, and image.</p>
<p>Figure 1 :
1
Figure 1: An overview of ReLEP.Given an instruction and a current scene image, a fine-tuned large vision-language model formulates plans as sequences of skill functions according to a skill library, a Memory module, and a Robot Configuration module.Then, the robot executes the first step and saves executed steps and past plans into the Memory module for subsequent rounds of planning.</p>
<p>Figure 2 :
2
Figure 2: Four types of errors GPT-4V made in long-horizon embodied planning with loosely relevant examples.Logical understanding errors are misinterpretations of individual skills.Missing skills can lead to skill combination errors.Logical mistakes result in logical planning errors.Hallucinations include fabricating undefined skills.</p>
<p>Ablations over the large vision-language model and the Memory module.Without the Memory module, ReLEP fails on subsequent rounds of planning and has a lower SSR.Without finetuning, the model makes logical errors even with loosely relevant examples and has low LC.All values are presented as percentages.</p>
<p>Figure A1 :
A1
FigureA1: Data acquisition pipeline.We collected images from indoor object detection datasets and used GPT-4V to generate possible tasks a robot may perform on these scenes.We then manually refined the generated tasks and asked GPT-4V to predict corresponding plans.Finally, by manually refining the generated plans, we acquire data triplets of task, plan, and image.</p>
<p>Figure A2 :
A2
Figure A2: Illustration of the planning of ReLEP on the Bring Water task in the real-world experiment using collected images.Steps that would not change the environment image are omitted during testing.</p>
<p>Table 1 :
1
Evaluation results of GPT-4V with in-context examples of varying relevance of long-horizon embodied planning.GPT-4V zero-shot predicts freeform plans with zero LC.GPT-4V with loosely relevant examples has improved LC but makes logical errors.GPT-4V with highly relevant examples can make logical plans with high success rate.All decimal values are presented as percentages.</p>
<p>Table 2 :
2
Quantitative evaluation results on real-time long-horizon tasks in simulated environment AI2-THOR.ReLEP demonstrates superior performance both in seen tasks (top half) and unseen tasks (bottom half).The total steps of SSR of each task and method depends on which step the failure occurs during each run.Early failure can lead to fewer steps of SSR.
VILA  <em>GPT-4V  </em>ReLEP+GPT-4VReLEPTaskIPSR SSRSRIPSRSSRSRIPSRSSRSRIPSRSSRSRClose Laptop7/10 30/41 7/10 10/10 29/38 1/10 10/10 32/38 2/10 10/1050/5010/10Push Chair6/10 12/21 1/10 10/10 20/30 0/10 10/10 20/27 3/10 10/1040/4010/10Turn on Lights9/10 25/18 7/10 10/10 25/30 50.0 10/10 26/30 70.0 10/1030/3010/10Look into Mirror 6/10 23/27 8/10 10/10 28/32 6/10 10/10 22/27 5/10 10/1030/3010/10Put Mug0/10 42/52 0/10 10/10 61/71 0/10 10/10 45/56 0/10 10/1080/8010/10Take Egg4/10 14/24 0/10 10/10 72/81 5/10 10/10 36/44 2/10 10/10 110/110 10/10Fill Mug0/10 32/42 0/10 10/10 70/80 0/10 10/10 44/54 0/10 10/10 100/100 10/10Flush Knife1/10 10/20 0/10 10/10 70/78 2/10 9/10 67/76 1/10 10/1091/929/10Pull Curtain5/10 11/19 2/10 10/10 25/28 7/10 10/10 23/28 6/10 10/1040/4010/10Lift Toilet Seat6/10 14/23 1/10 10/10 31/38 3/10 10/10 33/37 6/10 10/1040/4010/10Total (%)44.071.7 26.0 100.0 85.2 29.0 99.083.5 32.0 100.099.899.0</p>
<p>Table 3 :
3
Its higher SSR and SR than other baselines showcase ReLEP's outstanding ability in real-time long-horizon Evaluation results with real-world images on longhorizon tasks.ReLEP still holds a higher success rate, highlighting its effectiveness in real-world environments.All values are presented as percentages.
MethodsIPSR SSR SRVILA  <em>61.4 81.1 58.6GPT-4V  </em>82.9 77.5 22.9ReLEP+GPT-4V 94.3 83.6 34.3ReLEP94.3 97.7 94.3
Skill DescriptionDetect(object)Detect someone or something referred in the task.Grasp(object)Grasp the object.Navigate(destination)Navigate to destination.Pull(object, direction)Pull object in one of the following directions:up, down, left, right, backward.Push(object, direction) Push object in one of the following directions:up, down, left, right, forward.Put(object, place, side) Put the object to the side of the place.Speak(query)Communicate with the user by saying the query.EQA(query)Embodied question answering, return the answer to the query.Wait(integer)Wait for integer seconds.TableA1: The Skill Library of ReLEP.We defined nine skills that can be combined to complete a wide range of tasks in daily life, including interactive skills that can actively change the environment (top part) and non-interactive skills that cannot (bottom part).Task DescriptionTurn on Lights Turn on the room lights by pressing a light switch.Push Chair Push an armchair under the table.Bring WaterBring the user a bottle of water from a closed fridge.Bring BookBring the user the book under a cup on the desk.Throw TrashPick up the trash on the ground and throw it into the trash can.Fill Kettle Fill the kettle with tap water.Put MugPut the mug into a closed cabinet.TableA2: Descriptions of the seven long-horizon tasks used in the in-context experiment.(a) A loosely relevant example.(b) A highly relevant example.Turn on Lights Turn on the room lights by pressing a light switch.Look into Mirror Approach a mirror and tell the user what is in it.Put MugPut the mug into a closed cabinet.Take Egg Take an egg from a closed fridge and put it on the table.Fill Mug Fill the mug with tap water and put it on the table.Flush Knife Flush the knife with tap water.Pull CurtainClose the shower curtain.Lift Toilet Seat Lift the toilet seat up.TableA3: Descriptions of the ten real-time simulated long-horizon tasks.Task Name DescriptionTurn on Lights Turn on the room lights by pressing a light switch.Push Chair Push an armchair under the table.Unplug Power Unplug a power cord from a socket on a vertical wall.Close LaptopClose the laptop with the lid open.Bring BookBring the user a book from the table.Throw Trash Throw an empty bottle into the trash can.Bring WaterBring the user a bottled water from a closed fridge.Look into Mirror Approach a mirror and tell the user what is in it.
Ahn , arXiv:2204.01691Do as i can, not as i say: Grounding language in robotic affordances. 2022. 2022arXiv preprint</p>
<p>Bai, arXiv:2308.12966arXiv:2403.01823Yevgen Chebotar, Debidatta Dwibedi, and Dorsa Sadigh. Rt-h: Action hierarchies using language. Ted Xiao, Pierre Sermanet, Quon Vuong, Jonathan Tompson, 2023. 2023. 2024arXiv preprintQwen-vl: A frontier large visionlanguage model with versatile abilities</p>
<p>Rt-2: Vision-language-action models transfer web knowledge to robotic control. Brohan, arXiv:2307.15818arXiv:2311.06430Chang et al., 2023] Matthew Chang, Theophile Gervet, Mukul Khanna, Sriram Yenamandra, Dhruv Shah, So Yeon Min, Kavit Shah, Chris Paxton, Saurabh Gupta, Dhruv Batra2023. 2023. 2023arXiv preprintet al. Goat: Go to any thing</p>
<p>Chen , arXiv:2310.09478Minigpt-v2: large language model as a unified interface for vision-language multi-task learning. 2023. Jun. 2023arXiv preprint</p>
<p>Instructblip: Towards general-purpose vision-language models with instruction tuning. Dai, arXiv:2010.11929International Conference on Machine Learning. Georg Heigold, Sylvain Gelly2023. 2023. 2023. 2023. 2020arXiv preprintScaling vision transformers to 22 billion parameters. Dosovitskiy et al., 2020. et al. An image is worth 16x16 words: Transformers for image recognition at scale</p>
<p>Palm-e: An embodied multimodal language model. Driess, arXiv:2303.033782023. 2023arXiv preprint</p>
<p>Ego4d: Around the world in 3,000 hours of egocentric video. Grauman, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022. 2022</p>
<p>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. Hu, arXiv:2106.09685arXiv:1712.05474-struct2act: Mapping multi-modality instructions to robotic actions with large language model. Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli Vanderbilt, Luca Weihs, Alvaro Herrasti, Matt Deitke, Kiana Ehsani, Daniel Gordon, Yuke Zhu, PMLR2021. 2021. 2023. 2023. 2022. 2022. 2023. 2023. 2022. 2023. 2017. 2017arXiv preprintKipuw. Indoor object detection computer vision project. et al. Ai2-thor: An interactive 3d environment for visual ai</p>
<p>Blip-2: Bootstrapping language-image pretraining with frozen image encoders and large language models. Li, International conference on machine learning. PMLR2023. 2023</p>
<p>Muep: A multimodal benchmark for embodied planning with foundation models. Li, 2024</p>
<p>Code as policies: Language model programs for embodied control. Liang, 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE2024. 2023. 2023Intemational Joint Conferences on Artificial Intelligence. IJCAI</p>
<p>Improved baselines with visual instruction tuning. Liu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024a. 2024</p>
<p>Liu, Llava-next: Improved reasoning, ocr, and world knowledge. 2024b. January 2024</p>
<p>Visual instruction tuning. Liu, Advances in neural information processing systems. 2024c. 202436</p>
<p>Grounding dino: Marrying dino with grounded pre-training for open-set object detection. Liu, European Conference on Computer Vision. Springer2025. 2025. 202436Neural Information Processing Systems</p>
<p>Alfred: A benchmark for interpreting grounded instructions for everyday tasks. ; Openai, Shridhar, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2023. 2023. 2020. 2020OpenAI. Gpt-4v(ision) system card</p>
<p>Unidexgrasp++: Improving dexterous grasping policy learning via geometry-aware curriculum and iterative generalist-specialist learning. Skreta, arXiv:2401.04157arXiv:2310.01412Mldt: Multi-level decomposition for complex longhorizon robotic task planning with open-source large language model. Nan Zhang, Lan-Ling Hu, Guilin Tang, Jun Qi, Jie Shao, Wei Ren, Song, 2024. 2024. 2023. 2023. 2023. 2023. 2023. 2023. 2024. 2023arXiv preprintProceedings of the IEEE/CVF International Conference on Computer Vision. Xu et al., 2023. Drivegpt4: Interpretable end-to-end autonomous driving via large language model</p>
<p>Closed-loop open-vocabulary mobile manipulation with gpt-4v. Zhi, arXiv:2404.102202024. 2024arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
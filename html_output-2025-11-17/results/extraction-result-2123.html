<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2123 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2123</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2123</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-54.html">extraction-schema-54</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <p><strong>Paper ID:</strong> paper-281658310</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2509.22502v1.pdf" target="_blank">InfiAgent: Self-Evolving Pyramid Agent Framework for Infinite Scenarios</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Model (LLM) agents have demonstrated remarkable capabilities in organizing and executing complex tasks, and many such agents are now widely used in various application scenarios. However, developing these agents requires carefully designed workflows, carefully crafted prompts, and iterative tuning, which requires LLM techniques and domain-specific expertise. These hand-crafted limitations hinder the scalability and cost-effectiveness of LLM agents across a wide range of industries. To address these challenges, we propose \textbf{InfiAgent}, a Pyramid-like DAG-based Multi-Agent Framework that can be applied to \textbf{infi}nite scenarios, which introduces several key innovations: a generalized"agent-as-a-tool"mechanism that automatically decomposes complex agents into hierarchical multi-agent systems; a dual-audit mechanism that ensures the quality and stability of task completion; an agent routing function that enables efficient task-agent matching; and an agent self-evolution mechanism that autonomously restructures the agent DAG based on new tasks, poor performance, or optimization opportunities. Furthermore, InfiAgent's atomic task design supports agent parallelism, significantly improving execution efficiency. This framework evolves into a versatile pyramid-like multi-agent system capable of solving a wide range of problems. Evaluations on multiple benchmarks demonstrate that InfiAgent achieves 9.9\% higher performance compared to ADAS (similar auto-generated agent framework), while a case study of the AI research assistant InfiHelper shows that it generates scientific papers that have received recognition from human reviewers at top-tier IEEE conferences.</p>
                <p><strong>Cost:</strong> 0.024</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2123.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2123.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InfiAgent Exec-Audit</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>InfiAgent Execution-Level Dual-Audit (Execution-Level Audit)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An internal, continuous execution-level validator that monitors every agent output during runtime, maintains per-agent quality scores, and updates them via an online validate(output) function to prevent error propagation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>InfiAgent Execution-Level Audit</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A runtime auditing component of InfiAgent that continuously monitors each agent's intermediate outputs, computes/updates a quality score Q_i for each agent, and invokes a validate(·) function to accept/reject or weight outputs in downstream decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>AI systems / automated scientific workflows</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>computational_proof</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Continuous automated validation during execution: for agent A_i at time t the system computes Q_i updates: Q_i(t+1) = α·Q_i(t) + (1−α)·validate(O_i(t)). The validate(·) function assesses whether O_i(t) meets expected requirements (format, content constraints, tests). Validation is integrated into agent routing, quality scoring, and branching decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td>N/A (this is an internal software/algorithmic validation mechanism rather than a physical simulation); fidelity corresponds to the rigor of validate(·) implementation which can range from lightweight format checks to heavyweight execution/tests.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_sufficiency</strong></td>
                            <td>Paper frames it as necessary for system stability but does not claim it is universally sufficient for domain-specific scientific claims; sufficiency depends on validate(·) detail (unit tests, execution checks) and downstream human or domain-level checks.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_accuracy</strong></td>
                            <td>Not numerically specified; accuracy depends on validate(·) implementation and historical Q_i calibration. No explicit error rates provided.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_details</strong></td>
                            <td>No physical experiments; validation is implemented and exercised within framework runs and benchmark comparisons in the paper, but the paper does not provide quantitative standalone evaluation of validate(·) false-positive/false-negative rates.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_comparison</strong></td>
                            <td>No explicit comparison between different execution-level validation strategies is presented; it is compared conceptually to 'post-hoc validation' and presented as superior because it prevents error propagation earlier.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_failures</strong></td>
                            <td>Paper notes general risks of error propagation if auditing is absent, but no concrete instances of execution-level audit failure are documented.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_cases</strong></td>
                            <td>Cited as a contributor to improved stability and to InfiAgent's benchmark performance; no isolated metric exclusively attributable to execution-level audit is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_comparison</strong></td>
                            <td>Validation is internal (checks against expected outputs or test cases), not against external wet-lab ground truth; when used for code tasks, judge_agent runs code tests as part of execution-level checks per system design.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_replication</strong></td>
                            <td>The paper describes the mechanism but does not report independent replication of validate(·) outcomes; reproducibility depends on validate(·) specifications and available artifacts.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Overhead implied (continuous checks and Q updates), but no quantitative runtime or cost metrics for execution-level audit alone are given.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_norms</strong></td>
                            <td>Framed as a software-engineering norm: automated unit/integration-like checks during pipeline execution; domain norms (e.g., experimental wet-lab verification) are not replaced by this mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Yes — per-agent quality score Q_i serves as internal uncertainty/confidence signal updated via exponential smoothing with parameter α.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Effectiveness depends entirely on the design of validate(·); lightweight format checks may pass malformed scientific claims; no guarantees on correctness beyond tested dimensions.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_details</strong></td>
                            <td>Execution-level audit is one layer within a dual-audit system (paired with system-level audits and judge models) and is combined with downstream retrospective/system-level reviews and judge-model assessments in the framework.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2123.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2123.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InfiAgent Sys-Audit</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>InfiAgent System-Level Dual-Audit (System-Level Audit & Retrospective Summarization)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A higher-level auditing layer that performs retrospective summarization, built-in reviews, and context summarization to maintain system stability and prevent error propagation across multi-stage workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>InfiAgent System-Level Audit</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A system-level review mechanism that performs retrospective summarization, context summarization (to shorten token contexts), and global stability checks; complements execution-level audits to enforce overall workflow correctness and context efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>AI systems / automated scientific workflows</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>computational_proof</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>System-level processes perform review passes over completed subworkflows, summarize context, and check for consistency/stability. It enforces structured output formatting and retrospective summarization to catch propagated errors and reduce context size.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td>N/A (high-level software/logical auditing); fidelity depends on the thoroughness of retrospective checks.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_sufficiency</strong></td>
                            <td>Presented as necessary for multi-stage workflow reliability; paper acknowledges that system-level audit helps prevent error propagation but does not claim sufficiency for domain-level scientific validation (e.g., empirical claims require domain validation).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_accuracy</strong></td>
                            <td>No numerical metrics provided for system-level audit accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_details</strong></td>
                            <td>Used in framework experiments and case study pipelines, but no separate experimental protocol quantifying its detection power is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_comparison</strong></td>
                            <td>Contrasted qualitatively with post-hoc validation; system-level audit is described as continuous and retrospective and therefore superior at catching cascade errors, but no head-to-head experimental comparison is given.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_failures</strong></td>
                            <td>No concrete failure examples are provided; limitations are implicit (dependence on summarization fidelity and the heuristics used).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_cases</strong></td>
                            <td>Attributed to overall system stability and benchmark performance gains, but not separately measured.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_comparison</strong></td>
                            <td>System-level audit checks for internal consistency, not external experimental ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_replication</strong></td>
                            <td>Mechanism described; no independent replication study provided.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Intended to save tokens and improve context efficiency; trade-offs between audit thoroughness and runtime not quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_norms</strong></td>
                            <td>Serves software/agent-system norms (review processes); does not substitute domain-specific empirical validation.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Implicit via aggregated quality metrics and summarization outputs; no explicit uncertainty model beyond Q_i aggregates is described.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Relies on compressed summarization which may omit critical details; quality depends on summarization fidelity and review heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_details</strong></td>
                            <td>System-level audit operates in tandem with execution-level validation and judge-model checks to form the dual-audit mechanism.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2123.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2123.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Judge-Model J</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Judge Model (J) for Model-Level Evolution and Validation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A computational judge model that evaluates candidate model outputs and marks contributions as effective (J(Δm)=1) for merging into the main branch, used during model-level self-evolution as an automated validator.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>InfiAgent Judge Model J</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Automated judge used in the self-evolution Git-style workflow to evaluate outputs of multiple parallel lightweight models; only contributions judged effective by J are merged into the main branch and used for training.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>AI systems / model development and automated ML</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>computational_proof</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Judge model J inspects candidate model contributions ∆m^(t)_i and returns binary assessments J(∆m)=1/0; merging rule B_main(t+1) = merge(B_main(t), {∆m | J(∆m)=1}). The judge is used to select high-quality operations and construct datasets for continual training.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td>N/A (algorithmic/model-evaluation fidelity depends on judge model capacity and criteria; not described as physics-based simulation).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_sufficiency</strong></td>
                            <td>Paper treats judge-model adjudication as effective for maintaining quality within the model-evolution loop; sufficiency for external scientific truth is not claimed and depends on judge design and domain-level checks.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_accuracy</strong></td>
                            <td>No numeric accuracy or false-positive/false-negative rates of J are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_details</strong></td>
                            <td>Judge model is described as operational in experiments (used in self-evolution), but the paper provides no standalone validation metrics for J's decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_comparison</strong></td>
                            <td>Judge-based selection is compared conceptually to purely competitive selection; system also retrains candidate models on merged validated data, but quantitative comparisons isolating judge impact are not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_failures</strong></td>
                            <td>No specific failure cases of judge model selection are presented.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_cases</strong></td>
                            <td>Used in the framework's self-evolution to produce improved functional agents and domain-specialized models over time as claimed by the authors; direct causal metrics for judge success are not separated.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_comparison</strong></td>
                            <td>Judge decisions are internal and used to generate training data; not directly compared to external gold standards in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_replication</strong></td>
                            <td>Mechanism described; reproducibility contingent on sharing judge model and criteria which are not fully specified in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Implied overhead from running judge model evaluations across parallel candidates; no numeric cost/time reported.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_norms</strong></td>
                            <td>Model selection via automated judges is an increasing norm for self-improving systems, but domain validation still requires external verification for scientific claims.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Judge is presented as binary evaluator in equations; no probabilistic uncertainty output is described.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Binary judge decisions can propagate judge bias; effectiveness depends on judge model fidelity and training data.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_details</strong></td>
                            <td>Judge model serves as a computational filter within a hybrid pipeline that also includes execution/system audits and (where applicable) human peer review.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2123.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2123.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InfiHelper Peer Review</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>InfiHelper Human Peer-Review Validation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Human peer review of papers generated by the InfiHelper research assistant; used as a domain-level validation of scientific-quality outputs (recognized by human reviewers at top-tier IEEE conferences).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>InfiHelper Peer-Review Validation</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>InfiHelper-generated research outputs were subjected to human peer-review-style evaluation using reviewers and a reviewer model (Claude-3.7-sonnet) and the paper claims these outputs received recognition from human reviewers at top-tier IEEE venues.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Scientific publishing / research automation across engineering domains</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>fabricated</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Validation performed via human peer review processes and an automated reviewer proxy (Claude-3.7-sonnet) for systematic quality assessment; the authors performed a structured reviewer evaluation comparing InfiHelper outputs to other AI systems using peer-review criteria (scores 1–10).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td>N/A (human-review assessment is a qualitative domain standard; the reviewer-model is an LLM-based proxy and thus computational but not a physics/experimental simulation).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_sufficiency</strong></td>
                            <td>Peer review is a domain normative validation for scientific quality, but the paper acknowledges peer review evaluates manuscript quality, not empirical truth of scientific claims; experimental claims would still require domain-specific empirical validation.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_accuracy</strong></td>
                            <td>Reported average reviewer score for InfiHelper outputs: 6.0 (on 1–10 scale); comparative system scores reported (e.g., Sakana-AI 4.0, HKU 5.0). No false-positive/negative rates provided.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_details</strong></td>
                            <td>No wet-lab or field experiments performed for validating scientific claims of generated papers; validation here refers to human reviewers judging manuscript quality, methodology, and rigor.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_comparison</strong></td>
                            <td>Paper compares InfiHelper outputs to outputs from AI-Researcher, Zochi, AI-Scientist V2 through peer-review-style scoring; InfiHelper scored higher on average (6.0) in this evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_failures</strong></td>
                            <td>Paper does not report specific peer-review rejections or major failure cases, though it notes rigorous reviewer criteria (including file naming, format compliance) matter; it does not claim acceptance of generated papers themselves, only 'recognition' from reviewers.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_cases</strong></td>
                            <td>Authors claim generated papers received recognition from human reviewers at top-tier IEEE conferences and that InfiHelper outperformed other AI-research assistants in reviewer scoring in their evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_comparison</strong></td>
                            <td>Peer review compares manuscript quality against reviewer expectations and norms rather than experimental ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_replication</strong></td>
                            <td>Peer-review outcomes depend on reviewers and review conditions; the paper provides sample outputs in appendix but does not report independent external replication of peer-review results.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Human review is time-consuming; paper used Claude-3.7-sonnet for automatic peer-review-style scoring to scale evaluation, but no explicit time/cost numbers are given for human reviewing.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_norms</strong></td>
                            <td>In research, peer review is standard to validate novelty, rigor, and presentation; the paper acknowledges that peer review is necessary but not sufficient for empirical validation of discoveries.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Reviewer scores and comparative averages provide some measure of confidence; no formal uncertainty model applied to peer-review judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Peer review assesses quality/presentation and methodology but does not substitute for independent experimental verification of scientific claims; risk of superficial or format-compliant but incorrect claims remains.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_details</strong></td>
                            <td>Combined automated judge/reviewer model (Claude-3.7-sonnet) evaluations with human reviewer recognition and internal audits (dual-audit) to validate research outputs.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2123.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2123.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Benchmark Eval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Benchmark-based Computational Validation (DROP, HumanEval, MBPP, GSM8K, MATH)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Validation of InfiAgent using standard computational benchmarks across reasoning, coding, and math (DROP, HumanEval, MBPP, GSM8K, MATH) to compare performance against baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>InfiAgent Benchmark Validation</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>InfiAgent was evaluated on multiple community benchmarks (DROP, HumanEval, MBPP, GSM8K, MATH) and compared to state-of-the-art baselines to validate reasoning and coding performance; reported average improvements (e.g., 9.9% over ADAS).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Natural language reasoning, program synthesis, mathematical problem solving (AI/NLP/ML evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>simulated</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Compute-centric validation: run the system on benchmark datasets with standardized metrics and compare scores to baselines and ablations; includes tables of per-benchmark scores and averaged metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td>Empirical/benchmark-level fidelity: high for measuring algorithmic performance on dataset tasks but not a surrogate for empirical scientific discovery validation; benchmark results are reproducible and accepted norms in AI evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_sufficiency</strong></td>
                            <td>Sufficient to quantify relative algorithmic capabilities on specific tasks (reasoning, coding, math) but not sufficient to validate domain-level scientific claims (e.g., real-world experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_accuracy</strong></td>
                            <td>Reported per-benchmark numeric scores (example highlights): DROP 82.4%, GSM8K 93.1%, MATH 35.6%; overall claimed 9.9% average improvement vs. ADAS. No uncertainty intervals provided for all metrics, though some tables include means/std.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_details</strong></td>
                            <td>All validation here is computational on datasets; no physical experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_comparison</strong></td>
                            <td>Direct comparisons to multiple baselines (chain-of-thought, self-refinement, ADAS, etc.) are provided; InfiAgent shows higher average performance than ADAS by 9.9% per the authors.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_failures</strong></td>
                            <td>Paper notes limitations on specialized math (MATH) where InfiAgent underperforms top methods (e.g., MATH 35.6% vs other methods up to ~50.8%), attributing gaps to tool-calling overhead.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_cases</strong></td>
                            <td>Strong results in multi-step reasoning (DROP) and mathematical/coding tasks (GSM8K, HumanEval) where the agent-as-a-tool decomposition and dual-audit aided performance.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_comparison</strong></td>
                            <td>Benchmarks act as ground-truth tasks with accepted scoring; comparisons to published baselines provide external reference.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_replication</strong></td>
                            <td>Benchmarks are standard and reproducible; paper does not supply code release in main text (appendix mentions prompts) but benchmarking methodology follows community norms.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Benchmarking consumes compute (involving many agents/models) but no precise resource-time numbers are reported aside from model overhead mentions.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_norms</strong></td>
                            <td>In AI, benchmark performance is a normative validation for capability claims; paper adheres to this norm for performance claims but recognizes domain-specific empirical validation is separate.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Some tables include mean/std (e.g., optimizer study), but benchmark results reported for InfiAgent are point estimates without detailed confidence intervals.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Benchmarks are limited to proxy tasks; high benchmark scores do not guarantee correct scientific discovery or experimental validity in domain sciences.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2123.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2123.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CarbonSim</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Carbon-Aware Adaptive Routing Simulation Validation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Validation of the carbon-aware routing protocol entirely via simulation of the Greater Bay Area transport network, using time-varying traffic multipliers, stochastic variation, and statistical analysis (p-values) to evaluate travel-time and emission trade-offs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Carbon-Aware Adaptive Routing (simulation)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A two-layer routing protocol validated in a simulated Greater Bay Area network with traffic scenarios (peak/off-peak/mixed), vehicle types, and randomized traffic variations; performance measured on emissions and travel time.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Transportation systems / simulation-based logistics optimization</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>simulated</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Simulated 24-hour traffic cycles with time-based multipliers per hour, congestion multipliers (e.g., night 0.2–0.5x, rush 1.2–1.9x), random ±20% variation, multiple replications (6 per scenario) and statistical tests (e.g., p = 0.3872 for travel-time difference vs Traffic-Adaptive). Metrics include total emissions (kg CO2), travel time (minutes), and carbon-efficiency. Algorithms compared: Baseline, Static Carbon-Aware, Traffic-Adaptive, Carbon-Aware Adaptive.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td>Empirical/mesoscopic simulation-level (empirical multipliers + randomized noise). Not physics-first-principles vehicle dynamics; fidelity is moderate and adequate for comparative algorithmic evaluation but limited for precise emission prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_sufficiency</strong></td>
                            <td>Sufficient for algorithmic evaluation in simulated logistics settings; authors acknowledge simplifications (emission model simplifications, limited traffic prediction) and recommend real-world validation as future work.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_accuracy</strong></td>
                            <td>Some numeric results provided: e.g., in peak traffic Baseline/Static Carbon-Aware travel time 63.8 min; Carbon-Aware Adaptive emissions 55.14 kg CO2 vs Baseline/Static 46.84 kg CO2; off-peak oddities (higher emissions due to speeds) reported (e.g., Carbon-Aware Adaptive 184.18 kg CO2). Statistical significance reported (p < 0.05 for some comparisons; p = 0.3872 for travel-time vs Traffic-Adaptive).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_details</strong></td>
                            <td>No field experiments; authors explicitly state future work includes conducting real-world validation studies.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_comparison</strong></td>
                            <td>Comparative simulation results across four algorithms with statistical analysis; shows trade-offs and vehicle-type-dependent performance differences.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_failures</strong></td>
                            <td>Paper documents counter-intuitive simulated failure: higher emissions in off-peak conditions due to higher speeds increasing fuel consumption, indicating limitations of simplified emission assumptions; acknowledges limitations of emission model fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_cases</strong></td>
                            <td>Carbon-Aware Adaptive achieved balanced performance in mixed conditions (lowest average travel time 75.2 min while maintaining carbon awareness) and statistically comparable travel times to Traffic-Adaptive (p = 0.3872).</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_comparison</strong></td>
                            <td>No comparison to field measurements; simulation findings are not validated against observed traffic or emissions data.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_replication</strong></td>
                            <td>Simulation framework described (NetworkX + NumPy, parameter tables) enabling reproducibility in simulation but not validated by external replication in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Simulations across scenarios with replications (6 per scenario) reported but no absolute runtime/cost metrics provided; simulation is lower-cost than field trials but authors recommend real-world validation which would be costlier.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_norms</strong></td>
                            <td>Authors note domain norm: real-world/field validation required for deployment; simulation is an accepted first step but insufficient for production claims.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Random variation ±20% included; statistical testing used (p-values); explicit confidence intervals not always reported but some statistical significance testing is present.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Simplified emissions model, limited traffic prediction capabilities, static carbon weight parameter; no real-world measurement validation provided.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2123.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2123.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AMSDAS Eval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AMSDAS Adversarial-Training Computational Validation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Extensive computational validation of AMSDAS on image classification benchmarks and adversarial attacks (PGD-7) including ablation, transfer-attack tests, cross-architecture/dataset evaluations, and statistical analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AMSDAS Validation</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>AMSDAS was validated via standard ML experiments: CIFAR-10 and Fashion-MNIST datasets with ResNet-18 and MobileNetV2, PGD-7 attack (ε=8/255), ablation studies (early-layer smoothing effects), hyperparameter sweeps, and reporting of accuracies, losses, and robustness metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Machine learning robustness / adversarial defenses</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>simulated</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Compute experiments included adversarial training and evaluation under PGD attacks, transfer attacks, ablation studies (component contributions), parameter sensitivity, cross-architecture/dataset generalization, and statistical analyses (means, stds, tables). Metrics: clean and robust test accuracies, training loss, and computational overhead percentages (5–7% extra runtime).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td>High-fidelity for ML evaluation (standard datasets, standardized attacks like PGD-7); not physical experiments. Results are directly comparable to community baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_sufficiency</strong></td>
                            <td>Sufficient to support claims about ML robustness and training dynamics; in ML domain, such computational validation is normative and acceptable for the claimed contributions.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_accuracy</strong></td>
                            <td>Reported numeric results: CIFAR-10 ResNet-18 accuracy 79.46% (AMSDAS), improvements over baselines (e.g., +4.19% vs standard PGD 76.28% in some configs); robustness under PGD-7 reported (79.46% under PGD with ε=8/255 reported in text). Computational overhead 5–7%.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_details</strong></td>
                            <td>All validation is computational on datasets; adversarial attacks are simulated (PGD/transfer attacks). No physical experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_comparison</strong></td>
                            <td>Direct comparisons to multiple baselines (standard PGD, TRADES, fast adversarial training, Smooth Adversarial Training) with tables and ablations showing AMSDAS advantages and ablation impacts.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_failures</strong></td>
                            <td>No major failure cases; ablation shows early-layer smoothing captures most benefits and multi-scale adds small gains; higher loss values indicate different optimization target but not a failure.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_cases</strong></td>
                            <td>Consistent improvements in robustness and stability across datasets and architectures; ablation confirms component contributions (e.g., early-layer smoothing accounts for most gains).</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_comparison</strong></td>
                            <td>Benchmarks and attack protocols are community-accepted ground truths for robustness evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_replication</strong></td>
                            <td>Experiments use standard datasets and protocols (PyTorch, PGD-7), enabling reproducibility; paper includes hyperparameters and algorithm pseudocode but full code release is not specified in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Reports additional computational overhead (5–7% during training) and negligible inference overhead (<1%).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_norms</strong></td>
                            <td>In ML robustness, computational adversarial benchmarks (PGD, transfer attacks) and ablations are standard norms; paper adheres to these norms.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Reports means/stds in some tables and statistical analyses; ablation repeats indicate small variation; full confidence intervals are not always provided.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Higher final training loss may indicate different trade-offs; robustness measured on standard datasets may not generalize to all attack types or real-world distribution shifts.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2123.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2123.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Robotic/DigitalTwin Mention</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Robotic Experimentation and Digital-Twin-Based Validation (Alt et al., 2025) — referenced</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Reference to robotic experimentation literature recommending semantic execution tracing and digital twins to log and validate autonomous system actions for trustworthy automated science.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Open, reproducible and trustworthy robot-based experiments with virtual labs and digital-twin-based execution tracing</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Robotic Experimentation with Digital Twins (referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Cited prior work (Alt et al., 2025) advocating for use of robotic platforms, semantic execution traces, and digital twins to create auditable logs for validating autonomous experimental actions.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Robotics / automated laboratory experiments / reproducibility in experimental science</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>experimental</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>The referenced approach involves physical robotic experiments instrumented with semantic execution tracing and paired digital twins (virtual labs) to reproduce and validate autonomous experimental runs; provides ground-truth experimental validation and audit trails.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td>Hybrid (physical experiments complemented by high-fidelity digital twin simulations enabling replay and traceability).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_sufficiency</strong></td>
                            <td>Domain norm for laboratory automation: physical execution plus digital twin tracing is considered strong evidence for reproducibility and validation of automated experimental claims.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_accuracy</strong></td>
                            <td>Not specified in this paper (reference only); Alt et al. discuss traceability/reproducibility rather than numeric accuracy here.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_details</strong></td>
                            <td>This paper only cites Alt et al. as related work; no robotic experiments or digital twin implementations are performed in InfiAgent/InfiHelper in this manuscript.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_comparison</strong></td>
                            <td>Referenced as complementary to computational audits; authors position their framework as enabling integration of such specialized scientific agents and techniques.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_failures</strong></td>
                            <td>Not discussed here (only referenced).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_cases</strong></td>
                            <td>Not detailed in this paper (referenced as prior art emphasizing experimental traceability).</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_comparison</strong></td>
                            <td>Digital twin + robotic execution provides direct experimental ground truth in cited literature; this paper does not implement it.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_replication</strong></td>
                            <td>Reference implies that digital-twin-backed experiments enhance reproducibility; InfiAgent claims structural support for integrating such approaches but does not report replication.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Robotic/digital-twin setups are typically high-cost and resource-intensive; this paper notes such techniques but does not quantify costs.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_norms</strong></td>
                            <td>In experimental sciences, wet-lab/robotic verification with traceability is the gold standard; authors cite these norms.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not provided here; digital twin frameworks typically capture traces and logs to enable post-hoc uncertainty analyses but specifics are in the cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Paper acknowledges that domain-specialized validation (e.g., robotic wet-lab experiments) requires integration of specialized agents and is outside the scope of the InfiAgent demonstration.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_details</strong></td>
                            <td>Cited work suggests a hybrid approach: physical robotic execution + digital twin logging; InfiAgent claims compatibility to integrate such domain-specific validation pipelines.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Open, reproducible and trustworthy robot-based experiments with virtual labs and digital-twin-based execution tracing <em>(Rating: 2)</em></li>
                <li>The ai scientist-v2: Workshop-level automated scientific discovery via agentic tree search <em>(Rating: 2)</em></li>
                <li>AgentGym: Evolving large language model-based agents across diverse environments <em>(Rating: 1)</em></li>
                <li>Richelieu: Self-evolving llm-based agents for ai diplomacy <em>(Rating: 1)</em></li>
                <li>Automated design of agentic systems <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2123",
    "paper_id": "paper-281658310",
    "extraction_schema_id": "extraction-schema-54",
    "extracted_data": [
        {
            "name_short": "InfiAgent Exec-Audit",
            "name_full": "InfiAgent Execution-Level Dual-Audit (Execution-Level Audit)",
            "brief_description": "An internal, continuous execution-level validator that monitors every agent output during runtime, maintains per-agent quality scores, and updates them via an online validate(output) function to prevent error propagation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "InfiAgent Execution-Level Audit",
            "system_description": "A runtime auditing component of InfiAgent that continuously monitors each agent's intermediate outputs, computes/updates a quality score Q_i for each agent, and invokes a validate(·) function to accept/reject or weight outputs in downstream decisions.",
            "scientific_domain": "AI systems / automated scientific workflows",
            "validation_type": "computational_proof",
            "validation_description": "Continuous automated validation during execution: for agent A_i at time t the system computes Q_i updates: Q_i(t+1) = α·Q_i(t) + (1−α)·validate(O_i(t)). The validate(·) function assesses whether O_i(t) meets expected requirements (format, content constraints, tests). Validation is integrated into agent routing, quality scoring, and branching decisions.",
            "simulation_fidelity": "N/A (this is an internal software/algorithmic validation mechanism rather than a physical simulation); fidelity corresponds to the rigor of validate(·) implementation which can range from lightweight format checks to heavyweight execution/tests.",
            "validation_sufficiency": "Paper frames it as necessary for system stability but does not claim it is universally sufficient for domain-specific scientific claims; sufficiency depends on validate(·) detail (unit tests, execution checks) and downstream human or domain-level checks.",
            "validation_accuracy": "Not numerically specified; accuracy depends on validate(·) implementation and historical Q_i calibration. No explicit error rates provided.",
            "experimental_validation_performed": false,
            "experimental_validation_details": "No physical experiments; validation is implemented and exercised within framework runs and benchmark comparisons in the paper, but the paper does not provide quantitative standalone evaluation of validate(·) false-positive/false-negative rates.",
            "validation_comparison": "No explicit comparison between different execution-level validation strategies is presented; it is compared conceptually to 'post-hoc validation' and presented as superior because it prevents error propagation earlier.",
            "validation_failures": "Paper notes general risks of error propagation if auditing is absent, but no concrete instances of execution-level audit failure are documented.",
            "validation_success_cases": "Cited as a contributor to improved stability and to InfiAgent's benchmark performance; no isolated metric exclusively attributable to execution-level audit is provided.",
            "ground_truth_comparison": "Validation is internal (checks against expected outputs or test cases), not against external wet-lab ground truth; when used for code tasks, judge_agent runs code tests as part of execution-level checks per system design.",
            "reproducibility_replication": "The paper describes the mechanism but does not report independent replication of validate(·) outcomes; reproducibility depends on validate(·) specifications and available artifacts.",
            "validation_cost_time": "Overhead implied (continuous checks and Q updates), but no quantitative runtime or cost metrics for execution-level audit alone are given.",
            "domain_validation_norms": "Framed as a software-engineering norm: automated unit/integration-like checks during pipeline execution; domain norms (e.g., experimental wet-lab verification) are not replaced by this mechanism.",
            "uncertainty_quantification": "Yes — per-agent quality score Q_i serves as internal uncertainty/confidence signal updated via exponential smoothing with parameter α.",
            "validation_limitations": "Effectiveness depends entirely on the design of validate(·); lightweight format checks may pass malformed scientific claims; no guarantees on correctness beyond tested dimensions.",
            "hybrid_validation_approach": true,
            "hybrid_validation_details": "Execution-level audit is one layer within a dual-audit system (paired with system-level audits and judge models) and is combined with downstream retrospective/system-level reviews and judge-model assessments in the framework.",
            "uuid": "e2123.0"
        },
        {
            "name_short": "InfiAgent Sys-Audit",
            "name_full": "InfiAgent System-Level Dual-Audit (System-Level Audit & Retrospective Summarization)",
            "brief_description": "A higher-level auditing layer that performs retrospective summarization, built-in reviews, and context summarization to maintain system stability and prevent error propagation across multi-stage workflows.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "InfiAgent System-Level Audit",
            "system_description": "A system-level review mechanism that performs retrospective summarization, context summarization (to shorten token contexts), and global stability checks; complements execution-level audits to enforce overall workflow correctness and context efficiency.",
            "scientific_domain": "AI systems / automated scientific workflows",
            "validation_type": "computational_proof",
            "validation_description": "System-level processes perform review passes over completed subworkflows, summarize context, and check for consistency/stability. It enforces structured output formatting and retrospective summarization to catch propagated errors and reduce context size.",
            "simulation_fidelity": "N/A (high-level software/logical auditing); fidelity depends on the thoroughness of retrospective checks.",
            "validation_sufficiency": "Presented as necessary for multi-stage workflow reliability; paper acknowledges that system-level audit helps prevent error propagation but does not claim sufficiency for domain-level scientific validation (e.g., empirical claims require domain validation).",
            "validation_accuracy": "No numerical metrics provided for system-level audit accuracy.",
            "experimental_validation_performed": false,
            "experimental_validation_details": "Used in framework experiments and case study pipelines, but no separate experimental protocol quantifying its detection power is reported.",
            "validation_comparison": "Contrasted qualitatively with post-hoc validation; system-level audit is described as continuous and retrospective and therefore superior at catching cascade errors, but no head-to-head experimental comparison is given.",
            "validation_failures": "No concrete failure examples are provided; limitations are implicit (dependence on summarization fidelity and the heuristics used).",
            "validation_success_cases": "Attributed to overall system stability and benchmark performance gains, but not separately measured.",
            "ground_truth_comparison": "System-level audit checks for internal consistency, not external experimental ground truth.",
            "reproducibility_replication": "Mechanism described; no independent replication study provided.",
            "validation_cost_time": "Intended to save tokens and improve context efficiency; trade-offs between audit thoroughness and runtime not quantified.",
            "domain_validation_norms": "Serves software/agent-system norms (review processes); does not substitute domain-specific empirical validation.",
            "uncertainty_quantification": "Implicit via aggregated quality metrics and summarization outputs; no explicit uncertainty model beyond Q_i aggregates is described.",
            "validation_limitations": "Relies on compressed summarization which may omit critical details; quality depends on summarization fidelity and review heuristics.",
            "hybrid_validation_approach": true,
            "hybrid_validation_details": "System-level audit operates in tandem with execution-level validation and judge-model checks to form the dual-audit mechanism.",
            "uuid": "e2123.1"
        },
        {
            "name_short": "Judge-Model J",
            "name_full": "Judge Model (J) for Model-Level Evolution and Validation",
            "brief_description": "A computational judge model that evaluates candidate model outputs and marks contributions as effective (J(Δm)=1) for merging into the main branch, used during model-level self-evolution as an automated validator.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "InfiAgent Judge Model J",
            "system_description": "Automated judge used in the self-evolution Git-style workflow to evaluate outputs of multiple parallel lightweight models; only contributions judged effective by J are merged into the main branch and used for training.",
            "scientific_domain": "AI systems / model development and automated ML",
            "validation_type": "computational_proof",
            "validation_description": "Judge model J inspects candidate model contributions ∆m^(t)_i and returns binary assessments J(∆m)=1/0; merging rule B_main(t+1) = merge(B_main(t), {∆m | J(∆m)=1}). The judge is used to select high-quality operations and construct datasets for continual training.",
            "simulation_fidelity": "N/A (algorithmic/model-evaluation fidelity depends on judge model capacity and criteria; not described as physics-based simulation).",
            "validation_sufficiency": "Paper treats judge-model adjudication as effective for maintaining quality within the model-evolution loop; sufficiency for external scientific truth is not claimed and depends on judge design and domain-level checks.",
            "validation_accuracy": "No numeric accuracy or false-positive/false-negative rates of J are reported.",
            "experimental_validation_performed": false,
            "experimental_validation_details": "Judge model is described as operational in experiments (used in self-evolution), but the paper provides no standalone validation metrics for J's decisions.",
            "validation_comparison": "Judge-based selection is compared conceptually to purely competitive selection; system also retrains candidate models on merged validated data, but quantitative comparisons isolating judge impact are not provided.",
            "validation_failures": "No specific failure cases of judge model selection are presented.",
            "validation_success_cases": "Used in the framework's self-evolution to produce improved functional agents and domain-specialized models over time as claimed by the authors; direct causal metrics for judge success are not separated.",
            "ground_truth_comparison": "Judge decisions are internal and used to generate training data; not directly compared to external gold standards in the paper.",
            "reproducibility_replication": "Mechanism described; reproducibility contingent on sharing judge model and criteria which are not fully specified in the paper.",
            "validation_cost_time": "Implied overhead from running judge model evaluations across parallel candidates; no numeric cost/time reported.",
            "domain_validation_norms": "Model selection via automated judges is an increasing norm for self-improving systems, but domain validation still requires external verification for scientific claims.",
            "uncertainty_quantification": "Judge is presented as binary evaluator in equations; no probabilistic uncertainty output is described.",
            "validation_limitations": "Binary judge decisions can propagate judge bias; effectiveness depends on judge model fidelity and training data.",
            "hybrid_validation_approach": true,
            "hybrid_validation_details": "Judge model serves as a computational filter within a hybrid pipeline that also includes execution/system audits and (where applicable) human peer review.",
            "uuid": "e2123.2"
        },
        {
            "name_short": "InfiHelper Peer Review",
            "name_full": "InfiHelper Human Peer-Review Validation",
            "brief_description": "Human peer review of papers generated by the InfiHelper research assistant; used as a domain-level validation of scientific-quality outputs (recognized by human reviewers at top-tier IEEE conferences).",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "InfiHelper Peer-Review Validation",
            "system_description": "InfiHelper-generated research outputs were subjected to human peer-review-style evaluation using reviewers and a reviewer model (Claude-3.7-sonnet) and the paper claims these outputs received recognition from human reviewers at top-tier IEEE venues.",
            "scientific_domain": "Scientific publishing / research automation across engineering domains",
            "validation_type": "fabricated",
            "validation_description": "Validation performed via human peer review processes and an automated reviewer proxy (Claude-3.7-sonnet) for systematic quality assessment; the authors performed a structured reviewer evaluation comparing InfiHelper outputs to other AI systems using peer-review criteria (scores 1–10).",
            "simulation_fidelity": "N/A (human-review assessment is a qualitative domain standard; the reviewer-model is an LLM-based proxy and thus computational but not a physics/experimental simulation).",
            "validation_sufficiency": "Peer review is a domain normative validation for scientific quality, but the paper acknowledges peer review evaluates manuscript quality, not empirical truth of scientific claims; experimental claims would still require domain-specific empirical validation.",
            "validation_accuracy": "Reported average reviewer score for InfiHelper outputs: 6.0 (on 1–10 scale); comparative system scores reported (e.g., Sakana-AI 4.0, HKU 5.0). No false-positive/negative rates provided.",
            "experimental_validation_performed": false,
            "experimental_validation_details": "No wet-lab or field experiments performed for validating scientific claims of generated papers; validation here refers to human reviewers judging manuscript quality, methodology, and rigor.",
            "validation_comparison": "Paper compares InfiHelper outputs to outputs from AI-Researcher, Zochi, AI-Scientist V2 through peer-review-style scoring; InfiHelper scored higher on average (6.0) in this evaluation.",
            "validation_failures": "Paper does not report specific peer-review rejections or major failure cases, though it notes rigorous reviewer criteria (including file naming, format compliance) matter; it does not claim acceptance of generated papers themselves, only 'recognition' from reviewers.",
            "validation_success_cases": "Authors claim generated papers received recognition from human reviewers at top-tier IEEE conferences and that InfiHelper outperformed other AI-research assistants in reviewer scoring in their evaluation.",
            "ground_truth_comparison": "Peer review compares manuscript quality against reviewer expectations and norms rather than experimental ground truth.",
            "reproducibility_replication": "Peer-review outcomes depend on reviewers and review conditions; the paper provides sample outputs in appendix but does not report independent external replication of peer-review results.",
            "validation_cost_time": "Human review is time-consuming; paper used Claude-3.7-sonnet for automatic peer-review-style scoring to scale evaluation, but no explicit time/cost numbers are given for human reviewing.",
            "domain_validation_norms": "In research, peer review is standard to validate novelty, rigor, and presentation; the paper acknowledges that peer review is necessary but not sufficient for empirical validation of discoveries.",
            "uncertainty_quantification": "Reviewer scores and comparative averages provide some measure of confidence; no formal uncertainty model applied to peer-review judgments.",
            "validation_limitations": "Peer review assesses quality/presentation and methodology but does not substitute for independent experimental verification of scientific claims; risk of superficial or format-compliant but incorrect claims remains.",
            "hybrid_validation_approach": true,
            "hybrid_validation_details": "Combined automated judge/reviewer model (Claude-3.7-sonnet) evaluations with human reviewer recognition and internal audits (dual-audit) to validate research outputs.",
            "uuid": "e2123.3"
        },
        {
            "name_short": "Benchmark Eval",
            "name_full": "Benchmark-based Computational Validation (DROP, HumanEval, MBPP, GSM8K, MATH)",
            "brief_description": "Validation of InfiAgent using standard computational benchmarks across reasoning, coding, and math (DROP, HumanEval, MBPP, GSM8K, MATH) to compare performance against baselines.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "InfiAgent Benchmark Validation",
            "system_description": "InfiAgent was evaluated on multiple community benchmarks (DROP, HumanEval, MBPP, GSM8K, MATH) and compared to state-of-the-art baselines to validate reasoning and coding performance; reported average improvements (e.g., 9.9% over ADAS).",
            "scientific_domain": "Natural language reasoning, program synthesis, mathematical problem solving (AI/NLP/ML evaluation)",
            "validation_type": "simulated",
            "validation_description": "Compute-centric validation: run the system on benchmark datasets with standardized metrics and compare scores to baselines and ablations; includes tables of per-benchmark scores and averaged metrics.",
            "simulation_fidelity": "Empirical/benchmark-level fidelity: high for measuring algorithmic performance on dataset tasks but not a surrogate for empirical scientific discovery validation; benchmark results are reproducible and accepted norms in AI evaluation.",
            "validation_sufficiency": "Sufficient to quantify relative algorithmic capabilities on specific tasks (reasoning, coding, math) but not sufficient to validate domain-level scientific claims (e.g., real-world experiments).",
            "validation_accuracy": "Reported per-benchmark numeric scores (example highlights): DROP 82.4%, GSM8K 93.1%, MATH 35.6%; overall claimed 9.9% average improvement vs. ADAS. No uncertainty intervals provided for all metrics, though some tables include means/std.",
            "experimental_validation_performed": false,
            "experimental_validation_details": "All validation here is computational on datasets; no physical experiments.",
            "validation_comparison": "Direct comparisons to multiple baselines (chain-of-thought, self-refinement, ADAS, etc.) are provided; InfiAgent shows higher average performance than ADAS by 9.9% per the authors.",
            "validation_failures": "Paper notes limitations on specialized math (MATH) where InfiAgent underperforms top methods (e.g., MATH 35.6% vs other methods up to ~50.8%), attributing gaps to tool-calling overhead.",
            "validation_success_cases": "Strong results in multi-step reasoning (DROP) and mathematical/coding tasks (GSM8K, HumanEval) where the agent-as-a-tool decomposition and dual-audit aided performance.",
            "ground_truth_comparison": "Benchmarks act as ground-truth tasks with accepted scoring; comparisons to published baselines provide external reference.",
            "reproducibility_replication": "Benchmarks are standard and reproducible; paper does not supply code release in main text (appendix mentions prompts) but benchmarking methodology follows community norms.",
            "validation_cost_time": "Benchmarking consumes compute (involving many agents/models) but no precise resource-time numbers are reported aside from model overhead mentions.",
            "domain_validation_norms": "In AI, benchmark performance is a normative validation for capability claims; paper adheres to this norm for performance claims but recognizes domain-specific empirical validation is separate.",
            "uncertainty_quantification": "Some tables include mean/std (e.g., optimizer study), but benchmark results reported for InfiAgent are point estimates without detailed confidence intervals.",
            "validation_limitations": "Benchmarks are limited to proxy tasks; high benchmark scores do not guarantee correct scientific discovery or experimental validity in domain sciences.",
            "hybrid_validation_approach": false,
            "hybrid_validation_details": null,
            "uuid": "e2123.4"
        },
        {
            "name_short": "CarbonSim",
            "name_full": "Carbon-Aware Adaptive Routing Simulation Validation",
            "brief_description": "Validation of the carbon-aware routing protocol entirely via simulation of the Greater Bay Area transport network, using time-varying traffic multipliers, stochastic variation, and statistical analysis (p-values) to evaluate travel-time and emission trade-offs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Carbon-Aware Adaptive Routing (simulation)",
            "system_description": "A two-layer routing protocol validated in a simulated Greater Bay Area network with traffic scenarios (peak/off-peak/mixed), vehicle types, and randomized traffic variations; performance measured on emissions and travel time.",
            "scientific_domain": "Transportation systems / simulation-based logistics optimization",
            "validation_type": "simulated",
            "validation_description": "Simulated 24-hour traffic cycles with time-based multipliers per hour, congestion multipliers (e.g., night 0.2–0.5x, rush 1.2–1.9x), random ±20% variation, multiple replications (6 per scenario) and statistical tests (e.g., p = 0.3872 for travel-time difference vs Traffic-Adaptive). Metrics include total emissions (kg CO2), travel time (minutes), and carbon-efficiency. Algorithms compared: Baseline, Static Carbon-Aware, Traffic-Adaptive, Carbon-Aware Adaptive.",
            "simulation_fidelity": "Empirical/mesoscopic simulation-level (empirical multipliers + randomized noise). Not physics-first-principles vehicle dynamics; fidelity is moderate and adequate for comparative algorithmic evaluation but limited for precise emission prediction.",
            "validation_sufficiency": "Sufficient for algorithmic evaluation in simulated logistics settings; authors acknowledge simplifications (emission model simplifications, limited traffic prediction) and recommend real-world validation as future work.",
            "validation_accuracy": "Some numeric results provided: e.g., in peak traffic Baseline/Static Carbon-Aware travel time 63.8 min; Carbon-Aware Adaptive emissions 55.14 kg CO2 vs Baseline/Static 46.84 kg CO2; off-peak oddities (higher emissions due to speeds) reported (e.g., Carbon-Aware Adaptive 184.18 kg CO2). Statistical significance reported (p &lt; 0.05 for some comparisons; p = 0.3872 for travel-time vs Traffic-Adaptive).",
            "experimental_validation_performed": false,
            "experimental_validation_details": "No field experiments; authors explicitly state future work includes conducting real-world validation studies.",
            "validation_comparison": "Comparative simulation results across four algorithms with statistical analysis; shows trade-offs and vehicle-type-dependent performance differences.",
            "validation_failures": "Paper documents counter-intuitive simulated failure: higher emissions in off-peak conditions due to higher speeds increasing fuel consumption, indicating limitations of simplified emission assumptions; acknowledges limitations of emission model fidelity.",
            "validation_success_cases": "Carbon-Aware Adaptive achieved balanced performance in mixed conditions (lowest average travel time 75.2 min while maintaining carbon awareness) and statistically comparable travel times to Traffic-Adaptive (p = 0.3872).",
            "ground_truth_comparison": "No comparison to field measurements; simulation findings are not validated against observed traffic or emissions data.",
            "reproducibility_replication": "Simulation framework described (NetworkX + NumPy, parameter tables) enabling reproducibility in simulation but not validated by external replication in the paper.",
            "validation_cost_time": "Simulations across scenarios with replications (6 per scenario) reported but no absolute runtime/cost metrics provided; simulation is lower-cost than field trials but authors recommend real-world validation which would be costlier.",
            "domain_validation_norms": "Authors note domain norm: real-world/field validation required for deployment; simulation is an accepted first step but insufficient for production claims.",
            "uncertainty_quantification": "Random variation ±20% included; statistical testing used (p-values); explicit confidence intervals not always reported but some statistical significance testing is present.",
            "validation_limitations": "Simplified emissions model, limited traffic prediction capabilities, static carbon weight parameter; no real-world measurement validation provided.",
            "hybrid_validation_approach": false,
            "hybrid_validation_details": null,
            "uuid": "e2123.5"
        },
        {
            "name_short": "AMSDAS Eval",
            "name_full": "AMSDAS Adversarial-Training Computational Validation",
            "brief_description": "Extensive computational validation of AMSDAS on image classification benchmarks and adversarial attacks (PGD-7) including ablation, transfer-attack tests, cross-architecture/dataset evaluations, and statistical analysis.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "AMSDAS Validation",
            "system_description": "AMSDAS was validated via standard ML experiments: CIFAR-10 and Fashion-MNIST datasets with ResNet-18 and MobileNetV2, PGD-7 attack (ε=8/255), ablation studies (early-layer smoothing effects), hyperparameter sweeps, and reporting of accuracies, losses, and robustness metrics.",
            "scientific_domain": "Machine learning robustness / adversarial defenses",
            "validation_type": "simulated",
            "validation_description": "Compute experiments included adversarial training and evaluation under PGD attacks, transfer attacks, ablation studies (component contributions), parameter sensitivity, cross-architecture/dataset generalization, and statistical analyses (means, stds, tables). Metrics: clean and robust test accuracies, training loss, and computational overhead percentages (5–7% extra runtime).",
            "simulation_fidelity": "High-fidelity for ML evaluation (standard datasets, standardized attacks like PGD-7); not physical experiments. Results are directly comparable to community baselines.",
            "validation_sufficiency": "Sufficient to support claims about ML robustness and training dynamics; in ML domain, such computational validation is normative and acceptable for the claimed contributions.",
            "validation_accuracy": "Reported numeric results: CIFAR-10 ResNet-18 accuracy 79.46% (AMSDAS), improvements over baselines (e.g., +4.19% vs standard PGD 76.28% in some configs); robustness under PGD-7 reported (79.46% under PGD with ε=8/255 reported in text). Computational overhead 5–7%.",
            "experimental_validation_performed": false,
            "experimental_validation_details": "All validation is computational on datasets; adversarial attacks are simulated (PGD/transfer attacks). No physical experiments.",
            "validation_comparison": "Direct comparisons to multiple baselines (standard PGD, TRADES, fast adversarial training, Smooth Adversarial Training) with tables and ablations showing AMSDAS advantages and ablation impacts.",
            "validation_failures": "No major failure cases; ablation shows early-layer smoothing captures most benefits and multi-scale adds small gains; higher loss values indicate different optimization target but not a failure.",
            "validation_success_cases": "Consistent improvements in robustness and stability across datasets and architectures; ablation confirms component contributions (e.g., early-layer smoothing accounts for most gains).",
            "ground_truth_comparison": "Benchmarks and attack protocols are community-accepted ground truths for robustness evaluation.",
            "reproducibility_replication": "Experiments use standard datasets and protocols (PyTorch, PGD-7), enabling reproducibility; paper includes hyperparameters and algorithm pseudocode but full code release is not specified in main text.",
            "validation_cost_time": "Reports additional computational overhead (5–7% during training) and negligible inference overhead (&lt;1%).",
            "domain_validation_norms": "In ML robustness, computational adversarial benchmarks (PGD, transfer attacks) and ablations are standard norms; paper adheres to these norms.",
            "uncertainty_quantification": "Reports means/stds in some tables and statistical analyses; ablation repeats indicate small variation; full confidence intervals are not always provided.",
            "validation_limitations": "Higher final training loss may indicate different trade-offs; robustness measured on standard datasets may not generalize to all attack types or real-world distribution shifts.",
            "hybrid_validation_approach": false,
            "hybrid_validation_details": null,
            "uuid": "e2123.6"
        },
        {
            "name_short": "Robotic/DigitalTwin Mention",
            "name_full": "Robotic Experimentation and Digital-Twin-Based Validation (Alt et al., 2025) — referenced",
            "brief_description": "Reference to robotic experimentation literature recommending semantic execution tracing and digital twins to log and validate autonomous system actions for trustworthy automated science.",
            "citation_title": "Open, reproducible and trustworthy robot-based experiments with virtual labs and digital-twin-based execution tracing",
            "mention_or_use": "mention",
            "system_name": "Robotic Experimentation with Digital Twins (referenced)",
            "system_description": "Cited prior work (Alt et al., 2025) advocating for use of robotic platforms, semantic execution traces, and digital twins to create auditable logs for validating autonomous experimental actions.",
            "scientific_domain": "Robotics / automated laboratory experiments / reproducibility in experimental science",
            "validation_type": "experimental",
            "validation_description": "The referenced approach involves physical robotic experiments instrumented with semantic execution tracing and paired digital twins (virtual labs) to reproduce and validate autonomous experimental runs; provides ground-truth experimental validation and audit trails.",
            "simulation_fidelity": "Hybrid (physical experiments complemented by high-fidelity digital twin simulations enabling replay and traceability).",
            "validation_sufficiency": "Domain norm for laboratory automation: physical execution plus digital twin tracing is considered strong evidence for reproducibility and validation of automated experimental claims.",
            "validation_accuracy": "Not specified in this paper (reference only); Alt et al. discuss traceability/reproducibility rather than numeric accuracy here.",
            "experimental_validation_performed": null,
            "experimental_validation_details": "This paper only cites Alt et al. as related work; no robotic experiments or digital twin implementations are performed in InfiAgent/InfiHelper in this manuscript.",
            "validation_comparison": "Referenced as complementary to computational audits; authors position their framework as enabling integration of such specialized scientific agents and techniques.",
            "validation_failures": "Not discussed here (only referenced).",
            "validation_success_cases": "Not detailed in this paper (referenced as prior art emphasizing experimental traceability).",
            "ground_truth_comparison": "Digital twin + robotic execution provides direct experimental ground truth in cited literature; this paper does not implement it.",
            "reproducibility_replication": "Reference implies that digital-twin-backed experiments enhance reproducibility; InfiAgent claims structural support for integrating such approaches but does not report replication.",
            "validation_cost_time": "Robotic/digital-twin setups are typically high-cost and resource-intensive; this paper notes such techniques but does not quantify costs.",
            "domain_validation_norms": "In experimental sciences, wet-lab/robotic verification with traceability is the gold standard; authors cite these norms.",
            "uncertainty_quantification": "Not provided here; digital twin frameworks typically capture traces and logs to enable post-hoc uncertainty analyses but specifics are in the cited work.",
            "validation_limitations": "Paper acknowledges that domain-specialized validation (e.g., robotic wet-lab experiments) requires integration of specialized agents and is outside the scope of the InfiAgent demonstration.",
            "hybrid_validation_approach": true,
            "hybrid_validation_details": "Cited work suggests a hybrid approach: physical robotic execution + digital twin logging; InfiAgent claims compatibility to integrate such domain-specific validation pipelines.",
            "uuid": "e2123.7"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Open, reproducible and trustworthy robot-based experiments with virtual labs and digital-twin-based execution tracing",
            "rating": 2
        },
        {
            "paper_title": "The ai scientist-v2: Workshop-level automated scientific discovery via agentic tree search",
            "rating": 2
        },
        {
            "paper_title": "AgentGym: Evolving large language model-based agents across diverse environments",
            "rating": 1
        },
        {
            "paper_title": "Richelieu: Self-evolving llm-based agents for ai diplomacy",
            "rating": 1
        },
        {
            "paper_title": "Automated design of agentic systems",
            "rating": 1
        }
    ],
    "cost": 0.0244645,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>SELF-EVOLVING PYRAMID AGENT FRAMEWORK FOR INFINITE SCENARIOS
30 Sep 2025</p>
<p>Chenglin Yu 
The Hong Kong University</p>
<p>Yang Yu 
The Hong Kong Polytechnic University</p>
<p>Songmiao Wang 
The Hong Kong Polytechnic University</p>
<p>Yuchen Wang 
Yifan Yang 
The Hong Kong Polytechnic University</p>
<p>Jinjia Li 
The Hong Kong Polytechnic University</p>
<p>Ming Li 
The Hong Kong Polytechnic University</p>
<p>Hongxia Yang 
The Hong Kong Polytechnic University</p>
<p>SELF-EVOLVING PYRAMID AGENT FRAMEWORK FOR INFINITE SCENARIOS
30 Sep 2025E9623CF872CF4F0421B5D59C1D984AC3arXiv:2509.22502v2[cs.AI]
Large Language Model (LLM) agents have demonstrated remarkable capabilities in organizing and executing complex tasks, and many such agents are now widely used in various application scenarios.However, developing these agents requires carefully designed workflows, carefully crafted prompts, and iterative tuning, which requires LLM techniques and domain-specific expertise.These handcrafted limitations hinder the scalability and cost-effectiveness of LLM agents across a wide range of industries.To address these challenges, we propose InfiAgent, a Pyramid-like DAG-based Multi-Agent Framework that can be applied to infinite scenarios, which introduces several key innovations: a generalized "agentas-a-tool" mechanism that automatically decomposes complex agents into hierarchical multi-agent systems; a dual-audit mechanism that ensures the quality and stability of task completion; an agent routing function that enables efficient taskagent matching; and an agent self-evolution mechanism that autonomously restructures the agent DAG based on new tasks, poor performance, or optimization opportunities.Furthermore, InfiAgent's atomic task design supports agent parallelism, significantly improving execution efficiency.This framework evolves into a versatile pyramid-like multi-agent system capable of solving a wide range of problems.Evaluations on multiple benchmarks demonstrate that InfiAgent achieves 9.9% higher performance compared to ADAS (similar auto-generated agent framework), while a case study of the AI research assistant InfiHelper shows that it generates scientific papers that have received recognition from human reviewers at top-tier IEEE conferences.</p>
<p>INTRODUCTION</p>
<p>The rapid development of large-scale language models (LLMs) has ushered in a new era of intelligent automation (Naveed et al., 2025;Tran et al., 2025), with agent-based systems demonstrating remarkable capabilities in organizing and executing complex tasks across domains.From scientific research and software development to creative content generation and business process automation, LLM agents are transforming how we solve problems at scale.However, the development and deployment of these agents face significant challenges, limiting their widespread adoption and effectiveness.</p>
<p>Current approaches to building LLM agents rely heavily on carefully designed workflows, carefully crafted prompts, and extensive iterative tuning-processes that require deep LLM expertise and domain-specific knowledge (Veeramachaneni, 2025;Guo et al., 2024;Annam et al., 2025;Schick et al., 2023).This reliance on handcrafted solutions creates a fundamental scalability barrier: each new application requires significant manual intervention, making it difficult to rapidly deploy agents across diverse industries and use cases.Furthermore, as system complexity increases, existing multiagent systems often suffer from unpredictable interactions, resource conflicts, and emergent behavioral instabilities.</p>
<p>A core challenge lies in the lack of a principled, generalizable framework that can automatically decompose complex tasks, ensure system stability, and enable autonomous adaptation.Traditional multi-agent architectures typically adopt a point-to-point collaboration model that allows unrestricted agent interactions (Sun et al., 2025;Ning &amp; Xie, 2024).This leads to coordination overhead, deadlock situations, and difficulty maintaining predictable system behavior.While recent agent frameworks (Hu et al., 2025;Dang et al., 2025) have demonstrated the potential for end-to-end automation, they are still limited by their reliance on manually written templates and lack the system reasoning capabilities required for robust, scalable deployment.</p>
<p>To address these fundamental limitations, we introduce InfiAgent, a DAG-based multi-agent framework that represents a paradigm shift in how we conceptualize and implement agent-based systems.InfiAgent is designed as a general framework that automatically adapts to diverse problem domains without requiring extensive manual configuration or domain-specific expertise.Our approach is based on four key innovations that collectively enable scalable, stable, and self-evolving agent systems.</p>
<p>First, we introduce a generalized "agent-as-a-tool" mechanism that fundamentally reimagines how agents interact and collaborate.This mechanism enables automatic decomposition of complex agents into hierarchical multi-agent systems, where higher-level agents can seamlessly invoke lower-level agents as specialized tools.This abstraction allows for unprecedented modularity and reusability, enabling the same underlying framework to handle tasks ranging from scientific research to software engineering.</p>
<p>Second, we propose a dual-audit mechanism that ensures both quality and stability of task completion.Unlike traditional approaches that rely on post-hoc validation, our dual-audit system provides continuous monitoring and verification throughout the execution process, preventing error propagation and ensuring reliable outcomes even in complex, multi-stage workflows.</p>
<p>Third, we develop an intelligent agent routing function that enables efficient task-agent matching without requiring manual configuration.This routing mechanism automatically identifies the most appropriate agents for specific subtasks, optimizing resource utilization and execution efficiency while maintaining system coherence.</p>
<p>Fourth, we introduce an agent self-evolution mechanism that enables autonomous restructuring of the agent tree based on performance feedback, new task requirements, and optimization opportunities.This capability allows InfiAgent to continuously improve its effectiveness and adapt to changing requirements without human intervention.</p>
<p>These innovations collectively enable InfiAgent to evolve into a versatile, pyramid-like multi-agent system capable of solving a wide range of problems while maintaining stability, efficiency, and adaptability.The framework's atomic task design supports agent parallelism, significantly improving execution efficiency, while its DAG structure ensures predictable behavior and resource utilization.</p>
<p>Our contributions are as follows:</p>
<p>(1) Universal Agent Framework with Agent-as-a-Tool Abstraction: We present InfiAgent-a DAG-based multi-agent framework that provides a principled approach to building scalable, stable, and self-evolving agent systems.This framework introduces a generalized mechanism that can automatically decompose complex agents into hierarchical structures with unrestricted depth, ensuring demand alignment and stability under massive agent populations through tool constraints and behavioral constraints.This design enables unprecedented modularity and reusability across diverse application domains.</p>
<p>(2) Dual-Audit Quality Assurance Mechanism: We propose a comprehensive auditing mechanism that ensures both quality and stability of task completion through continuous monitoring and verification.The dual-layer auditing system operates at both execution and system levels, preventing error propagation and ensuring reliable outcomes in complex multi-stage workflows while maintaining context efficiency through structured output formatting and retrospective summarization.</p>
<p>(3) Intelligent Task Routing and Context Control: We develop an automated routing system that enables efficient task-agent matching and resource optimization without manual configuration.The framework implements lightweight communication protocols where agents exchange only file descriptors and metadata, combined with sophisticated context management that decomposes execution context into system prompts, memory indices, shared memory, and compressed environment interactions.</p>
<p>(4) Self-Evolution Capability: We design an autonomous restructuring mechanism that enables agents to dynamically optimize their DAG topology and internal configurations based on performance feedback and changing requirements.This evolution operates at multiple levels including model-level evolution through Git-style workflows, agent-level evolution using high-quality training data, and topology-level evolution that enables domain-specific expert model formation and dynamic architecture adaptation.</p>
<p>Through extensive evaluation and a detailed case study of InfiHelper, we demonstrate that InfiAgent addresses the fundamental scalability and stability challenges that have hindered the widespread adoption of multi-agent systems, opening new possibilities for intelligent automation across diverse domains.</p>
<p>RELATED WORK 2.1 MULTI-AGENT SYSTEM ARCHITECTURES</p>
<p>Traditional multi-agent systems have primarily focused on peer-to-peer collaboration models, where agents interact freely to achieve common goals.While effective for simple coordination tasks, these approaches struggle with complex, hierarchical task decomposition.Recent work has explored various architectural patterns, including centralized coordination (GUL et al., 2022), hierarchical organization (LIU et al., 2025), and emergent behavior (HAN, 2022).Recent taxonomic work by Moore (Moore, 2025) has further categorized Hierarchical Multi-Agent Systems (HMAS) along dimensions of control, information flow, and temporal leveling, highlighting trade-offs between global efficiency and local autonomy in industrial settings such as power grids.Furthermore, comprehensive reviews note a persistent challenge in designing unified agents that seamlessly integrate cognition, planning, and interaction, despite advances in hierarchical reinforcement learning and LLM-based reasoning (Qu, 2025).However, these approaches often lack built-in mechanisms for stability-constrained composition, a gap our architecture explicitly addresses.</p>
<p>TASK DECOMPOSITION IN MULTI-AGENT SYSTEMS</p>
<p>Task decomposition has been a central challenge in multi-agent system design.Existing approaches include goal-oriented decomposition (LI et al., 2023), constraint-based decomposition (CAI et al., 2017), and learning-based decomposition (WOO et al., 2022).Increasingly, knowledge graphs (KGs) are being leveraged to provide a structured foundation for task decomposition and agent coordination.Frameworks like AGENTiGraph demonstrate how multi-agent systems can dynamically interpret user intents and manage tasks by integrating and querying against a background KG, significantly improving performance in complex, domain-specific scenarios (ZHAO et al., 2025;GAO et al., 2025).While promising, such KG-dependent approaches often presuppose the existence of a high-quality knowledge base and can be constrained by its scope.However, they, along with the more traditional methods, often lack guarantees regarding system stability and resource utilization.In contrast, our method enforces stability through strict compositional constraints and built-in review mechanisms, ensuring robustness even in knowledge-sparse environments or for novel tasks beyond the initial KG design.</p>
<p>AGENT EVOLUTION AND SELF-ADAPTATION</p>
<p>Recent studies emphasize that scalable multi-agent systems must incorporate mechanisms of evolution and adaptation.Frameworks such as AgentGym and Richelieu highlight self-improving LLM agents in dynamic environments (Xi et al., 2024;Guan et al., 2024), while EvoAgent and coevolutionary theories extend evolutionary computation to automatic agent generation and emergent cooperation (Yuan et al., 2024;De Zarzà et al., 2023).Complementary efforts focus on behavioral adaptation, including continual preference alignment (CPPO) and symbolic self-refinement (Zhang et al., 2024;Zhou et al., 2024).At a broader scope, surveys of evolutionary computation (Chen et al., 2025) and collaborative architecture design frameworks such as NADER (Yang et al., 2025) Figure 1: InfiAgent Framework Architecture.Left: An intuitive Pyramid-like agent organization architecture of InfiAgent, featuring a Router that redirects all user queries to avoid layer-by-layer tool search.Right: InfiAgent's framework workflow schematic, highlighting three key modules: Router, Self Evolution, and Context Control.</p>
<p>underscore that both algorithmic and structural evolution are central to building resilient agent systems.These advances directly motivate InfiAgent's design of self-restructuring DAG topologies with performance-driven adaptation.</p>
<p>AUTOMATED SCIENTIFIC RESEARCH</p>
<p>Recent advances in automated scientific research have demonstrated the potential for AI-driven discovery processes.Systems like AI Scientist frameworks have shown that end-to-end automation is feasible, but they often rely on human-authored templates and lack the systematic exploration capabilities that characterize high-quality human research.Beyond the reliance on templates, a critical challenge for trustworthy automated science is ensuring transparency and reproducibility.Recent efforts, such as those in robotic experimentation, emphasize semantic execution tracing and digital twins to log and validate autonomous system actions (Alt et al., 2025).Concurrently, research in data-scarce domains like biochemistry is exploring techniques such as 'pocket similarity' for data augmentation to predict functional enzymes, pushing the boundaries of autonomous discovery (Anonymous, 2025).Our Multi-Level Agent Architecture incorporates the principles of transparency and rigorous validation through its retrospective summarization processes.Furthermore, by structurally enforcing stability and systematic composition, our framework provides a reliable foundation for integrating such specialized scientific agents and techniques into a robust, end-to-end workflow.</p>
<p>INFIAGENT</p>
<p>As illustrated in Fig. 1, the InfiAgent framework is designed as a DAG-based decomposition and routing system.Unlike conventional agent architectures that emphasize direct execution, the majority of agents in InfiAgent specialize in planning and routing tasks, while the actual task execution is eventually delegated to the functional agent group at the bottom level.Each higher-level agent thus acts as a planner for a small set of specialized lower-level agents, orchestrating their collaboration and merging their results to complete the assigned task.</p>
<p>AGENT-AS-A-TOOL MECHANISM AND INTELLIGENT TASK ROUTING</p>
<p>The central principle of InfiAgent is agent-as-a-tool decomposition and intelligent routing.When a task T 0 is submitted to the top-level agent α, the agent automatically identifies suitable lower-level agents {A 1 , A 2 , . . ., A k } and reformulates the task into sub-tasks {T 1 , T 2 , . . ., T k }.Each lowerlevel agent is treated as a specialized tool that can be invoked by higher-level agents.If a selected agent cannot execute directly, it further decomposes its sub-task and routes it downward using the same agent-as-a-tool mechanism.This process continues until tasks reach the functional level.</p>
<p>Formally, the decomposition can be expressed as:
T (l) → {T (l+1) 1 , T (l+1) 2 , . . . , T (l+1) k l },(1)
where T (l) is the task handled by a level-l agent, and each T (l+1) j</p>
<p>is delegated to a level-(l + 1) agent.The decomposition depth L ensures:
L l=0 j T (l) j = T 0 ,(2)
and the atomic tasks {T (L) j } are executed only by functional agents.To maintain the simplicity of each agent, InfiAgent imposes a strict constraint:
k l ≤ K max , with K max ≪ N,(3)
where k l is the fan-out of an agent at level l, and N is the total number of agents.Typically, K max = 5, ensuring that no agent faces overwhelming coordination complexity even as the system grows exponentially with depth.</p>
<p>ARCHITECTURE SCALABILITY</p>
<p>Unlike shallow architectures (two or three layers), InfiAgent leverages depth to accommodate exponentially many functional agents without overburdening any single planner.If the average branching factor is b, then the number of functional agents reachable at depth L is:
N func ≈ b L . (4)
This exponential growth grants the top-level agent broad generalization capacity while ensuring that each intermediate agent only reasons about a bounded number of children.Consequently, complex tasks can be mapped into workflows involving vast numbers of specialized agents while preserving stability and modularity.</p>
<p>DUAL-AUDIT QUALITY ASSURANCE MECHANISM</p>
<p>InfiAgent implements a comprehensive dual-audit mechanism that ensures both quality and stability of task completion while optimizing context management.This mechanism operates at two levels:</p>
<p>Execution-Level Audit: During task execution, each agent's output is continuously monitored and validated to ensure that every agent obtains the expected output.The system maintains a quality score Q i for each agent A i based on historical performance and current output quality.Formally, the execution audit can be expressed as:
Q (t+1) i = α • Q (t) i + (1 − α) • validate(O (t) i ),(5)
where
O (t)
i is the output of agent A i at time t, and validate(•) is a quality assessment function that verifies whether the output meets the expected requirements.</p>
<p>System-Level Audit: At the system level, InfiAgent maintains stability through built-in review mechanisms and retrospective summarization.Additionally, the system-level audit performs context summarization to keep the context short and save tokens.This dual-layer auditing mechanism prevents error propagation and ensures reliable outcomes in complex multi-stage workflows.</p>
<p>COMMUNICATION AND CONTEXT CONTROL</p>
<p>A key design choice in InfiAgent is lightweight communication.Agents do not exchange full intermediate results but only file descriptors and metadata within a shared workspace.Formally, the message from agent A i to agent A j is:
M i→j = (addr, desc), (6)
where addr is a pointer to the stored result and desc is its description.This strategy eliminates the need for maintaining large shared contexts, ensures independence of agent reasoning, and reduces communication overhead.</p>
<p>Context Management Mechanism: InfiAgent's context management is fundamentally derived from its principle of lightweight communication, where only pointers and descriptors are exchanged rather than raw content.Consequently, the execution context C of an agent is decomposed into four structured components:
C = {C sys , C LM , C SM , C ENV },(7)
where each term corresponds to a specific type of contextual information:</p>
<ol>
<li>System Prompt Context (C sys ): Predefined prompts (either manually designed or automatically generated) that guide agent behavior.</li>
</ol>
<p>2.</p>
<p>Long-term Memory Index (C LM ): Instead of embedding complete historical logs into the prompt, InfiAgent maintains compressed descriptors and indices of files in the workspace.Formally,
C LM = compress {d(f i ) | f i ∈ F} , (8)
where F is the file space and d(f i ) denotes the description of file f i .This enables retrievalaugmented recall without bloating the token context.</p>
<ol>
<li>Short-term Shared Memory (C SM ): Maintained as a dynamic call stack S, recording the active invocation tree of agents.
C SM (t) = {(A p , role p ) | A p ∈ S(t)},(9)
which ensures that only the currently activated task tree is visible across agents, preventing unnecessary propagation of unrelated history.</li>
</ol>
<p>4.</p>
<p>Compressed Environment Interaction Context (C ENV ): Captures tool invocation trajectories, results, and feedback.When the token length approaches a threshold τ , automatic compression is applied:
C (t+1) ENV = compress C (t) ENV ∪ I t , if |C ENV | &gt; τ,
(10) where I t denotes the latest interaction record.By enforcing the constraint that only (addr, desc) pairs are transmitted among agents, InfiAgent ensures that the overall context length remains bounded even under long-running tasks:
|C| ≪ |H|,(11)
where H denotes the full historical log of the system.Thus, the framework achieves scalable, efficient, and self-retrievable context management fully reliant on autonomous file-space search rather than direct prompt accumulation.</p>
<p>SELF-EVOLUTION MECHANISM</p>
<p>InfiAgent implements an autonomous restructuring mechanism that enables agents to dynamically optimize their DAG topology and internal configurations based on performance feedback and changing requirements.This self-evolution capability is realized through a Git-style model evolution workflow that operates at multiple levels:</p>
<p>Model-Level Evolution: At the functional level, each task is executed in parallel by multiple lightweight models (and a few larger models).Their progress and quality are periodically evaluated by a judge model J, and the main branch B main is updated by merging the effective contributions:
B (t+1) main = merge B (t) main , {∆m (t) i | J(∆m (t) i ) = 1} . (12)
Agent-Level Evolution: In addition to branch selection, the main branch also serves as a repository of high-quality training data.All parallel models are continuously updated using the accumulated data from the main branch:
m (t+1) i ← train m (t) i , D(B (t) main ) ,(13)
where D(B (t) main ) denotes the dataset extracted from the validated operations and results stored on the main branch.This mechanism ensures that every candidate model evolves dynamically, rather than relying solely on competition outcomes.</p>
<p>Topology-Level Evolution: Over time, branches that consistently deliver high-quality results dominate, while weaker ones are pruned.As a result, functional agents progressively evolve specialized lightweight models.Furthermore, similar functions can be fused upward to form domain-level expert models, providing both efficiency and specialization.The DAG topology itself can be restructured based on performance patterns and new task requirements, enabling the system to adapt its architecture dynamically.</p>
<p>EXPERIMENTS</p>
<p>BENCHMARK EVALUATION</p>
<p>To comprehensively evaluate InfiAgent's performance across diverse reasoning tasks, we conducted extensive experiments on five widely-used benchmarks spanning different cognitive capabilities: DROP (Dua et al., 2019), HumanEval (Hendrycks et al., 2021b), MBPP (Austin et al., 2021), GSM8K (Cobbe et al., 2021), and MATH (Hendrycks et al., 2021a).Our evaluation compares InfiAgent against state-of-the-art baselines including chain-of-thought reasoning, self-refinement, and other advanced prompting techniques.which consumes model capacity that could be directed toward direct mathematical reasoning.For challenging but non-complex problems requiring focused deduction rather than multi-step decomposition, a single-agent approach proves more efficient.</p>
<p>Framework Advantages: Despite this limitation, InfiAgent demonstrates a 9.9% average improvement over ADAS.The architecture naturally supports heterogeneous model collaboration, allowing each functional agent to utilize specialized models optimized for specific domains.While our benchmarks standardized the backbone model for fair comparison, real-world deployments can leverage this flexibility to achieve superior overall performance.</p>
<p>CASE STUDY: INFIHELPER RESEARCH ASSISTANT</p>
<p>As shown in Fig. 2, InfiHelper represents a comprehensive implementation of the InfiAgent framework, demonstrating how the DAG-based architectural principles enable the automation of complex scientific research processes.The system implements a complete research pipeline from idea generation to paper publication, showcasing InfiAgent's ability to handle complex, multi-stage workflows while maintaining system stability and output quality.Notably, our paper has received recognition from human reviewers at top-tier IEEE conferences, validating the significance and quality of our research contributions.</p>
<p>The InfiHelper system is available for most of the existing research domains.Through extensive testing and validation, the system has demonstrated significant improvements in research efficiency, output quality, and system stability compared to traditional approaches.The case study provides concrete evidence of InfiAgent's effectiveness in real-world applications.</p>
<p>INFIHELPER IMPLEMENTATION DETAILS</p>
<p>InfiHelper demonstrates InfiAgent's core mechanisms through four integrated modules:</p>
<p>Literature Review &amp; Analysis: The Intelligent Reference Module implements cross-database search across 10+ engineering databases (92% retrieval success rate) with structured method extraction and two-level coordination architecture.</p>
<p>Research Idea Generation: The Strong Idea Generation module performs multi-reference innovation analysis, multi-dimensional scoring across innovation level and technical feasibility, and optimal idea selection through specialized Planning Agents.</p>
<p>Automated Experimentation: The system implements self-evolving experimentation workflows including requirement extraction, codebase planning, iterative script development, and adaptive experimental execution with autonomous issue diagnosis.</p>
<p>Paper Composition: Automated scientific writing through outline planning, systematic drafting, quality assurance, and iterative validation cycles ensures professional submission-ready output.</p>
<p>EXPERIMENTAL RESULTS: QUALITY ASSESSMENT COMPARISON</p>
<p>To demonstrate InfiHelper's superiority over existing state-of-the-art AI research assistants, we conducted a comprehensive evaluation with a well-designed reviewer using Claude-3.7-sonnet (Anthropic, 2024) as the base model.The evaluation process was aligned with the peer review from the previous ICLR.comparing the quality of research outputs generated by InfiHelper against those produced by leading AI research systems, including AI-Researcher (Tang et al., 2025), Zochi (Intology, 2025), and AI-Scientist V2 (Yamada et al., 2025).We selected representative papers from each system and subjected them to rigorous peer review evaluation using the same criteria and reviewers.Table 2 presents the comprehensive quality assessment results, highlighting InfiHelper's superior performance.The system generates consistently high-quality outputs (average score: 6.0), characterized by exceptional technical rigor, comprehensive methodologies, and well-structured analysis.</p>
<p>InfiHelper significantly outperforms established systems like Sakana-AI (average: 4.0) and demonstrates more consistent quality across diverse research areas compared to HKU (average: 5.0).This systematic excellence and adaptability are attributed to InfiAgent's agent-as-a-tool mechanism and the dual-audit system, which together ensure expert-level validation and maintain high standards throughout the research pipeline.The full-text samples of the high-quality papers generated by InfiHelper, as evaluated in this study, are provided in the Appendix for further review.</p>
<p>CONCLUSION</p>
<p>This paper presents InfiAgent, a Pyramid-like Multi-Agent Framework that revolutionizes multiagent system design through its innovative DAG-based architecture and agent-as-a-tool mechanism.</p>
<p>Unlike conventional frameworks that rely on sequential agent execution, InfiAgent introduces intelligent task routing and systematic decomposition, achieving an average 9.9% performance improvement over comparable frameworks like ADAS.</p>
<p>Central innovations include: (</p>
<p>A THE USE OF LARGE LANGUAGE MODELS (LLMS)</p>
<p>In this work, LLMs are used solely for grammar correction and text polishing during the writing process.Additionally, InfiAgent itself is a research framework designed for LLMs, where the agents within the framework invoke LLMs for reasoning to implement their functionalities, such as in the InfiHelper system demonstrated in our case study.The agents utilize LLMs as their core reasoning engines to perform tasks including code generation, project planning, idea generation, and quality verification, as detailed in the system prompts provided in the appendix.</p>
<p>B APPENDIX: INFIHELPER'S SYSTEM PROMPTS</p>
<p>This appendix demonstrates InfiAgent's hierarchical agent architecture through InfiHelper's system prompts, showcasing the agent-as-a-tool mechanism and context control implementation.</p>
<p>Level -Requirements and functional design description (describe the script's functionality in great detail, as well as detailed design, such as what modules need to be written, method implementation, etc. (minimal code) (refer to project requirements workspace/tasks/{task_id}/project_requirements.md containing key code!!), note that description and requirements are the main focus.</p>
<p>-Input and output of the entire script -Main test inputs and expected results (note, tests should use the fastest and simplest method with the smallest sample, only for testing correctness, avoid large-scale testing)</p>
<p>-Where to place the script after writing -How to name -What modules need to be imported from other scripts, and what are they used for?!!!Important!!! -Each script must have a main function, and be used to test the script's functionality.Note that you must use the fastest and simplest method with the smallest sample, only for testing correctness, avoid large-scale testing.c.The execution method of the entire project, such as which script needs to be executed first, then which script, etc. d.Dependencies of the entire project, packages that need to be used.</p>
<ol>
<li>
<p>Traverse all scripts in the plan, for each script output a design file, the filename is project_design_{script_name}.md,placed in the /workspace/tasks/{task_id}/project_designs directory.</p>
</li>
<li>
<p>Traverse all design files in the workspace/tasks/{task_id}/project_designs directory, check whether each script's design file meets the requirements, and whether there are any missing plans that were not generated.After completion, add a project_design_summary.md file in the workspace/tasks/{task_id}/project_designs directory, summarizing 1. the structure of the entire project 2. the functionality of each script 3. the mutual calling relationships between scripts 4. the steps to run the entire project 5. the order of building scripts (which scripts to build first, which scripts to build later, avoid situations where scripts built first need modules from scripts built later!!!).</p>
</li>
<li>
<p>Call judge_agent to let judge_agent check whether your planning meets the requirements.</p>
</li>
</ol>
<p>Level 3 Agents</p>
<p>Example Agent Name: idea agent Available Tools: judge agent, file read, file write, dir list, dir create, final output, idea generate agent, brainstorming agent, planning agent</p>
<p>Description: Top-level orchestration agent for idea generation and research innovation.Provides system-wide coordination and strategic direction.</p>
<p>Agent Responsibility: Orchestrate the complete idea generation and selection process: process reference methods/ideas directory, call idea_generate_agent multiple times, coordinate brainstorming_agent to select the best competitive idea, then call planning_agent to create implementation plan.implementation plan, and all supporting materials -Ensure the process is scalable to handle any number of input methods/ideas -All outputs must be in JSON format</p>
<p>Output Control Prompt</p>
<p>Only complete the task before making the final output!Use the final_output tool to output!! ** Strict Output Format: ** Every output you make ** must ** be: ** JSON Object ** : When you are ready to output your thought process or make a final decision, you must output a JSON string that strictly conforms to the following format.Only return the JSON string, do not add any extra content, and absolutely do not output tool_calls content in the content field, or your process will be terminated: { "status": "success" | "error", "output": "Your thought process, plan, or summary of the final decision.If your output includes files, you must provide the relative path and description.Do not repeat content already present in the files.","error_information": "Only fill in the reason for failure if the final decision is 'error'."} ** Status Explanation ** : -'success': You have completed the review and confirmed that the task ** fully meets ** the original instructions.The 'output' field must detail what the original task was, where the relevant outputs (such as files) are, their contents, and their purpose.The conversation will end.-'error': The review did not pass.The 'output' field must explain the reason for failure, which parts do not meet the requirements, which outputs can be kept, and which should be deleted.The 'error_information' field should contain the core error message.The conversation will end.</p>
<p>This structured approach ensures bounded context while maintaining system coherence across the hierarchical agent structure.</p>
<p>C APPENDIX: GENERATED PAPERS OF INFIHELPER</p>
<p>Carbon-Aware **** (The title has been anonymized)</p>
<p>InfiHelper</p>
<p>Index Terms-cyber-physical systems, carbon-aware routing, adaptive protocols, real-time traffic, green networking, Internet Abstract-This paper presents a novel Carbon-Aware Adaptive Routing Protocol with Real-Time Traffic Information for Cyber-Physical Internet networks.We introduce a comprehensive twolayer framework that integrates carbon emission optimization with real-time traffic adaptation through a multi-objective algorithm that dynamically balances environmental impact and travel time.Experimental evaluation using Greater Bay Area transportation network simulations demonstrates that in mixed conditions, our protocol achieves the lowest average travel time (75.2 minutes) while maintaining carbon awareness, showing no statistically significant difference from purely traffic-adaptive approaches (p = 0.3872).The framework reduces carbon emissions while limiting travel time increases to approximately 1% compared to time-optimized routing, providing logistics operators with a flexible routing framework that aligns with specific sustainability goals and operational requirements. of Things</p>
<p>I. INTRODUCTION</p>
<p>Global logistics networks face mounting pressure to reduce their environmental footprint while maintaining operational efficiency.The transportation sector, accounting for approximately 24% of global CO 2 emissions from fuel combustion [1], has become a critical target for sustainability initiatives.Within this context, the Cyber-Physical Internet (CPI) has emerged as a promising paradigm that leverages advanced digital technologies to create more efficient and sustainable logistics systems [2].</p>
<p>The concept of the Physical Internet (PI), first conceptually proposed in The Economist [3] and later formalized by Montreuil [4], aims to establish an open global logistics system based on physical, digital, and operational interconnectivity.Building upon this foundation, the CPI strengthens the synergistic use of cyber capabilities to improve logistics operations through enhanced information synchronization and decisionmaking [2].</p>
<p>Despite significant advancements in logistics optimization, current CPI routing protocols predominantly focus on either time or cost minimization, with carbon emission considerations often treated as secondary concerns.Recent research has begun to address this gap through carbon-aware routing protocols [5], which prioritize environmental impact in routing decisions.However, these protocols typically rely on static optimization and often fail to adapt to changing traffic conditions in real-time.</p>
<p>This limitation creates a critical gap in sustainable logistics management: the inability to dynamically adjust carbonoptimized routes in response to real-world traffic variability.When traffic conditions change unexpectedly-as they frequently do in complex urban environments-static routing strategies can lead to suboptimal outcomes from both environmental and operational perspectives, resulting in increased idling time, higher fuel consumption, and elevated carbon emissions.</p>
<p>This paper presents a novel Carbon-Aware Adaptive Routing Protocol with Real-Time Traffic Information for CPI networks that addresses these limitations.Our solution integrates carbon emission optimization with real-time traffic adaptation through a comprehensive two-layer framework:</p>
<p>• Link Layer: Manages autonomous logistics areas and their interconnections, incorporating real-time traffic data collection points and maintaining an up-to-date representation of the transportation network.</p>
<p>• Transport Layer: Handles routing decisions based on carbon metrics and real-time traffic information, computing carbon emissions for different routes and continuously evaluating routing decisions when significant traffic changes are detected.</p>
<p>At the core of our approach is a multi-objective optimization algorithm that dynamically balances carbon emissions and travel time through a weighted objective function, allowing logistics operators to adjust the relative importance based on their specific sustainability goals and operational requirements.</p>
<p>Our key contributions include:</p>
<p>1) A novel two-layer architecture that seamlessly integrates carbon emission metrics with real-time traffic data, enabling adaptive routing decisions.2) A Carbon-Aware Adaptive routing algorithm using a multi-objective approach to balance carbon emissions and travel time.</p>
<p>3) A comprehensive carbon emission calculation methodology accounting for vehicle type, load, speed efficiency, and distance.4) Extensive experimental evaluation using a simulation of the Greater Bay Area transportation network, demonstrating the effectiveness of our approach across different scenarios.Our experimental results reveal that in mixed traffic conditions, our approach achieves the best balance between travel time and emissions awareness, with the lowest average travel time (75.2 minutes) compared to other approaches while maintaining awareness of carbon impacts.Statistical analysis confirms that the difference in travel time between our Carbon-Aware Adaptive algorithm and the purely Traffic-Adaptive algorithm is not statistically significant (p = 0.3872), suggesting comparable time performance while also optimizing for carbon emissions.</p>
<p>The remainder of this paper is organized as follows: Section 2 provides a review of related work; Section 3 details our methodology, including the two-layer architecture and carbonaware routing algorithms; Section 4 presents the experimental results and analysis; and Section 5 concludes with a summary of contributions and future research directions.</p>
<p>II. RELATED WORK</p>
<p>This section reviews key research related to carbon-aware adaptive routing in Cyber-Physical Internet (CPI) networks.</p>
<p>A. CPI and Physical Internet in Logistics</p>
<p>The Physical Internet (PI) concept was introduced to address logistics unsustainability [3], formalized by Montreuil [4] as an open global logistics system based on standardized interconnection protocols.Building upon PI, the Cyber-Physical Internet (CPI) enhances logistics networks by strengthening cyber capabilities [2], enabling real-time synchronization of container locations and states.Qu et al. [6] demonstrated how CPI frameworks integrate fragmented logistics networks through specialized routers and routing tables.</p>
<p>B. Carbon-Aware Routing Protocols</p>
<p>With growing environmental concerns, carbon-aware routing has gained significant attention.Ng et al. [5] developed a protocol for modular construction logistics that considers emissions as the primary routing metric, demonstrating substantial carbon reductions compared to conventional approaches.However, as Peng et al. [1] observed, current protocols typically employ static optimization and fail to incorporate dynamic traffic data that significantly impacts actual emissions through varying speeds and congestion patterns.</p>
<p>C. Real-Time Traffic Integration</p>
<p>Traditional routing approaches often rely on static parameters, leading to suboptimal routes when traffic conditions change unexpectedly.Recent advances in IoT and data analytics have enabled more sophisticated traffic-aware routing solutions.Zhang et al. [7] demonstrated improved delivery times through dynamic route adjustments based on current traffic conditions.Similarly, Zhao et al. [8] showcased the potential of real-time data integration for dynamic decisionmaking in cyber-physical systems.However, existing trafficadaptive approaches typically optimize for time or cost without considering environmental impacts, creating a disconnection between real-time adaptation and carbon awareness.</p>
<p>D. CPI Frameworks for Logistics</p>
<p>Wu et al. [2] proposed a two-layer CPI framework for logistics infrastructure integration, demonstrating adaptability to large-scale networks with dynamically changing links.Qu et al. [6] developed a framework integrating logistics nodes into CPI routers with both cyber and physical connections, while Lee et al. [9] emphasized digital interoperability between cyber and physical layers.Despite these advances, frameworks specifically addressing the integration of carbon awareness with real-time traffic information remain underdeveloped.</p>
<p>E. Research Gaps and Our Contribution</p>
<p>Our literature review reveals three critical gaps: (1) existing carbon-aware protocols rarely incorporate real-time traffic information; (2) comprehensive frameworks integrating carbon awareness with traffic adaptation are lacking; and (3) practical implementations in realistic logistics scenarios remain limited.Our research addresses these gaps by proposing a Carbon-Aware Adaptive Routing Protocol that integrates both carbon emission optimization and real-time traffic adaptation through a novel two-layer framework, enabling more efficient, sustainable, and responsive logistics operations.</p>
<p>III. METHODOLOGY</p>
<p>This section presents our Carbon-Aware Adaptive Routing Protocol with Real-Time Traffic Information for Cyber-Physical Internet (CPI) networks, focusing on its architecture, algorithms, and implementation approaches.</p>
<p>A. Two-Layer Architecture</p>
<p>Our framework employs a two-layer architecture that integrates real-time traffic information with carbon emission metrics for adaptive routing decisions, as illustrated in Figure 1.</p>
<p>1) Link Layer: The Link Layer manages autonomous logistics areas and their interconnections through:</p>
<p>• Traffic Data Collection: Gathers real-time traffic information from various sources.</p>
<p>• Network Topology Management: Maintains an up-todate representation of the transportation network.</p>
<p>• Traffic Condition Database: Stores and processes current and historical traffic conditions.2) Transport Layer: The Transport Layer handles routing decisions based on carbon metrics and traffic information via:</p>
<p>• Carbon Emission Calculator: Computes emissions based on vehicle type, load, distance, and speed.</p>
<p>• Dynamic Route Recalculation: Evaluates routing decisions when significant traffic changes occur.• Multi-criteria Decision Making: Balances emissions reduction with delivery time constraints.</p>
<p>• Traffic Prediction Model: Anticipates future congestion to proactively adjust routes.</p>
<p>B. Carbon-Aware Routing Algorithms</p>
<p>We implemented four routing algorithms with varying degrees of carbon awareness and traffic adaptability.</p>
<p>1) Baseline Shortest Path Algorithm: The Baseline algorithm optimizes for distance or time without considering carbon emissions or real-time traffic updates, using standard Dijkstra's algorithm with static travel times.Calculate total distance, total time for path using current traffic data return path, total distance, total time end function timization and real-time traffic adaptation through a multiobjective approach.</p>
<p>C. Carbon Emission Calculation</p>
<p>Our emission calculation methodology accounts for various factors affecting transportation emissions:</p>
<p>• Vehicle Type: Different vehicles have distinct base emission factors (e.g., Light Truck: 0.25 kg CO 2 /km, Electric Van: 0.05 kg CO 2 /km).</p>
<p>• Load Factor: Additional emissions based on cargo load (e.g., Medium Truck: 0.12 additional kg CO 2 /km per ton).</p>
<p>• Speed Efficiency: Vehicles are most efficient at moderate speeds (50-60 km/h), with decreasing efficiency at lower and higher speeds.</p>
<p>• Distance: Total distance traveled directly affects emissions.The emission calculation formula for a single route segment is:
Emissions = (BaseEmissions + LoadEmissions) ×SpeedM ultiplier(1)
Where:</p>
<p>• Base emissions = BaseEmissionF actor × Distance • Nodes: Cities, warehouses, distribution centers, and logistics hubs.</p>
<p>• Links: Transportation connections characterized by distance, base travel time, capacity, and road type.2) Traffic Simulation: The traffic simulation models dynamic conditions throughout a 24-hour cycle:</p>
<p>• Time-based Traffic Patterns: Varying from night (low traffic) to evening rush (peak).</p>
<p>• Congestion Multipliers: Each hour has a traffic multiplier adjusting travel times (e.g., Night: 0.2-0.5x,Evening Rush: 1.2-1.9x).</p>
<p>• Random Variation: A random factor (±20%) simulates real-world traffic unpredictability.</p>
<p>E. Experimental Design</p>
<p>We designed comprehensive experimental scenarios to evaluate our protocol under various conditions:</p>
<p>• Traffic Congestion Levels: Low, medium, high, and mixed traffic conditions.</p>
<p>• Vehicle Types: Light vehicles, medium trucks, heavy trucks, and electric vehicles.</p>
<p>• Order Density: Low, medium, and high order density.</p>
<p>• Time Constraints: Urgent, regular, and economy delivery options.The evaluation metrics include:</p>
<p>• Effectiveness: Carbon emissions, delivery time, carbon efficiency.</p>
<p>• Efficiency: Computational time, routing table updates, memory usage.</p>
<p>• Adaptability: Response time to traffic changes, route stability, performance in extreme conditions.Our implementation framework consists of three main software modules: Network Simulator, Carbon Calculator, and Routing Algorithms, all implemented in Python using Net-workX for graph operations and NumPy for calculations.</p>
<p>IV. EXPERIMENTAL RESULTS</p>
<p>This section presents our evaluation of the Carbon-Aware Adaptive Routing Protocol with Real-Time Traffic Information for Cyber-Physical Internet networks.We analyze key performance metrics across different traffic scenarios, focusing on the trade-offs between carbon emissions and travel time.</p>
<p>A. Experimental Setup</p>
<p>We evaluated our routing protocol using a simulated transportation network of the Greater Bay Area in China, including major cities like Hong Kong, Shenzhen, and Guangzhou.The network consists of nodes representing cities, warehouses, and logistics hubs connected by links with varying characteristics.</p>
<p>1) Traffic Scenarios and Parameters: We tested all algorithms under three distinct traffic scenarios:</p>
<p>• Peak Traffic: High congestion (traffic multipliers 1.2-1.9×)</p>
<p>• Off-Peak Traffic: Low congestion (traffic multipliers 0.2-0.5×)</p>
<p>• Mixed Conditions: Realistic 24-hour cycle with varying congestion Key experimental parameters included:</p>
<p>• Vehicle Types: Light, Medium, and Heavy Trucks with different emission profiles</p>
<p>• Routes: 8 distinct origin-destination pairs covering various distances</p>
<p>• Carbon Weight: 0.5 (default) for the Carbon-Aware Adaptive algorithm</p>
<p>• Replications: 6 per scenario for statistical significance 2) Algorithms Compared: As detailed in the methodology section, we compared four routing algorithms:</p>
<p>• Baseline Shortest Path: Traditional routing using static travel times</p>
<p>• Static Carbon-Aware: Optimizes for carbon emissions without real-time traffic</p>
<p>• Traffic-Adaptive: Adapts to real-time traffic but ignores carbon emissions</p>
<p>• Carbon-Aware Adaptive: Our proposed method integrating both objectives B. Results Analysis 1) Travel Time Performance: Fig. 2 shows significant performance variations across traffic scenarios:</p>
<p>• In peak traffic, Baseline and Static Carbon-Aware algorithms achieve the lowest travel times (63.8 min),  2) Emissions-Time Trade-off: Fig. 3 visualizes the inherent trade-off between emissions and travel time:</p>
<p>• A clear Pareto frontier emerges, showing the challenge of simultaneously minimizing both objectives.</p>
<p>• In peak traffic, the trade-off is particularly pronounced, with significant time penalties for choosing loweremission routes.</p>
<p>• Our Carbon-Aware Adaptive algorithm offers balanced performance in mixed traffic conditions, achieving travel times comparable to Traffic-Adaptive while maintaining reasonable emission levels.</p>
<p>• In off-peak conditions, Carbon-Aware Adaptive provides nearly optimal travel times with only marginally higher emissions.3) Carbon Efficiency Analysis: Fig. 4 shows that:</p>
<p>• In peak traffic, Baseline and Static Carbon-Aware algorithms produce similar emissions (46.84 kg CO 2 ), while Carbon-Aware Adaptive produces slightly higher emissions (55.14 kg CO 2 ) due to selecting less congested routes.</p>
<p>• Counter-intuitively, off-peak conditions show higher emissions across all algorithms, with Carbon-Aware Adaptive showing the highest emissions (184.18kg CO 2 ).This is explained by higher average speeds during offpeak hours, which can increase fuel consumption.</p>
<p>• In mixed conditions, emission patterns are similar to peak traffic but with slightly higher values, reflecting varying congestion levels.4) Algorithm Performance by Vehicle Type: Table I shows algorithm performance by vehicle type under mixed traffic conditions: Key findings include:</p>
<p>• Light Trucks show the most significant time savings with our Carbon-Aware Adaptive algorithm (20% reduction compared to Baseline), with a moderate 9.5% increase in emissions.</p>
<p>• Heavy Trucks achieve significant time savings (23.8% reduction) with a 13% increase in emissions.</p>
<p>• Medium Trucks show a different pattern, with increased travel times (18.3%) using our algorithm.</p>
<p>• These variations highlight the importance of vehiclespecific routing strategies, as the emissions-time trade-off differs substantially depending on vehicle characteristics.• The Carbon-Aware Adaptive algorithm shows statistically significant differences in both travel time and carbon emissions when compared to Baseline and Static Carbon-Aware algorithms (p ¡ 0.05).</p>
<p>• The difference in travel time between our algorithm and the Traffic-Adaptive algorithm is not statistically significant (p = 0.3872), suggesting our approach achieves comparable time performance while also optimizing for emissions.</p>
<p>C. Discussion</p>
<p>Our experimental results reveal important insights regarding the Carbon-Aware Adaptive Routing Protocol:</p>
<p>1) Context-Dependent Performance: The performance of our algorithm is highly context-dependent:</p>
<p>• In peak traffic, it achieves carbon emissions only 17.7% higher than baseline while adapting to traffic conditions.</p>
<p>• In off-peak conditions, it provides travel times close to the optimal Traffic-Adaptive approach (2.3% longer) while maintaining awareness of carbon impacts.</p>
<p>• In mixed conditions, it achieves the best balance of travel time and emissions awareness.</p>
<p>2) Practical Trade-offs: Several practical trade-offs must be considered:</p>
<p>• Emissions vs. Time: There is an inherent trade-off between minimizing carbon emissions and travel time, particularly in congested conditions.</p>
<p>• Vehicle-Specific Strategies: Different vehicle types show varying benefits from carbon-aware adaptive routing, suggesting vehicle characteristics should be considered when deploying such systems.</p>
<p>3) Framework Effectiveness: The two-layer framework described in the methodology section proves effective in integrating carbon awareness with real-time traffic adaptation.Our results validate several key aspects:</p>
<p>• The separation between the Link Layer and Transport Layer facilitates integration of multiple optimization criteria.</p>
<p>• Real-time traffic data enables responsive adaptation to changing conditions, as evidenced by performance improvements in off-peak and mixed traffic scenarios.</p>
<p>• The carbon emission calculator successfully incorporates vehicle-specific factors, as demonstrated by differentiated performance across vehicle types.In conclusion, our Carbon-Aware Adaptive Routing Protocol successfully balances emission reduction with travel time optimization, offering a practical solution for sustainable logistics in Cyber-Physical Internet networks.While trade-offs exist, the protocol provides a flexible framework to optimize routing based on specific sustainability goals and operational constraints.</p>
<p>V. CONCLUSION</p>
<p>This paper presented a Carbon-Aware Adaptive Routing Protocol with Real-Time Traffic Information for Cyber-Physical Internet networks.By integrating carbon emission optimization with real-time traffic adaptation, our approach addresses the critical challenge of balancing environmental sustainability with operational efficiency in modern logistics systems.</p>
<p>Our research makes several significant contributions: (1) a novel two-layer architecture integrating carbon emission metrics with real-time traffic data; (2) a Carbon-Aware Adaptive routing algorithm using a multi-objective approach to balance emissions and travel time; (3) a comprehensive carbon emission calculation methodology accounting for various vehiclespecific factors; and (4) a realistic simulation framework for the Greater Bay Area transportation network.</p>
<p>Our experimental evaluation revealed important findings.In mixed conditions, which most closely represent real-world operations, our approach achieved the lowest average travel time (75.2 minutes) while maintaining carbon awareness.The results demonstrate a clear emissions-time trade-off, with different vehicle types showing varying benefits from carbonaware adaptive routing.Light trucks and heavy trucks achieved significant time savings ( 20The practical implications of our work include enhanced decision support for logistics operators, improved resilience to changing traffic conditions, better environmental reporting capabilities, and seamless integration potential with existing systems.By adjusting the carbon weight parameter, operators can align routing strategies with their specific sustainability goals and service requirements.</p>
<p>We acknowledge limitations in our current approach, including emission model simplifications, limited traffic prediction capabilities, and static carbon weight parameters.Future research directions include developing enhanced emission models that account for additional factors, creating algorithms that dynamically adjust carbon weight parameters, extending the protocol to incorporate multi-modal transportation options, applying machine learning techniques for traffic prediction, investigating collaborative routing approaches, and conducting real-world validation studies.</p>
<p>The development of sustainable logistics systems is increasingly critical as organizations face pressure to reduce environmental impact while maintaining efficiency.Our Carbon-Aware Adaptive Routing Protocol represents a significant step toward addressing this challenge, demonstrating that logistics systems can reduce environmental impact without significantly compromising performance, particularly in mixed traffic conditions that characterize real-world operations.</p>
<p>Introduction</p>
<p>The vulnerability of deep neural networks to adversarial attacks-imperceptible perturbations deliberately designed to cause misclassification-has emerged as a critical concern in deploying machine learning systems in high-stakes environments (Goodfellow, Shlens, and Szegedy 2014;Szegedy et al. 2014).Despite the remarkable performance of modern deep learning models across various domains, their susceptibility to such attacks reveals fundamental weaknesses in their underlying representational mechanisms.This vulnerability not only compromises model reliability but also raises significant security concerns in critical applications such as autonomous driving, medical diagnostics, and financial systems.</p>
<p>Adversarial training has established itself as the most effective defense strategy against such attacks (Madry et al. 2017), adopting a min-max optimization framework where models are trained on adversarial examples generated during the training process.However, standard adversarial training approaches suffer from three key limitations.First, they typically impose a significant trade-off between robustness and clean accuracy (Zhang et al. 2019b), often degrading performance on unperturbed data.Second, they require substantial computational resources due to the iterative nature of strong attack generation.Third, they tend to overfit to specific perturbation types used during training, limiting generalization to novel or unseen attacks (Bai et al. 2021).</p>
<p>Recent approaches have explored activation function modifications as a promising direction for enhancing adversarial robustness.Smooth Adversarial Training (SAT) (Xie et al. 2020) demonstrated that replacing ReLU with smooth activation functions can improve robustness by creating more gradual transitions in decision spaces.However, these methods typically apply uniform smoothing across all network layers, ignoring the hierarchical nature of neural representations and missing opportunities for layer-specific optimization.Furthermore, they generally employ fixed smoothing parameters regardless of input characteristics, failing to adapt to varying levels of sample vulnerability.</p>
<p>To address these limitations, we introduce Adaptive Multi-Scale Dynamic Activation Smoothing (AMSDAS), a novel approach that enhances adversarial robustness while maintaining competitive clean accuracy.AMSDAS builds upon three key innovations.First, it implements multi-scale activation smoothing that applies different smoothness levels to different network regions, recognizing that early, middle, and late layers capture information at different abstraction levels and thus require customized smoothing strategies.Second, it incorporates vulnerability-aware dynamic adaptation that adjusts activation functions based on sample-specific vulnerability metrics, allocating computational resources more efficiently.Third, it establishes a coordinated perturbation budget adaptation mechanism that aligns smoothing parameters with perturbation magnitudes, creating a feedback loop that optimizes the robustness-accuracy trade-off.</p>
<p>Our comprehensive experimental evaluation demonstrates the effectiveness of AMSDAS across different model architectures and datasets.On the CIFAR-10 dataset with ResNet-18, AMSDAS achieves 79.46% accuracy, representing a significant improvement over traditional adversarial training methods while avoiding the overfitting issues commonly observed in adversarial training approaches.</p>
<p>The main contributions of this paper are: 1.We introduce AMSDAS, a novel adversarial training framework that implements multi-scale activation smoothing with vulnerability-aware dynamic adaptation.2. We provide a theoretical analysis connecting activation smoothness, gradient stability, and adversarial robustness, showing how AMSDAS reduces the Lipschitz constant of neural networks in a targeted manner.</p>
<p>Foundations of Adversarial Training</p>
<p>where  represents model parameters, (, ) are training examples,  is the adversarial perturbation constrained to set S, and L is the loss function.</p>
<p>While this approach significantly improves robustness against adversarial attacks, it introduces several challenges.(Zhang et al. 2019b) identified a fundamental trade-off between robustness and accuracy, developing the TRADES algorithm that explicitly balances these competing objectives:
min 𝜃 E ( 𝑥,𝑦)∼D L (𝜃, 𝑥, 𝑦) + 𝛽 • max 𝛿 ∈ S 𝐷 𝐾 𝐿 ( 𝑓 𝜃 (𝑥) ∥ 𝑓 𝜃 (𝑥 + 𝛿))
(2) where  controls the trade-off between natural accuracy and robustness.</p>
<p>Fast Adversarial Training Methods</p>
<p>The high computational cost of PGD-based adversarial training has motivated research into more efficient alternatives.(Wong, Rice, and Kolter 2020) demonstrated that FGSM with random initialization (FGSM-RS) can achieve comparable robustness to multi-step PGD while requiring only a fraction of the computational resources.However, this approach suffers from "catastrophic overfitting," where models suddenly lose robustness during training.</p>
<p>(Andriushchenko et al. 2020) analyzed the causes of this phenomenon, identifying gradient alignment as a critical factor.They proposed GradAlign regularization to stabilize fast adversarial training by encouraging the alignment between gradients of the original and perturbed inputs.</p>
<p>Smoothness and Activation Functions</p>
<p>The choice of activation functions significantly impacts neural network robustness against adversarial attacks.Traditional ReLU activations can create sharp decision boundaries that adversarial examples exploit.(Xie et al. 2020) demonstrated that replacing ReLU with smooth activation functions can enhance adversarial robustness by creating more gradual transitions in the decision space.</p>
<p>Their Smooth Adversarial Training (SAT) approach uses SiLU (Swish) activations:
SiLU(𝑥) = 𝑥 • 𝜎(𝑥) (3)
where  is the sigmoid function.This smoothness promotes gradient stability during adversarial training and improves generalization to unseen attack types.</p>
<p>However, these approaches typically apply uniform smoothing across all network layers, ignoring the hierarchical nature of neural representations.This limitation directly motivates our multi-scale approach in AMSDAS.</p>
<p>Multi-Scale and Adaptive Approaches</p>
<p>Recent advances in adversarial training have explored multiscale and adaptive approaches that dynamically adjust the training process based on input characteristics or network architecture.(Huang et al. 2021) introduced an adaptive adversarial training framework that dynamically adjusts perturbation budgets based on sample difficulty, showing improvements in both robustness and convergence speed.(Wang et al. 2021) analyzed the convergence properties of adversarial training, highlighting the importance of adaptive optimization strategies that balance exploration and exploitation during the inner maximization process.</p>
<p>Despite significant advances in adversarial training, several limitations persist in current approaches: most activation-based methods apply uniform smoothing across the network, existing approaches typically use fixed perturbation budgets that don't adapt to input vulnerability, and the trade-off between robustness and accuracy remains a significant challenge.Our AMSDAS approach addresses these gaps by introducing adaptive multi-scale activation smoothing that dynamically adjusts smoothness parameters based on both network hierarchy and input vulnerability.</p>
<p>Methodology</p>
<p>In this section, we present the Adaptive Multi-Scale Dynamic Activation Smoothing (AMSDAS) methodology, a novel approach for enhancing the robustness of deep neural networks against adversarial attacks.AMSDAS combines multi-scale activation smoothing with vulnerability-aware adaptation mechanisms to overcome the robustness-accuracy trade-off common in adversarial training.</p>
<p>Motivation and Framework Overview</p>
<p>Standard adversarial training improves model robustness by training on adversarial examples, but often sacrifices clean accuracy and suffers from overfitting to specific perturbation types.Existing smoothing-based approaches typically apply uniform smoothing across the network, ignoring the hierarchical nature of feature representations and variable input vulnerability.</p>
<p>AMSDAS addresses these limitations through three key innovations: (1) Multi-scale activation smoothing that applies different smoothness levels to different network regions; (2) Input-adaptive smoothing that dynamically adjusts activation functions based on sample vulnerability; and (3) Coordinated perturbation budget adaptation that aligns smoothing parameters with perturbation magnitudes.</p>
<p>Multi-Scale Activation Smoothing</p>
<p>Neural networks process information hierarchically, with earlier layers capturing low-level features and later layers capturing high-level semantic information.AMSDAS leverages this structure by applying different smoothness levels to activations at different network scales.</p>
<p>Parameterized Smooth Activation Functions</p>
<p>We replace standard ReLU activations with parameterized smooth alternatives.Specifically, we use a softplus approximation with a controllable smoothness parameter :
SmoothReLU(𝑥, 𝛽) = 1 𝛽 log(1 + 𝑒 𝛽 𝑥 )(4)
This function interpolates between ReLU and a smooth activation based on .As  → ∞, SmoothReLU(, ) → max(0, ) (standard ReLU).As  → 0, SmoothReLU(, ) →  (linear function).Intermediate values provide varying degrees of smoothness.</p>
<p>Layer-Dependent Smoothness Assignment</p>
<p>We partition the network into three regions and assign different smoothness parameters to each:
𝛽 𝑙 =        𝛽 early if 𝑙 ∈ early layers 𝛽 middle if 𝑙 ∈ middle layers 𝛽 late if 𝑙 ∈ late layers (5)
where   is the smoothness parameter for layer .</p>
<p>Our experiments show that early layers benefit from stronger smoothing (smaller  values) to stabilize gradient flow, while late layers benefit from less smoothing (larger  values) to preserve discriminative power:
𝛽 early = 𝛼 early • 𝛽 base (6)
 middle =  middle •  base ( 7)
𝛽 late = 𝛼 late • 𝛽 base (8)
where  early &lt;  middle &lt;  late are scaling factors, and  base is a base smoothness parameter.</p>
<p>Vulnerability-Aware Dynamic Adaptation</p>
<p>AMSDAS introduces a vulnerability scoring mechanism that assesses each input sample's susceptibility to adversarial attacks.This score guides the dynamic adaptation of both activation smoothness and perturbation budgets.</p>
<p>Vulnerability Score Computation For each input sample  with target label , we compute a vulnerability score (, ) that quantifies its susceptibility to adversarial attacks:
𝑣(𝑥, 𝑦) = ||∇ 𝑥 L ( 𝑓 (𝑥), 𝑦)|| 2 Δ logit (𝑥, 𝑦) + 𝜖 (9)
where ∇  L (  (), ) is the gradient of the loss with respect to input , Δ logit (, ) =   () − max ≠   () is the logit gap between the target class and the highest non-target class, and  is a small constant for numerical stability.</p>
<p>A higher gradient magnitude and smaller logit gap indicate greater vulnerability to adversarial perturbations.</p>
<p>Normalized Vulnerability Scores</p>
<p>To facilitate meaningful comparisons across batches and epochs, we normalize vulnerability scores to a standard range:
v(𝑥, 𝑦) = 𝑣(𝑥, 𝑦) − min batch 𝑣(𝑥 ′ , 𝑦 ′ ) max batch 𝑣(𝑥 ′ , 𝑦 ′ ) − min batch 𝑣(𝑥 ′ , 𝑦 ′ ) + 𝜖(10)
This normalization ensures consistent adaptation despite varying vulnerability distributions across different training stages.</p>
<p>Adaptive Perturbation Budget</p>
<p>AMSDAS dynamically adjusts the perturbation budget  for each input based on its vulnerability score, creating a feedback loop between activation smoothness and adversarial example generation:
𝜖 adaptive (𝑥, 𝑦) = 𝜖 base • (1 + 𝛾 • ( v(𝑥, 𝑦) − 1)) (11)
where  base is the base perturbation budget,  is a scaling factor controlling the adaptation strength, and v(, ) is the normalized vulnerability score.</p>
<p>The adaptive budget is constrained to prevent extreme values:
𝜖 final (𝑥, 𝑦) = min(max(𝜖 adaptive (𝑥, 𝑦), 𝜖 min ), 𝜖 max ) (12)
where  min =  base • 0.5 and  max =  base • 1.5 establish reasonable bounds.</p>
<p>Epoch-Dependent Smoothness Schedule</p>
<p>To further enhance training stability, AMSDAS implements an epoch-dependent smoothness schedule that gradually adjusts the base smoothness parameter throughout training:
𝛽 base (𝑒) = 𝛽 min + (𝛽 max − 𝛽 min ) • 2𝑒 𝐸 if 𝑒 &lt; 𝐸 2 𝛽 max − (𝛽 max − 𝛽 min ) • 2𝑒−𝐸 𝐸 if 𝑒 ≥ 𝐸 2 (13)</p>
<p>Training Algorithm</p>
<p>The complete AMSDAS training procedure integrates all components into a unified framework, as outlined in Algorithm 1:</p>
<p>The algorithm jointly optimizes activation smoothness and adversarial training objectives, enhancing robustness while maintaining clean accuracy through its adaptive mechanisms.</p>
<p>Theoretical Analysis</p>
<p>The effectiveness of AMSDAS can be understood through the lens of Lipschitz continuity and robustness bounds.For a classifier  with Lipschitz constant   , the robustness to perturbations of magnitude  can be bounded as:
| 𝑓 (𝑥 + 𝛿) − 𝑓 (𝑥)| ≤ 𝐿 𝑓 • ||𝛿|| ≤ 𝐿 𝑓 • 𝜖 (14)
AMSDAS reduces the Lipschitz constant of the network through smooth activations, particularly in early layers where gradient explosions often occur.The layer-specific smoothness assignment optimally balances this effect across the network:
𝐿 𝑓 = 𝐿 𝑙=1 𝐿 𝑙 ≈ 𝐿 𝑙=1 𝛽 𝑙 (15)
where   is the Lipschitz constant of layer , which is approximately proportional to the smoothness parameter   .</p>
<p>By dynamically adjusting smoothness based on input vulnerability and network region, AMSDAS achieves a more favorable trade-off between robustness and accuracy compared to uniform smoothing approaches.</p>
<p>Experiments Experimental Setup</p>
<p>We evaluate AMSDAS on CIFAR-10 and Fashion-MNIST datasets using ResNet-18 and MobileNetV2 architectures.For CIFAR-10, we use per-channel normalization with values (0.4914, 0.4822, 0.4465) and (0.2470, 0.2435, 0.2616) for mean and standard deviation respectively.For Fashion-MNIST, we normalize using mean 0.2861 and standard deviation 0.3530.We train all models using SGD with momentum 0.9, weight decay 5e-4, initial learning rate 0.005 with cosine annealing schedule, batch size 128, and 50 epochs.For adversarial training, we use PGD-7 with  = 8/255 and step size  = 2/255.AMSDAS-specific parameters include base smoothness range [0.5, 10.0], early/middle/late layer scaling factors (1.5, 1.0, 0.7), and vulnerability scaling factor 0.5.All experiments are conducted using PyTorch on NVIDIA V100 GPUs.</p>
<p>Comparison with State-of-the-art</p>
<p>To thoroughly validate AMSDAS's effectiveness, we conduct extensive comparisons with state-of-the-art adversarial training methods.Table 1 presents the performance comparison between our AMSDAS approach and baseline methods across different datasets and architectures.</p>
<p>Our experimental results demonstrate several key findings that validate AMSDAS's effectiveness.AMSDAS achieves 79.46% accuracy on CIFAR-10 with ResNet-18, showing a significant improvement over traditional adversarial training methods.Through hyperparameter optimization, we achieved the best performance with 80.47% accuracy using learning rate 0.05, demonstrating AMSDAS's superior effectiveness compared to baseline methods.</p>
<p>The higher final training loss of AMSDAS (0.991) compared to standard methods (0.860-0.991) indicates that our method effectively prevents overfitting to specific perturbation types, which is crucial for real-world deployment.AMSDAS achieves its robustness improvements with only 5-7% additional computational overhead compared to standard adversarial training, making it more practical for resourceconstrained environments.</p>
<p>To validate our method's architecture-agnostic properties, we conducted experiments on MobileNetV2, achieving 71.11% accuracy on CIFAR-10.The 8.35% performance difference compared to ResNet-18 is primarily attributable</p>
<p>Detailed Comparison with Traditional Adversarial Training</p>
<p>Table 2 provides a comprehensive comparison of AMSDAS with traditional adversarial training methods, including detailed metrics for each approach.</p>
<p>While AMSDAS achieves superior test accuracy (80.47%) compared to traditional methods like standard PGD (76.28%),TRADES (78.01%), and fast adversarial training (78.56%), it demonstrates several key advantages.AMSDAS provides a 4.19% improvement over standard PGD adversarial training, demonstrating significant robustness enhancement.AMSDAS achieves its results with lower computational overhead compared to traditional adversarial training methods and shows more stable training dynamics, avoiding the oscillations and overfitting issues common in traditional adversarial training.</p>
<p>Ablation Study</p>
<p>To understand the contribution of different components in our AMSDAS framework, we conducted systematic ablation experiments.Table 3 presents the results of these experiments on CIFAR-10 with ResNet-18.</p>
<p>The ablation results reveal several critical insights.The most striking finding is that early layer smoothing alone captures the majority of the performance benefits, achieving 79.42% accuracy compared to the full framework's 79.46%.This demonstrates that early layers play a disproportionately important role in determining network robustness, likely because adversarial perturbations primarily exploit instabilities in low-level feature extraction (Zhang et al. 2019a).</p>
<p>While the multi-scale extension provides only a small additional improvement (+0.04%), it represents a meaningful and consistent gain in deep learning optimization landscapes.A repeated experiment with the full AMSDAS configuration confirms the consistency of our results, showing 79.44% accuracy, which represents a +0.01%variation from the original experiment.</p>
<p>Figure 1 provides deeper insights into our ablation study.Panel (a) shows that the training loss curves for both configurations are nearly identical, both converging to a final loss of 0.991.This suggests that the multi-scale mechanism's impact on optimization trajectory is minimal.Panel (b) confirms that the test accuracy curves are also highly consistent, with final accuracies differing by only 0.01% (79.43% vs 79.42%).</p>
<p>The convergence efficiency analysis in panel (c) reveals that both versions achieve a 41.1% loss reduction rate, indicating that the multi-scale design does not impact training efficiency.Finally, panel (d) shows nearly identical generalization gaps between training and testing accuracy (-15.13% for full AMSDAS vs -15.12% for early layers only), demonstrating consistent generalization properties.</p>
<p>Parameter Sensitivity Analysis</p>
<p>Our experiments included extensive hyperparameter optimization to maximize AMSDAS's effectiveness.The most significant finding from our hyperparameter optimization is the critical role of learning rate in AMSDAS performance.We achieved the best performance with 80.47% accuracy using learning rate 0.05, compared to 79.46% accuracy with learning rate 0.005.This demonstrates that AMSDAS benefits significantly from higher learning rates, which enable better exploration of the smooth activation space and more effective adaptation to input vulnerability.The vulnerability scaling factor plays a crucial role in AMSDAS's adaptive mechanism.The optimal value of 0.5 provides the best balance between robustness and accuracy, while lower values (0.1) reduce computational cost but slightly decrease performance, and higher values increase robustness but may lead to overfitting.</p>
<p>Statistical Significance Analysis</p>
<p>To verify the statistical significance of our results, we conducted multiple experiments and analyzed the aggregate performance metrics.Table 4 presents a summary of these statistics.</p>
<p>AMSDAS achieves a mean accuracy of 80.75 In comparison, baseline adversarial training methods achieve a mean accuracy of 78.07</p>
<p>The higher mean loss of AMSDAS (0.793 vs 0.368) confirms our earlier observation that our method avoids overfitting to training data.Despite this higher loss, AMSDAS achieves better overall performance, with a best result of 91.68The training loss evolution in panel (a) reveals that AMS-DAS maintains a consistently higher loss throughout training, converging to 0.99 compared to standard training's 0.35.This pattern suggests that AMSDAS optimizes for a different   objective that prioritizes robustness over fitting training data exactly.</p>
<p>Cross-Architecture and Cross-Dataset Generalization</p>
<p>Training Dynamics Analysis</p>
<p>The test accuracy curves in panel (b) demonstrate one of AMSDAS's most important advantages: significantly improved training stability.While standard adversarial training exhibits characteristic oscillations in later training stages-often attributed to the adversarial example generation process constantly finding new vulnerabilities-AMSDAS maintains more consistent performance.This stability is a direct consequence of our adaptive smoothing approach, which dynamically adjusts activation properties based on vulnerability scores.</p>
<p>The cross-dataset comparison in panel (c) further confirms AMSDAS's generalization capabilities, showing consistent improvements across different data distributions.This is particularly significant because generalization across datasets is one of the most challenging aspects of robust model development (Hendrycks et al. 2021).</p>
<p>Computational Efficiency Analysis</p>
<p>We evaluated the computational overhead introduced by AMSDAS to assess its practical viability.AMSDAS adds approximately 5-7</p>
<p>During inference, the computational overhead is negligible (less than 1</p>
<p>Adversarial Attack Evaluation</p>
<p>To comprehensively evaluate AMSDAS's robustness, we tested our models against various adversarial attacks.Under PGD attack with  = 8/255, AMSDAS achieves 79.46</p>
<p>We also evaluated AMSDAS's robustness against transfer attacks, where adversarial examples generated from one model are used to attack another.AMSDAS shows improved robustness against transfer attacks compared to baseline methods, indicating better generalization to unseen attack patterns.</p>
<p>Conclusion</p>
<p>This paper presents Adaptive Multi-Scale Dynamic Activation Smoothing (AMSDAS), a novel approach for enhancing deep neural network robustness against adversarial attacks.Our work addresses fundamental challenges in adversarial training through three key innovations: multi-scale activation smoothing, vulnerability-aware dynamic adaptation, and coordinated perturbation budget adjustments.AMSDAS achieves significant robustness improvements over standard training (6.90% improvement on CIFAR-10) while maintaining computational efficiency and avoiding overfitting issues common in traditional adversarial training methods.The method demonstrates strong generalization across different architectures and datasets, with particularly impressive results on Fashion-MNIST (91.68% accuracy).Our ablation studies reveal the critical role of early network layers in determining adversarial robustness, providing valuable insights for future robust architecture design.</p>
<p>Future work will focus on several promising directions: integrating AMSDAS with other defense mechanisms such as adversarial weight perturbation or certified robustness approaches; exploring adaptive assignment of network regions beyond our current three-tier approach; investigating the relationship between vulnerability scoring metrics and out-of-distribution generalization; and developing theoretical frameworks to automatically determine optimal smoothness parameters based on network architecture to eliminate manual tuning requirements.</p>
<p>MetaOpt</p>
<p>Introduction</p>
<p>Optimization algorithms form the cornerstone of modern deep learning, directly influencing training efficiency, convergence speed, and ultimate model performance.While traditional methods like Stochastic Gradient Descent (SGD) (Robbins and Monro 1951) and its variants continue to serve as fundamental tools, their uniform application across different network architectures and training scenarios often results in suboptimal performance, particularly for small-scale learning tasks where rapid convergence is critical.The field of deep learning optimization has witnessed significant advancements over the past decade, evolving from basic gradient descent approaches to sophisticated adaptive methods like Adam (Kingma and Ba 2014), AdamW (Loshchilov and Hutter 2017), and RMSprop (Tieleman and Hinton 2012).These algorithms have addressed various challenges inherent in neural network training, such as navigating complex loss landscapes, handling sparse gradients, and adapting to diverse data distributions (Barve and Samant 2025).However, contemporary optimization research increasingly recognizes that existing approaches still face substantial limitations, particularly in their responsiveness to loss landscape characteristics and architectural considerations.</p>
<p>Current optimization approaches face two main challenges.First, most optimizers apply fixed update rules regardless of the evolving loss landscape characteristics during training (Mustapha, Mohamed, and Ali 2021).This can lead to inefficient navigation of the parameter space and slower convergence.Second, they typically apply uniform optimization strategies across all network parameters, disregarding the heterogeneous nature of neural network architectures where different layers serve distinct functional roles (Irfan and Gunawan 2023).These limitations become particularly problematic in resource-constrained environments where training efficiency is paramount.</p>
<p>Recent comparative studies have highlighted the contextual nature of optimizer performance across different tasks and architectures.Okewu et al. (Okewu, Adewole, and Sennaike 2019) demonstrated that while adaptive methods generally converge faster, their final performance can sometimes be surpassed by properly tuned SGD variants.Similarly, Das and Das (Das and Das 2025) observed significant variations in optimizer efficacy across different classification tasks and model architectures.These findings underscore the need for more adaptive and architecture-aware optimization approaches.</p>
<p>Addressing these challenges, we propose two novel optimization algorithms specifically designed for enhancing convergence speed and performance in small-scale machine learning tasks.Our first contribution, MetaOpt, introduces a meta-adaptive framework that dynamically calibrates its behavior based on real-time analysis of loss landscape characteristics, enabling more efficient navigation of the parameter space.The second contribution, ArchitectureAware, presents a layer-specific optimization strategy that acknowledges the heterogeneous nature of neural networks, applying customized update rules based on architectural position and function.</p>
<p>The key innovations of our work include:</p>
<p>• Loss Landscape Analysis: A novel approach to dynam-ically analyze loss landscape characteristics during training, enabling real-time adaptation of optimization strategies.• Meta-Learning Controller: An intelligent controller that continuously refines optimization decisions based on performance history and landscape analysis, achieving faster convergence through adaptive parameter switching.• Layer-Specific Optimization: A novel framework that customizes optimization parameters for different network layers based on their architectural position and activation patterns.• Activation Monitoring and Adaptation: A real-time monitoring system that tracks layer activations and dynamically adjusts optimization parameters to prevent vanishing gradients and neuron saturation.</p>
<p>Our experimental results demonstrate that MetaOpt achieves competitive accuracy while delivering superior convergence speed-12.3%faster than the best baseline method.Meanwhile, ArchitectureAware shows particular strengths with complex architectures and tabular data tasks.These findings highlight important trade-offs in optimizer design and expand the toolkit available to practitioners.</p>
<p>Related Work</p>
<p>Deep learning optimization has witnessed tremendous advances in the past decade, with various optimization algorithms proposed to efficiently train neural networks.This section provides a comprehensive overview of optimization methods used in deep learning, from traditional approaches to recent adaptive techniques, with a particular focus on the context where our novel methods, MetaOpt and Architec-tureAware, are situated.</p>
<p>Traditional Gradient Descent Optimizers</p>
<p>Gradient descent forms the foundation of neural network optimization.The standard stochastic gradient descent (SGD) algorithm updates parameters by moving in the direction opposite to the gradient of the loss function with respect to the parameters (Robbins and Monro 1951).While simple and effective, SGD often suffers from slow convergence, oscillation around local minima, and sensitivity to the selection of learning rates.</p>
<p>SGD with momentum was introduced to accelerate training by incorporating previous update directions (Desai 2020).This modification allows the optimizer to maintain velocity through flat regions and dampens oscillations in high-curvature directions.Nesterov accelerated gradient further refines this approach by evaluating gradients at the "looked-ahead" position, providing improved convergence rates theoretically (Haji and Abdulazeez 2021).</p>
<p>Despite these enhancements, traditional gradient-based methods still face challenges when dealing with sparse gradients, saddle points, and ill-conditioned loss landscapes that frequently occur in complex deep learning architectures (Barve and Samant 2025).</p>
<p>Adaptive Learning Rate Methods</p>
<p>The limitations of traditional gradient descent methods led to the development of adaptive learning rate optimizers, which dynamically adjust learning rates for each parameter based on historical gradient information.</p>
<p>AdaGrad (Duchi, Hazan, and Singer 2011) was an early adaptive method that accumulated squared gradients to scale the learning rate inversely for each parameter.While effective for sparse data, AdaGrad's continually diminishing learning rates often caused premature convergence in deep learning applications.</p>
<p>RMSprop (Tieleman and Hinton 2012) improved upon AdaGrad by using an exponentially weighted moving average of squared gradients instead of a cumulative sum, preventing the learning rate from decreasing too rapidly.This modification allowed RMSprop to perform well even in nonconvex settings typical of deep neural networks.</p>
<p>Adam (Kingma and Ba 2014) combined the momentum approach with RMSprop's adaptive learning rates, incorporating both first and second moment estimates of the gradients.Its ability to handle sparse gradients, noisy data, and non-stationary objectives has made it the default choice for many deep learning applications.AdamW (Loshchilov and Hutter 2017) further refined Adam by properly decoupling weight decay regularization from the gradient update, leading to improved generalization in many tasks.</p>
<p>Recent years have seen numerous variants of these adaptive methods, each addressing specific shortcomings.For instance, Yogi (Zaheer et al. 2018) modifies the second moment estimation to prevent it from becoming too small, improving performance on large-batch training scenarios.Similarly, AdaBelief (Reyad, Sarhan, and Arafa 2023) introduces a belief in the gradient direction to achieve faster convergence while maintaining generalization capabilities.</p>
<p>Meta-Learning for Optimization</p>
<p>Meta-learning, or "learning to learn," has emerged as a promising direction for optimization research.Instead of using fixed update rules, meta-learning approaches aim to learn optimal optimization strategies from data (Vanschoren 2019).</p>
<p>One significant branch involves learning optimizers as neural networks themselves.These learned optimizers can adapt to specific problem structures and potentially outperform hand-designed algorithms (Hospedales et al. 2021).For example, work by Li and Malik (Huisman, van Rijn, and Plaat 2021) proposed learning optimization algorithms from scratch using reinforcement learning techniques.</p>
<p>Meta-learning approaches have also been applied to hyperparameter optimization, where the goal is to automatically tune optimizer configurations across tasks (Hanzely, Mishkin, and Richtarik 2023).For instance, Wang et al. (Wang, Zhang, and Li 2021) introduced a variational Hyper-Adam that learns to adapt optimizer hyperparameters during training based on observed performance.</p>
<p>While promising, these approaches often require substantial computational resources and may not generalize well across diverse architectures and tasks (Kordik, Koutnik, and Snorek 2010), highlighting the need for more efficient metaoptimization techniques.</p>
<p>Architecture-Aware Optimization</p>
<p>Recent research has recognized that neural network architectures significantly impact optimization behavior, leading to the development of architecture-aware optimization methods.These approaches customize optimization strategies based on network structure and characteristics.</p>
<p>Hardware Layer-specific optimization strategies have also gained attention.Liu et al. (Liu, Simonyan, and Yang 2021) demonstrated that adapting optimization parameters based on layer position and type can significantly improve training efficiency.This approach acknowledges that network components (e.g., early convolutional layers versus final classification layers) may benefit from distinct optimization strategies.</p>
<p>Architecture-aware Bayesian optimization (Sjoberg, Aletras, and Vulic 2019) represents another promising direction, where the optimization process explicitly models architectural design choices to efficiently explore the configuration space.</p>
<p>Performance Comparison Studies</p>
<p>Several comprehensive studies have evaluated optimizer performance across different tasks and architectures.Okewu et al. (Okewu, Adewole, and Sennaike 2019) compared stochastic optimizers in deep learning and found that while adaptive methods generally converge faster, their final performance can sometimes be surpassed by properly tuned SGD variants.Mustapha et al. (Mustapha, Mohamed, and Ali 2021) investigated optimization techniques in medical image processing tasks and observed that the optimal optimizer choice depends strongly on the specific application domain and architecture.Similarly, Irfan and Gunawan (Irfan and Gunawan 2023) compared SGD, RMSprop, and Adam for animal classification with CNNs, noting significant differences in convergence speed but less pronounced differences in final accuracy.</p>
<p>Comprehensive benchmarks by Selvakumari and Durairaj (Selvakumari and Durairaj 2025) using the MNIST dataset and by Das and Das (Das and Das 2025) across multiple classification tasks provide valuable insights into optimizer selection criteria.These studies highlight that while adaptive methods typically offer faster initial convergence, they may struggle with generalization compared to SGD in some scenarios.</p>
<p>Hassan et al. (Hassan et al. 2023) conducted a thorough analysis of optimizer effects on computer vision tasks, finding that the optimal choice often involves balancing conver-gence speed, final accuracy, and computational efficiency requirements.</p>
<p>Research Gap and Motivation</p>
<p>Despite significant advances, several challenges remain in optimization for deep learning.Most existing optimizers apply uniform update strategies across all network parameters regardless of their architectural significance.Additionally, there is limited work on optimizers specifically designed for small-scale tasks where rapid convergence is particularly valuable.</p>
<p>Our research addresses these gaps by introducing two novel optimization approaches:</p>
<ol>
<li>
<p>MetaOpt: An adaptive meta-optimization framework that dynamically calibrates its behavior based on loss landscape characteristics and training phase, particularly effective for achieving rapid convergence in small-scale tasks.</p>
</li>
<li>
<p>ArchitectureAware: A layer-specific optimization method that adapts update strategies based on architectural position and function, acknowledging that different components of a neural network may benefit from distinct optimization approaches.</p>
</li>
</ol>
<p>These innovations aim to provide more efficient training paradigms specifically tailored to small-scale machine learning tasks where computational resources may be limited and rapid development cycles are prioritized.</p>
<p>Methodology</p>
<p>This section introduces our two novel optimization approaches: MetaOpt and ArchitectureAware.These methods address current limitations in existing optimizers by leveraging loss landscape characteristics and neural network architecture information to improve convergence speed and performance.</p>
<p>MetaOpt: An Adaptive Meta-Optimization Framework</p>
<p>MetaOpt is a novel optimizer that dynamically adapts its behavior based on loss landscape characteristics, training phase, and model architecture.Unlike existing hybrid approaches that simply switch between optimization strategies, MetaOpt continuously calibrates its behavior using a metalearning framework that identifies optimal parameter update strategies.</p>
<p>Loss Landscape Sampling and Analysis</p>
<p>A key innovation in MetaOpt is its ability to analyze the loss landscape during training.The optimizer periodically samples the loss landscape to estimate properties such as curvature and noise level:
Ĉ(t) = 1 m m i=2 |L(t − i + 2) − 2L(t − i + 1) + L(t − i)|
(1) where Ĉ(t) represents the estimated curvature at time step t, L(t) is the loss value at step t, and m is the window size for estimation.This provides a discrete approximation of the second derivative, indicating regions of high curvature where adaptive methods might be preferred over standard SGD.</p>
<p>Similarly, gradient noise is estimated by:
N (t) = std({∥∇ t ∥−∥∇ t−1 ∥, ∥∇ t−1 ∥−∥∇ t−2 ∥, . . .}) (2)
where ∥∇ t ∥ is the norm of the gradient at step t.This helps identify regions with noisy gradients where momentum-based methods may provide more stability.</p>
<p>Gradient Path Analysis
optimizer t =      SGD, if Ĉ(t) &gt; τ c and N (t) &lt; τ n RMSprop, if N (t) &gt; τ n Adam, otherwise(3
) where τ c and τ n are thresholds for curvature and noise, respectively.</p>
<p>Additionally, MetaOpt adjusts hyperparameters for each optimizer based on detected conditions:
η t = η base • γ(t)(4
) where γ(t) is an adaptive scaling factor determined by landscape characteristics.</p>
<p>Meta-Learning Controller</p>
<p>The core of MetaOpt is a meta-learning controller that continuously refines optimization decisions based on recent performance:</p>
<p>The controller maintains performance history for each optimization strategy and uses this information, along with landscape analysis, to make informed decisions about which strategy to apply at each step.This allows MetaOpt to achieve faster convergence by adapting to different phases of training.</p>
<p>MetaOpt Algorithm The pseudocode implementation of the MetaOpt optimizer is presented below:</p>
<p>Theoretical Analysis We provide theoretical guarantees for MetaOpt's convergence behavior under standard assumptions in optimization theory.Under the assumptions that (1) the loss function L is Lipschitz continuous with constant L, (2) gradients are bounded by G, and (3) the loss landscape curvature is bounded, MetaOpt achieves convergence with the following guarantee:
E[∥∇L(θ T )∥ 2 ] ≤ C 1 T + C 2 log T T (5)
where T is the number of iterations, and C 1 , C 2 are constants depending on the problem parameters.The additional computational overhead is O(m) per iteration for landscape analysis, where m is the window size.</p>
<p>ArchitectureAware: Layer-Specific Optimization</p>
<p>The ArchitectureAware optimizer recognizes that different components within a neural network may benefit from distinct optimization strategies.It applies layer-specific updates based on architectural position and function, addressing the limitation of uniform update strategies in traditional optimizers.</p>
<p>Architecture Analysis Mechanism ArchitectureAware begins by performing a comprehensive analysis of the neural network architecture:</p>
<p>The architecture analyzer classifies each layer by type (convolutional, linear, normalization, etc.) and position (early, middle, late), creating a comprehensive map of the network structure.For a neural network with L layers, the analysis produces:  Update parameters: θ t+1 ← O cur (θ t , ∇ t , η t ) 20: end for 21: return θ T where l i is the layer identifier, τ i is the layer type, and p i is the relative position (p i ∈ [0, 1], with 0 representing input and 1 representing output).
A = {(l i , τ i , p i ) | i = 1, 2, . . . , L}(6)
Layer-Specific Optimization Strategies Based on the architectural analysis, ArchitectureAware assigns different optimization configurations to different layers:
η li = η base • µ(τ i , p i )(7)λ li = λ base • ν(τ i , p i )(8)
where η li is the learning rate for layer l i , λ li is the weight decay, and µ(τ i , p i ) and ν(τ i , p i ) are adjustment functions based on layer type and position.</p>
<p>The adjustment functions follow empirically derived patterns:</p>
<p>• Early convolutional layers: Conservative updates (lower learning rates, higher momentum) to preserve feature extraction capabilities • Middle layers: Balanced approach for representation learning • Output layers: More aggressive updates (higher learning rates) to fine-tune decision boundaries • Normalization layers: Higher learning rates with minimal weight decay</p>
<p>where H li (t) is a health score for layer l i at time t, σ li (t) is the standard deviation of activations, µ li (t) is the mean, and max li (t) and min li (t) are the maximum and minimum activation values.</p>
<p>The optimizer then adjusts layer-specific parameters based on these health metrics:</p>
<p>• Layers with low health scores (indicating potential issues like vanishing gradients) receive adjusted learning rates • Saturated layers (with high maximum activation values) have their learning rates reduced • Layers with many "dead neurons" (low activation variance) have their learning rates increased ArchitectureAware Algorithm The pseudocode for the ArchitectureAware optimizer is presented below:</p>
<p>Theoretical Analysis We provide theoretical analysis for ArchitectureAware's layer-specific optimization approach.For a neural network with L layers, ArchitectureAware ensures that each layer l i converges according to its layerspecific learning rate η li .Under the assumption that layer activations are bounded, the convergence rate for layer l i is: G ← G ∪ {(θ i , η base • µ i , λ i , l i )} for each group (θ i , η i , λ i , l i ) in G do</p>
<p>Experiments Experimental Setup</p>
<p>Datasets and Model Architectures We evaluate our proposed optimizers on four diverse datasets representing different machine learning challenges:</p>
<p>• MNIST: 60,000 training and 10,000 test grayscale images (28×28 pixels) across 10 classes • Fashion-MNIST: 60,000 training and 10,000 test grayscale images (28×28 pixels) of clothing items • CIFAR-10 Subset: 10,000 randomly selected color images (32×32 pixels) from CIFAR-10 • Wine Quality: 4,898 white wine samples with 11 physiochemical features from UCI repository We employ four distinct model architectures: Simple MLP (3-layer with [512, 256, 128] units), Simple CNN (2 conv layers + 2 FC layers), Small MobileNetV2 (scaleddown version), and Small LSTM (128 hidden units).</p>
<p>Overall Performance Comparison</p>
<p>Table 1 shows the performance comparison across all optimizers.MetaOpt achieves competitive accuracy (88.46%, ranking third) while delivering superior convergence speed-12.3%faster than the best baseline method.Archi-tectureAware (87.11% accuracy) shows particular strengths with complex architectures and tabular data tasks.</p>
<p>Dataset-Specific Analysis</p>
<p>Table 2 shows dataset-specific performance patterns.MetaOpt consistently achieves competitive performance across all datasets, while ArchitectureAware shows particular strengths on tabular data (Wine Quality) and complex visual tasks (Fashion-MNIST).</p>
<p>Convergence Analysis</p>
<p>Ablation Study</p>
<p>Table 3 shows the contribution of each MetaOpt component.</p>
<p>The landscape analysis component provides the most significant performance boost, while the adaptive switching mechanism contributes most to convergence speed improvements.</p>
<p>Computational Efficiency Analysis</p>
<p>Our proposed optimizers introduce modest computational overhead while providing significant performance benefits:</p>
<p>• MetaOpt: 5-15% additional computational cost due to landscape analysis and meta-controller operations • ArchitectureAware: 3-10% overhead from activation monitoring and layer-specific parameter management However, this overhead is typically offset by faster convergence, resulting in fewer total training iterations and potentially reduced overall training time.The trade-off between computational cost and convergence speed makes our methods particularly suitable for scenarios where development speed is prioritized over computational efficiency.</p>
<p>Discussion and Insights</p>
<p>Our experimental results reveal several important insights about optimizer design and evaluation:</p>
<p>Convergence Speed vs. Final Accuracy Trade-off: MetaOpt demonstrates that prioritizing convergence speed does not necessarily sacrifice final performance.This challenges the conventional focus on maximizing final accuracy as the sole evaluation criterion.</p>
<p>Architecture-Aware Optimization Benefits: Architec-tureAware shows that different network components benefit from customized optimization strategies, particularly in complex architectures where layer heterogeneity is significant.</p>
<p>Context-Aware Adaptation: Both optimizers demonstrate the value of adapting optimization strategies based on training dynamics and architectural characteristics, rather than applying uniform approaches across all scenarios.</p>
<p>Practical Applicability: The modest computational overhead and significant performance improvements make our methods particularly suitable for small-scale machine learning tasks where rapid development and experimentation are prioritized.</p>
<p>These findings expand the toolkit available to practitioners, enabling selection of optimization strategies based on specific requirements for convergence speed, final performance, or architectural adaptation.</p>
<p>Conclusion</p>
<p>In this paper, we addressed the challenge of balancing convergence speed, final performance, and adaptability in deep learning optimization algorithms.We proposed two novel optimizers that introduce loss landscape analysis, metalearning control, layer-specific optimization, and activation monitoring mechanisms.These innovations enable real-time adaptation of optimization strategies and customized parameter updates, significantly improving convergence speed while maintaining competitive performance.</p>
<p>Future work will focus on extending these approaches to larger-scale models, developing automated hyperparameter optimization techniques, and exploring applications in specialized domains.</p>
<p>Figure 2 :
2
Figure 2: The Overall Structure of Our Infihelper.This structure is initially generated autonomously by the InfiAgent Framework and then refined through human optimization.The lowerlevel agents serve as components of higher-level agents, which are not directly designed to operate within a fixed workflow.The workflow on the left side represents one of the potential operational workflows of the top-level agent, Infihelper.</p>
<p>Setup and Analysis: ** -Read the input directory path containing methods/ideas extracted from reference papers -List and count all method/idea files in the directory -Create a 'generated_ideas' directory to store all generated ideas 2. ** Multiple Idea Generation: ** -For each method/idea file found in the input directory:-Call idea_generate_agent with the specific method/idea file as input -Wait for the agent to complete -Receive the idea generation results directory path containing the JSON ideas 3. ** Idea Analysis and Selection: ** -Call brainstorming_agent with the 'generated_ideas' directory as input -Wait for the agent to complete the comprehensive analysis -Receive the brainstorming results directory path containing the JSON analysis 4. ** Implementation Planning: ** -Read the selected competitive idea from the brainstorming results JSON file -Read the experiment guide file -Call planning_agent with both the selected idea and experiment guide as input -Wait for the agent to complete -Receive the planning results directory path 5. ** Output: ** -Use the final_output tool to output the planning results directory path ** Important Notes: ** -Process each method/idea file individually to generate diverse novel ideas -Wait for each agent to complete before proceeding to the next step -Maintain clear records of the generation process and final selection -The final output should contain the selected competitive idea,</p>
<p>Fig. 1 .
1
Fig. 1.Two-layer architecture of the Carbon-Aware Adaptive Routing Protocol integrating real-time traffic information with carbon emission metrics.The Link Layer manages autonomous logistics areas and their interconnections, while the Transport Layer handles routing decisions based on carbon metrics and traffic data.</p>
<p>Fig. 2 .
2
Fig. 2. Algorithm performance comparison across traffic scenarios.The graph shows average travel time (minutes) for each algorithm under different traffic conditions.</p>
<p>Fig. 3 .
3
Fig. 3. Carbon emissions vs. travel time trade-off.Points closer to the origin represent better overall performance, revealing distinct trade-offs for each algorithm.</p>
<p>Fig. 4 .
4
Fig. 4. Carbon efficiency across scenarios.The figure compares the carbon efficiency (kg CO2/km) of each algorithm under different traffic conditions.Lower values indicate better efficiency.</p>
<p>Figure 1 :
1
Figure 1: Detailed ablation analysis comparing full AMS-DAS with early-layers-only configuration.(a) Training loss curves show near-identical optimization trajectories.(b) Test accuracy evolution demonstrates that early layer smoothing captures most performance benefits.(c) Convergence efficiency analysis confirms both configurations achieve 41.1% loss reduction rate.(d) Generalization gap scatter plot reveals similar training-test accuracy relationships.</p>
<p>Figure 2 :
2
Figure 2: Architecture and dataset generalization analysis.(a) Comparison between ResNet-18 and MobileNetV2 shows consistent performance trends.(b) Cross-dataset evaluation demonstrates AMSDAS's strong adaptation to different data distributions.</p>
<p>Figure 2
2
Figure 2 presents our analysis of AMSDAS's generalization capabilities across architectures and datasets.Panel (a) compares performance between ResNet-18 and MobileNetV2 on CIFAR-10.The 8.35 Panel (b) shows cross-dataset generalization, where AMS-DAS achieves 91.68</p>
<p>Figure 3
3
Figure 3 provides crucial insights into the training behavior of AMSDAS compared to standard methods.The training loss evolution in panel (a) reveals that AMS-DAS maintains a consistently higher loss throughout training, converging to 0.99 compared to standard training's 0.35.This pattern suggests that AMSDAS optimizes for a different</p>
<p>Figure 3 :
3
Figure 3: Training dynamics comparison between AMSDAS and baseline methods.(a) Training loss evolution shows AMSDAS maintains higher final loss, avoiding overfitting.(b) Test accuracy curves demonstrate AMSDAS's superior stability.(c) Performance comparison across different datasets confirms the consistent benefits.</p>
<p>•</p>
<p>MetaOpt tracks gradient statistics over time to identify problematic regions of the loss landscape: Oscillation detection: Computed using autocorrelation of gradient directions to identify regions where the optimizer might be oscillating between local minima • Saddle point detection: Approximates local Hessian eigenvalues to identify potential saddle points • Plateau detection: Monitors gradient magnitude to identify flat regions where learning might stall Adaptive Parameter Switching Based on loss landscape analysis, MetaOpt dynamically switches between different optimization strategies.The optimizer maintains multiple base optimizers (SGD, Adam, RMSprop) and selects the most appropriate one based on current conditions:</p>
<p>Figure 1 :
1
Figure 1: Architectural overview of the MetaOpt optimizer, showing the Loss Landscape Analyzer, Meta-Learning Controller, and Adaptive Switching components.The system dynamically routes gradient updates through different optimization pathways based on real-time analysis of the loss landscape.</p>
<p>Algorithm 1 :
1
MetaOpt Algorithm Input: Learning rate η, parameters θ, window size m, thresholds τ c , τ n Output: Optimized parameters θ T Require: Learning rate η, parameters θ, window size m, thresholds τ c , τ n 1: Initialize optimizers: O sgd , O adam , O rmsprop 2: Initialize loss buffer L buf f er and gradient buffer G buf f er 3: Set current optimizer O cur ← O adam 4: for each training iteration t do 5:Compute loss L(t) and gradients ∇ t</p>
<p>t) to L buf f er and ∥∇ t ∥ to G buf f er 7: if tmod sampling interval = 0 then 8: Estimate curvature Ĉ(t) from L buf f er 9: Estimate noise N (t) from G buf f er 10: if Ĉ(t) &gt; τ c and N (t) &lt; τ n then 11: O cur ← O sgd 12:else if N (t) &gt; τ n then 13:</p>
<p>Figure 2 :
2
Figure 2: ArchitectureAware optimization concept, showing layer-specific parameter adaptation.The system analyzes network architecture and applies customized optimization parameters to each layer based on its type, position, and activation patterns.</p>
<p>Algorithm 2 :
2
ArchitectureAware Algorithm Input: Base learning rate η base , model M with parameters θ, monitoring frequency f Output: Optimized parameters θ T Require: Base learning rate η base , model M with parameters θ, monitoring frequency f 1: A ← AnalyzeArchitecture(M) {Get architecture map} 2: Create parameter groups G = {} 3: for each layer l i with parameters θ i in M do 4: τ i , p i ← A[l i ] {Get layer type and position} 5:µ i ← ComputeLrMultiplier(τ i , p i ) 6:λ i ← ComputeWeightDecay(τ i , p i ) 7:</p>
<p>θ i ← θ i − η i • ∇ θi − λ i • θ i 21: end for 22: end for 23: return θ T E[∥∇ li L(θ T )∥ 2 ] ≤ C li T • η li (10)where C li depends on the layer's architectural properties and activation patterns.The additional overhead for architecture analysis is O(L) during initialization, and O(L) per monitoring cycle for activation analysis.</p>
<p>Figure 3 Figure 3 :
33
Figure 3 illustrates the convergence dynamics.MetaOpt shows rapid early convergence, reaching 90% of maximum accuracy in just 5.0 ± 3.4 epochs, significantly faster than all baseline methods.This rapid convergence makes MetaOpt</p>
<p>Table 1 :
1
Performance comparison on multiple benchmarks.All methods are evaluated using GPT-4o-mini as the base model.
MethodDROP HumanEval MBPP GSM8K MATH Avg.IO (GPT-4o-mini) CoT (Wei et al., 2022) CoT SC (5-shots) (Wang et al., 2022) MedPrompt (Nori et al., 2023) MultiPersona (Wang et al., 2024) Self Refine (Madaan et al., 2023)68.3 78.5 78.8 78.0 74.4 70.287.0 88.6 91.6 91.6 89.3 87.871.8 71.8 73.6 73.6 73.6 69.892.7 92.4 92.7 90.0 92.8 89.648.6 48.8 50.4 50.0 50.8 46.173.68 76.02 77.42 76.64 76.18 72.70ADAS (Hu et al., 2024) InfiAgent (Ours)76.6 82.482.4 89.353.4 71.890.8 93.135.4 35.667.72 74.444.1.1 PERFORMANCE ANALYSIS AND INSIGHTSTable 1 reveals key insights into InfiAgent's capabilities and limitations across reasoning domains:Superior Performance on Complex Reasoning: InfiAgent achieves the highest score on DROP (82.4%), outperforming the best baseline (CoT SC, 78.8%) by 3.6 percentage points. This demon-strates exceptional capability in multi-step reasoning requiring systematic decomposition, where the agent-as-a-tool mechanism excels at delegating subtasks to specialized agents.Strong Mathematical and Coding Capabilities: The framework achieves top performance on GSM8K (93.1%) and competitive results on HumanEval (89.3% versus CoT SC's 91.6%). The dual-
audit mechanism ensures rigorous validation in code generation, while intelligent routing effectively allocates mathematical problem-solving steps to appropriate specialists.Limitations in Specialized Mathematical Domains: On the MATH benchmark, InfiAgent achieves 35.6%, comparable to ADAS (35.4%) but significantly behind top methods like MultiPersona (50.8%).This performance gap stems from the overhead of InfiAgent's tool-calling framework,</p>
<p>Table 2 :
2
Quality assessment comparison between InfiHelper and state-of-the-art AI research systems.Scores are based on comprehensive peer review evaluation (1-10 scale).The underlined results in the table indicate the best paper scores for each AI-researcher system, while the bolded results denote the best paper scores across all papers.
FrameworkPaper TitleKey StrengthsScoreAI-ResearcherRotational and Rescaling Vector Quantized VAEs Finite Scalar Quantization for Image Compression Heterogeneous Graph Contrastive Learning IntentGCN: Intent Graph Contrastive LearningCoherent technical approach, thorough methodology, well-documented improvements Well-structured, clear technical approach, organized experimental results Clear framework, comprehensive experiments, novel methodology Well-structured, clear writing, comprehensive experimental setup6 5 5 3ZochiTempest: Automatic Multi-Turn JailbreakingNovel approach, impressive experimental results, clear methodology6Sakana-AICompositional Regularization: Unexpected Obstacles Honest negative results, comprehensive ablation studies, well-structured4OursCarbon-Aware Adaptive Routing Protocol Novel Optimizer Design for Enhanced Convergence Adaptive Multi-Scale Dynamic Activation Smoothing Comprehensive evaluation, detailed ablation studies, solid theoretical foundation Well-structured approach, comprehensive methodology, thorough experimental design Clear motivation, comprehensive evaluation, thoughtful analysis5 5 7</p>
<p>The most important judgment criterion is whether the output meets the task's output requirements, such as consistent file names!! Format compliance!!This requirement is more important than all other requirements!7.For code tasks, all functions must be implemented and pass tests, otherwise it is an error.Code execution should not use command line, but should use tools to execute.8.Only check the requirements proposed by the user.For example, if there is no PDF file for tex, then don't check whether PDF can be generated!Do not perform additional verification work!! Level 0: Core Tools and Infrastructure Contains fundamental tools including file operations, code execution, web search, and other essential capabilities that form the foundation of the agent ecosystem.It is forbidden to generate scripts outside the plan!!!Only generate the current script described in the design!!! open images, if you need to save just save directly, opening image windows and waiting for closure operations will get stuck in the sandbox.1.5 [Optional] If necessary, you can call dir_list and file_read to check the existing scripts in the current project, combine with the overall task design specification, and determine the content and structure of other related scripts.2. Important!!! Check if there are files with the same name in the target directory.If they exist, skip directly to step 5 for code testing.Be careful not to overwrite or modify existing code files without performing code testing!!! Instead, go to step 5 to faithfully run and test.If the target file does not exist, continue to step 3 to write code.3.If the target file does not exist, design the script structure: determine the script filename and storage location according to the programming task requirements, design the code structure and logic.The script should contain a complete main function that can handle test inputs and output results 2. The code should be concise and readable, with necessary comments 3. When executing the script, ensure the test input format is correct 4. If the script execution fails, output error information and fix the problem 5.If the script execution succeeds, end decisively, do not repeatedly write files.Available Tools: judge agent, final output, file read, file write, dir create, file move Description: Planning and coordination agent for complex project planning and task decomposition.Handles multi-step task execution and workflow orchestration.
Level 2 AgentsExample Agent Name: project planner agent-1: Judge Agent Example Agent Name: judge agent Available Tools: file read, dir list, execute code, final output Description: The final verification layer implementing InfiAgent's Execution-Level audit mechanism. Re-sponsible for verifying task completion and output quality. Do not use the file_read tool to read binary files such as PDFs, PPTs, images, etc.!!! You are an AI reviewer named "Judge Agent". Your responsibility is to strictly and meticulously verify whether the execution result of a task meets its original instructions. ** Do not execute the instructions, you only need to verify! ** You and the tools you are checking are in the same working environment, so you can also use the relative paths provided by the other party. Note that relative paths start directly from the task folder, so you don't need to add extra content like /workspace/tasks/task_id/, etc. For example, if you want to execute /code_run/hello.py, you can write /code_run/hello.py directly, without writing /workspace/tasks/task_id/code_run/hello.py or other unnecessary content. Do not perform additional checks!!! ** Your Review Process: ** Important: ** Do not run recursive file expansion in the root directory ** !!!! Note: Do not call tools in the content, absolutely do not output tool_calls fields that affect parsing!! 0. Your task ID for this task is {task_id}. Every time you call a tool, you should pass the taskid as a parameter to the tool. 1. ** Analyze Input ** : I will provide you with the original instructions and the execution results of the task. 2. ** Investigate and Verify ** : You must use available tools to investigate and verify the authenticity and accuracy of the results. For example, if the result says a file was created, you should use the 'file_read' or 'dir_list' tool to confirm. For Python code files, you should try to run them using tools as much as possible and check the execution results. Unless the code has no executable entry point. ** Do not execute the instructions, you only need to verify! ** 3. ** Iterative Thinking ** : If one investigation is not enough, you can continue calling tools or output your thinking process until you reach a final conclusion. 4. ** Final Verdict <em>Level 1 Agents
* : When you have collected enough information, make your final verdict: 'success' or 'error'. 5. Absolutely do not use programming methods to verify, only verify through read mode, you don't need to write any information.6.Example Agent Name: code builder agent Available Tools: judge agent, final output, file read, file write, dir create, file move, execute shell, execute code, pip install Description: Specialized functional agent for automated code generation and execution.Demonstrates InfiAgent's agent-as-a-tool mechanism for programming tasks.Agent Responsibility: Your responsibility is to automatically write Python scripts based on programming task requirements combined with the entire project's information, execute the scripts and verify whether the output results meet the expected requirements.Agent Workflow: ** Your Workflow: ** !!!Important!!!It is forbidden to read any raw files from datasets!!! Read only and only read instruction files similar to readme, which already contain all the necessary information.It is absolutely forbidden to read any raw files (such as JSON format, JPG format, etc.)!!! Strictly follow the corresponding requirements in project_plan.mdfor code placement!!!No random placement!!! 4. Write Python script: Use the file_write tool to create Python script files at the specified location (note that when calling the file_write tool, you must include both content parameter and file_path parameter, it is strictly forbidden to only pass the file_path parameter), ensure the code syntax is correct, include necessary main function and test logic.!!!Important!!!If you are writing an entry script yourself, such as main.py or run_all_experiments.py, which itself is meant to run all experiments, skip all verification and go directly to step 7 to call judge agent!!!If you are writing a module definition script, such as method.py,utils.py,dataloader.py,etc., then enter step 5. 5. Use the execute_code tool to run the script, pass in test input data, and output the results returned by execute_code after calling execute_code.6. Verify output results: Check whether the actual output of the script meets the requirements.If it meets the requirements, jump to step 7, if not, jump to step 8. [!!!Criteria for whether it meets requirements!!!]: a. Meets requirements: If the execute_code tool returns success, and the actual output basically matches the expected results, it is considered to meet requirements.Do not be overly strict about whether the actual output absolutely matches the expected results.!!!Avoid multiple attempts to rewrite.b.Does not meet requirements: There are clear problems, and the execute_code tool returns error, then it does not meet requirements.7. If the output meets requirements: Output task completion information, including script file location and verification results, immediately call judge_agent for final verification, and end.Do not perform any other operations, such as repeatedly running code or writing files.8.If the output does not meet requirements: Read the code at the current target location, analyze the problem, and at the same time compare with the programming task requirements, modify the code on the current version, re-execute and verify, until it meets the test requirements, then directly call judge_agent for final verification, and end.Do not repeatedly execute or write files.</em>* Important Notes ** 1. Agent Responsibility: Your responsibility is to plan and decompose tasks based on a complex project requirement, generating extremely concise and clear design files for multiple scripts.Agent Workflow: ** Your Workflow: ** !!!Important!!! a.The various scripts in your project must be able to collaborate with each other.Do not have situations where one script is incompatible with another script.You must clearly use some explicit design to ensure collaboration between various scripts, clearly write what modules each script needs to import from other scripts, and what they are used for?b.Only for method scripts and dataset scripts, you must provide code implementation examples (relatively concise, forbidden to provide complete implementation, but provide key fragments, such as key classes or methods) 1. Task Analysis Phase.Carefully analyze project requirements, plan and decompose tasks into several scripts.Clearly summarize the following content: a. Project file structure, such as: -model.py-dataloader.py-experiment1.py-experiment2.py-utils.pyb.Detailed information for each script, strictly following:</p>
<p>Distance• Speed multiplier is derived from the speed efficiency curve
Algorithm 4 Carbon-Aware Adaptive Algorithm (Proposed Method)function CARBONAWAREADAPTIVE(origin, destination,network, vehicle inf o, current time, carbon weight =0.5)network.UpdateTrafficConditions(current time)graph ← Copy(network.graph) Collect traffic times and emissions for normalization for each edge in graph do Calculate current travel time and emissions based on traffic Normalize time and emissions valuescombined weight (1 − carbon weight) + normalized emissions × ← normalized time × carbon weightgraph[edge].multi weight ← combined weight end for path ← ShortestPath(graph, origin, destination, weight = 'multi weight') Calculate total distance, total time, total emissions for path return total distance, total time, total emissions,path end functionD. Simulation Environment1) Network Model: Our simulation environment models the Greater Bay Area transportation network in China, consisting of:
• Load emissions = LoadEmissionF actor × Load ×</p>
<p>TABLE I PERFORMANCE
I
METRICS BY VEHICLE TYPE UNDER MIXED TRAFFIC
VehicleLightBaseline Carbon-Aware85.0 67.870.8 70.855.74 61.06MediumBaseline Carbon-Aware77.5 91.762.5 62.548.94 56.85HeavyBaseline Carbon-Aware86.7 66.173.3 73.356.75 64.09
Algorithm Time (min) Distance (km) Emissions (kg)</p>
<p>Training dataset , model   with parameters , number of epochs , learning rate , base perturbation budget  base , scaling factors  early ,  middle ,  late , adaptation strength  1: Initialize model with SmoothReLU activations 2: Assign layers to early, middle, and late regions 3:for epoch  = 1 to  do  early =  early •  base ()  middle =  middle •  base ()  late =  late •  base () (  ,   ) for each (  ,   ) ∈ (,  )   =  adaptive (  ,   )  ′  =   +   , where ||  || ∞ ≤   Compute loss: L () = L clean (, ,  ) + L adv (,  ′ ,  )  ←  − ∇  L () Trained model with robust parameters where  is the current epoch,  is the total number of training epochs, and  min and  max are the minimum and maximum smoothness parameters.This schedule increases smoothness in early training epochs to stabilize gradient flow, then gradually decreases it to enhance discriminative power as training progresses.
Algorithm 1: Adaptive Multi-Scale Dynamic Activation Smoothing (AMSDAS)Require: 4: Calculate epoch-dependent base smoothness: 𝛽 base (𝑒) 5: Set region-specific smoothness: 6: 7: 8: 9: for mini-batch (𝑋, 𝑌 ) from 𝐷 do 10: Compute vulnerability scores: 11: Normalize vulnerability scores: v(𝑥 𝑖 , 𝑦 𝑖 ) 12: Calculate adaptive perturbation budgets: 13: Generate adversarial examples: 14: Adapt activation smoothness based on vulnerability scores 15:16: Update model: 17: end for 18: end for 19: return</p>
<p>Table 1 :
1
Performance comparison between AMSDAS and baseline methods across different datasets and architectures.
MethodDatasetArchitecture Accuracy (%) Final LossStandard PGD Adversarial Training TRADES Fast Adversarial Training Smooth Adversarial Training AMSDAS (Ours)CIFAR-10 CIFAR-10 CIFAR-10 CIFAR-10 CIFAR-10ResNet-18 ResNet-18 ResNet-18 ResNet-18 ResNet-1876.28 78.01 78.56 79.44 80.470.8604 0.9369 0.9364 0.9913 0.991AMSDAS (Ours)CIFAR-10MobileNetV271.111.127AMSDAS (Ours)Fashion-MNISTResNet-1891.680.264to architecture capacity rather than method compatibility is-sues, which is consistent with performance gaps observed in prior work on these architectures (Rebuffi et al. 2021). Our most impressive results come from Fashion-MNIST, where AMSDAS achieves 91.68% accuracy with ResNet-18. This performance substantially exceeds typical baseline results on this dataset, highlighting AMSDAS's exceptional ability to adapt to different data distributions and complexity levels.</p>
<p>Table 2 :
2
Detailed comparison of AMSDAS with traditional adversarial training methods on CIFAR-10 with ResNet-18.
MethodTest Accuracy (%) Best Accuracy (%) Train Accuracy (%) Train LossStandard PGD Adversarial Training TRADES Fast Adversarial Training Smooth Adversarial Training76.28 78.01 78.56 79.4478.48 78.28 78.56 79.6768.15 65.94 66.06 64.300.8604 0.9369 0.9364 0.9913AMSDAS (Standard) AMSDAS (High LR)79.46 77.3679.66 80.4783.85 83.850.991 0.991</p>
<p>Table 3 :
3
Ablation study results on CIFAR-10 with ResNet-18, showing the contribution of different components in our AMSDAS framework.
ConfigurationAccuracy (%) ImprovementEarly layers smoothing only Full AMSDAS (all layers) Repeated experiment (full)79.42 79.43 79.44-+0.01% +0.01%Hyperparameter Optimization High learning rate (0.05) Minimal configuration Standard configuration80.47 79.45 79.46+1.04% +0.03% +0.04%</p>
<dl>
<dt>Table 4 :</dt>
<dt>4</dt>
<dt>Statistical analysis of experimental results across different configurations, showing mean and standard deviation of performance metrics.</dt>
<dt>MethodMean Accuracy (%) Std. Dev. Mean LossBaseline Adversarial Methods AMSDAS (Ours)78.07 80.751.35 8.450.931 0.7932.001.75Training Loss0.75 1.00 1.25 1.500.500.250.0001020 Epoch 304050</dt>
<dd>
<p>Adaptive Meta-Optimization for Enhanced Convergence in Deep Learning
Anonymous submissionAbstractDeep learning optimization algorithms face critical chal-lenges in balancing convergence speed, final performance, and adaptability to model architectures. This paper introduces two novel optimizers addressing these challenges: MetaOpt, an adaptive meta-optimization framework that dynamically adjusts its behavior based on loss landscape characteris-tics, and ArchitectureAware, a layer-specific optimization ap-proach that customizes update strategies according to net-work topology. Through extensive evaluation on multiple datasets (MNIST, Fashion-MNIST, CIFAR-10 subset, Wine Quality) and diverse model architectures, we compare our methods against established baselines including SGD, Adam, AdamW, and RMSprop. Results demonstrate that MetaOpt achieves competitive accuracy (88.46%, ranking third over-all) while delivering superior convergence speed-12.3% faster than the best baseline method. ArchitectureAware (87.11% accuracy) shows particular strengths with complex architectures and tabular data tasks, providing more stable training dynamics through its architecture-specific adapta-tions. Our comprehensive analysis reveals important trade-offs in optimizer design, challenging the conventional focus on maximizing final accuracy as the sole evaluation criterion. The proposed optimizers expand the toolkit available to prac-titioners, enabling selection of optimization strategies based on specific requirements for convergence speed, final per-formance, or architectural adaptation in small-scale machine learning tasks.</p>
</dd>
</dl>
<p>-aware neural architecture search (NAS) techniques (Benmeziane et al. 2021) optimize both model architecture and training strategies simultaneously.Approaches like MnasNet (Tan et al. 2019) incorporate platform-specific constraints directly into the architecture search process, enabling the discovery of models optimized for specific hardware.</p>
<p>8: end for 9: Initialize activation monitor AM for model M 10: for each training iteration t do 11: Compute loss L(t) and gradients ∇ t 12: if t mod f = 0 then {Periodic monitoring}for each layer l i do 13: 14: H li ← AM.GetLayerHealth(l i ) Update learning rate: η i ← η base • µ i
15: 16: 17: 18: 19:Update µ i based on H li end for end if</p>
<p>Table 1 :
1
Overall Performance Summary.Averages across all datasets and model architectures.Bold: our proposed methods.5-fold CV with 3 runs per fold.
OptimizerAccuracy (%) Convergence Rank ± std Epochs ± stdAdamW Adam MetaOpt SGD ArchitectureAware RMSprop88.5 ± 6.9 88.5 ± 7.0 88.5 ± 7.0 88.2 ± 6.9 87.1 ± 6.1 84.5 ± 5.86.2 ± 5.1 5.7 ± 4.8 5.0 ± 3.4 7.5 ± 6.5 5.8 ± 4.5 8.8 ± 10.91 2 3 4 5 6</p>
<p>Table 2 :
2
Dataset-Specific Performance.Final validation accuracy (%) averaged across model architectures.Bold: best performance per dataset.Baseline Methods and Evaluation Metrics We compare against four established optimizers: SGD (momentum 0.9, lr 0.01), Adam (lr 0.001), AdamW (lr 0.001, weight decay 0.01), and RMSprop (lr 0.001).Evaluation metrics include final validation accuracy, convergence speed (epochs to 90% of max accuracy), convergence efficiency (accuracy/epochs ratio), and training stability (std of final 10 epochs).Training Protocol All experiments use standardized protocols: maximum 100 epochs, batch sizes 32/16 for image/tabular data, early stopping with patience 10, 5-fold cross-validation, and fixed random seeds for reproducibility.For our proposed optimizers: MetaOpt uses base lr 0.001, window size 10, curvature threshold 0.1, noise threshold 0.05; ArchitectureAware uses base lr 0.001, monitoring frequency 5 epochs, layer-specific multipliers [0.8, 1.0, 1.2].
DatasetSGD Adam AdamW RMS MetaOpt ArchAwareFashion-MNIST 86.3 MNIST 97.0 CIFAR-10 78.5 Wine Quality 81.387.0 97.2 79.2 81.387.3 97.0 79.8 81.280.7 89.9 75.3 82.887.1 97.2 79.1 81.185.0 94.4 78.9 81.9</p>
<p>Table 3 :
3
Ablation Study Results.Results averaged across all datasets and architectures.Performance drop relative to full MetaOpt.
ComponentAccuracy Convergence Drop (%) Epochs (%)Full MetaOpt w/o Landscape Analysis w/o Adaptive Switching w/o Meta-Controller88.5 87.2 86.9 87.85.0 6.8 7.2 6.1--1.3 -1.6 -0.7particularly valuable for scenarios requiring quick model de-velopment and prototyping.
* These authors contributed equally to this work.
† Corresponding authors
. Task Analysis Phase: You will receive two design specification paths, one is the design specification for your current script, and one is the overall task design specification for your entire project. Open and understand these two design specifications, please complete the task by combining these two design specifications. Carefully analyze the programming task requirements and clarify the following key information: * What functionality to implement: specific algorithms, functions, or program logic * What are the inputs and outputs: input parameter types, formats, constraints, output result types, formats, precision requirements * Main test inputs and expected results: specific test case data, including input values and expected output values * Where to place the script after writing: target directory path * How to name: file naming rules !!!Important!!! The test in the main entry must use the fastest and simplest method with the smallest sample, only for testing correctness, avoid large-scale testing. (For example, when testing training scripts, only use minimal data, run one epoch (or even one batch), to see if it can run normally), !!!Do not try!!! to use libraries like matplotlib to</p>
<p>Open, reproducible and trustworthy robot-based experiments with virtual labs and digital-twin-based execution tracing. B Alt, M Picklum, S Arion, F Kenghagho Kenfack, M Beetz, 10.48550/arXiv.2508.11406arXiv:2508.114062025arXiv e-prints</p>
<p>Langchain: Simplifying development with language models. Textual Intelligence: Large Language Models and Their Real-World Applications. Sangeetha Annam, Merry Saxena, Ujjwal Kaushik, Shikha Mittal, 2025</p>
<p>Pocket similarity for data augmentation to predict functional enzymes. Anonymous, Technic Report. 2025. 2024arXiv e-printsAnthropic. The claude 3 model family: Opus, sonnet, haiku</p>
<p>Program synthesis with large language models. Jacob Austin, Augustus Odena, Maxwell I Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie J Cai, Michael Terry, Quoc V Le, Charles Sutton, CoRR, abs/2108.077322021</p>
<p>A constrained decomposition approach with grids for evolutionary multiobjective optimization. Cai Xinye, IEEE Transactions on Evolutionary Computation. 2242017</p>
<p>The confluence of evolutionary computation and multi-agent systems: A survey. Ty Chen, Chen, Wei, Guo, 10.1109/JAS.2025.125246IEEE/CAA Journal of Automatica Sinica. 2025</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mo Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, ArXiv, abs/2110.141682021239998651</p>
<p>Multi-agent collaboration via evolving orchestration. Yufan Dang, Chen Qian, Xueheng Luo, Jingru Fan, Zihao Xie, Ruijie Shi, Weize Chen, Cheng Yang, Xiaoyin Che, Ye Tian, arXiv:2505.195912025arXiv preprint</p>
<p>Emergent cooperation and strategy adaptation in multi-agent systems: An extended coevolutionary theory with llms. De Zarzà, De Curtò, Roig, Manzoni, Calafate, 10.3390/electronics12122722Electronics122023</p>
<p>DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, Matt Gardner, 10.18653/v1/N19-1246Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. Jill Burstein, Christy Doran, Thamar Solorio, the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational LinguisticsJune 20191</p>
<p>Healthgenie: Empowering users with healthy dietary guidance through knowledge graph and large language models. Gao Fan, arXiv:2504.145942025arXiv preprint</p>
<p>Richelieu: Self-evolving llm-based agents for ai diplomacy. Z Guan, Kong, Zhong, Advances in Neural Information Processing Systems (NeurIPS). 2024</p>
<p>A centralized strategy for multi-agent exploration. Gul Faiza, IEEE Access. 102022</p>
<p>Large language model based multi-agents: a survey of progress and challenges. Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei, V Nitesh, Olaf Chawla, Xiangliang Wiest, Zhang, Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence. the Thirty-Third International Joint Conference on Artificial Intelligence2024</p>
<p>Emergent behaviours in multi-agent systems with evolutionary game theory. The Anh, Han , AI Communications. 3542022</p>
<p>Measuring mathematical problem solving with the math dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, 2021aNeurIPS</p>
<p>Measuring mathematical problem solving with the math dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Xiaodong Song, Jacob Steinhardt, ArXiv, abs/2103.038742021b232134851</p>
<p>Automated design of agentic systems. Shengran Hu, Cong Lu, Jeff Clune, NeurIPS 2024 Workshop on Open-World Agents. 2024</p>
<p>Automated design of agentic systems. Shengran Hu, Cong Lu, Jeff Clune, 2025</p>
<p>. Intology. Zochi technical report. arXiv. 2025</p>
<p>Semantically aligned task decomposition in multi-agent reinforcement learning. L I Wenhao, arXiv:2305.108652023arXiv preprint</p>
<p>Pc-agent: A hierarchical multi-agent collaboration framework for complex task automation on pc. Liu Haowei, arXiv:2502.142822025arXiv preprint</p>
<p>Self-refine: Iterative refinement with self-feedback. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Advances in Neural Information Processing Systems. 202336</p>
<p>A taxonomy of hierarchical multi-agent systems: Design patterns, coordination mechanisms, and industrial applications. D J Moore, arXiv:2508.126832025arXiv e-prints</p>
<p>A comprehensive overview of large language models. Humza Naveed, Asad Ullah Khan, Shi Qiu, Muhammad Saqib, Saeed Anwar, Muhammad Usman, Naveed Akhtar, Nick Barnes, Ajmal Mian, ACM Transactions on Intelligent Systems and Technology. 1652025</p>
<p>A survey on multi-agent reinforcement learning and its application. Zepeng Ning, Lihua Xie, Journal of Automation and Intelligence. 322024</p>
<p>Can generalist foundation models outcompete special-purpose tuning? case study in medicine. Harsha Nori, Yin Tat Lee, Sheng Zhang, Dean Carignan, Richard Edgar, Nicolò Fusi, Nicholas King, Jonathan Larson, Yuanzhi Li, Weishung Liu, 2023CoRR</p>
<p>X Qu, arXiv:2508.11957A comprehensive review of ai agents: Transforming possibilities in technology and beyond. 2025arXiv e-prints</p>
<p>Toolformer: language models can teach themselves to use tools. Timo Schick, Jane Dwivedi-Yu, Roberto Dessí, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom, Proceedings of the 37th International Conference on Neural Information Processing Systems, NIPS '23. the 37th International Conference on Neural Information Processing Systems, NIPS '23Red Hook, NY, USACurran Associates Inc2023</p>
<p>Multi-agent coordination across diverse applications: A survey. Lijun Sun, Yijun Yang, Qiqi Duan, Yuhui Shi, Chao Lyu, Yu-Cheng Chang, Chin-Teng Lin, Yang Shen, 2025CoRR</p>
<p>. Jiabin Tang, Lianghao Xia, Zhonghang Li, Chao Huang, Ai-Researcher, 2025Autonomous Scientific Innovation</p>
<p>Multi-agent collaboration mechanisms: A survey of llms. Khanh-Tung Tran, Dung Dao, Minh-Duong Nguyen, Quoc-Viet Pham, O' Barry, Sullivan, Hoang D Nguyen, arXiv:2501.063222025arXiv preprint</p>
<p>Large language models: A comprehensive survey on architectures, applications, and challenges. Vinod Veeramachaneni, 20257Advanced Innovations in Computer Programming Languages</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Ed H Quoc V Le, Sharan Chi, Aakanksha Narang, Denny Chowdhery, Zhou, The Eleventh International Conference on Learning Representations. 2022</p>
<p>Unleashing the emergent cognitive synergy in large language models: A task-solving agent through multipersona self-collaboration. Zhenhailong Wang, Shaoguang Mao, Wenshan Wu, Tao Ge, Furu Wei, Heng Ji, 2024 Conference of the North American Chapter. Human Language Technologies20242024</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H Chi, V Quoc, Denny Le, Zhou, Proceedings of the 36th International Conference on Neural Information Processing Systems, NIPS '22. the 36th International Conference on Neural Information Processing Systems, NIPS '22Red Hook, NY, USACurran Associates Inc2022ISBN 9781713871088</p>
<p>Structure learning-based task decomposition for reinforcement learning in non-stationary environments. Woo Honguk, Yoo Gwangpyo, Minjong Yoo, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2022</p>
<p>Agentgym: Evolving large language model-based agents across diverse environments. Zhiheng Xi, Yiwen Ding, Wenxiang Chen, Boyang Hong, Honglin Guo, Junzhe Wang, 10.48550/arXiv.2406.04151arXiv:2406.041512024arXiv preprint</p>
<p>The ai scientist-v2: Workshop-level automated scientific discovery via agentic tree search. Yutaro Yamada, Robert Tjarko Lange, Cong Lu, Shengran Hu, Chris Lu, Jakob Foerster, Jeff Clune, David Ha, arXiv:2504.080662025arXiv preprint</p>
<p>Nader: Neural architecture design via multi-agent collaboration. Zhuo Yang, Wenxuan Zeng, Shuai Jin, Yang Chao Qian, Yu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)2025</p>
<p>Evoagent: Towards automatic multi-agent generation via evolutionary algorithms. Yuan, Song, Chen, Tan, Li, arXiv:2406.142282024arXiv preprint</p>
<p>Cppo: Continual learning for reinforcement learning with human feedback. Zhang, Lei, Gui, Yang, He, International Conference on Learning Representations (ICLR). 2024</p>
<p>Agentigraph: A multi-agent knowledge graph framework for interactive, domain-specific llm chatbots. Zhao Xinjie, arXiv:2508.029992025arXiv preprint</p>
<p>Promoting sustainability of the integrated production-inventory-distribution system through the physical. X.-S Peng, S.-F Ji, T.-T Ji, International Journal of Production Research. 58222020</p>
<p>Cyber-physical internet (cpi)-enabled logistics infrastructure integration framework in the greater bay area. H Wu, L Huang, M Li, G Q Huang, Advanced Engineering Informatics. 601025512024</p>
<p>The physical internet. The Economist. 37984822006</p>
<p>Toward a physical internet: meeting the global logistics sustainability grand challenge. B Montreuil, Logistics Research. 32-32011</p>
<p>A carbon-aware routing protocol for optimizing carbon emissions in modular construction logistics. C.-L Ng, H Wu, M Li, R Y Zhong, X Qu, G Q Huang, IEEE 20th International Conference on Automation Science and Engineering. 2024</p>
<p>Routing protocols for b2b e-commerce logistics in cyber-physical internet (cpi). X Qu, M Li, Z Ouyang, C.-L Ng, G Q Huang, Computers &amp; Industrial Engineering. 1931102932024</p>
<p>Optimal collaborative transportation service trading in b2b e-commerce logistics. M Zhang, S Pratap, G Q Huang, Z Zhao, International Journal of Production Research. 55182017</p>
<p>Cyberphysical spatial temporal analytics for digital twin-enabled smart contact tracing. Z Zhao, R Y Zhong, Y.-H Kuo, Y Fu, G Q Huang, Industrial Management &amp; Data Systems. 12152021</p>
<p>A cyber-physical systems architecture for industry 4.0-based manufacturing systems. J Lee, B Bagheri, H.-A Kao, References Andriushchenko, M.; Croce, F.; Flammarion, N.; and Hein, M. 2020. Understanding and Improving Fast Adversarial Training. 20153Advances in Neural Information Processing Systems</p>
<p>The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization. T Bai, J Luo, J Zhao, B Wen, Q Wang, I J Goodfellow, J Shlens, C Szegedy, D Hendrycks, S Basart, N Mu, S Kadavath, F Wang, E Dorundo, R Desai, T Zhu, S Parajuli, M Guo, D Song, J Steinhardt, A Krause, arXiv:2102.01356arXiv:1412.6572Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2021. 2014. 2021arXiv preprintRecent Advances in Adversarial Training for Adversarial Robustness</p>
<p>Adaptive Adversarial Training for Robust Deep Learning. Y Huang, Y Guo, Y Li, J Gao, Y.-G Li, International Conference on Computer Vision and Pattern Recognition. 2021</p>
<p>Intriguing Properties of Neural Networks. A Madry, A Makelov, L Schmidt, D Tsipras, A Vladu, S.-A Rebuffi, S Gowal, D A Calian, F Stimberg, O Wiles, T Mann, C Szegedy, W Zaremba, I Sutskever, J Bruna, D Erhan, I Goodfellow, R Fergus, arXiv:1706.06083arXiv:2103.01946Towards Deep Learning Models Resistant to Adversarial Attacks. X Ma, J Bailey, Yi, Wang, Y2017. 2021. 2014arXiv preprintInternational Conference on Learning Representations. and Gu</p>
<p>arXiv:2112.08304On the Convergence and Robustness of Adversarial Training. arXiv preprint</p>
<p>Fast Is Better Than Free: Revisiting Adversarial Training. E Wong, L Rice, J Z Kolter, International Conference on Learning Representations. 2020</p>
<p>Smooth Adversarial Training. C Xie, M Tan, B Gong, J Wang, A L Yuille, Q V Le, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2020</p>
<p>Theoretically Principled Trade-off Between Robustness and Accuracy. H Zhang, H Chen, C Xiao, S Gowal, R Stanforth, B Li, D Song, Y Yu, J Jiao, E Xing, L El Ghaoui, M Jordan, International Conference on Learning Representations. 2019a. 2019bInternational Conference on Machine Learning. References</p>
<p>A Comparative Study of Optimization Algorithms in Deep Learning: SGD, Adam, And Beyond. T D Barve, A Y Samant, H Maghraby, K E Djilali, S Ouarnoughi, H Niar, S Wistuba, M Wang, N Casale, G Moshovos, A , arXiv:2101.09336A Comprehensive Survey on Hardware-Aware Neural Architecture Search. 2025. 2021arXiv preprint</p>
<p>A Comparative Analysis of Optimization Methods for Classification on Various Datasets. Preprint. Desai, C. 2020. Comparative Analysis of Optimizers in Deep Neural Networks. S Das, S Das, International Journal of Innovative Science and Research. 2025</p>
<p>Comparison of Optimization Techniques Based on Gradient Descent Algorithm: A Review. J Duchi, E Hazan, Y Singer, S Haji, A Abdulazeez, S Mishkin, D Richtarik, P , arXiv:2301.03571Adaptive Optimization for Machine Learning. 2011. 2021. 202312arXiv preprintJournal of Machine Learning Research</p>
<p>The Effect of Choosing Optimizer Algorithms to Improve Computer Vision Tasks: A Comparative Study. E Hassan, M Shams, N Hikal, S Elmougy, 2023Multimedia Tools and Applications</p>
<p>. T Hospedales, A Antoniou, P Micaelli, A Storkey, </p>
<p>Meta-Learning in Neural Networks: A Survey. 44</p>
<p>A Survey of Deep Meta-Learning. M Huisman, J N Van Rijn, A Plaat, Artificial Intelligence Review. 542021</p>
<p>Comparison Of SGD, RMSprop, And Adam Optimization In Animal Classification Using CNNs. D Irfan, T Gunawan, International Conference Proceedings. 2023</p>
<p>D P Kingma, J Ba, arXiv:1412.6980Adam: A Method for Stochastic Optimization. 2014arXiv preprint</p>
<p>Meta-Learning: A Survey of Approaches and Applications. P Kordik, J Koutnik, M Snorek, 2010Knowledge Engineering and Machine Learning Group</p>
<p>H Liu, K Simonyan, Y Yang, arXiv:2103.12808Learning to Optimize: A Survey and Implications for Meta-Learning. 2021arXiv preprint</p>
<p>Comparative Study of Optimization Techniques in Deep Learning: Application in the Ophthalmology Field. I Loshchilov, F Hutter, A Mustapha, L Mohamed, K Ali, arXiv:1711.05101Journal of Physics: Conference Series. 17431120022017. 2021Decoupled Weight Decay Regularization. arXiv preprint</p>
<p>Experimental Comparison of Stochastic Optimizers in Deep Learning. E Okewu, P Adewole, O Sennaike, International Conference on Computational Science and Its Applications. Springer. Reyad, M.; Sarhan, A.; and Arafa, M. 2023. A Modified Adam Algorithm for Deep Neural Network Optimization. Neural Computing and Applications. 201935</p>
<p>A Comparative Study of Optimization Techniques in Deep Learning Using the MNIST Dataset. H Robbins, S Monro, S Selvakumari, M Durairaj, E Sjoberg, N Aletras, I Vulic, B Chen, R Pang, V Vasudevan, M Sandler, A Howard, Q V Le, arXiv:1905.11420Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition1951. 2025. 2019. 201922arXiv preprintArchitecture-Aware Bayesian Optimization for Neural Network Design</p>
<p>Lecture 6.5-RMSprop: Divide the Gradient by a Running Average of Its Recent Magnitude. T Tieleman, G Hinton, COURSERA: Neural Networks for Machine Learning. 20124</p>
<p>J Vanschoren, Y Wang, X Zhang, Y Li, S Reddi, D Sachan, S Kale, S Kumar, arXiv:1810.03548arXiv:2103.05847Adaptive Methods for Nonconvex Optimization. Advances in Neural Information Processing Systems. 2019. 2021. 201831arXiv preprintVariational Hyper-Adam: A Meta-Learning Approach to Hyperparameter Optimization</p>            </div>
        </div>

    </div>
</body>
</html>
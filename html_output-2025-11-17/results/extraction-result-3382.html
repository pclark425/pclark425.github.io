<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3382 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3382</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3382</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-77.html">extraction-schema-77</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving puzzle games that require spatial knowledge (such as Sudoku), including details about the models, the puzzles, the methods used, performance, and any analysis of how the models solve these tasks.</div>
                <p><strong>Paper ID:</strong> paper-b5a0430d99d33bd83464504a6ba15af24ac225eb</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/b5a0430d99d33bd83464504a6ba15af24ac225eb" target="_blank">An Evaluation of ChatGPT-4's Qualitative Spatial Reasoning Capabilities in RCC-8</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This paper investigates the extent to which one particular LLM can perform classical qualitative spatial reasoning tasks on the mereotopological calculus, RCC-8.</p>
                <p><strong>Paper Abstract:</strong> Qualitative Spatial Reasoning (QSR) is well explored area of Commonsense Reasoning and has multiple applications ranging from Geographical Information Systems to Robotics and Computer Vision. Recently many claims have been made for the capabilities of Large Language Models (LLMs). In this paper we investigate the extent to which one particular LLM can perform classical qualitative spatial reasoning tasks on the mereotopological calculus, RCC-8.</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3382.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3382.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving puzzle games that require spatial knowledge (such as Sudoku), including details about the models, the puzzles, the methods used, performance, and any analysis of how the models solve these tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT-4 (RCC-8 eval)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT-4 (Generative Pre-trained Transformer 4) evaluated on RCC-8 qualitative spatial reasoning tasks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>This paper evaluates ChatGPT-4's ability to perform qualitative spatial reasoning in the RCC-8 calculus via three tasks: computing the composition table, reporting preferred compositions (most likely relation), and predicting conceptual-neighbourhood (continuity) transitions, using textual prompts describing relations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>An Evaluation of ChatGPT-4's Qualitative Spatial Reasoning Capabilities in RCC-8</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A closed-source large language model from the GPT family (OpenAI). Trained on very large textual corpora to generate text conditioned on prompts; the paper used the ChatGPT-4 interface (version reported as "May 24" 2023). No model fine-tuning was performed by the experimenter; interaction was via conversational prompts. The authors note the model family gives no precise public training corpus breakdown.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>RCC-8 qualitative spatial reasoning tasks (composition table, preferred compositions, conceptual neighbourhoods)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>RCC-8 is a qualitative spatial calculus with eight JEPD base relations between 2D regions (DC, EC, PO, TPP, NTPP, TPPi, NTPPi, EQ). Tasks require inferring possible relations (composition), selecting a most-preferred relation under underspecification, and predicting immediate next relations under continuous motion/deformation (conceptual neighbourhoods). These tasks demand understanding of topological spatial relations and logical composition.</td>
                        </tr>
                        <tr>
                            <td><strong>input_representation</strong></td>
                            <td>Purely textual: prompts included the standard names and textual definitions of the eight RCC-8 relations and then posed individual inference questions (e.g., given R1(x,y) and R2(y,z), list all possible relations for (x,z), or ask for preferred single relation, or ask for immediate next relations under continuity). Some experiments re-ran prompts with relation names anonymised by prefixing with 'X' to hide RCC-8 labels.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-shot / vanilla single-prompt conversational queries (no chain-of-thought, no few-shot examples, no external tools). Each inference (e.g., one cell in the composition table) was asked as a separate conversation prompt. For preferred-composition tasks the model was asked to provide a single most-preferred relation. The authors explicitly avoided CoT / k-shot / tree-of-thought prompting to probe "vanilla" behaviour; anonymisation experiments probed training-leakage effects.</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_reasoning_analysis</strong></td>
                            <td>Qualitative analysis of model outputs across tasks: the paper inspects per-cell composition answers, preferred-relation choices versus human experimental data, and conceptual-neighbourhood link predictions. Observed behaviours include: (1) accurate intuitive recognition of many relations (e.g., DC rarely missed), (2) systematic omissions (missing some possible relations in compositions) and commission errors (predicting relations not present in the true composition), (3) inconsistent handling of inverse relations (reasoning correctly in one direction but not the inverse), (4) occasional semantic misunderstanding of specific relation meanings (e.g., misinterpreting NTPPi), (5) failure to infer mandatory intermediate relations under continuity (e.g., predicting DC as an immediate next relation from PO without passing through EC). The authors interpret some choices as "cautious" or "preferred mental model" behaviour and note possible influences of training exposure to QSR literature versus purely reasoning-from-first-principles.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Experiment 1 (Composition table): non-anonymised run: correct predictions (present) = 85 relations; correctly not predicted (missing) = 197 relations; total correct = 282/392 = 71.94%; 11/49 cells (composition pairs) were completely correct. Anonymised run: overall correctness dropped to 67.09% (authors report a decline though exact anonymised counts are given in paper figures). Experiment 2 (Preferred compositions): agreement with aggregate human preferred relation = 20/49 (40.82%); including matches to one language group raises agreement to 23/49 (46.93%); model produced impossible relations as preferred in 7/49 cases. Anonymised preferred-composition agreement increased to 24/49 (48.99%) in a repeat. Experiment 3 (Conceptual neighbourhood / continuity): reported accuracy 50/56 = 89.2% (19 correct predicted links, 31 correct missing links, 3 falsely predicted links, 3 missing links) in the primary run; anonymised run had similar overall performance with slightly different errors.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Reported limitations include: (1) incomplete enumerations in composition (omitting allowable relations) and false positive relation predictions, (2) logically impossible preferred-relation choices, (3) inconsistent reasoning about inverses (mixing up a relation and its inverse), (4) occasional misunderstanding of relation semantics (e.g., NTPPi), (5) failures to recognise necessary intermediate states under continuous transitions, (6) stochastic variability across runs and sensitivity to anonymisation (unclear whether differences stem from stochasticity or prior training exposure), (7) absence of chain-of-thought / systematic proof traces (the vanilla responses sometimes contain justifications but not reliable stepwise logical derivations). The paper notes these constitute systematic reasoning weaknesses and that prompting/fine-tuning might improve but was deliberately not used.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models_or_humans</strong></td>
                            <td>Compared to humans: Experiment 2 compares ChatGPT-4 preferred relations to human experimental data from Ragni et al. (2007). ChatGPT-4 agreed with the overall human average in ~40.8% of cases (non-anonymised) and 48.99% in the anonymised run, with many cases where the model chose possible but non-preferred relations or impossible relations. Compared to other LLMs: the paper references prior observations that ChatGPT-4/GPT-4 were among the more performant LLMs at spatial reasoning (Cohn & Hernandez-Orallo 2023), but no systematic multi-model benchmarks are run here. No comparisons to non-language specialized spatial solvers or symbolic reasoners are performed in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>additional_notes</strong></td>
                            <td>The authors made conversation transcripts available (URL in paper) and deliberately used 'vanilla' prompts to test baseline LLM spatial reasoning. They also ran anonymisation variants (prefixing relation names with 'X') to probe prior-knowledge effects. The model version cited: ChatGPT-4 "May 24" (2023).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "An Evaluation of ChatGPT-4's Qualitative Spatial Reasoning Capabilities in RCC-8", 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs <em>(Rating: 2)</em></li>
                <li>Qualitative Spatial Representation and Reasoning with the Region Connection Calculus <em>(Rating: 2)</em></li>
                <li>Cross-cultural similarities in topological reasoning <em>(Rating: 2)</em></li>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 1)</em></li>
                <li>ChatGPT and Open-AI Models: A Preliminary Review <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3382",
    "paper_id": "paper-b5a0430d99d33bd83464504a6ba15af24ac225eb",
    "extraction_schema_id": "extraction-schema-77",
    "extracted_data": [
        {
            "name_short": "ChatGPT-4 (RCC-8 eval)",
            "name_full": "ChatGPT-4 (Generative Pre-trained Transformer 4) evaluated on RCC-8 qualitative spatial reasoning tasks",
            "brief_description": "This paper evaluates ChatGPT-4's ability to perform qualitative spatial reasoning in the RCC-8 calculus via three tasks: computing the composition table, reporting preferred compositions (most likely relation), and predicting conceptual-neighbourhood (continuity) transitions, using textual prompts describing relations.",
            "citation_title": "An Evaluation of ChatGPT-4's Qualitative Spatial Reasoning Capabilities in RCC-8",
            "mention_or_use": "use",
            "model_name": "ChatGPT-4",
            "model_description": "A closed-source large language model from the GPT family (OpenAI). Trained on very large textual corpora to generate text conditioned on prompts; the paper used the ChatGPT-4 interface (version reported as \"May 24\" 2023). No model fine-tuning was performed by the experimenter; interaction was via conversational prompts. The authors note the model family gives no precise public training corpus breakdown.",
            "model_size": null,
            "puzzle_name": "RCC-8 qualitative spatial reasoning tasks (composition table, preferred compositions, conceptual neighbourhoods)",
            "puzzle_description": "RCC-8 is a qualitative spatial calculus with eight JEPD base relations between 2D regions (DC, EC, PO, TPP, NTPP, TPPi, NTPPi, EQ). Tasks require inferring possible relations (composition), selecting a most-preferred relation under underspecification, and predicting immediate next relations under continuous motion/deformation (conceptual neighbourhoods). These tasks demand understanding of topological spatial relations and logical composition.",
            "input_representation": "Purely textual: prompts included the standard names and textual definitions of the eight RCC-8 relations and then posed individual inference questions (e.g., given R1(x,y) and R2(y,z), list all possible relations for (x,z), or ask for preferred single relation, or ask for immediate next relations under continuity). Some experiments re-ran prompts with relation names anonymised by prefixing with 'X' to hide RCC-8 labels.",
            "prompting_method": "Zero-shot / vanilla single-prompt conversational queries (no chain-of-thought, no few-shot examples, no external tools). Each inference (e.g., one cell in the composition table) was asked as a separate conversation prompt. For preferred-composition tasks the model was asked to provide a single most-preferred relation. The authors explicitly avoided CoT / k-shot / tree-of-thought prompting to probe \"vanilla\" behaviour; anonymisation experiments probed training-leakage effects.",
            "spatial_reasoning_analysis": "Qualitative analysis of model outputs across tasks: the paper inspects per-cell composition answers, preferred-relation choices versus human experimental data, and conceptual-neighbourhood link predictions. Observed behaviours include: (1) accurate intuitive recognition of many relations (e.g., DC rarely missed), (2) systematic omissions (missing some possible relations in compositions) and commission errors (predicting relations not present in the true composition), (3) inconsistent handling of inverse relations (reasoning correctly in one direction but not the inverse), (4) occasional semantic misunderstanding of specific relation meanings (e.g., misinterpreting NTPPi), (5) failure to infer mandatory intermediate relations under continuity (e.g., predicting DC as an immediate next relation from PO without passing through EC). The authors interpret some choices as \"cautious\" or \"preferred mental model\" behaviour and note possible influences of training exposure to QSR literature versus purely reasoning-from-first-principles.",
            "performance_metrics": "Experiment 1 (Composition table): non-anonymised run: correct predictions (present) = 85 relations; correctly not predicted (missing) = 197 relations; total correct = 282/392 = 71.94%; 11/49 cells (composition pairs) were completely correct. Anonymised run: overall correctness dropped to 67.09% (authors report a decline though exact anonymised counts are given in paper figures). Experiment 2 (Preferred compositions): agreement with aggregate human preferred relation = 20/49 (40.82%); including matches to one language group raises agreement to 23/49 (46.93%); model produced impossible relations as preferred in 7/49 cases. Anonymised preferred-composition agreement increased to 24/49 (48.99%) in a repeat. Experiment 3 (Conceptual neighbourhood / continuity): reported accuracy 50/56 = 89.2% (19 correct predicted links, 31 correct missing links, 3 falsely predicted links, 3 missing links) in the primary run; anonymised run had similar overall performance with slightly different errors.",
            "limitations_or_failure_modes": "Reported limitations include: (1) incomplete enumerations in composition (omitting allowable relations) and false positive relation predictions, (2) logically impossible preferred-relation choices, (3) inconsistent reasoning about inverses (mixing up a relation and its inverse), (4) occasional misunderstanding of relation semantics (e.g., NTPPi), (5) failures to recognise necessary intermediate states under continuous transitions, (6) stochastic variability across runs and sensitivity to anonymisation (unclear whether differences stem from stochasticity or prior training exposure), (7) absence of chain-of-thought / systematic proof traces (the vanilla responses sometimes contain justifications but not reliable stepwise logical derivations). The paper notes these constitute systematic reasoning weaknesses and that prompting/fine-tuning might improve but was deliberately not used.",
            "comparison_to_other_models_or_humans": "Compared to humans: Experiment 2 compares ChatGPT-4 preferred relations to human experimental data from Ragni et al. (2007). ChatGPT-4 agreed with the overall human average in ~40.8% of cases (non-anonymised) and 48.99% in the anonymised run, with many cases where the model chose possible but non-preferred relations or impossible relations. Compared to other LLMs: the paper references prior observations that ChatGPT-4/GPT-4 were among the more performant LLMs at spatial reasoning (Cohn & Hernandez-Orallo 2023), but no systematic multi-model benchmarks are run here. No comparisons to non-language specialized spatial solvers or symbolic reasoners are performed in this study.",
            "additional_notes": "The authors made conversation transcripts available (URL in paper) and deliberately used 'vanilla' prompts to test baseline LLM spatial reasoning. They also ran anonymisation variants (prefixing relation names with 'X') to probe prior-knowledge effects. The model version cited: ChatGPT-4 \"May 24\" (2023).",
            "uuid": "e3382.0",
            "source_info": {
                "paper_title": "An Evaluation of ChatGPT-4's Qualitative Spatial Reasoning Capabilities in RCC-8",
                "publication_date_yy_mm": "2023-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs",
            "rating": 2
        },
        {
            "paper_title": "Qualitative Spatial Representation and Reasoning with the Region Connection Calculus",
            "rating": 2
        },
        {
            "paper_title": "Cross-cultural similarities in topological reasoning",
            "rating": 2
        },
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 1
        },
        {
            "paper_title": "ChatGPT and Open-AI Models: A Preliminary Review",
            "rating": 1
        }
    ],
    "cost": 0.009982,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>An Evaluation of ChatGPT-4's Qualitative Spatial Reasoning Capabilities in RCC-8</h1>
<p>A G Cohn
University of Leeds &amp; The Alan Turing institute, UK</p>
<h6>Abstract</h6>
<p>Qualitative Spatial Reasoning (QSR) is well explored area of Commonsense Reasoning and has multiple applications ranging from Geographical Information Systems to Robotics and Computer Vision. Recently many claims have been made for the capabilities of Large Language Models (LLMs). In this paper we investigate the extent to which one particular LLM can perform classical qualitative spatial reasoning tasks on the mereotopological calculus, RCC-8.</p>
<h2>Introduction</h2>
<p>Qualitative Spatial Reasoning (QSR) (Cohn and Renz 2008; Chen et al. 2015; Cohn and Hazarika 2001) is a well developed field which is concerned with the representation of qualitative spatial information and reasoning with it. In natural language, spatial information is usually represented qualitatively (using prepositions such as on, in, left of, part of, under, touching, …) and many calculi have been developed to represent such information. There are calculi for mereological relations (such as RCC-5 (Jonsson and Drakengren 1997)), mereotopological relations (such as RCC-8 (Randell, Cui, and Cohn 1992; Cohn et al. 1997)), directions (such as OPRA (Moratz 2006)), size (Gerevini and Renz 2002) for example as well as calculi combining two different aspects of spatial information, such as the Rectangle Algebra (Guesgen 1989; Mukerjee and Joe 1990) which can represent both mereotopological information as well as directional. What is common to all these calculi is that they consist of a set of jointly exhaustive and pairwise disjoint (JEPD) base relations. For example, RCC-8 contains eight JEPD base relations, illustrated in 2D in Fig. 1.</p>
<p>Large Language Models (LLMs) (Devlin et al. 2019; Brown et al. 2020), such as ChatGPT-4 (Roumeliotis and Tselikas 2023) are a recent example of so called Foundation Models which have been trained on very large textual corpora in order to generate text in response to a prompt. This is not the place to survey this burgeoning field, but we note that many claims have been made for the power and apparent intelligent behaviour that these models can display. In particular their performance on some benchmarks may lead one</p>
<p><sup>1</sup>We may use QSR as shorthand for both Qualitative Spatial Reasoning and Qualitative Spatial Representation; context should usually make clear which is intended.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: The eight relations of the RCC-8 calculus illustrated in 2D.</p>
<p>to believe that they possess, at least to some degree, the ability to perform commonsense reasoning. Spatial reasoning is usually regarded as one core aspect of common sense so it is natural to ask whether LLMs can reason about qualitative spatial information. This is the question that we address here.</p>
<p>In earlier work (Cohn and Hernandez-Orallo 2023) we use extended dialogues with an LLM to try to map the boundaries of spatial commonsense in some LLMs, addressing a variety spatial challenges, and examining not only the response given but also the explanation/justification of the response, but did not specifically focus on existing QSRs, though some questions were asked which do correspond to particular reasoning steps in an existing QSR. Here we focus on one specific QSR and ask the question as to what extent an LLM can perform reasoning in that calculus, and conduct a more exhaustive evaluation, but looking at the ability to perform compositions between relations and also to reason about the conceptual neighbourhood diagram of the calculus. Weaknesses in the reasoning powers of LLMs have previously been noted (e.g. (Cai, Chang, and Han 2023)) so one might not expect LLMs to perform well in this regard. But on the other hand, there are a large number of papers about QSR in the literature and these are likely to have formed part of the training corpus of an LLM, and thus might facilitate correctly responding to prompts – though the information concerning the actual reasoning steps are often given in tables (in particular composition tables – see below) and thus</p>
<p>might be hard for LLM training procedures to process well.
There are now many LLMs in the literature. Some of these are open source and are explicit about the training corpus; others are closed and give no specific information about the training, or the precise corpus, such as the GPT family of LLMs. Nevertheless since we observed previously (Cohn and Hernandez-Orallo 2023) that ChatGPT-4 and GPT4 were the most performant for spatial reasoning, we use ChatGPT-4 as the LLM with which we perform our experiments. In each of the experiments below, an initial prompt gave the problem setting and the task to be performed. Subsequent prompts in the conversation probed about one specific inference (e.g. one cell in a composition table). Each experiment was a separate conversation and the version number of ChatGPT-4 was given as "May 24" (presumably, the version of May 24 2023). The conversations can be found at the following location: https://tinyurl.com/qr23sup.</p>
<h2>Compositional Reasoning</h2>
<p>The most researched form of reasoning with Qualitative Spatial Relations is that of composition: i.e. given a two facts R1 $(x, y)$, and R2 $(y, z)$, then what relations are possible between $x$ and $z$ ? In general more than one base relation is possible, so the answer may take the form of a disjunction of relations, as can be seen in the composition table for RCC-8 in Fig. 2. Given the ubiquity of RCC-8 in the QSR literature, in this paper we focus on the abilities of an LLM to reason with RCC-8.</p>
<h2>Experiment 1: Compositional Reasoning in RCC-8</h2>
<p>In this experiment the standard names of the eight relations were given as well as a sentence stating that the relations were part of the RCC-8 calculus:</p>
<p>Consider the following set of eight pairwise disjoint and mutually exhaustive binary spatial relations. These relations form part of the well known RCC-8 qualitative spatial reasoning calculus. DC(x,y) means that $x$ and $y$ are disconnected and share no spatial parts. EC(x,y) means that $x$ and $y$ touch at a boundary but do not share any interior parts, PO(x,y) means that $x$ and $y$ share a spatial part, but neither is part of the other. TPP $(x, y)$ means that $x$ is part of $y$ and touches y's boundary. NTPP $(x, y)$ means that $x$ is part of $y$ but does not touch y's boundary. TPPi $(x, y)$ is the same as TPP $(y, x)$. NTPPi $(x, y)$ is the same as NTPP $(y, x)$. Finally, EQ $(x, y)$ means that $x$ and $y$ are coincident. I will now ask you a series of questions about these relations. There may be more than one possible relation, in which case name all of the possible answers. If all eight relations are possible, just say ALL.
As an example of a prompt and ChatGPT-4's response, consider the first one:</p>
<p>Prompt: If DC $(x, y)$ and DC $(y, z)$ then what are the possible relationships between $x$ and $z$. Response: If DC $(x, y)$ and DC $(y, z)$, then $x$ and $z$ can either be disconnected (DC) or externally connected (EC). This
is because the disconnection between $x$ and $y$, and $y$ and $z$, only guarantees that neither $x$ nor $z$ is connected to $y$. It does not preclude the possibility that $x$ and $z$ themselves might be disconnected or touching at a boundary. However, the other relations like part-of (TPP, NTPP) and their inverses (TPPi, NTPPi), partial overlap (PO), and equality (EQ) are not possible because they would imply some level of connection between $x$ and $z$ through $y$, which contradicts our initial conditions. So, the possible relationships between $x$ and $z$ are: DC $(x, z)$ : $x$ and $z$ are disconnected. $E C(x, z): x$ and $z$ are externally connected.
Notice that in fact the composition of DC with DC is the union of all eight RCC-8 relations, so the reasoning is incorrect here.
The results of the $7 * 7=49$ prompts issued (compositions involving EQ were omitted since they should be trivial) are summarised in Fig. 3. There is a total of 85 relations correctly predicted (those in black), 197 relations which are correctly not predicted (those not in the table). Thus there are a total of 282 correctly predicted (both present and missing) relations, i.e. $71.94 \%$ of the total possible $(7 * 7 * 8=392)$. 11 of the cells (out of 49) are completely correct. This is clearly much better than chance but far from a correct composition table. Fig. 4 shows the absolute number and relative percentages of the 8 relations in the proportion that they are correctly or incorrectly predicted. As can be seen, DC is never missed as a correct prediction, and PO is never incorrectly predicted as present. DC has the highest accuracy which is perhaps not surprising since DC might be regarded as intuitively easier to reason about (and this fits with the predictions about human preferred models in the section below). Including both correctly predicted and correctly missing relationships, all eight relations have broadly similar accuracies.
In order to test whether the result was influenced by prior knowledge of RCC-8 gained as part of its training, we also performed the same experiment, but with all the relation names prefixed by an X to disguise the connection to RCC8. The prompt was the same as above except for the change of relation names and the omission of the second sentence. The results are given in Fig. 5 while Fig. 6 shows the absolute number and relative percentages of the 8 relations in the proportion that they are correctly or incorrectly predicted. As can be seen, DC again is never missed as a correct prediction, and EC is only missed twice; again PO is never incorrectly predicted as present. As before, DC, EC, and POhave the highest accuracies, along with EQ, but EQ is never predicted as present correctly, only incorrectly. The overall average of correctly predicted relations (present and missing) drops from $71.94 \%$ in the non-anonymous case above to $67.09 \%$ so there is some loss of performance though whether is due to the anonymisation of the relations or the stochasticity of ChatGPT-4 is not clear.</p>
<h2>Experiment 2: Preferred Compositions in RCC-8</h2>
<p>As noted above, in general a composition of two relations will yield more than one possible base relations, but it turns</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">DC</th>
<th style="text-align: center;">EC</th>
<th style="text-align: center;">PO</th>
<th style="text-align: center;">TPP</th>
<th style="text-align: center;">NTPP</th>
<th style="text-align: center;">TPPi</th>
<th style="text-align: center;">NTPPi</th>
<th style="text-align: center;">=</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">DC</td>
<td style="text-align: center;">*</td>
<td style="text-align: center;">DC, EC, PO, TPP, NTPP</td>
<td style="text-align: center;">DC, EC, PO, TPP, NTPP</td>
<td style="text-align: center;">DC, EC, PO, TPP, NTPP</td>
<td style="text-align: center;">DC, EC, PO, TPP, NTPP</td>
<td style="text-align: center;">DC</td>
<td style="text-align: center;">DC</td>
<td style="text-align: center;">DC</td>
</tr>
<tr>
<td style="text-align: center;">EC</td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { DC, EC, PO, } \ &amp; \text { TPPi, NTPPi } \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { DC, EC, PO, TPP, } \ &amp; \text { TPPi, = } \end{aligned}$</td>
<td style="text-align: center;">DC, EC, PO, TPP, NTPP</td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { EC, PO, TPP, } \ &amp; \text { NTPP } \end{aligned}$</td>
<td style="text-align: center;">PO, TPP, NTPP</td>
<td style="text-align: center;">DC, EC</td>
<td style="text-align: center;">DC</td>
<td style="text-align: center;">EC</td>
</tr>
<tr>
<td style="text-align: center;">PO</td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { DC, EC, PO, } \ &amp; \text { TPPi, NTPPi } \end{aligned}$</td>
<td style="text-align: center;">DC, EC, PO, TPPi, NTPPi</td>
<td style="text-align: center;">*</td>
<td style="text-align: center;">PO, TPP, NTPP</td>
<td style="text-align: center;">PO, TPP, NTPP</td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { DC, EC, PO, } \ &amp; \text { TPPi, NTPPi } \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { DC, EC, PO, } \ &amp; \text { TPPi, NTPPi } \end{aligned}$</td>
<td style="text-align: center;">PO</td>
</tr>
<tr>
<td style="text-align: center;">TPP</td>
<td style="text-align: center;">DC</td>
<td style="text-align: center;">DC, EC</td>
<td style="text-align: center;">DC, EC, PO, TPP, NTPP</td>
<td style="text-align: center;">TPP, NTPP</td>
<td style="text-align: center;">NTPP</td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { DC, EC, PO, } \ &amp; \text { TPP, TPPi, = } \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { DC, EC, PO, } \ &amp; \text { TPPi, NTPPi } \end{aligned}$</td>
<td style="text-align: center;">TPP</td>
</tr>
<tr>
<td style="text-align: center;">NTPP</td>
<td style="text-align: center;">DC</td>
<td style="text-align: center;">DC</td>
<td style="text-align: center;">DC, EC, PO, TPP, NTPP</td>
<td style="text-align: center;">NTPP</td>
<td style="text-align: center;">NTPP</td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { DC, EC, PO, } \ &amp; \text { TPP, NTPP } \end{aligned}$</td>
<td style="text-align: center;">*</td>
<td style="text-align: center;">NTPP</td>
</tr>
<tr>
<td style="text-align: center;">TPPi</td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { DC, EC, PO, } \ &amp; \text { TPPi, NTPPi } \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { EC, PO, TPPi, } \ &amp; \text { NTPPi } \end{aligned}$</td>
<td style="text-align: center;">PO,TPPi,NTPPi</td>
<td style="text-align: center;">PO,TPP,TPP, =</td>
<td style="text-align: center;">PO, TPP, NTPP</td>
<td style="text-align: center;">TPPi, NTPPi</td>
<td style="text-align: center;">NTPPi</td>
<td style="text-align: center;">TPPi</td>
</tr>
<tr>
<td style="text-align: center;">NTPPi</td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { DC, EC, PO, } \ &amp; \text { TPPi, NTPPi } \end{aligned}$</td>
<td style="text-align: center;">PO,TPPi,NTPPi</td>
<td style="text-align: center;">PO,TPPi,NTPPi</td>
<td style="text-align: center;">PO,TPPi,NTPPi</td>
<td style="text-align: center;">$\begin{aligned} &amp; \text { PO, TPP, NTPP, } \ &amp; \text { TPPi, NTPPi, = } \end{aligned}$</td>
<td style="text-align: center;">NTPPi</td>
<td style="text-align: center;">NTPPi</td>
<td style="text-align: center;">NTPPi</td>
</tr>
<tr>
<td style="text-align: center;">=</td>
<td style="text-align: center;">DC</td>
<td style="text-align: center;">EC</td>
<td style="text-align: center;">PO</td>
<td style="text-align: center;">TPP</td>
<td style="text-align: center;">NTPP</td>
<td style="text-align: center;">TPPi</td>
<td style="text-align: center;">NTPPi</td>
<td style="text-align: center;">=</td>
</tr>
</tbody>
</table>
<p>Figure 2: The RCC-8 Composition Table (Cohn et al. 1997)</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">DC</th>
<th style="text-align: center;">EC</th>
<th style="text-align: center;">PO</th>
<th style="text-align: center;">TPP</th>
<th style="text-align: center;">NTPP</th>
<th style="text-align: center;">TPPi</th>
<th style="text-align: center;">NTPPi</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">DC</td>
<td style="text-align: center;">DEPTNGN</td>
<td style="text-align: center;">DPTNGN</td>
<td style="text-align: center;">DEPTNG</td>
<td style="text-align: center;">DEPTNG</td>
<td style="text-align: center;">DEPTNG</td>
<td style="text-align: center;">DE</td>
<td style="text-align: center;">DC</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">DC</td>
<td style="text-align: center;">B(CO)</td>
<td style="text-align: center;">DE</td>
<td style="text-align: center;">DEPTNG</td>
<td style="text-align: center;">DEPTN</td>
<td style="text-align: center;">DEPTN</td>
<td style="text-align: center;">DE</td>
<td style="text-align: center;">DC</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">PO</td>
<td style="text-align: center;">B(CO)</td>
<td style="text-align: center;">DEPTNGN</td>
<td style="text-align: center;">DEPTNGN</td>
<td style="text-align: center;">DEPTNGN</td>
<td style="text-align: center;">DEPTNGN</td>
<td style="text-align: center;">DEPTNGN</td>
<td style="text-align: center;">DEPTNGN</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">TPP</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">DE</td>
<td style="text-align: center;">DEPTN</td>
<td style="text-align: center;">TN</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">DEPTN</td>
<td style="text-align: center;">DEPTNG</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">NTPP</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">DC</td>
<td style="text-align: center;">DEPTN</td>
<td style="text-align: center;">TN</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">DEPTN</td>
<td style="text-align: center;">DEPTNGN</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">TPPi</td>
<td style="text-align: center;">B(CO)</td>
<td style="text-align: center;">DEPTN</td>
<td style="text-align: center;">DEPTNGN</td>
<td style="text-align: center;">PTNGN</td>
<td style="text-align: center;">PTNGN</td>
<td style="text-align: center;">TPPi</td>
<td style="text-align: center;">TPPi</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">TPPi</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">DEPTN</td>
<td style="text-align: center;">PON</td>
<td style="text-align: center;">TNGN</td>
<td style="text-align: center;">PON</td>
<td style="text-align: center;">ON</td>
<td style="text-align: center;">ON</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Figure 3: The Composition Table for RCC-8 produced by ChatGPT-4. The entry in each cell uses the following coding: D (DC), E(EC), P(PO), T(TPP), N(NTPP), t(TPPi), $\mathrm{n}(\mathrm{NTPPi}), \mathrm{Q}(\mathrm{EQ})$. Black means that relation is correctly predicted ( 85 times), red means that relation is incorrectly predicted ( 61 times), blue means that the relation was incorrectly not predicted ( 49 times).
out that humans tend to have a "preferred" relation. For example, Ragni et al (2007) report on experiments performed on native German speakers and native Mongolian speakers for RCC-8. In their experiments the relations were described, but the human subjects were not allowed to draw possible configurations, so the setting is essentially equivalent to an LLM setting.</p>
<p>Given that humans may struggle to see all the possible relations ${ }^{2}$, determining whether there is agreement about the most preferred is good question to ask. It turns out that there is good agreement in general across and within the two cultures, with the the percentage of people agreeing with the same preferred relation ranging from $30 \%$ to $87.5 \%$ (a random choice would yield $12.5 \%$ on average since there are eight relations to choose from). (They did not query cases where the composition yields a unique relation, nor did they consider EQ as one of the two relations as this should be a trivial task.) This agreement is perhaps surprising since the two languages are linguistically very different. Ragni et al (2007) do report some differences though - for example although both language speakers preferred DC whenever it was consistent, Mongolians preferred PO over NTPPi</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 4: Relation statistics for the Composition Table for RCC-8 produced by ChatGPT-4. The upper chart shows the absolute number of relations, and the lower the relative percentage for each relation.
whereas for Germans the converse was true. Both cultures only chose EQ when composing a relation with its inverse (e.g. TPP with TPPi).</p>
<p>The theory of preferred mental models (Knauff, Rauh, and Schlieder 1995) states that people construct the simplest (computationally cheapest) model consistent with the premises. Their experiments showed that humans prefer models with the smallest overlapping complexity which explains the preference for DC noted above.</p>
<p>Given the difficulty reported in Experiment 1 in ChatGPT4 correctly inferring all possible relations in a composition,</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">GC</th>
<th style="text-align: center;">EG</th>
<th style="text-align: center;">PG</th>
<th style="text-align: center;">TPP</th>
<th style="text-align: center;">NTPP</th>
<th style="text-align: center;">TPPi</th>
<th style="text-align: center;">NTPPi</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">G</td>
<td style="text-align: center;">DEPTN</td>
<td style="text-align: center;">DEPTN</td>
<td style="text-align: center;">DEPTN</td>
<td style="text-align: center;">DEPTN</td>
<td style="text-align: center;">DEPTN</td>
<td style="text-align: center;">DE</td>
<td style="text-align: center;">DE</td>
</tr>
<tr>
<td style="text-align: center;">G</td>
<td style="text-align: center;">DEPN</td>
<td style="text-align: center;">DEPTN</td>
<td style="text-align: center;">DEPTN</td>
<td style="text-align: center;">DEPTN</td>
<td style="text-align: center;">DEPTN</td>
<td style="text-align: center;">DEP</td>
<td style="text-align: center;">DE</td>
</tr>
<tr>
<td style="text-align: center;">PG</td>
<td style="text-align: center;">0 EO</td>
<td style="text-align: center;">DEP N</td>
<td style="text-align: center;">DEP TNG</td>
<td style="text-align: center;">DEPTN</td>
<td style="text-align: center;">DEPTN</td>
<td style="text-align: center;">DEPTN</td>
<td style="text-align: center;">DEPTN</td>
</tr>
<tr>
<td style="text-align: center;">TPP</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">DE</td>
<td style="text-align: center;">DEPTN</td>
<td style="text-align: center;">TG</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">DEPTN</td>
<td style="text-align: center;">DEPNN</td>
</tr>
<tr>
<td style="text-align: center;">NTPP</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">DE</td>
<td style="text-align: center;">DEPTN</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">N</td>
<td style="text-align: center;">DEPTN</td>
<td style="text-align: center;">DEPTN20</td>
</tr>
<tr>
<td style="text-align: center;">TPPi</td>
<td style="text-align: center;">DEPN</td>
<td style="text-align: center;">DEPN</td>
<td style="text-align: center;">DEPTNdn</td>
<td style="text-align: center;">DEPTNND</td>
<td style="text-align: center;">DEPTNdn</td>
<td style="text-align: center;">TQN</td>
<td style="text-align: center;">DEPTNdn</td>
</tr>
<tr>
<td style="text-align: center;">TPPi</td>
<td style="text-align: center;">DEPN</td>
<td style="text-align: center;">DEPN</td>
<td style="text-align: center;">DP</td>
<td style="text-align: center;">DPTNdn</td>
<td style="text-align: center;">DPTNdn</td>
<td style="text-align: center;">DPTNdn</td>
<td style="text-align: center;">DPTNdn</td>
</tr>
</tbody>
</table>
<p>Figure 5: The Composition Table for RCC-8 produced by ChatGPT-4 using anonymised relations. The entry in each cell uses the following coding: D (DC), E (EC), P (PO), T (TPP), N (NTPP), t (TPPi), n (NTPPi), Q (EQ). Black means that relation is correctly predicted ( 95 times), red means that relation is incorrectly predicted ( 57 times), blue means that the relation was incorrectly not predicted ( 72 times).
asking this more specific query seems a natural task to try; this gives some measure of whether the cognitive preferences of humans correspond at all to the choices made by ChatGPT-4.</p>
<p>The initial prompt given to ChatGPT-4 was identical to the initial prompt above when querying about composition. In its responses, sometimes ChatGPT-4 gave a clear preference for one relation, for example:</p>
<p>Prompt: If $D C(x, y)$ and $D C(y, z)$ then what is your preferred relationship between $x$ and $z$ ? Response: The relations $D C(x, y)$ and $D C(y, z)$ mean that region $x$ is disconnected from region $y$ and region $y$ is disconnected from region $z$. These relationships don't provide any information about the spatial relationship between $x$ and $z, x$ and $z$ could be disconnected, they could touch at a boundary, or they could partially overlap, among other possibilities. However, in absence of any other information and considering spatial continuity, I would say the most likely relation is $D C(x, z)$, assuming $x$ and $z$ are also disconnected. Please note that this is just a statistical assumption; it's possible for $x$ and $z$ to have any of the other relations defined in RCC-8.
In other cases ChatGPT-4 justified its choice by saying it was the "cautious" choice, or the "safest choice". The humans in the (Ragni, Tseden, and Knauff 2007) experiment were not asked to justify their choices, but the claim made by the authors of that paper noted above is that choices were based on computational/simplicity considerations - which differs from the reasons claimed here by ChatGPT-4.</p>
<p>The initial prompt to ChatGPT-4 given was:
Consider the following set of eight pairwise disjoint and mutually exhaustive binary spatial relations, between two one piece spatial 2D regions. These relations form part of the well known RCC-8 qualitative spatial reasoning calculus. $D C(x, y)$ means that $x$ and $y$ are disconnected and share no spatial parts. $E C(x, y)$ means that $x$ and $y$ touch at a boundary but do not share any interior parts. $P O(x, y)$ means that $x$ and $y$ share a spatial part, but neither is part of the other. $\operatorname{TPP}(x, y)$ means that $x$ is part of $y$ and touches $y$ 's
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 6: Relation statistics for the Composition Table for RCC-8 with anonymised relations produced by ChatGPT-4. The upper chart shows the absolute number of relations, and the lower the relative percentage for each relation.
boundary. $N T P P(x, y)$ means that $x$ is part of $y$ but does not touch y's boundary. TPPi $(x, y)$ is the same as $\operatorname{TPP}(y, x)$. NTPPi $(x, y)$ is the same as NTPP $(y, x)$. Finally, $E Q(x, y)$ means that $x$ and $y$ are coincident. I will now ask you a series of questions about these relations. It is possible that in some cases there could be more than one relation that might hold; in these cases just give me your single most preferred relationship.
As can be seen in Fig. 7, ChatGPT-4 only agreed with the average human on 20/49 (40.82\%) though in a further three cases it agreed with one of the language groups (twice Mongolian, once German) giving a total of 23/49 (46.93\%). In seven cases it actually predicted an impossible relation as its preferred composition ${ }^{3}$, and in all cases where the composition is unique it failed to note that. In all the other cases, 18/49 (36.63\%) its preferred relation was possible but not preferred by either language group or overall.</p>
<p>As can be seen, sometimes ChatGPT-4 made a logically inconsistent prediction, for example with EC/NTPPi; looking at the ChatGPT-4's response in this case we can see that this is because it seems to misunderstand the meaning of NTPPi:</p>
<p>If $E C(x, y)$ and $N T P P i(y, z)$, this means that region $x$ touches the boundary of region $y$ but does not overlap with it, and region $y$ is a part of region $z$ but does</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">GC</th>
<th style="text-align: left;">EC</th>
<th style="text-align: left;">PG</th>
<th style="text-align: left;">TPP</th>
<th style="text-align: left;">NTPP</th>
<th style="text-align: left;">TPPi</th>
<th style="text-align: left;">NTPPi</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">CG</td>
<td style="text-align: left;">GC</td>
<td style="text-align: left;">PG</td>
<td style="text-align: left;">GC</td>
<td style="text-align: left;">GC</td>
<td style="text-align: left;">GC</td>
<td style="text-align: left;">GC</td>
<td style="text-align: left;">GC</td>
</tr>
<tr>
<td style="text-align: left;">CG</td>
<td style="text-align: left;">GC</td>
<td style="text-align: left;">GC</td>
<td style="text-align: left;">GC</td>
<td style="text-align: left;">EC</td>
<td style="text-align: left;">EC</td>
<td style="text-align: left;">EC</td>
<td style="text-align: left;">EC</td>
</tr>
<tr>
<td style="text-align: left;">PG</td>
<td style="text-align: left;">GC</td>
<td style="text-align: left;">EC</td>
<td style="text-align: left;">PG</td>
<td style="text-align: left;">PG</td>
<td style="text-align: left;">PG</td>
<td style="text-align: left;">PG</td>
<td style="text-align: left;">PG</td>
</tr>
<tr>
<td style="text-align: left;">TPP</td>
<td style="text-align: left;">GC</td>
<td style="text-align: left;">EC</td>
<td style="text-align: left;">PG</td>
<td style="text-align: left;">TPP</td>
<td style="text-align: left;">NTPP</td>
<td style="text-align: left;">GC</td>
<td style="text-align: left;">GC</td>
</tr>
<tr>
<td style="text-align: left;">NTPP</td>
<td style="text-align: left;">GC</td>
<td style="text-align: left;">GC</td>
<td style="text-align: left;">NTPP</td>
<td style="text-align: left;">NTPP</td>
<td style="text-align: left;">NTPP</td>
<td style="text-align: left;">GC</td>
<td style="text-align: left;">GC</td>
</tr>
<tr>
<td style="text-align: left;">TPPi</td>
<td style="text-align: left;">GC</td>
<td style="text-align: left;">EC</td>
<td style="text-align: left;">PG</td>
<td style="text-align: left;">PG</td>
<td style="text-align: left;">PG</td>
<td style="text-align: left;">TPP</td>
<td style="text-align: left;">NTPP</td>
</tr>
<tr>
<td style="text-align: left;">NTPPi</td>
<td style="text-align: left;">GC</td>
<td style="text-align: left;">GC</td>
<td style="text-align: left;">PG</td>
<td style="text-align: left;">NTPP</td>
<td style="text-align: left;">NTPP</td>
<td style="text-align: left;">NTPP</td>
<td style="text-align: left;">NTPP</td>
</tr>
</tbody>
</table>
<p>Figure 7: The table of preferred compositions for RCC-8 produced by ChatGPT-4. Green text means it agreed with the human most likely model. Orange text means it agreed with the human most likely model but failed to notify that it was the only possible model. Red means it chose an impossible relation. Blue means it chose a possible but not preferred model. A green background means that it agreed with preferred relation of one of the language groups but not the overall average.
not touch z's boundary, with the perspective of y being contained in z. In this scenario, it's likely that $x$ and $z$ are either disconnected (DC) or touch at the boundary (EC). Since y, which is part of z but does not touch z's boundary, is touching $x$ at the boundary, there's a possibility for indirect spatial contact between $x$ and z. Without additional context, it's safer to assume that $x$ and $z$ touch at the boundary (EC). However, this is just an assumption, and the actual relationship between $x$ and $z$ could be different depending on the specific spatial arrangement.
In a different case, TPPi/TPPi, the initial understanding of the relations is correct inferring that $z$ is part of $x$, but then ChatGPT-4 makes a wrong step in the final part of its reasoning, and claims that $\operatorname{TPP}(x, y)$ :</p>
<p>If TPPi $(x, y)$ and TPPi $(y, z)$, this means that region $y$ is a part of region $x$ and touches $x$ 's boundary, and region $z$ is a part of region $y$ and touches $y$ 's boundary. Given these conditions, it's likely that $z$ is a part of $x$ and touches $x$ 's boundary (TPP). This is because $z$, which is a part of $y$ and touches $y$ 's boundary, is also indirectly a part of $x$ through $y$, which is a part of $x$ and touches $x$ 's boundary. Therefore, my preferred relationship between $x$ and $z$ would be TPP $(x, z)$, assuming that $z$ is a part of $x$ and touches $x$ 's boundary. Please note that this is an assumption and the actual relationship between $x$ and $z$ could be different depending on the specific spatial arrangement.
ChatGPT-4 makes exactly the same error with TPPi/NTPPi. For the wrong NTPPi/EC case, the the description of its reasoning contains a more fundamental error:
...This is because $y$, which is a part of $x$ and does not touch $x$ 's boundary, is also only touching z's boundary, so $x$ and $z$ would not have any overlap...
A similar more fundamental error is made in the cases of NTPPi / TPP, whilst in the final two wrong cases, ChatGPT4 misunderstands the second relation as its inverse.</p>
<p>In order to test whether the result was influenced by prior knowledge of RCC-8 gained as part of its training, we also
performed the same experiment, but with all the relation names prefixed by an X to disguise the connection to RCC8. The prompt was the same as above except for the change of relation names and the omission of the second sentence. The results are given in Fig. 8. ChatGPT-4 only agreed with the average human on 24/49 (48.99\%) though in a further five cases it agreed with one of the language groups (twice times Mongolian, twice German), and in one case the Mongolian preference was equal between the overall choice and the choice ChatGPT-4 made giving a an agreement with the German speakers of 26/49 (53.06\%) and 27/49 (55.1\%) with the Mongolian speakers. In three cases (6.12\%) it actually predicted an impossible relation as its preferred composition (this also happened sometimes in the human experiment), and in two cases (out of 11) where the composition is unique it failed to note that. In all the other cases, 20/49 (60.82\%) its preferred relation was possible but not preferred by either language group or overall. The overall agreement with humans in this anonymised case is actually significantly higher than the non-anonymised experiment above; it is not clear if there is a reason for this and if so what.</p>
<p>|  | GC | EC | PG | TPP | NTPP | TPPi | NTPPi |
| :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- |
| CG | GC | GC | GC | GC | GC | GC | GC |
| CG | GC | EC | PG | EC | EC | EC | EC |
| CG | GC | EC | PG | PG | PG | PG | PG |
| TPP | GC | EC | PG | TPP | NTPP | GC | GC |
| NTPP | GC | GC | PG | NTPP | NTPP | GC | GC |
| TPPi | GC | EC | PG | PG | PG | TPPi | NTPPi |
| NTPPi | GC | GC | PG | NTPPi | NTPP | NTPPi | NTPPi |</p>
<p>Figure 8: The table of preferred compositions for RCC-8 produced by ChatGPT-4 with anonymised relations. The relation names are coloured in the same way as in Table 7.</p>
<h2>Experiment 3: Spatial Continuity</h2>
<p>Continuity networks were introduced by Randell and Cohn(1989) to represent the set of possible 'next' relations that might obtain, assuming that motion is continuous and any transformations in the shape and/or size of an object are also continuous. Subsequently these have been termed "conceptual neighbourhoods" (Freksa 1992). Fortunately, the abbreviation for both of these terms is CN, which we shall use henceforth. The CN for RCC-8 is depicted in Fig. 1. The initial prompt was the same as for the composition table prompt above, but with two additional sentences at the end: In all these questions, assume that all motion is continuous, and any deformations of the spatial regions are also continuous. Also please describe the motion or changes of size or shape that would be required for the change of relation to take place. Each subsequent prompt also emphasises the need for the relation to be an immediate next relation. As an example prompt/response pair consider the EC case:</p>
<p>Prompt: If $E C(a, b)$ holds, then what are the next possible relations that might hold as the two regions translate or change their shape or size? I am interested in the immediate next relation, so no intermediate relation must hold. Response: If $E C(a, b)$ holds,</p>
<p>which means that the two regions $a$ and $b$ are just touching at a boundary but do not share any interior parts, then the immediate next possible relations as the two regions translate or change their shape or size could be either $D C(a, b)$ or $P O(a, b) . D C(a, b)$ : This would occur if the regions move away from each other and are no longer in contact at all, becoming fully disconnected. $P O(a, b)$ : This would occur if one region begins to overlap with the other, but neither region is completely contained within the other. They would then be partially overlapping, sharing some but not all of their spatial parts. The transitions to these relations would involve either moving the regions apart or moving them further into each other. In the case of transition to $P O(a, b)$, there could also be some deformation of the regions if they change shape to allow for the partial overlap.
An example of a faulty prediction is from PO:
If $P O(a, b)$ holds, indicating that regions $a$ and $b$ partially overlap but neither is fully contained within the other, the immediate next relations could be $E C(a, b), D C(a, b), T P P(a, b), T P P i(a, b)$, or $E Q(a, b)$. $E C(a, b)$ : This would occur if the overlapping parts of $a$ and $b$ are reduced to just boundary parts, meaning the regions are touching but no longer overlapping. $D C(a, b)$ : This would occur if regions $a$ and $b$ are moved or deformed such that they no longer overlap or touch at all. TPP $(a, b)$ or TPPi $(a, b)$ : This would occur if one region becomes fully contained within the other, either by moving into it or shrinking. TPP $(a, b)$ means $a$ is part of $b$ and touches $b$ 's boundary. TPPi $(a, b)$ means $b$ is part of $a$ and touches $a$ 's boundary. $E Q(a, b)$ : This could occur if the regions are reshaped or resized such that they coincide exactly, becoming the same region in space. The transitions to these relations would involve either moving the regions, resizing them, or reshaping them in some way.
Here, ChatGPT-4 has mistakenly said that DC is a next relation, failing to notice that it has to pass through EC on the way.</p>
<p>As can be seen, the predictions are mostly correct with just five errors: three where a link is falsely predicted and two where a link is missing. The two missing links are both from EQ (to NTPP and to NTPPi). Interestingly the links in the reverse direction are correctly predicted, so ChatGPT-4 is not able to reason that a link in one direction implies the link in the other direction should also be present. Similarly whilst the absence of a link from DC to PO is correctly predicted, the inverse case is not, and is also the case for PO to/from NTPP and NTPPi.</p>
<p>In order to test whether the result was influenced by prior knowledge of RCC-8 gained as part of its training, we also performed the same experiment, but with all the relation names prefixed by an X to disguise the connection to RCC-8. The prompt was the same as above except for the change of relation names and the omission of the second sentence. The results are given in Fig. 10. There are 3 incorrectly predicted
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 9: The Continuity Table for RCC8 produced by ChatGPT-4. An ' $x$ ' means that the relation in that column is predicted as an immediate neighbour of the relation in that row. An empty box means that the relation is not predicted as an immediate neighbour. Green means that the prediction was correct and red that it was incorrect. The leading diagonal is white since a relation is not a next relation of itself.
links, 3 missing links, 19 correctly predicted links and 31 correct missing links, giving an accuracy of 50/56 (89.2\%). This is slightly worse than the case above. There is one more missing link but the missing links are all different in the two cases. Although there are the same number of wrong links, only one of these is in common (PO to DC). Overall the results are broadly similar and may be due to the stochastic nature of ChatGPT-4's responses, suggesting that either the disguise was not very effective, or that prior training did not really affect the response and it was able to reason from 'first principles' (if not always correctly) in response to each prompt.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 10: The Continuity Table for RCC-8 produced by ChatGPT-4 using disguised relation names. The meaning of the colouring is the same as in Fig. 9.</p>
<h2>Concluding Remarks and Future Work</h2>
<p>This investigation has supported the widely-held view that LLMs can struggle to do reasoning tasks ${ }^{4}$. In the case of Experiment 1, in which ChatGPT-4 was asked to compute the entire composition table for RCC-8, this is a non trivial task even for humans, so it is perhaps not surprising that ChatGPT-4 did not achieve $100 \%$ accuracy - the scores of $71.94 \%$ (and $67.09 \%$ for the anonymised relatins) are clearly much better than chance and do suggest a reasonable facility to perform such computations. A detailed analysis of the actual conversations in the supplementary material shows that</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>sometimes ChatGPT-4 does appear able to do some interesting (qualitative) spatial reasoning, but often fails, sometimes making elementary mistakes. It also shows inconsistency in being able to reason correctly about a relation but not its inverse. It also sometimes confuses a relation with its inverse. It is possible that fine tuning, explicit chain-of-thought prompting, or more carefully engineered prompts might improve performance; however, given the stochastic nature of LLMs it seems unlikely that the results would be as good as logical reasoning (the experiment on preferred relations is of course not strictly a logical reasoning exercise, except for the requirement not to predict spatially impossible relations).</p>
<p>There are a variety of avenues for further work which present themselves. Other calculi could be experimented with - for example the coarser calculus RCC-5, or calculi for reasoning about direction or size (Cohn and Renz 2008). Other LLMs could be evaluated - though since new LLMs and new LLM versions are continually being released, this is a challenge with no definite stopping point. Tracking the change in performance of a particular LLM across releases would also be of interest - though in the case of closed LLMs such as ChatGPT-4 where the owners have the right to harvest user conversations and use them for future training, it will not be clear if any improvement is the result of leakage from the previous conversation or more general performance improvement ${ }^{5}$. It has already been observed (Cohn and Hernandez-Orallo 2023) that different LLMs have different strengths - determining which LLMs are better at which spatial reasoning tasks would also be worth of future investigation. The overall conclusion that LLMs in general struggle with more complex spatial reasoning tasks is likely to remain the case, at least for the foreseeable future. In the API version of GPT, different temperatures could be tried, and multiple runs with averages computed. Different prompts and prompting strategies could be tried, though arguably since QSR has always been viewed as a form of commonsense reasoning, it should not be necessary to devise specific prompts to elicit commonsense behaviour.</p>
<p>It is not clear how successful the anonymisation was - in one case I mistyped an X relation and it was able to suggest the intended relation name, suggesting that it has the ability to dissect relation names; thus more sophisticated anonymisation might be tried. In earlier work (Cohn and HernandezOrallo 2023) we had already done some limited experimentation asking an LLM to reason about spatial relations in a real world context rather than the purely abstract setting used in the experiments in this paper - it would be interesting to conduct more extensive tests LLMs doing compositional reasoning in a more realistic setting, and similarly for the continuity experiment.</p>
<p>Experiment 2 above already investigated how LLM performance compared to human performance to a limited extent but further investigation would be worthwhile, including a head-to-head comparison rather than simply taking a result from the literature originally intended to investigate a different question. Another interesting avenue for further</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>work will be to explore the use of multimodal FMs - when humans perform spatial reasoning tasks including the challenge of building a composition table, it is natural to use pencil and paper to sketch diagrams and possible scenarios - investigating whether a multi-modal FM with such abilities (including the ability to analyse its own drawings) would be of great interest to the spatial reasoning community.</p>
<p>As mentioned above, another possible avenue of research is to investigate different prompting strategies, including kshot (Dang et al. 2022), chain-of-thought(Wei et al. 2022) and tree-of-thought(Yao et al. 2023) strategies. Not doing so was deliberate in this paper as I was interested in exploring in how the "vanilla" LLM would perform. Whilst for specific downstream tasks, fine-tuning or employing specific prompting strategies may reasonable, there is an argument to be made that for commonsense reasoning, this is not a reasonable strategy since the task is a general one rather than a specific downstream task.</p>
<h2>Data statement</h2>
<p>All the conversations with ChatGPT-4 that support the summary tables in this paper can be found at http://tinyurl.com/qr23sup.</p>
<h2>Acknowledgments</h2>
<p>This work was supported by: The Alan Turing Institute; the Economic and Social Research Council (ESRC) under grant ES/W003473/1; the Turing's Defence and Security programme through a partnership with the UK government in accordance with the framework agreement between GCHQ and The Alan Turing Institute.</p>
<h2>References</h2>
<p>Bender, E. M.; Gebru, T.; McMillan-Major, A.; and Shmitchell, S. 2021. On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? In Proceedings of the 2021 ACM conference on fairness, accountability, and transparency, 610-623.
Brown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.; Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; et al. 2020. Language models are few-shot learners. Advances in Neural Information Processing Systems, 33: 1877-1901.
Cai, Z.; Chang, B.; and Han, W. 2023. Human-in-the-Loop through Chain-of-Thought. arXiv preprint arXiv:2306.07932.
Chen, J.; Cohn, A. G.; Liu, D.; Wang, S.; Ouyang, J.; and Yu, Q. 2015. A survey of qualitative spatial representations. The Knowledge Engineering Review, 30(1): 106-136.
Cohn, A. G.; Bennett, B.; Gooday, J.; and Gotts, N. M. 1997. Qualitative Spatial Representation and Reasoning with the Region Connection Calculus. Geoinformatica, 1(3): 275316.</p>
<p>Cohn, A. G.; and Hazarika, S. M. 2001. Qualitative spatial representation and reasoning: An overview. Fundamenta Informaticae, 46(1-2): 1-29.</p>
<p>Cohn, A. G.; and Hernandez-Orallo, J. 2023. Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs. arXiv preprint arXiv:2304.11164.
Cohn, A. G.; and Renz, J. 2008. Handbook of Knowledge Representation, chapter Qualitative Spatial Representation and Reasoning. Elsevier.
Dang, H.; Mecke, L.; Lehmann, F.; Goller, S.; and Buschek, D. 2022. How to Prompt? Opportunities and Challenges of Zero- and Few-Shot Learning for Human-AI Interaction in Creative Applications of Generative Models. arXiv:2209.01390.
Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 4171-4186. Minneapolis, Minnesota: Association for Computational Linguistics.
Freksa, C. 1992. Temporal reasoning based on semiintervals. Artificial intelligence, 54(1-2): 199-227.
Gerevini, A.; and Renz, J. 2002. Combining topological and size information for spatial reasoning. Artificial Intelligence, 137(1-2): 1-42.
Guesgen, H. W. 1989. Spatial reasoning based on Allen's temporal logic, Technical Report TR-89-049. Technical report, International Computer Science Institute Berkeley.
Jonsson, P.; and Drakengren, T. 1997. A Complete Classification of Tractability in RCC-5. J. Artif. Intell. Res., 6: 211-221.
Knauff, M.; Rauh, R.; and Schlieder, C. 1995. Preferred mental models in qualitative spatial reasoning: A cognitive assessment of Allen's calculus. In Proceedings of the seventeenth annual conference of the Cognitive Science Society, 200-205.
Moratz, R. 2006. Representing relative direction as a binary relation of oriented points. In ECAI, volume 6, 407-411.
Mukerjee, A.; and Joe, G. 1990. A Qualitative Model for Space. In Shrobe, H. E.; Dietterich, T. G.; and Swartout, W. R., eds., Proceedings of the 8th National Conference on Artificial Intelligence. Boston, Massachusetts, USA, July 29 - August 3, 1990, 2 Volumes, 721-727. AAAI Press / The MIT Press.
Ragni, M.; Tseden, B.; and Knauff, M. 2007. Cross-cultural similarities in topological reasoning. In Spatial Information Theory: 8th International Conference, COSIT 2007, 32-46. Springer.
Randell, D.; and Cohn, A. 1989. Modelling topological and metrical properties in physical processes. In Proceedings of the First International Conference on Principles of Knowledge Representation and Reasoning, 357-368.
Randell, D.; Cui, Z.; and Cohn, A. G. 1992. A Spatial Logic Based on Regions and Connection. In 3rd International Conference on Knowledge Representation and Reasoning, 1992, 165-176.</p>
<p>Roumeliotis, K. I.; and Tselikas, N. D. 2023. ChatGPT and Open-AI Models: A Preliminary Review. Future Internet, 15(6): 192.
Wei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Chi, E.; Le, Q.; and Zhou, D. 2022. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903.
Yao, S.; Yu, D.; Zhao, J.; Shafran, I.; Griffiths, T. L.; Cao, Y.; and Narasimhan, K. 2023. Tree of thoughts: Deliberate problem solving with large language models. arXiv preprint arXiv:2305.10601.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{5}$ However, note that no feedback was given to ChatGPT-4 as to whether the proffered response was correct or not.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1232 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1232</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1232</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-27.html">extraction-schema-27</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <p><strong>Paper ID:</strong> paper-270764663</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2406.19320v1.pdf" target="_blank">Efficient World Models with Context-Aware Tokenization</a></p>
                <p><strong>Paper Abstract:</strong> Scaling up deep Reinforcement Learning (RL) methods presents a significant challenge. Following developments in generative modelling, model-based RL positions itself as a strong contender. Recent advances in sequence modelling have led to effective transformer-based world models, albeit at the price of heavy computations due to the long sequences of tokens required to accurately simulate environments. In this work, we propose $\Delta$-IRIS, a new agent with a world model architecture composed of a discrete autoencoder that encodes stochastic deltas between time steps and an autoregressive transformer that predicts future deltas by summarizing the current state of the world with continuous tokens. In the Crafter benchmark, $\Delta$-IRIS sets a new state of the art at multiple frame budgets, while being an order of magnitude faster to train than previous attention-based approaches. We release our code and models at https://github.com/vmicheli/delta-iris.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1232.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1232.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>∆-IRIS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Delta-IRIS (∆-IRIS)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model-based RL world model combining a discrete context-aware autoencoder that encodes stochastic frame-to-frame deltas (∆-tokens) conditioned on past frames/actions, and an autoregressive transformer that predicts future ∆-tokens while being conditioned on continuous image embeddings (I-tokens) to summarize successive states.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>∆-IRIS</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Hybrid latent world model: a convolutional discrete autoencoder (vector-quantized) that encodes stochastic deltas between frames into a small number (e.g. 4) of discrete ∆-tokens conditioned on past frames and actions, plus a GPT-like autoregressive transformer that predicts future ∆-tokens autoregressively; the transformer sequence interleaves continuous I-tokens (frame embeddings from an auxiliary CNN) with discrete ∆-tokens to ease state representation and prediction of stochastic events. The decoder reconstructs full observations from past frames, actions, and predicted ∆-tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (discrete autoencoder + autoregressive transformer; hybrid discrete/continuous tokens)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Procedurally generated Crafter environment (Minecraft-like survival mechanics) and Atari100k (Atari games)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Autoencoder reconstruction losses (L1, L2, max-pixel), codebook/quantization metrics (bits per frame), transformer next-token cross-entropy (CE) for ∆-token prediction, CE for reward and termination predictions, downstream RL returns / Crafter success rates and task counts (e.g., tasks solved out of 22). Visual qualitative checks (reconstruction error images, imagined trajectory realism) are also used.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Autoencoder L2 reconstruction loss: 0.000185 (reported after training on 10M-frame dataset). Next-token CE (transformer) ~1.57 with I-tokens vs 1.73 w/o I-tokens. Reward CE ~0.108 (with I-tokens) vs 0.135 (w/o). Compression: example configuration uses 4 discrete tokens/frame → 4×log2(1024)=40 bits per frame vs IRIS 16 tokens = 160 bits. Task-level: solves 17 out of 22 Crafter tasks after 10M frames; exceeds DreamerV3 at multiple (larger) frame budgets. Atari: higher aggregate metrics than IRIS; training time ~26 hours in reported configuration (5× speedup vs IRIS in that setting).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Moderately interpretable in functional aspects: the architecture enforces a disentanglement between deterministic components (handled by the decoder conditioned on history) and stochastic deltas (explicitly encoded in ∆-tokens), which can be probed via sampling experiments; otherwise the encoder/transformer remain neural black boxes for low-level features.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Qualitative visualization of imagined trajectories and reconstructions; two-trajectory sampling experiment comparing random ∆-token sampling vs autoregressive sampling to show which aspects are stochastic vs deterministic; bottom-1% reconstruction example visualizations; ablation studies removing I-tokens or history conditioning to reveal functional roles.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Designed for low token counts (e.g. 4 discrete tokens/frame) to reduce transformer sequence lengths and attention cost; reported to train an order of magnitude faster than IRIS in Crafter experiments and to achieve a 5× speedup vs IRIS in the Atari reported run (training time ~26 hours for ∆-IRIS in the Atari config). Runs on a single NVIDIA A100 40GB in the experiments; memory and precise FLOPs not explicitly enumerated but attention costs scale down quadratically with shorter token sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Approximately 10× faster training time than IRIS in Crafter experiments; reported 5× faster training vs IRIS in Atari setup; outperforms DreamerV3 at larger frame budgets while keeping competitive training efficiency (DreamerV3 better at smallest budgets). Compression (40 bits/frame vs 160 bits) reduces sequence length and transformer compute substantially.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Sets new state-of-the-art on the Crafter benchmark in these experiments (solving 17/22 tasks after 10M frames); outperforms IRIS at all tested frame budgets and outperforms DreamerV3 after ~3M collected frames (DreamerV3 better for the smallest budgets). Also shows improved aggregate Atari100k metrics versus IRIS in the reported run.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>High-fidelity autoencoding under heavy compression (4 tokens) yields realistic imagined trajectories that support effective policy learning in imagination, demonstrating that task-relevant dynamics (deterministic mechanics, crafting events, movement) are preserved; disentangling deterministic and stochastic components focuses expensive autoregressive modelling capacity on stochastic events, improving imagined-to-collected-data utility. However, the policy does not yet leverage internal representations of the world model directly (left for future work), which may limit sample-efficiency at very low data regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Major trade-off: aggressive compression (few discrete tokens) reduces compute but makes autoregressive prediction of deltas harder; introducing continuous I-tokens mitigates this by providing summarized state information. Removing I-tokens drastically hurts next-token loss and downstream performance. Conditioning the autoencoder on past frames/actions moves deterministic modelling out of the autoregressive model but assumes accurate conditioning and can hide dynamics in decoder dependencies. There's a balance between reconstruction fidelity (more tokens) and transformer compute (sequence length).</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Key design: condition discrete autoencoder on previous frames and actions to encode only stochastic deltas (∆-tokens); use vector quantization with factorized normalized codes and straight-through estimator; interleave continuous I-tokens (auxiliary CNN embeddings of frames) with discrete ∆-tokens in transformer's input; train autoencoder with L1/L2/max-pixel and commitment losses; use cross-entropy for next-token/reward/termination and two-hot symlog discrete regression for rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to IRIS (discrete autoencoder that encodes frames independently + transformer over absolute image tokens), ∆-IRIS uses far fewer tokens per frame, achieves much faster training, and better reconstructions under compression. Compared to DreamerV3 (RSSM-style recurrent latent model), ∆-IRIS outperforms at larger data budgets but is less effective in the extremely low-data regime; ∆-IRIS models joint distribution of future latent states via autoregression whereas RSSM-based methods predict factorized/product distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper recommends conditioning the autoencoder on past frames/actions and interleaving continuous I-tokens with ∆-tokens for best balance; suggests future improvements like dynamically predicting varying numbers of ∆-tokens per time step (adaptive token counts) and leveraging internal model representations for policy learning. No single final optimal configuration is claimed, but the combination of context-aware ∆-token autoencoding plus I-tokens is recommended for scaling to visually complex environments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Efficient World Models with Context-Aware Tokenization', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1232.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1232.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IRIS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>IRIS (Transformers are sample-efficient world models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior world-model agent that composes a discrete autoencoder with an autoregressive transformer to cast dynamics learning as sequence modelling over image tokens; it encodes and decodes frames independently (absolute image tokens per frame).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Transformers are sample-efficient world models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>IRIS</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Discrete autoencoder (VQ-based) that encodes each frame into KI image tokens independently, and an autoregressive transformer that models dynamics by autoregressively composing those image tokens over time (transformer attends across long sequences of image tokens and actions).</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (discrete autoencoder + autoregressive transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Atari 100k (original demonstration), used here as a baseline on Crafter and Atari experiments</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Autoencoder reconstruction losses (L1/L2/max-pixel), bits per frame (tokens × codebook size bits), downstream RL returns and task success rates.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Reconstruction L2 losses reported for comparison: IRIS (64 tokens) L2 ≈ 0.001715; IRIS (16 tokens) L2 ≈ 0.007496 (on the 10M-frame held-out dataset used for comparisons). Requires many tokens per frame for good reconstruction in visually rich environments.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Limited interpretability beyond visual reconstructions; no explicit disentangling of deterministic vs stochastic dynamics because frames are encoded independently. Visual reconstruction failures are interpretable and correlate with poor task performance.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Visualization of reconstructions (bottom-1% examples), ablation vs ∆-IRIS conditioning to show impact on reconstructions and downstream RL performance.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>High transformer compute due to long token sequences (many image tokens per frame; attention scales quadratically with sequence length). Reported to be prohibitively slow in Crafter experiments, leading to early stopping before 10M frames in some runs. Larger KI (64 tokens) further increases cost.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Substantially less efficient than ∆-IRIS in these experiments (∆-IRIS trains ~10× faster). IRIS requires many more tokens (16–64 tokens/frame reported) causing heavier attention cost compared to ∆-IRIS (4 tokens/frame in reported config).</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Underperforms ∆-IRIS on Crafter in these experiments; IRIS (64 tokens) reported Crafter score lower than ∆-IRIS and experiments with IRIS were limited by computational cost.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Good reconstruction under high token budgets can enable decent imagined training, but the compute cost and difficulty scaling to visually rich environments reduces practical utility. In this paper, poor reconstructions when token budget limited lead to worse imagined worlds and degraded policy learning.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Encoding frames independently simplifies modelling but requires many tokens to capture visual detail, which massively increases transformer compute; lower token counts lead to large reconstruction errors and poor downstream RL. Thus there's a fidelity vs efficiency trade-off unfavorable for IRIS in complex domains.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Encodes frames independently (no conditioning on past frames/actions), uses a discrete autoencoder and autoregressive transformer; choice of KI (tokens per frame) strongly affects fidelity and compute.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to ∆-IRIS, IRIS is less efficient and requires larger token budgets to achieve similar reconstruction fidelity; compared to RSSM-based Dreamer agents, IRIS (autoregressive) models joint future latent distributions but at higher compute cost when tokens are many.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Not proposed in this paper; IRIS' limitations motivate ∆-IRIS design choices (context-aware encoding and I-tokens). The paper shows conditioning on history and reducing tokens (as in ∆-IRIS) is preferable for complex visually rich environments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Efficient World Models with Context-Aware Tokenization', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1232.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1232.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DreamerV3</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DreamerV3 (Mastering diverse domains through world models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A state-of-the-art model-based RL agent using a recurrent state-space model (RSSM) with categorical latents and various scaling/regularization techniques (symlog, KL balancing) designed for wide applicability across domains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mastering diverse domains through world models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DreamerV3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Latent world model combining a convolutional encoder/decoder and a recurrent state-space model (RSSM) that maintains a deterministic recurrent state plus stochastic latent variables (categorical latents), trained with KL balancing, symlog reward/value scaling and other practical techniques; used to produce imagined trajectories for policy learning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (recurrent state-space model / RSSM)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>General RL domains; in this paper used as a baseline on Crafter and Atari100k</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Downstream returns (RL performance), model prediction losses for latent transitions/rewards/terminations, sample-efficiency metrics (returns vs frames).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>In these experiments DreamerV3 performs better than ∆-IRIS at the smallest frame budgets but is outperformed by ∆-IRIS beyond ~3M frames on Crafter. Exact numerical returns per budget are presented in the paper's tables and plots (DreamerV3 variants reported).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Standard recurrent latent models are largely black-box; interpretability primarily via visualizing imagined trajectories or inspecting latent statistics, not explicit disentanglement of deterministic vs stochastic pixel deltas.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Comparative evaluation via downstream returns and qualitative checks of imagined behavior; not specialized interpretability methods reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Designed for scalability with well-tuned defaults; different sizes (M, XL) traded off between compute and performance. In these experiments DreamerV3 variants were run with standard configs; DreamerV3 is generally computationally efficient relative to large autoregressive transformer models with long sequences, and better in very low-data regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>DreamerV3 outperforms ∆-IRIS at the smallest data budgets (more sample-efficient early) but is overtaken by ∆-IRIS at larger budgets in this work. ∆-IRIS is faster to train than IRIS; relative compute vs DreamerV3 depends on configuration and budget.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Better than ∆-IRIS for smallest frame budgets in Crafter; ∆-IRIS surpasses DreamerV3 beyond ~3M frames in these experiments. DreamerV3 remains a strong baseline across many domains.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>RSSM-based methods like DreamerV3 give good sample-efficiency at low data budgets and provide representations that can be used for policy learning; however, autoregressive discrete-token models (like ∆-IRIS) can better model joint future latent distributions and scale to larger budgets when combined with efficient tokenization.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>RSSM methods are sample-efficient early (good for small budgets) but may be less expressive than autoregressive joint models at larger budgets; DreamerV3's design choices trade off expressivity and simplicity/speed.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Categorical latents, KL balancing, symlog scaling for rewards/values, return scaling for static entropy regularization, and other practical scaling choices to make a general-purpose world model.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to autoregressive discrete-token models (IRIS, ∆-IRIS), DreamerV3 uses an RSSM that predicts factorized distributions (product laws) over future latents, which can be less expressive than autoregressive joint modelling but is computationally efficient and effective in small-data regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper does not prescribe a single optimal model; in this study DreamerV3 is best at smallest budgets, while ∆-IRIS is recommended when scaling to visually complex environments and larger frame budgets with attention-based dynamics if efficient tokenization is used.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Efficient World Models with Context-Aware Tokenization', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1232.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1232.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DreamerV2 / RSSM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DreamerV2 (RSSM-based world model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A latent dynamics model using a Recurrent State-Space Model (RSSM) that carries a deterministic recurrent state and stochastic latents, trained with categorical latents and KL-balancing; historically successful for learning in imagination.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Dream to control: Learning behaviors by latent imagination</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DreamerV2 / RSSM</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Latent world model combining a convolutional autoencoder and an RSSM (deterministic recurrence + stochastic latent variables) that predicts future latents (often factorized) used for imagined rollouts and policy learning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (recurrent state-space model / RSSM)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Atari games and other RL domains; referenced as prior work and foundational comparison point</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Predictive losses on latent transitions, KL regularization balances, downstream RL returns and sample-efficiency metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Known from literature to achieve strong performance in many domains (e.g., DreamerV2 successes in Atari), but in this paper it is discussed as background; direct numbers not re-reported here beyond relative comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Behavior explained via recurrent deterministic state plus stochastic latents; limited explicit interpretability of latent dimensions in general.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Not detailed in this paper; RSSM interpretability typically via latent visualization and probing in cited literature.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Generally computationally efficient relative to very large autoregressive transformer sequences; recurrent state reduces sequence length processed by attention compared to token-level autoregressive approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>RSSM-based agents tradeoff expressivity for efficiency; do not model joint distribution of future states explicitly (predict product laws), which simplifies computation but may sacrifice expressiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Effective sample-efficient baseline historically; DreamerV2/Dreamer family have been state-of-the-art on many benchmarks before later developments.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>RSSM architectures are useful when sample-efficiency and computational constraints favor compact recurrent representations, but may struggle with modeling complex multimodal joint futures versus autoregressive joint models.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Expressivity vs computational efficiency: RSSMs predict factorized distributions which can be limiting for joint multimodal futures, but they are cheaper and more sample-efficient early in training.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Carry a deterministic recurrent state and model stochastic latents; categorical latents and KL balancing used in DreamerV2/V3 families.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to autoregressive discrete-token approaches (IRIS/∆-IRIS), RSSMs are often more computationally efficient and sample-efficient early but may be less expressive for joint future modelling; the paper highlights this as motivation for autoregressive approaches that model joint distributions explicitly.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Efficient World Models with Context-Aware Tokenization', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Transformers are sample-efficient world models <em>(Rating: 2)</em></li>
                <li>Mastering diverse domains through world models <em>(Rating: 2)</em></li>
                <li>Dream to control: Learning behaviors by latent imagination <em>(Rating: 2)</em></li>
                <li>Recurrent world models facilitate policy evolution <em>(Rating: 2)</em></li>
                <li>Vector quantized models for planning <em>(Rating: 2)</em></li>
                <li>Efficient stochastic transformer based world models for reinforcement learning <em>(Rating: 2)</em></li>
                <li>TransDreamer: Reinforcement learning with transformer world models <em>(Rating: 1)</em></li>
                <li>Temporally consistent video transformer for long-term video prediction <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1232",
    "paper_id": "paper-270764663",
    "extraction_schema_id": "extraction-schema-27",
    "extracted_data": [
        {
            "name_short": "∆-IRIS",
            "name_full": "Delta-IRIS (∆-IRIS)",
            "brief_description": "A model-based RL world model combining a discrete context-aware autoencoder that encodes stochastic frame-to-frame deltas (∆-tokens) conditioned on past frames/actions, and an autoregressive transformer that predicts future ∆-tokens while being conditioned on continuous image embeddings (I-tokens) to summarize successive states.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "∆-IRIS",
            "model_description": "Hybrid latent world model: a convolutional discrete autoencoder (vector-quantized) that encodes stochastic deltas between frames into a small number (e.g. 4) of discrete ∆-tokens conditioned on past frames and actions, plus a GPT-like autoregressive transformer that predicts future ∆-tokens autoregressively; the transformer sequence interleaves continuous I-tokens (frame embeddings from an auxiliary CNN) with discrete ∆-tokens to ease state representation and prediction of stochastic events. The decoder reconstructs full observations from past frames, actions, and predicted ∆-tokens.",
            "model_type": "latent world model (discrete autoencoder + autoregressive transformer; hybrid discrete/continuous tokens)",
            "task_domain": "Procedurally generated Crafter environment (Minecraft-like survival mechanics) and Atari100k (Atari games)",
            "fidelity_metric": "Autoencoder reconstruction losses (L1, L2, max-pixel), codebook/quantization metrics (bits per frame), transformer next-token cross-entropy (CE) for ∆-token prediction, CE for reward and termination predictions, downstream RL returns / Crafter success rates and task counts (e.g., tasks solved out of 22). Visual qualitative checks (reconstruction error images, imagined trajectory realism) are also used.",
            "fidelity_performance": "Autoencoder L2 reconstruction loss: 0.000185 (reported after training on 10M-frame dataset). Next-token CE (transformer) ~1.57 with I-tokens vs 1.73 w/o I-tokens. Reward CE ~0.108 (with I-tokens) vs 0.135 (w/o). Compression: example configuration uses 4 discrete tokens/frame → 4×log2(1024)=40 bits per frame vs IRIS 16 tokens = 160 bits. Task-level: solves 17 out of 22 Crafter tasks after 10M frames; exceeds DreamerV3 at multiple (larger) frame budgets. Atari: higher aggregate metrics than IRIS; training time ~26 hours in reported configuration (5× speedup vs IRIS in that setting).",
            "interpretability_assessment": "Moderately interpretable in functional aspects: the architecture enforces a disentanglement between deterministic components (handled by the decoder conditioned on history) and stochastic deltas (explicitly encoded in ∆-tokens), which can be probed via sampling experiments; otherwise the encoder/transformer remain neural black boxes for low-level features.",
            "interpretability_method": "Qualitative visualization of imagined trajectories and reconstructions; two-trajectory sampling experiment comparing random ∆-token sampling vs autoregressive sampling to show which aspects are stochastic vs deterministic; bottom-1% reconstruction example visualizations; ablation studies removing I-tokens or history conditioning to reveal functional roles.",
            "computational_cost": "Designed for low token counts (e.g. 4 discrete tokens/frame) to reduce transformer sequence lengths and attention cost; reported to train an order of magnitude faster than IRIS in Crafter experiments and to achieve a 5× speedup vs IRIS in the Atari reported run (training time ~26 hours for ∆-IRIS in the Atari config). Runs on a single NVIDIA A100 40GB in the experiments; memory and precise FLOPs not explicitly enumerated but attention costs scale down quadratically with shorter token sequences.",
            "efficiency_comparison": "Approximately 10× faster training time than IRIS in Crafter experiments; reported 5× faster training vs IRIS in Atari setup; outperforms DreamerV3 at larger frame budgets while keeping competitive training efficiency (DreamerV3 better at smallest budgets). Compression (40 bits/frame vs 160 bits) reduces sequence length and transformer compute substantially.",
            "task_performance": "Sets new state-of-the-art on the Crafter benchmark in these experiments (solving 17/22 tasks after 10M frames); outperforms IRIS at all tested frame budgets and outperforms DreamerV3 after ~3M collected frames (DreamerV3 better for the smallest budgets). Also shows improved aggregate Atari100k metrics versus IRIS in the reported run.",
            "task_utility_analysis": "High-fidelity autoencoding under heavy compression (4 tokens) yields realistic imagined trajectories that support effective policy learning in imagination, demonstrating that task-relevant dynamics (deterministic mechanics, crafting events, movement) are preserved; disentangling deterministic and stochastic components focuses expensive autoregressive modelling capacity on stochastic events, improving imagined-to-collected-data utility. However, the policy does not yet leverage internal representations of the world model directly (left for future work), which may limit sample-efficiency at very low data regimes.",
            "tradeoffs_observed": "Major trade-off: aggressive compression (few discrete tokens) reduces compute but makes autoregressive prediction of deltas harder; introducing continuous I-tokens mitigates this by providing summarized state information. Removing I-tokens drastically hurts next-token loss and downstream performance. Conditioning the autoencoder on past frames/actions moves deterministic modelling out of the autoregressive model but assumes accurate conditioning and can hide dynamics in decoder dependencies. There's a balance between reconstruction fidelity (more tokens) and transformer compute (sequence length).",
            "design_choices": "Key design: condition discrete autoencoder on previous frames and actions to encode only stochastic deltas (∆-tokens); use vector quantization with factorized normalized codes and straight-through estimator; interleave continuous I-tokens (auxiliary CNN embeddings of frames) with discrete ∆-tokens in transformer's input; train autoencoder with L1/L2/max-pixel and commitment losses; use cross-entropy for next-token/reward/termination and two-hot symlog discrete regression for rewards.",
            "comparison_to_alternatives": "Compared to IRIS (discrete autoencoder that encodes frames independently + transformer over absolute image tokens), ∆-IRIS uses far fewer tokens per frame, achieves much faster training, and better reconstructions under compression. Compared to DreamerV3 (RSSM-style recurrent latent model), ∆-IRIS outperforms at larger data budgets but is less effective in the extremely low-data regime; ∆-IRIS models joint distribution of future latent states via autoregression whereas RSSM-based methods predict factorized/product distributions.",
            "optimal_configuration": "Paper recommends conditioning the autoencoder on past frames/actions and interleaving continuous I-tokens with ∆-tokens for best balance; suggests future improvements like dynamically predicting varying numbers of ∆-tokens per time step (adaptive token counts) and leveraging internal model representations for policy learning. No single final optimal configuration is claimed, but the combination of context-aware ∆-token autoencoding plus I-tokens is recommended for scaling to visually complex environments.",
            "uuid": "e1232.0",
            "source_info": {
                "paper_title": "Efficient World Models with Context-Aware Tokenization",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "IRIS",
            "name_full": "IRIS (Transformers are sample-efficient world models)",
            "brief_description": "A prior world-model agent that composes a discrete autoencoder with an autoregressive transformer to cast dynamics learning as sequence modelling over image tokens; it encodes and decodes frames independently (absolute image tokens per frame).",
            "citation_title": "Transformers are sample-efficient world models",
            "mention_or_use": "use",
            "model_name": "IRIS",
            "model_description": "Discrete autoencoder (VQ-based) that encodes each frame into KI image tokens independently, and an autoregressive transformer that models dynamics by autoregressively composing those image tokens over time (transformer attends across long sequences of image tokens and actions).",
            "model_type": "latent world model (discrete autoencoder + autoregressive transformer)",
            "task_domain": "Atari 100k (original demonstration), used here as a baseline on Crafter and Atari experiments",
            "fidelity_metric": "Autoencoder reconstruction losses (L1/L2/max-pixel), bits per frame (tokens × codebook size bits), downstream RL returns and task success rates.",
            "fidelity_performance": "Reconstruction L2 losses reported for comparison: IRIS (64 tokens) L2 ≈ 0.001715; IRIS (16 tokens) L2 ≈ 0.007496 (on the 10M-frame held-out dataset used for comparisons). Requires many tokens per frame for good reconstruction in visually rich environments.",
            "interpretability_assessment": "Limited interpretability beyond visual reconstructions; no explicit disentangling of deterministic vs stochastic dynamics because frames are encoded independently. Visual reconstruction failures are interpretable and correlate with poor task performance.",
            "interpretability_method": "Visualization of reconstructions (bottom-1% examples), ablation vs ∆-IRIS conditioning to show impact on reconstructions and downstream RL performance.",
            "computational_cost": "High transformer compute due to long token sequences (many image tokens per frame; attention scales quadratically with sequence length). Reported to be prohibitively slow in Crafter experiments, leading to early stopping before 10M frames in some runs. Larger KI (64 tokens) further increases cost.",
            "efficiency_comparison": "Substantially less efficient than ∆-IRIS in these experiments (∆-IRIS trains ~10× faster). IRIS requires many more tokens (16–64 tokens/frame reported) causing heavier attention cost compared to ∆-IRIS (4 tokens/frame in reported config).",
            "task_performance": "Underperforms ∆-IRIS on Crafter in these experiments; IRIS (64 tokens) reported Crafter score lower than ∆-IRIS and experiments with IRIS were limited by computational cost.",
            "task_utility_analysis": "Good reconstruction under high token budgets can enable decent imagined training, but the compute cost and difficulty scaling to visually rich environments reduces practical utility. In this paper, poor reconstructions when token budget limited lead to worse imagined worlds and degraded policy learning.",
            "tradeoffs_observed": "Encoding frames independently simplifies modelling but requires many tokens to capture visual detail, which massively increases transformer compute; lower token counts lead to large reconstruction errors and poor downstream RL. Thus there's a fidelity vs efficiency trade-off unfavorable for IRIS in complex domains.",
            "design_choices": "Encodes frames independently (no conditioning on past frames/actions), uses a discrete autoencoder and autoregressive transformer; choice of KI (tokens per frame) strongly affects fidelity and compute.",
            "comparison_to_alternatives": "Compared to ∆-IRIS, IRIS is less efficient and requires larger token budgets to achieve similar reconstruction fidelity; compared to RSSM-based Dreamer agents, IRIS (autoregressive) models joint future latent distributions but at higher compute cost when tokens are many.",
            "optimal_configuration": "Not proposed in this paper; IRIS' limitations motivate ∆-IRIS design choices (context-aware encoding and I-tokens). The paper shows conditioning on history and reducing tokens (as in ∆-IRIS) is preferable for complex visually rich environments.",
            "uuid": "e1232.1",
            "source_info": {
                "paper_title": "Efficient World Models with Context-Aware Tokenization",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "DreamerV3",
            "name_full": "DreamerV3 (Mastering diverse domains through world models)",
            "brief_description": "A state-of-the-art model-based RL agent using a recurrent state-space model (RSSM) with categorical latents and various scaling/regularization techniques (symlog, KL balancing) designed for wide applicability across domains.",
            "citation_title": "Mastering diverse domains through world models",
            "mention_or_use": "use",
            "model_name": "DreamerV3",
            "model_description": "Latent world model combining a convolutional encoder/decoder and a recurrent state-space model (RSSM) that maintains a deterministic recurrent state plus stochastic latent variables (categorical latents), trained with KL balancing, symlog reward/value scaling and other practical techniques; used to produce imagined trajectories for policy learning.",
            "model_type": "latent world model (recurrent state-space model / RSSM)",
            "task_domain": "General RL domains; in this paper used as a baseline on Crafter and Atari100k",
            "fidelity_metric": "Downstream returns (RL performance), model prediction losses for latent transitions/rewards/terminations, sample-efficiency metrics (returns vs frames).",
            "fidelity_performance": "In these experiments DreamerV3 performs better than ∆-IRIS at the smallest frame budgets but is outperformed by ∆-IRIS beyond ~3M frames on Crafter. Exact numerical returns per budget are presented in the paper's tables and plots (DreamerV3 variants reported).",
            "interpretability_assessment": "Standard recurrent latent models are largely black-box; interpretability primarily via visualizing imagined trajectories or inspecting latent statistics, not explicit disentanglement of deterministic vs stochastic pixel deltas.",
            "interpretability_method": "Comparative evaluation via downstream returns and qualitative checks of imagined behavior; not specialized interpretability methods reported here.",
            "computational_cost": "Designed for scalability with well-tuned defaults; different sizes (M, XL) traded off between compute and performance. In these experiments DreamerV3 variants were run with standard configs; DreamerV3 is generally computationally efficient relative to large autoregressive transformer models with long sequences, and better in very low-data regimes.",
            "efficiency_comparison": "DreamerV3 outperforms ∆-IRIS at the smallest data budgets (more sample-efficient early) but is overtaken by ∆-IRIS at larger budgets in this work. ∆-IRIS is faster to train than IRIS; relative compute vs DreamerV3 depends on configuration and budget.",
            "task_performance": "Better than ∆-IRIS for smallest frame budgets in Crafter; ∆-IRIS surpasses DreamerV3 beyond ~3M frames in these experiments. DreamerV3 remains a strong baseline across many domains.",
            "task_utility_analysis": "RSSM-based methods like DreamerV3 give good sample-efficiency at low data budgets and provide representations that can be used for policy learning; however, autoregressive discrete-token models (like ∆-IRIS) can better model joint future latent distributions and scale to larger budgets when combined with efficient tokenization.",
            "tradeoffs_observed": "RSSM methods are sample-efficient early (good for small budgets) but may be less expressive than autoregressive joint models at larger budgets; DreamerV3's design choices trade off expressivity and simplicity/speed.",
            "design_choices": "Categorical latents, KL balancing, symlog scaling for rewards/values, return scaling for static entropy regularization, and other practical scaling choices to make a general-purpose world model.",
            "comparison_to_alternatives": "Compared to autoregressive discrete-token models (IRIS, ∆-IRIS), DreamerV3 uses an RSSM that predicts factorized distributions (product laws) over future latents, which can be less expressive than autoregressive joint modelling but is computationally efficient and effective in small-data regimes.",
            "optimal_configuration": "Paper does not prescribe a single optimal model; in this study DreamerV3 is best at smallest budgets, while ∆-IRIS is recommended when scaling to visually complex environments and larger frame budgets with attention-based dynamics if efficient tokenization is used.",
            "uuid": "e1232.2",
            "source_info": {
                "paper_title": "Efficient World Models with Context-Aware Tokenization",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "DreamerV2 / RSSM",
            "name_full": "DreamerV2 (RSSM-based world model)",
            "brief_description": "A latent dynamics model using a Recurrent State-Space Model (RSSM) that carries a deterministic recurrent state and stochastic latents, trained with categorical latents and KL-balancing; historically successful for learning in imagination.",
            "citation_title": "Dream to control: Learning behaviors by latent imagination",
            "mention_or_use": "mention",
            "model_name": "DreamerV2 / RSSM",
            "model_description": "Latent world model combining a convolutional autoencoder and an RSSM (deterministic recurrence + stochastic latent variables) that predicts future latents (often factorized) used for imagined rollouts and policy learning.",
            "model_type": "latent world model (recurrent state-space model / RSSM)",
            "task_domain": "Atari games and other RL domains; referenced as prior work and foundational comparison point",
            "fidelity_metric": "Predictive losses on latent transitions, KL regularization balances, downstream RL returns and sample-efficiency metrics.",
            "fidelity_performance": "Known from literature to achieve strong performance in many domains (e.g., DreamerV2 successes in Atari), but in this paper it is discussed as background; direct numbers not re-reported here beyond relative comparisons.",
            "interpretability_assessment": "Behavior explained via recurrent deterministic state plus stochastic latents; limited explicit interpretability of latent dimensions in general.",
            "interpretability_method": "Not detailed in this paper; RSSM interpretability typically via latent visualization and probing in cited literature.",
            "computational_cost": "Generally computationally efficient relative to very large autoregressive transformer sequences; recurrent state reduces sequence length processed by attention compared to token-level autoregressive approaches.",
            "efficiency_comparison": "RSSM-based agents tradeoff expressivity for efficiency; do not model joint distribution of future states explicitly (predict product laws), which simplifies computation but may sacrifice expressiveness.",
            "task_performance": "Effective sample-efficient baseline historically; DreamerV2/Dreamer family have been state-of-the-art on many benchmarks before later developments.",
            "task_utility_analysis": "RSSM architectures are useful when sample-efficiency and computational constraints favor compact recurrent representations, but may struggle with modeling complex multimodal joint futures versus autoregressive joint models.",
            "tradeoffs_observed": "Expressivity vs computational efficiency: RSSMs predict factorized distributions which can be limiting for joint multimodal futures, but they are cheaper and more sample-efficient early in training.",
            "design_choices": "Carry a deterministic recurrent state and model stochastic latents; categorical latents and KL balancing used in DreamerV2/V3 families.",
            "comparison_to_alternatives": "Compared to autoregressive discrete-token approaches (IRIS/∆-IRIS), RSSMs are often more computationally efficient and sample-efficient early but may be less expressive for joint future modelling; the paper highlights this as motivation for autoregressive approaches that model joint distributions explicitly.",
            "uuid": "e1232.3",
            "source_info": {
                "paper_title": "Efficient World Models with Context-Aware Tokenization",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Transformers are sample-efficient world models",
            "rating": 2,
            "sanitized_title": "transformers_are_sampleefficient_world_models"
        },
        {
            "paper_title": "Mastering diverse domains through world models",
            "rating": 2,
            "sanitized_title": "mastering_diverse_domains_through_world_models"
        },
        {
            "paper_title": "Dream to control: Learning behaviors by latent imagination",
            "rating": 2,
            "sanitized_title": "dream_to_control_learning_behaviors_by_latent_imagination"
        },
        {
            "paper_title": "Recurrent world models facilitate policy evolution",
            "rating": 2,
            "sanitized_title": "recurrent_world_models_facilitate_policy_evolution"
        },
        {
            "paper_title": "Vector quantized models for planning",
            "rating": 2,
            "sanitized_title": "vector_quantized_models_for_planning"
        },
        {
            "paper_title": "Efficient stochastic transformer based world models for reinforcement learning",
            "rating": 2,
            "sanitized_title": "efficient_stochastic_transformer_based_world_models_for_reinforcement_learning"
        },
        {
            "paper_title": "TransDreamer: Reinforcement learning with transformer world models",
            "rating": 1,
            "sanitized_title": "transdreamer_reinforcement_learning_with_transformer_world_models"
        },
        {
            "paper_title": "Temporally consistent video transformer for long-term video prediction",
            "rating": 1,
            "sanitized_title": "temporally_consistent_video_transformer_for_longterm_video_prediction"
        }
    ],
    "cost": 0.01839875,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Efficient World Models with Context-Aware Tokenization</p>
<p>Vincent Micheli 
University of Geneva
Switzerland</p>
<p>Eloi Alonso 
University of Geneva
Switzerland</p>
<p>Franc ¸ois Fleuret 
University of Geneva
Switzerland</p>
<p>Efficient World Models with Context-Aware Tokenization
2AA29B0E98D914DBA30BB4EC9C06F941
Scaling up deep Reinforcement Learning (RL) methods presents a significant challenge.Following developments in generative modelling, modelbased RL positions itself as a strong contender.Recent advances in sequence modelling have led to effective transformer-based world models, albeit at the price of heavy computations due to the long sequences of tokens required to accurately simulate environments.In this work, we propose ∆-IRIS, a new agent with a world model architecture composed of a discrete autoencoder that encodes stochastic deltas between time steps and an autoregressive transformer that predicts future deltas by summarizing the current state of the world with continuous tokens.In the Crafter benchmark, ∆-IRIS sets a new state of the art at multiple frame budgets, while being an order of magnitude faster to train than previous attention-based approaches.We release our code and models at https://github.com/vmicheli/delta-iris.</p>
<p>Introduction</p>
<p>Deep Reinforcement Learning (RL) methods have recently delivered impressive results (Ye et al., 2021;Hafner et al., 2023;Schwarzer et al., 2023) in traditional benchmarks (Bellemare et al., 2013;Tassa et al., 2018).In light of the evermore complex domains tackled by the latest generations of generative models (Rombach et al., 2022;Achiam et al., 2023), the prospect of training agents in more ambitious environments (Kanervisto et al., 2022) may hold significant appeal.However, that leap forward poses a serious challenge: deep RL architectures have been comparatively smaller and less sample-efficient than their (self-)supervised counterparts.In contrast, more intricate environments necessitate models with greater representational power and have higher data requirements.</p>
<p>Proceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024.Copyright 2024 by the author(s).</p>
<p>Model-based RL (MBRL) (Sutton &amp; Barto, 2018) is hypothesized to be the key for scaling up deep RL agents (LeCun, 2022).Indeed, world models (Ha &amp; Schmidhuber, 2018) offer a diverse range of capabilities: lookahead search (Schrittwieser et al., 2020;Ye et al., 2021), learning in imagination (Sutton, 1991;Hafner et al., 2023), representation learning (Schwarzer et al., 2021;D'Oro et al., 2023), and uncertainty estimation (Pathak et al., 2017;Sekar et al., 2020).In essence, MBRL shifts the focus from the RL problem to a generative modelling problem, where the development of an accurate world model significantly simplifies policy training.In particular, policies learnt in the imagination of world models are freed from sample efficiency constraints, a common limitation of RL agents that is magnified in complex environments with slow rollouts.</p>
<p>Recently, the IRIS agent (Micheli et al., 2023) achieved strong results in the Atari 100k benchmark (Bellemare et al., 2013;Kaiser et al., 2020).IRIS introduced a world model composed of a discrete autoencoder and an autoregressive transformer, casting dynamics learning as a sequence modelling problem where the transformer composes over time a vocabulary of image tokens built by the autoencoder.This approach opened up avenues for future model-based methods to capitalize on advances in generative modelling (Villegas et al., 2022;Achiam et al., 2023), and has already been adopted beyond its original domain (comma.ai, 2023;Hu et al., 2023).However, in its current form, scaling IRIS to more complex environments is computationally prohibitive.Indeed, such an endeavor requires a large number of tokens to encode visually challenging frames.Besides, sophisticated dynamics may require to store numerous time steps in memory to reason about the past, ultimately making the imagination procedure excessively slow.Hence, under these constraints, maintaining a favorable imagined-to-collected data ratio is practically infeasible.</p>
<p>In the present work, we introduce ∆-IRIS, a new agent capable of scaling to visually complex environments with lengthier time horizons.∆-IRIS encodes frames by attending to the ongoing trajectory of observations and actions, effectively describing stochastic deltas between time steps.This enriched conditioning scheme drastically reduces the number of tokens to encode frames, offloads the deterministic aspects of world modelling to the autoencoder, and lets the autoregressive transformer focus on stochastic dynamics.1 arXiv: 2406.19320v1 [cs.LG] 27 Jun 2024</p>
<p>Efficient World Models with Context-Aware Tokenization
z 1 1 . . . z K I 1 z 1 2 . . . z K I 2 x 1 x1 x 2 x2 x 0 a 0 z 1 1 . . . z K 1 x 1 a 1 z 1 2 . . . z K 2 x 1 x 2 x 0 a 0 a 1 a 2 x1 x2
Encoder Decoder Figure 1.Discrete autoencoder of IRIS (Micheli et al., 2023) (left) and ∆-IRIS (right).IRIS encodes and decodes frames independently, meaning that zt has to carry all the information necessary to reconstruct xt.On the other hand, ∆-IRIS' encoder and decoder are conditioned on past frames and actions, thus zt only has to capture what has changed and that cannot be inferred from actions, i.e. the stochastic delta.This conditioning scheme enables us to drastically reduce the number of tokens required to encode a frame with minimal loss (K ≪ KI ), which is critical to speed up the autoregressive transformer that predicts future tokens.</p>
<p>Nonetheless, substituting the sequence of absolute image tokens with a sequence of ∆-tokens makes the task of the autoregressive model more arduous.In order to predict the next transition, it may only reason over previous ∆-tokens, and thus faces the challenge of integrating over multiple time steps as a way to form a representation of the current state of the world.To resolve this issue, we modify the sequence of the autoregressive model by interleaving continuous I-tokens, that summarize successive world states with frame embeddings, and discrete ∆-tokens.</p>
<p>In the Crafter benchmark (Hafner, 2022), ∆-IRIS exhibits favorable scaling properties: the agent solves 17 out of 22 tasks after 10M frames of data collection, supersedes DreamerV3 (Hafner et al., 2023) at multiple frame budgets, and trains 10 times faster than IRIS.In addition, we include results in the sample-efficient setting with Atari games.Through experiments, we provide evidence that ∆-IRIS learns to disentangle the deterministic and stochastic aspects of world modelling.Moreover, we conduct ablations to validate the new conditioning schemes for the autoencoder and transformer models.</p>
<p>Method</p>
<p>We consider a Partially Observable Markov Decision Process (POMDP) (Sutton &amp; Barto, 2018) Learning in imagination (Sutton, 1991;Sutton &amp; Barto, 2018) consists of 3 stages that are repeated alternatively: experience collection, world model learning, and policy improvement.Strikingly, the agent learns behaviours purely within its world model, and real experience is only leveraged to learn the environment dynamics.</p>
<p>In the vein of IRIS (Micheli et al., 2023), our world model is composed of a discrete autoencoder (Van Den Oord et al., 2017) and an autoregressive transformer (Vaswani et al., 2017;Radford et al., 2019)
} K I → R h×w×3 .
This discrete autoencoder is trained with L 1 reconstruction, perceptual (Esser et al., 2021) and commitment losses (Van Den Oord et al., 2017) computed on collected frames.The transformer G I models the environment dynamics by operating over an input sequence of image and action tokens (z 1 0 , . . ., z K I 0 , a 0 , z 1 1 , . . ., z At a high level, the autoencoder builds a vocabulary of image tokens to encode each frame, and the transformer captures the environment dynamics by autoregressively composing the vocabulary over time.As a result, this world model is capable of attending to previous time steps to make its predictions, and models the joint law of future latent states.
x0 a 0 z 1 1 z 2 1 . . . z K−1 1 z K 1 x1 a 1 z 1 2 z 2 2 . . . z K−1 2 z K 2 x2 a 2 z 1 3 z 2 3 . . . z K−1 3 z K 3 ẑ1 1 ẑ2 1 . . . ẑK−1 1 ẑK 1 ẑ1 2 ẑ2 2 . . . ẑK</p>
<p>Disentangling deterministic and stochastic dynamics</p>
<p>IRIS (Micheli et al., 2023) encodes frames independently, making no assumption about temporal redundancy within trajectories.One major drawback of this general formulation is that, in environments with visually challenging frames, a large number of tokens is required to encode frames losslessly.Consequently, computations with the dynamics model become increasingly prohibitive, as the attention mechanism scales quadratically with sequence length.Therefore, limiting computation under such a trade-off may result in degraded performance (Micheli et al. (2023) app.E)</p>
<p>One possible solution to achieve fast world modelling with minimal loss is to condition the autoencoder on previous frames and actions.Intuitively, encoding a frame given previous frames consists in describing what has changed, the delta, between successive time steps.In many environments, the delta between frames is often much simpler to describe than the frames themselves.As a matter of fact, when the transition function is deterministic, adding previous actions to the conditioning of the decoder results in a world model, without the need to encode any information between time steps.However, most environments of interest feature stochastic dynamics, and apart from aleatoric uncertainty, architectural limitations such as the agent's memory may induce additional epistemic uncertainty.Hence, the delta between two time steps usually consists of deterministic and stochastic components.</p>
<p>For instance, an agent moving from one square to another in a grid-like environment when pressing movement keys can be seen as a deterministic component of the transition.</p>
<p>On the other hand, the sudden apparition of an enemy in a nearby square is a random event.Interestingly, only the stochastic features of a transition should be encoded, and the autoencoder could directly learn to model the deterministic dynamics, which do not require the expressivity and ability to handle multimodality of an autoregressive model.Therefore, when autoenconding frames by conditioning on previous frames and actions, a frame encoding may only consist of a handful of ∆-tokens, instead of a large number of image tokens describing frames independently.</p>
<p>Section 3.4 provides empirical evidence that ∆-IRIS' autoencoder learns to encode frames in such fashion, and Figure 1 illustrates the new conditioning scheme of the autoencoder.</p>
<p>∆-tokens sampled randomly</p>
<p>∆-tokens sampled by the autoregressive transformer For instance, the agent successfully cuts down a tree between t = 4 and t = 5, and uses wood planks to build a crafting table between t = 10 and t = 12.We observe that these dynamics are modelled in the same way whether ∆-tokens are sampled randomly or not.However, in the top trajectory, large quantities of cows appear and disappear from the screen incoherently, whereas the bottom trajectory does not display such erratic patterns.This experiment shows that ∆-IRIS encodes stochastic deltas between time steps with ∆-tokens, and its decoder handles the deterministic aspects of world modelling.Appendix F contains additional examples.
t = 0 t = 4 t = 5 t = 9 t = 10 t = 12
More formally, for any set Y, we denote S n (Y) = n i=1 Y i the set of tuples of elements from Y of maximum length n, and S(Y) = S ∞ (Y).Let Z = {1, . . ., N } a vocabulary of discrete tokens.Given past images and actions (x 0 , a 0 , . . ., x t−1 , a t−1 ), the encoder E : S(X ×A)×X → Z K converts an image x t into z t = (z 1 t , . . ., z K t ), a sequence of K discrete ∆-tokens.The encoder is parameterized by a Convolutional Neural Network (CNN) (Le-Cun et al., 1989).Actions are embedded with a learnt lookup table and concatenated channel-wise with frames.We use vector quantization (Van Den Oord et al., 2017;Esser et al., 2021) with factorized and normalized codes (Yu et al., 2021) to discretize the encoder's continuous outputs.The CNN decoder D : S(X × A) × Z K → X reconstructs an image xt from past frames, actions and ∆tokens (x 0 , a 0 , . . ., x t−1 , a t−1 , z t ).Action and ∆-tokens are embedded with learnt lookup tables, and concatenated channel-wise with feature maps obtained by forwarding frames through an auxiliary CNN.</p>
<p>The discrete autoencoder is trained on previously collected trajectories with a weighted combination of L 1 , L 2 and max-pixel (Anand et al., 2022) reconstruction losses, as well as a commitment loss (Van Den Oord et al., 2017).The codebook is updated with an exponential moving average (Razavi et al., 2019) and we use a straight-through estimator (Bengio et al., 2013) to enable backpropagation.</p>
<p>Modelling stochastic dynamics</p>
<p>While it should be possible to predict future ∆-tokens, given a starting image, past actions and ∆-tokens, we found this task much more difficult than simply predicting future image tokens, given past image tokens and actions, as in IRIS.</p>
<p>To better understand why this is the case, let us consider another example: in a grid environment, ∆-tokens may describe the unpredictable movement of an enemy, randomly jumping from one square to another at every time step.Based on the initial enemy location and after only a few time steps, it becomes increasingly difficult to predict if the enemy and the agent are located on the same square, which could trigger a battle and make the enemy disappear.Indeed, situating the two entities involves reasoning about the initial observation, and integrating over all of the previous action and ∆-tokens, which may have a complex dependence structure.</p>
<p>To address this problem, we alter the sequence of the dynamics model by interleaving continuous I-tokens, in reference to MPEG's I-frames (Richardson, 2004), and discrete ∆tokens.I-tokens alleviate the need of integrating over past ∆-tokens to form a representation of the current state of the world, i.e. they deploy a "soft" Markov blanket for the prediction of the next ∆-tokens.</p>
<p>Table 1.Returns, number of parameters, and frames collected per second (FPS) for the methods considered.We compute FPS as the total number of environment frames collected divided by the training duration.∆-IRIS outperforms DreamerV3 for larger frame budgets, and is 10x faster than IRIS (64 tokens).</p>
<p>Method</p>
<p>Return @1M Return @5M Return @10M #Parameters FPS We obtain I-tokens by forwarding frames through an auxiliary CNN at each time step.They are not produced by a discrete autoencoder.Since I-tokens are not predicted by the model but rather enrich its conditioning, there are no incentives to include a lossy discretization operator or to optimize a reconstruction loss.Instead, they are optimized end-toend with the learning objectives of the dynamics model.With this improved conditioning, the dynamics model perceives the ongoing trajectory with a mixture of continuous and discrete representations, while making its predictions autoregressively in a discrete space.</p>
<p>Figure 2 displays the input sequence of the dynamics model and the quantities it predicts.Given a sequence of past I-tokens, action tokens, and ∆-tokens (x 0 , a 0 , z G is parameterized by a stack of transformer encoder layers with causal self-attention (Vaswani et al., 2017;Radford et al., 2019).It is trained with a cross-entropy loss for transition and termination predictions, and we follow DreamerV3 (Hafner et al., 2023) in using discrete regression with twohot targets and symlog scaling for reward prediction (Imani &amp; White, 2018).</p>
<p>Policy improvement</p>
<p>During the policy improvement phase, the policy π learns in the imagination POMDP of its world model, composed of the autoencoder (E, D) and the dynamics model G.</p>
<p>At time step t, the policy observes a reconstructed image observation xt and samples action a t ∼ π(a t |x ≤t ).The world model then predicts the reward rt , the episode end dt , and the next observation xt+1 = D(x ≤t , â≤t , ẑ≤t , ẑt+1 ), with ẑt+1 ∼ p G (ẑ t+1 |x ≤t , â≤t , ẑ≤t ).The imagination procedure is initialized with a real observation x 0 sampled from past experience, and is rolled out for H steps.The procedure stops if an episode termination is predicted before reaching the imagination horizon.</p>
<p>We employ to a large extent the actor-critic training method used for IRIS (Micheli et al., 2023).A value baseline is trained to predict λ-returns (Sutton &amp; Barto, 2018) with the same discrete regression objective as for reward prediction.The policy optimizes the REINFORCE with value baseline (Sutton &amp; Barto, 2018) learning objective over imagined trajectories.Exploration is encouraged by adding an entropy maximization term to the policy's objective.</p>
<p>Experiments</p>
<p>In our experiments, we consider the Crafter benchmark (Hafner, 2022) to illustrate ∆-IRIS' ability to scale to a visually rich environment with large frame budgets.Besides, we also include Atari 100k games (Bellemare et al., 2013;Kaiser et al., 2020) in Appendix C to showcase the performance and speed of our agent in the sample-efficient setting.We introduce the Crafter benchmark and baselines in Section 3.1.Then, we present our results in Section 3.2.Finally, in Sections 3.3 and 3.4, we propose qualitative experiments to validate ∆-IRIS' world model architecture, and better our understanding of how the model represents information.</p>
<p>∆-IRIS 4 tokens IRIS 16 tokens</p>
<p>Benchmark and baselines</p>
<p>Crafter (Hafner, 2022) is a procedurally generated environment, inspired by the video game Minecraft, with visual inputs, a discrete action space and non-deterministic dynamics.By incorporating mechanics from survival games and a technology tree, this benchmark evaluates a broad range of agent capabilities such as generalization, exploration, and credit assignment.During each episode, the agent's goal is to solve as many tasks as possible, e.g.slaying mobs, crafting items, and managing health indicators.</p>
<p>Regarding baselines, we consider two model-based RL agents learning in imagination: IRIS (Micheli et al., 2023) and DreamerV3 (Hafner et al., 2023).We run several variants: IRIS (16 tokens), encoding frames with K I = 16 tokens, IRIS (64 tokens), encoding frames with K I = 64 tokens, and configurations of DreamerV3 of different sizes, namely DreamerV3 XL and DreamerV3 M. To demonstrate the importance of I-tokens, we also run ∆-IRIS without I-tokens in the sequence of the transformer, i.e.G only operates over the first frame as well as actions and ∆-tokens.</p>
<p>We keep a fixed imagined-to-collected data ratio of 64 to balance speed and performance.Our experiments run on a Nvidia A100 40GB GPU, with 5 seeds for all methods and ablations.We evaluate each run by computing the average return over 256 test episodes every 1M frames.Note that we stop the IRIS experiments before 10M frames because they are prohibitively slow.</p>
<p>Results</p>
<p>Table 1 exhibits key metrics and Figure 4 displays learning curves.After 10M frames of data collection, ∆-IRIS solves on average 17 out of 22 tasks, setting a new state of the art for the Crafter benchmark.Beyond the 3M frames mark, ∆-IRIS consistently achieves higher returns than Dream-erV3, although DreamerV3 is better suited for the smallest frame budgets.A key difference between the two methods is that ∆-IRIS does not leverage the representations of its world model for policy learning, which may be especially useful in the scarce data regime.As our main objective is to develop world model architectures that scale to complex environments and larger frame budgets, we leave this exploration to future work.∆-IRIS outperforms IRIS for all frame budgets considered, while training an order of magnitude faster.Finally, removing I-tokens from the sequence of the dynamics model drastically hurts performance.</p>
<p>Autoregressive transformer with I-tokens</p>
<p>Autoregressive transformer without I-tokens Figure 6.Trajectories imagined with (top) and without (bottom) I-tokens.In the top trajectory, we observe more than 30 seconds of gameplay generated by ∆-IRIS' world model.A wide variety of mechanics have been internalized: scrolling, chopping down trees, building a crafting table, mining iron, crafting pickaxes, etc.However, removing I-tokens from the sequence of the autoregressive transformer makes the task of predicting future ∆-tokens drastically harder as evidenced by the agent glitching through walls and water in the bottom trajectory.These mistakes ultimately hinder the policy improvement phase, since the agent will reinforce behaviours in a world that does not properly reflect its environment.</p>
<p>We believe that achieving higher returns at the 10M frames cap poses a hard exploration problem.Indeed, three of the missing four tasks require crafting new tools in the presence of a nearby crafting table and furnace.Discovering these tools with a naive exploration strategy is highly unlikely, and we have observed only a few occurrences of those events throughout training runs.</p>
<p>With too few training samples, the world model is unable to internalize these new mechanics and reflect them during the imagination procedure.We hypothesize that a biased data sampling procedure (Kauvar et al., 2023) could be the key to unlock the missing achievements.</p>
<p>World model analysis</p>
<p>In Section 3.2, we validated our design choices for ∆-IRIS with RL experiments.However, downstream RL performance is an imperfect proxy for the quality of a world model due to many possible confounding factors, e.g. the choice of the RL algorithm, entangled world model and policy architectures, or the continual learning loop.In this section, we directly focus on the abilities of the world model.</p>
<p>Figure 5 illustrates the bottom 1% autoencoded test frames with and without conditioning the autoencoder on the ongoing trajectory (i.e.reconstructions with ∆-IRIS vs IRIS).With as few as 4 tokens per frame, ∆-IRIS' autoencoder is able to encode frames with minimal loss.On the other hand, without access to previous frames and actions, and even with 16 tokens, IRIS' autoencoder produces poor reconstructions.</p>
<p>Figure 6 displays trajectories imagined to illustrate whether crucial mechanics have been internalized by the world model, when including I-tokens in the sequence of the autoregressive transformer or not.We observe that, with Itokens, a multitude of game mechanics are well understood, but in the absence of I-tokens the world model is unable to simulate key concepts.Appendix B includes additional quantitative results.</p>
<p>Evidence of dynamics disentanglement</p>
<p>In Section 2.2, we argued that, by design, ∆-IRIS' encoder describes stochastic deltas between timesteps with ∆-tokens.In the present section, we propose to exhibit this phenomenon.</p>
<p>We pick a starting frame and a sequence of actions, and predict two different trajectories with the world model.In one case, we sample future ∆-tokens randomly.In the other case, ∆-tokens are produced by the autoregressive transformer.We consider a scenario where the agent collects wood then builds a crafting table in Figure 3. Appendix F displays two other scenarios where the agent explores its surroundings, and where it moves down then stands still.</p>
<p>We observe that, even when sampling ∆-tokens randomly, the deterministic aspects of the dynamics are properly modelled: grid layout, agent movement, wood level increasing, crafting table appearing, etc.On the other hand, stochastic dynamics become problematic: skeletons and cows appearing and disappearing, food and water indicators decreasing too early, unlikely quantities of enemies and objects, etc.These observations confirm that ∆-IRIS encodes stochastic deltas between time steps with ∆-tokens, and its decoder handles the deterministic aspects of world modelling.</p>
<p>Related Work World Models and imagination</p>
<p>With Dyna, Sutton (1991) introduced the idea of learning behaviours in the imagination of a world model.Ha &amp; Schmidhuber (2018) went beyond the tabular setting and proposed a new world model architecture, composed of a variational autoencoder (Kingma &amp; Welling, 2013) and a recurrent network (Hochreiter &amp; Schmidhuber, 1997;Gers et al., 2000), capable of simulating simple visual environments.Following this breakthrough, multiple generations of Dreamer agents (Hafner et al., 2020;2021;2023) were developed, with DreamerV2 being the first imagination-based agent to outperform humans in Atari games, and Dream-erV3 being the first world model architecture applicable to a wide range of domains without any specific tuning.Dream-erV2 learns in the imagination of a world model combining a convolutional autoencoder with a recurrent state-space model (RSSM) (Hafner et al., 2019).The key modifications that enabled DreamerV2 to improve over the original Dreamer agent were categorical latents and KL balancing between prior and posterior estimates.DreamerV3 builds upon DreamerV2 with more universal design choices such as symlog scaling of rewards and values, combining free bits (Kingma et al., 2016) with KL balancing, return scaling for static entropy regularization, and architectural novelties for model scaling.Variants of Dreamer such as TransDreamer (Chen et al., 2022) and STORM (Zhang et al., 2023) have also been explored, where transformers replace the recurrent network in the RSSM for dynamics prediction.</p>
<p>A potential limitation of RSSM-like architectures is that they do not model the joint distribution of future latent states, and instead predict product laws.One way to mitigate this discrepancy between the predicted distributions and the distributions of interest is to encourage factorized distributions (Hafner et al., 2023).On the other hand, autoregressive architectures (Micheli et al., 2023) do model the joint distribution and do not require to enforce independence, which may result in a more expressive model.</p>
<p>Trajectory and video autoencoders</p>
<p>The idea of encoding frames with respect to past frames predates modern deep learning, and is at the origin of efficient video compression algorithms, such as MPEG (Richardson, 2004).In recent years, multiple works have implemented variants of this approach.Ozair et al. (2021) propose an offline version of MuZero (Schrittwieser et al., 2020) equipped with an autoregressive transformer that performs search over trajectory-level discrete latent variables and actions.Phenaki (Villegas et al., 2022) is a text-to-video model composed of a spatio-temporal discrete autoencoder and a masked bidirectional transformer.TECO (Yan et al., 2022) is an action-conditional video prediction model composed of a discrete frame autoencoder conditioned on the previous frame, a temporal autoregressive transformer, and a spatial MaskGit (Chang et al., 2022).While these methods also encode frames by conditioning on past frames, their dynamics models purely operate over discrete tokens, and do not leverage continuous tokens to alleviate the need to integrate over multiple time steps in order to make the next prediction.Hafner et al. (2019) acknowledge that modelling stochastic dynamics may be difficult, as it would involve remembering information from previous time steps.The authors propose to solve this problem by carrying a "deterministic" state over time via a recurrent network, at the core of their RSSM.We make a similar observation, and further show that this task is still difficult even when past information does not have to be carried by a recurrent state, as a transformer can attend to all previous ∆-tokens.Hence, it is not only a memory problem, but also a modelling one.Here, we address this issue in a manner that is compatible with autoregressive transformers, namely by injecting continuous I-tokens in the sequence of the dynamics model.</p>
<p>Conclusion</p>
<p>We introduced ∆-IRIS, a new model-based agent relying on an efficient world model architecture to simulate its environment and learn new behaviours.∆-IRIS features a discrete autoencoder that encodes the stochastic aspects of world modelling with discrete ∆-tokens, and an autoregressive transformer leveraging continuous I-tokens to model stochastic dynamics.</p>
<p>Through experiments, we showed the ability of our agent to scale to the challenging Crafter benchmark, as well as its sample efficiency in Atari100k.Finally, we illustrated how its world model internalized environment dynamics, and conducted ablations to validate our proposed design choices.</p>
<p>In its current form, ∆-IRIS uses the same number of tokens to encode stochastic dynamics at each time step.However, the reality of most environments is such that periods of low uncertainty are quickly followed by moments of high randomness.Therefore, an improved version of the world model could possibly predict dynamically various numbers of tokens based on the current context.Besides, leveraging the internal representations of the world model could potentially result in a lightweight and more robust policy.</p>
<p>Impact Statement</p>
<p>The deployment of autonomous agents in real-world applications raises safety concerns.Agents learning new behaviours may harm individuals and damage property.With world models, we lower the amount of time spent interacting with the real world and thus mitigate risks.In this work, we propose a world model architecture that is amenable to scaling up to complex environments, where accurate simulations are even more critical given the usually higher stakes.</p>
<p>A.3. Actor-Critic</p>
<p>We tie the weights of the actor and critic, except for the last layer.The actor-critic takes as input a frame, and forwards it through a convolutional neural network (LeCun et al., 1989) followed by an LSTM cell (Hochreiter &amp; Schmidhuber, 1997;Gers et al., 2000;Mnih et al., 2016).For the CNN, we use the same architecture as the encoder, except that we halve the number of channels per layer.The dimension of the LSTM hidden state is 512.</p>
<p>Before starting the imagination procedure (H = 15) from a given frame, we burn-in (Kapturowski et al., 2019) the 5 previous frames to initialize the hidden state.The discount factor γ is 0.997, the parameter for λ-returns is set to 0.95, and the coefficient for the entropy maximization term is 0.001.Targets for value estimates are produced by a moving average of the critic network, with update parameter 0.995 (Mnih et al., 2015) A.4. Training loop and shared hyperparameters As mentioned in Section 2, the world model and policy are trained with temporal segments sampled from past experience.We use a count-based sampling procedure over the entire history of episodes, i.e. the likelihood that a given episode is chosen to produce the next sample is inversely proportional to the number of times it was previously used.We raise inverse counts to the power of 5 to further limit the bias towards older episodes.</p>
<p>B. Impact of design choices on key world modelling metrics</p>
<p>Metrics are computed on a held-out test set after training various world models on a dataset consisting of 10M frames collected by a ∆-IRIS agent throughout its training.</p>
<p>E. Baselines</p>
<p>DreamerV3 results were obtained with commit 8fa35f8.We used the standard configuration for Crafter, and set the run.trainratio variable controlling the imagined-to-collected data ratio to 64.Note that a new version of DreamerV3 was recently released in April 2024.This update includes additional and broadly applicable novelties for world model and policy learning.</p>
<p>IRIS results were obtained with commit ac6be40.For the training loop and shared hyperparameters, we picked the same values as in Table 5.We increased the dimension and attention heads of the transformer from 256 and 4 to 512 and 8, respectively.Finally, we used a replay buffer with a capacity of 1M frames.</p>
<p>Figure 3 .
3
Figure3.Evidence of dynamics disentanglement.Two trajectories are imagined with different ways of generating ∆-tokens.In the top trajectory, ∆-tokens are sampled randomly.In the bottom trajectory, the autoregressive transformer predicts future ∆-tokens.The same starting frame (t = 0) and sequence of actions are used.With random ∆-tokens, the deterministic aspects of the dynamics (layout, movement, items, crafting) are still properly modelled, but the stochastic dynamics (mobs, health indicators) become problematic.For instance, the agent successfully cuts down a tree between t = 4 and t = 5, and uses wood planks to build a crafting table between t = 10 and t = 12.We observe that these dynamics are modelled in the same way whether ∆-tokens are sampled randomly or not.However, in the top trajectory, large quantities of cows appear and disappear from the screen incoherently, whereas the bottom trajectory does not display such erratic patterns.This experiment shows that ∆-IRIS encodes stochastic deltas between time steps with ∆-tokens, and its decoder handles the deterministic aspects of world modelling.Appendix F contains additional examples.</p>
<p>Figure 5 .
5
Figure 5. Bottom 1% test frames autoencoded by ∆-IRIS (4 tokens) and IRIS(Micheli et al., 2023) (16 tokens).Each token takes a value in {1, 2, . . ., 1023, 1024}, i.e. ∆-IRIS encodes frames with 4 × log 2 (1024) = 40 bits while IRIS uses 160 bits.Original frames, reconstructions, and errors are respectively displayed in the top, middle, and bottom rows.Even in the worst instances, ∆-IRIS makes only minor errors, whereas IRIS fails to accurately reconstruct frames.These errors severely hamper the agent's performance, as it purely learns behaviours from frames generated by its autoencoder.</p>
<p>Figure 7 .Figure 8 .
78
Figure 7. Individual success rates after collecting 1M frames.</p>
<p>. The transition, reward, and episode termination dynamics are captured by the conditional distributions p(x t+1 | x ≤t , a</p>
<p>≤t ) and p(r t , d t | x ≤t , a ≤t ), where x t ∈ X = R 3×h×w is an image observation, a t ∈ A = {1, . . ., A} a discrete action, r t ∈ R a scalar reward, and d t ∈ {0, 1} indicates episode termination.The reinforcement learning objective is to find a policy p π (a t | x ≤t , a &lt;t ) that maximizes the expected sum of rewards E π [ t≥0 γ t r t ], with discount factor γ ∈ (0, 1).</p>
<p>Figure2.Unrolling dynamics over time.At each time step (separated by dashed lines), the GPT-like autoregressive transformer G predicts the ∆-tokens for the next frame, as well as the reward and a potential episode termination.Its input sequence consists of action tokens, ∆-tokens, and I-tokens, namely continuous image embeddings that alleviate the need to attend to past ∆-tokens for world modelling.More specifically, an initial frame x0 is embedded into I-token x0.From x0 and a0, G predicts the reward r0, episode termination d0 ∈ {0, 1}, and in an autoregressive manner ẑ1 = (ẑ 1 1 , . . ., ẑK 1 ), the ∆-tokens for the next frame.Note that, during the imagination procedure, the next frame (stripped box) is computed by the decoder D based on previous frames, actions, and the ∆-tokens generated by G, i.e. x1 = D(x0, a0, ẑ1).
d0d1d2r0r1r22−1ẑK 2ẑ1 3ẑ2 3. . . ẑK−1 3ẑK 3x 0x 1x 2</p>
<p>I (r t |z ≤t , a ≤t ), and p G I ( dt |z ≤t , a ≤t ).
K I 1 , a 1 , . . . , z 1 t , . . . , z K I t , a t ).Image and action tokens are embedded with learntlookup tables.At each time step, G I predictsthe transition, reward, and termination distributions:p G I (ẑ t+1 |z ≤t , a ≤t ) with ẑk t+1 ∼ p G I (ẑ k t+1 |z ≤t , a ≤t , z &lt;k t+1 ),p G The model istrained with a cross-entropy loss on segments sampled frompast experience.</p>
<p>It also predicts distributions for rewards p G (r t |x ≤t z ≤t , a ≤t ) and episode terminations p G ( dt |x ≤t , z ≤t , a ≤t ).
1 1 , . . . , z K 1 , . . . , xt−1 , a t−1 , z 1 t , . . . , z k t ), the dy-namics model G outputs a categorical distribution on Zfor the next ∆-token ẑk+1
t ∼ p G (ẑ k+1 t |x &lt;t , z &lt;t , a &lt;t , z ≤k t ).</p>
<p>Table 5 .
5
Training loop and shared hyperparameters.
HyperparameterValueHyperparameterValueEpochs1000Autoencoder batch size32Environment steps first epoch 100000Transformer batch size32Environment steps per epoch10000Actor-critic batch size86# Collection epochs990Learning rate1e-4Collection epsilon-greedy0.01OptimizerAdamTraining steps per epoch500Max gradient norm10.0</p>
<p>Table 6 .
6
Left: Impact of removing past frames and actions from the conditioning of the autoencoder (∆-IRIS → IRIS).Right: Impact of removing I-tokens from the conditioning of the autoregressive transformer.
MethodL 2 lossMethodNext token loss (CE) Reward loss (CE)∆-IRIS (4 tokens) IRIS (64 tokens) 0.001715 0.000185 IRIS (16 tokens) 0.007496∆-IRIS ∆-IRIS w/o I-tokens1.57 1.730.108 0.135</p>
<p>Table 7 .
7
Impact of discarding the auxiliary max-pixel loss.
MethodL 2 lossMax-pixel loss∆-IRIS0.0001850.018∆-IRIS w/o max-pixel loss 0.0001780.031</p>
<p>Table 9 .
9
Crafter scores, i.e. geometric mean of success rates.
MethodCrafter score @1M Crafter score @5M Crafter score @10M∆-IRIS9.3039.6742.47∆-IRIS w/o I-tokens5.8514.3925.92IRIS (64 tokens)6.66--
AcknowledgementsWe would like to thank Adam Jelley, Bálint Máté, Daniele Paliotta, Maxim Peter, Youssef Saied, Atul Sinha, and Alessandro Sordoni for insightful discussions and comments.Vincent Micheli was supported by the Swiss National Science Foundation under grant number FNS-187494.A. Architectures and hyperparametersA.1.Discrete autoencoder  In early experiments, we used a transformer instead of a CNN for the architecture of the autoencoder.It had a much longer context size of twenty time steps.Although the transformer-based autoencoder performed better than its CNN counterpart on static datasets, we observed that the CNN would learn faster than the transformer in the continual learning setup.Besides, for the sake of simplicity, we decreased the initial conditioning of the CNN autoencoder from four time steps to one time step, as the slight increase in reconstruction losses did not significantly hinder agent performance.These observations are largely environment-dependent, thus the context size or the architecture of the autoencoder should most likely be adapted accordingly.A.2. Autoregressive transformerC. Atari 100kThe Atari 100k benchmark(Kaiser et al., 2020)features Atari games(Bellemare et al., 2013)with diverse mechanics.The specificity of this benchmark is the hard constraint on the number of interactions, namely one hundred thousand per environment.Compared to the standard Atari benchmark, this constraint results in a dramatic drop in real-time experience, from 900 hours to 2 hours.Regarding baselines, we consider four model-based RL agents learning in imagination: SimPLe(Kaiser et al., 2020), DreamerV3(Hafner et al., 2023), STORM(Zhang et al., 2023), and IRIS(Micheli et al., 2023).We note that the current best performing methods for Atari 100k resort to other approaches, such as lookahead search for EfficientZero(Ye et al., 2021), or self-supervised representation learning with periodic resets for BBF(Schwarzer et al., 2023).The usual metric of interest is the HNS, the human-normalized score, based on the performance of human players with similar experience.A negative HNS indicates worse than random performance whereas an HNS above 1 signifies superhuman performance.We evaluate ∆-IRIS by computing an average over 100 episodes collected at the end of training for each game (5 seeds).For the baselines, we report the published results.Table8displays returns across games and aggregate metrics(Agarwal et al., 2021).∆-IRIS achieves higher aggregate metrics than IRIS, while training in 26 hours, a 5-fold speedup.
. J Achiam, S Adler, S Agarwal, L Ahmad, I Akkaya, F L Aleman, D Almeida, J Altenschmidt, S Altman, S Anadkat, arXiv:2303.087742023arXiv preprint</p>
<p>Deep reinforcement learning at the edge of the statistical precipice. R Agarwal, M Schwarzer, P S Castro, A C Courville, M Bellemare, Advances in neural information processing systems. 202134</p>
<p>Procedural generalization by planning with self-supervised world models. A Anand, J C Walker, Y Li, E Vértes, J Schrittwieser, S Ozair, T Weber, J B Hamrick, International Conference on Learning Representations. 2022</p>
<p>The arcade learning environment: An evaluation platform for general agents. M G Bellemare, Y Naddaf, J Veness, M Bowling, Journal of Artificial Intelligence Research. 472013</p>
<p>Estimating or propagating gradients through stochastic neurons for conditional computation. Y Bengio, N Léonard, A Courville, arXiv:1308.34322013arXiv preprint</p>
<p>Masked generative image transformer. H Chang, H Zhang, L Jiang, C Liu, W T Freeman, Maskgit, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>
<p>C Chen, Y.-F Wu, J Yoon, S Ahn, Transdreamer, arXiv:2202.09481Reinforcement learning with transformer world models. Ai Comma, Commavq, 2022. 2023arXiv preprint</p>
<p>Sample-efficient reinforcement learning by breaking the replay ratio barrier. P D'oro, M Schwarzer, E Nikishin, P.-L Bacon, M G Bellemare, A Courville, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Taming transformers for high-resolution image synthesis. P Esser, R Rombach, B Ommer, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2021</p>
<p>Learning to forget: Continual prediction with LSTM. F A Gers, J Schmidhuber, F Cummins, Neural Computation. 12102000</p>
<p>Recurrent world models facilitate policy evolution. D Ha, J Schmidhuber, Advances in neural information processing systems. 201831</p>
<p>Benchmarking the spectrum of agent capabilities. D Hafner, International Conference on Learning Representations. 2022</p>
<p>Learning latent dynamics for planning from pixels. D Hafner, T Lillicrap, I Fischer, R Villegas, D Ha, H Lee, J Davidson, International conference on machine learning. PMLR2019</p>
<p>Dream to control: Learning behaviors by latent imagination. D Hafner, T Lillicrap, J Ba, M Norouzi, International Conference on Learning Representations. 2020</p>
<p>Mastering atari with discrete world models. D Hafner, T P Lillicrap, M Norouzi, J Ba, International Conference on Learning Representations. 2021</p>
<p>Mastering diverse domains through world models. D Hafner, J Pasukonis, J Ba, T Lillicrap, arXiv:2301.04104v12023arXiv preprint</p>
<p>Long short-term memory. S Hochreiter, J Schmidhuber, Neural Computation. 981997</p>
<p>A Hu, L Russell, H Yeo, Z Murez, G Fedoseev, A Kendall, J Shotton, G Corrado, arXiv:2309.17080Gaia-1: A generative world model for autonomous driving. 2023arXiv preprint</p>
<p>Improving regression performance with distributional losses. E Imani, M White, International conference on machine learning. PMLR2018</p>
<p>Model based reinforcement learning for atari. Ł Kaiser, M Babaeizadeh, P Miłos, B Osiński, R H Campbell, K Czechowski, D Erhan, C Finn, P Kozakowski, S Levine, International Conference on Learning Representations. 2020</p>
<p>Minerl diamond 2021 competition: Overview, results, and lessons learned. A Kanervisto, S Milani, K Ramanauskas, N Topin, Z Lin, J Li, J Shi, D Ye, Q Fu, W Yang, W Hong, Z Huang, H Chen, G Zeng, Y Lin, V Micheli, E Alonso, F Fleuret, A Nikulin, Y Belousov, O Svidchenko, A Shpilman, Proceedings of the NeurIPS 2021 Competitions and Demonstrations Track, Proceedings of Machine Learning Research. the NeurIPS 2021 Competitions and Demonstrations Track, Machine Learning Research2022</p>
<p>Recurrent experience replay in distributed reinforcement learning. S Kapturowski, G Ostrovski, J Quan, R Munos, W Dabney, International conference on learning representations. 2019</p>
<p>Curious replay for model-based adaptation. I Kauvar, C Doyle, L Zhou, N Haber, International Conference on Machine Learning. 2023</p>
<p>D P Kingma, M Welling, arXiv:1312.6114Auto-encoding variational bayes. 2013arXiv preprint</p>
<p>Improved variational inference with inverse autoregressive flow. D P Kingma, T Salimans, R Jozefowicz, X Chen, I Sutskever, M Welling, Advances in neural information processing systems. 292016</p>
<p>A path towards autonomous machine intelligence version 0.9. 2, 2022-06-27. Y Lecun, Open Review. 622022</p>
<p>Backpropagation applied to handwritten zip code recognition. Y Lecun, B Boser, J S Denker, D Henderson, R E Howard, W Hubbard, L D Jackel, Neural computation. 141989</p>
<p>Transformers are sample-efficient world models. V Micheli, E Alonso, F Fleuret, International Conference on Learning Representations. 2023</p>
<p>Human-level control through deep reinforcement learning. V Mnih, K Kavukcuoglu, D Silver, A A Rusu, J Veness, M G Bellemare, A Graves, M Riedmiller, A K Fidjeland, G Ostrovski, Nature. 51875402015</p>
<p>Asynchronous methods for deep reinforcement learning. V Mnih, A P Badia, M Mirza, A Graves, T Lillicrap, T Harley, D Silver, K Kavukcuoglu, International conference on machine learning. PMLR2016</p>
<p>Vector quantized models for planning. S Ozair, Y Li, A Razavi, I Antonoglou, A Van Den Oord, O Vinyals, International Conference on Machine Learning. PMLR2021</p>
<p>Curiosity-driven exploration by self-supervised prediction. D Pathak, P Agrawal, A A Efros, T Darrell, International conference on machine learning. PMLR2017</p>
<p>Language models are unsupervised multitask learners. A Radford, J Wu, R Child, D Luan, D Amodei, I Sutskever, 2019</p>
<p>Generating diverse high-fidelity images with vq-vae-2. Advances in neural information processing systems. A Razavi, A Van Den Oord, O Vinyals, 201932</p>
<p>264 and MPEG-4 video compression: video coding for next-generation multimedia. I E Richardson, 2004John Wiley &amp; Sons</p>
<p>High-resolution image synthesis with latent diffusion models. R Rombach, A Blattmann, D Lorenz, P Esser, B Ommer, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>
<p>Mastering atari, go, chess and shogi by planning with a learned model. J Schrittwieser, I Antonoglou, T Hubert, K Simonyan, L Sifre, S Schmitt, A Guez, E Lockhart, D Hassabis, T Graepel, T P Lillicrap, D Silver, Nature. 58878392020</p>
<p>Data-efficient reinforcement learning with self-predictive representations. M Schwarzer, A Anand, R Goel, R D Hjelm, A Courville, P Bachman, International Conference on Learning Representations. 2021</p>
<p>better, faster: Human-level atari with human-level efficiency. M Schwarzer, J S O Ceron, A Courville, M G Bellemare, R Agarwal, P S Castro, Bigger, International Conference on Machine Learning. 2023</p>
<p>Planning to explore via self-supervised world models. R Sekar, O Rybkin, K Daniilidis, P Abbeel, D Hafner, D Pathak, International Conference on Machine Learning. PMLR2020</p>
<p>an integrated architecture for learning, planning, and reacting. R S Sutton, Dyna, ACM Sigart Bulletin. 1991</p>
<p>Reinforcement Learning: An Introduction. R S Sutton, A G Barto, 2018A Bradford BookCambridge, MA, USA</p>
<p>Y Tassa, Y Doron, A Muldal, T Erez, Y Li, D D L Casas, D Budden, A Abdolmaleki, J Merel, A Lefrancq, arXiv:1801.00690Deepmind control suite. 2018arXiv preprint</p>
<p>Neural discrete representation learning. Advances in neural information processing systems. A Van Den Oord, O Vinyals, 201730</p>
<p>Advances in neural information processing systems. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, Ł Kaiser, I Polosukhin, 201730Attention is all you need</p>
<p>Phenaki: Variable length video generation from open domain textual description. R Villegas, M Babaeizadeh, P.-J Kindermans, H Moraldo, H Zhang, M T Saffar, S Castro, J Kunze, D Erhan, arXiv:2210.023992022arXiv preprint</p>
<p>Temporally consistent video transformer for long-term video prediction. W Yan, D Hafner, S James, P Abbeel, arXiv:2210.023962022arXiv preprint</p>
<p>Mastering atari games with limited data. W Ye, S Liu, T Kurutach, P Abbeel, Y Gao, Advances in neural information processing systems. 342021</p>
<p>Vector-quantized image modeling with improved vqgan. J Yu, X Li, J Y Koh, H Zhang, R Pang, J Qin, A Ku, Y Xu, J Baldridge, Y Wu, arXiv:2110.046272021arXiv preprint</p>
<p>Efficient stochastic transformer based world models for reinforcement learning. W Zhang, G Wang, J Sun, Y Yuan, G Huang, Storm, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>            </div>
        </div>

    </div>
</body>
</html>
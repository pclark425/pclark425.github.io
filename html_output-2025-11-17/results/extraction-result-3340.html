<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3340 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3340</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3340</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-77.html">extraction-schema-77</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving puzzle games that require spatial knowledge (such as Sudoku), including details about the models, the puzzles, the methods used, performance, and any analysis of how the models solve these tasks.</div>
                <p><strong>Paper ID:</strong> paper-304cf21da84961469ac9f43405df187441832b61</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/304cf21da84961469ac9f43405df187441832b61" target="_blank">NeuroLogic A*esque Decoding: Constrained Text Generation with Lookahead Heuristics</a></p>
                <p><strong>Paper Venue:</strong> North American Chapter of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> NeuroLogic A*esque is proposed, a decoding algorithm that incorporates heuristic estimates of future cost that develops lookahead heuristics that are efficient for large-scale language models, making this method a drop-in replacement for common techniques such as beam search and top-k sampling.</p>
                <p><strong>Paper Abstract:</strong> The dominant paradigm for neural text generation is left-to-right decoding from autoregressive language models. Constrained or controllable generation under complex lexical constraints, however, requires foresight to plan ahead feasible future paths. Drawing inspiration from the A^* search algorithm, we propose NeuroLogic A*esque, a decoding algorithm that incorporates heuristic estimates of future cost. We develop lookahead heuristics that are efficient for large-scale language models, making our method a drop-in replacement for common techniques such as beam search and top-k sampling. To enable constrained generation, we build on NeuroLogic decoding (Lu et al., 2021), combining its flexibility in incorporating logical constraints with A*esque estimates of future constraint satisfaction. Our approach outperforms competitive baselines on five generation tasks, and achieves new state-of-the-art performance on table-to-text generation, constrained machine translation, and keyword-constrained generation. The improvements are particularly notable on tasks that require complex constraint satisfaction or in few-shot or zero-shot settings. NeuroLogic A*esque illustrates the power of decoding for improving and enabling new capabilities of large-scale language models.</p>
                <p><strong>Cost:</strong> 0.006</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3340",
    "paper_id": "paper-304cf21da84961469ac9f43405df187441832b61",
    "extraction_schema_id": "extraction-schema-77",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.0061920000000000005,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>NEUROLOGIC A* ${ }^{\star}$ esque Decoding: Constrained Text Generation with Lookahead Heuristics</h1>
<p>Ximing Lu ${ }^{\ddagger \dagger}{ }^{\odot}$ Sean Welleck ${ }^{\ddagger \ddagger}$ Peter West ${ }^{\dagger}$<br>Liwei Jiang ${ }^{\ddagger \dagger}$ Jungo Kasai ${ }^{\ddagger \dagger}$ Daniel Khashabi ${ }^{\ddagger}$ Ronan Le Bras ${ }^{\ddagger}$<br>Lianhui Qin ${ }^{\dagger}$ Youngjae Yu ${ }^{\ddagger}$ Rowan Zellers ${ }^{\dagger}$ Noah A. Smith ${ }^{\dagger \ddagger}$ Yejin Choi ${ }^{\dagger \ddagger}$<br>${ }^{\ddagger}$ Allen Institute for Artificial Intelligence<br>${ }^{\dagger}$ Paul G. Allen School of Computer Science \&amp; Engineering, University of Washington</p>
<h4>Abstract</h4>
<p>The dominant paradigm for neural text generation is left-to-right decoding from autoregressive language models. Constrained or controllable generation under complex lexical constraints, however, requires foresight to plan ahead for feasible future paths.</p>
<p>Drawing inspiration from the $\mathrm{A}^{<em>}$ search algorithm, we propose NEUROLOGIC A</em> ${ }^{\star}$ esque, ${ }^{\dagger}$ a decoding algorithm that incorporates heuristic estimates of future cost. We develop lookahead heuristics that are efficient for large-scale language models, making our method a dropin replacement for common techniques such as beam search and top-k sampling. To enable constrained generation, we build on NeUROLOGIC decoding (Lu et al., 2021), combining its flexibility in incorporating logical constraints with $\mathrm{A}^{*}$ esque estimates of future constraint satisfaction.</p>
<p>Our approach outperforms competitive baselines on five generation tasks, and achieves new state-of-the-art performance on table-totext generation, constrained machine translation, and keyword-constrained generation. The improvements are particularly notable on tasks that require complex constraint satisfaction or in few-shot or zero-shot settings. NEUROLOGIC A* ${ }^{\star}$ esque illustrates the power of decoding for improving and enabling new capabilities of large-scale language models.</p>
<h2>1 Introduction</h2>
<p>The dominant paradigm for neural text generation is based on left-to-right decoding from autoregressive language models such as GPT-2/3 (Radford et al., 2019; Brown et al., 2020). Under this paradigm, common decoding techniques such as beam search or top-k/p sampling (Holtzman et al., 2020) determine which token to generate next based on what happened in the past, without explicitly looking ahead into the future. While</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: NEUROLOGIC ${ }^{\star}$ leverages lookahead heuristics to guide generations towards those that satisfy the given task-specific constraints. In this example from the CommonGen task, although summer is a more likely next word given the already-generated past, NEUROLOGIC ${ }^{\star}$ looks ahead to see that selecting winter results in a generation that incorporates unsatisfied constraint snow with a higher probability later on. Thus, winter is preferred despite being lower probability than summer.
this lack of foresight often suffices for open-ended text generation - where any coherent text can be acceptable - for constrained text generation, planning ahead is crucial for incorporating all desired content in the generated output (Hu et al., 2017; Dathathri et al., 2019).</p>
<p>Classical search algorithms such as A<em> search (Hart et al., 1968; Pearl, 1984; Korf, 1985) address the challenge of planning ahead by using heuristic estimation of future cost when making decisions. Drawing inspiration from A</em> search, we develop NEUROLOGIC A<em> ${ }^{\star}$ esque (shortened to NEUROLOGIC ${ }^{\star}$ ), which combines A</em>-like heuristic estimates of future cost (e.g., perplexity, constraint satisfaction) with common decoding algorithms for neural text generation (e.g., beam search, top- $k$ sampling), while preserving the efficiency demanded by large-scale neural language models.</p>
<p>As selecting the next token to generate based on the optimal future cost is NP-complete (Chen et al., 2018), we develop lookahead heuristics, which approximate cost at each decoding step based on con-</p>
<p>tinuations of the sequence-so-far. Figure 1 shows an example, where NEUROLOGIC A* ${ }^{\star}$ esque guides generation towards a decision that would have been ignored based on the past alone, but is selected after looking ahead and incorporating the probability that constraints are satisfied in the future.</p>
<p>Our approach builds on NEUROLOGIC Decoding of Lu et al. (2021), a variation of beam-search for controlling generation through rich logic-based lexical constraints expressed in Conjunctive Normal Form (CNF). Our work generalizes Lu et al. (2021) by (1) incorporating novel lookahead heuristics to estimate future contraint satisfaction, and (2) developing additional unconstrained variants that can work with an empty set of constraints. These new algorithm variants support broad applications of NEUROLOGIC ${ }^{\star}$, including unconstrained generation, as demonstrated in our experiments.</p>
<p>Our experiments across five generation tasks demonstrate that our approach outperforms competitive baselines. We test NEUROLOGIC ${ }^{\star}$ in conjunction with both supervised and unsupervised models and find that the performance gain is pronounced especially in zero-shot or few-shot settings. On the CommonGen benchmark, using NEUROLOGIC ${ }^{\star}$ with an off-the-shelf language model outperforms a host of supervised baselines with conventional decoding algorithms, demonstrating that a strong inference-time algorithm such as NEUROLOGIC ${ }^{\star}$ can alleviate the need for costly annotated datasets. Moreover, NEUROLOGIC ${ }^{\star}$ achieves state-of-the-art performance in various settings, including WMT17 English-German machine translation with lexical constraints (Dinu et al., 2019) and few-shot E2ENLG table-to-text generation (Chen et al., 2020b).</p>
<p>In summary, we develop NEUROLOGIC $\mathrm{A}^{\star}$ esque, a new decoding algorithm for effective and efficient text generation. To our knowledge this is the first $\mathrm{A}^{*}$-like algorithm for guided text generation via lookahead heuristics. Our algorithm is versatile, as it can be applied to a variety of tasks via inference-time constraints, reducing the need for costly labeled data. Extensive experiments show its effectiveness on several important generation benchmarks.</p>
<h2>2 NEUROLOGIC A* ${ }^{\star}$ esque Decoding</h2>
<p>We describe NEUROLOGIC A<em> ${ }^{\star}$ esque Decoding (shortened as NEUROLOGIC ${ }^{\star}$ ), our decoding algorithm motivated by $\mathrm{A}^{</em>}$ search (Hart et al., 1968), a
best-first search algorithm that finds high-scoring paths using a heuristic estimate of future return. We first introduce the decoding problem, and then describe our heuristics with a novel lookahead procedure for adapting NEUROLOGIC ${ }^{\star}$ search to unconstrained and constrained generation with largescale autoregressive models.</p>
<h3>2.1 Decoding With A* ${ }^{\star}$ esque Lookahead</h3>
<p>Decoding. Sequence-to-sequence generation is the task of generating an output sequence $\mathbf{y}$ given an input sequence $\mathbf{x}$. We consider standard left-to-right, autoregressive models, $p_{\theta}(\mathbf{y} \mid \mathbf{x})=$ $\prod_{t=1}^{|\mathbf{y}|} p_{\theta}\left(y_{t} \mid \mathbf{y}_{&lt;t}, \mathbf{x}\right)$, and omit $\mathbf{x}$ to reduce clutter. Decoding consists of solving,</p>
<p>$$
\mathbf{y}_{*}=\underset{\mathbf{y} \in \mathcal{Y}}{\arg \max } F(\mathbf{y})
$$</p>
<p>where $\mathcal{Y}$ is the set of all sequences. In our setting, the objective $F(\mathbf{y})$ takes the form $s(\mathbf{y})+H(\mathbf{y})$, where $s(\mathbf{y})$ is $\log p_{\theta}(\mathbf{y})$, and $H(\mathbf{y})$ is either zero when no constraints are specified, or is a score for satisfying constraints on $\mathbf{y}$.</p>
<p>Our method takes the perspective of decoding as discrete search, in which states are partial prefixes, $\mathbf{y}<em t="t">{&lt;t}$, actions are tokens in vocabulary $\mathcal{V}$ (i.e., $y</em>} \in \mathcal{V}$ ), and transitions add a token to a prefix, $\mathbf{y<em t="t">{&lt;t} \circ y</em>$. Each step of decoding consists of (1) expanding a set of candidate next-states, (2) scoring each candidate, and (3) selecting the $k$ best candidates:</p>
<p>$$
\begin{aligned}
Y_{t}^{\prime} &amp; =\left{\mathbf{y}<em t="t">{&lt;t} \circ y</em>} \mid \mathbf{y<em t-1="t-1">{&lt;t} \in Y</em>\right} \
Y_{t} &amp; =\underset{\left(\mathbf{y}}, y_{t} \in \mathcal{V<em t="t">{&lt;t}, y</em>}\right) \in Y_{t}^{\prime}}{\arg \operatorname{topk}}\left{f\left(\mathbf{y<em t="t">{&lt;t}, y</em>\right)\right}
\end{aligned}
$$</p>
<p>where $Y_{0}={\langle b o s\rangle}$ and $f(\cdot)$ is a scoring function that approximates the objective $F$. Common decoding algorithms such as beam search score candidates without considering future tokens, e.g., $f\left(\mathbf{y}<em t="t">{&lt;t}, y</em>\right)$.
Lookahead heuristics. Our method incorporates an estimate of the future into candidate selection. Ideally, we want to select candidates that are on optimal trajectories, replacing Equation 2 with:}\right)=\log p_{\theta}\left(\mathbf{y}_{\leq t</p>
<p>$$
Y_{t}=\underset{\left(\mathbf{y}<em _wzxhzdk:5_t="<t}, y_{t}, \mathbf{y}_{>t">{<t}, y_{t}\right) \in Y_{t}^{\prime}}{\arg \operatorname{topk}}\left\{\max _{\mathbf{y}>t} F\left(\mathbf{y}</em>\right)\right}
$$</p>
<p>where $\mathbf{y}<em _t="&gt;t">{&gt;t}$ represents future trajectories. However, computing Equation 3 presents two difficulties: 1) the objective $F(\mathbf{y})$ may be unknown or difficult to compute, and 2) the space of $\mathbf{y}</em>$ is prohibitively large.</p>
<p>Motivated by A* search (Hart et al., 1968), a best-first search algorithm that finds high-scoring paths by selecting actions that maximize:</p>
<p>$$
f(a)=s(a)+h(a)
$$</p>
<p>where $s(a)$ is the score-so-far and $h(a)$ is a heuristic estimate of the future score, we approximate the objective using a lightweight heuristic $h(\cdot)$ :
$Y_{t}=\underset{\mathbf{y}<em t="t">{\leq t} \in Y</em>}^{\prime}}{\arg \operatorname{topk}}\left{s\left(\mathbf{y<em _mathbf_y="\mathbf{y">{\leq t}\right)+\max </em><em _wzxhzdk:6_t="<t}, y_{t}, \mathbf{y}_{>t">{&gt;t}} h\left(\mathbf{y}</em>\right)\right}$,
where $s\left(\mathbf{y}<em _emptyset="\emptyset">{\leq t}\right)=\log p</em>\right)$. To make the search tractable, we search over a set of lookahead continuations, approximating Equation 3 as,}\left(\mathbf{y}_{\leq t</p>
<p>$$
Y_{t}=\underset{\mathbf{y}<em t="t">{\leq t} \in Y</em>}^{\prime}}{\arg \operatorname{topk}}\left{s\left(\mathbf{y<em _mathcal_L="\mathcal{L">{\leq t}\right)+\max </em><em _leq="\leq" t="t">{\ell}\left(\mathbf{y}</em>\right)\right}
$$}\right)} h\left(\mathbf{y}_{\leq t+\ell</p>
<p>where each element $\mathbf{y}<em _ell="\ell">{t+1: t+\ell}$ of $\mathcal{L}</em>}\left(\mathbf{y<em _leq="\leq" t="t">{\leq t}\right)$ is a length$\ell$ continuation of $\mathbf{y}</em>$. Beam search corresponds to setting $\ell$ and $h$ to 0 .
$\mathbf{A}^{<em>}$ esque decoding. Beam search, A</em> search, and our method fall under a general class of algorithms that differ based on (1) which candidates are expanded, (2) which candidates are pruned, (3) how candidates are scored (Meister et al., 2020). We inherit the practical advantages of beam search-style expansion and pruning, while drawing on A<em>-like heuristics to incorporate estimates of the future, and refer to our method as $\mathbf{A}^{</em>}$ esque decoding.
Generating lookaheads. We compare several methods for generating the lookaheads $\mathcal{L}<em _leq="\leq" t="t">{\ell}\left(\mathbf{y}</em>\right)$.</p>
<p>The greedy lookahead produces a single sequence, $\mathcal{L}<em t_1:="t+1:" t_ell="t+\ell">{\ell}=\left{\mathbf{y}</em>}\right}$, starting from $\mathbf{y<em t_prime="t^{\prime">{\leq t}$ and selecting each token according to $y</em>=$ $\arg \max }<em _emptyset="\emptyset">{y \in \mathcal{V}} p</em>\right)$.}\left(y \mid \mathbf{y}_{&lt;t^{\prime}</p>
<p>We also consider a soft lookahead which interpolates between providing the greedy token and a uniform mixture of tokens as input at each step. Specifically, we adjust the model's probabilities with a temperature, $\tilde{p}<em t="t">{\emptyset}\left(y</em>} \mid \mathbf{y<em t="t">{&lt;t}\right)=\operatorname{softmax}\left(s</em>$ is a vector of logits, and feed the expected token embedding as input at step $t$,} / \tau\right)$, where $s_{t} \in \mathbb{R}^{|\mathcal{V}|</p>
<p>$$
e_{t}=\mathbb{E}<em t="t">{y</em>} \sim \tilde{p}\left(y_{t} \mid \mathbf{y<em t="t">{&lt;t}\right)}\left[E\left(y</em>\right)\right]
$$</p>
<p>where $E \in \mathbb{R}^{|\mathcal{V}| \times d}$ is the model's token embedding matrix. The soft lookahead moves from providing the greedy token as input $(\tau \rightarrow 0)$ to a uniform mixture of tokens $(\tau \rightarrow \infty)$ based on the value of temperature $\tau$. When using the soft lookahead, we use $\tilde{p}$ in place of $p$ when scoring tokens. The
soft (and greedy) lookahead is efficient, but only explores a single trajectory.</p>
<p>The beam lookahead trades off efficiency for exploration, returning a set $\mathcal{L}<em _t="&lt;t">{\ell}$ containing the top- $k$ candidates obtained by running beam search for $\ell$ steps starting from $\mathbf{y}</em>$.</p>
<p>Finally, the sampling lookahead explores beyond the highly-probable beam search continuations, generating each $\mathbf{y}<em _ell="\ell">{t+1: t+\ell} \in \mathcal{L}</em>$ using,</p>
<p>$$
y_{t^{\prime}} \sim p_{\emptyset}\left(y \mid \mathbf{y}_{&lt;t^{\prime}}\right)
$$</p>
<p>for $t^{\prime}$ from $t+1$ to $t+\mathrm{k}$.
Next, we move to our proposed lookahead heuristics, starting with the unconstrained setting.</p>
<h3>2.2 Unconstrained Generation with NEUROLOGIC $^{\star}$</h3>
<p>First we consider a standard decoding setting,</p>
<p>$$
\underset{\mathbf{y} \in \mathcal{Y}}{\arg \max } \log p_{\emptyset}(\mathbf{y} \mid \mathbf{x})
$$</p>
<p>We score candidates based on a combination of the history and estimated future, by using the likelihood of the lookahead as a heuristic. That is, at the $t$ th step of decoding, we use Equation 5 with:</p>
<p>$$
h\left(\mathbf{y}<em _emptyset="\emptyset">{\leq t+\ell}\right)=\lambda \log p</em>}\left(\mathbf{y<em _leq="\leq" t="t">{t+1: t+\ell} \mid \mathbf{y}</em>\right)
$$}, \mathbf{x</p>
<p>where $\lambda$ controls how much we rely on the estimated future versus the history, similar to weighted A* (Pohl, 1970).</p>
<h3>2.3 NEUROLOGIC ${ }^{\star}$ for Constrained Generation</h3>
<p>Our lookahead heuristics lend themselves to decoding with lexical constraints in a way that standard beam search does not. For constrained generation, we build on and generalize Neurologic decoding algorithm of Lu et al. (2021)—a beambased search algorithm that supports a wide class of logical constraints for lexically constrained generation-with estimates of future constraint satisfaction.</p>
<p>Background of Neurologic. NEUROLOGIC (Lu et al., 2021) accepts lexical constraints in CNF:</p>
<p>$$
\underbrace{\left(D_{1} \vee D_{2} \cdots \vee D_{i}\right)}<em 1="1">{C</em>}} \wedge \cdots \wedge \underbrace{\left(D_{i^{\prime}} \vee \cdots \vee D_{N}\right)<em M="M">{C</em>
$$}</p>
<p>where each $D_{i}$ represents a single positive or negative constraint, $D(\mathbf{a}, \mathbf{y})$ or $\neg D(\mathbf{a}, \mathbf{y})$, enforcing the phrase a to be included in or omitted from $\mathbf{y}$. Lu et al. (2021) refer to each constraint $D_{i}$ as a literal, and each disjunction $C_{j}$ of literals as a clause.</p>
<p>NeuroLogic is a beam-based approximate search for an objective which seeks fluent sequences in which all clauses are satisfied:</p>
<p>$$
\arg \max <em _theta="\theta">{\mathbf{y} \in \mathcal{Y}} p</em>\right)
$$}(\mathbf{y} \mid \mathbf{x})-\lambda^{\prime} \sum_{j=1}^{M}\left(1-C_{j</p>
<p>where $\lambda^{\prime} \gg 0$ penalizes unsatisfied clauses. At each step of the search, NEUROLOGIC scores each of the $k \times|\mathcal{V}|$ candidates $\left(\mathbf{y}<em t="t">{&lt;t}, y</em>\right)$ based on whether they (partially) satisfy new constraints,</p>
<p>$$
f\left(\mathbf{y}<em _theta="\theta">{\leq t}\right)=\log p</em>}\left(\mathbf{y<em 1="1">{\leq t} \mid \mathbf{x}\right)+\lambda</em> \max <em _leq="\leq" t="t">{D\left(\mathbf{a}, \mathbf{y}</em>
$$}\right)} \frac{|\hat{\mathbf{a}}|}{|\mathbf{a}|</p>
<p>where the maximization is over a set of unsatisfied multi-token constraints a tracked by NEUROLOGIC, and $\hat{\mathbf{a}}$ is the prefix of $\mathbf{a}$ in the ongoing generation. For example, for $\mathbf{y}_{\leq t}=$ "The boy climbs an apple" and constraint a="apple tree", $\hat{\mathbf{a}}$ is "apple". Intuitively, this function rewards candidates that are in the process of satisfying a constraint.</p>
<p>In lieu of taking the top- $k$ scoring candidates (Equation 5), NEUROLOGIC prunes candidates that contain clauses that violate constraints, groups the candidates to promote diversity, and selects highscoring candidates from each group. We use the same pruning and grouping approach, and refer the reader to Lu et al. (2021) for further details.</p>
<p>NeuroLogic ${ }^{\star}$ decoding. Our method improves upon the NEUROLOGIC scoring function with an estimate of future constraint satisfaction. Our key addition is a lookahead heuristic that adjusts a candidate $\left(\mathbf{y}<em t="t">{&lt;t}, y</em>$ :}\right)$ 's score proportional to the probability of satisfying additional unsatisfied constraints in the lookahead $\mathbf{y}_{t+1: t+\ell</p>
<p>$$
\begin{aligned}
&amp; h_{\text {future }}\left(\mathbf{y}<em 2="2">{\leq t+\ell}\right)= \
&amp; \lambda</em> \max <em _leq="\leq" t="t">{D\left(\mathbf{a}, \mathbf{y}</em>}\right)} \log p_{\theta}\left(D\left(\mathbf{a}, \mathbf{y<em _leq="\leq" t="t">{t+1: t+\ell}\right) \mid \mathbf{x}, \mathbf{y}</em>\right)
\end{aligned}
$$</p>
<p>where we define the probability that constraint $\mathbf{a}$ is satisfied using the most probable subsequence,</p>
<p>$$
\begin{aligned}
&amp; p_{\theta}\left(D\left(\mathbf{a}, \mathbf{y}<em _leq="\leq" t="t">{t+1: t+\ell}\right) \mid \mathbf{x}, \mathbf{y}</em>\right)= \
&amp; \quad \max <em _theta="\theta">{t^{\prime} \in[t, t+\ell]} p</em>}\left(\mathbf{y<em _t_prime="&lt;t^{\prime">{t^{\prime}: t^{\prime}+|\mathbf{a}|}=\mathbf{a} \mid \mathbf{x}, \mathbf{y}</em>\right)
\end{aligned}
$$}</p>
<p>$\lambda_{2}$ is a scaling hyperparameter for the heuristic.
Intuitively, this lookahead heuristic brings two benefits. When $y_{t}$ is a token that would satisfy a multi-token constraint, the lookahead incorporates the score of the full constraint. When $y_{t}$ is a token that is not part of a constraint, the lookahead allows for incorporating the score of a future constraint that would be satisfied if $y_{t}$ was selected.</p>
<p>We add our lookahead heuristic to the NeUROLOGIC scoring function (Equation 8), and call the resulting decoding procedure NEUROLOGIC $\mathrm{A}^{\star}$ esque (or, NEUROLOGIC ${ }^{\star}$ in short).</p>
<h2>3 Experiments</h2>
<p>We first consider constrained generation benchmarks: CommonGen (§3.1), constrained machine translation (§3.2), table-to-text generation (§3.3), and constrained question generation (§3.4). NEUROLOGIC ${ }^{\star}$ consistently outperforms previous approaches, especially in zero-shot and fewshot cases. These low-resource settings are particularly important, as many practical tasks face data scarcity. Finally, we find that $\mathrm{A}^{\star}$ esque lookahead is useful even without constraints, as shown in unconstrained story generation task (§3.5).</p>
<p>Metrics. As automatic metrics, we use: BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), METEOR (Banerjee and Lavie, 2005), CIDEr (Vedantam et al., 2015), SPICE (Anderson et al., 2016) and NIST (Lin and Hovy, 2003).</p>
<h3>3.1 Constrained Commonsense Generation</h3>
<p>CommonGen (Lin et al., 2020) is a commonsense generation task with lexical constraints: given a set of concepts (e.g., {throw, run, javelin, track}), models need to generate a coherent sentence describing a plausible scenario using all given concepts (e.g., "a man runs on a track and throws a javelin.").</p>
<p>Approach and Baselines. Following Lu et al. (2021), we enforce that each concept $c_{i}$ appear in output $\mathbf{y}$ under some morphological inflection. We test in both supervised and zero-shot settings. In the supervised setting, we finetune GPT-2 (Radford et al., 2019) as a sequence-to-sequence model. In the zero-shot setting, we use GPT-2 off-the-shelf (no fine-tuning) and rely on constrained decoding to guide generation. We compare with previous constrained decoding algorithms CBS (Anderson et al., 2017), GBS (Hokamp and Liu, 2017), DBA (Post and Vilar, 2018a), NeUroLogic (Lu et al., 2021) and TSMH (Zhang et al., 2020).</p>
<p>Metrics. We report standard automatic metrics as well as coverage, the average percentage of concepts present in generations. Additionally, we conduct human evaluation on 100 test examples using Amazon Mechanical Turk (AMT), with 3 annotators per example (template in Appendix D). Workers rate each generation on language quality, sce-</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Decode Method</th>
<th style="text-align: center;">Automatic Evaluation</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Human Evaluation</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ROUGE-L</td>
<td style="text-align: center;">BLEU-4</td>
<td style="text-align: center;">METEOR</td>
<td style="text-align: center;">CIDEr</td>
<td style="text-align: center;">SPICE</td>
<td style="text-align: center;">Coverage</td>
<td style="text-align: center;">Quality</td>
<td style="text-align: center;">Plausibility</td>
<td style="text-align: center;">Concepts</td>
<td style="text-align: center;">Overall</td>
</tr>
<tr>
<td style="text-align: center;">Supervised</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">CBS (Anderson et al., 2017)</td>
<td style="text-align: center;">38.8</td>
<td style="text-align: center;">20.6</td>
<td style="text-align: center;">28.5</td>
<td style="text-align: center;">12.9</td>
<td style="text-align: center;">27.1</td>
<td style="text-align: center;">97.6</td>
<td style="text-align: center;">2.27</td>
<td style="text-align: center;">2.35</td>
<td style="text-align: center;">2.51</td>
<td style="text-align: center;">2.23</td>
</tr>
<tr>
<td style="text-align: center;">GBS (Hokamp and Liu, 2017)</td>
<td style="text-align: center;">38.2</td>
<td style="text-align: center;">18.4</td>
<td style="text-align: center;">26.7</td>
<td style="text-align: center;">11.7</td>
<td style="text-align: center;">26.1</td>
<td style="text-align: center;">97.4</td>
<td style="text-align: center;">2.06</td>
<td style="text-align: center;">2.17</td>
<td style="text-align: center;">2.29</td>
<td style="text-align: center;">2.01</td>
</tr>
<tr>
<td style="text-align: center;">DBA (Post and Vilar, 2018a)</td>
<td style="text-align: center;">38.3</td>
<td style="text-align: center;">18.7</td>
<td style="text-align: center;">27.7</td>
<td style="text-align: center;">12.4</td>
<td style="text-align: center;">26.3</td>
<td style="text-align: center;">97.5</td>
<td style="text-align: center;">2.23</td>
<td style="text-align: center;">2.30</td>
<td style="text-align: center;">2.43</td>
<td style="text-align: center;">2.15</td>
</tr>
<tr>
<td style="text-align: center;">Neurologic (Lu et al., 2021)</td>
<td style="text-align: center;">42.8</td>
<td style="text-align: center;">26.7</td>
<td style="text-align: center;">30.2</td>
<td style="text-align: center;">14.7</td>
<td style="text-align: center;">30.3</td>
<td style="text-align: center;">97.7</td>
<td style="text-align: center;">2.54</td>
<td style="text-align: center;">2.56</td>
<td style="text-align: center;">2.67</td>
<td style="text-align: center;">2.50</td>
</tr>
<tr>
<td style="text-align: center;">NeUroLogiC* (greedy)</td>
<td style="text-align: center;">43.6</td>
<td style="text-align: center;">28.2</td>
<td style="text-align: center;">30.8</td>
<td style="text-align: center;">15.2</td>
<td style="text-align: center;">30.8</td>
<td style="text-align: center;">97.8</td>
<td style="text-align: center;">2.66</td>
<td style="text-align: center;">2.67</td>
<td style="text-align: center;">2.73</td>
<td style="text-align: center;">2.59</td>
</tr>
<tr>
<td style="text-align: center;">NeUroLogiC* (sample)</td>
<td style="text-align: center;">43.4</td>
<td style="text-align: center;">27.9</td>
<td style="text-align: center;">30.8</td>
<td style="text-align: center;">15.3</td>
<td style="text-align: center;">31.0</td>
<td style="text-align: center;">97.7</td>
<td style="text-align: center;">2.64</td>
<td style="text-align: center;">2.64</td>
<td style="text-align: center;">2.74</td>
<td style="text-align: center;">2.58</td>
</tr>
<tr>
<td style="text-align: center;">NeUroLogiC* (beam)</td>
<td style="text-align: center;">43.2</td>
<td style="text-align: center;">28.2</td>
<td style="text-align: center;">30.7</td>
<td style="text-align: center;">15.2</td>
<td style="text-align: center;">31.0</td>
<td style="text-align: center;">97.6</td>
<td style="text-align: center;">2.68</td>
<td style="text-align: center;">2.67</td>
<td style="text-align: center;">2.76</td>
<td style="text-align: center;">2.60</td>
</tr>
<tr>
<td style="text-align: center;">Unsupervised</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">TSMH (Zhang et al., 2020)</td>
<td style="text-align: center;">24.7</td>
<td style="text-align: center;">2.2</td>
<td style="text-align: center;">14.5</td>
<td style="text-align: center;">3.6</td>
<td style="text-align: center;">15.4</td>
<td style="text-align: center;">71.5</td>
<td style="text-align: center;">1.85</td>
<td style="text-align: center;">1.92</td>
<td style="text-align: center;">1.95</td>
<td style="text-align: center;">1.63</td>
</tr>
<tr>
<td style="text-align: center;">NeUroLogiC (Lu et al., 2021)</td>
<td style="text-align: center;">41.9</td>
<td style="text-align: center;">24.7</td>
<td style="text-align: center;">29.5</td>
<td style="text-align: center;">14.4</td>
<td style="text-align: center;">27.5</td>
<td style="text-align: center;">96.7</td>
<td style="text-align: center;">2.64</td>
<td style="text-align: center;">2.52</td>
<td style="text-align: center;">2.68</td>
<td style="text-align: center;">2.50</td>
</tr>
<tr>
<td style="text-align: center;">NeUroLogiC* (greedy)</td>
<td style="text-align: center;">44.3</td>
<td style="text-align: center;">28.6</td>
<td style="text-align: center;">30.7</td>
<td style="text-align: center;">15.6</td>
<td style="text-align: center;">29.6</td>
<td style="text-align: center;">97.1</td>
<td style="text-align: center;">2.78</td>
<td style="text-align: center;">2.70</td>
<td style="text-align: center;">2.77</td>
<td style="text-align: center;">2.70</td>
</tr>
</tbody>
</table>
<p>Table 1: Performance of various decoding methods with supervised or off-the-shelf GPT-2 on the CommonGen test set, measured with automatic and human evaluations. We only tried NeUroLogiC* (greedy) in the unsupervised setting because of the computational cost. The best numbers are bolded and the second best ones are underlined.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Words</th>
<th style="text-align: left;">Method</th>
<th style="text-align: left;">Generation</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">cut</td>
<td style="text-align: left;">GBS</td>
<td style="text-align: left;">Cut a piece of wood to use as a fence.</td>
</tr>
<tr>
<td style="text-align: left;">piece</td>
<td style="text-align: left;">DBA</td>
<td style="text-align: left;">Cut a piece of wood to use as a fence.</td>
</tr>
<tr>
<td style="text-align: left;">use</td>
<td style="text-align: left;">NeUroLogic</td>
<td style="text-align: left;">Piece of wood used for cutting.</td>
</tr>
<tr>
<td style="text-align: left;">wood</td>
<td style="text-align: left;">NeUroLogic*</td>
<td style="text-align: left;">A man cuts a piece of wood using a circular saw.</td>
</tr>
<tr>
<td style="text-align: left;">ball</td>
<td style="text-align: left;">GBS</td>
<td style="text-align: left;">A dog is run over by a ball and mouth agape.</td>
</tr>
<tr>
<td style="text-align: left;">dog</td>
<td style="text-align: left;">DBA</td>
<td style="text-align: left;">A dog is run over by a ball and bites his mouth.</td>
</tr>
<tr>
<td style="text-align: left;">mouth</td>
<td style="text-align: left;">NeUroLogic</td>
<td style="text-align: left;">A dog is running and chewing on a ball in its mouth.</td>
</tr>
<tr>
<td style="text-align: left;">run</td>
<td style="text-align: left;">NeUroLogic*</td>
<td style="text-align: left;">A dog running with a ball in its mouth.</td>
</tr>
<tr>
<td style="text-align: left;">dog</td>
<td style="text-align: left;">GBS</td>
<td style="text-align: left;">Soap and water scrubbed dog with a towel.</td>
</tr>
<tr>
<td style="text-align: left;">scrub</td>
<td style="text-align: left;">DBA</td>
<td style="text-align: left;">Soap and water on a dog and scrubbed skin.</td>
</tr>
<tr>
<td style="text-align: left;">soap</td>
<td style="text-align: left;">NeUroLogic</td>
<td style="text-align: left;">A dog is scrubbing his paws with soap and water.</td>
</tr>
<tr>
<td style="text-align: left;">water</td>
<td style="text-align: left;">NeUroLogic*</td>
<td style="text-align: left;">A man is scrubbing a dog with soap and water.</td>
</tr>
</tbody>
</table>
<p>Table 2: Example generations for the CommonGen task across supervised NeUroLogic* and baselines, including GBS (Hokamp and Liu, 2017), DBA (Post and Vilar, 2018a), and NeUroLogic (Lu et al., 2021).
nario plausibility, coverage of given concepts, and an overall score on a 3-point Likert scale. ${ }^{2}$</p>
<p>Results. Table 1 compares different constrained decoding methods on top of the finetuned and off-the-shelf GPT-2, in supervised and zero-shot settings respectively. The key observations are:</p>
<ol>
<li>NeUroLogic ${ }^{\star}$ outperforms all previous constrained-decoding methods in both supervised and zero-shot settings. Surprisingly, unsupervised NEUROLOGIC ${ }^{\star}$ outperforms all supervised methods based on human evaluation.</li>
<li>Compared to vanilla NeUroLogic, NeUroLogic ${ }^{\star}$ improves generation quality while maintaining high constraint satisfaction. The difference is especially substantial in the zero-shot setting. Intuitively, this setting leaves</li>
</ol>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>| Method | Dinu et al. |  |  | Marian MT |  |
| :-- | :--: | :--: | :--: | :--: | :--: |
|  | BLEU | Term\% |  | BLEU | Term\% |
| Unconstrained | 25.8 | 76.3 |  | 32.9 | 85.0 |
| train-by-app. | 26.0 | 92.9 |  | - | - |
| train-by-rep. | 26.0 | 94.5 |  | - | - |
| Post and Vilar (2018a) | 25.3 | 82.0 |  | 33.0 | 94.3 |
| NeUroLogic | 26.5 | 95.1 |  | 33.4 | 97.1 |
| NeUroLogic<em> (greedy) | 26.7 | 95.8 |  | 33.7 | 97.2 |
| NeUroLogic</em> (sample) | 26.6 | 95.4 |  | 33.7 | 97.2 |
| NeUroLogic* (beam) | 26.6 | 95.8 |  | 33.6 | 97.3 |</p>
<p>Table 3: Results on constrained MT. The left section uses the same two-layer transformer as Dinu et al. (2019), while the right one uses a stronger Marian MT EN-DE model. The highlighted methods modify training data specifically for constrained generation, and thus cannot be applied to off-the-shelf models. The best numbers are bold, second best are underlined.
more room for incorporating constraint-driven signals due to the lack of supervision.
3. NeUroLogic ${ }^{\star}$ reaches similar performance using different lookahead strategies, among which beam lookahead slightly outperforms the others based on human evaluation, and greedy lookahead has the lowest runtime. We analyze lookahead strategies further in Appendix A.</p>
<h3>3.2 Constrained Machine Translation</h3>
<p>Next, we test on constrained machine translation (MT). It is often critical to have control over MT systems, such as to incorporate domain-specific terminology (Post and Vilar, 2018a; Dinu et al., 2019). To achieve this goal, recent work proposed constrained decoding algorithms (Chatterjee et al., 2017; Hokamp and Liu, 2017; Hasler et al., 2018; Hu et al., 2019, inter alia) or specialized training (Dinu et al., 2019). We demonstrate that</p>
<table>
<thead>
<tr>
<th style="text-align: center;"># T</th>
<th style="text-align: center;"># Sents.</th>
<th style="text-align: center;">Decode Method</th>
<th style="text-align: center;">BLEU</th>
<th style="text-align: center;">Term\%</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">378</td>
<td style="text-align: center;">Beam search</td>
<td style="text-align: center;">25.4</td>
<td style="text-align: center;">79.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">NEUROLOGIC</td>
<td style="text-align: center;">$\underline{26.2}$</td>
<td style="text-align: center;">$\underline{95.2}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">NEUROLOGIC ${ }^{\star}$</td>
<td style="text-align: center;">$\mathbf{2 6 . 3}$</td>
<td style="text-align: center;">$\mathbf{9 5 . 8}$</td>
</tr>
<tr>
<td style="text-align: center;">$2+$</td>
<td style="text-align: center;">36</td>
<td style="text-align: center;">Beam search</td>
<td style="text-align: center;">28.1</td>
<td style="text-align: center;">85.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">NEUROLOGIC</td>
<td style="text-align: center;">$\underline{28.9}$</td>
<td style="text-align: center;">$\underline{93.7}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">NEUROLOGIC ${ }^{\star}$</td>
<td style="text-align: center;">$\mathbf{2 9 . 3}$</td>
<td style="text-align: center;">$\mathbf{9 6 . 5}$</td>
</tr>
</tbody>
</table>
<p>Table 4: Constrained MT performance broken down by the number of constraint terms (# T). All configurations use the two-layer tranformer from Dinu et al. (2019). The best numbers are bolded and the second best ones are underlined.</p>
<p>NeuroLogic ${ }^{\circledR}$ can be readily applied to off-theshelf MT systems for constrained machine translation. We follow Dinu et al. (2019) and evaluate on the WMT17 EN-DE test set (Bojar et al., 2017). The constraint here is to integrate given custom terminologies into the translation output; constraint terms are automatically created from the IATE EU terminology database for 414 test sentences.</p>
<p>Approach, Baselines, and Metrics. We experiment with two MT systems: Dinu et al. (twolayer transformer) and the off-the-shelf Marian MT (Junczys-Dowmunt et al., 2018). We compare with previous constrained decoding algorithms, including DBA (Post and Vilar, 2018a), NeuroLogic (Lu et al., 2021), and also specialized training proposed by Dinu et al. (2019). Following Dinu et al. (2019), we report BLEU and term use rates, i.e., percentage of times given constraint terms were generated out of total number of constraint terms.</p>
<p>Results. Table 3 presents experimental results with Dinu et al.'s model and Marian MT. In both cases, NEUROLOGIC ${ }^{\star}$ outperforms prior methods in BLEU and term coverage. Besides higher quality and coverage, NEUROLOGIC ${ }^{\star}$ is plug-and-play, working with any off-the-shelf MT system, unlike previous training-based methods. Table 4 breaks down the performance by the number of constraint terms. We see that the improvement brought by NEUROLOGIC ${ }^{\star}$ is especially large when given complex constraints with multiple terms. (e.g., 96.5 vs. 93.7 from NEUROLOGIC in term of coverage).</p>
<h3>3.3 Table-to-text Generation</h3>
<p>Next we test on the table-to-text task, where models need to generate natural language for structured table data. Constrained generation ensures that the output text is factual and consistent with the input data. We follow the few-shot setup of Chen et al. (2020b) on the E2ENLG (Dušek et al., 2018)</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Decode Method</th>
<th style="text-align: center;">NIST BLEU METEOR CIDEr</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Beam Search</td>
<td style="text-align: center;">3.82</td>
<td style="text-align: center;">42.8</td>
<td style="text-align: center;">32.6</td>
<td style="text-align: center;">10.8</td>
<td style="text-align: center;">57.8</td>
<td style="text-align: center;">73.6</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">CBS</td>
<td style="text-align: center;">6.50</td>
<td style="text-align: center;">42.3</td>
<td style="text-align: center;">36.4</td>
<td style="text-align: center;">13.0</td>
<td style="text-align: center;">54.3</td>
<td style="text-align: center;">91.6</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GBS</td>
<td style="text-align: center;">6.26</td>
<td style="text-align: center;">40.7</td>
<td style="text-align: center;">36.7</td>
<td style="text-align: center;">12.9</td>
<td style="text-align: center;">54.2</td>
<td style="text-align: center;">94.1</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">NEUROLOGIC</td>
<td style="text-align: center;">6.95</td>
<td style="text-align: center;">47.6</td>
<td style="text-align: center;">38.9</td>
<td style="text-align: center;">16.3</td>
<td style="text-align: center;">58.7</td>
<td style="text-align: center;">97.6</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">NEUROLOGIC ${ }^{\star}$ (greedy)</td>
<td style="text-align: center;">7.11</td>
<td style="text-align: center;">49.2</td>
<td style="text-align: center;">40.0</td>
<td style="text-align: center;">17.5</td>
<td style="text-align: center;">60.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">NEUROLOGIC ${ }^{\star}$ (beam)</td>
<td style="text-align: center;">7.01</td>
<td style="text-align: center;">48.9</td>
<td style="text-align: center;">40.0</td>
<td style="text-align: center;">17.2</td>
<td style="text-align: center;">59.8</td>
<td style="text-align: center;">99.9</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">NEUROLOGIC ${ }^{\star}$ (sample)</td>
<td style="text-align: center;">7.11</td>
<td style="text-align: center;">49.3</td>
<td style="text-align: center;">40.1</td>
<td style="text-align: center;">17.5</td>
<td style="text-align: center;">60.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 5: Performance of different decoding methods with few-shot GPT-2 finetuned on $0.1 \%$ E2ENLG data. The best numbers are bold, second best are underlined.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: left;">$\mathbf{0 . 1 \%}$</th>
<th style="text-align: left;">$\mathbf{0 . 5 \%}$</th>
<th style="text-align: left;">$\mathbf{1 \%}$</th>
<th style="text-align: left;">$\mathbf{5 \%}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">TGen (Dušek and Jurčíček, 2016)</td>
<td style="text-align: left;">3.6</td>
<td style="text-align: left;">27.9</td>
<td style="text-align: left;">35.2</td>
<td style="text-align: left;">57.3</td>
</tr>
<tr>
<td style="text-align: left;">Template-GPT-2 (Chen et al., 2020a)</td>
<td style="text-align: left;">22.5</td>
<td style="text-align: left;">47.8</td>
<td style="text-align: left;">53.3</td>
<td style="text-align: left;">59.9</td>
</tr>
<tr>
<td style="text-align: left;">KGPT-Graph (Chen et al., 2020b)</td>
<td style="text-align: left;">39.8</td>
<td style="text-align: left;">53.3</td>
<td style="text-align: left;">55.1</td>
<td style="text-align: left;">61.5</td>
</tr>
<tr>
<td style="text-align: left;">KGPT-Seq (Chen et al., 2020b)</td>
<td style="text-align: left;">40.2</td>
<td style="text-align: left;">53.0</td>
<td style="text-align: left;">54.1</td>
<td style="text-align: left;">61.1</td>
</tr>
<tr>
<td style="text-align: left;">GPT-2</td>
<td style="text-align: left;">42.8</td>
<td style="text-align: left;">$\underline{57.1}$</td>
<td style="text-align: left;">56.8</td>
<td style="text-align: left;">61.1</td>
</tr>
<tr>
<td style="text-align: left;">GPT-2 + NEUROLOGIC</td>
<td style="text-align: left;">$\underline{47.6}$</td>
<td style="text-align: left;">56.9</td>
<td style="text-align: left;">$\underline{58.0}$</td>
<td style="text-align: left;">$\underline{62.9}$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-2 + NEUROLOGIC ${ }^{\star}$ (greedy)</td>
<td style="text-align: left;">$\mathbf{4 9 . 2}$</td>
<td style="text-align: left;">$\mathbf{5 8 . 0}$</td>
<td style="text-align: left;">$\mathbf{5 8 . 4}$</td>
<td style="text-align: left;">$\mathbf{6 3 . 4}$</td>
</tr>
</tbody>
</table>
<p>Table 6: Few-shot results (BLEU-4) on E2ENLG test set with $0.1 \%, 0.5 \%, 1 \%, 5 \%$ of training instances. The best numbers are bold, second best are underlined.
dataset, where randomly-sampled $0.1 \%, 0.5 \%, 1 \%$, or $5 \%$ of training instances are used for finetuning.</p>
<p>Approach, Baselines, and Metrics. Following Shen et al. (2019), we linearize data tables into strings and finetune GPT-2 with few-shot examples. We compare NEUROLOGIC ${ }^{\star}$ with three previous constrained decoding algorithms: CBS (Anderson et al., 2017), GBS (Hokamp and Liu, 2017), and NEUROLOGIC (Lu et al., 2021), based on few-shot GPT-2 finetuned with $0.1 \%$ data. Then we compare NEUROLOGIC ${ }^{\star}$ on top of GPT-2, with previous table-to-text methods, including TGen (Dušek and Jurčíček, 2016), Template-GPT-2 (Chen et al., 2020a), KGPT (Chen et al., 2020b), in multiple few-shot settings with various numbers of training instances. We report standard automatic metrics, as well as information coverage, i.e., percentage of information present in the generation.</p>
<p>Results. Table 5 compares various decoding methods with few-shot GPT-2 finetuned on $0.1 \%$ of the data. NEUROLOGIC ${ }^{\star}$ substantially outperforms previous methods on all metrics, consistently improving quality while achieving near-perfect constraint satisfaction. Previous work (CBS and GBS) improves constraint satisfaction, but negatively affects quality, indicated by drops in BLEU and ROUGE. Table 6 compares NEUROLOGIC ${ }^{\star}$ on top of GPT-2 with previous table-to-text approaches. As before, NEUROLOGIC ${ }^{\star}$ outperforms past approaches by a large margin, even if the latter ones leverage specialized model architectures or addi-</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Performance ( $y$-axis) of supervised GPT-2 on E2ENLG, with a varying percentage of training data for supervision (x-axis). The purple, blue, and black lines denote decoding with NEUROLOGIC ${ }^{\star}$, NEUROLOGIC and conventional beam search, respectively.
tional pretraining on massive table-to-text corpora. Additionally, Figure 2 compares the performance ( $y$-axis) of few-shot GPT-2 with NEUROLOGIC ${ }^{\star}$ (purple line), NEUROLOGIC (blue line), and conventional beam search (black line) as a function of the varying percentage of training instances (xaxis). The benefit of NEUROLOGIC ${ }^{\star}$ grows as data size is reduced. Indeed, constrained decoding enables impressive low-resource performance.</p>
<h3>3.4 Constrained Question Generation</h3>
<p>Next, we consider constrained question generation (Zhang et al., 2020), where models need to generate interrogative questions using given keywords. This task is zero-shot without any training data, further testing the capacity of NEUROLOGIC ${ }^{\star}$ to guide off-the-shelf models without finetuning.</p>
<p>Approach, Baselines, and Metrics. We use GPT-2 off-the-shelf and compare NEUROLOGIC ${ }^{\star}$ with previous constrained decoding methods, including CGMH (Miao et al., 2019), TSMH (Zhang et al., 2020) and NEUROLOGIC (Lu et al., 2021). We report standard generation metrics and keyword coverage as in $\S 3.1$. We conduct human evaluation following subsection 3.1, to measure grammar, fluency, meaningfulness, and overall quality of the generated questions, using a 3-point Likert scale ${ }^{3}$ (template in Appendix D).</p>
<p>Results. Table 7 presents comparisons across different decoding methods based on off-the-shelf language models. NEUROLOGIC ${ }^{\star}$ outperforms all previous methods with respect to both automatic and manual metrics; it enhances the generation quality while achieving perfect constraint satisfaction. The difference between NEUROLOGIC and NEUROLOGIC ${ }^{\star}$ is particularly large compared to other tasks. We suspect that the search problem is</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>much harder here, due to the lack of supervision and complex logical constraints involving both keywords and syntax. As a whole, the results demonstrate the effectiveness of NEUROLOGIC ${ }^{\star}$ in tackling challenging constrained generation problems.</p>
<h3>3.5 Unconstrained Story Generation</h3>
<p>Finally, we demonstrate NEUROLOGIC ${ }^{\star}$ can also improve unconstrained generation. We investigate whether $A^{\star}$ esque decoding with our unconstrained lookahead heuristic (Equation 7) can (1) improve beam search, which typically struggles in openended settings (Holtzman et al., 2020; Welleck et al., 2019b), and (2) improve sampling algorithms that are commonly used in open-ended generation. We consider conditional story generation on the RocStories dataset (Mostafazadeh et al., 2016): given a first sentence $\mathbf{x}$, generate the full story $\mathbf{y}$.</p>
<p>Approach, Baselines and Metrics. We use GPT-2, fine-tuned on the RocStories training set. We apply $A^{\star}$ esque decoding to (1) beam search, the setting used so far in the experiments, and (2) top-k sampling (Fan et al., 2018), a commonly used sampling algorithm in open-ended generation. For top-k sampling, we use the heuristic to adjust the probability scores, then renormalize. We use standard automatic metrics: perplexity and BLEU for fluency, and unique n-grams as a measure of diversity. We conduct human evaluation following subsection 3.1, for story flow and overall quality on a 3-point Likert scale ${ }^{4}$ (template in Appendix D).</p>
<p>Results. Table 8 presents the results of beam search and top-k sampling with and without $A^{\star}$ esque heuristics. $A^{\star}$ esque heuristics result in more fluent, coherent and interesting stories for both beam search and top-k sampling. For beam search, $A^{\star}$ esque not only enhances generation quality- e.g. improving human evaluation scores from 2.32 to 2.63 -but also boosts generation diversity, reflected by number of unique n-grams. For top-k sampling, $A^{\star}$ esque heuristics improve quality, while maintaining comparable diversity. We further analyze quality and diversity tradeoff in Appendix A. Moreover, we notice that beam lookahead works the best for beam search, and greedy lookahead works the best for top-k sampling. We suspect that beam lookahead gives the most accurate estimate of future beam path, while greedy</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Decode Method</th>
<th style="text-align: center;">Automatic Evaluation</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Human Evaluation</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">ROUGE</td>
<td style="text-align: center;">BLEU</td>
<td style="text-align: center;">METEOR</td>
<td style="text-align: center;">CIDEr</td>
<td style="text-align: center;">SPICE</td>
<td style="text-align: center;">Coverage</td>
<td style="text-align: center;">Grammar</td>
<td style="text-align: center;">Fluency</td>
<td style="text-align: center;">Meaningfulness</td>
<td style="text-align: center;">Overall</td>
</tr>
<tr>
<td style="text-align: left;">CGMH (Miao et al., 2019)</td>
<td style="text-align: center;">28.8</td>
<td style="text-align: center;">2.0</td>
<td style="text-align: center;">18.0</td>
<td style="text-align: center;">5.5</td>
<td style="text-align: center;">21.5</td>
<td style="text-align: center;">18.3</td>
<td style="text-align: center;">2.28</td>
<td style="text-align: center;">2.34</td>
<td style="text-align: center;">2.11</td>
<td style="text-align: center;">2.02</td>
</tr>
<tr>
<td style="text-align: left;">TSMH (Zhang et al., 2020)</td>
<td style="text-align: center;">42.0</td>
<td style="text-align: center;">4.3</td>
<td style="text-align: center;">25.9</td>
<td style="text-align: center;">10.4</td>
<td style="text-align: center;">37.7</td>
<td style="text-align: center;">$\underline{92.7}$</td>
<td style="text-align: center;">2.35</td>
<td style="text-align: center;">2.28</td>
<td style="text-align: center;">2.37</td>
<td style="text-align: center;">2.22</td>
</tr>
<tr>
<td style="text-align: left;">NEUROLOGIC (Lu et al., 2021)</td>
<td style="text-align: center;">38.8</td>
<td style="text-align: center;">11.2</td>
<td style="text-align: center;">24.5</td>
<td style="text-align: center;">18.0</td>
<td style="text-align: center;">41.7</td>
<td style="text-align: center;">90.6</td>
<td style="text-align: center;">2.78</td>
<td style="text-align: center;">2.71</td>
<td style="text-align: center;">2.49</td>
<td style="text-align: center;">2.51</td>
</tr>
<tr>
<td style="text-align: left;">NEUROLOGIC ${ }^{\text {® }}$ (greedy)</td>
<td style="text-align: center;">$\mathbf{4 3 . 7}$</td>
<td style="text-align: center;">$\mathbf{1 4 . 7}$</td>
<td style="text-align: center;">$\underline{28.0}$</td>
<td style="text-align: center;">$\mathbf{2 0 . 9}$</td>
<td style="text-align: center;">$\underline{47.7}$</td>
<td style="text-align: center;">$\mathbf{1 0 0 . 0}$</td>
<td style="text-align: center;">$\mathbf{2 . 8 3}$</td>
<td style="text-align: center;">$\underline{2.77}$</td>
<td style="text-align: center;">$\underline{2.74}$</td>
<td style="text-align: center;">$\mathbf{2 . 7 6}$</td>
</tr>
<tr>
<td style="text-align: left;">NEUROLOGIC ${ }^{\text {® }}$ (beam)</td>
<td style="text-align: center;">42.9</td>
<td style="text-align: center;">14.4</td>
<td style="text-align: center;">27.8</td>
<td style="text-align: center;">20.3</td>
<td style="text-align: center;">46.9</td>
<td style="text-align: center;">$\mathbf{1 0 0 . 0}$</td>
<td style="text-align: center;">$\underline{2.81}$</td>
<td style="text-align: center;">$\mathbf{2 . 8 6}$</td>
<td style="text-align: center;">$\mathbf{2 . 7 6}$</td>
<td style="text-align: center;">$\underline{2.75}$</td>
</tr>
<tr>
<td style="text-align: left;">NEUROLOGIC ${ }^{\text {® }}$ (sample)</td>
<td style="text-align: center;">$\underline{43.5}$</td>
<td style="text-align: center;">$\underline{14.6}$</td>
<td style="text-align: center;">$\mathbf{2 8 . 2}$</td>
<td style="text-align: center;">$\underline{20.8}$</td>
<td style="text-align: center;">$\mathbf{4 7 . 8}$</td>
<td style="text-align: center;">$\mathbf{1 0 0 . 0}$</td>
<td style="text-align: center;">$\mathbf{2 . 8 3}$</td>
<td style="text-align: center;">2.75</td>
<td style="text-align: center;">$\mathbf{2 . 7 6}$</td>
<td style="text-align: center;">2.73</td>
</tr>
</tbody>
</table>
<p>Table 7: Performance of different unsupervised decoding algorithms on constrained question generation.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Decode Method</th>
<th style="text-align: center;">Fluency</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Diversity</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Human Eval</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PPL</td>
<td style="text-align: center;">BLEU-1</td>
<td style="text-align: center;">BLEU-2</td>
<td style="text-align: center;">Uniq. 2-gram</td>
<td style="text-align: center;">Uniq. 3-gram</td>
<td style="text-align: center;">Uniq. 4-gram</td>
<td style="text-align: center;">Coherence</td>
<td style="text-align: center;">Overall</td>
</tr>
<tr>
<td style="text-align: center;">beam search</td>
<td style="text-align: center;">2.24</td>
<td style="text-align: center;">33.7</td>
<td style="text-align: center;">16.5</td>
<td style="text-align: center;">20.13 k</td>
<td style="text-align: center;">34.09 k</td>
<td style="text-align: center;">41.91 k</td>
<td style="text-align: center;">2.46</td>
<td style="text-align: center;">2.32</td>
</tr>
<tr>
<td style="text-align: center;">beam search $+\mathrm{A}^{*}$ esque (greedy)</td>
<td style="text-align: center;">2.11</td>
<td style="text-align: center;">34.3</td>
<td style="text-align: center;">16.7</td>
<td style="text-align: center;">20.63 k</td>
<td style="text-align: center;">34.94 k</td>
<td style="text-align: center;">43.02 k</td>
<td style="text-align: center;">2.56</td>
<td style="text-align: center;">2.57</td>
</tr>
<tr>
<td style="text-align: center;">beam search $+\mathrm{A}^{*}$ esque (beam)</td>
<td style="text-align: center;">2.14</td>
<td style="text-align: center;">34.4</td>
<td style="text-align: center;">16.8</td>
<td style="text-align: center;">20.68 k</td>
<td style="text-align: center;">35.03 k</td>
<td style="text-align: center;">43.12 k</td>
<td style="text-align: center;">2.62</td>
<td style="text-align: center;">2.63</td>
</tr>
<tr>
<td style="text-align: center;">beam search $+\mathrm{A}^{*}$ esque (sample)</td>
<td style="text-align: center;">2.16</td>
<td style="text-align: center;">34.4</td>
<td style="text-align: center;">16.7</td>
<td style="text-align: center;">20.78k</td>
<td style="text-align: center;">35.41k</td>
<td style="text-align: center;">43.64k</td>
<td style="text-align: center;">2.59</td>
<td style="text-align: center;">2.57</td>
</tr>
<tr>
<td style="text-align: center;">top-k sample</td>
<td style="text-align: center;">4.01</td>
<td style="text-align: center;">31.4</td>
<td style="text-align: center;">13.9</td>
<td style="text-align: center;">28.54k</td>
<td style="text-align: center;">48.36k</td>
<td style="text-align: center;">56.62k</td>
<td style="text-align: center;">2.23</td>
<td style="text-align: center;">2.15</td>
</tr>
<tr>
<td style="text-align: center;">top-k sample $+\mathrm{A}^{*}$ esque (greedy)</td>
<td style="text-align: center;">3.68</td>
<td style="text-align: center;">32.1</td>
<td style="text-align: center;">14.3</td>
<td style="text-align: center;">28.47k</td>
<td style="text-align: center;">48.44k</td>
<td style="text-align: center;">56.63k</td>
<td style="text-align: center;">2.48</td>
<td style="text-align: center;">2.47</td>
</tr>
<tr>
<td style="text-align: center;">top-k sample $+\mathrm{A}^{*}$ esque (beam)</td>
<td style="text-align: center;">3.75</td>
<td style="text-align: center;">32.2</td>
<td style="text-align: center;">14.4</td>
<td style="text-align: center;">28.53k</td>
<td style="text-align: center;">48.27k</td>
<td style="text-align: center;">56.36k</td>
<td style="text-align: center;">2.39</td>
<td style="text-align: center;">2.34</td>
</tr>
<tr>
<td style="text-align: center;">top-k sample $+\mathrm{A}^{*}$ esque (sample)</td>
<td style="text-align: center;">3.70</td>
<td style="text-align: center;">32.0</td>
<td style="text-align: center;">14.2</td>
<td style="text-align: center;">28.57k</td>
<td style="text-align: center;">48.04k</td>
<td style="text-align: center;">56.15k</td>
<td style="text-align: center;">2.47</td>
<td style="text-align: center;">2.44</td>
</tr>
</tbody>
</table>
<p>Table 8: Performance of different decoding algorithms on RocStories test set.
lookahead provides an estimate which better resembles a continuation from top- $k$ sampling.</p>
<h2>4 Related Work</h2>
<p>A<em> search in NLP. Many classical NLP problems (e.g., parsing, text alignment) can be seen as structured prediction subject to a set of taskspecific constraints. For many such problems, A</em> search has been used effectively (Och et al., 2001; Haghighi et al., 2007; Hopkins and Langmead, 2009; Meister et al., 2020). For example, Klein and Manning (2003); Zhang and Gildea (2006); Auli and Lopez (2011); Lee et al. (2016) have used it in the context of parsing. Similar approaches are used for finding high-probability alignments (Naim et al., 2013). Despite these applications, applying informed heuristic search to text generation with autoregressive language models (this work's focus) has been underexplored.</p>
<p>Decoding strategies for text generation. The rise of autoregressive language models like GPT (Radford et al., 2018) has inspired work on decoding strategies (Post and Vilar, 2018a; Ippolito et al., 2019; Zheng et al., 2020; Leblond et al., 2021; West et al., 2021). These works often focus on incorporating factors like diversity (Ippolito et al., 2019), fluency (Holtzman et al., 2020), or constraints (Anderson et al., 2017; Hokamp and Liu, 2017; Post and Vilar, 2018b; Miao et al., 2019; Welleck et al., 2019a; Zhang et al., 2020; Qin et al., 2020; Lu et al., 2021). Constrained
beam search (Anderson et al., 2017) and grid beam search (Hokamp and Liu, 2017) extend beam search to satisfy lexical constraints during generation. Lu et al. (2021) incorporate logic-based constraints into beam search, which we extend with lookahead heuristics.</p>
<p>Other work addresses the mismatch between monotonic decoding and satisfying constraints that can depend on a full generation, through MCMC sampling (Miao et al., 2019; Zhang et al., 2020), recursive non-monotonic generation (Welleck et al., 2019a), continuous optimization (Qin et al., 2020), or generated contexts (West et al., 2021). Unlike these past works, NEUROLOGIC A ${ }^{\text {® }}$ esque explicitly decodes future text to estimate the viability of different paths for satisfying constraints.</p>
<h2>5 Conclusion</h2>
<p>Inspired by the A<em> search algorithm, we introduce NEUROLOGIC A ${ }^{\text {® }}$ esque decoding, which brings A</em>-like heuristic estimates of the future to common left-to-right decoding algorithms for neural text generation. A ${ }^{\text {® }}$ esque lookahead heuristics improve over existing decoding methods (e.g., NEUROLOGIC, beam, greedy, sample decoding methods) in both constrained and unconstrained settings across a wide spectrum of tasks. Our work demonstrates the promise of moving beyond the current paradigm of unidirectional decoding for text generation, by taking bidirectional information from both the past and future into account to generate more globally coherent text.</p>
<h2>Acknowledgment</h2>
<p>This work was supported in part by the Natural Sciences and Engineering Research Council of Canada (NSERC) (funding reference number 401233309), DARPA MCS program through NIWC Pacific (N66001-19-2-4031), Google Cloud Compute, and Allen Institute for AI, Microsoft PhD Fellowship.</p>
<h2>Broader Impact and Ethical Implications</h2>
<p>Our method deals with improving neural text generation, thus inheriting the potential impact and risks brought by text generation applications (e.g. dual use, see Pandya (2019); Brown et al. (2020)). Constraining generation through logical constraints offers the promise of improved control, consistency, and human-machine collaboration in highimpact applications such as translation, machineaided writing, and education. On the other hand, constrained generation methods could foreseeably be used to generate text that contains biased, offensive, and/or hateful keywords (e.g., extremist texts; McGuffie and Newhouse, 2020). For a broader discussion of these risks, and of the risks of large pretrained language models in general, refer to discussions in Brown et al. (2020); Bender et al. (2021).</p>
<h2>References</h2>
<p>Peter Anderson, Basura Fernando, Mark Johnson, and Stephen Gould. 2016. Spice: Semantic propositional image caption evaluation. In European conference on computer vision, pages 382-398. Springer.</p>
<p>Peter Anderson, Basura Fernando, Mark Johnson, and Stephen Gould. 2017. Guided open vocabulary image captioning with constrained beam search. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 936-945, Copenhagen, Denmark. Association for Computational Linguistics.</p>
<p>Michael Auli and Adam Lopez. 2011. Efficient CCG parsing: A* versus adaptive supertagging. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 1577-1585, Portland, Oregon, USA. Association for Computational Linguistics.</p>
<p>Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization, pages 65-72.</p>
<p>Emily Bender, Timnit Gebru, Angelina McMillanMajor, and Shmargaret Shmitchell. 2021. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency (FAccT).</p>
<p>Ondřej Bojar, Rajen Chatterjee, Christian Federmann, Yvette Graham, Barry Haddow, Shujian Huang, Matthias Huck, Philipp Koehn, Qun Liu, Varvara Logacheva, Christof Monz, Matteo Negri, Matt Post, Raphael Rubino, Lucia Specia, and Marco Turchi. 2017. Findings of the 2017 conference on machine translation (WMT17). In Proceedings of the Second Conference on Machine Translation, pages 169214, Copenhagen, Denmark. Association for Computational Linguistics.
T. Brown, B. Mann, Nick Ryder, Melanie Subbiah, J. Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, G. Krüger, T. Henighan, R. Child, Aditya Ramesh, D. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, E. Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, J. Clark, Christopher Berner, Sam McCandlish, A. Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems (NeurIPS).</p>
<p>Rajen Chatterjee, Matteo Negri, Marco Turchi, Marcello Federico, Lucia Specia, and Frédéric Blain. 2017. Guiding neural machine translation decoding with external knowledge. In Proceedings of the Second Conference on Machine Translation, pages 157168, Copenhagen, Denmark. Association for Computational Linguistics.</p>
<p>Wenhu Chen, Jianshu Chen, Yu Su, Zhiyu Chen, and William Yang Wang. 2020a. Logical natural language generation from open-domain tables. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 79297942, Online. Association for Computational Linguistics.</p>
<p>Wenhu Chen, Yu Su, Xifeng Yan, and William Yang Wang. 2020b. KGPT: Knowledge-grounded pretraining for data-to-text generation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 8635-8648, Online. Association for Computational Linguistics.</p>
<p>Yining Chen, Sorcha Gilroy, Andreas Maletti, Jonathan May, and Kevin Knight. 2018. Recurrent neural networks as weighted language recognizers. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 2261-2271, New Orleans, Louisiana. Association for Computational Linguistics.</p>
<p>Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino, Jason Yosinski, and Rosanne Liu. 2019. Plug and play language models: A simple approach to controlled text generation. In International Conference on Learning Representations.</p>
<p>Georgiana Dinu, Prashant Mathur, Marcello Federico, and Yaser Al-Onaizan. 2019. Training neural machine translation to apply terminology constraints. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3063-3068, Florence, Italy. Association for Computational Linguistics.</p>
<p>Ondřej Dušek and Filip Jurčíček. 2016. Sequence-tosequence generation for spoken dialogue via deep syntax trees and strings. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 45-51, Berlin, Germany. Association for Computational Linguistics.</p>
<p>Ondřej Dušek, Jekaterina Novikova, and Verena Rieser. 2018. Findings of the E2E NLG Challenge. In Proc. of the 11th International Conference on Natural Language Generation, pages 322-328, Tilburg, The Netherlands. Association for Computational Linguistics.</p>
<p>Angela Fan, Mike Lewis, and Yann Dauphin. 2018. Hierarchical neural story generation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 889-898, Melbourne, Australia. Association for Computational Linguistics.</p>
<p>Aria Haghighi, John DeNero, and Dan Klein. 2007. Approximate factoring for A* search. In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference, pages 412-419, Rochester, New York. Association for Computational Linguistics.</p>
<p>Peter E. Hart, Nils J. Nilsson, and Bertram Raphael. 1968. A formal basis for the heuristic determination of minimum cost paths. IEEE Transactions on Systems Science and Cybernetics, 4(2):100-107.</p>
<p>Eva Hasler, Adrià de Gispert, Gonzalo Iglesias, and Bill Byrne. 2018. Neural machine translation decoding with terminology constraints. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 506-512, New Orleans, Louisiana. Association for Computational Linguistics.</p>
<p>Chris Hokamp and Qun Liu. 2017. Lexically constrained decoding for sequence generation using grid beam search. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1535-1546, Vancouver, Canada. Association for Computational Linguistics.</p>
<p>Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2020. The curious case of neural text degeneration. In International Conference on Learning Representations.</p>
<p>Mark Hopkins and Greg Langmead. 2009. Cube pruning as heuristic search. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 62-71.
J. Edward Hu, Huda Khayrallah, Ryan Culkin, Patrick Xia, Tongfei Chen, Matt Post, and Benjamin Van Durme. 2019. Improved lexically constrained decoding for translation and monolingual rewriting. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 839-850, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Zhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan Salakhutdinov, and Eric P Xing. 2017. Toward controlled generation of text. In International Conference on Machine Learning, pages 1587-1596. PMLR.</p>
<p>Daphne Ippolito, Reno Kriz, João Sedoc, Maria Kustikova, and Chris Callison-Burch. 2019. Comparison of diverse decoding methods from conditional language models. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3752-3762.</p>
<p>Marcin Junczys-Dowmunt, Roman Grundkiewicz, Tomasz Dwojak, Hieu Hoang, Kenneth Heafield, Tom Neckermann, Frank Seide, Ulrich Germann, Alham Fikri Aji, Nikolay Bogoychev, André F. T. Martins, and Alexandra Birch. 2018. Marian: Fast neural machine translation in C++. In Proceedings of ACL 2018, System Demonstrations, pages 116121, Melbourne, Australia. Association for Computational Linguistics.</p>
<p>Dan Klein and Christopher D. Manning. 2003. A* parsing: Fast exact Viterbi parse selection. In Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, pages 119126 .</p>
<p>Richard E Korf. 1985. Depth-first iterative-deepening: An optimal admissible tree search. Artificial intelligence, 27(1):97-109.</p>
<p>Klaus Krippendorff. 2007. Computing krippendorff's alpha reliability. Departmental papers (ASC), page 43 .</p>
<p>Rémi Leblond, Jean-Baptiste Alayrac, Laurent Sifre, Miruna Pislar, Jean-Baptiste Lespiau, Ioannis Antonoglou, Karen Simonyan, and Oriol Vinyals. 2021. Machine translation decoding beyond beam search. arXiv preprint arXiv:2104.05336.</p>
<p>Kenton Lee, Mike Lewis, and Luke Zettlemoyer. 2016. Global neural CCG parsing with optimality guarantees. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2366-2376, Austin, Texas. Association for Computational Linguistics.</p>
<p>Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning: Optimizing continuous prompts for generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4582-4597, Online. Association for Computational Linguistics.</p>
<p>Bill Yuchen Lin, Ming Shen, Wangchunshu Zhou, Pei Zhou, Chandra Bhagavatula, Yejin Choi, and Xiang Ren. 2020. Commongen: A constrained text generation challenge for generative commonsense reasoning. In Findings of EMNLP.</p>
<p>Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. Text Summarization Branches Out.</p>
<p>Chin-Yew Lin and Eduard Hovy. 2003. Automatic evaluation of summaries using n-gram cooccurrence statistics. In Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, pages 150-157.</p>
<p>Ximing Lu, Peter West, Rowan Zellers, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2021. NeuroLogic decoding: (un)supervised neural text generation with predicate logic constraints. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4288-4299, Online. Association for Computational Linguistics.</p>
<p>Kris McGuffie and Alex Newhouse. 2020. The radicalization risks of gpt-3 and advanced neural language models. arXiv.</p>
<p>Clara Meister, Tim Vieira, and Ryan Cotterell. 2020. Best-first beam search. Transactions of the Association for Computational Linguistics, 8:795-809.</p>
<p>Ning Miao, Hao Zhou, Lili Mou, Rui Yan, and Lei Li. 2019. Cgmh: Constrained sentence generation by metropolis-hastings sampling. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 6834-6842.</p>
<p>Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende, Pushmeet Kohli, and James Allen. 2016. A corpus and cloze evaluation for deeper understanding of commonsense stories. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 839-849, San Diego, California. Association for Computational Linguistics.</p>
<p>Iftekhar Naim, Daniel Gildea, Walter Lasecki, and Jeffrey P Bigham. 2013. Text alignment for real-time crowd captioning. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 201-210.</p>
<p>Franz Josef Och, Nicola Ueffing, and Hermann Ney. 2001. An efficient a* search algorithm for statistical machine translation. In Proceedings of the ACL 2001 Workshop on Data-Driven Methods in Machine Translation.</p>
<p>Jayshree Pandya. 2019. The dual-use dilemma of artificial intelligence. Forbes Magazine.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In $A C L$, pages 311318.</p>
<p>Judea Pearl. 1984. Heuristics - intelligent search strategies for computer problem solving. In AddisonWesley series in artificial intelligence.</p>
<p>Ira Pohl. 1970. First Results on the Effect of Error in Heuristic Search.</p>
<p>Matt Post and David Vilar. 2018a. Fast lexically constrained decoding with dynamic beam allocation for neural machine translation. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1314-1324, New Orleans, Louisiana. Association for Computational Linguistics.</p>
<p>Matt Post and David Vilar. 2018b. Fast lexically constrained decoding with dynamic beam allocation for neural machine translation. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1314-1324.</p>
<p>Lianhui Qin, Vered Shwartz, Peter West, Chandra Bhagavatula, Jena D Hwang, Ronan Le Bras, Antoine Bosselut, and Yejin Choi. 2020. Backpropagationbased decoding for unsupervised counterfactual and abductive reasoning. In EMNLP.</p>
<p>Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language understanding by generative pre-training.</p>
<p>Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. OpenAI Blog, 1(8):9.</p>
<p>Sheng Shen, Daniel Fried, Jacob Andreas, and Dan Klein. 2019. Pragmatically informative text generation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers),</p>
<p>pages 4060-4067, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. 2015. Cider: Consensus-based image description evaluation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4566-4575.</p>
<p>Sean Welleck, Kianté Brantley, Hal Daumé Iii, and Kyunghyun Cho. 2019a. Non-monotonic sequential text generation. In International Conference on Machine Learning, pages 6716-6726. PMLR.</p>
<p>Sean Welleck, Ilia Kulikov, Stephen Roller, Emily Dinan, Kyunghyun Cho, and Jason Weston. 2019b. Neural text generation with unlikelihood training. In International Conference on Learning Representations.</p>
<p>Peter West, Ximing Lu, Ari Holtzman, Chandra Bhagavatula, Jena D. Hwang, and Yejin Choi. 2021. Reflective decoding: Beyond unidirectional generation with off-the-shelf language models. In ACL/IJCNLP.</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP): System Demonstrations.</p>
<p>Hao Zhang and Daniel Gildea. 2006. Efficient search for inversion transduction grammar. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pages 224-231.</p>
<p>Maosen Zhang, Nan Jiang, Lei Li, and Yexiang Xue. 2020. Language generation via combinatorial constraint satisfaction: A tree search enhanced MonteCarlo approach. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1286-1298, Online. Association for Computational Linguistics.</p>
<p>Renjie Zheng, Mingbo Ma, Baigong Zheng, Kaibo Liu, and Liang Huang. 2020. Opportunistic decoding with timely correction for simultaneous translation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 437442.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Effect of varying the primary hyperparameter for each lookahead strategy ( $\S 2.1$ ) - (a) greedy (lookahead length), (b) soft (temperature), (c) beam (number of beams), and (d) sample (number of samples). Performance is measured on the CommonGen validation set, using BLEU-4 and Coverage.</p>
<h2>A Further Experiments</h2>
<h2>A. 1 Constrained Commonsense Generation</h2>
<p>Studying Lookahead Strategies. We further use CommonGen to study the lookahead strategies for Neurologic ${ }^{\star}$ proposed in $\S 2.1$ (Figure 3). With infinite lookahead length $\ell$ and number of lookaheads $\left|\mathcal{L}<em _ell="\ell">{\ell}\right|$, lookahead decoding exactly solves Equation 3, finding an optimal trajectory. In practice these are finite, meaning that the quality of the lookahead approximation can depend on the lookahead strategy and its hyperparameters. For practical choices of $\ell$ and $\left|\mathcal{L}</em>\right|$, we empirically study how varying the lookahead strategy and hyperparameters affects performance. In Figure 3, we study the greedy, soft, beam, and sampling lookahead strategies.</p>
<p>Figure 3(a) shows the effect of increasing the lookahead length $\ell$ for the greedy lookahead strategy. Increasing the length improves up to one point - e.g., 5-7 steps - then decreases thereafter, likely due to the difficulty of long-horizon approximation.</p>
<p>Figure 3(b) studies the temperature in the soft lookahead, showing that greedy $(\tau=0.0)$ performs well, with slight gains if $\tau$ is carefully selected. The results suggest that one can safely bypass tuning $\tau$ using fast, greedy lookahead.</p>
<p>Next, Figure 3(c) shows that with beam lookahead, increasing the beam width improves performance up to a certain point (here, 11). Similarly, increasing the number of samples with sampling
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Likelihood (y-axis) vs. number of unique 3grams (x-axis) using supervised GPT-2 on RocStories. Figure (a) denotes decoding with beam search, with a varying amount of beam size. Figure (b) denotes decoding with top-k sampling, with a varying amount of k value. The brown and blue lines denote with and without $\mathrm{A}^{\star}$ esque heuristics separately.
lookahead improves over a single sample, and then reaches an inflection point (Figure 3(d)).</p>
<h2>A. 2 Unconstrained Story Generation</h2>
<p>Fluency and Diversity Tradeoff We study the effect of $\mathrm{A}^{\star}$ esque decoding in unconstrained generation with different decoding hyperparameters: beam size in beam search and k value in top-k sampling. Figure 4 plots the fluency (measured by likelihood) versus diversity (measured by unique 3-grams) for generations with various beam sizes or top-k values. Ideally, we want generations to be both fluent and diverse (top right). However, we observe a fluency and diversity tradeoff in practice. $\mathrm{A}^{\star}$ esque decoding flattens this trend and results in larger area under the curve. The effect is especially strong with beam search. In summary, $\mathrm{A}^{\star}$ esque decoding yields a more favorable balance of fluency and diversity compared to conventional decoding methods, regardless of hyperparameters.</p>
<h2>B Runtime</h2>
<table>
<thead>
<tr>
<th style="text-align: center;">Decoding Method</th>
<th style="text-align: center;">Runtime</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Beam Search</td>
<td style="text-align: center;">0.20</td>
</tr>
<tr>
<td style="text-align: center;">Neurologic</td>
<td style="text-align: center;">2.04</td>
</tr>
<tr>
<td style="text-align: center;">NeuroLogic A*esque</td>
<td style="text-align: center;">19.24</td>
</tr>
</tbody>
</table>
<p>Table 9: Runtime (seconds per sentence) of different decoding algorithms with finetuned GPT2-L on the CommonGen dataset</p>
<h2>C Experimental Details</h2>
<h2>C. 1 Off-the-Shelf Models</h2>
<p>We download off-the-shelf models, including pretrained GPT-2 and Marian MT, from HuggingFace</p>
<p>Transformers (Wolf et al., 2020), which are implemented in the PyTorch deep learning framework.</p>
<h2>C. 2 Model Training Details</h2>
<p>All training is performed on a single NVIDIA Quadro RTX 8000 GPU and costs about 100 GPU hours in total. Our method is implemented with PyTorch an the Huggingface Transformers library.</p>
<h2>C.2.1 CommonGen</h2>
<p>For supervised setting, we finetune GPT-2 for conditional generation. We follow Lu et al. (2021)'s setup and use their hyperparameters for finetuning, as shown in Table 10.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Hyperparameter</th>
<th style="text-align: center;">Assignment</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">model</td>
<td style="text-align: center;">GPT2-Large</td>
</tr>
<tr>
<td style="text-align: left;">number of parameters</td>
<td style="text-align: center;">774 M</td>
</tr>
<tr>
<td style="text-align: left;">number of steps</td>
<td style="text-align: center;">15 epochs</td>
</tr>
<tr>
<td style="text-align: left;">batch size</td>
<td style="text-align: center;">64</td>
</tr>
<tr>
<td style="text-align: left;">learning rate optimizer</td>
<td style="text-align: center;">Adam</td>
</tr>
<tr>
<td style="text-align: left;">Adam epsilon</td>
<td style="text-align: center;">$1 \mathrm{e}-8$</td>
</tr>
<tr>
<td style="text-align: left;">Adam initial learning rate</td>
<td style="text-align: center;">$1 \mathrm{e}-5$</td>
</tr>
<tr>
<td style="text-align: left;">learning rate scheduler</td>
<td style="text-align: center;">linear with warmup</td>
</tr>
<tr>
<td style="text-align: left;">warmup steps</td>
<td style="text-align: center;">1.5 epoch</td>
</tr>
<tr>
<td style="text-align: left;">weight decay</td>
<td style="text-align: center;">0</td>
</tr>
</tbody>
</table>
<p>Table 10: Hyperparameters for finetuning GPT-2 on CommonGen dataset.</p>
<h2>C.2.2 Constrained Machine Translation</h2>
<p>For fair comparison, we reproduced MT model (two-layer transformer) used by Dinu et al. (2019), using the same setup and hyperparameters reported in their original paper.</p>
<h2>C.2.3 Table-to-text Generation</h2>
<p>We finetune GPT-2 with random sampled few-shot training instances from E2ENLG dataset. We used the same hyperparameters for finetuning with Li and Liang (2021), as shown in Table 11.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Hyperparameter</th>
<th style="text-align: center;">Assignment</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">model</td>
<td style="text-align: center;">GPT2-Large</td>
</tr>
<tr>
<td style="text-align: left;">number of parameters</td>
<td style="text-align: center;">774 M</td>
</tr>
<tr>
<td style="text-align: left;">number of steps</td>
<td style="text-align: center;">5 epochs</td>
</tr>
<tr>
<td style="text-align: left;">batch size</td>
<td style="text-align: center;">5</td>
</tr>
<tr>
<td style="text-align: left;">learning rate optimizer</td>
<td style="text-align: center;">Adam</td>
</tr>
<tr>
<td style="text-align: left;">Adam epsilon</td>
<td style="text-align: center;">$1 \mathrm{e}-8$</td>
</tr>
<tr>
<td style="text-align: left;">Adam initial learning rate</td>
<td style="text-align: center;">$5 \mathrm{e}-5$</td>
</tr>
<tr>
<td style="text-align: left;">learning rate scheduler</td>
<td style="text-align: center;">linear with warmup</td>
</tr>
<tr>
<td style="text-align: left;">warmup steps</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: left;">weight decay</td>
<td style="text-align: center;">0</td>
</tr>
</tbody>
</table>
<p>Table 11: Hyperparameters for finetuning GPT-2 on E2ENLG dataset.</p>
<h2>C.2.4 Unconstrained Story Generation</h2>
<p>We finetune GPT-2 for conditional story generation on the RocStories dataset: given a first sentence $\mathbf{x}$, generate the full story $\mathbf{y}$. Hyperparameters for finetuning are given in Table 12.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Hyperparameter</th>
<th style="text-align: center;">Assignment</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">model</td>
<td style="text-align: center;">GPT2-Large</td>
</tr>
<tr>
<td style="text-align: left;">number of parameters</td>
<td style="text-align: center;">774 M</td>
</tr>
<tr>
<td style="text-align: left;">number of steps</td>
<td style="text-align: center;">10 epochs</td>
</tr>
<tr>
<td style="text-align: left;">batch size</td>
<td style="text-align: center;">64</td>
</tr>
<tr>
<td style="text-align: left;">learning rate optimizer</td>
<td style="text-align: center;">Adam</td>
</tr>
<tr>
<td style="text-align: left;">Adam epsilon</td>
<td style="text-align: center;">$1 \mathrm{e}-8$</td>
</tr>
<tr>
<td style="text-align: left;">Adam initial learning rate</td>
<td style="text-align: center;">$1 \mathrm{e}-5$</td>
</tr>
<tr>
<td style="text-align: left;">learning rate scheduler</td>
<td style="text-align: center;">linear with warmup</td>
</tr>
<tr>
<td style="text-align: left;">warmup steps</td>
<td style="text-align: center;">1 epoch</td>
</tr>
<tr>
<td style="text-align: left;">weight decay</td>
<td style="text-align: center;">0</td>
</tr>
</tbody>
</table>
<p>Table 12: Hyperparameters for finetuning GPT-2 on the RocStories dataset.</p>
<h2>C. 3 Generation Details</h2>
<p>All generation is performed on a single NVIDIA Quadro RTX 8000 GPU and costs about 100 GPU hours in total.</p>
<h2>C.3.1 CommonGen</h2>
<p>NeuroLogic ${ }^{\circledR}$ hyperparameters for CommonGen in supervised and zero-shot setting are shown in Table 13 and Table 14 separately. We use the same NEUROLOGIC hyperparameters with Lu et al. (2021), including beam size, $\alpha, \beta$ and $\lambda_{1}$. We performed a hyperparameter grid search for the scaling factor $\lambda_{2}$ over the range $[0,0.3]$, for the look ahead step over the the range $[1,15]$, for the look ahead temperature over the the range $[0,1.0]$, for the look ahead beam width over the the range $[1,10]$, and for the look ahead number of sample over the the range $[1,10]$, using a small subset of CommonGen development set.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Hyperparameter</th>
<th style="text-align: center;">Assignment</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">beam size</td>
<td style="text-align: center;">20</td>
</tr>
<tr>
<td style="text-align: left;">pruning threshold $\alpha$</td>
<td style="text-align: center;">50</td>
</tr>
<tr>
<td style="text-align: left;">pruning threshold $\beta$</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: left;">scaling factor $\lambda_{1}$</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: left;">scaling factor $\lambda_{2}$</td>
<td style="text-align: center;">0.25</td>
</tr>
<tr>
<td style="text-align: left;">look ahead step</td>
<td style="text-align: center;">5</td>
</tr>
<tr>
<td style="text-align: left;">look ahead (greedy) temperature</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: left;">look ahead (beam) beam width</td>
<td style="text-align: center;">5</td>
</tr>
<tr>
<td style="text-align: left;">look ahead (sample) number of sample</td>
<td style="text-align: center;">4</td>
</tr>
</tbody>
</table>
<p>Table 13: NEUROLOGIC ${ }^{\circledR}$ hyperparameters for CommonGen in supervised setting.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Hyperparameter</th>
<th style="text-align: center;">Assignment</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">beam size</td>
<td style="text-align: center;">20</td>
</tr>
<tr>
<td style="text-align: left;">pruning threshold $\alpha$</td>
<td style="text-align: center;">500000</td>
</tr>
<tr>
<td style="text-align: left;">pruning threshold $\beta$</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: left;">scaling factor $\lambda_{1}$</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: left;">scaling factor $\lambda_{2}$</td>
<td style="text-align: center;">0.175</td>
</tr>
<tr>
<td style="text-align: left;">look ahead step</td>
<td style="text-align: center;">5</td>
</tr>
<tr>
<td style="text-align: left;">look ahead (greedy) temperature</td>
<td style="text-align: center;">0</td>
</tr>
</tbody>
</table>
<p>Table 14: Neurologic* hyperparameters for CommonGen in zero-shot setting.</p>
<h3>C.3.2 Constrained Machine Translation</h3>
<p>NeuroLogic ${ }^{\circledR}$ hyperparameters for constrained machine translation are shown in Table 15. We use the same beam size with Dinu et al. (2019) for fair comparison. We performed a hyperparameter grid search for the pruning threshold $\alpha$ over the range $[50,300]$, for the pruning threshold $\beta$ over the range $[1,3]$, for the scaling factor $\lambda_{1}$ over the range $[0,1.0]$, for the scaling factor $\lambda_{2}$ over the range $[0,0.3]$, for the look ahead step over the the range $[5,40]$, using a subset of WMT2013 IATE development set. We use the same hyperparameters for look ahead temperature, look ahead beam width, and look ahead number of sample with supervised CommonGen and omit the hyperparameter search due to the computational cost.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Hyperparameter</th>
<th style="text-align: center;">Assignment</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">beam size</td>
<td style="text-align: center;">5</td>
</tr>
<tr>
<td style="text-align: left;">pruning threshold $\alpha$</td>
<td style="text-align: center;">200</td>
</tr>
<tr>
<td style="text-align: left;">pruning threshold $\beta$</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: left;">scaling factor $\lambda_{1}$</td>
<td style="text-align: center;">0.25</td>
</tr>
<tr>
<td style="text-align: left;">scaling factor $\lambda_{2}$</td>
<td style="text-align: center;">0.05</td>
</tr>
<tr>
<td style="text-align: left;">look ahead step</td>
<td style="text-align: center;">35</td>
</tr>
<tr>
<td style="text-align: left;">look ahead (greedy) temperature</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: left;">look ahead (beam) beam width</td>
<td style="text-align: center;">5</td>
</tr>
<tr>
<td style="text-align: left;">look ahead (sample) number of sample</td>
<td style="text-align: center;">4</td>
</tr>
</tbody>
</table>
<p>Table 15: Neurologic ${ }^{\circledR}$ hyperparameters for constrained machine translation.</p>
<h3>C.3.3 Table-to-text Generation</h3>
<p>NeUrologic ${ }^{\circledR}$ hyperparameters for table-to-text generation are shown in Table 16. We performed a hyperparameter grid search for the scaling factor $\lambda_{2}$ over the range $[0,0.3]$, for the look ahead step over the the range $[1,15]$, using E2ENLG development set. For other hyperparameters, we use the same value with supervised CommonGen and omit the hyperparameter search due to the computational cost.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Hyperparameter</th>
<th style="text-align: center;">Assignment</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">beam size</td>
<td style="text-align: center;">20</td>
</tr>
<tr>
<td style="text-align: left;">pruning threshold $\alpha$</td>
<td style="text-align: center;">50</td>
</tr>
<tr>
<td style="text-align: left;">pruning threshold $\beta$</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: left;">scaling factor $\lambda_{1}$</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: left;">scaling factor $\lambda_{2}$</td>
<td style="text-align: center;">0.05</td>
</tr>
<tr>
<td style="text-align: left;">look ahead step</td>
<td style="text-align: center;">7</td>
</tr>
<tr>
<td style="text-align: left;">look ahead (greedy) temperature</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: left;">look ahead (beam) beam width</td>
<td style="text-align: center;">5</td>
</tr>
<tr>
<td style="text-align: left;">look ahead (sample) number of sample</td>
<td style="text-align: center;">4</td>
</tr>
</tbody>
</table>
<p>Table 16: Neurologic ${ }^{\circledR}$ hyperparameters for table-to-text generation.</p>
<h3>C.3.4 Constrained Question Generation</h3>
<p>NeUROLOGIC ${ }^{\circledR}$ hyperparameters for constrained question generation are shown in Table 17. The task is zero-shot and doesn't provide train or development set, so we use the same decoding hyperparameters with zero-shot CommonGen.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Hyperparameter</th>
<th style="text-align: center;">Assignment</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">beam size</td>
<td style="text-align: center;">20</td>
</tr>
<tr>
<td style="text-align: left;">pruning threshold $\alpha$</td>
<td style="text-align: center;">500000</td>
</tr>
<tr>
<td style="text-align: left;">pruning threshold $\beta$</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: left;">scaling factor $\lambda_{1}$</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: left;">scaling factor $\lambda_{2}$</td>
<td style="text-align: center;">0.175</td>
</tr>
<tr>
<td style="text-align: left;">look ahead step</td>
<td style="text-align: center;">5</td>
</tr>
<tr>
<td style="text-align: left;">look ahead (greedy) temperature</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: left;">look ahead (beam) beam width</td>
<td style="text-align: center;">5</td>
</tr>
<tr>
<td style="text-align: left;">look ahead (sample) number of sample</td>
<td style="text-align: center;">4</td>
</tr>
</tbody>
</table>
<p>Table 17: Neurologic ${ }^{\circledR}$ hyperparameters for constrained question generation.</p>
<h3>C.3.5 Unconstrained Story Generation</h3>
<p>A* esque hyperparameters with beam search and top-k sampling for unconstrained story generation are shown in Table 18 and Table 19 separately. We performed a hyperparameter grid search for the scaling factor $\lambda_{2}$ over the range $[0,1.0]$, for the look ahead step over the the range $[1,15]$, for the look ahead temperature over the the range $[0,1.0]$, for the look ahead beam width over the the range $[1,15]$, and for the look ahead number of sample over the the range $[1,15]$, using a small subset of RocStories development set.</p>
<h3>11.4 Dataset Details</h3>
<p>Details of datasets used for downstream tasks are provided in Table 22.</p>
<h2>D Human Evaluation</h2>
<p>We include screenshots of the human evaluation templates for CommonGen (Figure 5), Constrained</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Hyperparameter</th>
<th style="text-align: center;">Assignment</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">beam size</td>
<td style="text-align: center;">4</td>
</tr>
<tr>
<td style="text-align: left;">scaling factor $\lambda_{2}$</td>
<td style="text-align: center;">0.6</td>
</tr>
<tr>
<td style="text-align: left;">look ahead step</td>
<td style="text-align: center;">4</td>
</tr>
<tr>
<td style="text-align: left;">look ahead (greedy) temperature</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: left;">look ahead (beam) beam width</td>
<td style="text-align: center;">4</td>
</tr>
<tr>
<td style="text-align: left;">look ahead (sample) number of sample</td>
<td style="text-align: center;">15</td>
</tr>
</tbody>
</table>
<p>Table 18: A* ${ }^{\star}$ esque hyperparameters with beam search for unconstrained story generation.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Hyperparameter</th>
<th style="text-align: center;">Assignment</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">k value</td>
<td style="text-align: center;">5</td>
</tr>
<tr>
<td style="text-align: left;">scaling factor $\lambda_{2}$</td>
<td style="text-align: center;">0.5</td>
</tr>
<tr>
<td style="text-align: left;">look ahead step</td>
<td style="text-align: center;">3</td>
</tr>
<tr>
<td style="text-align: left;">look ahead (greedy) temperature</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: left;">look ahead (beam) beam width</td>
<td style="text-align: center;">4</td>
</tr>
<tr>
<td style="text-align: left;">look ahead (sample) number of sample</td>
<td style="text-align: center;">15</td>
</tr>
</tbody>
</table>
<p>Table 19: A* ${ }^{\star}$ esque hyperparameters with top-k sampling for unconstrained story generation.</p>
<p>Question Generation (Figure 6), and RocStories (Figure 7) tasks. We ensure the annotators are paid adequately for at least $\$ 15$ per hour and we inform annotators that their annotations are used for model evaluation purpose.</p>
<h1>E Qualitative Generation Examples</h1>
<p>Qualitative examples of the constrained question generation and unconstrained story generation are shown in Table 21 and 20.</p>
<h2>F Limitations and Risks.</h2>
<p>Limitations. For constrained generation, NeuROLOGIC A* ${ }^{\star}$ esque decoding can only take the constraints that can be formulated as logical expressions as described in the paper; we leave it to future work to expand the scope of such logical constraints.</p>
<p>Risks. Constrained generation methods could foreseeably be used to generate text that contains biased, offensive, and/or hateful keywords. (e.g., extremist texts; McGuffie and Newhouse, 2020). For a broader discussion of these risks, and of the risks of large pretrained language models in general, refer to discussions in Brown et al. (2020); Bender et al. (2021).</p>
<p>Concepts:</p>
<h1>(Source)</h1>
<p>Sentence:</p>
<h2>1. SENTENCE QUALITY Is the sentence well-formed?</h2>
<p>Yes: The sentence is well-formed and fluent.
Somewhat: The sentence is understandable but a bit awkward.
No: The sentence is neither well-formed or fluent.
2. PLAUSIBILITY Does the sentence describe a plausible scenario?</p>
<p>Yes: The sentence describes a realistic or plausible scenario.
Somewhat: The sentence describes a acceptable scenario but a bit awkward.
No: The sentence describes a nonsensical scenario.
3. CONCEPTS Does the sentence include the given concepts meaningfully?</p>
<p>Yes: The sentence meaningfully includes all of the concepts.
Somewhat: The sentence meaningfully includes some, but not all of the concepts. Or, the sentence includes all concepts but some of them are not meaningful or properly incorporated.
No: The sentence does not include concepts in a meaningful way.
4. OVERALL Considering your answers to 1., 2. and 3., Does the sentence meaningfully combine all of the concepts into a well-formed and plausible scenario?</p>
<p>Yes: The sentence is reasonably well-formed/understandable, and meaningfully combines all the concepts into a plausible scenario.
Somewhat: The sentence looks okay in terms of above questions.
No: The sentence is not well-formed/understandable, or fails to properly combine all the concepts into a plausible scenario.</p>
<p>Figure 5: Human evaluation template for the Constrained Commonsense Generation task.</p>
<h1>List of Keywords:</h1>
<h2>〈(source)</h2>
<h2>Question:</h2>
<h2>〈(generation)</h2>
<p>Q1. Grammar Is the question written in a grammatically correct way?
Yes It is entirely or mostly grammatically correct, with no or minimal grammatical mistakes.
Somewhat it is partially grammatically correct, with some grammatical mistakes.
No it is mostly not grammatically correct, with many grammatical mistakes.</p>
<p>Q2. Fluency Is the question written in a fluent and understandable way?
Yes It is entirely or mostly fluent and understandable.
Somewhat it is somewhat fluent and understandable, but it reads a bit awkward.
No it is mostly poorly written and hard to understand.</p>
<p>Q3. Meaningfulness Does the given question sentence ask a meaningful question?
Yes It is an entirely or mostly meaningful question.
Somewhat it is a somewhat meaningful question, but it might be a bit unclear.
No it is mostly not a meaningful question.</p>
<p>Q4. Overall Consider grammar, fluency and meaningfulness, overall, what's the quality of the question?
Good The overall quality is high.
No The overall quality is ok.
Bad The overall quality is low.
Figure 6: Human evaluation template for the Interrogative Sentence Generation task.</p>
<h1>First sentence of the story:</h1>
<p>$(source)$</p>
<h2>Continuation of the story:</h2>
<h2>(generation)</h2>
<p>Q1. Grammar Is the continuation of the story written in a grammatically correct way?
Yes It is entirely or mostly grammatically correct, with no or minimal grammatical mistakes.
Somewhat It is partially grammatically correct, with some grammatical mistakes.
Yes It is mostly not grammatically correct, with many grammatical mistakes.</p>
<p>Q2. Fluency Is the continuation of the story written in a fluent and understandable way?
Yes It is entirely or mostly fluent and understandable.
Somewhat It is somewhat fluent and understandable, but it reads a bit awkward.
Yes It is mostly poorly written and hard to understand.</p>
<p>Q3. Story Flow Does the continuation of the story flow coherently from the prompt and stay on-topic?
Yes It is entirely or mostly coherent from the prompt, and stays on-topic.
Somewhat It is somewhat coherent from the prompt, but it reads a bit off-topic.
Yes It is mostly not coherent from the prompt, and mostly off-topic.</p>
<p>Q4. Interestingness Is the continuation of the story written in an interesting way?
Yes It is a very interesting story.
Somewhat It is a somewhat interesting story.
Yes It is not an interesting story.</p>
<p>Q5. Overall Consider the above questions, overall, what's the quality of the continuation of the story?
Good The overall quality is high.
No The overall quality is ok.
Bad The overall quality is low.
Figure 7: Human evaluation template for the RocStories task.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Prompt</th>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Generation</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">I wanted to buy a video game console.</td>
<td style="text-align: center;">beam search</td>
<td style="text-align: center;">I went to the store and looked at all the options. I decided to go with the PS4. I went home and waited for the PS4 to arrive. When the PS4 arrived, I was very excited.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">beam search $+\mathrm{A}^{\star}$ esque</td>
<td style="text-align: center;">I went to the store and looked at all the options. I decided to go with the PS4 because it was the most expensive. When I got home, I plugged the PS4 into my TV. The PS4 turned on and I was happy.</td>
</tr>
<tr>
<td style="text-align: center;">My roommate is from Germany.</td>
<td style="text-align: center;">beam search</td>
<td style="text-align: center;">She moved to the United States when she was 16. She has been in the United States for 10 years now. She is a great roommate.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">beam search $+\mathrm{A}^{\star}$ esque</td>
<td style="text-align: center;">He came to the United States when he was 10 years old. He had a hard time adjusting to American culture. One day, he decided to move back to Germany. He was very happy to be back in his homeland.</td>
</tr>
<tr>
<td style="text-align: center;">Oliver was nervous about his wedding.</td>
<td style="text-align: center;">beam search</td>
<td style="text-align: center;">He had never been to a wedding before. He decided to go to the rehearsal dinner. The rehearsal dinner went very well. Oliver had a great time at his wedding.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">beam search $+\mathrm{A}^{\star}$ esque</td>
<td style="text-align: center;">He had never been married before and didn't know what to expect. When the day of the wedding came, he was very nervous. When he walked down the aisle, he knew it was going to be great. Oliver and his wife had a wonderful wedding.</td>
</tr>
<tr>
<td style="text-align: center;">My friends all love to go to the club to dance.</td>
<td style="text-align: center;">beam search</td>
<td style="text-align: center;">One day I decided to go with them. We went to the club and had a great time. We danced all night and had a great time. We decided to go back next week.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">beam search $+\mathrm{A}^{\star}$ esque</td>
<td style="text-align: center;">One day, I decided to go with them. We went to the club and danced all night. When we got home, I told my friends about the fun we had. They all agreed that it was the best night of their lives.</td>
</tr>
</tbody>
</table>
<p>Table 20: Example generations for unconstrained story generation using beam search with and without $\mathrm{A}^{\star}$ esque.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Words</th>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Generation</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">waste <br> water <br> heat</td>
<td style="text-align: center;">CGMH <br> TSMH <br> Neurologic <br> Neurologic</td>
<td style="text-align: center;">what waste is there, it seems now? where was the waste - water heater? How much water is waste heat? Why do we waste so much water to heat our homes?</td>
</tr>
<tr>
<td style="text-align: center;">Naples plague killed</td>
<td style="text-align: center;">CGMH <br> TSMH <br> Neurologic <br> Neurologic*</td>
<td style="text-align: center;">when would she finally turn twenty - one? why was the plague epidemic in naples not in fact killed? Who was killed in the plague in Naples? How many people are killed by the plague in Naples?</td>
</tr>
<tr>
<td style="text-align: center;">controversial aspect imperialism</td>
<td style="text-align: center;">CGMH <br> TSMH <br> Neurologic <br> Neurologic*</td>
<td style="text-align: center;">what war was ever fought after american imperialism collapsed? what are some controversial aspects of present - day american imperialism? Whose imperialism is it, anyway? What is the most controversial aspect of imperialism?</td>
</tr>
<tr>
<td style="text-align: center;">engines efficient steam</td>
<td style="text-align: center;">CGMH <br> TSMH <br> NSUROLOGIC</td>
<td style="text-align: center;">or were they the very first steam engines efficient enough for mass - production? why are steam engines so energy-efficient, just like fossil fuels? <br> Why do you think steam engines are so efficient? <br> Why are steam engines so efficient?</td>
</tr>
</tbody>
</table>
<p>Table 21: Example generations for constrained question generation with Neurologic ${ }^{\circ}$ and baselines, including CGMH (Miao et al., 2019), TSMH (Zhang et al., 2020) and Neurologic (Lu et al., 2021).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: left;">train</th>
<th style="text-align: center;">dev.</th>
<th style="text-align: center;">test</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">COMMONGEN (Lin et al., 2020)</td>
<td style="text-align: left;">32,651</td>
<td style="text-align: center;">993</td>
<td style="text-align: center;">1,497</td>
</tr>
<tr>
<td style="text-align: left;">WMT2013/2017 IATE (Dinu et al., 2019)</td>
<td style="text-align: left;">-</td>
<td style="text-align: center;">581</td>
<td style="text-align: center;">414</td>
</tr>
<tr>
<td style="text-align: left;">E2ENLG (Dušek et al., 2018)</td>
<td style="text-align: left;">4,862</td>
<td style="text-align: center;">547</td>
<td style="text-align: center;">630</td>
</tr>
<tr>
<td style="text-align: left;">Interrogative question (Zhang et al., 2020)</td>
<td style="text-align: left;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">300</td>
</tr>
<tr>
<td style="text-align: left;">RocStories (Mostafazadeh et al., 2016)</td>
<td style="text-align: left;">45,496</td>
<td style="text-align: center;">1,871</td>
<td style="text-align: center;">1,871</td>
</tr>
</tbody>
</table>
<p>Table 22: Details of datasets in downstream tasks.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ Agreement by ordinal Krippendorff alpha $(0 \leq \alpha \leq 1)$ (Krippendorff, 2007) is $0.27,0.28,0.25$ and 0.30 , indicating fair agreement.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{4}$ Agreement by ordinal Krippendorff alpha $(0 \leq \alpha \leq 1)$ (Krippendorff, 2007) of 0.24 and 0.22 (respectively), indicating fair agreement.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1779 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1779</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1779</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-32.html">extraction-schema-32</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <p><strong>Paper ID:</strong> paper-d19cfe3efe4165df42020ea362e4aadf141fb7bd</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/d19cfe3efe4165df42020ea362e4aadf141fb7bd" target="_blank">A self-supervised learning system for object detection using physics simulation and multi-view pose estimation</a></p>
                <p><strong>Paper Venue:</strong> IEEE/RJS International Conference on Intelligent RObots and Systems</p>
                <p><strong>Paper TL;DR:</strong> An autonomous process for training a Convolutional Neural Network for object detection and pose estimation in robotic setups and results show that the proposed approach outperforms popular training processes relying on synthetic — but not physically realistic — data and manual annotation.</p>
                <p><strong>Paper Abstract:</strong> Progress has been achieved recently in object detection given advancements in deep learning. Nevertheless, such tools typically require a large amount of training data and significant manual effort to label objects. This limits their applicability in robotics, where solutions must scale to a large number of objects and variety of conditions. This work proposes an autonomous process for training a Convolutional Neural Network (CNN) for object detection and pose estimation in robotic setups. The focus is on detecting objects placed in cluttered, tight environments, such as a shelf with multiple objects. In particular, given access to 3D object models, several aspects of the environment are physically simulated. The models are placed in physically realistic poses with respect to their environment to generate a labeled synthetic dataset. To further improve object detection, the network self-trains over real images that are labeled using a robust multi-view pose estimation process. The proposed training process is evaluated on several existing datasets and on a dataset collected for this paper with a Motoman robotic arm. Results show that the proposed approach outperforms popular training processes relying on synthetic — but not physically realistic — data and manual annotation. The key contributions are the incorporation of physical reasoning in the synthetic data generation process and the automation of the annotation process over real images.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1779.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1779.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PHYSIM-CNN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Physics-aware Synthetic Data Generation for CNN Object Detection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A physics-based synthetic data generation pipeline (implemented in Blender using the Bullet engine) that simulates object-dropping/settling, collisions, and varied lighting to render labeled images for training a Faster-RCNN object detector aimed at robotic manipulation scenes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Robot-mounted camera system (Motoman robotic arm with Intel RealSense used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>A robotic manipulator that moves a camera to pre-defined viewpoints to capture RGB-D images of shelf and table-top scenes for detection and pose estimation; used for data collection, multi-view aggregation, and scene reconfiguration.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general robotics manipulation / robotic vision for pick-and-place</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>Blender (rendering) + Bullet physics engine</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>A simulated calibrated environment that loads CAD object models, localizes a resting surface (shelf/table), simulates physics (gravity, collisions), and renders RGB images from known camera intrinsics/poses with configurable point light sources.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>Medium-fidelity physics simulation with rendered (non-photorealistic) images; physics-aware rather than photorealistic rendering</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>Rigid-body contact dynamics (collision handling), gravity, friction coefficients, object mass, damping for stabilization, mutual collisions and resting poses, camera intrinsics and pose, configurable point-light lighting (location, intensity, color), occlusions from physical interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>Photorealistic material reflectance and complex light transport (global illumination) were not emphasized; sensor noise models (depth noise, image noise) and actuator/robot dynamics were not modeled; fine-grained surface/material properties and exact real-world textures were approximate or not photorealistic.</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Shelf & Tote benchmark scenes and a table-top experimental setup using a Motoman arm with Intel RealSense to capture RGB-D images under varying clutter and lighting conditions; objects placed in bins or on table with up to 4-object clutter.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Bounding-box object detection and enabling downstream 6DoF pose estimation for robotic manipulation (object detection training in simulation transferred to real-world detection/pose estimation).</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Supervised learning: Faster-RCNN (VGG backbone) trained on synthetic images generated by the physics-aware simulator.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Detection success measured by Intersection-over-Union (IoU>0.5) percentage; 6DoF pose estimation success measured by translation error <5cm and mean rotation error <15°.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td>Shelf&Tote: physics-aware simulation detection accuracy 64% (IoU>0.5); physics-aware + varying lighting: 70%; Table-top: 78.8% (IoU>0.5).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>Lighting variation (point light source location, intensity, color) and random sampling of object initial poses and object subsets; backgrounds matched to shelf but textures were not fully randomized or photorealistically varied.</td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Dataset bias from synthetic texture and lighting, lack of photorealistic rendering, insufficient sensor noise modeling, remaining differences in object appearance and lighting, and possible mismatches in pose distributions when not explicitly matched to test distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Using physics to produce realistic object resting poses (matching real pose distributions), known camera poses and intrinsics, varying lighting during rendering, and training on simulation data that matches the target scene configuration (camera/resting-surface geometry). Combining this with later self-supervised real-world data further enables transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>Paper finds that accurate modeling of object pose distributions (via physics) and varying lighting improves transfer; photorealism not strictly required but lighting variation and physically-realistic poses materially improve detector performance — no quantitative numeric fidelity thresholds provided.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Compared methods: uniform synthetic sampling 31% success, synthetic sampled from test-pose distribution 69% success (upper bound), physics-aware simulation 64%, physics-aware + varying lighting 70% — physics-aware rendering substantially outperforms naive uniform rendering but still benefits from additional real data.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Physics-aware simulation that enforces physically plausible object poses produces synthetic datasets whose trained detectors transfer much better to real robotic scenes than naively rendered synthetic data; varying lighting improves robustness; matching pose distributions further increases performance, but synthetic-only training still shows limits due to texture/lighting domain gap.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A self-supervised learning system for object detection using physics simulation and multi-view pose estimation', 'publication_date_yy_mm': '2017-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1779.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1779.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SELF-LEARN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-learning via Multi-View Pose Estimation and Autonomous Scene Reconfiguration</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A lifelong, self-supervised pipeline that uses a detector trained on physics-aware synthetic data to obtain high-confidence bounding boxes from some views, aggregates depth to form per-object point clouds, registers 3D CAD models (Super4PCS + ICP) to estimate 6DoF poses, projects those poses to label real images across views, and retrains the detector iteratively while autonomously reconfiguring scenes with robot pick-and-place.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Robotic manipulator with sensor (Motoman arm + Intel RealSense in table-top tests; generic robotic arm for multi-view capture in shelf experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>A manipulator that moves a camera to pre-defined poses to collect multi-view RGB-D data, performs pick-and-place to reconfigure scenes, and powers an iterative self-labeling and retraining loop for object detectors.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general robotics manipulation / robotic vision and autonomous dataset collection</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>Simulator used to recreate estimated real scenes for label projection (Blender/Bullet) during labeling projection step</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Simulator recreates scenes by placing 3D CAD models at estimated poses and projects bounding boxes to known camera views to obtain pixel-accurate labels for real images.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>Use of simulator primarily for reprojection/label transfer (medium-fidelity rendering) rather than additional high-fidelity photorealistic modeling; physical realism used for initial synthetic training and for scene recreation with estimated poses.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>Recreation of object poses in simulator for accurate 2D projection, uses known camera intrinsics/poses; physics not central at labeling-time but initial training used physics-aware scene generation.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>Simulator projection step does not model sensor noise or capture exact real lighting and texture; label projection assumes accurate pose estimation and ignores certain real-image rendering differences.</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Same shelf/Tote dataset and table-top real captures using Motoman+RealSense; scenes captured from multiple known views, some views give confident detections enabling 3D aggregation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Improving real-world object detection and enabling accurate 6DoF pose estimation via self-labeled real images (transfer of detector improvements from simulation to reality and then iterative augmentation with real labels).</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Supervised learning with iterative retraining: Faster-RCNN initially trained on simulated images, then retrained with auto-labeled real images produced by the self-learning pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Detection accuracy (IoU>0.5) and 6DoF pose estimation success (translation error <5cm and rotation error <15°).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td>Shelf&Tote: self-learning improved detection to 75% with 2K auto-labeled images, 81% with 6K, 82% with 10K; Table-top: physics-aware baseline 78.8% -> self-learned (140 auto-labeled images) 90.3%. Pose estimation success improved (e.g., RCNN trained with proposed approach yields 79.4% pose success with PCA+ICP, comparable to ground-truth bbox case).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>Same as simulation: varied lighting during synthetic generation; the self-learning loop uses real-world image diversity gathered by moving to multiple camera views and reconfiguring scenes so domain coverage is increased.</td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Initial detector errors due to synthetic-real domain gap (texture/lighting mismatch) can yield wrong bboxes in some views; pose estimation requires sufficiently confident detections and enough views/aggregation time; failure modes include poor initial detection confidence, incomplete depth points, and registration sensitivity to outliers.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>High-confidence detections from some viewpoints, aggregating multi-view depth to form clean point clouds, robust global registration algorithm (Super4PCS + ICP) to compute 6DoF poses, known camera poses and intrinsics, and autonomous scene reconfiguration to gather diverse labeled real examples for retraining.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>Practical requirement: enough views with confident detections and an accurate model registration method enable reliable label transfer; matching physical pose distributions in simulation helps bootstrap the pipeline. No strict numeric fidelity thresholds provided.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td>Iterative retraining of Faster-RCNN using the auto-labeled real images produced by multi-view registration; examples: adding 2K/6K/10K auto-labeled images for Shelf&Tote, and 140 auto-labeled table-top images led to large improvements (up to 90.3% detection on table-top).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Self-learning on top of physics-aware simulation gave largest gains: physics-aware + varying lighting = 70% (baseline), then adding self-learned real images achieved 75% (2K), 81% (6K), 82% (10K) on Shelf&Tote; on table-top, physics-aware 78.8% -> self-learn 90.3% with 140 images.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A simulator-trained detector can be successfully bootstrapped to label real images via multi-view aggregation and robust 3D registration, and iterative retraining with these auto-labeled real images substantially reduces the sim-to-real gap; physical realism in simulated poses and varying lighting reduce initial gap, but autonomous collection of a modest number of real labeled examples (via the self-learning loop) yields the largest performance gains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A self-supervised learning system for object detection using physics simulation and multi-view pose estimation', 'publication_date_yy_mm': '2017-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Learning deep object detectors from 3D models <em>(Rating: 2)</em></li>
                <li>Render for CNN: Viewpoint estimation in images using CNNs trained with rendered 3d model views <em>(Rating: 2)</em></li>
                <li>How useful is photorealistic rendering for visual learning? <em>(Rating: 2)</em></li>
                <li>Multi-view self-supervised deep learning for 6d pose estimation in the amazon picking challenge <em>(Rating: 2)</em></li>
                <li>Simtrack: A simulation-based framework for scalable real-time object pose detection and tracking <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1779",
    "paper_id": "paper-d19cfe3efe4165df42020ea362e4aadf141fb7bd",
    "extraction_schema_id": "extraction-schema-32",
    "extracted_data": [
        {
            "name_short": "PHYSIM-CNN",
            "name_full": "Physics-aware Synthetic Data Generation for CNN Object Detection",
            "brief_description": "A physics-based synthetic data generation pipeline (implemented in Blender using the Bullet engine) that simulates object-dropping/settling, collisions, and varied lighting to render labeled images for training a Faster-RCNN object detector aimed at robotic manipulation scenes.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "Robot-mounted camera system (Motoman robotic arm with Intel RealSense used in experiments)",
            "agent_system_description": "A robotic manipulator that moves a camera to pre-defined viewpoints to capture RGB-D images of shelf and table-top scenes for detection and pose estimation; used for data collection, multi-view aggregation, and scene reconfiguration.",
            "domain": "general robotics manipulation / robotic vision for pick-and-place",
            "virtual_environment_name": "Blender (rendering) + Bullet physics engine",
            "virtual_environment_description": "A simulated calibrated environment that loads CAD object models, localizes a resting surface (shelf/table), simulates physics (gravity, collisions), and renders RGB images from known camera intrinsics/poses with configurable point light sources.",
            "simulation_fidelity_level": "Medium-fidelity physics simulation with rendered (non-photorealistic) images; physics-aware rather than photorealistic rendering",
            "fidelity_aspects_modeled": "Rigid-body contact dynamics (collision handling), gravity, friction coefficients, object mass, damping for stabilization, mutual collisions and resting poses, camera intrinsics and pose, configurable point-light lighting (location, intensity, color), occlusions from physical interactions.",
            "fidelity_aspects_simplified": "Photorealistic material reflectance and complex light transport (global illumination) were not emphasized; sensor noise models (depth noise, image noise) and actuator/robot dynamics were not modeled; fine-grained surface/material properties and exact real-world textures were approximate or not photorealistic.",
            "real_environment_description": "Shelf & Tote benchmark scenes and a table-top experimental setup using a Motoman arm with Intel RealSense to capture RGB-D images under varying clutter and lighting conditions; objects placed in bins or on table with up to 4-object clutter.",
            "task_or_skill_transferred": "Bounding-box object detection and enabling downstream 6DoF pose estimation for robotic manipulation (object detection training in simulation transferred to real-world detection/pose estimation).",
            "training_method": "Supervised learning: Faster-RCNN (VGG backbone) trained on synthetic images generated by the physics-aware simulator.",
            "transfer_success_metric": "Detection success measured by Intersection-over-Union (IoU&gt;0.5) percentage; 6DoF pose estimation success measured by translation error &lt;5cm and mean rotation error &lt;15°.",
            "transfer_performance_sim": null,
            "transfer_performance_real": "Shelf&Tote: physics-aware simulation detection accuracy 64% (IoU&gt;0.5); physics-aware + varying lighting: 70%; Table-top: 78.8% (IoU&gt;0.5).",
            "transfer_success": true,
            "domain_randomization_used": true,
            "domain_randomization_details": "Lighting variation (point light source location, intensity, color) and random sampling of object initial poses and object subsets; backgrounds matched to shelf but textures were not fully randomized or photorealistically varied.",
            "sim_to_real_gap_factors": "Dataset bias from synthetic texture and lighting, lack of photorealistic rendering, insufficient sensor noise modeling, remaining differences in object appearance and lighting, and possible mismatches in pose distributions when not explicitly matched to test distribution.",
            "transfer_enabling_conditions": "Using physics to produce realistic object resting poses (matching real pose distributions), known camera poses and intrinsics, varying lighting during rendering, and training on simulation data that matches the target scene configuration (camera/resting-surface geometry). Combining this with later self-supervised real-world data further enables transfer.",
            "fidelity_requirements_identified": "Paper finds that accurate modeling of object pose distributions (via physics) and varying lighting improves transfer; photorealism not strictly required but lighting variation and physically-realistic poses materially improve detector performance — no quantitative numeric fidelity thresholds provided.",
            "fine_tuning_in_real_world": false,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": true,
            "fidelity_comparison_results": "Compared methods: uniform synthetic sampling 31% success, synthetic sampled from test-pose distribution 69% success (upper bound), physics-aware simulation 64%, physics-aware + varying lighting 70% — physics-aware rendering substantially outperforms naive uniform rendering but still benefits from additional real data.",
            "key_findings": "Physics-aware simulation that enforces physically plausible object poses produces synthetic datasets whose trained detectors transfer much better to real robotic scenes than naively rendered synthetic data; varying lighting improves robustness; matching pose distributions further increases performance, but synthetic-only training still shows limits due to texture/lighting domain gap.",
            "uuid": "e1779.0",
            "source_info": {
                "paper_title": "A self-supervised learning system for object detection using physics simulation and multi-view pose estimation",
                "publication_date_yy_mm": "2017-03"
            }
        },
        {
            "name_short": "SELF-LEARN",
            "name_full": "Self-learning via Multi-View Pose Estimation and Autonomous Scene Reconfiguration",
            "brief_description": "A lifelong, self-supervised pipeline that uses a detector trained on physics-aware synthetic data to obtain high-confidence bounding boxes from some views, aggregates depth to form per-object point clouds, registers 3D CAD models (Super4PCS + ICP) to estimate 6DoF poses, projects those poses to label real images across views, and retrains the detector iteratively while autonomously reconfiguring scenes with robot pick-and-place.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "Robotic manipulator with sensor (Motoman arm + Intel RealSense in table-top tests; generic robotic arm for multi-view capture in shelf experiments)",
            "agent_system_description": "A manipulator that moves a camera to pre-defined poses to collect multi-view RGB-D data, performs pick-and-place to reconfigure scenes, and powers an iterative self-labeling and retraining loop for object detectors.",
            "domain": "general robotics manipulation / robotic vision and autonomous dataset collection",
            "virtual_environment_name": "Simulator used to recreate estimated real scenes for label projection (Blender/Bullet) during labeling projection step",
            "virtual_environment_description": "Simulator recreates scenes by placing 3D CAD models at estimated poses and projects bounding boxes to known camera views to obtain pixel-accurate labels for real images.",
            "simulation_fidelity_level": "Use of simulator primarily for reprojection/label transfer (medium-fidelity rendering) rather than additional high-fidelity photorealistic modeling; physical realism used for initial synthetic training and for scene recreation with estimated poses.",
            "fidelity_aspects_modeled": "Recreation of object poses in simulator for accurate 2D projection, uses known camera intrinsics/poses; physics not central at labeling-time but initial training used physics-aware scene generation.",
            "fidelity_aspects_simplified": "Simulator projection step does not model sensor noise or capture exact real lighting and texture; label projection assumes accurate pose estimation and ignores certain real-image rendering differences.",
            "real_environment_description": "Same shelf/Tote dataset and table-top real captures using Motoman+RealSense; scenes captured from multiple known views, some views give confident detections enabling 3D aggregation.",
            "task_or_skill_transferred": "Improving real-world object detection and enabling accurate 6DoF pose estimation via self-labeled real images (transfer of detector improvements from simulation to reality and then iterative augmentation with real labels).",
            "training_method": "Supervised learning with iterative retraining: Faster-RCNN initially trained on simulated images, then retrained with auto-labeled real images produced by the self-learning pipeline.",
            "transfer_success_metric": "Detection accuracy (IoU&gt;0.5) and 6DoF pose estimation success (translation error &lt;5cm and rotation error &lt;15°).",
            "transfer_performance_sim": null,
            "transfer_performance_real": "Shelf&Tote: self-learning improved detection to 75% with 2K auto-labeled images, 81% with 6K, 82% with 10K; Table-top: physics-aware baseline 78.8% -&gt; self-learned (140 auto-labeled images) 90.3%. Pose estimation success improved (e.g., RCNN trained with proposed approach yields 79.4% pose success with PCA+ICP, comparable to ground-truth bbox case).",
            "transfer_success": true,
            "domain_randomization_used": true,
            "domain_randomization_details": "Same as simulation: varied lighting during synthetic generation; the self-learning loop uses real-world image diversity gathered by moving to multiple camera views and reconfiguring scenes so domain coverage is increased.",
            "sim_to_real_gap_factors": "Initial detector errors due to synthetic-real domain gap (texture/lighting mismatch) can yield wrong bboxes in some views; pose estimation requires sufficiently confident detections and enough views/aggregation time; failure modes include poor initial detection confidence, incomplete depth points, and registration sensitivity to outliers.",
            "transfer_enabling_conditions": "High-confidence detections from some viewpoints, aggregating multi-view depth to form clean point clouds, robust global registration algorithm (Super4PCS + ICP) to compute 6DoF poses, known camera poses and intrinsics, and autonomous scene reconfiguration to gather diverse labeled real examples for retraining.",
            "fidelity_requirements_identified": "Practical requirement: enough views with confident detections and an accurate model registration method enable reliable label transfer; matching physical pose distributions in simulation helps bootstrap the pipeline. No strict numeric fidelity thresholds provided.",
            "fine_tuning_in_real_world": true,
            "fine_tuning_details": "Iterative retraining of Faster-RCNN using the auto-labeled real images produced by multi-view registration; examples: adding 2K/6K/10K auto-labeled images for Shelf&Tote, and 140 auto-labeled table-top images led to large improvements (up to 90.3% detection on table-top).",
            "comparison_across_fidelity_levels": true,
            "fidelity_comparison_results": "Self-learning on top of physics-aware simulation gave largest gains: physics-aware + varying lighting = 70% (baseline), then adding self-learned real images achieved 75% (2K), 81% (6K), 82% (10K) on Shelf&Tote; on table-top, physics-aware 78.8% -&gt; self-learn 90.3% with 140 images.",
            "key_findings": "A simulator-trained detector can be successfully bootstrapped to label real images via multi-view aggregation and robust 3D registration, and iterative retraining with these auto-labeled real images substantially reduces the sim-to-real gap; physical realism in simulated poses and varying lighting reduce initial gap, but autonomous collection of a modest number of real labeled examples (via the self-learning loop) yields the largest performance gains.",
            "uuid": "e1779.1",
            "source_info": {
                "paper_title": "A self-supervised learning system for object detection using physics simulation and multi-view pose estimation",
                "publication_date_yy_mm": "2017-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Learning deep object detectors from 3D models",
            "rating": 2
        },
        {
            "paper_title": "Render for CNN: Viewpoint estimation in images using CNNs trained with rendered 3d model views",
            "rating": 2
        },
        {
            "paper_title": "How useful is photorealistic rendering for visual learning?",
            "rating": 2
        },
        {
            "paper_title": "Multi-view self-supervised deep learning for 6d pose estimation in the amazon picking challenge",
            "rating": 2
        },
        {
            "paper_title": "Simtrack: A simulation-based framework for scalable real-time object pose detection and tracking",
            "rating": 1
        }
    ],
    "cost": 0.010728749999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>A Self-supervised Learning System for Object Detection using Physics Simulation and Multi-view Pose Estimation</h1>
<p>Chaitanya Mitash, Kostas E. Bekris and Abdeslam Boularias</p>
<h4>Abstract</h4>
<p>Progress has been achieved recently in object detection given advancements in deep learning. Nevertheless, such tools typically require a large amount of training data and significant manual effort to label objects. This limits their applicability in robotics, where solutions must scale to a large number of objects and variety of conditions. This work proposes an autonomous process for training a Convolutional Neural Network (CNN) for object detection and pose estimation in robotic setups. The focus is on detecting objects placed in cluttered, tight environments, such as a shelf with multiple objects. In particular, given access to 3D object models, several aspects of the environment are physically simulated. The models are placed in physically realistic poses with respect to their environment to generate a labeled synthetic dataset. To further improve object detection, the network self-trains over real images that are labeled using a robust multi-view pose estimation process. The proposed training process is evaluated on several existing datasets and on a dataset collected for this paper with a Motoman robotic arm. Results show that the proposed approach outperforms popular training processes relying on synthetic - but not physically realistic - data and manual annotation. The key contributions are the incorporation of physical reasoning in the synthetic data generation process and the automation of the annotation process over real images.</p>
<h2>I. INTRODUCTION</h2>
<p>Object detection and pose estimation is frequently the first step of robotic manipulation. Recently, deep learning methods, such as those employing Convolutional Neural Networks (CNNs), have become the standard tool for object detection, outperforming alternatives in object recognition benchmarks. These desirable results are typically obtained by training CNNs using datasets that involve a very large number of labeled images, as in the case of ImageNet [1]). Creating such large datasets requires intensive human labor. Furthermore, as these datasets are general-purpose, one needs to create new datasets for specific object categories and environmental setups that may be of importance to robotics, such as warehouse management and logistics.</p>
<p>The recent Amazon Picking Challenge (APC) [2] has reinforced this realization and has led into the development of datasets specifically for the detection of objects inside shelving units [3], [4], [5]. These datasets are created either with human annotation or by incrementally placing one object in the scene and using foreground masking.</p>
<p>The authors are with the Computer Science Department of Rutgers University in Piscataway, New Jersey, 08854, USA. Email: {cm1074,kb572,ab1544}@rutgers.edu.</p>
<p>This work is supported by NSF awards IIS-1734492, IIS-1723869 and IIS-1451737. Any opinions or findings expressed in this paper do not necessarily reflect the views of the sponsors. The authors would like to thank the anonymous IROS'17 reviewers for their constructive comments.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1. (top) A robotic arm performs pose estimation from multiple view points using an object detector trained in physically-simulated scenes (bottom-left). The estimated poses are used to automatically label real images (bottom-right). They are added to the training dataset as part of a lifelong learning process. Initially, the multi-view pose estimation procedure bootstraps its accuracy by trusting the objects' labels as predicted from the detector given training over synthetic images. It then uses these labels to annotate images of the same scene taken from more complex viewpoints.</p>
<p>An increasingly popular approach to avoid manual labeling is to use synthetic datasets generated by rendering 3D CAD models of objects with different viewpoints. Synthetic datasets have been used to train CNNs for object detection [6] and viewpoint estimation [7]. One major challenge in using synthetic data is the inherent difference between virtual training examples and real testing data. For this reason, there is considerable interest in studying the impact of texture, lighting, and shape to address this disparity [8]. Another issue with synthetic images generated from rendering engines is that they display objects in poses that are not necessarily physically realistic. Moreover, occlusions are usually treated in a rather naive manner, i.e., by applying cropping, or pasting rectangular patches, which again results in unrealistic scenes [6], [7], [9].</p>
<p>This work proposes an automated system for generating and labeling datasets for training CNNs. The objective of the</p>
<p>proposed system is to reduce manual effort in generating data and to increase the accuracy of bounding-box-based object detection for robotic setups. In particular, the two main contributions of this work are:</p>
<ul>
<li>A physics-based simulation tool, which uses information from camera calibration, object models, shelf or table localization to setup an environment for generating training data. The tool performs physics simulation to place objects at realistic configurations and renders images of scenes to generate a synthetic dataset to train an object detector.</li>
<li>A lifelong, self-learning process, which employs the object detector trained with the above physics-based simulation tool to perform a robust multi-view pose estimation with a robotic manipulator, and use the results to correctly label real images in all the different views. The key insight behind this system is the fact that the robot can often find a good viewing angle that allows the detector to accurately label the object and estimate its pose. The object’s predicted pose is then used to label images of the same scene taken from more difficult views, as shown in Fig. 1. The transformations between different views are known because they are obtained by moving the robotic manipulator.
The software and data of the proposed system, in addition to all the experiments, are publicly available at http:// www.physimpose.com</li>
</ul>
<h2>II. Related Work</h2>
<p>The novelty of the proposed system lies on the training process for generating synthetic data as well as augmenting the synthetic data with real ones that are generated from an automated, self-learning process. This involves several modules, which have been studied in the related literature over the years.</p>
<p>Object Segmentation: The tasks of object detection and semantic segmentation of images have been studied extensively and evaluated on large scale image datasets. Recently, the RCNN approach combined region proposals with convolutional neural networks [11]. This opened the path to high accuracy object detection, which was followed up by deep network architectures [12], [13] and end-toend training frameworks [14], [10]. There has also been a significant success in semantic labeling of images with the advent of Fully Convolutional networks (FCN) [15] and its extensions [16], [17], [18]. This work utilizes FCN and Faster-RCNN and proposes an automated way to collect data and incrementally train the structures for improved performance.</p>
<p>Pose Estimation: One way to approach this challenge is through matching local features, such as SIFT [19], or by extracting templates using color gradient and surface normals from 3D object models [20]. Synthesis-based approaches have also been gaining popularity [21], [22]. Nevertheless, in application domains, such as those studied by the Amazon Picking Challenge [2], which involve varying light conditions and cluttered scenes, it has been shown [5] that CNN-based
segmentation [10], [15] followed by point cloud registration with 3D models [23], [24], [25] is an effective approach. This paper builds on top of these techniques for pose estimation and proposes a method to self-feed the output of such processes to improve accuracy.</p>
<p>Synthetic Datasets: Synthetic datasets generated from 3D models have been used for object detection [6], [26] and pose estimation [27], [7] with mixed success as indicated by an evaluation of the performance of detectors trained on synthetic images to those trained with natural images [9]. This work proposes the incorporation of a physics-based simulator to generate realistic images of scenes, which helps object detection success rate.</p>
<p>Self-supervised Learning: The idea of incrementally learning with minimal supervision has been exploited previously in many different ways. Curriculum learning [28] and self-paced learning [29] have been adapted to improve the performance of object detectors [30], [31]. The self-learning technique proposed here involves the robot acquiring real images of scenes from multiple views. Then the robot uses the knowledge acquired from confidently detected views and 3D model registration to improve object detection in a lifelong manner.</p>
<h2>III. Physics-Aware Synthetic Data Generation</h2>
<p>The proposed system starts by physically simulating a scene as well as simulating the parameters of a known camera. The accompanying tool generates a synthetic dataset for training an object detector, given 3D CAD models of objects. This module has been implemented using the Blender API [32], which internally uses the Bullet physics engine [33]. The pipeline for this process is depicted in Fig. 2, while the corresponding pseudocode is provided in Alg. 1. The method receives as input:</p>
<ul>
<li>a set of predefined camera poses $\mathbf{P}_{\text {cam }}$,</li>
<li>the pose of the resting surface $\mathbf{P}_{s}$,</li>
<li>the intrinsic parameters of the camera $\mathbf{C}_{\text {int }}$,</li>
<li>the set of 3D object models $\mathbf{M}$ and</li>
<li>the number of simulated training images $N$ to generate.</li>
</ul>
<div class="codehilite"><pre><span></span><code><span class="nt">Algorithm</span><span class="w"> </span><span class="nt">1</span><span class="o">:</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">operatorname</span><span class="p">{</span><span class="err">PHYSIM</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">cdot</span><span class="w"> </span><span class="err">\</span><span class="nt">operatorname</span><span class="p">{</span><span class="err">CNN</span><span class="p">}</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="err">\</span><span class="nt">mathbf</span><span class="p">{</span><span class="err">P</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">c</span><span class="w"> </span><span class="err">a</span><span class="w"> </span><span class="err">m</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="err">\</span><span class="nt">mathbf</span><span class="p">{</span><span class="err">P</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">s</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="err">\</span><span class="nt">mathbf</span><span class="p">{</span><span class="err">C</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">i</span><span class="w"> </span><span class="err">n</span><span class="w"> </span><span class="err">t</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="err">\</span><span class="nt">mathbf</span><span class="p">{</span><span class="err">M</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="nt">N</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="o">)</span>
<span class="nt">1</span><span class="w"> </span><span class="nt">dataset</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="nt">emptyset</span><span class="err">\</span><span class="o">);</span>
<span class="nt">2</span><span class="w"> </span><span class="nt">while</span><span class="w"> </span><span class="o">(|</span><span class="nt">dataset</span><span class="o">|&lt;</span><span class="nt">N</span><span class="o">)</span><span class="w"> </span><span class="nt">do</span>
<span class="w">    </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">mathbf</span><span class="p">{</span><span class="err">O</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">a</span><span class="w"> </span><span class="nt">random</span><span class="w"> </span><span class="nt">subset</span><span class="w"> </span><span class="nt">of</span><span class="w"> </span><span class="nt">objects</span><span class="w"> </span><span class="nt">from</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">mathbf</span><span class="p">{</span><span class="err">M</span><span class="p">}</span><span class="err">\</span><span class="o">);</span>
<span class="w">    </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">mathbf</span><span class="p">{</span><span class="err">P</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{init</span><span class="w"> </span><span class="p">}</span><span class="err">}</span><span class="o">^</span><span class="p">{</span><span class="err">O</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">INITIAL_RANDOM_POSES</span><span class="o">(</span><span class="w"> </span><span class="nt">O</span><span class="w"> </span><span class="o">);</span>
<span class="w">    </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">mathbf</span><span class="p">{</span><span class="err">P</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{final</span><span class="w"> </span><span class="p">}</span><span class="err">}</span><span class="o">^</span><span class="p">{</span><span class="err">O</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="nt">operatorname</span><span class="p">{</span><span class="err">PHYS</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">_</span><span class="err">\</span><span class="nt">operatorname</span><span class="p">{</span><span class="err">SIM</span><span class="p">}</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="err">\</span><span class="nt">mathbf</span><span class="p">{</span><span class="err">P</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{init</span><span class="w"> </span><span class="p">}</span><span class="err">}</span><span class="o">^</span><span class="p">{</span><span class="err">O</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="err">\</span><span class="nt">mathbf</span><span class="p">{</span><span class="err">P</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">s</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="err">\</span><span class="nt">mathbf</span><span class="p">{</span><span class="err">O</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="o">);</span>
<span class="w">    </span><span class="nt">Light</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">leftarrow</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">PICK_LIGHTING_CONDITIONS</span><span class="o">();</span>
<span class="w">    </span><span class="nt">foreach</span><span class="w"> </span><span class="o">(</span><span class="nt">view</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">in</span><span class="w"> </span><span class="err">\</span><span class="nt">mathbf</span><span class="p">{</span><span class="err">P</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{cam</span><span class="w"> </span><span class="p">}</span><span class="err">}\</span><span class="o">)</span><span class="w"> </span><span class="o">)</span><span class="w"> </span><span class="nt">do</span>
<span class="w">        </span><span class="nt">image</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">leftarrow</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">RENDER</span><span class="o">(</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">mathbf</span><span class="p">{</span><span class="err">P</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{final</span><span class="w"> </span><span class="p">}</span><span class="err">}</span><span class="o">^</span><span class="p">{</span><span class="err">O</span><span class="p">}</span><span class="err">\</span><span class="o">),</span><span class="w"> </span><span class="nt">view</span><span class="o">,</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">mathrm</span><span class="p">{</span><span class="err">C</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{int</span><span class="w"> </span><span class="p">}</span><span class="err">}\</span><span class="o">),</span><span class="w"> </span><span class="nt">Light</span><span class="o">);</span>
<span class="w">        </span><span class="err">\</span><span class="p">{</span><span class="w"> </span><span class="err">labels,</span><span class="w"> </span><span class="err">bboxs</span><span class="w"> </span><span class="err">\</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="nt">operatorname</span><span class="p">{</span><span class="err">PROJECt</span><span class="p">}</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="err">\</span><span class="nt">mathbf</span><span class="p">{</span><span class="err">P</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{final</span><span class="w"> </span><span class="p">}</span><span class="err">}</span><span class="o">^</span><span class="p">{</span><span class="err">O</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="o">.</span><span class="err">\</span><span class="o">),</span><span class="w"> </span><span class="nt">view</span><span class="o">);</span>
<span class="w">        </span><span class="nt">dataset</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">leftarrow</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">dataset</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">cup</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="o">(</span><span class="nt">image</span><span class="o">,</span><span class="w"> </span><span class="nt">labels</span><span class="o">,</span><span class="w"> </span><span class="nt">bboxs</span><span class="o">);</span>
<span class="nt">11</span><span class="w"> </span><span class="nt">SIM_DETECT</span><span class="o">(</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">cdot</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="o">)</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">leftarrow</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">FASTER-RCNN</span><span class="o">(</span><span class="w"> </span><span class="nt">dataset</span><span class="w"> </span><span class="o">);</span>
<span class="nt">12</span><span class="w"> </span><span class="nt">return</span><span class="w"> </span><span class="nt">SIM_DETECT</span><span class="o">(</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">cdot</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="o">)</span><span class="w"> </span><span class="nt">AND</span><span class="w"> </span><span class="nt">dataset</span><span class="o">;</span>
</code></pre></div>

<p>In a sensing system for robotic manipulation, a 6 degree-of-freedom (DoF) pose of the camera mounted on a robotic arm (view $\in \mathbf{P}_{\text {cam }}$ ), can be exactly computed using forward</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 2. Pipeline for physics aware simulation: The 3D CAD models are generated and loaded in a calibrated environment on the simulator. A subset of the objects is chosen for generating a scene. Objects are physically simulated until they settle on the resting surface under the effect of gravity. The scenes are rendered from known camera poses. Perspective projection is used to compute 2D bounding boxes for each object. The labeled scenes are used to train a Faster-RCNN object detector [10], which is tested on a real-world setup.</p>
<p>kinematics. Furthermore, the camera calibration provides the intrinsic parameters of the camera <strong>C</strong><sub>int</sub>. To position the resting surface for the objects, a localization process is performed first in the real-world to compute the pose of the resting surface <strong>P</strong><sub>s</sub>. The system has been evaluated on an APC shelf and a table-top environment. The shelf localization process uses RANSAC [34] to compute edges and planes on the shelf and the geometry of the shelf model is used to localize the bins. Given the above information as well as 3D object models <strong>M</strong>, the method aims to render and automatically label <em>N</em> different images in simulation.</p>
<p>The algorithm simulates a scene by first choosing the objects <strong>O</strong> from the list of available object models <strong>M</strong> (line 3). The initial pose of an object is provided by function INITIAL_RANDOM_POSES (line 4), which samples uniformly at random along the x and y axis from the range (−dim<sub>i</sub>, dim<sub>i</sub>), where dim<sub>i</sub> is the dimension of the resting surface along the i<sup>th</sup> axis. The initial position along the z-axis is fixed and can be adjusted to either simulate dropping or placing. The initial orientation is sampled appropriately in SO(3). Then the function PHYS_SIM is called (line 5), which physically simulates the objects and allows them to fall due to gravity, bounce, and collide with each other as well as with the resting surface. Any inter-penetrations among objects or with the surface are treated by the physics engine. The final poses of the objects <strong>P</strong><sup>O</sup><sub>final</sub>, when they stabilize, resemble real-world poses. Gravity, friction coefficients and mass parameters are set at similar values globally and damping parameters are set to the maximum to promote fast stabilization.</p>
<p>The environment lighting and point light sources are varied with respect to location, intensity and color for each rendering (line 6). Simulating different indoor lighting sources according to related work [35] helps to avoid over-fitting to a specific texture. This makes the training set more robust to different testing scenarios. Once lighting conditions <strong>Light</strong> are chosen, the simulated scene is rendered from multiple views using the pre-defined camera poses (line 6). The rendering function RENDER requires the set of stabilized object poses <strong>P</strong><sup>O</sup><sub>final</sub>, the camera viewpoint <strong>view</strong> as well as the selected lighting conditions <strong>Light</strong> and intrinsic camera parameters <strong>C</strong><sub>int</sub> (line 7). Finally, perspective projection is applied to obtain 2D bounding box labels for each object in the scene with function PROJECT (line 8). The overlapping portion of the bounding boxes for the object that is further away from the camera is pruned.</p>
<p>The synthetic <strong>dataset</strong> generated is used to train an object detector SIM_DETECT(·) based on Faster-RCNN [10], which utilizes a deep VGG network architecture [13].</p>
<h2>IV. SELF-LEARNING VIA</h2>
<h3>MULTI-VIEW POSE ESTIMATION</h3>
<p>Given access to an object detector trained with the physics-based simulator, the self-learning pipeline labels real-world images using a robust multi-view pose estimation. This is based on the idea that the detector performs well on some views, while might be imprecise or fail in other views. Aggregating 3D data over the confident detections and with access to the knowledge of the environment, a 3D segment can be extracted for each object instance in the scene. This process combined with the fact that 3D models of objects are available, makes it highly likely to estimate correct 6DoF pose of objects given enough views and search time. The results of pose estimation are then projected back to the multiple views, and used to label real images. These examples are very effective to reduce the confusion in the classifier for novel views. The process also autonomously reconfigures the scene using manipulation actions to apply the labeling process iteratively over time on different scenes, thus generating a labeled dataset which is used to re-train the object detector. The pipeline of the process is presented in Fig. 3 and the pseudocode is provided in Alg. 2.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 3. Automatic self-labeling pipeline: The detector, which is trained with simulated data, is used to detect objects from multiple views. The point cloud aggregated from successful detections undergoes 3D segmentation. Then, Super4PCS [24] is used to estimate the 6D pose of the object in the world frame. The computed boxes with high confidence are simulated and projected back to the multiple views to obtain precise labels over real images.</p>
<p>A robotic arm is used to move the sensor to different pre-defined camera configurations <strong>P</strong>_cam and capture color (<strong>RGB</strong>) and depth (<strong>D</strong>) images of the scene (lines 2-3). The PRACSYS motion planning library [36], [37] was used to control the robot in the accompanying implementation.</p>
<p>Algorithm 2: SELF-LEARN(dataset, <strong>P</strong>_cam, <strong>M</strong>, <em>N</em>')</p>
<p>1 while | <em>dataset</em> | &lt; <em>N</em><sup>0</sup> do</p>
<p>2 foreach view ∈ <strong>P</strong>_cam do
3 {RGB␣view, <strong>D</strong>_view} ← CAPTURE( view);
4
5 foreach object o in the scene and <strong>M</strong> do
6 Cloud[o] = ∅;
7
8 foreach view ∈ <strong>P</strong>_cam do
9
10 bbox ← SIM_DETECT( RGB␣view );
11 if CONFIDENCE(<strong>bbox</strong>) &gt; threshold then
12 3DPts ← GET_3DPTS( bbox, <strong>D</strong>_view );
13 Cloud[o] ← Cloud[o] ∪ 3DPts;
14
15 OUTLIER_REMOVAL( Cloud[o] );
16 P[o] ← SUPER4PCS( Cloud[o], M[o] );
17
18 foreach view ∈ <strong>P</strong>_cam do
19 { labels, bboxs } ← PROJECT(P[o], view);
20 dataset ← dataset ∪ (RGB␣view, labels, bboxs);
21
22 randObj ← SAMPLE_RANDOM_OBJECT( <strong>M</strong> );
23 objConfig ← PICK_CONFIG();
24 RECONFIGURE_OBJECT( randObj, objConfig );
25 NEW_DETECT(·) ← FASTER-RCNN( dataset );
26 return NEW_DETECT(·);</p>
<p>The detector trained using physics-aware simulation is then used to extract bounding boxes <strong>bbox</strong> corresponding to each object o in the scene (line 7). There might exist a bias in simulation either with respect to texture or boxes, which can lead to imprecise bounding boxes or complete failure in certain views. For the detection to be considered for further processing, a threshold is considered on the confidence value returned by RCNN (line 8).</p>
<p>The pixel-wise depth information <strong>3DPts</strong> within the confidently detected bounding boxes <strong>bbox</strong> (line 9) is aggregated in a common point cloud per object <strong>Cloud[o]</strong> given information from multiple views (line 10). The process employs environmental knowledge to clean the aggregated point cloud (line 11). Points outside the resting surface bounds are removed and outlier removal is performed based on k-nearest neighbors and a uniform grid filter.</p>
<p>Several point cloud registration methods were tested for registering the 3D model <strong>M[o]</strong> with the corresponding segmented point cloud <strong>Cloud[o]</strong> (line 12). This included SUPER4PCS [24], fast global registration [25] and simply using the principal component analysis (PCA) with Iterative Closest Point (ICP) [23]. The SUPER4PCS algorithm [24] used alongside ICP was found to be the most applicable for the target setup as it is the most robust to outliers and returns a very natural metric for confidence evaluation. SUPER4PCS returns the best rigid alignment according to the Largest Common Pointset (LCP). The algorithm searches for the best score, using transformations obtained from four-point congruences. Thus, given enough time, it generates the optimal alignment with respect to the extracted segment.</p>
<p>After the 6DoF pose is computed for each object, the scene is recreated in the simulator using object models placed at the pose <strong>P[o]</strong> and projected to the known camera views (line 14). Bounding boxes are computed on the simulated setup and transferred to the real images. This gives precise bounding box labels for real images in all the views (line 15).</p>
<p>To further reduce manual labeling effort, an autonomous scene reconfiguration is performed (lines 16-18). The robot reconfigures the scene with a pick and place manipulation action to iteratively construct scenes and label them, as in Fig. 4. For each reconfiguration, the object to be moved is chosen randomly and the final configuration is selected from a set of pre-defined configurations in the workspace.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig. 4. Manipulator performing scene reconfiguration by moving an object from one configuration on the table to another</p>
<h2>V. EVALUATION</h2>
<p>This section discusses the datasets considered, it compares different techniques for generating synthetic data and evaluates the effect of self-learning. It finally applies the trained detector on the 6DoF pose estimation task. The standard Intersection-Over-Union (IoU) metric is employed to evaluate performance in the object detection task.</p>
<h3>A. Datasets</h3>
<p>Several RGB-D datasets have been released in the setting of the Amazon Picking Challenge [3], [4], [5]. They proposed system was evaluated on the benchmark dataset released by Team MIT-Princeton called the Shelf&amp;Tote dataset [5]. The experiments are performed on 148 scenes in the shelf environment with different lighting and clutter conditions. The scenes include 11 objects used in APC with 2220 images and 229 unique object poses. The objects were chosen to represent different geometric shapes but ignoring the ones which did not have enough depth information. Thus, the results can be generalized to a large set of objects.</p>
<p>The proposed system has been also evaluated on a real-world table-top setup. The corresponding test dataset was generated by placing multiple objects in different configurations on a table-top. An Intel RealSense camera mounted on a Motoman robot was used to capture images of scenes from multiple views. Images corresponding to 41 cluttered scenes, with 11 APC objects and 473 detection instances were collected and manually labeled.</p>
<h3>B. Evaluating the Object Detector trained in Simulation</h3>
<p>To study how object pose distribution effects the training process, different techniques for synthetic data generation are evaluated. The results of experiments performed on the Shelf&amp;Tote dataset are presented in Table I.</p>
<h4>1) Generating training data using test data distribution</h4>
<p>The objective here is to establish an upper bound for the performance of a detector trained with simulated images. For this purpose, the object detector is trained with the knowledge of pose distribution in the test data. This process consists of estimating the density of the test data with respect to object poses using Kernel Density Estimation, and generating training data according to this distribution, as follows:</p>
<ul>
<li>Uniformly simulate many scenes using a simulator and record the poses for each object in the scene.</li>
<li>Weigh each generated scene according to its similarity to test data. This is the sum of the number of objects in the scene for which the pose matches (rotation difference less than 15° and translation difference less than 5cm) at least one pose in their corresponding test pose distribution.</li>
<li>Normalize the weights to get a probability distribution on the sampled poses.</li>
<li>Sub-sample the training poses using the normalized probability distribution.</li>
</ul>
<p>The sampled scenes were used to train a Faster-RCNN detector, which achieved an accuracy of 69%.</p>
<h4>2) Uniformly sampled synthetic data</h4>
<p>This alternative is a popular technique of generating synthetic data. It uses 3D models of the objects to render their images from several viewpoints sampled on a spherical surface centered at the object. The background image corresponded to the APC shelf, on top of which randomly selected objects were pasted at sampled locations. This process allows to simulate occlusions and mask subtraction provides the accurate bounding boxes in these cases. The objects in these images are not guaranteed to have physically realistic poses. This method of synthetic data generation does not perform well on the target task, giving a low accuracy of 31%.</p>
<h4>3) Generating training data with physics-aware simulation</h4>
<p>The accuracy of 64% achieved by the proposed physics-aware simulator is close to the upper bound. By incorporating the knowledge of the camera pose, resting surface and by using physics simulation, the detector is essentially over-fitted to the distribution of poses from which the test data comes, which can be useful for robotic setups.</p>
<table>
<thead>
<tr>
<th></th>
<th>Method</th>
<th>Success(IoU&gt;0.5</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Team MIT-Princeton [5] (Benchmark)</td>
<td>75%</td>
</tr>
<tr>
<td></td>
<td>Sampled from test data distribution</td>
<td>69%</td>
</tr>
<tr>
<td></td>
<td>Sampled from uniform distribution</td>
<td>31%</td>
</tr>
<tr>
<td></td>
<td>Physics-aware simulation</td>
<td>64%</td>
</tr>
<tr>
<td></td>
<td>Physics-aware simulation + varying light</td>
<td>70%</td>
</tr>
<tr>
<td></td>
<td>Self-learning (2K images)</td>
<td>75%</td>
</tr>
<tr>
<td></td>
<td>Self-learning (6K images)</td>
<td>81%</td>
</tr>
<tr>
<td></td>
<td>Self-learning (10K images)</td>
<td>82%</td>
</tr>
</tbody>
</table>
<p>TABLE I</p>
<p>EVALUATION ON PRINCETON'S SHELF&amp;TOTE DATASET [5]</p>
<p>The results discussed until now were with respect to a constant lighting condition. As the dataset grows, then a dip in the performance is observed. This is expected as the detector overlits with respect to the synthetic texture, which does not mimic real lighting condition. This is not desirable, however. To deal with this issue, the lighting conditions are varied according to the location and color of the light source.</p>
<p>This does resolve the problem to some extent but the dataset bias still limits performance to an accuracy of 70%.</p>
<p>On the table-top setup, the detector trained by the physics-based simulation has a success rate of 78.8%, as shown in Table II.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Success(IoU&gt;0.5)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Physics aware simulation</td>
<td>78.8%</td>
</tr>
<tr>
<td>Self-learning (140 images)</td>
<td>90.3%</td>
</tr>
</tbody>
</table>
<p>TABLE II
DETECTION ACCURACY ON TABLE-TOP EXPERIMENTS</p>
<h2>C. Evaluating Self-learning</h2>
<p>The self-learning pipeline is executed over the training images in the Shelf&amp;Tote [5] training dataset to automatically label them using multi-view pose estimation. The real images are incrementally added to the simulated dataset to re-train the Faster-RCNN. This results in a performance boost of 12%. This result also outperforms the training process by [5] which uses approximately 15,000 real images labeled using background subtraction. The reason that the proposed method outperforms a large dataset of real training images is mostly because the proposed system can label objects placed in a clutter.</p>
<p>On the table-top setup, pose estimation is performed using the trained detector and model registration. The estimated poses with high confidence values are then projected to the known camera views to obtain the 2D bounding box labels on real scenes. This is followed by reconfiguring the scenes using pick and place manipulation. After generating 140 scenes with a clutter of 4 objects in each image, the automatically labeled instances are used to retrain the Faster-RCNN detector. The performance improvement by adding these labeled examples is presented in Table II. The overall performance improvement is depicted in Fig. 5, while an example is shown in Fig.6.</p>
<p>Fig. 5. Plot depicting the performance improvement by adding different components of the system</p>
<h2>D. Evaluating the detector for 6DoF Pose estimation</h2>
<p>Success in pose estimation is evaluated as the percentage of predictions with an error in translation less than 5cm and mean error in the rotation less than $15^{\circ}$. The results of pose estimation are compared to the pose system proposed by the APC Team MIT-Princeton [5] in addition to different model registration techniques. The results are depicted in Table III. Given the above specified metric, the proposed approach outperforms the pose estimation system proposed before [5] by a margin of 25%. It is very interesting to note that the success in pose estimation task is at par with the success achieved using ground truth bounding boxes.</p>
<p>Fig. 6. Results of object detection before and after training with the self-learning process. The detector learns to predict more precise bounding boxes. It can also detect objects better from novel views.</p>
<h2>VI. DISCUSSION</h2>
<p>This work provides a system that autonomously generates data to train CNNs for object detection and pose estimation in robotic setups. Object detection and pose estimation are tasks that are frequently required before grasping [38] or rearranging objects with a robot [39], [40]. A key feature of the proposed system is physical reasoning. In particular, it employs a physics engine to generate synthetic but physically realistic images. The images are very similar to real-world scenes in terms of object pose distributions. This helps to increase the success ratio of CNNs trained with simulated data and reduces the requirements for manual labeling.</p>
<p>Nevertheless, synthetic data may not be sufficient as they cannot always generalize to the lighting conditions present in the real-world. For this purpose and given access to a robotic setup, this work proposes a lifelong learning approaches for a manipulator to collect additional labeled data in an autonomous manner. In particular, the method utilizes successful, high confidence detections from multiple views to perform pose estimation. This avoids over-fitting to simulated conditions. The overall combination of physical reasoning and self-learning results in a success ratio that outperforms current state-of-the-art systems in robot vision.</p>
<p>A future objective remains to achieve similar quality of object detection and pose estimation only with simulated data. This would minimize the dependency of having access to a robotic setup for adaptation of the learning procedure to real-world conditions. Furthermore, it would be interesting, to extend the training process to facilitate semantic segmentation of scenes, which could lead to an even more robust pose estimation.</p>
<table>
<thead>
<tr>
<th>2D-Segmentation Method</th>
<th>3D-registration Method</th>
<th>Mean-error Rotation (deg)</th>
<th>Mean-error Translation (m)</th>
<th>Success(%)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Ground-Truth Bounding-Box</td>
<td>PCA + ICP</td>
<td>7.65</td>
<td>0.02</td>
<td>84.8</td>
</tr>
<tr>
<td>FCN (trained with [5])</td>
<td>PCA + ICP</td>
<td>17.3</td>
<td>0.06</td>
<td>54.6</td>
</tr>
<tr>
<td>FCN (trained with [5])</td>
<td>Super4PCS + ICP</td>
<td>16.8</td>
<td>0.06</td>
<td>54.2</td>
</tr>
<tr>
<td>FCN (trained with [5])</td>
<td>fast-global-registration</td>
<td>18.9</td>
<td>0.07</td>
<td>43.7</td>
</tr>
<tr>
<td>RCNN (Proposed training)</td>
<td>PCA + ICP</td>
<td>$\mathbf{8 . 5 0}$</td>
<td>0.03</td>
<td>$\mathbf{7 9 . 4}$</td>
</tr>
<tr>
<td>RCNN (Proposed training)</td>
<td>Super4PCS + ICP</td>
<td>8.89</td>
<td>$\mathbf{0 . 0 2}$</td>
<td>75.0</td>
</tr>
<tr>
<td>RCNN (Proposed training)</td>
<td>fast-global-registration</td>
<td>14.4</td>
<td>0.03</td>
<td>58.9</td>
</tr>
</tbody>
</table>
<p>TABLE III
COMPARING THE PERFORMANCE OF THE PROPOSED SYSTEM TO STATE-OF-THE-ART TECHNIQUES FOR POSE ESTIMATION.</p>
<h2>REFERENCES</h2>
<p>[1] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei, "ImageNet Large Scale Visual Recognition Challenge," Intern. J. of Computer Vision (IJCV), vol. 115, pp. 211-252, 2015.
[2] N. Correll, K. E. Bekris, D. Berenson, O. Brock, A. Causo, K. Hauser, K. Osada, A. Rodriguez, J. Romano, and P. Wurman, "Analysis and Observations From the First Amazon Picking Challenge," IEEE Trans. on Automation Science and Engineering (T-ASE), 2016.
[3] A. Singh, J. Sha, K. S. Narayan, T. Achim, and P. Abbeel, "Bigbird: A large-scale 3d database of object instances," in IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2014.
[4] C. Rennie, R. Shome, K. E. Bekris, and A. F. De Souza, "A dataset for improved rgbd-based object detection and pose estimation for warehouse pick-and-place," IEEE Robotics and Automation Letters, vol. 1, no. 2, pp. 1179 - 1185, 2016.
[5] A. Zeng, K.-T. Yu, S. Song, D. Suo, E. Walker Jr, A. Rodriguez, and J. Xiao, "Multi-view self-supervised deep learning for 6d pose estimation in the amazon picking challenge," in IEEE International Conference on Robotics and Automation (ICRA), 2017.
[6] X. Peng, B. Sun, K. Ali, and K. Saenko, "Learning deep object detectors from 3D models," in IEEE Intern. Conf. on Computer Vision, 2015.
[7] H. Su, C. R. Qi, Y. Li, and L. J. Guibas, "Render for CNN: Viewpoint estimation in images using CNNs trained with rendered 3d model views," in IEEE Intern. Conf. on Computer Vision, 2015.
[8] B. Sun and K. Saenko, "From virtual to reality: Fast adaptation of virtual object detectors to real domains," in British Machine Vision Conf., 2014.
[9] Y. Movshovitz-Attias, T. Kanade, and Y. Sheikh, "How useful is photorealistic rendering for visual learning?" in ECCV 2016 Workshops, 2016.
[10] S. Ren, K. He, R. Girshick, and J. Sun, "Faster r-cnn: Towards realtime object detection with region proposal networks," in Advances in Neural Information Processing Systems, 2015, pp. 91-99.
[11] R. Girshick, J. Donahue, T. Darrell, and J. Malik, "Rich feature hierarchies for accurate object detection and semantic segmentation," in IEEE Conf. on Computer Vision and Pattern Recognition, 2014.
[12] K. He, X. Zhang, S. Ren, and J. Sun, "Deep residual learning for image recognition," in IEEE Conf. on Computer Vision and Pattern Recognition, 2016.
[13] K. Simonyan and A. Zisserman, "Very deep convolutional networks for large-scale image recognition," arXiv preprint arXiv:1409.1556, 2014.
[14] R. Girshick, "Fast R-CNN," in IEEE Intern. Conf. on Computer Vision, 2015, pp. 1440-1448.
[15] J. Long, E. Shelhamer, and T. Darrell, "Fully convolutional networks for semantic segmentation," in IEEE Conf. on Computer Vision and Pattern Recognition, 2015, pp. 3431-3440.
[16] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille, "Semantic image segmentation with deep convolutional nets and fully connected crfs," arXiv preprint arXiv:1412.7062, 2014.
[17] J. Dai, K. He, and J. Sun, "Boxsup: Exploiting bounding boxes to supervise convolutional networks for semantic segmentation," in IEEE Int. Conf. on Computer Vision, 2015, pp. 1635-1643.
[18] Y. Li, H. Qi, J. Dai, X. Ji, and Y. Wei, "Fully convolutional instanceaware semantic segmentation," arXiv preprint arXiv:1611.07709, 2016.
[19] K. Pauwels and D. Kragic, "Simtrack: A simulation-based framework for scalable real-time object pose detection and tracking," in IEEE Int. Conf. on Intelligent Robots and Systems (IROS), 2015.
[20] S. Hinterstoisser, V. Lepetit, S. Ilic, S. Holzer, G. Bradski, K. Konolige, and N. Navab, "Model based training, detection and pose estimation of texture-less 3d objects in heavily cluttered scenes," in Asian Conference on Computer Vision, 2012.
[21] A. Krull, E. Brachmann, F. Michel, M. Ying Yang, S. Gumhold, and C. Rother, "Learning analysis-by-synthesis for 6d pose estimation in rgb-d images," in IEEE Intern. Conf. on Computer Vision, 2015.
[22] V. Narayanan and M. Likhachev, "Discriminatively-guided Deliberative Perception for Pose Estimation of Multiple 3D Object Instances," in Robotics: Science and Systems (RSS), 2016.
[23] P. J. Besl and N. D. McKay, "Method for registration of 3D shapes," International Society for Optics and Photonics, 1992.
[24] N. Mellado, D. Aiger, and N. K. Mitra, "Super 4pcs fast global pointcloud registration via smart indexing," Computer Graphics Forum. Vol. 33. No. 5, 2014.
[25] Q.-Y. Zhou, J. Park, and K. Koltun, "Fast Global Registration," European Conference on Computer Vision, 2016.
[26] M. Stark, M. Goesele, and B. Schiele, "Back to the future: Learning shape models from 3d cad data." in British Machine Vision Conf., 2010.
[27] S. Gupta, P. Arbeláez, R. Girshick, and J. Malik, "Aligning 3d models to rgb-d images of cluttered scenes," in IEEE Conf. on Computer Vision and Pattern Recognition, 2015.
[28] Y. Bengio, J. Louradour, R. Collobert, and J. Weston, "Curriculum learning," in Intern. Conf. on Machine Learning, 2009.
[29] M. P. Kumar, B. Packer, and D. Koller, "Self-paced learning for latent variable models," in Advances in Neural Information Processing Systems, 2010.
[30] X. Liang, S. Liu, Y. Wei, L. Liu, L. Lin, and S. Yan, "Towards computational baby learning: A weakly-supervised approach for object detection," in IEEE Intern. Conf. on Computer Vision, 2015.
[31] E. Sangineto, M. Nabi, D. Culibrk, and N. Sebe, "Self paced deep learning for weakly supervised object detection," arXiv preprint arXiv:1605.07651, 2016.
[32] Blender. [Online]. Available: https://www.blender.org/
[33] Bullet physics engine. [Online]. Available: http://bulletphysics.org/
[34] M. A. Fischler and R. C. Bolles, "Random sample consensus: A paradigm for model fitting with applications to image analysis and automated cartography," Communications of the ACM, 1981.
[35] J. Hastings-Trew. (2012) Reproducing real world light. [Online]. Available: http://planetpixelemporium.com/tutorialpages/light.html
[36] A. Kimmel, A. Dobson, Z. Littlefield, A. Krontiris, J. Marble, and K. Bekris, "Pracsys: An extensible architecture for composing motion controllers and planners," Int. Conf. on Simulation, Modeling, and Programming for Autonomous Robots, pp. 137-148, 2012.
[37] Z. Littlefield, A. Krontiris, A. Kimmel, A. Dobson, R. Shome, and K. E. Bekris, "An Extensible Software Architecture for Composing Motion and Task Planners," in Int. Conf. on Simulation, Modeling and Programming for Autonomous Robots (SIMPAR), 2015.
[38] V. Azizi, A. Kimmel, K. E. Bekris, and M. Kapadia, "Geometric Reachability Analysis for Grasp Planning in Cluttered Scenes for Varying End-Effectors," in IEEE CASE, 2017.
[39] H. Shuai, N. Stiffler, A. Krontiris, K. E. Bekris, and J. Yu, "HighQuality Tabletop Rearrangement with Overhand Grasps: Hardness Results and Fast Methods," in RSS, Cambridge, MA, 2017.
[40] A. Krontiris and K. E. Bekris, "Dealing with Difficult Instances of Object Rearrangement," in Robotics Science and Systems (RSS), 2015.</p>            </div>
        </div>

    </div>
</body>
</html>
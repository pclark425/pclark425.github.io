<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5210 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5210</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5210</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-110.html">extraction-schema-110</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <p><strong>Paper ID:</strong> paper-6d4bacb69923e1e94fb4de468b939ce6db32fb51</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/6d4bacb69923e1e94fb4de468b939ce6db32fb51" target="_blank">Large Language Models Cannot Self-Correct Reasoning Yet</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> It is indicated that LLMs struggle to self-correct their responses without external feedback, and at times, their performance even degrades after self-correction.</p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) have emerged as a groundbreaking technology with their unparalleled text generation capabilities across various applications. Nevertheless, concerns persist regarding the accuracy and appropriateness of their generated content. A contemporary methodology, self-correction, has been proposed as a remedy to these issues. Building upon this premise, this paper critically examines the role and efficacy of self-correction within LLMs, shedding light on its true potential and limitations. Central to our investigation is the notion of intrinsic self-correction, whereby an LLM attempts to correct its initial responses based solely on its inherent capabilities, without the crutch of external feedback. In the context of reasoning, our research indicates that LLMs struggle to self-correct their responses without external feedback, and at times, their performance even degrades after self-correction. Drawing from these insights, we offer suggestions for future research and practical applications in this field.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5210.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5210.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5 (gpt-3.5-turbo-0613)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5-Turbo (gpt-3.5-turbo-0613)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A chat-oriented GPT-3.5 family model accessed via OpenAI API (kernel gpt-3.5-turbo-0613) used in the paper's experiments to evaluate self-correction strategies on reasoning benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo-0613</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI GPT-3.5 family chat model (API kernel gpt-3.5-turbo-0613) used with temperature 1 in the experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Correct (generate-then-reflect; intrinsic and oracle variants)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Three-step prompting: (1) initial generation, (2) prompt the model to review its prior answer and produce feedback, (3) answer original question again using that feedback. Two variants tested: (A) oracle-guided loop that uses ground-truth labels to decide when to stop (oracle self-correct), (B) intrinsic self-correction where the model must judge correctness itself and may perform up to two rounds.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>2</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K, CommonSenseQA, HotpotQA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>GSM8K: grade-school math word problems (1,319 test questions); CommonSenseQA: multiple-choice commonsense questions (dev set 1,221); HotpotQA: open-domain multi-hop QA (closed-book subset of 100 questions used by prior work).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Oracle-guided self-correct: GSM8K 84.3% (vs standard 75.9%), CommonSenseQA 89.7% (vs 75.8%), HotpotQA 29.0% (vs 26.0%) (Table 2). Intrinsic self-correction (no oracle): round1 (3 calls) GSM8K 75.1%, CommonSenseQA 38.1%, HotpotQA 25.0%; round2 (5 calls) GSM8K 74.7%, CommonSenseQA 41.8%, HotpotQA 25.0% (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Standard prompting (single call): GSM8K 75.9%, CommonSenseQA 75.8%, HotpotQA 26.0% (Table 2/3).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Clear quantitative improvement when oracle labels were used to guide the self-correction loop (e.g., GSM8K +~8.4 percentage points). However, in intrinsic (no-oracle) setting, performance decreased or did not improve relative to standard prompting, showing no evidence of intrinsic self-reflection improving reasoning for this model.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Without oracle labels GPT-3.5 often changes correct answers to incorrect ones more frequently than correcting incorrect answers (authors report high 'correct -> incorrect' rate); intrinsic self-correction degrades performance on CommonSenseQA and HotpotQA dramatically, implying inability to reliably judge its own answers; reliance on oracle labels is unrealistic in deployed settings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Cannot Self-Correct Reasoning Yet', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5210.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5210.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (gpt-4)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (OpenAI GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI's GPT-4 model evaluated in both oracle-guided and intrinsic self-correction setups to test whether self-reflection improves reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-4 (accessed 2023-08-29)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI GPT-4 model (API), evaluated with temperature 1 for the reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Correct (generate-then-reflect; intrinsic and oracle variants)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Same three-step prompting pipeline as for GPT-3.5. Oracle variant uses ground-truth to decide when to stop; intrinsic variant relies on model self-judgment and up to two rounds of reflection.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>2</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K, CommonSenseQA, HotpotQA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>See GPT-3.5 entry.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Oracle-guided self-correct: GSM8K 97.5% (vs standard 95.5%), CommonSenseQA 85.5% (vs 82.0%), HotpotQA 59.0% (vs 49.0%) (Table 2). Intrinsic self-correction (no oracle): round1 (3 calls) GSM8K 91.5%, CommonSenseQA 79.5%, HotpotQA 49.0%; round2 (5 calls) GSM8K 89.0%, CommonSenseQA 80.0%, HotpotQA 43.0% (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Standard prompting: GSM8K 95.5%, CommonSenseQA 82.0%, HotpotQA 49.0% (Table 2/3).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Oracle-guided self-correction yields modest improvements over standard prompting (e.g., HotpotQA +10 pts; GSM8K +2 pts). Intrinsic self-correction typically degrades performance (GSM8K and HotpotQA drop across rounds), showing that improvements depend on oracle feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Although GPT-4 is more conservative (retains answers more often), intrinsic reflection still leads to answer degradation in several settings; authors find model confidence/robustness affects susceptibility to reflection prompts, and intrinsic judging of correctness remains unreliable.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Cannot Self-Correct Reasoning Yet', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5210.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5210.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4-Turbo (gpt-4-1106-preview)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A GPT-4 family model variant (gpt-4-1106-preview) evaluated for intrinsic self-correction with different feedback prompt phrasings; reported to be somewhat robust but still not reliably improving via intrinsic reflection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-4-1106-preview (GPT-4-Turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-4-Turbo variant accessed by authors and evaluated at temperature 0 for intrinsic self-correction experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Intrinsic Self-Correction (generate-then-reflect with multiple feedback prompt variants)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Three-step pipeline (initial answer, self-review feedback prompt, revised answer); authors tested multiple feedback prompt formulations (e.g., 'Assume that this answer could be either correct or incorrect...' ; 'Review your previous answer and determine whether it's correct...' ; 'Verify whether your answer is correct, and provide an explanation.'). Up to two rounds.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>2</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K, CommonSenseQA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>See earlier entries.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Depending on feedback prompt: default feedback prompt: round1 (3 calls) GSM8K 88.0%, CommonSenseQA 81.5%; round2 (5 calls) GSM8K 90.0%, CommonSenseQA 83.0%. With alternate prompts performance varies, e.g., 'determine whether it's correct' prompt: round1 GSM8K 90.0%, CommonSenseQA 74.5%; 'Verify and explain' prompt: round1 GSM8K 91.0%, CommonSenseQA 81.5% (Table 5).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Standard prompting (single call): GSM8K 91.5%, CommonSenseQA 84.0% (Table 5).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Some feedback prompts show minor recovery toward baseline on round2 (e.g., some combinations reach near standard prompting), but overall intrinsic self-correction does not yield consistent improvements over the single-call baseline; in many cases reflection gives worse results.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Performance depends strongly on the exact feedback prompt; no feedback prompt tested produced consistent intrinsic improvement across tasks. Authors conclude that variant phrasing can bias the model and that better initial prompts obviate the apparent benefit of reflection.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Cannot Self-Correct Reasoning Yet', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5210.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5210.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-2 (70b-chat)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama-2-70b-chat</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 70-billion-parameter open-source chat model (Llama-2-70b-chat) evaluated for intrinsic self-correction and shown to degrade substantially under reflection prompts on reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-2-70b-chat</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source Llama-2 family chat model (70B) evaluated at temperature 0 in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Intrinsic Self-Correction (generate-then-reflect with multiple feedback prompt variants)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Same three-step pipeline; authors tested multiple feedback prompts (same variants as GPT-4-Turbo) and allowed up to two rounds.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>2</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K, CommonSenseQA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>See earlier entries.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Default feedback prompt: round1 (3 calls) GSM8K 43.5%, CommonSenseQA 37.5%; round2 (5 calls) GSM8K 36.5%, CommonSenseQA 36.5%. Alternative feedback prompts produce variable but generally poor results: e.g., 'determine whether it's correct' prompt: round1 GSM8K 46.5%, CommonSenseQA 26.0% ; 'Verify and explain' prompt: round1 GSM8K 58.0%, CommonSenseQA 24.0% (Table 6).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Standard prompting (single call): GSM8K 62.0%, CommonSenseQA 64.0% (Table 4/6).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>No evidence of intrinsic improvement; in nearly all prompt variants self-correction substantially reduced accuracy relative to standard prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Llama-2 frequently changes correct answers to incorrect ones under reflection; sensitive to feedback prompt phrasing and overall shows substantial degradation when asked to self-criticize without external verification.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Cannot Self-Correct Reasoning Yet', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5210.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5210.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multi-Agent Debate (Du et al. replication)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-Agent Debate (multi-agent critique / debate among model copies)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method where multiple instances/agents of the same LLM generate, critique, and debate to arrive at a final answer; authors replicate Du et al.'s setup and compare it to self-consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo-0301 (replication)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Replication using gpt-3.5-turbo-0301 with 3 agents and 2 rounds of debate on GSM8K (authors followed Du et al.'s prompts).</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Multi-Agent Debate (agent critique/debate)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Multiple agent instances iteratively critique each other's outputs (3 agents, 2 rounds in replication) producing multiple responses; a model-driven selection mechanism chooses final answer (debate-based selection rather than simple majority voting).</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>2</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Grade-school math word problems; authors run full GSM8K test set for replication.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Multi-Agent Debate (6 responses; round1 reported) GSM8K 83.2% (Table 7). With 9 responses (round2) result ~83.0%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Standard prompting (1 response) GSM8K 76.7%; Self-Consistency baselines: 3 responses 82.5%, 6 responses 85.3%, 9 responses 88.2% (Table 7).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Multi-agent debate improves over single-call baseline (76.7% -> 83.2%) but when compared to self-consistency with the same number of responses it underperforms (multi-agent 83.2% vs self-consistency 85.3% with 6 responses). Authors interpret the gains as due to multiple-response selection rather than genuine self-correction.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Authors argue multi-agent debate's apparent gains are attributable to aggregating multiple responses (selection effect) rather than model-internal correction; when controlling for number of responses, self-consistency (majority voting) outperforms multi-agent debate, undermining claims that debate alone enables intrinsic self-correction.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Cannot Self-Correct Reasoning Yet', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5210.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5210.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Consistency (majority-vote over multiple samples)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Consistency (Wang et al., 2022)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A strategy that samples multiple chain-of-thought answers from an LLM and selects the final answer by majority vote; used as a baseline comparison in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-consistency improves chain of thought reasoning in language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Self-consistency (applied to GPT samplings)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Non-reflective ensemble method: sample multiple independent generations (different seeds/temperatures) and choose the most frequent final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Consistency (majority voting over multiple generations)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generate N answers (with chain-of-thought), then choose final answer by plurality/majority vote; no explicit reflection/feedback steps between generations.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Grade-school math problems used to compare multi-response baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Self-Consistency (3 responses) GSM8K 82.5%; (6 responses) 85.3%; (9 responses) 88.2% (Table 7).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Standard prompting (1 response) 76.7% (Table 7).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Consistent quantitative gains as number of sampled responses increases; outperforms multi-agent debate when controlling for number of responses, indicating multiple independent draws and majority vote are effective.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not a self-reflection method per se (no internal critique); gains come from ensembling multiple independent samples and thus incur increased inference cost; does not address intrinsic verification of correctness by a single model run.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Cannot Self-Correct Reasoning Yet', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5210.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5210.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Refine / Self-Refine (Madaan et al.) - Constrained Generation case</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Refine: Iterative refinement with self-feedback (Madaan et al., 2023) - evaluated on Constrained Generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative self-feedback refinement method (Self-Refine) previously reported to improve constrained generation (CommonGen-Hard); authors analyze prompt-design confounds and show that a stronger initial prompt can obviate reported gains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-refine: Iterative refinement with self-feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo-0613 (replication of Madaan et al. prompts/results)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Self-refine method as in Madaan et al. (2023) was replicated using gpt-3.5-turbo-0613 in the paper's constrained-generation experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Refine / Self-Correct (iterative refine with self-feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Iterative pipeline where model generates a candidate, identifies missing constraints/concepts in feedback step, then refines the output to include missing elements; in Madaan et al. prompts the missing-concepts question was part of feedback rather than initial instruction.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>7</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>CommonGen-Hard (Constrained Generation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Task requires generating a coherent sentence/paragraph that includes all provided concepts (20-30 concepts), metric: concept coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Reported Madaan et al. self-correct: CommonGen-Hard up to 67.0% (starred results from Madaan et al.). Replication shows Self-Correct* 7 calls -> 61.1% when applied to their prompt set; when authors apply Self-Correct* to their stronger initial prompt, Self-Correct* (7 calls) -> 75.1% (Table 8).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Original Standard Prompting* (Madaan) 44.0% (starred) or 53.0% in another reported run; Authors' improved Standard Prompting (explicitly instructing to include ALL concepts) achieves 81.8% without any self-correction (Table 8).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Madaan et al. reported large gains from self-refine over their original standard prompt (e.g., 44.0% -> 67.0%). However, the paper shows these gains largely stem from an under-specified original prompt; when the initial prompt explicitly required inclusion of all concepts, standard single-pass prompting (authors' prompt) outperforms the self-refine results, demonstrating that self-refinement improvements can be attributable to prompt-design confounds.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>If the initial prompt omits crucial task constraints that are only provided in the feedback step, self-refinement appears to help but is actually compensating for suboptimal initial instructions. When the initial prompt is properly specified, self-refinement can harm performance (authors show self-correct on top of the strong prompt decreases performance). Thus, reported improvements can be due to unequal prompt effort rather than intrinsic self-correction capability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Cannot Self-Correct Reasoning Yet', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language models can solve computer tasks. <em>(Rating: 2)</em></li>
                <li>Reflexion: Language agents with verbal reinforcement learning <em>(Rating: 2)</em></li>
                <li>Self-refine: Iterative refinement with self-feedback. <em>(Rating: 2)</em></li>
                <li>Improving factuality and reasoning in language models through multiagent debate. <em>(Rating: 2)</em></li>
                <li>Teaching large language models to self-debug. <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models. <em>(Rating: 2)</em></li>
                <li>Critic: Large language models can self-correct with tool-interactive critiquing. <em>(Rating: 1)</em></li>
                <li>RARR: Researching and revising what language models say, using language models. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5210",
    "paper_id": "paper-6d4bacb69923e1e94fb4de468b939ce6db32fb51",
    "extraction_schema_id": "extraction-schema-110",
    "extracted_data": [
        {
            "name_short": "GPT-3.5 (gpt-3.5-turbo-0613)",
            "name_full": "GPT-3.5-Turbo (gpt-3.5-turbo-0613)",
            "brief_description": "A chat-oriented GPT-3.5 family model accessed via OpenAI API (kernel gpt-3.5-turbo-0613) used in the paper's experiments to evaluate self-correction strategies on reasoning benchmarks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo-0613",
            "model_description": "OpenAI GPT-3.5 family chat model (API kernel gpt-3.5-turbo-0613) used with temperature 1 in the experiments.",
            "reflection_method_name": "Self-Correct (generate-then-reflect; intrinsic and oracle variants)",
            "reflection_method_description": "Three-step prompting: (1) initial generation, (2) prompt the model to review its prior answer and produce feedback, (3) answer original question again using that feedback. Two variants tested: (A) oracle-guided loop that uses ground-truth labels to decide when to stop (oracle self-correct), (B) intrinsic self-correction where the model must judge correctness itself and may perform up to two rounds.",
            "num_iterations": 2,
            "task_name": "GSM8K, CommonSenseQA, HotpotQA",
            "task_description": "GSM8K: grade-school math word problems (1,319 test questions); CommonSenseQA: multiple-choice commonsense questions (dev set 1,221); HotpotQA: open-domain multi-hop QA (closed-book subset of 100 questions used by prior work).",
            "performance_with_reflection": "Oracle-guided self-correct: GSM8K 84.3% (vs standard 75.9%), CommonSenseQA 89.7% (vs 75.8%), HotpotQA 29.0% (vs 26.0%) (Table 2). Intrinsic self-correction (no oracle): round1 (3 calls) GSM8K 75.1%, CommonSenseQA 38.1%, HotpotQA 25.0%; round2 (5 calls) GSM8K 74.7%, CommonSenseQA 41.8%, HotpotQA 25.0% (Table 3).",
            "performance_without_reflection": "Standard prompting (single call): GSM8K 75.9%, CommonSenseQA 75.8%, HotpotQA 26.0% (Table 2/3).",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Clear quantitative improvement when oracle labels were used to guide the self-correction loop (e.g., GSM8K +~8.4 percentage points). However, in intrinsic (no-oracle) setting, performance decreased or did not improve relative to standard prompting, showing no evidence of intrinsic self-reflection improving reasoning for this model.",
            "limitations_or_failure_cases": "Without oracle labels GPT-3.5 often changes correct answers to incorrect ones more frequently than correcting incorrect answers (authors report high 'correct -&gt; incorrect' rate); intrinsic self-correction degrades performance on CommonSenseQA and HotpotQA dramatically, implying inability to reliably judge its own answers; reliance on oracle labels is unrealistic in deployed settings.",
            "uuid": "e5210.0",
            "source_info": {
                "paper_title": "Large Language Models Cannot Self-Correct Reasoning Yet",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GPT-4 (gpt-4)",
            "name_full": "GPT-4 (OpenAI GPT-4)",
            "brief_description": "OpenAI's GPT-4 model evaluated in both oracle-guided and intrinsic self-correction setups to test whether self-reflection improves reasoning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-4 (accessed 2023-08-29)",
            "model_description": "OpenAI GPT-4 model (API), evaluated with temperature 1 for the reported experiments.",
            "reflection_method_name": "Self-Correct (generate-then-reflect; intrinsic and oracle variants)",
            "reflection_method_description": "Same three-step prompting pipeline as for GPT-3.5. Oracle variant uses ground-truth to decide when to stop; intrinsic variant relies on model self-judgment and up to two rounds of reflection.",
            "num_iterations": 2,
            "task_name": "GSM8K, CommonSenseQA, HotpotQA",
            "task_description": "See GPT-3.5 entry.",
            "performance_with_reflection": "Oracle-guided self-correct: GSM8K 97.5% (vs standard 95.5%), CommonSenseQA 85.5% (vs 82.0%), HotpotQA 59.0% (vs 49.0%) (Table 2). Intrinsic self-correction (no oracle): round1 (3 calls) GSM8K 91.5%, CommonSenseQA 79.5%, HotpotQA 49.0%; round2 (5 calls) GSM8K 89.0%, CommonSenseQA 80.0%, HotpotQA 43.0% (Table 3).",
            "performance_without_reflection": "Standard prompting: GSM8K 95.5%, CommonSenseQA 82.0%, HotpotQA 49.0% (Table 2/3).",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Oracle-guided self-correction yields modest improvements over standard prompting (e.g., HotpotQA +10 pts; GSM8K +2 pts). Intrinsic self-correction typically degrades performance (GSM8K and HotpotQA drop across rounds), showing that improvements depend on oracle feedback.",
            "limitations_or_failure_cases": "Although GPT-4 is more conservative (retains answers more often), intrinsic reflection still leads to answer degradation in several settings; authors find model confidence/robustness affects susceptibility to reflection prompts, and intrinsic judging of correctness remains unreliable.",
            "uuid": "e5210.1",
            "source_info": {
                "paper_title": "Large Language Models Cannot Self-Correct Reasoning Yet",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GPT-4-Turbo",
            "name_full": "GPT-4-Turbo (gpt-4-1106-preview)",
            "brief_description": "A GPT-4 family model variant (gpt-4-1106-preview) evaluated for intrinsic self-correction with different feedback prompt phrasings; reported to be somewhat robust but still not reliably improving via intrinsic reflection.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-4-1106-preview (GPT-4-Turbo)",
            "model_description": "GPT-4-Turbo variant accessed by authors and evaluated at temperature 0 for intrinsic self-correction experiments.",
            "reflection_method_name": "Intrinsic Self-Correction (generate-then-reflect with multiple feedback prompt variants)",
            "reflection_method_description": "Three-step pipeline (initial answer, self-review feedback prompt, revised answer); authors tested multiple feedback prompt formulations (e.g., 'Assume that this answer could be either correct or incorrect...' ; 'Review your previous answer and determine whether it's correct...' ; 'Verify whether your answer is correct, and provide an explanation.'). Up to two rounds.",
            "num_iterations": 2,
            "task_name": "GSM8K, CommonSenseQA",
            "task_description": "See earlier entries.",
            "performance_with_reflection": "Depending on feedback prompt: default feedback prompt: round1 (3 calls) GSM8K 88.0%, CommonSenseQA 81.5%; round2 (5 calls) GSM8K 90.0%, CommonSenseQA 83.0%. With alternate prompts performance varies, e.g., 'determine whether it's correct' prompt: round1 GSM8K 90.0%, CommonSenseQA 74.5%; 'Verify and explain' prompt: round1 GSM8K 91.0%, CommonSenseQA 81.5% (Table 5).",
            "performance_without_reflection": "Standard prompting (single call): GSM8K 91.5%, CommonSenseQA 84.0% (Table 5).",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Some feedback prompts show minor recovery toward baseline on round2 (e.g., some combinations reach near standard prompting), but overall intrinsic self-correction does not yield consistent improvements over the single-call baseline; in many cases reflection gives worse results.",
            "limitations_or_failure_cases": "Performance depends strongly on the exact feedback prompt; no feedback prompt tested produced consistent intrinsic improvement across tasks. Authors conclude that variant phrasing can bias the model and that better initial prompts obviate the apparent benefit of reflection.",
            "uuid": "e5210.2",
            "source_info": {
                "paper_title": "Large Language Models Cannot Self-Correct Reasoning Yet",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Llama-2 (70b-chat)",
            "name_full": "Llama-2-70b-chat",
            "brief_description": "A 70-billion-parameter open-source chat model (Llama-2-70b-chat) evaluated for intrinsic self-correction and shown to degrade substantially under reflection prompts on reasoning tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama-2-70b-chat",
            "model_description": "Open-source Llama-2 family chat model (70B) evaluated at temperature 0 in these experiments.",
            "reflection_method_name": "Intrinsic Self-Correction (generate-then-reflect with multiple feedback prompt variants)",
            "reflection_method_description": "Same three-step pipeline; authors tested multiple feedback prompts (same variants as GPT-4-Turbo) and allowed up to two rounds.",
            "num_iterations": 2,
            "task_name": "GSM8K, CommonSenseQA",
            "task_description": "See earlier entries.",
            "performance_with_reflection": "Default feedback prompt: round1 (3 calls) GSM8K 43.5%, CommonSenseQA 37.5%; round2 (5 calls) GSM8K 36.5%, CommonSenseQA 36.5%. Alternative feedback prompts produce variable but generally poor results: e.g., 'determine whether it's correct' prompt: round1 GSM8K 46.5%, CommonSenseQA 26.0% ; 'Verify and explain' prompt: round1 GSM8K 58.0%, CommonSenseQA 24.0% (Table 6).",
            "performance_without_reflection": "Standard prompting (single call): GSM8K 62.0%, CommonSenseQA 64.0% (Table 4/6).",
            "has_performance_comparison": true,
            "evidence_of_improvement": "No evidence of intrinsic improvement; in nearly all prompt variants self-correction substantially reduced accuracy relative to standard prompting.",
            "limitations_or_failure_cases": "Llama-2 frequently changes correct answers to incorrect ones under reflection; sensitive to feedback prompt phrasing and overall shows substantial degradation when asked to self-criticize without external verification.",
            "uuid": "e5210.3",
            "source_info": {
                "paper_title": "Large Language Models Cannot Self-Correct Reasoning Yet",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Multi-Agent Debate (Du et al. replication)",
            "name_full": "Multi-Agent Debate (multi-agent critique / debate among model copies)",
            "brief_description": "A method where multiple instances/agents of the same LLM generate, critique, and debate to arrive at a final answer; authors replicate Du et al.'s setup and compare it to self-consistency.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo-0301 (replication)",
            "model_description": "Replication using gpt-3.5-turbo-0301 with 3 agents and 2 rounds of debate on GSM8K (authors followed Du et al.'s prompts).",
            "reflection_method_name": "Multi-Agent Debate (agent critique/debate)",
            "reflection_method_description": "Multiple agent instances iteratively critique each other's outputs (3 agents, 2 rounds in replication) producing multiple responses; a model-driven selection mechanism chooses final answer (debate-based selection rather than simple majority voting).",
            "num_iterations": 2,
            "task_name": "GSM8K",
            "task_description": "Grade-school math word problems; authors run full GSM8K test set for replication.",
            "performance_with_reflection": "Multi-Agent Debate (6 responses; round1 reported) GSM8K 83.2% (Table 7). With 9 responses (round2) result ~83.0%.",
            "performance_without_reflection": "Standard prompting (1 response) GSM8K 76.7%; Self-Consistency baselines: 3 responses 82.5%, 6 responses 85.3%, 9 responses 88.2% (Table 7).",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Multi-agent debate improves over single-call baseline (76.7% -&gt; 83.2%) but when compared to self-consistency with the same number of responses it underperforms (multi-agent 83.2% vs self-consistency 85.3% with 6 responses). Authors interpret the gains as due to multiple-response selection rather than genuine self-correction.",
            "limitations_or_failure_cases": "Authors argue multi-agent debate's apparent gains are attributable to aggregating multiple responses (selection effect) rather than model-internal correction; when controlling for number of responses, self-consistency (majority voting) outperforms multi-agent debate, undermining claims that debate alone enables intrinsic self-correction.",
            "uuid": "e5210.4",
            "source_info": {
                "paper_title": "Large Language Models Cannot Self-Correct Reasoning Yet",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Self-Consistency (majority-vote over multiple samples)",
            "name_full": "Self-Consistency (Wang et al., 2022)",
            "brief_description": "A strategy that samples multiple chain-of-thought answers from an LLM and selects the final answer by majority vote; used as a baseline comparison in the paper.",
            "citation_title": "Self-consistency improves chain of thought reasoning in language models.",
            "mention_or_use": "use",
            "model_name": "Self-consistency (applied to GPT samplings)",
            "model_description": "Non-reflective ensemble method: sample multiple independent generations (different seeds/temperatures) and choose the most frequent final answer.",
            "reflection_method_name": "Self-Consistency (majority voting over multiple generations)",
            "reflection_method_description": "Generate N answers (with chain-of-thought), then choose final answer by plurality/majority vote; no explicit reflection/feedback steps between generations.",
            "num_iterations": null,
            "task_name": "GSM8K",
            "task_description": "Grade-school math problems used to compare multi-response baselines.",
            "performance_with_reflection": "Self-Consistency (3 responses) GSM8K 82.5%; (6 responses) 85.3%; (9 responses) 88.2% (Table 7).",
            "performance_without_reflection": "Standard prompting (1 response) 76.7% (Table 7).",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Consistent quantitative gains as number of sampled responses increases; outperforms multi-agent debate when controlling for number of responses, indicating multiple independent draws and majority vote are effective.",
            "limitations_or_failure_cases": "Not a self-reflection method per se (no internal critique); gains come from ensembling multiple independent samples and thus incur increased inference cost; does not address intrinsic verification of correctness by a single model run.",
            "uuid": "e5210.5",
            "source_info": {
                "paper_title": "Large Language Models Cannot Self-Correct Reasoning Yet",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Self-Refine / Self-Refine (Madaan et al.) - Constrained Generation case",
            "name_full": "Self-Refine: Iterative refinement with self-feedback (Madaan et al., 2023) - evaluated on Constrained Generation",
            "brief_description": "An iterative self-feedback refinement method (Self-Refine) previously reported to improve constrained generation (CommonGen-Hard); authors analyze prompt-design confounds and show that a stronger initial prompt can obviate reported gains.",
            "citation_title": "Self-refine: Iterative refinement with self-feedback.",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo-0613 (replication of Madaan et al. prompts/results)",
            "model_description": "Self-refine method as in Madaan et al. (2023) was replicated using gpt-3.5-turbo-0613 in the paper's constrained-generation experiments.",
            "reflection_method_name": "Self-Refine / Self-Correct (iterative refine with self-feedback)",
            "reflection_method_description": "Iterative pipeline where model generates a candidate, identifies missing constraints/concepts in feedback step, then refines the output to include missing elements; in Madaan et al. prompts the missing-concepts question was part of feedback rather than initial instruction.",
            "num_iterations": 7,
            "task_name": "CommonGen-Hard (Constrained Generation)",
            "task_description": "Task requires generating a coherent sentence/paragraph that includes all provided concepts (20-30 concepts), metric: concept coverage.",
            "performance_with_reflection": "Reported Madaan et al. self-correct: CommonGen-Hard up to 67.0% (starred results from Madaan et al.). Replication shows Self-Correct* 7 calls -&gt; 61.1% when applied to their prompt set; when authors apply Self-Correct* to their stronger initial prompt, Self-Correct* (7 calls) -&gt; 75.1% (Table 8).",
            "performance_without_reflection": "Original Standard Prompting* (Madaan) 44.0% (starred) or 53.0% in another reported run; Authors' improved Standard Prompting (explicitly instructing to include ALL concepts) achieves 81.8% without any self-correction (Table 8).",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Madaan et al. reported large gains from self-refine over their original standard prompt (e.g., 44.0% -&gt; 67.0%). However, the paper shows these gains largely stem from an under-specified original prompt; when the initial prompt explicitly required inclusion of all concepts, standard single-pass prompting (authors' prompt) outperforms the self-refine results, demonstrating that self-refinement improvements can be attributable to prompt-design confounds.",
            "limitations_or_failure_cases": "If the initial prompt omits crucial task constraints that are only provided in the feedback step, self-refinement appears to help but is actually compensating for suboptimal initial instructions. When the initial prompt is properly specified, self-refinement can harm performance (authors show self-correct on top of the strong prompt decreases performance). Thus, reported improvements can be due to unequal prompt effort rather than intrinsic self-correction capability.",
            "uuid": "e5210.6",
            "source_info": {
                "paper_title": "Large Language Models Cannot Self-Correct Reasoning Yet",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language models can solve computer tasks.",
            "rating": 2,
            "sanitized_title": "language_models_can_solve_computer_tasks"
        },
        {
            "paper_title": "Reflexion: Language agents with verbal reinforcement learning",
            "rating": 2,
            "sanitized_title": "reflexion_language_agents_with_verbal_reinforcement_learning"
        },
        {
            "paper_title": "Self-refine: Iterative refinement with self-feedback.",
            "rating": 2,
            "sanitized_title": "selfrefine_iterative_refinement_with_selffeedback"
        },
        {
            "paper_title": "Improving factuality and reasoning in language models through multiagent debate.",
            "rating": 2,
            "sanitized_title": "improving_factuality_and_reasoning_in_language_models_through_multiagent_debate"
        },
        {
            "paper_title": "Teaching large language models to self-debug.",
            "rating": 2,
            "sanitized_title": "teaching_large_language_models_to_selfdebug"
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models.",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "Critic: Large language models can self-correct with tool-interactive critiquing.",
            "rating": 1,
            "sanitized_title": "critic_large_language_models_can_selfcorrect_with_toolinteractive_critiquing"
        },
        {
            "paper_title": "RARR: Researching and revising what language models say, using language models.",
            "rating": 1,
            "sanitized_title": "rarr_researching_and_revising_what_language_models_say_using_language_models"
        }
    ],
    "cost": 0.018251999999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Large Language Models Cannot Self-Correct REASONING YET</h1>
<p>Jie Huang ${ }^{1,2 <em>}$ Xinyun Chen ${ }^{1 </em>}$ Swaroop Mishra ${ }^{1}$ Huaixiu Steven Zheng ${ }^{1}$ Adams Wei Yu ${ }^{1}$ Xinying Song ${ }^{1}$ Denny Zhou ${ }^{1}$<br>${ }^{1}$ Google DeepMind ${ }^{2}$ University of Illinois at Urbana-Champaign<br>jeffhj@illinois.edu, {xinyunchen, dennyzhou}@google.com</p>
<h4>Abstract</h4>
<p>Large Language Models (LLMs) have emerged as a groundbreaking technology with their unparalleled text generation capabilities across various applications. Nevertheless, concerns persist regarding the accuracy and appropriateness of their generated content. A contemporary methodology, self-correction, has been proposed as a remedy to these issues. Building upon this premise, this paper critically examines the role and efficacy of self-correction within LLMs, shedding light on its true potential and limitations. Central to our investigation is the notion of intrinsic self-correction, whereby an LLM attempts to correct its initial responses based solely on its inherent capabilities, without the crutch of external feedback. In the context of reasoning, our research indicates that LLMs struggle to selfcorrect their responses without external feedback, and at times, their performance even degrades after self-correction. Drawing from these insights, we offer suggestions for future research and practical applications in this field.</p>
<h2>1 INTRODUCTION</h2>
<p>The rapid advancements in the domain of artificial intelligence have ushered in the era of Large Language Models (LLMs). These models, characterized by their expansive parameter counts and unparalleled capabilities in text generation, have showcased promising results across a multitude of applications (Chowdhery et al., 2023; Anil et al., 2023; OpenAI, 2023, inter alia). However, concerns about their accuracy, reasoning capabilities, and the safety of their generated content have drawn significant attention from the community (Bang et al., 2023; Alkaissi \&amp; McFarlane, 2023; Zheng et al., 2023; Shi et al., 2023; Carlini et al., 2021; Huang et al., 2022; Shao et al., 2023; Li et al., 2023; Wei et al., 2023; Zhou et al., 2023b; Zou et al., 2023, inter alia).</p>
<p>Amidst this backdrop, the concept of "self-correction" has emerged as a promising solution, where LLMs refine their responses based on feedback to their previous outputs (Madaan et al., 2023; Welleck et al., 2023; Shinn et al., 2023; Kim et al., 2023; Bai et al., 2022; Ganguli et al., 2023; Gao et al., 2023; Paul et al., 2023; Chen et al., 2023b; Pan et al., 2023, inter alia). However, the underlying mechanics and efficacy of self-correction in LLMs remain underexplored. A fundamental question arises: If an LLM possesses the ability to self-correct, why doesn't it simply offer the correct answer in its initial attempt? This paper delves deeply into this paradox, critically examining the self-correction capabilities of LLMs, with a particular emphasis on reasoning (Wei et al., 2022; Zhou et al., 2023b; Huang \&amp; Chang, 2023).</p>
<p>To study this, we first define the concept of intrinsic self-correction, a scenario wherein the model endeavors to rectify its initial responses based solely on its inherent capabilities, without the crutch of external feedback. Such a setting is crucial because high-quality external feedback is often unavailable in many real-world applications. Moreover, it is vital to understand the intrinsic capabilities of LLMs. Contrary to the optimism surrounding self-correction (Madaan et al., 2023; Kim et al., 2023; Shinn et al., 2023; Pan et al., 2023, inter alia), our findings indicate that LLMs struggle to self-correct their reasoning in this setting. In most instances, the performance after self-correction</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>even deteriorates. This observation is in contrast to prior research such as Kim et al. (2023); Shinn et al. (2023). Upon closer examination, we observe that the improvements in these studies result from using oracle labels to guide the self-correction process, and the improvements vanish when oracle labels are not available.</p>
<p>Besides the reliance on oracle labels, we also identify other issues in the literature regarding measuring the improvement achieved by self-correction. First, we note that self-correction, by design, utilizes multiple LLM responses, thus making it crucial to compare it to baselines with equivalent inference costs. From this perspective, we investigate multi-agent debate (Du et al., 2023; Liang et al., 2023) as a means to improve reasoning, where multiple LLM instances (can be multiple copies of the same LLM) critique each other's responses. However, our results reveal that its efficacy is no better than self-consistency (Wang et al., 2022) when considering an equivalent number of responses, highlighting the limitations of such an approach.</p>
<p>Another important consideration for self-correction involves prompt design. Specifically, each selfcorrection process involves designing prompts for both the initial response generation and the selfcorrection steps. Our evaluation reveals that the self-correction improvement claimed by some existing work stems from the sub-optimal prompt for generating initial responses, where self-correction corrects these responses with more informative instructions about the initial task in the feedback prompt. In such cases, simply integrating the feedback into the initial instruction can yield better results, and self-correction again decreases performance.</p>
<p>In light of our findings, we provide insights into the nuances of LLMs' self-correction capabilities and initiate discussions to encourage future research focused on exploring methods that can genuinely correct reasoning.</p>
<h1>2 BACKGROUND AND RELATED WORK</h1>
<p>With the LLM evolution, the notion of self-correction gained prominence. The discourse on selfcorrection pivots around whether these advanced models can recognize the correctness of their outputs and provide refined answers (Bai et al., 2022; Madaan et al., 2023; Welleck et al., 2023, inter alia). For example, in the context of mathematical reasoning, an LLM might initially solve a complex problem but make an error in one of the calculation steps. In an ideal self-correction scenario, the model is expected to recognize the potential mistake, revisit the problem, correct the error, and consequently produce a more accurate solution.</p>
<p>Yet, the definition of "self-correction" varies across the literature, leading to ambiguity. A pivotal distinction lies in the source of feedback (Pan et al., 2023), which can purely come from the LLM, or can be drawn from external inputs. Internal feedback relies on the model's inherent knowledge and parameters to reassess its outputs. In contrast, external feedback incorporates inputs from humans, other models (Wang et al., 2023b; Paul et al., 2023, inter alia), or external tools and knowledge sources (Gou et al., 2023; Chen et al., 2023b; Olausson et al., 2023; Gao et al., 2023, inter alia).</p>
<p>In this work, we focus on examining the self-correction capability of LLMs for reasoning. Reasoning is a fundamental aspect of human cognition, enabling us to understand the world, draw inferences, make decisions, and solve problems. To enhance the reasoning performance of LLMs, Kim et al. (2023); Shinn et al. (2023) use oracle labels about the answer correctness to guide the self-correction process. However, in practice, high-quality external feedback such as answer correctness is often unavailable. For effective self-correction, the ability to judge the correctness of an answer is crucial and should ideally be performed by the LLM itself. Consequently, our focus shifts to self-correction without any external or human feedback. We term this setting intrinsic self-correction. For brevity, unless explicitly stated otherwise (e.g., self-correction with oracle feedback), all references to "selfcorrection" in the remainder of this paper pertain to intrinsic self-correction.</p>
<p>In the following sections, we will evaluate a variety of existing self-correction techniques. We demonstrate that existing techniques actually decrease reasoning performance when oracle labels are not used (Section 3), perform worse than methods without self-correction when utilizing the same number of model responses (Section 4), and lead to less effective outcomes when using informative prompts for generating initial responses (Section 5). We present an overview of issues in the evaluation setups of previous LLM self-correction works in Table 1, with detailed discussions in the corresponding sections.</p>
<p>Table 1: Summary of issues in previous LLM self-correction evaluation.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: left;">Issue</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">RCI (Kim et al., 2023); Reflexion (Shinn et al., 2023)</td>
<td style="text-align: left;">Use of oracle labels (Section 3)</td>
</tr>
<tr>
<td style="text-align: left;">Multi-Agent Debate (Du et al., 2023)</td>
<td style="text-align: left;">Unfair comparison to self-consistency (Section 4)</td>
</tr>
<tr>
<td style="text-align: left;">Self-Refine (Madaan et al., 2023)</td>
<td style="text-align: left;">Sub-optimal prompt design (Section 5)</td>
</tr>
</tbody>
</table>
<h1>3 LLMs CANNOT SELF-CORRECT REASONING INTRINSICALLY</h1>
<p>In this section, we evaluate existing self-correction methods and compare their performance with and without oracle labels regarding the answer correctness.</p>
<h3>3.1 EXPERIMENTAL SETUP</h3>
<p>Benchmarks. We use datasets where existing self-correction methods with oracle labels have demonstrated significant performance improvement, including</p>
<ul>
<li>GSM8K (Cobbe et al., 2021): GSM8K comprises a test set of 1,319 linguistically diverse grade school math word problems, curated by human problem writers. There is a notable improvement of approximately $7 \%$ as evidenced by Kim et al. (2023) after self-correction.</li>
<li>CommonSenseQA (Talmor et al., 2019): This dataset offers a collection of multi-choice questions that test commonsense reasoning. An impressive increase of around $15 \%$ is showcased through the self-correction process, as demonstrated by Kim et al. (2023). Following Kojima et al. (2022); Kim et al. (2023), we utilize the dev set for our evaluation, which encompasses 1,221 questions.</li>
<li>HotpotQA (Yang et al., 2018): HotpotQA is an open-domain multi-hop question answering dataset. Shinn et al. (2023) demonstrate significant performance improvement through selfcorrection. We test models' performance in a closed-book setting and evaluate them using the same set as Shinn et al. (2023). This set contains 100 questions, with exact match serving as the evaluation metric.</li>
</ul>
<p>Test Models and Setup. We first follow Kim et al. (2023); Shinn et al. (2023) to evaluate the performance of self-correction with oracle labels, using GPT-3.5-Turbo (gpt-3.5-turbo-0613) and GPT-4 accessed on 2023/08/29. For intrinsic self-correction, to provide a more thorough analysis, we also evaluate GPT-4-Turbo (gpt-4-1106-preview) and Llama-2 (Llama-2-70b-chat) (Touvron et al., 2023). For GPT-3.5-Turbo, we employ the full evaluation set. For other models, to reduce the cost, we randomly sample 200 questions for each dataset (100 for HotpotQA) for testing. We prompt the models to undergo a maximum of two rounds of self-correction. We use a temperature of 1 for GPT-3.5-Turbo and GPT-4, and a temperature of 0 for GPT-4-Turbo and Llama-2, to provide evaluation across different decoding algorithms.</p>
<p>Prompts. Following Kim et al. (2023); Shinn et al. (2023), we apply a three-step prompting strategy for self-correction: 1) prompt the model to perform an initial generation (which also serves as the results for Standard Prompting); 2) prompt the model to review its previous generation and produce feedback; 3) prompt the model to answer the original question again with the feedback.</p>
<p>For our experiments, we mostly adhere to the prompts from the source papers. For GSM8K and CommonSenseQA, we integrate format instructions into the prompts of Kim et al. (2023) to facilitate a more precise automatic evaluation (detailed prompts can be found in Appendix A). For HotpotQA, we use the same prompt as Shinn et al. (2023). We also assess the performance of various selfcorrection prompts for intrinsic self-correction. For example, we use "Assume that this answer could be either correct or incorrect. Review the answer carefully and report any serious problems you find." as the default feedback prompt for the evaluation on GPT-4-Turbo and Llama-2.</p>
<h3>3.2 ReSULTS</h3>
<p>Self-Correction with Oracle Labels. Following previous works (Kim et al., 2023; Shinn et al., 2023), we use the correct label to determine when to stop the self-correction loop. This means we</p>
<p>Table 2: Results of GPT-3.5 and GPT-4 on reasoning benchmarks with oracle labels.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: center;">GSM8K</th>
<th style="text-align: center;">CommonSenseQA</th>
<th style="text-align: center;">HotpotQA</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT-3.5</td>
<td style="text-align: left;">Standard Prompting</td>
<td style="text-align: center;">75.9</td>
<td style="text-align: center;">75.8</td>
<td style="text-align: center;">26.0</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Self-Correct (Oracle)</td>
<td style="text-align: center;">84.3</td>
<td style="text-align: center;">89.7</td>
<td style="text-align: center;">29.0</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4</td>
<td style="text-align: left;">Standard Prompting</td>
<td style="text-align: center;">95.5</td>
<td style="text-align: center;">82.0</td>
<td style="text-align: center;">49.0</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Self-Correct (Oracle)</td>
<td style="text-align: center;">97.5</td>
<td style="text-align: center;">85.5</td>
<td style="text-align: center;">59.0</td>
</tr>
</tbody>
</table>
<p>Table 3: Results of GPT-3.5 and GPT-4 on reasoning benchmarks with intrinsic self-correction.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: center;"># calls</th>
<th style="text-align: center;">GSM8K</th>
<th style="text-align: center;">CommonSenseQA</th>
<th style="text-align: center;">HotpotQA</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT-3.5</td>
<td style="text-align: left;">Standard Prompting</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$\mathbf{7 5 . 9}$</td>
<td style="text-align: center;">$\mathbf{7 5 . 8}$</td>
<td style="text-align: center;">$\mathbf{2 6 . 0}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Self-Correct (round 1)</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">75.1</td>
<td style="text-align: center;">38.1</td>
<td style="text-align: center;">25.0</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Self-Correct (round 2)</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">74.7</td>
<td style="text-align: center;">41.8</td>
<td style="text-align: center;">25.0</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4</td>
<td style="text-align: left;">Standard Prompting</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$\mathbf{9 5 . 5}$</td>
<td style="text-align: center;">$\mathbf{8 2 . 0}$</td>
<td style="text-align: center;">$\mathbf{4 9 . 0}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Self-Correct (round 1)</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">91.5</td>
<td style="text-align: center;">79.5</td>
<td style="text-align: center;">$\mathbf{4 9 . 0}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Self-Correct (round 2)</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">89.0</td>
<td style="text-align: center;">80.0</td>
<td style="text-align: center;">43.0</td>
</tr>
</tbody>
</table>
<p>utilize the ground-truth label to verify whether each step's generated answer is correct. If the answer is already correct, no (further) self-correction will be performed. Table 2 summarizes the results of self-correction under this setting, showcasing significant performance improvements, consistent with the findings presented in Kim et al. (2023); Shinn et al. (2023).</p>
<p>However, these results require careful consideration. For reasoning tasks, like solving mathematical problems, the availability of oracle labels seems counter-intuitive. If we are already in possession of the ground truth, there seems to be little reason to deploy LLMs for problem-solving. Therefore, the results can only be regarded as indicative of an oracle's performance.</p>
<p>Intrinsic Self-Correction. Per the above discussion, performance improvements achieved using oracle labels do not necessarily reflect true self-correction ability. Therefore, we turn our focus to the results in the intrinsic self-correction setting as defined in Section 2. To achieve this, we eliminate the use of labels, requiring LLMs to independently determine when to stop the self-correction process, i.e., whether to retain their previous answers.</p>
<p>Tables 3 and 4 report the accuracies and the number of model calls. We observe that, after selfcorrection, the accuracies of all models drop across all benchmarks.</p>
<p>To provide a more comprehensive assessment, we also design several different self-correction prompts to determine if there are better prompts that could enhance reasoning performance. Nonetheless, as shown in Tables 5 and 6, without the use of oracle labels, self-correction consistently results in a decrease in performance.</p>
<h1>3.3 WHY DOES THE PERFORMANCE NOT INCREASE, BUT INSTEAD DECREASE?</h1>
<p>Empirical Analysis. Figure 1 summarizes the results of changes in answers after two rounds of self-correction, with two examples of GPT-3.5 illustrated in Figure 2. For GSM8K, 74.7\% of the time, GPT-3.5 retains its initial answer. Among the remaining instances, the model is more likely to modify a correct answer to an incorrect one than to revise an incorrect answer to a correct one. The fundamental issue is that LLMs cannot properly judge the correctness of their reasoning. For CommonSenseQA, there is a higher chance that GPT-3.5 alters its answer. The primary reason for this is that false answer options in CommonSenseQA often appear somewhat relevant to the question, and using the self-correction prompt might bias the model to choose another option, leading to a high "correct $\Rightarrow$ incorrect" ratio. Similarly, Llama-2 also frequently converts a correct answer into an incorrect one. Compared to GPT-3.5 and Llama-2, both GPT-4 and GPT-4-Turbo are more likely to retain their initial answers. This may be because GPT-4 and GPT-4-Turbo have higher confidence</p>
<p>Table 4: Results of GPT-4-Turbo and Llama-2 with intrinsic self-correction.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: center;"># calls</th>
<th style="text-align: center;">GSM8K</th>
<th style="text-align: center;">CommonSenseQA</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT-4-Turbo</td>
<td style="text-align: left;">Standard Prompting</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$\mathbf{9 1 . 5}$</td>
<td style="text-align: center;">$\mathbf{8 4 . 0}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Self-Correct (round 1)</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">88.0</td>
<td style="text-align: center;">81.5</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Self-Correct (round 2)</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">90.0</td>
<td style="text-align: center;">83.0</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Standard Prompting</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$\mathbf{6 2 . 0}$</td>
<td style="text-align: center;">$\mathbf{6 4 . 0}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Self-Correct (round 1)</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">43.5</td>
<td style="text-align: center;">37.5</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Self-Correct (round 2)</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">36.5</td>
<td style="text-align: center;">36.5</td>
</tr>
</tbody>
</table>
<p>Table 5: Results of GPT-4-Turbo with different feedback prompts.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;"># calls</th>
<th style="text-align: center;">GSM8K</th>
<th style="text-align: center;">CommonSenseQA</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Standard Prompting</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">91.5</td>
<td style="text-align: center;">84.0</td>
</tr>
<tr>
<td style="text-align: left;">Feedback Prompt: Assume that this answer could be either correct or incorrect. <br> Review the answer carefully and report any serious problems you find.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Self-Correct (round 1)</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">88.0</td>
<td style="text-align: center;">81.5</td>
</tr>
<tr>
<td style="text-align: left;">Self-Correct (round 2)</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">90.0</td>
<td style="text-align: center;">83.0</td>
</tr>
<tr>
<td style="text-align: left;">Feedback Prompt: Review your previous answer and determine whether it's correct. <br> If wrong, find the problems with your answer.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Self-Correct (round 1)</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">90.0</td>
<td style="text-align: center;">74.5</td>
</tr>
<tr>
<td style="text-align: left;">Self-Correct (round 2)</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">90.0</td>
<td style="text-align: center;">81.0</td>
</tr>
<tr>
<td style="text-align: left;">Feedback Prompt: Verify whether your answer is correct, and provide an explanation.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Self-Correct (round 1)</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">91.0</td>
<td style="text-align: center;">81.5</td>
</tr>
<tr>
<td style="text-align: left;">Self-Correct (round 2)</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">91.0</td>
<td style="text-align: center;">83.5</td>
</tr>
</tbody>
</table>
<p>Table 6: Results of Llama-2 with different feedback prompts.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;"># calls</th>
<th style="text-align: center;">GSM8K</th>
<th style="text-align: center;">CommonSenseQA</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Standard Prompting</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">62.0</td>
<td style="text-align: center;">64.0</td>
</tr>
<tr>
<td style="text-align: left;">Feedback Prompt: Assume that this answer could be either correct or incorrect. <br> Review the answer carefully and report any serious problems you find.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Self-Correct (round 1)</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">43.5</td>
<td style="text-align: center;">37.5</td>
</tr>
<tr>
<td style="text-align: left;">Self-Correct (round 2)</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">36.5</td>
<td style="text-align: center;">36.5</td>
</tr>
<tr>
<td style="text-align: left;">Feedback Prompt: Review your previous answer and determine whether it's correct. <br> If wrong, find the problems with your answer.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Self-Correct (round 1)</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">46.5</td>
<td style="text-align: center;">26.0</td>
</tr>
<tr>
<td style="text-align: left;">Self-Correct (round 2)</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">30.5</td>
<td style="text-align: center;">37.0</td>
</tr>
<tr>
<td style="text-align: left;">Feedback Prompt: Verify whether your answer is correct, and provide an explanation.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Self-Correct (round 1)</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">58.0</td>
<td style="text-align: center;">24.0</td>
</tr>
<tr>
<td style="text-align: left;">Self-Correct (round 2)</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">41.5</td>
<td style="text-align: center;">43.0</td>
</tr>
</tbody>
</table>
<p>in their initial answers, or because they are more robust and thus less prone to being biased by the self-correction prompt. ${ }^{1}$</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Analysis of the changes in answers after two rounds of self-correction. No Change: The answer remains unchanged; Correct $\Rightarrow$ Incorrect: A correct answer is changed to an incorrect one; Incorrect $\Rightarrow$ Correct: An incorrect answer is revised to a correct one; Incorrect $\Rightarrow$ Incorrect: An incorrect answer is altered but remains incorrect.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Examples on GSM8K with GPT-3.5. Left: successful self-correction; Right: failed selfcorrection. Full prompts and responses can be viewed in Figures 3 and 4 of Appendix A.</p>
<p>Let's take another look at the results presented in Table 2. These results use ground-truth labels to prevent the model from altering a correct answer to an incorrect one. However, determining how to prevent such mischanges is, in fact, the key to ensuring the success of self-correction.</p>
<p>Intuitive Explanation. If the model is well-aligned and paired with a thoughtfully designed initial prompt, the initial response should already be optimal relative to the prompt and the specific decoding algorithm. Introducing feedback can be viewed as adding an additional prompt, potentially skewing the model towards generating a response that is tailored to this combined input. In an intrinsic self-correction setting, on the reasoning tasks, this supplementary prompt may not offer any extra advantage for answering the question. In fact, it might even bias the model away from producing an optimal response to the initial prompt, resulting in a performance drop.</p>
<p>Table 7: Results of multi-agent debate and self-consistency.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;"># responses</th>
<th style="text-align: center;">GSM8K</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Standard Prompting</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">76.7</td>
</tr>
<tr>
<td style="text-align: left;">Self-Consistency</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">82.5</td>
</tr>
<tr>
<td style="text-align: left;">Multi-Agent Debate (round 1)</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">83.2</td>
</tr>
<tr>
<td style="text-align: left;">Self-Consistency</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">85.3</td>
</tr>
<tr>
<td style="text-align: left;">Multi-Agent Debate (round 2)</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">83.0</td>
</tr>
<tr>
<td style="text-align: left;">Self-Consistency</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">$\mathbf{8 8 . 2}$</td>
</tr>
</tbody>
</table>
<p>Table 8: Results of Constrained Generation.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;"># calls</th>
<th style="text-align: center;">CommonGen-Hard</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Standard Prompting*</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$44.0^{*}$</td>
</tr>
<tr>
<td style="text-align: left;">Self-Correct*</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">$67.0^{*}$</td>
</tr>
<tr>
<td style="text-align: left;">Standard Prompting*</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">53.0</td>
</tr>
<tr>
<td style="text-align: left;">Self-Correct*</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">61.1</td>
</tr>
<tr>
<td style="text-align: left;">Standard Prompting (ours)</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$\mathbf{8 1 . 8}$</td>
</tr>
<tr>
<td style="text-align: left;">Self-Correct*</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">75.1</td>
</tr>
</tbody>
</table>
<ul>
<li>Prompts and results from Madaan et al. (2023).</li>
</ul>
<h1>4 Multi-Agent Debate does not Outperform Self-Consistency</h1>
<p>Another potential approach for LLMs to self-correct their reasoning involves allowing the models to critique and debate through multiple model calls (Du et al., 2023; Liang et al., 2023; Chen et al., 2023a). Du et al. (2023) implement a multi-agent debate method by leveraging multiple instances of a single ChatGPT model and demonstrate significant improvements on reasoning tasks. We adopt their method to test performance on GSM8K. For an unbiased implementation, we use the exact same prompt as Du et al. (2023) and replicate their experiment with the gpt-3.5-turbo-0301 model, incorporating 3 agents and 2 rounds of debate. The only distinction is that, to reduce result variance, we test on the complete test set of GSM8K, compared to their usage of 100 examples. For reference, we also report the results of self-consistency (Wang et al., 2022), which prompts models to generate multiple responses and performs majority voting to select the final answer.</p>
<p>Table 7 presents the results. The results indicate that both multi-agent debate and self-consistency achieve significant improvements over standard prompting. However, when comparing multi-agent debate to self-consistency, we observe that the performance of multi-agent is only slightly better than that of self-consistency with the same number of agents ( 3 responses, the baseline also compared in Du et al. (2023)). Furthermore, for self-consistency with an equivalent number of responses, multi-agent debate significantly underperforms simple self-consistency using majority voting.
In fact, rather than labeling the multi-agent debate as a form of "debate" or "critique", it is more appropriate to perceive it as a means to achieve "consistency" across multiple model generations. Fundamentally, its concept mirrors that of self-consistency; the distinction lies in the voting mechanism, whether voting is model-driven or purely based on counts. The observed improvement is evidently not attributed to "self-correction", but rather to "self-consistency". If we aim to argue that LLMs can self-correct reasoning through multi-agent debate, it is preferable to exclude the effects of selection among multiple generations.</p>
<h2>5 Prompt Design Issues in Self-Correction Evaluation</h2>
<p>In Section 3, we observe that although self-correction decreases reasoning performance with all types of feedback prompts we have evaluated, performance varies with different feedback prompts. In this section, we further emphasize the importance of proper prompt design in generating initial LLM responses to fairly measure the performance improvement achieved by self-correction. For example, if a task requires that the model response should meet criteria that can be easily specified</p>
<p>in the initial instruction (e.g., the output should contain certain words, the generated code should be efficient, the sentiment should be positive, etc.), instead of including such requirements only in the feedback prompt, an appropriate comparison would be to directly and explicitly incorporate these requirements into the prompt for generating initial responses. Otherwise, when the instruction for generating initial predictions is not informative enough, even if the performance improves, it is unclear whether the improvement merely comes from more detailed instructions in the feedback prompt or from the self-correction step itself.</p>
<p>To illustrate such prompt design issues in the self-correction evaluation of some prior work, we take the Constrained Generation task in Madaan et al. (2023) as an example, where the task requires models to generate coherent sentences using all 20-30 input concepts. The original prompt in Madaan et al. (2023) (Figure 7) does not clearly specify that the LLM needs to include all concepts in the prompt; thus, they show that self-correction improves task performance by asking the model to identify missing concepts and then guiding it to incorporate these concepts through feedback.</p>
<p>Based on this observation, we add the following instruction "Write a reasonable paragraph that includes <em>ALL</em> of the above concepts" to the prompt for initial response generation (refer to Figure 8 for the full prompt). Following Madaan et al. (2023), we use concept coverage as the metric. We reference their results and replicate their experiments using gpt-3.5-turbo-0613. Table 8 demonstrates that our new prompt, denoted as Standard Prompting (ours), significantly outperforms the results after self-correction of Madaan et al. (2023), and applying their self-correction prompt on top of model responses from our stronger version of the standard prompting again leads to a decrease in performance.</p>
<h1>6 CONCLUSION AND DISCUSSION</h1>
<p>Our work shows that current LLMs struggle to self-correct their reasoning without external feedback. This implies that expecting these models to inherently recognize and rectify their reasoning mistakes is overly optimistic so far. In light of these findings, it is imperative for the community to approach the concept of self-correction with a discerning perspective, acknowledging its potential and recognizing its boundaries. By doing so, we can better equip the self-correction technique to address the limitations of LLMs and develop the next generation of LLMs with enhanced capabilities. In the following, we provide insights into scenarios where self-correction shows the potential strengths and offer guidelines on the experimental design of future self-correction techniques to ensure a fair comparison.</p>
<p>Leveraging external feedback for correction. In this work, we demonstrate that current LLMs cannot improve their reasoning performance through intrinsic self-correction. Therefore, when valid external feedback is available, it is beneficial to leverage it properly to enhance model performance. For example, Chen et al. (2023b) show that LLMs can significantly improve their code generation performance through self-debugging by including code execution results in the feedback prompt to fix issues in the predicted code. In particular, when the problem description clearly specifies the intended code execution behavior, e.g., with unit tests, the code executor serves as the perfect verifier to judge the correctness of predicted programs, while the error messages also provide informative feedback that guides the LLMs to improve their responses. Gou et al. (2023) demonstrate that LLMs can more effectively verify and correct their responses when interacting with various external tools such as search engines and calculators. Cobbe et al. (2021); Lightman et al. (2023); Wang et al. (2023b) train a verifier or a critique model on a high-quality dataset to verify or refine LLM outputs, which can be used to provide feedback for correcting prediction errors. Besides automatically generated external feedback, we also often provide feedback ourselves when interacting with LLMs, guiding them to produce the content we desire. Designing techniques that enable LLMs to interact with the external environment and learn from different kinds of available feedback is a promising direction for future work.</p>
<p>Evaluating self-correction against baselines with comparable inference costs. By design, selfcorrection requires additional LLM calls, thereby increasing the costs for encoding and generating extra tokens. Section 4 demonstrates that the performance of asking the LLM to produce a final response based on multiple previous responses, such as with the multi-agent debate approach, is inferior to that of self-consistency (Wang et al., 2022) with the same number of responses. Regarding</p>
<p>this, we encourage future work proposing new self-correction methods to always include an in-depth inference cost analysis to substantiate claims of performance improvement. Moreover, strong baselines that leverage multiple model responses, like self-consistency, should be used for comparison. An implication for future work is to develop models with a higher probability of decoding the optimal solution in their answer distributions, possibly through some alignment techniques. This would enable the model to generate better responses without necessitating multiple generations.</p>
<p>Putting equal efforts into prompt design. As discussed in Section 5, to gain a better understanding of the improvements achieved by self-correction, it is important to include a complete task description in the prompt for generating initial responses, rather than leaving part of the task description for the feedback prompt. Broadly speaking, equal effort should be invested in designing the prompts for initial response generation and for self-correction; otherwise, the results could be misleading.</p>
<h1>7 LIMITATIONS AND BROADER IMPACT</h1>
<p>Although we have conducted a comprehensive evaluation spanning a variety of self-correction strategies, prompts, and benchmarks, our work focuses on evaluating reasoning of LLMs. Thus, it is plausible that there exist self-correction strategies that could enhance LLM performance in other domains. For example, prior works have demonstrated the successful usage of self-correction that aligns model responses with specific preferences, such as altering the style of responses or enhancing their safety (Bai et al., 2022; Ganguli et al., 2023; Madaan et al., 2023). A key distinction arises in the capability of LLMs to accurately assess their responses in relation to the given tasks. For example, LLMs can properly evaluate whether a response is inappropriate (Ganguli et al., 2023), but they may struggle to identify errors in their reasoning.</p>
<p>Furthermore, several prior works have already shown that LLM self-correction performance becomes significantly weaker without access to external feedback (Gou et al., 2023; Zhou et al., 2023a) and can be easily biased by misleading feedback (Wang et al., 2023a), which is consistent with our findings in this work. However, we still identified prevailing ambiguity in the wider community. Some existing literature may inadvertently contribute to this confusion, either by relegating crucial details about label usage to less prominent sections or by failing to clarify that their designed selfcorrection strategies actually incorporate external feedback. Regarding this, our paper serves as a call to action, urging researchers to approach this domain with a discerning and critical perspective. We also encourage future research to explore approaches that can genuinely enhance reasoning.</p>
<h2>REPRODUCIBILITY STATEMENT</h2>
<p>Our experiments utilize GPT-3.5 and GPT-4, which are accessible via the public API at https: //platform.openai.com/docs/models, as well as Llama-2, an open-source model. To facilitate reproducibility, we detail the specific kernels used, e.g., gpt-3.5-turbo-0613, or provide the access times for each experiment. We use prompts from previous works when possible. For our designed prompts, we include the exact prompts in Appendix A.</p>
<h2>ACKNOWLEDGEMENT</h2>
<p>We would like to thank Chen Liang, William Cohen, Uri Alon, and other colleagues at Google DeepMind for valuable discussion and feedback.</p>
<h2>REFERENCES</h2>
<p>Hussam Alkaissi and Samy I McFarlane. Artificial hallucinations in chatgpt: implications in scientific writing. Cureus, 15(2), 2023.</p>
<p>Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023.</p>
<p>Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022.</p>
<p>Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, et al. A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. In Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 675-718, 2023.</p>
<p>Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom B Brown, Dawn Song, Ulfar Erlingsson, et al. Extracting training data from large language models. In USENIX Security Symposium, volume 6, 2021.</p>
<p>Justin Chih-Yao Chen, Swarnadeep Saha, and Mohit Bansal. Reconcile: Round-table conference improves reasoning via consensus among diverse llms. arXiv preprint arXiv:2309.13007, 2023a.</p>
<p>Xinyun Chen, Maxwell Lin, Nathanael Schrli, and Denny Zhou. Teaching large language models to self-debug. arXiv preprint arXiv:2304.05128, 2023b.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1-113, 2023. URL http: //jmlr.org/papers/v24/22-1144.html.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.</p>
<p>Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, and Igor Mordatch. Improving factuality and reasoning in language models through multiagent debate. arXiv preprint arXiv:2305.14325, 2023.</p>
<p>Deep Ganguli, Amanda Askell, Nicholas Schiefer, Thomas Liao, Kamil Lukoit, Anna Chen, Anna Goldie, Azalia Mirhoseini, Catherine Olsson, Danny Hernandez, et al. The capacity for moral self-correction in large language models. arXiv preprint arXiv:2302.07459, 2023.</p>
<p>Luyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony Chen, Arun Tejasvi Chaganty, Yicheng Fan, Vincent Zhao, Ni Lao, Hongrae Lee, Da-Cheng Juan, et al. Rarr: Researching and revising what language models say, using language models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 16477-16508, 2023.</p>
<p>Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, and Weizhu Chen. Critic: Large language models can self-correct with tool-interactive critiquing. arXiv preprint arXiv:2305.11738, 2023.</p>
<p>Jie Huang and Kevin Chen-Chuan Chang. Towards reasoning in large language models: A survey. In Findings of the Association for Computational Linguistics: ACL 2023. Association for Computational Linguistics, 2023.</p>
<p>Jie Huang, Hanyin Shao, and Kevin Chen-Chuan Chang. Are large pre-trained language models leaking your personal information? In Findings of the Association for Computational Linguistics: EMNLP 2022, pp. 2038-2047, Abu Dhabi, United Arab Emirates, 2022. Association for Computational Linguistics.</p>
<p>Geunwoo Kim, Pierre Baldi, and Stephen McAleer. Language models can solve computer tasks. Advances in Neural Information Processing Systems, 2023.</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, $35: 22199-22213,2022$.</p>
<p>Haoran Li, Dadi Guo, Wei Fan, Mingshi Xu, Jie Huang, Fanpu Meng, and Yangqiu Song. Multistep jailbreaking privacy attacks on chatgpt. In Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 4138-4153, 2023.</p>
<p>Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Zhaopeng Tu, and Shuming Shi. Encouraging divergent thinking in large language models through multiagent debate. arXiv preprint arXiv:2305.19118, 2023.</p>
<p>Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let's verify step by step. arXiv preprint arXiv:2305.20050, 2023.</p>
<p>Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. Advances in Neural Information Processing Systems, 2023.</p>
<p>Theo X Olausson, Jeevana Priya Inala, Chenglong Wang, Jianfeng Gao, and Armando SolarLezama. Demystifying gpt self-repair for code generation. arXiv preprint arXiv:2306.09896, 2023.</p>
<p>OpenAI. Gpt-4 technical report, 2023.
Liangming Pan, Michael Saxon, Wenda Xu, Deepak Nathani, Xinyi Wang, and William Yang Wang. Automatically correcting large language models: Surveying the landscape of diverse selfcorrection strategies. arXiv preprint arXiv:2308.03188, 2023.</p>
<p>Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges, Antoine Bosselut, Robert West, and Boi Faltings. Refiner: Reasoning feedback on intermediate representations. arXiv preprint arXiv:2304.01904, 2023.</p>
<p>Hanyin Shao, Jie Huang, Shen Zheng, and Kevin Chen-Chuan Chang. Quantifying association capabilities of large language models and its implications on privacy leakage. arXiv preprint arXiv:2305.12707, 2023.</p>
<p>Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H Chi, Nathanael Schrli, and Denny Zhou. Large language models can be easily distracted by irrelevant context. In International Conference on Machine Learning, pp. 31210-31227. PMLR, 2023.</p>
<p>Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 2023.</p>
<p>Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: A question answering challenge targeting commonsense knowledge. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4149-4158, 2019.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.</p>
<p>Boshi Wang, Xiang Yue, and Huan Sun. Can chatgpt defend its belief in truth? evaluating llm reasoning via debate. In Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 11865-11881, 2023a.</p>
<p>Tianlu Wang, Ping Yu, Xiaoqing Ellen Tan, Sean OBrien, Ramakanth Pasunuru, Jane Dwivedi-Yu, Olga Golovneva, Luke Zettlemoyer, Maryam Fazel-Zarandi, and Asli Celikyilmaz. Shepherd: A critic for language model generation. arXiv preprint arXiv:2308.04592, 2023b.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, 2022.</p>
<p>Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. Jailbroken: How does llm safety training fail? arXiv preprint arXiv:2307.02483, 2023.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824-24837, 2022.</p>
<p>Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi, and Yejin Choi. Generating sequences by learning to self-correct. In The Eleventh International Conference on Learning Representations, 2023.</p>
<p>Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 2018.</p>
<p>Shen Zheng, Jie Huang, and Kevin Chen-Chuan Chang. Why does chatgpt fall short in providing truthful answers? ArXiv preprint, abs/2304.10513, 2023.</p>
<p>Aojun Zhou, Ke Wang, Zimu Lu, Weikang Shi, Sichun Luo, Zipeng Qin, Shaoqing Lu, Anya Jia, Linqi Song, Mingjie Zhan, et al. Solving challenging math word problems using gpt-4 code interpreter with code-based self-verification. arXiv preprint arXiv:2308.07921, 2023a.</p>
<p>Denny Zhou, Nathanael Schrli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc V Le, et al. Least-to-most prompting enables complex reasoning in large language models. In The Eleventh International Conference on Learning Representations, 2023b.</p>
<p>Andy Zou, Zifan Wang, J Zico Kolter, and Matt Fredrikson. Universal and transferable adversarial attacks on aligned language models. arXiv preprint arXiv:2307.15043, 2023.</p>
<h1>A Prompts and Example Outputs</h1>
<p>Can you solve the following math problem? Christina is planning a birthday party and needs . 75 gift bags per invited guest, because $1 / 4$ of attendees don't show up. She invited 16 friends. Gift bags are $\$ 2$ each. How much will she spend? Explain your reasoning. Your final answer should be a single numerical number, in the form \boxed{answer}, at the end of your response.</p>
<p>If Christina invited 16 friends, then $1 / 4$ of them won't show up, which means there will be $16 * 1 / 4=4$ friends who won't attend the party. To determine how many gift bags to buy, we need to subtract the number of friends who won't show up from the total number of invited guests. The number of invited guests is $16-4=12$. Since Christina needs 0.75 gift bags per invited guest, we can multiply this quantity by the number of invited guests to find the total number of gift bags needed, which is $0.75 * 12=9$ gift bags. If each gift bag is $\$ 2$, then Christina will spend a total of $2 * 9=\$&lt;&lt;2 * 9=18&gt;&gt;18$ on gift bags. Therefore, Christina will spend $\$ 18$ on gift bags. Answer: \boxed{18}. (Incorrect)</p>
<h2>Review your previous answer and find problems with your answer.</h2>
<p>Upon reviewing my previous answer, I recognize a mistake. I incorrectly stated that Christina needs 0.75 gift bags per invited guest. The given information states that $1 / 4$ of the attendees won't show up, so the correct calculation should take this into account. To calculate the number of gift bags needed, we will multiply the number of invited guests by the ratio of attendees who will actually show up (75\%). The number of invited guests is 16 , and if $1 / 4$ of them won't show up, then the ratio of attendees who will show up is $1-1 / 4=3 / 4$. Multiplying the number of invited guests by the ratio of attendees who will show up gives us: 16 * $(3 / 4)=12$ gift bags needed. Since each gift bag costs $\$ 2$, the total amount Christina will spend on gift bags is: $12 * \$ 2=\$&lt;&lt;12 * 2=24&gt;&gt;24$.</p>
<p>Based on the problems you found, improve your answer. Please reiterate your answer, with your final answer a single numerical number, in the form \boxed{answer}.</p>
<p>Upon reviewing my previous answer, I recognize a mistake. I incorrectly stated that Christina needs 0.75 gift bags per invited guest. The given information states that $1 / 4$ of the attendees won't show up, so the correct calculation should take this into account. To calculate the number of gift bags needed, we will multiply the number of invited guests by the ratio of attendees who will actually show up (75\%). The number of invited guests is 16 , and if $1 / 4$ of them won't show up, then the ratio of attendees who will show up is $1-1 / 4=3 / 4$. Multiplying the number of invited guests by the ratio of attendees who will show up gives us: 16 * $(3 / 4)=12$ gift bags needed. Since each gift bag costs $\$ 2$, the total amount Christina will spend on gift bags is: $12 * \$ 2=\$&lt;&lt;12 * 2=24&gt;&gt;24$. Therefore, Christina will spend $\$ 24$ on gift bags. Answer: \boxed{24}. (Correct)</p>
<p>Figure 3: Example on GSM8K where self-correction changes an incorrect answer to a correct one. Text in bold denotes user prompts.</p>
<p>Can you solve the following math problem? Terry eats 2 yogurts a day. They are currently on sale at 4 yogurts for $\$ 5.00$. How much does he spend on yogurt over 30 days? Explain your reasoning. Your final answer should be a single numerical number, in the form \boxed{answer}, at the end of your response.</p>
<p>First, we need to calculate how many sets of 4 yogurts Terry buys in 30 days. Since Terry eats 2 yogurts a day, he will need $2 / 4=0.5$ sets of 4 yogurts per day. Next, we multiply the number of sets by the price of each set to calculate how much Terry spends per day. Each set of 4 yogurts costs $\$ 5.00$, so Terry spends $0.5 * \$ 5.00=\$&lt;&lt;0.5 * 5=2.50&gt;&gt;2.50$ on yogurt per day. Finally, we multiply the amount Terry spends per day by the number of days to find the total amount he spends on yogurt over 30 days. Terry spends $\$ 2.50$ per day for 30 days, so he spends $\$ 2.50 * 30=$ $\$&lt;&lt;2.5 * 30=75.00&gt;&gt;75.00$ on yogurt over 30 days. So, Terry spends $\$ \backslash$ boxed ${75.00} \$$ on yogurt over 30 days. (Correct)</p>
<h1>Review your previous answer and find problems with your answer.</h1>
<p>Upon reviewing my previous answer, I realized that I made a mistake in calculating Terry's spending on yogurt over 30 days. I incorrectly stated that Terry spends $\$ 2.50$ per day for 30 days, resulting in a total of $\$ 75.00$. However, since Terry eats 2 yogurts a day and the yogurts are sold in sets of 4 for $\$ 5.00$, Terry would actually need to buy 1 set of 4 yogurts to last for 2 days. Therefore, his spending on yogurt per set would be $\$ 5.00$ for 2 days, or $\$ 5.00 / 2=\$ 2.50$ per day. To calculate the total amount spent on yogurt over 30 days, we would multiply the amount spent per day ( $\$ 2.50$ ) by the number of sets needed in 30 days (which is 30 divided by 2 days per set). Therefore, the correct calculation would be $\$ 2.50 *(30 / 2)=\$ 2.50 * 15=\$ 37.50$. Apologies for the mistake. The correct answer is Terry spends $\$ \backslash$ boxed ${37.50} \$$ on yogurt over 30 days.</p>
<p>Based on the problems you found, improve your answer. Please reiterate your answer, with your final answer a single numerical number, in the form $\backslash$ boxed{answer}.</p>
<p>Apologies for the error in my previous response. Upon reviewing the problem, here is the correct answer: Since Terry eats 2 yogurts a day and the yogurts are sold in sets of 4 for $\$ 5.00$, Terry needs to buy 1 set of 4 yogurts to last for 2 days. Therefore, Terry's spending on yogurt per set is $\$ 5.00$ for 2 days, or $\$ 5.00 / 2=\$ 2.50$ per day. To calculate the total amount spent on yogurt over 30 days, we multiply the amount spent per day ( $\$ 2.50$ ) by the number of sets needed in 30 days. In this case, Terry needs $30 / 2=15$ sets of yogurts over 30 days. Multiplying $\$ 2.50$ by 15 , we find that Terry spends a total of $\$ 2.50 * 15=\$ 37.50$ on yogurt over 30 days. Therefore, the final answer is Terry spends $\$ \backslash$ boxed ${37.50} \$$ on yogurt over 30 days. (Incorrect)</p>
<p>Figure 4: Example on GSM8K where self-correction changes a correct answer to an incorrect one.</p>
<p>Can you solve the following math problem? Toulouse has twice as many sheep as Charleston. Charleston has 4 times as many sheep as Seattle. How many sheep do Toulouse, Charleston, and Seattle have together if Seattle has 20 sheep? Explain your reasoning. Your final answer should be a single numerical number, in the form \boxed{answer}, at the end of your response.</p>
<p>Let's start by figuring out how many sheep Charleston has. We are given that Charleston has 4 times as many sheep as Seattle, so we can calculate this by multiplying Seattle's number of sheep by 4: Charleston $=4 * 20=$ 80 sheep. Next, we are told that Toulouse has twice as many sheep as Charleston. Therefore, we can find Toulouse's number of sheep by multiplying Charleston's number of sheep by 2: Toulouse $=2 * 80=160$ sheep. Finally, to find the total number of sheep, we add up the number of sheep for each city: Total = Seattle + Charleston + Toulouse $=20+80$ $+160=260$ sheep. Therefore, Toulouse, Charleston, and Seattle have a total of 260 sheep. Answer: \boxed{260}. (Correct)</p>
<p>Review your previous answer and find problems with your answer.
I apologize for the mistake in my previous response. Let's go through the problem again and find the correct answer. We are given that Charleston has 4 times as many sheep as Seattle. If Seattle has 20 sheep, then Charleston would have: Charleston $=4 * 20=80$ sheep. Next, we are told that Toulouse has twice as many sheep as Charleston. Therefore, Toulouse would have: Toulouse $=2 * 80=160$ sheep. To find the total number of sheep, we add up the number of sheep for each city: Total = Seattle + Charleston + Toulouse $=20+80+160=260$ sheep. After reviewing my previous answer, I have determined that there are no problems with my updated response. The correct answer is 260 sheep.</p>
<p>Based on the problems you found, improve your answer. Please reiterate your answer, with your final answer a single numerical number, in the form \boxed{answer}.</p>
<p>Apologies for the confusion. Let me provide an improved answer. We are given that Charleston has 4 times as many sheep as Seattle, and Seattle has 20 sheep. So, Charleston would have $4 * 20=80$ sheep. According to the information, Toulouse has twice as many sheep as Charleston. Therefore, Toulouse would have $2 * 80=160$ sheep. To determine the total number of sheep, we add up the number of sheep for each city: Total = Seattle + Charleston + Toulouse $=20+80+160=260$ sheep. After reviewing the previous answer, the correct final answer is \boxed{260}. (Correct)</p>
<p>Figure 5: Example on GSM8K where self-correction does not change the answer.</p>
<p>Q: A fencing thrust with a sharp sword towards a person would result in what?
Answer Choices: (A) injury (B) small cuts (C) fever (D) competition (E) puncture wound.
Explain your reasoning. You must choose only one option from A to E. Your final answer should be a single letter from A to E, in the form (answer), at the end of your response.</p>
<p>A: A fencing thrust with a sharp sword towards a person would result in a puncture wound. This is because a fencing thrust involves forcefully stabbing with a sword, which can penetrate the skin and create a deep, narrow wound. Puncture wounds can be very dangerous as they can damage internal organs and cause bleeding. Therefore, the most appropriate answer is (E) puncture wound. (Correct)</p>
<h1>Review your previous answer and find problems with your answer.</h1>
<p>Upon reviewing my previous answer, I realize that there are some problems with it. While a fencing thrust with a sharp sword can potentially result in a puncture wound, it is important to note that in a controlled fencing match, the swords used are typically blunted or have safety tips, designed to minimize the risk of injury. Therefore, the likelihood of a puncture wound from a fencing thrust with a sharp sword in a regulated fencing context is quite low. A more appropriate and accurate answer would be (D) competition, as a fencing thrust with a sharp sword is typically performed in the context of a fencing match or competition, rather than with the intention to cause injury.</p>
<p>Based on the problems you found, improve your answer. You must choose only one option from A to E. Please reiterate your answer, with your final answer a single letter from A to E, in the form (answer).</p>
<p>Based on the problems with my previous answer, I apologize for the incorrect response. In a controlled fencing match with a sharp sword, a fencing thrust is likely to result in (D) competition, rather than injury or a puncture wound. Therefore, the correct final answer is (D) competition. (Incorrect)</p>
<p>Figure 6: Example on CommonSenseQA where self-correction changes a correct answer to an incorrect one.</p>
<div class="codehilite"><pre><span></span><code>Concepts: [&#39;footage&#39;, &#39;motion&#39;, &#39;ruin&#39;, &#39;tilt&#39;, &#39;window&#39;]
Sentence: time lapse footage with tilt up motion of the sun streaking
through window of ruin
###
Concepts: [&#39;cause&#39;, &#39;hate&#39;, &#39;hut&#39;, &#39;local&#39;, &#39;love&#39;]
</code></pre></div>

<p>Sentence: new beach huts on the island have caused some controversy some
locals love them others hate them
[8 more in-context examples]
###</p>
<div class="codehilite"><pre><span></span><code><span class="n">Concepts</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s1">&#39;vest&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;snow&#39;</span><span class="p">,</span><span class="s1">&#39;remove&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;silhouette&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;dunk&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;step&#39;</span><span class="p">,</span>
<span class="s1">&#39;item&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;toddler&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;plaster&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;wall&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;piece&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;tool&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;circle&#39;</span><span class="p">,</span>
<span class="s1">&#39;pull&#39;</span><span class="p">,</span><span class="s1">&#39;wrap&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;headphone&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;deck&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;suit&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;press&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;mother&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;pit&#39;</span><span class="p">,</span>
<span class="s1">&#39;cupcake&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;priest&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;rink&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;counter&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;rest&#39;</span><span class="p">,</span><span class="s1">&#39;piano&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;wine&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;serve&#39;</span><span class="p">,</span>
<span class="s1">&#39;winter&#39;</span><span class="p">]</span>
</code></pre></div>

<p>Sentence: A toddler wearing a vest and snow boots steps onto the icy deck, removes a circle-shaped piece of plaster from the wall, and pulls out a tool from their winter suit to wrap their head around a headphone.</p>
<p>Figure 7: Example output for Constrained Generation using the prompt from Madaan et al. (2023). The constraint that the output needs to contain all the concepts is not explicitly mentioned in the prompt for initial response generation. However, in the feedback \&amp; refine prompts, the constraint is implied by "what concepts from the concept list are missing from the sentence?" (refer to Figures 25 and 26 of Madaan et al. (2023) for the feedback \&amp; refine prompts).</p>
<div class="codehilite"><pre><span></span><code><span class="n">Concepts</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s1">&#39;vest&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;snow&#39;</span><span class="p">,</span><span class="s1">&#39;remove&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;silhouette&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;dunk&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;step&#39;</span><span class="p">,</span>
<span class="s1">&#39;item&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;toddler&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;plaster&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;wall&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;piece&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;tool&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;circle&#39;</span><span class="p">,</span>
<span class="s1">&#39;pull&#39;</span><span class="p">,</span><span class="s1">&#39;wrap&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;headphone&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;deck&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;suit&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;press&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;mother&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;pit&#39;</span><span class="p">,</span>
<span class="s1">&#39;cupcake&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;priest&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;rink&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;counter&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;rest&#39;</span><span class="p">,</span><span class="s1">&#39;piano&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;wine&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;serve&#39;</span><span class="p">,</span>
<span class="s1">&#39;winter&#39;</span><span class="p">]</span>
</code></pre></div>

<p>Write a reasonable paragraph that includes <em>ALL</em> of the above concepts.</p>
<p>In the winter wonderland, a mother dressed in a warm vest watched her toddler play in the snow. As the child attempted to remove the snow from the ground, their silhouette created an enchanting image against the white backdrop. Nearby, a group of friends gathered around a basketball hoop, taking turns to dunk the ball with skillful steps. One of them, holding an item resembling a tool, used it to plaster a piece of the wall that had been damaged. Meanwhile, on the other side of the park, a circle of children pulled their hats tightly to wrap around their heads, protecting their ears from the cold wind as they listened to music through their colorful headphones. On the deck of a nearby house, a man in a sleek suit pressed a button, causing a hidden mechanism to reveal a grand piano. As the music filled the air, the host served glasses of wine to the guests, creating a warm and inviting atmosphere in the winter evening. In the distance, a priest skated gracefully in an ice rink, his movements mirroring the smoothness of his words during a sermon. At the counter of a bakery, a customer ordered a cupcake and watched as the baker skillfully decorated it with a winter-themed design. Not far away, a group of friends gathered around a pit, roasting marshmallows and enjoying the warmth of the fire. The scene painted a picture of joy, friendship, and the beauty of winter.</p>
<p>Figure 8: Example output for Constrained Generation using a prompt casually written by the authors.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ We omit the analysis on HotpotQA because the sample size used in the source paper is quite small, which may not produce meaningful statistics.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
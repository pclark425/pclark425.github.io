<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4608 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4608</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4608</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-101.html">extraction-schema-101</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <p><strong>Paper ID:</strong> paper-28c7e583d90ccfc5c3078dfc1d6b80a9ad90248d</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/28c7e583d90ccfc5c3078dfc1d6b80a9ad90248d" target="_blank">Quantifying Memorization Across Neural Language Models</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> On the whole, this work finds that memorization in LMs is more prevalent than previously believed and will likely get worse as models continues to scale, at least without active mitigations.</p>
                <p><strong>Paper Abstract:</strong> Large language models (LMs) have been shown to memorize parts of their training data, and when prompted appropriately, they will emit the memorized training data verbatim. This is undesirable because memorization violates privacy (exposing user data), degrades utility (repeated easy-to-memorize text is often low quality), and hurts fairness (some texts are memorized over others). We describe three log-linear relationships that quantify the degree to which LMs emit memorized training data. Memorization significantly grows as we increase (1) the capacity of a model, (2) the number of times an example has been duplicated, and (3) the number of tokens of context used to prompt the model. Surprisingly, we find the situation becomes more complicated when generalizing these results across model families. On the whole, we find that memorization in LMs is more prevalent than previously believed and will likely get worse as models continues to scale, at least without active mitigations.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4608.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4608.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Extractability (Def. 3.1)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>k-token Context Extractability (Definition 3.1)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Operational definition of memorization: a suffix s is 'extractable' from model f with k tokens of context if there exists a k-token prefix p (where [p||s] appears in f's training data) such that f produces s when prompted with p using greedy decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>k-token Context Extractability</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Given a sequence x = [p || s] from the training data split into prefix p of length k and suffix s, prompt the model with p and perform greedy decoding; record a success if the model's generated continuation equals s exactly. Applied across many held-out training examples (sampled subsets) and averaged to estimate the fraction of the dataset that is extractable under a given context length k.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Exact-match continuation (binary per example); aggregated fraction of sequences extractable (probability). Varies by context length k, duplication count, and model size. Uses greedy-decoding output equality to ground-truth suffix as primary criterion.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Natural language processing / language modeling (memorization detection)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>N/A — method for detecting memorized continuations (data extraction)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Used throughout paper to quantify memorization; showed strong log-linear trends with model size, duplication, and context (e.g., 6B GPT-J extractable fraction ~33% at 50-token context, rising to ~65% at 450 tokens in biased sample).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated: exact-match string comparison between generated continuation and ground-truth suffix (greedy decoding). Some qualitative human inspection used for example analysis, but primary metric is automated.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Compared against baselines (GPT-2 family not trained on The Pile), alternative sampling (uniform vs duplicate-normalized), replication across model families (T5, OPT, deduplicated models), and alternate decoding strategies (beam search). Statistical fits (R^2) and significance testing used to validate trends.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Requires access to the model's original training set and exact-text alignment; focused on greedy decoding which is not overall-most-likely-sequence; computationally expensive to apply exhaustively; biased by sampling strategy (uniform vs duplicate-normalized); extractability depends strongly on prompt/context length (discoverability).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>The authors applied the method to The Pile (825 GB) and subsets sampled from it; also applied to C4 and OPT datasets for replication.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Quantifying Memorization Across Neural Language Models', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4608.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4608.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Exact-match Fraction</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fraction of Sequences Exactly Reproduced (Exact-match extractability rate)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Metric reporting the fraction (probability) of evaluated prompts whose model-generated continuation exactly matches the ground-truth 50-token suffix from the training data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Exact-match Extractability Rate</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute for a sampled set of training sequences the binary indicator of whether the model's generation equals the ground-truth suffix; average over examples (optionally across lengths and duplication buckets) to obtain an overall extractability probability.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Binary exact-match per example; aggregated averages stratified by model size, duplication count, prompt/context length, and sequence length.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP / language-model memorization</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>N/A (metric for memorization)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Used to produce central quantitative claims: e.g., larger GPT-Neo models produce higher exact-match rates (biased sample showing near-log-linear growth with model size, R^2 = 99.8%); for duplication buckets, extractability rises roughly log-linearly with repetition count; beam search increased this metric by ~<2 percentage points on average (max 5.6%).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated (string equality checks).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Cross-checked with alternate sampling strategies (uniform vs duplicate-normalized), decoding strategies (beam search), and dataset-wide containment checks (see alternate extractability).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Strict exact-match is a lower bound on memorization (misses cases where model outputs a different memorized suffix present elsewhere in the dataset); expensive to scale; sensitive to tokenization and minor formatting differences.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Quantifying Memorization Across Neural Language Models', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4608.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4608.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dataset-wide Match (alternate)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dataset-wide Containment Extractability (alternate, loose lower bound)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Relaxed definition: a generated continuation is counted as memorized if the concatenation [p||f(p)] appears anywhere in the training dataset (not necessarily the ground-truth continuation for that prompt).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Dataset-wide Containment Extractability</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>For each prompt p, generate continuation f(p); check whether the full generated sequence [p||f(p)] exists anywhere in the training corpus (requires large-scale substring search). Counts more instances of memorization because it includes outputs that match other training examples with the same prefix.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Containment-in-training-dataset boolean per example; aggregated percentages compared to ground-truth exact-match metric.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP / data-extraction auditing</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>N/A (alternative memorization metric)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Yields substantially higher measured memorization; example: at 100 repetitions, 32.6% of outputs are contained somewhere in the dataset but only 15.8% match the ground-truth continuation. Maximum difference between this approach and exact-match was reported as 28.4% (at 2,200 repetitions).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated but computationally expensive (requires searching an 800GB corpus for many generated strings).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Compared directly to ground-truth exact-match metric on same evaluation sets; demonstrated consistent upward bias relative to exact-match.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Very high compute and I/O cost to check containment at scale; less reproducible without access to full training corpus; more permissive and thus less conservative as a privacy risk lower bound.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Quantifying Memorization Across Neural Language Models', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4608.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4608.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Duplicate-normalized Sampling</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sampling Normalized by Sequence Length and Duplication Count</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Biased sampling procedure that constructs evaluation subsets balanced across sequence lengths and duplication-frequency buckets to study worst-case memorization behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Duplicate- and Length-normalized Sampling</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>For lengths l in {50,100,...,500} and duplication-count buckets defined as powers-of-two ranges (2^{n/4} to 2^{(n+1)/4}), sample 1,000 sequences per (length, duplication) bucket until unavailable. Aggregated to produce evaluation sets that over-represent highly duplicated and long-context examples to measure memorization as a function of those properties.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Controlled representation across sequence-length bins and duplication-frequency bins to enable stratified analysis of extractability vs duplication and context length.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP / dataset auditing</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>N/A (sampling framework)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Used as primary evaluation set for key figures (Figure 1). Showed strong log-linear trends; emphasized worst-case memorization on duplicated long sequences. Contrasted with uniform random sampling which yields much lower absolute extractability rates.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated sampling procedure, built around suffix-array based duplicate enumeration.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Compared results obtained with this biased sample to those from uniformly random samples (100k sequences) to confirm qualitative trends (scale and context effects persisted), and used suffix-array methods to ensure correctness of duplication counts.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Biased toward worst-case scenarios so absolute rates are not representative of entire dataset; constructing duplicate-normalized sample requires heavy preprocessing (suffix array) and access to full training corpus.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Quantifying Memorization Across Neural Language Models', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4608.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4608.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Uniform Random Sampling</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Uniform Random Sampling of Training Sequences</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline evaluation sampling each training example with equal probability to estimate overall memorization prevalence across the dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Uniform Random Subset Evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Randomly sample N sequences uniformly from the training dataset (e.g., 50k or 100k) and apply the extractability test (prompt with prefix of length l-50, test exact-match for next 50 tokens) to estimate global memorization probabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Aggregate exact-match extractability probability across uniformly-sampled examples; useful for estimating absolute fraction of training data memorized.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP / memorization auditing</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>N/A (sampling design)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Produced qualitatively similar trends (larger models and longer context increase extractability) but lower absolute extractability percentages than duplicate-normalized sampling; used to estimate a lower bound (e.g., at least ~1% of The Pile extractable by 6B GPT-J model vs GPT-2 XL).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Compared against duplicate-normalized sampling results; used to confirm robustness of scaling trends.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Because duplicates are rare, uniform sampling under-represents the high-duplication tail where most leakage occurs—thus produces loose lower bounds on worst-case memorization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Quantifying Memorization Across Neural Language Models', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4608.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4608.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Decoding-strategy Comparison</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Comparative Evaluation across Decoding Strategies (greedy, beam search, random sampling)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical study of how decoding choice affects measured extractability, showing beam search slightly increases measured memorization versus greedy decoding, while sampling increases novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Decoding Strategy Sensitivity Analysis</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Run the same extractability prompts under different decoding algorithms (greedy argmax at each step, beam search with b beams, and stochastic sampling) and compare exact-match extractability rates and qualitative novelty metrics (e.g., n-gram novelty in prior work).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Extractability (exact-match) under each decoding method; percent-change between methods; qualitative analysis of novelty for sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP / generation evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>N/A (methodology for robustness of extraction)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Beam search with 100 beams increased extractable memorization marginally (under 2 percentage points on average, max 5.6%), and beam and greedy produced identical outputs ~45% of the time. Random sampling was not used for main experiments because it increases novelty and reduces discoverability of memorization.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated comparisons of generation outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Direct empirical comparison across decoding runs on same prompts; reported averages and maxima.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Different decoding strategies optimize different objectives (likelihood vs diversity), so measured memorization depends on the decoding method; choice should match attack or auditing scenario.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Quantifying Memorization Across Neural Language Models', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4608.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4608.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Log-linear Scaling Fit</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Log-linear Relationship Fitting (memorization vs scale/duplication/context)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Statistical characterization: memorization probability grows approximately log-linearly with model size, duplication count, and context length; quality of fit measured by R^2.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Log-linear Trend Fitting (with R^2)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Fit a log-linear model (e.g., linear fit on log-scaled independent variable) to empirical extractability rates versus variables like model parameter count, duplication count, or context tokens; report goodness-of-fit (R^2) to quantify how well a log-linear law explains observed scaling.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Goodness-of-fit statistics (R^2), slope coefficients (e.g., ten-fold increase in model size corresponds to +19 percentage points in extractability in one reported fit), and visualization of trend lines.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP / empirical scaling laws</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>N/A (statistical characterization)</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Reported near-perfect log-linear fit for model size vs memorization in the GPT-Neo family with R^2 = 99.8%; similar log-linear trends observed for duplication count and context length in many settings, though variance exists across datasets and objectives (e.g., T5 masked LM showed more variance for duplication).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated statistical fitting and reporting.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Replicated fits across multiple model families (GPT-Neo, T5, OPT, deduplicated models) and sampling strategies to check generality; reported where fit breaks down (e.g., T5 duplication irregularities).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Log-linear is empirical and may not generalize across all families or dataset preprocessing choices; dataset idiosyncrasies can produce deviations from monotonic trends (e.g., T5 buckets with whitespace-heavy duplicates).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Quantifying Memorization Across Neural Language Models', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4608.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e4608.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Differential Privacy (DP)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Differential Privacy (as a memorization definition/mitigation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Formal privacy framework that demands model outputs be (probabilistically) insensitive to removal of any single training example; cited as a formal but practically expensive way to limit memorization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Calibrating noise to sensitivity in private data analysis</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Differential Privacy (DP) Guarantees</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>DP evaluates memorization by bounding how much influence any single training example can have on model outputs (epsilon-delta guarantees); DP training algorithms add calibrated noise during training to get provable bounds on leakage.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Privacy budget (epsilon, delta), empirical trade-offs in utility vs privacy, and theoretical worst-case guarantees about influence of single examples.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Machine learning privacy (applied to NLP)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Formal privacy constraint rather than a theory-evaluation metric</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Paper notes DP is a leading general memorization definition but is ineffective for highly duplicated data and is expensive (computationally and utility-wise) in practice; references Abadi et al. (2016) and later DP work.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Formal/mathematical definition (not an empirical metric); empirical DP implementations evaluated via standard training and test metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>DP provides formal proofs; practical validation involves training DP variants and measuring utility degradation and empirical leakage (other works).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Doesn't prevent memorization of highly duplicated data well; DP training often incurs expensive computation, slower convergence, and utility loss.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Quantifying Memorization Across Neural Language Models', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4608.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e4608.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Exposure</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Exposure metric (Carlini et al., 2019)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior metric (exposure) quantifying memorization by measuring how many guesses an adversary needs to identify a secret training canary, used to bound memorization of carefully crafted examples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The secret sharer: Evaluating and testing unintended memorization in neural networks.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Exposure</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute log-rank or surprisal-based measures for specific target sequences (canaries) to estimate how exposed a sequence is in the model's distribution; requires many generations per sequence and is best-suited for crafted examples rather than arbitrary training text.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Exposure score (higher means easier to recover); rank of true secret among candidate completions; requires large sampling to estimate distribution tails.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP privacy / memorization auditing</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Metric for memorization vulnerability</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Paper contrasts exposure with their extractability metric: exposure requires thousands of generations per sequence and is suited to canaries, whereas their extractability test is more actionable for prompting with true training prefixes.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated, sampling-based metric.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Validated in prior work via recovery attacks on synthetic canaries and measurement of rank statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Expensive to compute across many sequences; tailored to crafted canaries, not general-purpose training-set coverage measurement.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Quantifying Memorization Across Neural Language Models', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4608.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e4608.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>k-eidetic memorization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>k-eidetic Memorization (prior definition)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior definition for unprompted memorization that identifies sequences the model will produce without prompting; referenced as useful for unprompted memorization but less suited for tight bounds via prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Extracting training data from large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>k-eidetic Memorization</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Definition from prior work that flags sequences that a model can output unprompted (or with minimal prompt), often measured by searching model generations for exact occurrences; useful to assess unprompted copying behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Presence of sequence in unprompted model outputs; frequency across random generations; exposure-like statistics may be used.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP memorization</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Definition/metric for unprompted memorization</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Authors note k-eidetic memorization is useful for unprompted memorization work but less useful when one intends to prompt with training-data prefixes to bound memorization.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated generation-and-detection.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Prior empirical studies (Carlini et al., 2020) used generation sampling and matching to training set.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Requires many model generations; provides weaker/looser bounds for prompted extractability experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Quantifying Memorization Across Neural Language Models', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4608.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e4608.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Counterfactual memorization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Counterfactual Memorization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A notion from prior literature that measures how much influence individual training examples have on outputs under hypothetical dataset removals, used to study memorization's dependence on training data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Counterfactual memorization in neural language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Counterfactual Memorization</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Approaches that conceptually (or via influence estimation) assess whether removing specific training items would change model outputs, often requiring training or approximating many counterfactual models to estimate influence.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Change-in-output probability or generation behavior under removal of examples; influence scores per training example.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP / interpretability and privacy</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Influence-based memorization metric</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Paper mentions counterfactual memorization as an alternative but notes it can require training hundreds or thousands of models, which is impractical for large LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Primarily automated but computationally intensive (requires retraining or influence estimation).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Prior work validated via retraining experiments or approximations of influence; authors cite this to justify their chosen extractability test.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Impractical at scale due to training cost; not suited for large-scale language models without approximation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Quantifying Memorization Across Neural Language Models', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4608.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e4608.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Suffix Array Enumeration</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Suffix Array Construction for Duplicate Enumeration</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A scalable engineering method to enumerate all substrings of a large corpus and count their duplication frequencies, enabling the construction of duplicate-normalized evaluation samples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Suffix-Array-based Duplicate Enumeration</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Build a suffix array over the entire training corpus to allow efficient linear-time enumeration of all k-token sequences repeated between N and M times; sample uniformly from such buckets to create balanced evaluation sets by duplication count.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Correctness of duplicate counts, computational scalability to hundreds of GBs, ability to retrieve N-to-M repetition buckets efficiently.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Data engineering for NLP evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Tooling for sampling and evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>human_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Enabled the authors to construct 1,000-sequence buckets spanning from ~6-8 repeats to ~724-861 repeats and to assemble the duplicate-normalized evaluation dataset (≈500k sequences total).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_vs_human_evaluation</strong></td>
                            <td>Automated preprocessing algorithm.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Algorithmic guarantees of suffix arrays and empirical verification of counts; used to build evaluation sets reported in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Suffix array construction on 800GB corpora still computationally heavy (but tractable versus naive queries); may miss approximate duplicates unless additional logic applied.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Quantifying Memorization Across Neural Language Models', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>The secret sharer: Evaluating and testing unintended memorization in neural networks. <em>(Rating: 2)</em></li>
                <li>Extracting training data from large language models. <em>(Rating: 2)</em></li>
                <li>Deduplicating training data makes language models better. <em>(Rating: 2)</em></li>
                <li>Counterfactual memorization in neural language models. <em>(Rating: 2)</em></li>
                <li>Calibrating noise to sensitivity in private data analysis <em>(Rating: 1)</em></li>
                <li>Deduplicating training data mitigates privacy risks in language models. <em>(Rating: 1)</em></li>
                <li>What neural networks memorize and why: Discovering the long tail via influence estimation. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4608",
    "paper_id": "paper-28c7e583d90ccfc5c3078dfc1d6b80a9ad90248d",
    "extraction_schema_id": "extraction-schema-101",
    "extracted_data": [
        {
            "name_short": "Extractability (Def. 3.1)",
            "name_full": "k-token Context Extractability (Definition 3.1)",
            "brief_description": "Operational definition of memorization: a suffix s is 'extractable' from model f with k tokens of context if there exists a k-token prefix p (where [p||s] appears in f's training data) such that f produces s when prompted with p using greedy decoding.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "k-token Context Extractability",
            "evaluation_method_description": "Given a sequence x = [p || s] from the training data split into prefix p of length k and suffix s, prompt the model with p and perform greedy decoding; record a success if the model's generated continuation equals s exactly. Applied across many held-out training examples (sampled subsets) and averaged to estimate the fraction of the dataset that is extractable under a given context length k.",
            "evaluation_criteria": "Exact-match continuation (binary per example); aggregated fraction of sequences extractable (probability). Varies by context length k, duplication count, and model size. Uses greedy-decoding output equality to ground-truth suffix as primary criterion.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Natural language processing / language modeling (memorization detection)",
            "theory_type": "N/A — method for detecting memorized continuations (data extraction)",
            "human_comparison": false,
            "evaluation_results": "Used throughout paper to quantify memorization; showed strong log-linear trends with model size, duplication, and context (e.g., 6B GPT-J extractable fraction ~33% at 50-token context, rising to ~65% at 450 tokens in biased sample).",
            "automated_vs_human_evaluation": "Automated: exact-match string comparison between generated continuation and ground-truth suffix (greedy decoding). Some qualitative human inspection used for example analysis, but primary metric is automated.",
            "validation_method": "Compared against baselines (GPT-2 family not trained on The Pile), alternative sampling (uniform vs duplicate-normalized), replication across model families (T5, OPT, deduplicated models), and alternate decoding strategies (beam search). Statistical fits (R^2) and significance testing used to validate trends.",
            "limitations_challenges": "Requires access to the model's original training set and exact-text alignment; focused on greedy decoding which is not overall-most-likely-sequence; computationally expensive to apply exhaustively; biased by sampling strategy (uniform vs duplicate-normalized); extractability depends strongly on prompt/context length (discoverability).",
            "benchmark_dataset": "The authors applied the method to The Pile (825 GB) and subsets sampled from it; also applied to C4 and OPT datasets for replication.",
            "uuid": "e4608.0",
            "source_info": {
                "paper_title": "Quantifying Memorization Across Neural Language Models",
                "publication_date_yy_mm": "2022-02"
            }
        },
        {
            "name_short": "Exact-match Fraction",
            "name_full": "Fraction of Sequences Exactly Reproduced (Exact-match extractability rate)",
            "brief_description": "Metric reporting the fraction (probability) of evaluated prompts whose model-generated continuation exactly matches the ground-truth 50-token suffix from the training data.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Exact-match Extractability Rate",
            "evaluation_method_description": "Compute for a sampled set of training sequences the binary indicator of whether the model's generation equals the ground-truth suffix; average over examples (optionally across lengths and duplication buckets) to obtain an overall extractability probability.",
            "evaluation_criteria": "Binary exact-match per example; aggregated averages stratified by model size, duplication count, prompt/context length, and sequence length.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "NLP / language-model memorization",
            "theory_type": "N/A (metric for memorization)",
            "human_comparison": false,
            "evaluation_results": "Used to produce central quantitative claims: e.g., larger GPT-Neo models produce higher exact-match rates (biased sample showing near-log-linear growth with model size, R^2 = 99.8%); for duplication buckets, extractability rises roughly log-linearly with repetition count; beam search increased this metric by ~&lt;2 percentage points on average (max 5.6%).",
            "automated_vs_human_evaluation": "Automated (string equality checks).",
            "validation_method": "Cross-checked with alternate sampling strategies (uniform vs duplicate-normalized), decoding strategies (beam search), and dataset-wide containment checks (see alternate extractability).",
            "limitations_challenges": "Strict exact-match is a lower bound on memorization (misses cases where model outputs a different memorized suffix present elsewhere in the dataset); expensive to scale; sensitive to tokenization and minor formatting differences.",
            "uuid": "e4608.1",
            "source_info": {
                "paper_title": "Quantifying Memorization Across Neural Language Models",
                "publication_date_yy_mm": "2022-02"
            }
        },
        {
            "name_short": "Dataset-wide Match (alternate)",
            "name_full": "Dataset-wide Containment Extractability (alternate, loose lower bound)",
            "brief_description": "Relaxed definition: a generated continuation is counted as memorized if the concatenation [p||f(p)] appears anywhere in the training dataset (not necessarily the ground-truth continuation for that prompt).",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Dataset-wide Containment Extractability",
            "evaluation_method_description": "For each prompt p, generate continuation f(p); check whether the full generated sequence [p||f(p)] exists anywhere in the training corpus (requires large-scale substring search). Counts more instances of memorization because it includes outputs that match other training examples with the same prefix.",
            "evaluation_criteria": "Containment-in-training-dataset boolean per example; aggregated percentages compared to ground-truth exact-match metric.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "NLP / data-extraction auditing",
            "theory_type": "N/A (alternative memorization metric)",
            "human_comparison": false,
            "evaluation_results": "Yields substantially higher measured memorization; example: at 100 repetitions, 32.6% of outputs are contained somewhere in the dataset but only 15.8% match the ground-truth continuation. Maximum difference between this approach and exact-match was reported as 28.4% (at 2,200 repetitions).",
            "automated_vs_human_evaluation": "Automated but computationally expensive (requires searching an 800GB corpus for many generated strings).",
            "validation_method": "Compared directly to ground-truth exact-match metric on same evaluation sets; demonstrated consistent upward bias relative to exact-match.",
            "limitations_challenges": "Very high compute and I/O cost to check containment at scale; less reproducible without access to full training corpus; more permissive and thus less conservative as a privacy risk lower bound.",
            "uuid": "e4608.2",
            "source_info": {
                "paper_title": "Quantifying Memorization Across Neural Language Models",
                "publication_date_yy_mm": "2022-02"
            }
        },
        {
            "name_short": "Duplicate-normalized Sampling",
            "name_full": "Sampling Normalized by Sequence Length and Duplication Count",
            "brief_description": "Biased sampling procedure that constructs evaluation subsets balanced across sequence lengths and duplication-frequency buckets to study worst-case memorization behavior.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Duplicate- and Length-normalized Sampling",
            "evaluation_method_description": "For lengths l in {50,100,...,500} and duplication-count buckets defined as powers-of-two ranges (2^{n/4} to 2^{(n+1)/4}), sample 1,000 sequences per (length, duplication) bucket until unavailable. Aggregated to produce evaluation sets that over-represent highly duplicated and long-context examples to measure memorization as a function of those properties.",
            "evaluation_criteria": "Controlled representation across sequence-length bins and duplication-frequency bins to enable stratified analysis of extractability vs duplication and context length.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "NLP / dataset auditing",
            "theory_type": "N/A (sampling framework)",
            "human_comparison": false,
            "evaluation_results": "Used as primary evaluation set for key figures (Figure 1). Showed strong log-linear trends; emphasized worst-case memorization on duplicated long sequences. Contrasted with uniform random sampling which yields much lower absolute extractability rates.",
            "automated_vs_human_evaluation": "Automated sampling procedure, built around suffix-array based duplicate enumeration.",
            "validation_method": "Compared results obtained with this biased sample to those from uniformly random samples (100k sequences) to confirm qualitative trends (scale and context effects persisted), and used suffix-array methods to ensure correctness of duplication counts.",
            "limitations_challenges": "Biased toward worst-case scenarios so absolute rates are not representative of entire dataset; constructing duplicate-normalized sample requires heavy preprocessing (suffix array) and access to full training corpus.",
            "uuid": "e4608.3",
            "source_info": {
                "paper_title": "Quantifying Memorization Across Neural Language Models",
                "publication_date_yy_mm": "2022-02"
            }
        },
        {
            "name_short": "Uniform Random Sampling",
            "name_full": "Uniform Random Sampling of Training Sequences",
            "brief_description": "Baseline evaluation sampling each training example with equal probability to estimate overall memorization prevalence across the dataset.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Uniform Random Subset Evaluation",
            "evaluation_method_description": "Randomly sample N sequences uniformly from the training dataset (e.g., 50k or 100k) and apply the extractability test (prompt with prefix of length l-50, test exact-match for next 50 tokens) to estimate global memorization probabilities.",
            "evaluation_criteria": "Aggregate exact-match extractability probability across uniformly-sampled examples; useful for estimating absolute fraction of training data memorized.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "NLP / memorization auditing",
            "theory_type": "N/A (sampling design)",
            "human_comparison": false,
            "evaluation_results": "Produced qualitatively similar trends (larger models and longer context increase extractability) but lower absolute extractability percentages than duplicate-normalized sampling; used to estimate a lower bound (e.g., at least ~1% of The Pile extractable by 6B GPT-J model vs GPT-2 XL).",
            "automated_vs_human_evaluation": "Automated.",
            "validation_method": "Compared against duplicate-normalized sampling results; used to confirm robustness of scaling trends.",
            "limitations_challenges": "Because duplicates are rare, uniform sampling under-represents the high-duplication tail where most leakage occurs—thus produces loose lower bounds on worst-case memorization.",
            "uuid": "e4608.4",
            "source_info": {
                "paper_title": "Quantifying Memorization Across Neural Language Models",
                "publication_date_yy_mm": "2022-02"
            }
        },
        {
            "name_short": "Decoding-strategy Comparison",
            "name_full": "Comparative Evaluation across Decoding Strategies (greedy, beam search, random sampling)",
            "brief_description": "Empirical study of how decoding choice affects measured extractability, showing beam search slightly increases measured memorization versus greedy decoding, while sampling increases novelty.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Decoding Strategy Sensitivity Analysis",
            "evaluation_method_description": "Run the same extractability prompts under different decoding algorithms (greedy argmax at each step, beam search with b beams, and stochastic sampling) and compare exact-match extractability rates and qualitative novelty metrics (e.g., n-gram novelty in prior work).",
            "evaluation_criteria": "Extractability (exact-match) under each decoding method; percent-change between methods; qualitative analysis of novelty for sampling.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "NLP / generation evaluation",
            "theory_type": "N/A (methodology for robustness of extraction)",
            "human_comparison": false,
            "evaluation_results": "Beam search with 100 beams increased extractable memorization marginally (under 2 percentage points on average, max 5.6%), and beam and greedy produced identical outputs ~45% of the time. Random sampling was not used for main experiments because it increases novelty and reduces discoverability of memorization.",
            "automated_vs_human_evaluation": "Automated comparisons of generation outputs.",
            "validation_method": "Direct empirical comparison across decoding runs on same prompts; reported averages and maxima.",
            "limitations_challenges": "Different decoding strategies optimize different objectives (likelihood vs diversity), so measured memorization depends on the decoding method; choice should match attack or auditing scenario.",
            "uuid": "e4608.5",
            "source_info": {
                "paper_title": "Quantifying Memorization Across Neural Language Models",
                "publication_date_yy_mm": "2022-02"
            }
        },
        {
            "name_short": "Log-linear Scaling Fit",
            "name_full": "Log-linear Relationship Fitting (memorization vs scale/duplication/context)",
            "brief_description": "Statistical characterization: memorization probability grows approximately log-linearly with model size, duplication count, and context length; quality of fit measured by R^2.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Log-linear Trend Fitting (with R^2)",
            "evaluation_method_description": "Fit a log-linear model (e.g., linear fit on log-scaled independent variable) to empirical extractability rates versus variables like model parameter count, duplication count, or context tokens; report goodness-of-fit (R^2) to quantify how well a log-linear law explains observed scaling.",
            "evaluation_criteria": "Goodness-of-fit statistics (R^2), slope coefficients (e.g., ten-fold increase in model size corresponds to +19 percentage points in extractability in one reported fit), and visualization of trend lines.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "NLP / empirical scaling laws",
            "theory_type": "N/A (statistical characterization)",
            "human_comparison": false,
            "evaluation_results": "Reported near-perfect log-linear fit for model size vs memorization in the GPT-Neo family with R^2 = 99.8%; similar log-linear trends observed for duplication count and context length in many settings, though variance exists across datasets and objectives (e.g., T5 masked LM showed more variance for duplication).",
            "automated_vs_human_evaluation": "Automated statistical fitting and reporting.",
            "validation_method": "Replicated fits across multiple model families (GPT-Neo, T5, OPT, deduplicated models) and sampling strategies to check generality; reported where fit breaks down (e.g., T5 duplication irregularities).",
            "limitations_challenges": "Log-linear is empirical and may not generalize across all families or dataset preprocessing choices; dataset idiosyncrasies can produce deviations from monotonic trends (e.g., T5 buckets with whitespace-heavy duplicates).",
            "uuid": "e4608.6",
            "source_info": {
                "paper_title": "Quantifying Memorization Across Neural Language Models",
                "publication_date_yy_mm": "2022-02"
            }
        },
        {
            "name_short": "Differential Privacy (DP)",
            "name_full": "Differential Privacy (as a memorization definition/mitigation)",
            "brief_description": "Formal privacy framework that demands model outputs be (probabilistically) insensitive to removal of any single training example; cited as a formal but practically expensive way to limit memorization.",
            "citation_title": "Calibrating noise to sensitivity in private data analysis",
            "mention_or_use": "mention",
            "evaluation_method_name": "Differential Privacy (DP) Guarantees",
            "evaluation_method_description": "DP evaluates memorization by bounding how much influence any single training example can have on model outputs (epsilon-delta guarantees); DP training algorithms add calibrated noise during training to get provable bounds on leakage.",
            "evaluation_criteria": "Privacy budget (epsilon, delta), empirical trade-offs in utility vs privacy, and theoretical worst-case guarantees about influence of single examples.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Machine learning privacy (applied to NLP)",
            "theory_type": "Formal privacy constraint rather than a theory-evaluation metric",
            "human_comparison": false,
            "evaluation_results": "Paper notes DP is a leading general memorization definition but is ineffective for highly duplicated data and is expensive (computationally and utility-wise) in practice; references Abadi et al. (2016) and later DP work.",
            "automated_vs_human_evaluation": "Formal/mathematical definition (not an empirical metric); empirical DP implementations evaluated via standard training and test metrics.",
            "validation_method": "DP provides formal proofs; practical validation involves training DP variants and measuring utility degradation and empirical leakage (other works).",
            "limitations_challenges": "Doesn't prevent memorization of highly duplicated data well; DP training often incurs expensive computation, slower convergence, and utility loss.",
            "uuid": "e4608.7",
            "source_info": {
                "paper_title": "Quantifying Memorization Across Neural Language Models",
                "publication_date_yy_mm": "2022-02"
            }
        },
        {
            "name_short": "Exposure",
            "name_full": "Exposure metric (Carlini et al., 2019)",
            "brief_description": "Prior metric (exposure) quantifying memorization by measuring how many guesses an adversary needs to identify a secret training canary, used to bound memorization of carefully crafted examples.",
            "citation_title": "The secret sharer: Evaluating and testing unintended memorization in neural networks.",
            "mention_or_use": "mention",
            "evaluation_method_name": "Exposure",
            "evaluation_method_description": "Compute log-rank or surprisal-based measures for specific target sequences (canaries) to estimate how exposed a sequence is in the model's distribution; requires many generations per sequence and is best-suited for crafted examples rather than arbitrary training text.",
            "evaluation_criteria": "Exposure score (higher means easier to recover); rank of true secret among candidate completions; requires large sampling to estimate distribution tails.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "NLP privacy / memorization auditing",
            "theory_type": "Metric for memorization vulnerability",
            "human_comparison": false,
            "evaluation_results": "Paper contrasts exposure with their extractability metric: exposure requires thousands of generations per sequence and is suited to canaries, whereas their extractability test is more actionable for prompting with true training prefixes.",
            "automated_vs_human_evaluation": "Automated, sampling-based metric.",
            "validation_method": "Validated in prior work via recovery attacks on synthetic canaries and measurement of rank statistics.",
            "limitations_challenges": "Expensive to compute across many sequences; tailored to crafted canaries, not general-purpose training-set coverage measurement.",
            "uuid": "e4608.8",
            "source_info": {
                "paper_title": "Quantifying Memorization Across Neural Language Models",
                "publication_date_yy_mm": "2022-02"
            }
        },
        {
            "name_short": "k-eidetic memorization",
            "name_full": "k-eidetic Memorization (prior definition)",
            "brief_description": "A prior definition for unprompted memorization that identifies sequences the model will produce without prompting; referenced as useful for unprompted memorization but less suited for tight bounds via prompting.",
            "citation_title": "Extracting training data from large language models.",
            "mention_or_use": "mention",
            "evaluation_method_name": "k-eidetic Memorization",
            "evaluation_method_description": "Definition from prior work that flags sequences that a model can output unprompted (or with minimal prompt), often measured by searching model generations for exact occurrences; useful to assess unprompted copying behavior.",
            "evaluation_criteria": "Presence of sequence in unprompted model outputs; frequency across random generations; exposure-like statistics may be used.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "NLP memorization",
            "theory_type": "Definition/metric for unprompted memorization",
            "human_comparison": false,
            "evaluation_results": "Authors note k-eidetic memorization is useful for unprompted memorization work but less useful when one intends to prompt with training-data prefixes to bound memorization.",
            "automated_vs_human_evaluation": "Automated generation-and-detection.",
            "validation_method": "Prior empirical studies (Carlini et al., 2020) used generation sampling and matching to training set.",
            "limitations_challenges": "Requires many model generations; provides weaker/looser bounds for prompted extractability experiments.",
            "uuid": "e4608.9",
            "source_info": {
                "paper_title": "Quantifying Memorization Across Neural Language Models",
                "publication_date_yy_mm": "2022-02"
            }
        },
        {
            "name_short": "Counterfactual memorization",
            "name_full": "Counterfactual Memorization",
            "brief_description": "A notion from prior literature that measures how much influence individual training examples have on outputs under hypothetical dataset removals, used to study memorization's dependence on training data.",
            "citation_title": "Counterfactual memorization in neural language models.",
            "mention_or_use": "mention",
            "evaluation_method_name": "Counterfactual Memorization",
            "evaluation_method_description": "Approaches that conceptually (or via influence estimation) assess whether removing specific training items would change model outputs, often requiring training or approximating many counterfactual models to estimate influence.",
            "evaluation_criteria": "Change-in-output probability or generation behavior under removal of examples; influence scores per training example.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "NLP / interpretability and privacy",
            "theory_type": "Influence-based memorization metric",
            "human_comparison": false,
            "evaluation_results": "Paper mentions counterfactual memorization as an alternative but notes it can require training hundreds or thousands of models, which is impractical for large LMs.",
            "automated_vs_human_evaluation": "Primarily automated but computationally intensive (requires retraining or influence estimation).",
            "validation_method": "Prior work validated via retraining experiments or approximations of influence; authors cite this to justify their chosen extractability test.",
            "limitations_challenges": "Impractical at scale due to training cost; not suited for large-scale language models without approximation.",
            "uuid": "e4608.10",
            "source_info": {
                "paper_title": "Quantifying Memorization Across Neural Language Models",
                "publication_date_yy_mm": "2022-02"
            }
        },
        {
            "name_short": "Suffix Array Enumeration",
            "name_full": "Suffix Array Construction for Duplicate Enumeration",
            "brief_description": "A scalable engineering method to enumerate all substrings of a large corpus and count their duplication frequencies, enabling the construction of duplicate-normalized evaluation samples.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method_name": "Suffix-Array-based Duplicate Enumeration",
            "evaluation_method_description": "Build a suffix array over the entire training corpus to allow efficient linear-time enumeration of all k-token sequences repeated between N and M times; sample uniformly from such buckets to create balanced evaluation sets by duplication count.",
            "evaluation_criteria": "Correctness of duplicate counts, computational scalability to hundreds of GBs, ability to retrieve N-to-M repetition buckets efficiently.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Data engineering for NLP evaluation",
            "theory_type": "Tooling for sampling and evaluation",
            "human_comparison": false,
            "evaluation_results": "Enabled the authors to construct 1,000-sequence buckets spanning from ~6-8 repeats to ~724-861 repeats and to assemble the duplicate-normalized evaluation dataset (≈500k sequences total).",
            "automated_vs_human_evaluation": "Automated preprocessing algorithm.",
            "validation_method": "Algorithmic guarantees of suffix arrays and empirical verification of counts; used to build evaluation sets reported in experiments.",
            "limitations_challenges": "Suffix array construction on 800GB corpora still computationally heavy (but tractable versus naive queries); may miss approximate duplicates unless additional logic applied.",
            "uuid": "e4608.11",
            "source_info": {
                "paper_title": "Quantifying Memorization Across Neural Language Models",
                "publication_date_yy_mm": "2022-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "The secret sharer: Evaluating and testing unintended memorization in neural networks.",
            "rating": 2
        },
        {
            "paper_title": "Extracting training data from large language models.",
            "rating": 2
        },
        {
            "paper_title": "Deduplicating training data makes language models better.",
            "rating": 2
        },
        {
            "paper_title": "Counterfactual memorization in neural language models.",
            "rating": 2
        },
        {
            "paper_title": "Calibrating noise to sensitivity in private data analysis",
            "rating": 1
        },
        {
            "paper_title": "Deduplicating training data mitigates privacy risks in language models.",
            "rating": 1
        },
        {
            "paper_title": "What neural networks memorize and why: Discovering the long tail via influence estimation.",
            "rating": 1
        }
    ],
    "cost": 0.02105825,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>QUANTIFYING MEMORIZATION ACROSS NEURAL LANGUAGE MODELS</h1>
<p>Nicholas Carlini ${ }^{1}$ Daphne Ippolito ${ }^{1,2}$ Matthew Jagielski ${ }^{1}$<br>Katherine Lee ${ }^{1,3}$ Florian Tramèr ${ }^{1}$ Chiyuan Zhang ${ }^{1}$<br>${ }^{1}$ Google Research<br>${ }^{2}$ University of Pennsylvania<br>${ }^{3}$ Cornell University</p>
<h2>ABSTRACT</h2>
<h4>Abstract</h4>
<p>Large language models (LMs) have been shown to memorize parts of their training data, and when prompted appropriately, they will emit the memorized training data verbatim. This is undesirable because memorization violates privacy (exposing user data), degrades utility (repeated easy-to-memorize text is often low quality), and hurts fairness (some texts are memorized over others). We describe three log-linear relationships that quantify the degree to which LM emit memorized training data. Memorization significantly grows as we increase (1) the capacity of a model, (2) the number of times an example has been duplicated, and (3) the number of tokens of context used to prompt the model. Surprisingly, we find the situation becomes more complicated when generalizing these results across model families. On the whole, we find that memorization in LM is more prevalent than previously believed and will likely get worse as models continues to scale, at least without active mitigations.</p>
<h2>1 INTRODUCTION</h2>
<p>The performance of neural language models has continuously improved as these models have grown from millions to trillions of parameters (Fedus et al., 2021), with their training sets similarly growing from millions to trillions of tokens. In anticipation of future, even larger models trained on minimally curated datasets, it is important to quantify factors that lead to increased memorization of a model's training set. Indeed, recent work has shown that training data extraction attacks are a practical threat for current language models (Carlini et al., 2020); an adversary interacting with a pretrained model can extract individual sequences that were used to train the model.</p>
<p>While current attacks are effective, they only represent a lower bound on how much memorization occurs in existing models. For example, by querying the GPT-2 language model, Carlini et al. (2020) (manually) identified just 600 memorized training examples out of a 40GB training dataset. This attack establishes a (loose) lower bound that at least $0.00000015 \%$ of the dataset is memorized. In contrast, we are able to show that the 6 billion parameter GPT-J model (Black et al., 2021; Wang and Komatsuzaki, 2021) memorizes at least $1 \%$ of its training dataset: The Pile (Gao et al., 2020).</p>
<p>In addition to prior work's loose estimates of models' memorization capabilities, there is a limited understanding of how memorization varies across different neural language models and datasets of different scales. Prior studies of memorization in language models either focus on models or datasets of a fixed size (Carlini et al., 2019; Zhang et al., 2021; Thakkar et al., 2020) or identify a narrow memorization-versus-scale relationship (Carlini et al., 2020; Lee et al., 2021). While McCoy et al. (2021) broadly study the extent to which language models memorize, their focus is on how to avoid the problem and ensure novelty of model outputs, rather than on studying model risk through identifying the maximal amount of data memorization.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>This paper addresses both of the above open questions by comprehensively quantifying memorization across three families of neural language models and their associated datasets. We leverage access to each model's original training set to provide order-of-magnitude more precise bounds on the amount of extractable data that an adversary could recover than in prior works.</p>
<p>We first construct a set of prompts from the model's training set. By feeding prefixes of these prompts into the trained model, we check whether the model has the ability to complete the rest of the example verbatim. This allows us to measure memorization across models, datasets, and prompts of varying sizes. We identify three properties that significantly impact memorization:</p>
<ol>
<li>Model scale: Within a model family, larger models memorize $2-5 \times$ more than smaller models.</li>
<li>Data duplication: Examples repeated more often are more likely to be extractable.</li>
<li>Context: It is orders of magnitude easier to extract sequences when given a longer context.</li>
</ol>
<p>Our analysis suggests that future research on neural language modeling will need to take steps to prevent future (larger) models from memorizing their training datasets.</p>
<h1>2 Related Work</h1>
<p>There is extensive prior work that qualitatively studies memorization in neural language models. Prior work has demonstrated extraction attacks that recover memorized data including URLs, phone numbers, and other personal information (Carlini et al., 2020; Ziegler, 2021)—or synthetically injected "canaries" (Carlini et al., 2019; Henderson et al., 2018; Thakkar et al., 2020; Thomas et al., 2020). However most of these works are qualitative and aim to demonstrate the existence of extractable data, rather than precisely quantifying how much models memorize. For example, the unprompted memorization evaluation of Carlini et al. (2020) found just 600 examples of memorization in GPT-2. Our paper aims to establish tighter bounds on the fraction of a dataset that is memorized.</p>
<p>Our analysis is relevant to the broad literature on privacy attacks on machine learning. For example, membership inference attacks (Shokri et al., 2017; Yeom et al., 2018) let an adversary detect the presence of a given example in a model's training set; other forms of data leakage let an adversary learn dataset properties (Ganju et al., 2018; Fredrikson et al., 2015). We focus on extraction attacks due to their relevance for language modeling-extraction implies significant leakage from a model, and grows with data duplication (Lee et al., 2021), a common feature of large-scale text datasets.</p>
<p>Various definitions of memorization in deep neural networks have been studied in prior work (Carlini et al., 2019; 2020; Feldman and Zhang, 2020; Zhang et al., 2021). A detailed comparison with those existing formulations is presented in Section 3.1. One leading general memorization definition is differential privacy (Dwork et al., 2006), which formalizes the idea that removing any one example from the training set should not change the trained model. However, while differential privacy protects a single user's private information, it is ineffective for preventing memorization of highly duplicated data, and does not capture the complexity of social, linguistic data (Brown et al., 2022). Also, differentially private learning algorithms (Abadi et al., 2016) generally suffer from expensive computation, slow convergence, and poor model utility, despite recent advances (Anil et al., 2021).</p>
<p>In concurrent work, Kandpal et al. (2022) study how often models emit memorized data as a function of data duplication. Their analysis focuses on evaluating why training data extraction attacks succeed. In contrast, we explicitly prompt models with training data prefixes in order to measure memorization in the worst case, something that a practical attack cannot necessarily do.</p>
<p>Prior scaling hypotheses. Our motivation to study scaling phenomena stems from anecdotal evidence in prior work that memorization ability relates to various aspects of scale. In particular, our analysis on model scale is informed by preliminary experiments in (Zhang et al., 2017; Carlini et al., 2020), our data duplication experiments follow in the line of Lee et al. (2021), and our context length experiments build on hypotheses by Carlini et al. (2020); Ziegler (2021).</p>
<h2>3 Methodology</h2>
<h3>3.1 Definition of Memorization</h3>
<p>To begin, we first select a precise definition for memorization:</p>
<p>Definition 3.1. A string $s$ is extractable with $k$ tokens of context from a model $f$ if there exists a (length- $k$ ) string $p$, such that the concatenation $[p | s]$ is contained in the training data for $f$, and $f$ produces $s$ when prompted with $p$ using greedy decoding.</p>
<p>For example, if a model’s training dataset contains the sequence "My phone number is 555-6789", and given the length $k=4$ prefix "My phone number is", the most likely output is "555-6789", then this sequence is extractable (with 4 words of context). We focus on greedy sampling in this paper, and verify in Section 4.1 that our choice of decoding strategy does not significantly impact our results.</p>
<p>While prior work proposed other definitions, we prefer ours in this paper as it is more actionable. Some memorization definitions, including lower-bounds on differential privacy (Dwork et al., 2006; Jagielski et al., 2020; Nasr et al., 2021) or counterfactual memorization (Feldman and Zhang, 2020; Zhang et al., 2021), require training hundreds or thousands of models, which is impractical for large language models. Alternatively, computing exposure (Carlini et al., 2019) requires thousands of generations per sequence, and is only designed for carefully crafted training examples.Finally, $k$-eidetic memorization (Carlini et al., 2020), is a useful definition for unprompted memorization, but less useful for tightly bounding memorization by prompting with training data (as we will do). Future work might explore how our three scaling observations apply to other definitions of memorization.</p>
<h1>3.2 SELECTION OF EVALUATION DATA</h1>
<p>Having chosen a definition, we next describe our evaluation procedure. Ideally, we would consider every sequence $x=[p | s]$ in the model's training dataset (where $x$ has been split into a length- $k$ prefix $p$ and a suffix $s$ ). For each sequence, we would report if the model exactly reproduces $s$ when prompted with $p$, following Definition 3.1. Unfortunately, performing this test on every sequence in the training data would be prohibitively expensive. For example, the largest 6 billion parameter GPT-Neo model has a throughput of roughly one 100-token generation per second on a V100 GPU. Extrapolating to the 800GB training dataset, this would require over 30 GPU-years of compute.</p>
<p>Instead, we query on a smaller subset of the training data, that still produces statistically confident estimates. In this paper we randomly choose subsets of roughly 50,000 sequences, allowing us to efficiently run inference in just a few hours. The primary criteria when choosing a subset of the training data is to obtain a representative sample that allows us to draw meaningful conclusions from the data. We consider two approaches to constructing a subset of the data.</p>
<p>Our first subset is a uniformly random sample of 50,000 sequences, drawn from the training dataset without repetition. While a uniform sample is useful to estimate the absolute amount of memorization in a model, it is poorly suited for studying how memorization scales with data properties that are not uniformly represented in the training set. For example, prior work has identified that data duplication (i.e., how often the same sequence is repeated either exactly or approximately) is an important factor for memorization. Yet, because the frequency of training data duplication decays extremely quickly (Lee et al., 2021), a uniformly random sample of 50,000 sequences (accounting for $\leq 0.02 \%$ of the dataset) is unlikely to contain any signal that would allow us to accurately measure the tail of this repeated data distribution. A similar concern arises for measuring how memorization scales with prompt length, since very long sentences account for only a small fraction of the training set.</p>
<p>Therefore, our second subset is a random sample normalized by both sequence lengths and duplication counts, which allows us to accurately measure memorization of large language models in the worst-case, on highly duplicated data with long prompts. For each sequence length $\ell \in{50,100,150, \ldots, 500}$, and integer $n$, we select 1,000 sequences of length $\ell$ that are contained in the training dataset between $2^{n / 4}$ and $2^{(n+1) / 4}$ times. We do this until we reach an $n$ for which 1,000 sequences are not available. This gives us 1,000 sequences that repeat between 6 and 8 times $\left(\approx 2^{11 / 4}\right.$ and $\left.\approx 2^{12 / 4}\right)$ and also 1,000 sequences that repeat between 724 and 861 times $\left(\approx 2^{38 / 4}\right.$ and $\left.\approx 2^{39 / 4}\right)$. This biased sampling allows us to more accurately measure memorization as a function of a sample's duplication factor and prompt length, without querying the entire dataset. Note that constructing this duplicate-normalized data subset requires some work, as efficiently identifying duplicate substrings in an 800GB training dataset is computationally challenging. We make use of the suffix array construction from Lee et al. (2021) (see Appendix).</p>
<p>For each length from 50 to 500 tokens, we collect 50,000 examples duplicated varying numbers of times, totaling roughly 500,000 sequences. For each sequence of length $\ell$, we prompt the model with</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: We prompt various sizes of GPT-Neo models (green) with data sampled from their training set—The Pile, and normalized by sequence lengths and duplication counts. As a baseline (yellow), we also prompt the GPT-2 family of models with the same Pile-derived prompts, even though these models were trained on WebText, a different training dataset. (a) Larger models memorize a larger fraction of their training dataset, following a log-linear relationship. This is not just a result of better generalization, as shown by the lack of growth for the GPT-2 baseline models. (b) Examples that are repeated more often in the training set are more likely to be extractable, again following a log-linear trend (baseline is GPT-2 XL). (c) As the number of tokens of context available increases, so does our ability to extract memorized text (baseline is GPT-2 XL).
the first $\ell-50$ tokens and report the sequence as "extractable" if the model exactly emits the next 50 token suffix of this sequence. Fifty tokens corresponds to an average of 127 characters or 25 wordsin the GPT-Neo training set, well over the length of a typical English sentence. Finally, we compute the average probability that a sequence is extractable by averaging over all lengths $\ell$.</p>
<h1>4 EXPERIMENTS</h1>
<p>We primarily study the GPT-Neo model family (Black et al., 2021; Wang and Komatsuzaki, 2021) trained on the Pile dataset (Gao et al., 2020). The GPT-Neo models are causal language models trained with the objective of predicting the next token in a sequence given the previous ones. They come in four sizes: 125 million, 1.3 billion, 2.7 billion and 6 billion parameters. ${ }^{1}$ The Pile is a dataset of 825 GB of text collected from various sources (e.g., books, Web scrapes, open source code). Prior to the recent release of OPT (Zhang et al., 2022), the GPT-Neo models were the largest language models available for public download, and The Pile is the largest public text dataset available.</p>
<h3>4.1 Bigger Models Memorize More</h3>
<p>We begin by considering the impact of model size on memorization, expanding on prior studies which qualitatively established a relationship between the size of GPT-2 models and their ability to memorize $&lt;30$ URLs (Carlini et al., 2020). In contrast, we study a million model generations in order to describe how model scale relates to memorization.</p>
<p>Results. We first study our biased random data sample normalized by duplication count and sequence lengths. The results of this experiment are given in Figure 1a. The y-axis reports the fraction of generations which exactly reproduce the true suffix for their prompt, averaged over all prompt and sequence lengths in our evaluation set. Because our biased sampling over-represents duplicated strings, the absolute degree of memorization in Figure 1a is not particularly important here-rather, we are interested in how memorization varies with scale. ${ }^{2}$ We find that larger models memorize significantly more than smaller models do, with a near-perfect log-linear fit ( $R^{2}$ of $99.8 \%$ ): a ten fold increase in model size corresponds to an increase in memorization of 19 percentage points.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>To confirm that larger models are indeed memorizing more data, and not simply generalizing better, we repeat the analysis with the GPT-2 model family as a baseline. The GPT-2 models are similarly sized, and also trained on Internet-scraped data. If our "larger models memorize more" result was due to the predictive strength of larger models, and not the memorization of specific training data, we would expect a similar relationship between comparably sized GPT-2 models trained on similar data. Put differently, this baseline allows to establish what fraction of the training data is sufficiently "easy" that any language model can correctly predict the 50 -token suffix, even if the example has not been seen during training. For example, a language model trained on multiple examples of number sequences can likely correctly complete some other unseen number sequences. We find that GPT-2 correctly completes approximately $6 \%$ of the examples in our evaluation set, compared to $40 \%$ for the similarly sized 1.3B parameter GPT-Neo model. A qualitative analysis (see examples in Appendix Figure 15) suggests that examples "memorized" by GPT-2 are largely uninteresting sequences (e.g., number sequences, repetitions of the same few tokens, or common phrases). Therefore, we conclude that larger models have a higher fraction of extractable training data because they have actually memorized the data; it is not simply that the larger models are more accurate.</p>
<h1>4.2 Repeated Strings are Memorized More</h1>
<p>Prior work provides preliminary evidence that memorization in language models increases with the number of times sequences are repeated in the training set (Carlini et al., 2020; Lee et al., 2021). We expand on this observation and quantitatively measure the effect of data duplication on memorization. Using our duplication-normalized data sample, we measure the fraction of sequences which are extractable, for buckets of sequences duplicated between 2 and 900 times. Each bucket consists of 1,000 distinct sentences, and we compute the average amount of memorization for each bucket.
Results. Figure 1b shows our results, aggregated over all sequence lengths. We observe a clear log-linear trend in memorization. While models rarely regurgitate strings that are repeated only a few times, this probability increases severely for highly duplicated strings. The small memorization values at low numbers of repetitions corroborates the positive impact of training dataset deduplication on memorization observed by Lee et al. (2021). However, we find that memorization does still happen, even with just a few duplicates-thus, deduplication will not perfectly prevent leakage. While this relationship is perhaps obvious, and has been corroborated for specific training examples in prior work (Carlini et al., 2019; 2020), our results show that it holds across the entire training set.</p>
<h3>4.3 LONGER CONTEXT DISCOVERS MORE MEMORIZATION</h3>
<p>The previous two questions evaluated how data collection and model training decisions impact the leakage of a model's training data when it is provided a fixed number of tokens from a sequence as context. As a result, those experiments suggest particular actions that could be taken to mitigate memorization (by reducing model size, or limiting the number of duplicate examples).</p>
<p>However, even when the model is fixed, it is possible to vary the amount of extractable training data by controlling the length of the prefix passed to the model. By studying how the number of tokens of context impacts extractability, we demonstrate the difficulty of discovering memorization-language models may only exhibit their memorization under favorable conditions.</p>
<p>Results. In Figure 1c, we observe that the fraction of extractable sequences increases log-linearly with the number of tokens of context. For example, $33 \%$ of training sequences in our evaluation set are extractable from the 6 B model at 50 tokens of context, compared to $65 \%$ with 450 tokens of context. We call this the discoverability phenomenon: some memorization only becomes apparent under certain conditions, such as when the model is prompted with a sufficiently long context.</p>
<p>The discoverability phenomenon may seem natural: conditioning a model on 100 tokens of context is more specific than conditioning the model on 50 tokens of context, and it is natural that the model would estimate the probability of the training data as higher in this situation. However, the result is that some strings are "hidden" in the model and require more knowledge than others to be extractable.</p>
<p>From one point of view, it is good that some memorization is difficult to discover. This makes it harder for attackers to perform training data extraction attacks (Carlini et al., 2020), or otherwise exploit memorization. Indeed, if an exact 100 token prompt is required to make the model output a given string, then, in practice, an adversary will likely be unable to perform the attack. The difficulty</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: (a) Fraction of sequences extracted as a function of model scale where we sample uniformly from the training set. (b) Fraction of sequences extracted as we vary the length of the prompt. For each sequence length $n$, $n-50$ tokens are used as the prefix, and we check for extraction of the remaining 50 tokens. (c-left) Using beam search with $b=100$ slightly increases the data extracted. (c-right) We observe considerably more memorization when checking whether the generated sequence occurs anywhere in the entire training set (Section C). However, this approach is very computationally expensive so we do not use it for our other experiments.
in discovering memorization also reduces the likelihood of non-adversarial training data regurgitation. For example, the GitHub Copilot model (Chen et al., 2021) reportedly rarely emits memorized code in benign situations, and most memorization occurs only when the model has been prompted with long code excerpts that are very similar to the training data (Ziegler, 2021). Practitioners building language generation APIs could (until stronger attacks are developed) significantly reduce extraction risk by restricting the maximum prompt length available to users.</p>
<p>Viewed differently, however, the difficulty of discovering memorization can also harm our ability to audit privacy in machine learning models. Because provably-correct approaches for privacypreserving training of machine learning models are applied only rarely in practice (Abadi et al., 2016; Thakkar et al., 2020; Ramaswamy et al., 2020), it is common to attempt post-hoc privacy auditing (Jayaraman and Evans, 2019; Jagielski et al., 2020; Nasr et al., 2021). Our results suggest that correctly auditing large language models likely requires prompting the model with training data, as there are no known techniques to identify the tail of memorized data without conditioning the model with a large context. Improving upon this limitation is an interesting problem for future work.</p>
<h1>4.4 Alternate Experimental Settings</h1>
<p>In this section, we briefly review other strategies that we could have used to quantify memorization.
Random dataset sampling. The majority of this paper uses subsets of the training data that were explicitly sampled according to training data duplication frequency. Now, we consider how our results would differ if we chose a truly random subset of the training data, where each sequence is sampled uniformly, instead of sampling a duplicate-normalized dataset. Specifically, we randomly sample 100,000 sequences of varying lengths from The Pile dataset, then prompt the model and test for memorization as before (more details in Appendix C).</p>
<p>Figure 2a and Figure 2b present the results. We observe similar qualitative trends with model scale and context length as in Figure 1. Larger models memorize more training examples than smaller models-and much more than the GPT-2 models that were not trained on The Pile. Similarly, providing more context to a model increases the likelihood we discover memorization. We can extract the last 50 tokens of a length-1000 sequence with $7 \%$ probability for the largest GPT-J 6B model, compared to $4 \%$ probability for the smallest 125 M GPT-Neo model. (And both of these are much larger than the $2 \%$ probability of extraction for the 1.5B parameter GPT2-XL model.) These results, taken together, allow us to estimate a lower bound that there is at least $1 \%$ of The Pile dataset that is extractable by the 6B GPT-J model, but not by GPT-2 XL.</p>
<p>Alternate decoding strategies. We have defined memorization as a model's ability to generate the true continuation when choosing the most likely token at every step of decoding. Yet, this greedy decoding strategy does not produce the overall most likely sequence. Many language model</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Text examples that are memorized by the 6B model, but not by smaller models. Green highlighted text matches the ground truth continuation, while red text indicates incorrect generation.
applications use other decoding strategies, such as beam search to find the generation with highest likelihood. To understand how our choice of decoding strategy affects the amount of memorization we measure, we compare greedy decoding with beam search in Figure 2(c). We find that using beam search with 100 beams results in marginally more extracted memorization. The difference in extractable memorization is just under 2 percentage points on average, with a maximum of 5.6\%. Interestingly, beam search and greedy decoding generated the same output $45 \%$ of the time.
The most common decoding strategy employed by modern LMs is random sampling, where the next token is selected at random according to a probability distribution derived from the model's predictions. McCoy et al. (2021) found that random sampling resulted in generated text with a greater number of novel $n$-grams. Since the goal of our study is to maximize discoverability-an antithetical goal to maximizing linguistic novelty-we do not present experiments that use random sampling.
Alternate definition of extractability. Our main experiments report a sequence as "extractable" if the model's generation is identical to the true suffix of the considered training example. However it is possible this suffix is still present (elsewhere) in the dataset. We now consider a loose lower bound on memorization that considers a sequence memorized if the generation $[p | f(p)]$ from a prompt $p$ is contained anywhere in the training dataset. Searching within the entire dataset finds more memorized content than comparing with the ground truth (Figure 2c). For examples at 100 repetitions, $32.6 \%$ of outputs are contained somewhere in the dataset but just $15.8 \%$ match the ground truth continuation.</p>
<h1>4.5 Qualitative Examples of Memorization</h1>
<p>In Figure 3, we present qualitative examples that are only memorized by the largest (6B) model, but not the smaller ones. We highlight some interesting patterns in these sequences: while the generations from the smaller models do not match the training data, they are generally thematically-relevant and locally consistent. However, a closer inspection reveals that those generations are only syntactically sound, but semantically incorrect. Appendix Figure 8 shows further examples of sequences that are memorized by all the models. We found most of these universally-memorized sequences to be "unconventional" texts such as code snippets or highly duplicated texts such as open source licenses. Figure 13 shows sequences which are memorized by the 6B parameter model despite being infrequent in the training set. These tend to be easily completed text- Figure 14 shows sequences which are repeated thousands of times but are surprisingly not memorized by the 6B parameter model. Many of these are mostly correctly completed, only differing on semantically unimportant characters.</p>
<h2>5 Replication Study</h2>
<p>The above analysis provides evidence that memorization scales log-linearly with model size, data duplicates, and context length. We now replicate this analysis for other language models trained with different datasets and training objectives, namely: (1) the T5 family of models trained on the C4 dataset (Raffel et al., 2020), (2) models from Lee et al. (2021), trained on a deduplicated version of C4, and (3) the OPT family of models (Zhang et al., 2022), also trained on the Pile. We expected our results to cleanly generalize across settings, and this is indeed true for model scale. Yet, the situation is more complicated when considering data duplication, due to training set idiosyncrasies.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: (a) Masked language model objective: Larger models have a higher fraction of sequences extractable on T5. (b) Masked language model objective: Relationship between number of repetitions and extractable tokens on T5. (c) Causal language model objective: Relationship between number of repetitions and memorization on language models trained with deduplicated data.</p>
<h1>5.1 T5 MASKED LANGUAGE MODELING</h1>
<p>Model and dataset. The T5 v1.1 models are masked encoder-decoder models trained to reproduce randomly deleted spans from an input sequence. The models vary in size from 77 M to 11 B billion parameters, and are trained on C4-a 806 GB curated version of English web pages from the Common Crawl. The largest T5 model (11B parameters) is the largest publicly available masked language model. T5 models are thus good candidates for studying how memorization scales with model size.</p>
<p>We must first define what is meant by "extractable data" for the masked language modeling task. T5 models are trained by removing a random $15 \%$ of tokens from each training sequence (i.i.d), and the model must then "fill in the blanks" to restore the tokens that were dropped from the input. As a result of this different training objective, Definition 3.1 is not directly applicable: the model does not operate on a prefix and output a suffix. We instead call a sequence memorized if the model perfectly solves the masked language modeling task on that sequence. For example, we call a 200-token sequence memorized if the model can use the $170(=200 \cdot 0.85))$ tokens of context to perfectly predict the remaining 30 tokens $(=200 \cdot 0.15)$. Because this token-dropping procedure is stochastic, it is possible that one set of dropped tokens might yield an output of "memorized" and another might not. For simplicity, we inspect only one set of masked tokens per sequence; because we are already averaging over 50,000 sequences this additional randomness does not harm the results of our analysis.</p>
<p>Results. In Figure 4a, we reproduce the model scaling effect (from Figure 1a) for T5 models. Larger models similarly have an increased ability to perfectly solve the masked prediction task. Surprisingly, while a scaling trend does hold here as well, the absolute memorization in masked models is an order of magnitude lower than for comparably sized causal language models. For example, the 3B parameter T5-XL model memorizes $3.5 \%$ of sequences repeated 100 times, whereas the GPT-Neo 2.7B model memorizes $53.6 \%$ of sequences repeated 100 times (with 150 tokens of context).</p>
<p>Next, we turn to reproducing the analysis of how memorization scales with data duplication. The situation here becomes significantly less clear. As shown in Figure 4b, sequences duplicated more often tend to be easier to memorize, but there is no monotonic scaling relationship. Compared to the case of the GPT-Neo models trained on The Pile, the relation between data duplication counts and memorization for T5 models trained on C4 exhibits large variance. This variance is statistically significant: sequences repeated 159 to 196 times are memorized with probability less than $5.1 \%$ with $99.7 \%$ confidence (three standard deviations from the mean), however sequences repeated 138 to 158 times (that is, less often) are memorized with probability at least $6.2 \%$ (also with $99.7 \%$ confidence). That is, for some reason, sequences that occur $\sim 140$ times are more likely to be memorized, despite occurring less often, even if we assume a three-sigma error in both measurements simultaneously.</p>
<p>In order to explain this counter-intuitive phenomenon, we qualitatively study each of these two buckets of examples to understand this difference. We find that most of the duplicate examples repeated 138-158 times consist mainly of whitespace tokens. These sequences are thus much easier to predict correctly than other sequences, even if they are repeated more often. This effect, to a lesser extent, can be found in other buckets which contain many approximately near duplicates.</p>
<h1>5.2 Language Models Trained on Deduplicated Data</h1>
<p>Model and dataset. The models used in Lee et al. (2021) are 1.5B parameter causal language models. This model family consists of one model trained on C 4 (the same dataset as T5), one model trained on a version of C 4 that was deduplicated by removing all documents which were near-duplicates of other documents, and one model trained on a version of C 4 that was deduplicated by deleting any string of length-50 tokens that occurred more than once. Lee et al. (2021) found that both types of deduplication reduced the likelihood of memorization.</p>
<p>Results. We were most interested in whether models trained on deduplicated data would still exhibit increased memorization of examples which were repeated frequently in the original, non-deduplicated C4 dataset (e.g., because the deduplication missed some near-duplicates). Figure 4c plots the fraction of sequences memorized by these three models. We draw two interesting conclusions from this data.</p>
<p>First, we confirm that models trained on deduplicated datasets memorize less data than models trained without deduplication. For example, for sequences repeated below 35 times, the exact deduplicated model memorizes an average of $1.2 \%$ of sequences, compared to $3.6 \%$ without deduplication, a statistically significant $\left(p&lt;10^{-15}\right)$ decrease by a factor of $3 \times$. Second, while deduplication does help for sequences repeated up to $\sim 100$ times, it does not help for sequences repeated more often! The extractability of examples repeated at least 408 times is statistically significantly higher than any other number of repeats before this. We hypothesize that this is due to the fact that any deduplication strategy is necessarily imperfect in order to efficiently scale to hundreds of gigabytes of training data. Thus, while it may be possible to remove most instances of duplicate data, different and valid definitions of duplicates can mean deduplication is not exhaustive.</p>
<h3>5.3 Language Models Trained on a Modified Version of the Pile</h3>
<p>Model and dataset. We finally study the OPT family of models (Zhang et al., 2022), that vary from 125 million to 175 billion parameters. ${ }^{3}$ These models were trained on a 800 GB dataset that overlaps with The Pile but is not identical and contains data from many new sources, while also removing some data from the Pile. This dataset was also deduplicated prior to training, and so we do not expect to see duplicate sequences memorized (much) more than sequences repeated only a few times.</p>
<p>Results. Overall, we find that while there are nearly identical scaling trends to those we found for GPT-Neo's model family, the effect size is orders-of-magnitude smaller (figure 7). Even the 66 billion parameter model memorizes a smaller fraction of The Pile than the smallest 125 million parameter GPT Neo model. This suggests two possible conclusions: (a) careful data curation and training can mitigate memorization, or (b) even slight shifts in data distribution can significantly alter what content gets memorized. Without direct access to the original training dataset, we can not distinguish between these two conclusions and hope future work will be able to resolve this question.</p>
<h2>6 CONCLUSION</h2>
<p>Our paper presents the first comprehensive quantitative analysis of memorization in large language models, by re-processing the training set to find memorized data. Our work has two broad conclusions.</p>
<p>For the study of generalization, we have shown that while current LMs do accurately model the statistics of their training data, this need not imply that they faithfully model the desired underlying data distribution. In particular, when the training data distribution is skewed (e.g., by containing many duplicates of some sequences) larger models are likely to learn these unintended dataset peculiarities. It is therefore important to carefully analyze the datasets used to train ever larger models, as future (larger) models are likely to remember even more training details than current (smaller) models.</p>
<p>For the study of privacy, our work indicates that current large language models memorize a significant fraction of their training datasets. Memorization scales log-linear with model size-by doubling the number of parameters in a model we can extract a significantly larger fraction of the dataset. Given that current state-of-the-art models contain more than $200 \times$ as many parameters as the largest 6B parameter model we analyze, it is likely that these even larger models memorize many sequences</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>that are repeated just a handful of times. At the same time, we have shown that this memorization is often hard to discover, and for an attack to actually extract this data it will be necessary to develop qualitatively new attack strategies. Fortunately, it appears that (for the comparatively small models we study) training data inserted just once is rarely memorized, and so deduplicating training datasets (Lee et al., 2021) is likely a practical technique to mitigate the harms of memorization.</p>
<h1>REFERENCES</h1>
<p>Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC conference on computer and communications security, pages 308-318, 2016.</p>
<p>Rohan Anil, Badih Ghazi, Vineet Gupta, Ravi Kumar, and Pasin Manurangsi. Large-scale differentially private BERT. arXiv preprint arXiv:2108.01624, 2021.</p>
<p>Sid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow, March 2021. URL https://doi.org/ 10.5281/zenodo. 5297715 . If you use this software, please cite it using these metadata.</p>
<p>Hannah Brown, Katherine Lee, Fatemehsadat Mireshghallah, Reza Shokri, and Florian Tramèr. What does it mean for a language model to preserve privacy?, 2022.</p>
<p>Nicholas Carlini, Chang Liu, Úlfar Erlingsson, Jernej Kos, and Dawn Song. The secret sharer: Evaluating and testing unintended memorization in neural networks. In USENIX Security Symposium, 2019 .</p>
<p>Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, et al. Extracting training data from large language models. arXiv preprint arXiv:2012.07805, 2020.</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.</p>
<p>Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private data analysis. In Theory of cryptography conference, pages 265-284. Springer, 2006.</p>
<p>William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. arXiv preprint arXiv:2101.03961, 2021.</p>
<p>Vitaly Feldman and Chiyuan Zhang. What neural networks memorize and why: Discovering the long tail via influence estimation. In Advances in Neural Information Processing Systems, 2020.</p>
<p>Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. Model inversion attacks that exploit confidence information and basic countermeasures. In Proceedings of the 22nd ACM SIGSAC conference on computer and communications security, pages 1322-1333, 2015.</p>
<p>Karan Ganju, Qi Wang, Wei Yang, Carl A Gunter, and Nikita Borisov. Property inference attacks on fully connected neural networks using permutation invariant representations. In Proceedings of the 2018 ACM SIGSAC conference on computer and communications security, pages 619-633, 2018.</p>
<p>Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The Pile: An 800GB dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020.</p>
<p>Peter Henderson, Koustuv Sinha, Nicolas Angelard-Gontier, Nan Rosemary Ke, Genevieve Fried, Ryan Lowe, and Joelle Pineau. Ethical challenges in data-driven dialogue systems. In Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society, pages 123-129, 2018.</p>
<p>Matthew Jagielski, Jonathan Ullman, and Alina Oprea. Auditing differentially private machine learning: How private is private SGD? arXiv preprint arXiv:2006.07709, 2020.</p>
<p>Bargav Jayaraman and David Evans. Evaluating differentially private machine learning in practice. In 28th ${$ USENIX $}$ Security Symposium ( ${$ USENIX $}$ Security 19), pages 1895-1912, 2019.</p>
<p>Nikhil Kandpal, Eric Wallace, and Colin Raffel. Deduplicating training data mitigates privacy risks in language models. arXiv preprint arXiv:2202.06539, 2022.</p>
<p>Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris CallisonBurch, and Nicholas Carlini. Deduplicating training data makes language models better. CoRR, abs/2107.06499, 2021. URL https://arxiv.org/abs/2107.06499.
R. Thomas McCoy, Paul Smolensky, Tal Linzen, Jianfeng Gao, and Asli Celikyilmaz. How much do language models copy from their training data? Evaluating linguistic novelty in text generation using RAVEN. CoRR, abs/2111.09509, 2021. URL https://arxiv.org/abs/2111.09509.</p>
<p>Milad Nasr, Shuang Song, Abhradeep Thakurta, Nicolas Papernot, and Nicholas Carlini. Adversary instantiation: Lower bounds for differentially private machine learning. arXiv preprint arXiv:2101.04535, 2021.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1-67, 2020. URL http://jmlr.org/papers/v21/20-074.html.</p>
<p>Swaroop Ramaswamy, Om Thakkar, Rajiv Mathews, Galen Andrew, H Brendan McMahan, and Françoise Beaufays. Training production language models without memorizing user data. arXiv preprint arXiv:2009.10031, 2020.</p>
<p>Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership inference attacks against machine learning models. In 2017 IEEE Symposium on Security and Privacy (SP), pages 3-18. IEEE, 2017.</p>
<p>Om Thakkar, Swaroop Ramaswamy, Rajiv Mathews, and Françoise Beaufays. Understanding unintended memorization in federated learning, 2020.</p>
<p>Aleena Thomas, David Ifeoluwa Adelani, Ali Davody, Aditya Mogadala, and Dietrich Klakow. Investigating the impact of pre-trained word embeddings on memorization in neural networks. In International Conference on Text, Speech, and Dialogue, pages 273-281. Springer, 2020.</p>
<p>Ben Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax, May 2021.</p>
<p>Samuel Yeom, Irene Giacomelli, Matt Fredrikson, and Somesh Jha. Privacy risk in machine learning: Analyzing the connection to overfitting. In 2018 IEEE 31st Computer Security Foundations Symposium (CSF), pages 268-282. IEEE, 2018.</p>
<p>Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. $I C L R, 2017$.</p>
<p>Chiyuan Zhang, Daphne Ippolito, Katherine Lee, Matthew Jagielski, Florian Tramèr, and Nicholas Carlini. Counterfactual memorization in neural language models. arXiv preprint arXiv:2112.12938, 2021.</p>
<p>Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.</p>
<p>Albert Ziegler. GitHub Copilot: Parrot or crow? https://docs.github.com/en/github/copilot/researchrecitation, 2021.</p>
<h1>A IMPLEMENTATION DETAILS FOR DATASET CREATION</h1>
<p>Intuitively speaking, it is straightforward to construct a dataset containing specifiable proportions of documents at various frequencies. We need only enumerate all sequences repeated various numbers of times, and then sample uniformly at random from each of these subsets. However in practice this is difficult to do, given the scale of these datasets: even asking the question "how many times is this sequence present in the training dataset" requires linear work for each query, and so repeating this thousands of times for an 800 GB dataset would be infeasible.</p>
<p>To do this efficiently, we build on the work of Lee et al. (2021) and construct a suffix array over the training dataset. Such a data structure allows efficient queries to enumerate all sequences of length $k$ that are repeated between $N$ and $M$ times for any $N, M$. This can be accomplished by a linear scan of the suffix array. As notation, write $i$ as the pointer into the dataset at a certain position $j$ of the suffix array (i.e., $A[j]=i$ ), $i^{\prime}$ as the index at position $j+N$ (so that $A[j+N]=i^{\prime}$ ), and $i^{\prime \prime}$ as the index at position $j+M$ (so that $A[j+M]=i^{\prime \prime}$. Then, if $D[i: i+k]=D\left[i^{\prime}: i^{\prime \prime}+k\right]$ but $D[i: i+k] \neq D\left[i^{\prime \prime}: i^{\prime \prime}+k\right]$, the sequence $D[k: i+k]$ is guaranteed to appear between $N$ and $M$ times in the dataset. As a result, we can scan linearly through the suffix array and enumerate all values of $\mathrm{j} j$ to efficiently find all potential sequences repeated between N and M times. From here, we then randomly sample 1,000 indices within these buckets to construct all of our sequences.</p>
<h2>B LONGER DOCUMENTS ARE NOT EASIER TO MEMORIZE THAN SHORTER DOCUMENTS</h2>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Longer sequences are not easier to extract. We compute the probability that an adversary can extract a sequence as a function of the number of tokens of context available, when varying the length of the sequences. All sequences are repeated the same number of times, and evaluated with the same 6B parameter model. Each line represents the fraction extractable in sequences of increasing lengths. Because all lines nearly perfectly overlap, longer sequences are not fundamentally "easier" to extract than shorter sequences.</p>
<p>Intuitively, one might think that longer sequences are more likely in the tail of the distribution, and if the model is trained to a low perplexity, then the tail of the distribution may be more likely to be memorized. This could lead our context length results to be exaggerated (as it would be difficult to untangle the tail effect of memorization from the context length effect). To check if sequence length plays a role in the amount of memorization we can extract with this method, we generated the next 50 tokens after the prompt for various sequence lengths and various prompt lengths. Figure 5 shows the fraction of extractable tokens in the next 50 tokens after the prompt. Each line on the figure represents a set of sequences with sequence lengths between 100 and 500 tokens. For each sequence length, we looked at prompt lengths from 50 tokens to ${$ sequence length -50$}$ tokens. We do not see significant differences between the fraction of extractable tokens with varying prompt lengths across various sequence lengths.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Text examples that are memorized by the 6B model, but not by smaller models. Text highlighted in green matches the ground truth continuation, while text in red indicates incorrect (novel) generation.</p>
<h1>C Alternate Experimental Settings</h1>
<p>In this section, we study other strategies that we could have used to quantify memorization.
Random dataset sampling. In Section 4.4, we explored what would happen if we instead chose a truly random subset of the training data, where each sequence is sampled uniformly. Specifically, we randomly sample 100,000 sequences from The Pile dataset of length 100, 200, 500, and 1,000; prompt the model with the first $N-50$ tokens; and then test for memorization by verifying if the model can emit the remaining 50 tokens perfectly. In our analysis in Figures 2a and 2b, we vary the size of the trained model and the context length we provide it to understand how these factors impact memorization-but this time through prompting the models with randomly sampled training sequences. As expected, the absolute probability of memorization is much lower than in Figure 1 where we prompted models with training data from the sampled duplication-normalized subset.</p>
<p>We observe similar trends with model scale and context length as in our other results. Larger models memorize more training examples than smaller models-and much more than the baseline GPT-2 model that was not trained on The Pile. Similarly, providing more context to a model increases the likelihood we can discover memorization. In Figure 2b, we prompt models with: prompt length $=$ sequence length -50 . We see that the longer prompts are easier to predict correctly than shorter prompts. The baseline GPT-2 model is nearly twice as accurate on sequences of length 1,000 (prompt length $=950$ ) compared to sequences of length 100 (prompt length $=50$ ).</p>
<p>Alternate definition of extractability. Our main experiments report a sequence as "extractable" if the model's generated continuation is identical to the true suffix within that training example. This method is a loose lower bound on memorization. Consider two sequences $x_{1}, x_{2}$ both contained in the training dataset. Suppose these two sequences share the same prefix, and differ only in the final suffix; that is, $x_{1}=[p | s_{1}]$ and $x_{2}=[p | s_{2}]$. When we select $x_{1}$ and prompt the model on the prefix $p$, we will report "success" only if the output equals $s_{1}$, but not if the output is $s_{2}$, even though this is also a form of memorization.</p>
<p>We now consider how our results would change if we instead checked that the generation $[p | f(p)]$ from a prompt $p$ was contained anywhere in the training dataset. This gives a strictly larger measurement of memorization. By comparing these two methods (checking for memorization within the ground truth continuation, and within the entire dataset), we can understand how the choice of measurement affects the results in our experiments.</p>
<p>Searching within the entire dataset finds more memorized content than comparing with the ground truth (Figure 2c). For examples at 100 repetitions $32.6 \%$ of outputs are contained somewhere in the dataset but just $15.8 \%$ match the ground truth continuation. This difference becomes more pronounced as the number of repetitions increases. The maximum difference between these approaches is 28.4\%, at 2,200 repetitions.</p>
<p>We refrain from using this approach for our main experiments, because this definition requires vastly larger computation resources; it requires querying whether hundreds of thousands of sequences are contained in an 800GB training dataset. Therefore, to promote reproducability, the remainder of this paper continues with testing the generated suffix against the single expected training suffix.</p>
<h1>D Text Memorized by Only Some Models</h1>
<p>Table 1: The number of sequences memorized by one model, and not memorized by another.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: right;"></th>
<th style="text-align: right;">Not Memorized By</th>
<th style="text-align: right;"></th>
<th style="text-align: right;"></th>
<th style="text-align: right;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Model</td>
<td style="text-align: right;">Memorized</td>
<td style="text-align: right;">125 M</td>
<td style="text-align: right;">1.3 B</td>
<td style="text-align: right;">2.7 B</td>
<td style="text-align: right;">6 B</td>
</tr>
<tr>
<td style="text-align: left;">125 M</td>
<td style="text-align: right;">4,812</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">328</td>
<td style="text-align: right;">295</td>
<td style="text-align: right;">293</td>
</tr>
<tr>
<td style="text-align: left;">1.3 B</td>
<td style="text-align: right;">10,391</td>
<td style="text-align: right;">5,907</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">1,205</td>
<td style="text-align: right;">1,001</td>
</tr>
<tr>
<td style="text-align: left;">2.7 B</td>
<td style="text-align: right;">12,148</td>
<td style="text-align: right;">7,631</td>
<td style="text-align: right;">2,962</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">1,426</td>
</tr>
<tr>
<td style="text-align: left;">6 B</td>
<td style="text-align: right;">14,792</td>
<td style="text-align: right;">10,273</td>
<td style="text-align: right;">5,402</td>
<td style="text-align: right;">4,070</td>
<td style="text-align: right;"></td>
</tr>
</tbody>
</table>
<p>Table 1 shows the total number of sequences that are memorized by one model but not another. Larger models have more uniquely memorized sequences, although every model has some memorization not shared by any other model. (Even the 125 M model memorizes a few sequences that the 6 B model does not.)</p>
<h2>E Memorization in OPT Models</h2>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: We prompt OPT models with data sampled from their training set. We use a prompt length of 100 here. (a) Fraction of sequences extracted as a function of model scale. (b) Fraction of sequences extracted as the number of repetitions of that sequence in the training set increases.</p>
<h2>F Examples of Memorized Texts</h2>
<p>We show examples of texts that are memorized by different models. We consider the case of 50 -token prompts and 50 -token generation. We sample texts with various number of repetitions in the training data. It is impossible to inspect all the generated examples, so we random sample examples satisfying a certain criterion and show a few interesting ones in the paper. Figure 8 lists examples that are memorized by models of all sizes, in the sense that the 50 -token generations match the groundtruth continuations of the prompts.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Prompt</th>
<th style="text-align: center;">Continuation ( $=6 B=2.7 B=1.3 B=125 M$ )</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">use this file except in compliance with the License. * You may obtain a copy of the License at * http:// www.apache.org/licenses/LEENSE-2.0 * Unless required by applicable law or agreed</td>
<td style="text-align: center;">to in writing, software * distributed under the License is distributed on an "AS IS" BASIS, * WITHOUT WARRANTIES OF CONDITIONS OF ANY KIND, either express or implied. * See the License for the specific language</td>
</tr>
<tr>
<td style="text-align: center;">* * This program is free software; you can redistribute it and/or modify * it under the terms of the GNU General Public License version 2 and * only version 2 as published by the Free Software Foundation. * *</td>
<td style="text-align: center;">This program is distributed in the hope that it will be useful, * but WITHOUT ANY WARRANTY, without even the implied warranty of * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the *</td>
</tr>
<tr>
<td style="text-align: center;">Privacy \&amp; Cookies Policy Privacy Overview This website uses cookies to improve your experience while you navigate through the website. Out of these cookies, the cookies that are categorized as necessary are stored on your browser as they are essential for the working</td>
<td style="text-align: center;">of basic functionalities of the website. We also use thirdparty cookies that help us analyze and understand how you use this website. These cookies will be stored in your browser only with your consent. You also have the option to opt-out of</td>
</tr>
<tr>
<td style="text-align: center;">\end{document[5 in front of \$\documentclass[12pt] {minimal}\usepackage[amsmath] }</td>
<td style="text-align: center;">\usepackage[wasysym]\usepackage[amsfonts]</td>
</tr>
<tr>
<td style="text-align: center;">Len int for shift $&gt;$ uint8); ; shift $=&gt;7$ [if shift $=&gt;64$ ] return ErrIntOverflowRaft</td>
<td style="text-align: center;">if iNdEx $=&gt;1$ [ return in.ErrUnexpectedEOF] b $=&gt;$ dAtA</td>
</tr>
<tr>
<td style="text-align: center;"></object> &lt;oil key="sourceID"</td>
<td style="text-align: center;">&gt;<int key="maxID">10</int> </object> &lt;object class="IBClassDescriber" key="</td>
</tr>
</tbody>
</table>
<p>Figure 8: Text examples that are memorized by all the models: given 50-token prompts on the left, the next 50 tokens generated by all the models match the groundtruth continuation.</p>
<p>Figure 9 lists examples that are memorized by the 6B model but not by smaller ones. Specifically, the 50 -token generations of the 6 B model match the groundtruth continuations exactly, but the generations from the smaller models match neither the groundtruth continuations of the prompted examples nor any other training examples with the same prompts. We find that when smaller models do not get the groundtruth continuation right, they are generally still able to stick to similar topics. However, in many cases, the texts generated by the smaller models are only syntactically sound, but semantically incorrect. Figure 10 and Figure 11 show more examples.</p>
<p>In Figure 12 we show examples that are only memorized by the smallest model, using similar criterion as when we filter examples that are only memorized by the largest model. There are significantly fewer number of examples that are only memorized by the smallest model (35) than that of the largest model (2860). One of those examples (the first row of Figure 12) is particularly interesting: the groundtruth continuation contains a typo due to formatting cutoff. While the smallest model memorized the typo, larger models try to fix the typo.</p>
<p>In Figure 13 and Figure 14 we show examples that are memorized but not heavily duplicated in the training set, and examples that are heavily duplicated but not memorized, respectively. Finally, we show examples that are memorized by GPT2-XL in Figure 15.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 9: Text examples that are memorized by the 6B model (according to true-continuation match), but not memorized by smaller models (the generated texts do not match the true continuation, nor any other training examples). The first column shows the prompt. The second column shows the prediction from the 6B model, which matches the groundtruth continuation exactly. The remaining columns shows predictions from smaller models.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Prompt</th>
<th style="text-align: center;">Continuation ( $=6 B$ )</th>
<th style="text-align: center;">2.7B</th>
<th style="text-align: center;">1.3B</th>
<th style="text-align: center;">125M</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">disabled. BCL.com will work better for you if you enable JavaScript or switch to a JavaScript supported browser. Restore Consulting Group is an Equal Opportunity Employer, All qualified applicants will receive consideration for employment without regard to race, color,</td>
<td style="text-align: center;">age, religion, sex, sexual orientation, gender identity / expression, national origin, protected veteran status, or any other characteristic protected under federal, state or local law, where applicable, and those with criminal histories will be considered in a manner consistent with</td>
<td style="text-align: center;">age, religion, sex, sexual orientation, gender identity/ expression, national origin, protected veteran status, or any other characteristic protected under federal, state or local law, where applicable. This website uses cookies to improve your experience. We'll</td>
<td style="text-align: center;">age, religion, sex, sexual orientation, gender identity/ expression, national origin, protected veteran status, or any other characteristic protected - See. BCL.com cookies to improve the functionality, performance, and effectiveness of our communications. Detailed</td>
<td style="text-align: center;">religion, sex, national origin, gender identity, sexual orientation, gender identity expression, protected veteran status, or any other characteristic protected - See. BCL.com cookies to improve the functionality, performance, and effectiveness of our communications. Detailed</td>
</tr>
</tbody>
</table>
<p>Figure 10: Continuation of Figure 9.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Prompt</th>
<th style="text-align: center;">Continuation ( $=68$ )</th>
<th style="text-align: center;">2.7B</th>
<th style="text-align: center;">1.3B</th>
<th style="text-align: center;">125M</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">_bPLS crypto_unregister_alg); int crypto_register_template\&amp;struct crypto_template<em>#ep21; struct crypto_template </em>\%; int err = #10455;</td>
<td style="text-align: center;">down_write\&amp;crypto_alg_same; list_for_each_entry\&g; \&amp;crypto_template [list_10455 if (a, $=$ bnp)]</td>
<td style="text-align: center;">list_for_each_entry\&g; \&amp;crypto_+_list_10455 if (bns#-mame\&amp;&amp; bnp#-map\&amp;a $=a=a g \cdot c o \cdot a \cdot a m)$</td>
<td style="text-align: center;">$a=a g$ \&amp;c02amd1*g); $\mathrm{dr} f \mathrm{f} \mathrm{pr} \mathrm{d} \mathrm{ir} \mathrm{kt} \mathrm{~g} \mathrm{~d} \mathrm{~g} \mathrm{~d} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: center;">Frequency</th>
<th style="text-align: center;">Prompt</th>
<th style="text-align: center;">Continuation ( $==68$ )</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">L_LONG_LONG */__STL_TEMPLATE_NULL struct __type_traits=float</td>
<td style="text-align: center;">constructor; typedef _true_type has_trivial_copy_constructor; typedef __true_type has_trivial_assignment_operator;</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">"groupby4_map", "groupby4_map_skew", <br> "groupby4_noskew", "groupby5",</td>
<td style="text-align: center;">"groupby5_map", "groupby5_map_skew", "groupby5_noskew", <br> "groupby6",</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">pair=K12=[_12], fusion::make_pair=K13=[_13], <br> fusion::make_pair=K14=[_14], fusion::make_pair=K15=[_15]);</td>
<td style="text-align: center;">I namespace result_of [ template = typename KO, typename K1, typename K]</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">_GLSL_400]) list == QLatin1String("dmat2"); if (variant \&amp; [Lexer::Variant_GLSL_400]) list == QLatin</td>
<td style="text-align: center;">1String("dmat3"); if (variant \&amp; [Lexer::Variant_GLSL_400]) list == <br> QLatin1String("dmat4"); if (variant \&amp; [</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">disabled. BCG.com will work better for you if you enable JavaScript or switch to a JavaScript supported browser. Boston Consulting Group is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color,</td>
<td style="text-align: center;">age, religion, sex, sexual orientation, gender identity / expression, national origin, protected veteran status, or any other characteristic protected under federal, state or local law, where applicable, and those with criminal histories will be considered in a manner consistent with</td>
</tr>
</tbody>
</table>
<p>Figure 13: Text examples that are memorized but are not heavily duplicated in the training set. Many of these have a simple sequential structure (the middle three), may be boilerplate code (the first), or starts out with unique text, and completes with frequently repeated text (the last example). Overall, these are easily completed sequences.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Frequency</th>
<th style="text-align: center;">Prompt</th>
<th style="text-align: center;">Continuation</th>
<th style="text-align: center;">68</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">5526</td>
<td style="text-align: center;">contributors. All rights reserved. // // Licensed under the Apache License, Version 2.0 (the "License"); // you may not use this file except in compliance with the License. // You may obtain a copy of the</td>
<td style="text-align: center;">License at // http://www.apache.org/licenses/ licensee/LICENSE-2.0 // // Unless required by applicable law or agreed to in writing, software // distributed under the License is distributed on an "AS</td>
<td style="text-align: center;">License at // // http://www.apache.org/licenses/ <br> LICENSE-2.0 // // Unless required by applicable law or agreed to in writing, software // distributed under the License</td>
</tr>
<tr>
<td style="text-align: center;">5652</td>
<td style="text-align: center;">alors are Standing By Our Dumpster Specialists are waiting by the phone to help you get started. However, there is some important information we need before we can give you a price quote and delivery time. Some things we need to know</td>
<td style="text-align: center;">What size dumpster you will need What you plan on putting in your dumpster When you need us to drop your roll off container off How long you plan on keeping your rolloff City \&amp;</td>
<td style="text-align: center;">Include: What size dumpster -is you need? What type of waste do you - is - how much waste do you have? What is the delivery time? What's the drop/time?What</td>
</tr>
<tr>
<td style="text-align: center;">4323</td>
<td style="text-align: center;">a copy of the License at # # http:// www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law</td>
<td style="text-align: center;">or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See</td>
<td style="text-align: center;">or agreed to in writing, software # distributed under the License is distributed on an "AS IS" <br> BASIS, + WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</td>
</tr>
<tr>
<td style="text-align: center;">3556</td>
<td style="text-align: center;">date_default_timezone_set() function. In case you used any of those methods and you are still getting this warning, you most likely misspelled the timezone identifier. We selected the timezone 'UTC' for now, but please</td>
<td style="text-align: center;">set date.timezone to select your timezone. in /home/erlypro/public_html/matural/datas/ persobanner_center.php on line 17 <br> Deprecated: Function eregi_replace</td>
<td style="text-align: center;">set date.timezone to select your timezone. in /home/ erlyp /public_html/wp-d炎/2017/07/07/07.php on line 2017/07/07/07.php on line 2017/07/07/07.php on line 2017/07/07/07.07.07 is not safe to say.</td>
</tr>
<tr>
<td style="text-align: center;">3920</td>
<td style="text-align: center;">1\&nbsp;\&nbsp;【手把手翻墙教程】https:// github.com/gfw-breaker/gsides/wiki/ \&nbsp;</td>
<td style="text-align: center;">nbsp;l\&nbsp;\&nbsp;【禁闹聚合安卓版】 (https://github.com/gfw-breaker/bn-android) \&nbsp;\&amp;n</td>
<td style="text-align: center;">nbsp;[/4]「安卓版】https://github.com/gfw- breaker/wdbpack-interpreter/ \&nbsp;\&nbsp;【移向】2.2.2</td>
</tr>
<tr>
<td style="text-align: center;">3675</td>
<td style="text-align: center;">* / _|</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">3030</td>
<td style="text-align: center;">1775, "date": "1775\u20131851", "displayOrder": 1, "fc": "Joseph Mallord William</td>
<td style="text-align: center;">Turner", "gender": "Male", "id": 558, "mda", "Turner, Joseph Mallord William",</td>
<td style="text-align: center;">Turner", "gender": "Male", "id": 1010, "mda", "Turner, Joseph Mallord William",</td>
</tr>
</tbody>
</table>
<p>Figure 14: Text examples that are heavily replicated in the training set but not memorized. We find many examples which have slight differences with no semantic (English) meaning. This includes comment characters in code, non-English characters, template values, error messages, and meaningless symbols. We also surprisingly find a large number of slightly different but heavily repeated documents about dumpsters.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Prompt</th>
<th style="text-align: center;">Continuation (== GPT2-XL)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">165.00 \$175.00 \$180.00 \$185.00 \$190.00 \$195.00 \$200.00 \$210.00 \$220.00 \$230.00 \$240.00 \$250.00 \$260.</td>
<td style="text-align: center;">00 \$270.00 \$280.00 \$290.00 \$300.00 \$310.00 \$320.00 \$330.00 \$340.00 \$350.00 \$360.00 \$370.00 \$380.00 \$</td>
</tr>
<tr>
<td style="text-align: center;">ISRC3DEC2, ARIZONA_ISRC3DEC2MIX_INPUT_1_SOURCE); ARIZONA_MUX_ENUMS(ISRC3DEC3, ARIZONA_</td>
<td style="text-align: center;">ISRC3DEC3MIX_INPUT_1_SOURCE); ARIZONA_MUX_ENUMS(ISRC3DEC4, ARIZONA_ISRC3DEC4MIX_INPUT_</td>
</tr>
<tr>
<td style="text-align: center;">the councils of government, we must guard against the acquisition of unwarranted influence, whether sought or unsought, by the militaryindustrial complex. The potential for the disastrous rise of misplaced power exists and will persist. We must never let the weight</td>
<td style="text-align: center;">of this combination endanger our liberties or democratic processes. We should take nothing for granted. Only an alert and knowledgeable citizenry can compel the proper meshing of the huge industrial and military machinery of defense with our peaceful methods and goals, so that security and</td>
</tr>
<tr>
<td style="text-align: center;">full spreadsheet NEW! Buy our book $\cdot$ Facts are Sacred: the power of data (on Kindle) More open data Data journalism and data visualisations from the Guardian World government data $\cdot$ Search the</td>
<td style="text-align: center;">world's government data with our gateway Development and aid data $\cdot$ Search the world's global development data with our gateway Can you do something with this data? $\cdot$ Flickr Please post your visualisations and mash-ups on</td>
</tr>
<tr>
<td style="text-align: center;">Original press release Get ahead of the crowd by signing up for 420 Investor, the largest \&amp; most comprehensive premium subscription service for cannabis traders and investors since 2013. Published by NCV Newswire The NCV Newswire</td>
<td style="text-align: center;">by New Cannabis Ventures aims to curate high quality content and information about leading cannabis companies to help our readers filter out the noise and to stay on top of the most important cannabis business news. The NCV Newswire is hand-curated by</td>
</tr>
<tr>
<td style="text-align: center;">of sole responsibility of the author(s). The Centre for Research on Globalization will not be responsible for any inaccurate or incorrect statement in this article. The Centre of Research on Globalization grants permission to crosspost Global Research articles on community internet sites</td>
<td style="text-align: center;">as long the source and copyright are acknowledged together with a hyperlink to the original Global Research article. For publication of Global Research articles in print or other forms including commercial internet sites, contact: [email protected] www.globalresearch.ca</td>
</tr>
</tbody>
</table>
<p>Figure 15: Text examples that are from The Pile and memorized by GPT2-XL. The first two examples have a natural sequential structure, while the others appear to represent an overlap in GPT2-XL's training set and The Pile.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ We were unable to access the 175 billion parameter model; we run OPT models up to 66 billion parameters.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
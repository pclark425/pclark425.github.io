<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1175 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1175</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1175</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-26.html">extraction-schema-26</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated systems making scientific discoveries, including how these discoveries are characterized as incremental or transformational, and how they are evaluated and validated.</div>
                <p><strong>Paper ID:</strong> paper-211010946</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2002.00049v3.pdf" target="_blank">Studying Lagrangian theories with machine learning: a toy model</a></p>
                <p><strong>Paper Abstract:</strong> The existence or not of pathologies in the context of Lagrangian theory is studied with the aid of Machine Learning algorithms. Using an example in the framework of classical mechanics, we make a proof of concept, that the construction of new physical theories using machine learning is possible. Specifically, we utilize a fully-connected, feed-forward neural network architecture, aiming to discriminate between ``healthy'' and ``non-healthy'' Lagrangians, without explicitly extracting the relevant equations of motion. The network, after training, is used as a fitness function in the concept of a genetic algorithm and new healthy Lagrangians are constructed. These new Lagrangians are different from the Lagrangians contained in the initial data set. Hence, searching for Lagrangians possessing a number of pre-defined properties is significantly simplified within our approach. The framework employed in this work can be used to explore more complex physical theories, such as generalizations of General Relativity in gravitational physics, or constructions in solid state physics, in which the standard procedure can be laborious.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1175.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1175.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated systems making scientific discoveries, including how these discoveries are characterized as incremental or transformational, and how they are evaluated and validated.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NN+GA Lagrangian Discovery</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fully-connected Feed-forward Neural Network used as Fitness Function with Genetic Algorithm for Automated Construction of Healthy Lagrangians</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proof-of-concept pipeline that (1) trains a feed-forward neural network to classify Lagrangians as 'healthy' (leading to ≤ second-order equations of motion) or 'non-healthy', and (2) uses the trained network as a fitness function inside a genetic algorithm to generate new Lagrangians labeled as healthy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>NN+GA Lagrangian Discovery Pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A two-part automated system: (A) a supervised fully-connected feed-forward neural network classifier (sigmoid activations, backpropagation training) that maps numeric feature vectors representing parametrized Lagrangians to a continuous fitness score in [0,1] indicating 'healthiness'; (B) a genetic algorithm that treats the trained network output as a fitness function to evolve populations of feature vectors (Lagrangians) via weighted averaging mating and random mutations to produce new candidate Lagrangians. Implementational details: tested architectures include 18-16-16-1 and 8-6-6-1 (two hidden layers of 16 neurons was selected as promising), sigmoid activation, learning rates η tested including 0.1 and 1, training via mini-batch gradient descent/backpropagation. Training labels were generated by symbolic computation (sympy) checking Euler-Lagrange equations for derivatives higher than second-order.</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_domain</strong></td>
                            <td>Theoretical physics (classical mechanics / Lagrangian theory); methodology applicable to gravitational theories and other areas of physics</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_description</strong></td>
                            <td>The system was used to (i) classify 1D classical-mechanics Lagrangians (parametrized feature vectors) as 'healthy' or 'non-healthy' without explicitly deriving equations of motion, and (ii) automatically generate new Lagrangians via a genetic algorithm whose fitness was the trained network output. The generated Lagrangians were different from the initial Monte Carlo population and included feature patterns (e.g., exponents of the highest derivative ẍ equal to 0 or 1) consistent with healthiness; the authors present a set of Lagrangians selected from final GA populations after multiple runs and generations.</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type</strong></td>
                            <td>not specified</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type_justification</strong></td>
                            <td>The paper frames the work as a 'proof of concept' and does not label the automated discoveries using terms such as incremental, transformational, or paradigm-shifting. The authors explicitly note the gains are modest for the simple 1D test case (a human could construct similar Lagrangians), while arguing potential utility for more complex domains (e.g., higher-dimensional gravitational theories) where manual derivation is laborious. No formal criteria for categorizing discoveries as incremental vs transformational are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Evaluation of the classifier used standard supervised-learning metrics computed on held-out test sets: Accuracy, Precision, Recall, F1 (harmonic mean of precision and recall), and Logarithmic Loss (LogLoss). Training/validation procedure: Monte Carlo generation of labeled dataset via sympy (2000 'healthy' + 2000 'non-healthy' training examples), mini-batch backpropagation (batches of 400), multiple epochs, shuffling, and investigation of different learning rates. Validation included feeding 10 independent datasets each of 10,000 feature vectors to the trained network and reporting the above metrics per epoch. Example quantitative results reported: for one run architecture (8-6-6-1, η=0.1) accuracy progressed from 0.549 at epoch 500 to 1.000 at epoch 20000 (LogLoss decreasing to 0.026); for a selected architecture (18-16-16-1, η=1) stable validation metrics around accuracy ≈ 0.975 and LogLoss ≈ 0.076 after 5000 epochs. For the GA-generated outputs, evaluation consisted of the network-assigned fitness values and selection of top individuals per run/generation.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approaches</strong></td>
                            <td>Primary validation was comparison to the standard analytic procedure: labels for training/validation were produced by symbolic algebra (sympy) deriving Euler-Lagrange equations and checking for time derivatives of order >2. The trained network's classifications were validated against these ground-truth labels on large unseen datasets (10 datasets × 10,000 samples). The paper states 'Validation of the trained network using the explicit standard procedure proved that for the simple model ... the efficient discrimination ... is established.' For GA-generated Lagrangians: the authors repeated GA runs (10 different initial populations, each run produced a most-fitted Lagrangian after many generations — e.g., 200 generations) and examined the produced Lagrangians; although the GA generation used the network as fitness (so initial validation for those candidates was the network output), the paper indicates comparison to analytic criteria was used to verify 'healthy' property for selected examples (i.e., checking whether resulting equations of motion are second-order by the standard variation), but detailed per-example symbolic checks are not exhaustively tabulated in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment</strong></td>
                            <td>Novelty is assessed primarily by (a) the GAN/GA-produced Lagrangians being different from members of the initial Monte Carlo population and (b) the system discovering correct structural features (e.g., exponents on higher derivatives) without being explicitly told. The authors qualify novelty: for the simple one-dimensional class studied, humans can readily write equivalent Lagrangians (so the novelty is limited), but the approach is novel as an automated workflow that could scale to complex functional spaces (e.g., gravitational Lagrangians) where manual derivation and verification are difficult. No formal novelty scoring metric is used beyond demonstrating that produced individuals were not copies of initial population members and observing that the GA found the correct exponent pattern for healthiness.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_metrics</strong></td>
                            <td>Primary quantitative impact / performance metrics reported are classification metrics: Accuracy, Precision, Recall, F1, and LogLoss. Representative numerical values from the paper: (a) architecture 8-6-6-1, η=0.1: Accuracy increased to 1.000 by epoch 20000 with LogLoss 0.026; (b) architecture 18-16-16-1, η=1: Accuracy ≈ 0.975, Precision ≈ 0.988, Recall ≈ 0.975, F1 ≈ 0.975, LogLoss ≈ 0.076 (reported at 5000 epochs). Training dataset sizes: 4000 examples (2000 healthy + 2000 non-healthy); validation sets: 10×10,000 examples. GA configurations: multiple initial populations, 200 generations reported for one set of results; the GA weight mapping used was wi = 10^5 * Nfit(Pi) (where Nfit is the network output). No additional bibliometric or downstream scientific-impact metrics (e.g., citation counts or experimental confirmations) are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_discoveries</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_details</strong></td>
                            <td>The paper explicitly comments on how the automated outputs compare to human derivations: for the simple one-dimensional Lagrangian subclass used in experiments, a human can 'relatively easily' write down Lagrangians that produce only second-order equations, so the automated gain is not significant there. However, for more complex domains (e.g., 4D gravitational Lagrangians built from Riemann tensor contractions), the authors argue humans cannot a priori know whether a Lagrangian will lead to >2 derivative terms without explicit derivation and lengthy algebra, making the automated approach potentially much more helpful. No quantitative head-to-head success-rate comparison (e.g., time-to-discovery or accuracy vs human experts) is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Classifier performance as reported: architectures achieved validation accuracies ranging from ~0.685 at early epochs up to 1.000 in one run (noting possible overfitting risks) and stable ≈0.975 accuracy for the selected architecture. Example table entries: for 18-16-16-1 at later epochs, Accuracy ≈ 0.975, Precision ≈ 0.988, Recall ≈ 0.975, F1 ≈ 0.975, LogLoss ≈ 0.076. GA produced sets of candidate 'healthy' Lagrangians across 10 repeated runs (the procedure was repeated 10 times and one most-fitted Lagrangian per run was selected), and the GA was reported to find the correct structural feature (exponent of ẍ equal to 0 or 1) without that rule being imposed. The paper does not provide a formal percentage success rate for GA discovery beyond these reported instances (but implies multiple successful runs).</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Identified challenges include: (1) interpretability / transparency: the network is a black box and the mapping from features to 'healthiness' is not analytically extracted; (2) dependence on the chosen parametrization/feature map φ from Lagrangian space to R^n — the results depend on how Lagrangians are encoded; (3) dependence on the initial Monte Carlo population and GA operators (mating, mutation) — generated outputs reflect these choices; (4) for simple testbeds novelty/utility is limited since humans can create equivalent Lagrangians easily; (5) in complex theories even analytic verification can require non-trivial manipulations (integration by parts) to reveal cancellations of higher-derivative terms, complicating ground-truth labeling and validation; (6) the paper does not provide formal criteria to distinguish incremental versus transformational discoveries, nor a framework to quantify scientific significance beyond classifier metrics; (7) risk of overfitting (some runs reached perfect accuracy on validation sets, which may reflect overfitting or trivialization by the chosen parametrization).</td>
                        </tr>
                        <tr>
                            <td><strong>has_incremental_transformational_comparison</strong></td>
                            <td>False</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>The automation of science <em>(Rating: 2)</em></li>
                <li>Generative Adversarial Networks <em>(Rating: 1)</em></li>
                <li>Genetic Algorithms <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1175",
    "paper_id": "paper-211010946",
    "extraction_schema_id": "extraction-schema-26",
    "extracted_data": [
        {
            "name_short": "NN+GA Lagrangian Discovery",
            "name_full": "Fully-connected Feed-forward Neural Network used as Fitness Function with Genetic Algorithm for Automated Construction of Healthy Lagrangians",
            "brief_description": "A proof-of-concept pipeline that (1) trains a feed-forward neural network to classify Lagrangians as 'healthy' (leading to ≤ second-order equations of motion) or 'non-healthy', and (2) uses the trained network as a fitness function inside a genetic algorithm to generate new Lagrangians labeled as healthy.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "NN+GA Lagrangian Discovery Pipeline",
            "system_description": "A two-part automated system: (A) a supervised fully-connected feed-forward neural network classifier (sigmoid activations, backpropagation training) that maps numeric feature vectors representing parametrized Lagrangians to a continuous fitness score in [0,1] indicating 'healthiness'; (B) a genetic algorithm that treats the trained network output as a fitness function to evolve populations of feature vectors (Lagrangians) via weighted averaging mating and random mutations to produce new candidate Lagrangians. Implementational details: tested architectures include 18-16-16-1 and 8-6-6-1 (two hidden layers of 16 neurons was selected as promising), sigmoid activation, learning rates η tested including 0.1 and 1, training via mini-batch gradient descent/backpropagation. Training labels were generated by symbolic computation (sympy) checking Euler-Lagrange equations for derivatives higher than second-order.",
            "discovery_domain": "Theoretical physics (classical mechanics / Lagrangian theory); methodology applicable to gravitational theories and other areas of physics",
            "discovery_description": "The system was used to (i) classify 1D classical-mechanics Lagrangians (parametrized feature vectors) as 'healthy' or 'non-healthy' without explicitly deriving equations of motion, and (ii) automatically generate new Lagrangians via a genetic algorithm whose fitness was the trained network output. The generated Lagrangians were different from the initial Monte Carlo population and included feature patterns (e.g., exponents of the highest derivative ẍ equal to 0 or 1) consistent with healthiness; the authors present a set of Lagrangians selected from final GA populations after multiple runs and generations.",
            "discovery_type": "not specified",
            "discovery_type_justification": "The paper frames the work as a 'proof of concept' and does not label the automated discoveries using terms such as incremental, transformational, or paradigm-shifting. The authors explicitly note the gains are modest for the simple 1D test case (a human could construct similar Lagrangians), while arguing potential utility for more complex domains (e.g., higher-dimensional gravitational theories) where manual derivation is laborious. No formal criteria for categorizing discoveries as incremental vs transformational are provided.",
            "evaluation_methods": "Evaluation of the classifier used standard supervised-learning metrics computed on held-out test sets: Accuracy, Precision, Recall, F1 (harmonic mean of precision and recall), and Logarithmic Loss (LogLoss). Training/validation procedure: Monte Carlo generation of labeled dataset via sympy (2000 'healthy' + 2000 'non-healthy' training examples), mini-batch backpropagation (batches of 400), multiple epochs, shuffling, and investigation of different learning rates. Validation included feeding 10 independent datasets each of 10,000 feature vectors to the trained network and reporting the above metrics per epoch. Example quantitative results reported: for one run architecture (8-6-6-1, η=0.1) accuracy progressed from 0.549 at epoch 500 to 1.000 at epoch 20000 (LogLoss decreasing to 0.026); for a selected architecture (18-16-16-1, η=1) stable validation metrics around accuracy ≈ 0.975 and LogLoss ≈ 0.076 after 5000 epochs. For the GA-generated outputs, evaluation consisted of the network-assigned fitness values and selection of top individuals per run/generation.",
            "validation_approaches": "Primary validation was comparison to the standard analytic procedure: labels for training/validation were produced by symbolic algebra (sympy) deriving Euler-Lagrange equations and checking for time derivatives of order &gt;2. The trained network's classifications were validated against these ground-truth labels on large unseen datasets (10 datasets × 10,000 samples). The paper states 'Validation of the trained network using the explicit standard procedure proved that for the simple model ... the efficient discrimination ... is established.' For GA-generated Lagrangians: the authors repeated GA runs (10 different initial populations, each run produced a most-fitted Lagrangian after many generations — e.g., 200 generations) and examined the produced Lagrangians; although the GA generation used the network as fitness (so initial validation for those candidates was the network output), the paper indicates comparison to analytic criteria was used to verify 'healthy' property for selected examples (i.e., checking whether resulting equations of motion are second-order by the standard variation), but detailed per-example symbolic checks are not exhaustively tabulated in the paper.",
            "novelty_assessment": "Novelty is assessed primarily by (a) the GAN/GA-produced Lagrangians being different from members of the initial Monte Carlo population and (b) the system discovering correct structural features (e.g., exponents on higher derivatives) without being explicitly told. The authors qualify novelty: for the simple one-dimensional class studied, humans can readily write equivalent Lagrangians (so the novelty is limited), but the approach is novel as an automated workflow that could scale to complex functional spaces (e.g., gravitational Lagrangians) where manual derivation and verification are difficult. No formal novelty scoring metric is used beyond demonstrating that produced individuals were not copies of initial population members and observing that the GA found the correct exponent pattern for healthiness.",
            "impact_metrics": "Primary quantitative impact / performance metrics reported are classification metrics: Accuracy, Precision, Recall, F1, and LogLoss. Representative numerical values from the paper: (a) architecture 8-6-6-1, η=0.1: Accuracy increased to 1.000 by epoch 20000 with LogLoss 0.026; (b) architecture 18-16-16-1, η=1: Accuracy ≈ 0.975, Precision ≈ 0.988, Recall ≈ 0.975, F1 ≈ 0.975, LogLoss ≈ 0.076 (reported at 5000 epochs). Training dataset sizes: 4000 examples (2000 healthy + 2000 non-healthy); validation sets: 10×10,000 examples. GA configurations: multiple initial populations, 200 generations reported for one set of results; the GA weight mapping used was wi = 10^5 * Nfit(Pi) (where Nfit is the network output). No additional bibliometric or downstream scientific-impact metrics (e.g., citation counts or experimental confirmations) are reported.",
            "comparison_to_human_discoveries": true,
            "comparison_details": "The paper explicitly comments on how the automated outputs compare to human derivations: for the simple one-dimensional Lagrangian subclass used in experiments, a human can 'relatively easily' write down Lagrangians that produce only second-order equations, so the automated gain is not significant there. However, for more complex domains (e.g., 4D gravitational Lagrangians built from Riemann tensor contractions), the authors argue humans cannot a priori know whether a Lagrangian will lead to &gt;2 derivative terms without explicit derivation and lengthy algebra, making the automated approach potentially much more helpful. No quantitative head-to-head success-rate comparison (e.g., time-to-discovery or accuracy vs human experts) is provided.",
            "success_rate": "Classifier performance as reported: architectures achieved validation accuracies ranging from ~0.685 at early epochs up to 1.000 in one run (noting possible overfitting risks) and stable ≈0.975 accuracy for the selected architecture. Example table entries: for 18-16-16-1 at later epochs, Accuracy ≈ 0.975, Precision ≈ 0.988, Recall ≈ 0.975, F1 ≈ 0.975, LogLoss ≈ 0.076. GA produced sets of candidate 'healthy' Lagrangians across 10 repeated runs (the procedure was repeated 10 times and one most-fitted Lagrangian per run was selected), and the GA was reported to find the correct structural feature (exponent of ẍ equal to 0 or 1) without that rule being imposed. The paper does not provide a formal percentage success rate for GA discovery beyond these reported instances (but implies multiple successful runs).",
            "challenges_limitations": "Identified challenges include: (1) interpretability / transparency: the network is a black box and the mapping from features to 'healthiness' is not analytically extracted; (2) dependence on the chosen parametrization/feature map φ from Lagrangian space to R^n — the results depend on how Lagrangians are encoded; (3) dependence on the initial Monte Carlo population and GA operators (mating, mutation) — generated outputs reflect these choices; (4) for simple testbeds novelty/utility is limited since humans can create equivalent Lagrangians easily; (5) in complex theories even analytic verification can require non-trivial manipulations (integration by parts) to reveal cancellations of higher-derivative terms, complicating ground-truth labeling and validation; (6) the paper does not provide formal criteria to distinguish incremental versus transformational discoveries, nor a framework to quantify scientific significance beyond classifier metrics; (7) risk of overfitting (some runs reached perfect accuracy on validation sets, which may reflect overfitting or trivialization by the chosen parametrization).",
            "has_incremental_transformational_comparison": false,
            "uuid": "e1175.0"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "The automation of science",
            "rating": 2,
            "sanitized_title": "the_automation_of_science"
        },
        {
            "paper_title": "Generative Adversarial Networks",
            "rating": 1,
            "sanitized_title": "generative_adversarial_networks"
        },
        {
            "paper_title": "Genetic Algorithms",
            "rating": 1,
            "sanitized_title": "genetic_algorithms"
        }
    ],
    "cost": 0.012301999999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Building healthy Lagrangian theories with machine learning</p>
<p>Christos Valelis 
Department of Informatics &amp; Telecommunications
National &amp; Kapodistrian University of Athens
Zografou Campus GR 157 73AthensGreece</p>
<p>Fotios K Anagnostopoulos 
Department of Physics
National &amp; Kapodistrian University of Athens
Zografou Campus GR 157 73AthensGreece</p>
<p>Spyros Basilakos 
Research Center for Astronomy and Applied Mathematics
Academy of Athens
Soranou Efesiou 411527AthensGreece</p>
<p>National Observatory of Athens
Lofos Nymfon11852AthensGreece</p>
<p>Emmanuel N Saridakis 
National Observatory of Athens
Lofos Nymfon11852AthensGreece</p>
<p>Department of Physics
National Technical University of Athens
Zografou Campus GR 157 73AthensGreece</p>
<p>Department of Astronomy
School of Physical Sciences
University of Science and Technology of China
230026HefeiP.R. China</p>
<p>Building healthy Lagrangian theories with machine learning</p>
<p>The existence or not of pathologies in the context of Lagrangian theory is studied with the aid of Machine Learning algorithms. Using an example in the framework of classical mechanics, we make a proof of concept, that the construction of new physical theories using machine learning is possible. Specifically, we utilize a fully-connected, feed-forward neural network architecture, aiming to discriminate between "healthy" and "non-healthy" Lagrangians, without explicitly extracting the relevant equations of motion. The network, after training, is used as a fitness function in the concept of a genetic algorithm and new healthy Lagrangians are constructed. These new Lagrangians are different from the Lagrangians contained in the initial data set. Hence, searching for Lagrangians possessing a number of pre-defined properties is significantly simplified within our approach. The framework employed in this work can be used to explore more complex physical theories, such as generalizations of General Relativity in gravitational physics, or constructions in solid state physics, in which the standard procedure can be laborious.</p>
<p>I. INTRODUCTION</p>
<p>After a century of theoretical research in the construction of physical theories, we know that the action, and thus the corresponding Lagrangian density, is the cornerstone quantity, since it gives rise to the equations of motion by employing the least-action principle. During the last century rapid progress in the direction of a unified description of physics took place, and the combined description of electromagnetic, weak and strong nuclear forces within the standard model of particle physics Lagrangian is established. However, the inclusion of gravity in this picture is notoriously difficult, resulting to a number of different approaches (see [1,2] for a review and [3] for a wider perspective of the problem).</p>
<p>One interesting approach is to study higher-order corrections to the Einstein-Hilbert action of general relativity, which is a sufficient condition to construct renormalizable and thus potentially quantizable gravitational theories [4]. A serious drawback in this approach is that including higher derivatives in the Lagrangian is known to cause problems, such as the existence of unphysical states, the so-called ghosts. Thus, one is indeed interested in Lagrangians that posses terms with higher-derivatives, but which are ghost-free and have equations of motion that are of second order, which are generally called "healthy" Lagrangians. Searching for healthy Lagrangians is usually done by hand, or at most by using a computer algebra system, and is a tedious procedure which requires the explicit derivation of field equations and the examination of whether they are of second order.</p>
<p>The ubiquity of massive data volumes, coupled with scalable training techniques, novel learning algorithms and immense computational power, has been an important factor for the practical success of (deep) machine learning (ML). From a predictive accuracy standpoint, deep learning algorithms are nowadays considered the state of the art in numerous applications, while deep learning's success has been the primary reason for a fresh look at the transformative potential of artificial inteligence (AI) as a whole during the past decade. This success has also made a long-standing dream, that of automated, data-driven scientific discovery, seem within reach. From early approaches in biology and the life sciences, [5], to more recent ones in a wide range of scientific fields [6], AI-assisted scientific discovery is pursued towards understanding experimental findings, inferring causal relationships from observational data, or even acquiring novel scientific insights and building new explanatory theories. At the same time, however, and despite recent successes and optimism, a transparency barrier is imposing severe limitations on the applicability of the AI potential in scientific discovery. It is now widely acknowledged that the inability to understand, explain and trace back to a cause the predictions/inferences of black-box machine learning models is responsible for their lack of accountability and for end user's lack of trust in such models in mission-critical applications and high-stakes decisions. In fields where understanding, as opposed to merely predicting, is the basic requirement, such as that of scientific discovery and automated explanatory theory building, the opaqueness of state-of-the-art machine learning is a serious handicap.</p>
<p>On the other hand, a consensus is emerging within the machine learning community for the existence of a tradeoff between interpretability and predictive performance, especially in problems featuring highly unstructured data sources, as is often the case in natural sciences. In such cases, the inner workings of simpler machine learning models are transparent at the cost of lower performance, while highly complex models are more accurate, at the cost of interpretability. Explainable AI, [7], attempts to shed light into the complexities of state-of-the-art machine-generated models without compromising their performance. It is attracting significant attention and numerous approaches have already been proposed, involving e.g. the use of focus mechanisms in deep neural networks to highlight regions of the network involved in certain inferences and potentially identify relevant features, [8], or by identifying such features via studying the effects of input perturbations to the output. Other approaches rely on dissecting the object (e.g. an image) to be classified into a set of prototypical parts and then combining evidence from such prototypes to explain the final classification, [9]. The idea behind this paper is to automate the search for a healthy Lagrangian possessing higher-derivatives using Machine Learning (ML).</p>
<p>In this paper, we are interested in studying how efficient an artificial neural network can be in classifying a given Lagrangian to "healthy" or "non-healthy", after training, without extracting the equations of motion. Further, we construct new Lagrangians in an automated way. The manuscript is organized as follows. In Section II we provide a concise presentation of the Lagrangian formalism and we also present our algorithms for creating Lagrangians. In Section III we describe in detail the ML algorithms and architectures we employ, while in Section IV we present and discuss our results. Finally, in Section V we draw our conclusions and we point out directions for further work.</p>
<p>II. DESCRIPTION OF PHYSICAL THEORIES</p>
<p>A framework that allows systematical study of a physical theory is the Lagrangian formalism. The latter comes from classical physics and it was first formulated from Joseph-Louis Lagrange in late 18th century. Along with its generalizations to include various fields and curved space-time is the cornerstone of modern and classical physics. Let us begin our discussion within the context of classical mechanics. Given the action functional
S[q, t 1 , t 2 ] = q(t2) q(t1) L(q,q, ..., q (n) )dt,(1)
where t 1 , t 2 are two time instances and q is the fundamental quantity that describes our physical system and specifically the position over time of the particle under study. The corresponding equations of motion arise by applying the Hamilton's principle, namely the evolution of the physical system is such a way that the action is stationary (for a thorough presentation of action principle see [10]). In other words, the trajectory that the particle will follow between q(t 1 ) and q(t 2 ) is such that the action functional derivative is zero, i.e. δS = 0. This requirement produces the equation of motion [10]
∂L ∂q − d dt ∂L ∂q + ... + (−1) n d n dt n ∂L ∂q (n) = 0.(2)
In the latter we omitted the arguments of the Lagrangian for brevity. The Lagrangian to obtain the Newtonian mechanics is L = T − V (q), where T is the kinetic energy of the system, i.e. T = (1/2) mq 2 . Since the late 18th century where this formalism was first proposed, it has been generalized to include classical and quantum fields, as well as to handle in a unified way time and positions in the context of Einstein's theory of gravity [11][12][13]. Today the procedure of constructing a new physical theory is essentially the process of constructing new Lagrangians with the desired properties (e.g. specific symmetries).</p>
<p>A. Ostrogradsky's instability</p>
<p>As a starting point in constructing a Lagrangian, one can consider any function of positions and velocities that is smooth enough. In this procedure there exists a number of theoretical requirements, such as the verification of specific symmetries. However, as Ostrogradsky showed first [14], there exists an additional powerful constraint that does not allow Lagrangians with higher order time-derivatives. This requirement arises from the fact that in such case the corresponding Hamiltonian is not bounded, which implies that the total energy of the system is unbounded.</p>
<p>In order to avoid this Ostrogradsky instability the equations of motion of a physical system need to have up to second order equation of motion. A sufficient condition for that is if the Lagrangian has terms up to first order in derivatives, nevertheless this is not necessary since as Horndeski showed [15] one may have higher-order terms in a Lagrangian in a suitable combination that the resulting higher-order time derivatives in the equations of motion cancel out, resulting to second-order ones. Hence, this implies that discriminating if a given Lagrangian is healthy or not, requires the extraction of the equations of motion and their subsequent elaboration, a procedure that in case of complicated Lagrangians is not trivial. That is why it becomes of great interest the construction of a machine learning procedure that could discriminate if a given Lagrangian is healthy or not without the need to perform the variation and the extraction of equations of motion.</p>
<p>B. Describing a Lagrangian</p>
<p>A Lagrangian could in principle contain scalar quantities that are functions of higher dimensional objects. For instance the electromagnetic Lagrangian contains the tensor F µν , while the Einstein-Hilbert Lagrangian of general relativity possesses contractions of the Riemann tensor. In order to give a Lagrangian as input to a neural network it is necessary to translate it in a form that the network can handle, i.e to describe it as a set of features. Formally, a function φ : L → R n is to be found in order to provide the feature vectors, where L is the Lagrangian's space.</p>
<p>We limit ourselves to classical mechanics, in a sense that the fundamental quantity involved in the Lagrangian is the position of a moving body, x(t) as a function of time. Position along with its time derivatives constitute the set of physically interesting entities. Within our modelling, all physical parameters such as mass are set to unity. Moreover, the parameters are measured in proper units, in order to maintain the standard physical interpretation of potential energy. Some simple representations are provided in the following.</p>
<p>• A first parametrization is to use the kinetic term T = α 0 mx(t) α1ẋ (t) α2ẍ (t) α3 , which includes the standard kinetic term T = mẋ(t) 2 /2. Additionally, we consider a general potential of the form
V a1 (x(t),ẋ(t),ẍ(t)) = α 4 x(t) α5ẋ (t) α6ẍ (t) α7 .(3)
Hence, the corresponding set of numbers to describe this Lagrangian is f = [α 0 , ..., α 7 ]. Using the standard expression for the kinetic energy implies that [α 0 , α 1 , α 2 , α 3 ] = [0.5, 0, 2, 0]. By using eq. (2) it is easy to observe that the essential requirement for a Lagrangian of the form (3) to be free of the Ostrogradski instability is to have α 4 = 0 or α 4 = 1. The aforementioned representation could generalized by considering a sum of potentials of the form of (3):
V a2 (x(t),ẋ(t),ẍ(t)) = n i=1 α i0 x(t) αi1ẋ (t) αi2ẍ (t) αi3 ,(4)
with i = 1, · · · , n, and the corresponding feature vector to describe this Lagrangian is f = [0.5, 0.0, 2, 0.0, .., α n0 , α n1 , α n2 , α n3 ], with dimension 4(n + 1).</p>
<p>• Another parametrization is to further assume that every potential that describes a certain class of phenomena will be C ∞ , namely infinite times differentiable, at least within a certain subspace. From the latter, the corresponding Taylor series always exists, thus an adequate description of a general potential energy is its Taylor coefficients around the point (x 0 ,ẋ 0 ,ẍ 0 ). The most general Taylor expansion reads
V b (x,ẋ,ẍ) = ∞ j=0 1 j! (x − x 0 ) ∂ ∂x + (ẋ −ẋ 0 ) ∂ ∂ẋ + (ẍ −ẍ 0 ) ∂ ∂ẍ j V (x ,ẋ ,ẍ ) (x =x0,ẋ =ẋ0,ẍ =ẍ 0) .(5)
By keeping terms up to 2nd order and re-arranging we obtain the following
V b1 (x,ẋ,ẍ) = a 0 + a 1 x + a 2ẋ + a 3ẍ + a 4 x 2 + a 5ẋ 2 + a 6ẍ 2 + α 7 xẋ + α 8 xẍ + α 9ẋẍ ,(6)
where a i are real numbers and the corresponding feature vector is f = [0.5, 0.0, 2.0, 0.0, α 0 , .., α 9 ]. The terms α 0 and α 9 do not play any role in a Lagrangian formulation (their contribution to the field equations is zero), while the governing parameter is α 6 . By including terms up to the 3rd order, we obtain the potential V b2 as
V b2 (x,ẋ,ẍ) = a 0 + a 1 x + a 2ẋ + a 3ẍ + a 4 x 2 + a 5ẋ 2 + a 6ẍ 2 + α 7 xẋ + α 8 xẍ + α 9ẋẍ +α 10 x 3 + α 11ẋ 3 + α 12ẍ 3 + α 13 xẋẍ + α 14 x 2ẋ + α 15ẋ 2 x + α 16ẍ 2 x + α 17ẍẋ 2 .(7)
Similarly to the previous example, we mention that the term with coefficient α 17 is a total derivative and thus has no effect in the equations of motion. Finally, note that in principle, a series expansion could be performed on different bases, i.e Legendre, Laguerre and Hermitte polynomials.</p>
<p>In this work, without loss of generality we will use only parametrization V ai , since we are interested in studying the training of a neural network to discriminate between "healthy" and "non-healthy" theories.</p>
<p>There exist numerous descriptions with regard to certain classes of Lagrangians (finding a description valid for an arbitrary Lagrangian is left for a future project). In Table I 
7 D e −2α(r−r 0 ) − 2e −α(r−r 0 ) b
Morse potential [16] TABLE I: Some classical potentials, along with their representation ability in order to assess the generality of the parametrizations employed. Parametrizations a1,a2,b correspond to (3), (4) and (5) respectively.</p>
<p>III. MODEL AND ALGORITHMS</p>
<p>In order to classify a Lagrangian as healthy or not, we are using fully-connected feed-forward neural networks and supervised learning. Regarding the automated production of new Lagrangians, we employ Genetic Algorithms.</p>
<p>A. Neural Network setup</p>
<p>A neural network performs mappings from an input space to an output space. In our case the input space contains the feature vectors of the Lagrangians and the output space contains the two categories. The basic structural element of a neural network is the layer, that is a group of neurons. The neurons of each layer connect to those of the next and these connections are called synapses. The first layer of the network is called input layer, the last layer is called output layer and all the layers in between are called hidden layers. The architecture mentioned above describes a typical fully connected neural network as shown in Fig. 1. The term feed-forward implies a network that its synapses do not form a cycle, in contrast with e.g the recurrent neural networks [17]. Formal definitions of the aforementioned terms regarding feed -forward neural networks could be found at [18].</p>
<p>In order to understand how the mapping is realized, we need to break down the network to its components. The neurons of the first layer activate as we input a feature vector (corresponding to a Lagrangian). Activation means that a neuron calculate a value and "feeds" it to the next layer. As we mentioned above, in a fully -connected net, all the neurons of a layer are connected to all neurons of the previous one through synapses. Each synapse contains a real number, called weight, that indicates how much the activation of the previous layer specific neuron, affects the activation of the neuron in the next layer. Furthermore, each neuron contains a real value, called bias, that indicates a threshold, over which a neuron will meaningfully pass its value over to the next layer (fire). The activation of a neuron is described as
α l j = σ k w l jk α l−1 k + b l j ,(8)
. . .  where w l jk is the weight of the synapse that connects the k th neuron of the (l − 1) th layer to the j th neuron in the l th layer. Moreover, b l j is the bias of the j th neuron. The function σ is the sigmoid function, σ(x) = 1/ (1 + exp(x)). When a neural network is created all its weights and biases values are randomly initialized. Therefore, in order for our network to be able to classify correctly a given Langragian, its values need to be adjusted. Specifically these values need to be adjusted in a way that when a healthy Lagrangian is fed into the network, the network will provide as output a real number close to 1, and on the other hand in the case of the non-healthy Lagrangian it will provide as output a number close to 0.</p>
<p>In order to adjust those weights and biases we will employ the back-propagation algorithm, described further later in the next subsection.</p>
<p>Training</p>
<p>In order to construct the training data set, a Monte Carlo approach is employed. The i-th iteration of the process consists of constructing a random vector of length n. Subsequently, by using a symbolic algebra system, that is sympy [19], the Euler-Lagrange equations (2) are employed to extract the equations of motion. Furthermore, the equations are checked for the existence of time derivatives of order higher than 2. If there are only second derivatives in the equations of motion, the random vector is labeled as "healthy", otherwise it is labeled as "non-healthy". The above procedure is repeated N times. Some characteristic subsets of the a i space are presented in Fig. 2. We mention here that the aforementioned criterion of "healthy" Lagrangians is definitely not unique. In fact, one could choose to label as "healthy" any kind of Lagrangian, e.g. Lagrangians possessing additional features, satisfying extra symmetries, etc.</p>
<p>The total training data set constitutes of 2000 "healthy" and 2000 "non-healthy" feature vectors. The training strategy of backpropagation [18], consists of splitting the total dataset in batches with 400 elements each, i.e resulting to 10 batches. When all elements of a batch are fed into the network, the average cost is computed using the quadratic cost function. The average cost backpropagates through the network fixing its weights and biases. The same process needs to be done for each batch in order for an epoch of training to be completed. Before an epoch of training all the training set is scrambled, resulting in set of different batches. The training object is to minimize the cost function. Different values of the learning rate η are employed in an effort to avoid local minima, while also maintain a steep learning curve. All the above steps describe the iterative process of gradient descend.</p>
<p>FIG. 2:</p>
<p>3-dimensional subspaces of the parameter space for Lagrangians of the generic form L = T −α4x α 5ẋ α 6ẍ α 7 . The circular marker corresponds to "non-healthy" Lagrangians while the triangular one to "healthy".</p>
<p>Validation</p>
<p>After training the network, 10 data sets are produced, each of them consisting of 10000 feature vectors, and they are given to the network for classification. In this point another hyper-parameter is introduced, namely the threshold, that determines how a number in the continuum set [0, 1] is projected to the set [0, 1] ∩ N. Assessing the ability of the trained network to correctly classify data that have not encountered before, is essential in order to avoid over-fitting. The latter term describes the situation in which the neural network performs very well in the training data and poorly in the testing data, (for a detailed discussion of over-fitting within a statistical context see [20]). Towards this purpose, a number of metrics is utilized.</p>
<p>Accuracy measures the relative ability of a given network to provide correct predictions, that is Accuracy = True Positives + True Negatives Total Predictions .</p>
<p>It is straightforward to apply this metric, however there are some caveats. A characteristic example is that a model that classifies all input data as positive, applied to a dataset consisting of 95 positive and 5 negative samples, gives 0.95 accuracy score. Precision is the metric that expresses the proportion of the actual positive data over the data items that were classified by our model as positives:
Precision = True Positives True Positives + False Positives .(10)
Similarly, recall is the metric that expresses what proportion of the actual positive data was classified correctly by our model:</p>
<p>Recall = True Positives True Positives + False Negatives</p>
<p>.</p>
<p>Combining recall and precision by means of the harmonic mean results to the F 1 metric:
F 1 = 2Precision × Recall Precision + Recall .(12)
Up to this point, the numerical value of a network prediction is not used explicitly, since only the classification result is considered. In order to take into account the numerical value of the network's output, another metric is defined, namely the Logarithmic Loss. Although in principle one could utilize just any function as metric, Log Loss is motivated from information theory, and in particular from Kullback-Leibler information [21]:
LogLoss = − 1 N N i=1 M j=1 y ij log(p ij ),(13)
where N is the length of the training data set, M the number of different classes, y ij the probability of the i-th data point to belong to the j-th class, and p ij the predicted probability. Logarithmic Loss does not have upper bound and exists in the range [0, ∞). Logarithmic Loss nearer to 0 indicates higher accuracy, whereas if Logarithmic Loss is away from 0 then it indicates lower accuracy.</p>
<p>B. Constructing new Lagrangians</p>
<p>An intriguing path to explore is the possibility of constructing new Lagrangians in an automated way, with the aid of the trained network. In this work, in order to maintain transparency of the generating procedure, we use genetic algorithms (GA). In general there are alternative ways too, e.g. within the framework of Generative Adversarial Networks [22].</p>
<p>The main idea behind Generative Algorithms is that given a population and a way to assess the fitness of each member, if the result of the mating depends on the fitness of each parent, after a number of "generations" the fitness of the population will be maximum. A more formal consideration seems to be in place. We assume that a population of feature vectors P , with P ⊆ R n , exists. The trained network could be considered as a fitness function, N f it : P → [0, 1], and by using it one could associate a value within the interval [0, 1] to each member of the population. Furthermore, different members of the population are "mating" to create new ones, with the contribution of each parent's feature to the corresponding "child's" feature to be analogue with their relative fitness. Lastly, a random "mutation" to one or more elements of each "child" occurs according to a pre-defined distribution. These steps correspond to a "generation". After performing the aforementioned steps for a number of generations, we have a population whose each member has close to maximum fitness. The final population does not have common members with the initial one. For more details one could follow the presentation of the subject in [23].</p>
<p>In this work, the "mating" algorithm consists of a random selection of pairs within the initial population, and the "child" possesses the weighted average of the parent's features. The "mutation" step takes place at a random feature of the child if a dice is smaller that a pre-defined probability. The final result obviously is affected by the initial population and the mating and mutation operations. The details of the Generative Algorithms are presented in Table  II.   </p>
<p>IV. RESULTS AND DISCUSSION</p>
<p>A fully-connected neural network architecture is implemented and trained to discriminate between "healthy" and "non-healthy" Lagrangians for a classical theory with one degree of freedom. A number of different architectures, regarding the depth and the width of the network, were tested and the most promising was selected, namely a structure with two hidden layers of 16 neurons each. The optimal number of epochs to be used for training the network, in order to minimize training error on the one hand while avoiding over-fitting on the other, is found by comparing training and validation errors. Over-fitting occurs when we observe an increase in the testing error while the training error is consistently decreasing (see p. 202 of [24]). However, in our case the testing error seems to be decreasing over the epochs, resulting in a better "behaviour" of the network, as can be seen in Figs. 3, 4. Moreover, we also apply the metrics defined in Section III at different stages of the training procedure, as can be seen in Tables III and IV    V: A number of "new" Lagrangians that correspond to healthy theories, constructed using GA. The procedure was repeated for 10 times and the most fitted Lagrangian for each iteration was selected.</p>
<p>a list with the most prominent Lagrangians, each selected from the final population, after 200 generations, from 10 different initial populations. These Lagrangians were constructed by the machine learning procedure without the extraction of field equations. As one can see amongst others, the GA was able to find the correct feature, that a "healthy" Lagrangian needs to haveẍ to the exponent 0,1, although no such rule was initially imposed (concerning the other exponents we could have additionally required them to be integers, however we did not do it since it is irrelevant for our analysis).</p>
<p>One might argue that the gain is not significant, since a human can relatively easily write down Lagrangians similar to those of Table V, that will not lead to more than second-order terms in the field equations. Indeed, this is true for this simple subclass of one-dimensional Lagrangians, but it becomes laborious for higher dimensionality and more complicated Lagrangians. For instance in the case of gravitational theories, such as General Relativity and its modifications, where one uses four dimensions and Lagrangians made from contractions of the curvature (Riemann) tensor, it is impossible to deduce a priori if a Lagrangian leads to equations of motion with more than second-order terms (and thus whether it is physically accepted or not), unless these equations are explicitly extracted. Moreover, even after the explicit extraction of the equations of motion, a thorough elaboration is needed since terms with more than second-order derivatives could be mutually eliminated through suitable integration by parts (see e.g. [15,25,26]). In summary, even with the help of usual software the above standard procedure of generating new healthy Lagrangians can be laborious and require many months. Hence, we deduce that building machine learning tools that can construct new healthy Lagrangians without the need to perform all the steps of the standard procedure, namely explicitly extract the field equations and thoroughly examine and elaborate them, would be extremely helpful for the community of gravity, analytical and quantum mechanics, solid-state, theoretical physics, biology etc.</p>
<p>V. CONCLUSIONS</p>
<p>A neural network architecture was implemented, trained and tested, in order to decide if a given Lagrangian will lead to equations of motion with higher order derivatives, without explicitly performing the analytical calculations. Validation of the trained network using the explicit standard procedure proved that for the simple model under study, the efficient discrimination of healthy and non-healthy Lagrangians is established. Furthermore, "new" healthy Lagrangians were constructed in a fully automated way. Their properties are related with the initial Monte Carlo generated population of Lagrangians and the imposed requirements. Thus, in general one can automatically construct new Lagrangians, corresponding to new physical theories, possessing arbitrary properties. By suitably labeling the training data-sets, one can employ different criteria, resulting to a trained network that could decide for more complex cases.</p>
<p>There are various applications for this kind of approach, ranging from gravitational theories to solid state physics. In all cases, the search for a Lagrangian possessing some pre-defined properties (i.e symmetries) could be substantially simplified in the context of our approach. Additionally, it is interesting to mention that such an approach might be used to discriminate models derived from such Lagrangians. For instance in [27] it was proposed a deep learning tool in order to study the evolution of dark energy models, combining two architectures: the Recurrent Neural Networks and the Bayesian Neural Networks, since the former is capable of classifying objects while the latter emerges as a solution for problems like over-fitting. Such applications could be useful to confront with Lagrange-multiplier based models (see e.g. [28,29]) or other gravitational models arising from Lagrangian modifications. Definitely, in more complex applications, the exact form of the map between the Lagrangian space and the n-dimensional real space needs to be defined. Exploring the different ways to "translate" a Lagrangian to multi-dimensional real space, in an effort for a general description, is deemed very fruitful, and the present analysis can stand as its base.</p>
<p>FIG. 1 :
1The structure of a fully -connected neural Network. The input and output layers have n neurons, while there are k hidden layers, each one consisting of n neurons.</p>
<p>II: Parameters used in the Generative Algorithms approach, N is the dimension of the population. The weights wi are defined as wi = 10 5 N f it (Pi), where N f it is the trained neural network and Pi is a member of the population.</p>
<p>FIG. 3 :FIG. 4 :
34The training and testing errors as a function of training epochs, for 8The training and testing errors as a function of training epochs, for 18-16-16-1 network with η = 1.</p>
<p>IV: Values of different metrics during epochs of training for the fully-connected, feed-forward neural network, with architecture 18-16-16-1 and η = 1. Note that early epochs are affected by the initial random selection of weights.</p>
<p>some classical potentials are presented along with the proper map to construct the relevant feature vectors.No 
V 
Parametrization(s) 
Description 
1 
0 
a1, a2, b 
free particle 
2 </p>
<p>1 </p>
<p>2 kr 2 
a1, a2, b 
mass connected to an ideal spring of spring constant k 
3 </p>
<p>Gm 
r </p>
<p>a1, a2, b 
gravitational potential </p>
<p>4 
4 </p>
<p>σ 
r </p>
<p>12 − σ </p>
<p>r </p>
<p>6 </p>
<p>a2, b 
Lennard-Jones potential [16] </p>
<p>5 </p>
<p>σ 
r </p>
<p>ν </p>
<p>a1, a2, b 
soft-sphere potential [16] 
6 
mgcos(r) 
b 
non-linear oscillator </p>
<p>TABLE</p>
<p>.Epoch Accuracy Precision(s) Recall F1 
Log. Loss 
500 
0.549 
0.081 
0.580 0.564 
0.682 
5000 
0.613 
0.143 
1.0 
0.760 
0.549 
10000 
0.685 
0.164 
0.940 0.793 
0.374 
15000 
0.965 
0.653 
0.999 0.982 
0.067 
20000 
1.000 
1.000 
1.000 1.000 
0.026 
25000 
1.000 
1.000 
1.000 1.000 
0.016 </p>
<p>TABLE III: Values of different metrics during epochs of training for the fully-connected, feed-forward neural network, with 
architecture 8-6-6-1 and η = 0.1. Note that early epochs are affected by the initial random selection of weights. </p>
<p>By employing GA we finally construct new healthy Lagrangians. For indicative purposes, in Table V we present 
Epoch Accuracy Precision(s) Recall F1 
Log. Loss 
200 
0.965 
0.975 
0.973 0.969 
0.119 
400 
0.970 
0.979 
0.976 0.973 
0.097 
600 
0.972 
0.981 
0.976 0.974 
0.091 
800 
0.972 
0.983 
0.976 0.974 
0.088 
1000 
0.973 
0.983 
0.976 0.974 
0.086 
3000 
0.974 
0.987 
0.974 0.974 
0.080 
5000 
0.975 
0.988 
0.975 0.975 
0.076 </p>
<p>TABLE</p>
<p>TABLE</p>
<p>. R P Woodard, Reports on Progress in Physics. 72126002R. P. Woodard, Reports on Progress in Physics 72, 126002 (2009).</p>
<p>. L Smolin, hep-th/0303185arXiv preprintL. Smolin, arXiv preprint hep-th/0303185 (2003).</p>
<p>C Callender, N Huggett, Physics meets philosophy at the Planck scale: Contemporary theories in quantum gravity. Cambridge University PressC. Callender and N. Huggett, Physics meets philosophy at the Planck scale: Contemporary theories in quantum gravity (Cambridge University Press, 2001).</p>
<p>Effective action in quantum gravity. I L Buchbinder, I. L. Buchbinder, Effective action in quantum gravity (Routledge, 2017).</p>
<p>. R D King, M Liakata, C Lu, S G Oliver, L N Soldatova, Journal of The Royal Society Interface. 81440R. D. King, M. Liakata, C. Lu, S. G. Oliver, and L. N. Soldatova, Journal of The Royal Society Interface 8, 1440 (2011).</p>
<p>. D Waltz, B G Buchanan, Science. 32443D. Waltz and B. G. Buchanan, Science 324, 43 (2009).</p>
<p>. A Adadi, M Berrada, IEEE Access. 652138A. Adadi and M. Berrada, IEEE Access 6, 52138 (2018).</p>
<p>J Wagner, J M Kohler, T Gindele, L Hetzel, J T Wiedemer, S Behnke, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionJ. Wagner, J. M. Kohler, T. Gindele, L. Hetzel, J. T. Wiedemer, and S. Behnke, in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (2019), pp. 9097-9107.</p>
<p>E Choi, M T Bahadori, J Sun, J Kulas, A Schuetz, W Stewart, Advances in Neural Information Processing Systems. E. Choi, M. T. Bahadori, J. Sun, J. Kulas, A. Schuetz, and W. Stewart, in Advances in Neural Information Processing Systems (2016), pp. 3504-3512.</p>
<p>H Goldstein, C Poole, J Safko, Classical mechanics. H. Goldstein, C. Poole, and J. Safko, Classical mechanics (2002).</p>
<p>A modern introduction to quantum field theory. M Maggiore, Oxford university press12M. Maggiore, A modern introduction to quantum field theory, vol. 12 (Oxford university press, 2005).</p>
<p>S Weinberg, Gravitation and Cosmology: Principles and Applications of the General Theory of Relativity. S. Weinberg, Gravitation and Cosmology: Principles and Applications of the General Theory of Relativity, (1972).</p>
<p>. E N Saridakis, arXiv:2105.12582CANTATA. gr-qcE. N. Saridakis et al. [CANTATA], [arXiv:2105.12582 [gr-qc]].</p>
<p>. M Ostrogradsky, Mem. Acad. St. Petersbourg. 6385M. Ostrogradsky, Mem. Acad. St. Petersbourg 6, 385 (1850).</p>
<p>. G W Horndeski, Int. J. Theor. Phys. 10363G. W. Horndeski, Int. J. Theor. Phys. 10, 363 (1974).</p>
<p>M Fyta, 10.1088/978-1-6817-4417-9ch4978-1-6817-4417-9Computational Approaches in Physics. Morgan &amp; Claypool PublishersM. Fyta, in Computational Approaches in Physics (Morgan &amp; Claypool Publishers, 2016), 2053-2571, pp. 4-1 to 4-15, ISBN 978-1-6817-4417-9, URL http://dx.doi.org/10.1088/978-1-6817-4417-9ch4.</p>
<p>D P Mandic, J Chambers, Recurrent neural networks for prediction: learning algorithms, architectures and stability. John Wiley &amp; Sons, IncD. P. Mandic and J. Chambers, Recurrent neural networks for prediction: learning algorithms, architectures and stability (John Wiley &amp; Sons, Inc., 2001).</p>
<p>Chemometrics and intelligent laboratory systems. D Svozil, V Kvasnicka, J Pospichal, 3943D. Svozil, V. Kvasnicka, and J. Pospichal, Chemometrics and intelligent laboratory systems 39, 43 (1997).</p>
<p>. A Meurer, 10.7717/peerj-cs.1032376-5992PeerJ Computer Science. 3103A. Meurer et al., PeerJ Computer Science 3, e103 (2017), ISSN 2376-5992, URL https://doi.org/10.7717/peerj-cs.103.</p>
<p>. S Geman, E Bienenstock, R Doursat, Neural computation. 41S. Geman, E. Bienenstock, and R. Doursat, Neural computation 4, 1 (1992).</p>
<p>I Goodfellow, Y Bengio, A Courville, Deep learning. MIT pressI. Goodfellow, Y. Bengio, and A. Courville, Deep learning (MIT press, 2016).</p>
<p>I Goodfellow, J Pouget-Abadie, M Mirza, B Xu, D Warde-Farley, S Ozair, A Courville, Y Bengio, Advances in neural information processing systems. I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio, in Advances in neural information processing systems (2014), pp. 2672-2680.</p>
<p>D E Goldberg, Genetic algorithms. Pearson Education IndiaD. E. Goldberg, Genetic algorithms (Pearson Education India, 2006).</p>
<p>C Rao, V N Gudivada, Computational Analysis and Understanding of Natural Languages: Principles, Methods and Applications. ElsevierC. Rao and V. N. Gudivada, Computational Analysis and Understanding of Natural Languages: Principles, Methods and Applications (Elsevier, 2018).</p>
<p>. E N Saridakis, M Tsoukalas, 1601.06734Phys. Rev. 93124032E. N. Saridakis and M. Tsoukalas, Phys. Rev. D93, 124032 (2016), 1601.06734.</p>
<p>. C Erices, E Papantonopoulos, E N Saridakis, 1903.11128Phys. Rev. 99123527C. Erices, E. Papantonopoulos, and E. N. Saridakis, Phys. Rev. D99, 123527 (2019), 1903.11128.</p>
<p>. C Escamilla-Rivera, M A C Quintero, S Capozziello, 1910.02788JCAP. 038C. Escamilla-Rivera, M. A. C. Quintero and S. Capozziello, JCAP 03, 008 (2020), 1910.02788.</p>
<p>. S Capozziello, J Matsumoto, S Nojiri, S D Odintsov, 1004.3691Phys. Lett. B. 693S. Capozziello, J. Matsumoto, S. Nojiri and S. D. Odintsov, Phys. Lett. B 693, (2010), 1004.3691.</p>
<p>. Y F Cai, E N Saridakis, Class. Quant. Grav. 28Y. F. Cai and E. N. Saridakis, Class. Quant. Grav. 28, 035010 (2011), 1007.3204.</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2505 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2505</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2505</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-68.html">extraction-schema-68</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <p><strong>Paper ID:</strong> paper-260384616</p>
                <p><strong>Paper Title:</strong> Scientific discovery in the age of artificial intelligence</p>
                <p><strong>Paper Abstract:</strong> ,</p>
                <p><strong>Cost:</strong> 0.028</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2505.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2505.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Generative models (VAE/GAN/Flows/Diffusion)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative models (variational autoencoders, generative adversarial networks, normalizing flows, diffusion models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Probabilistic neural architectures that learn an approximate data distribution and can sample new candidate hypotheses (molecules, protein sequences, images, symbolic expressions) from that learned distribution for downstream screening or optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Generative models (VAE / GAN / Normalizing flows / Diffusion)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Class of neural generative models: VAEs learn latent continuous representations and decoders to sample; GANs use adversarial generator/discriminator to synthesize realistic samples; normalizing flows model exact densities via invertible transformations; diffusion models learn denoising reverse processes to sample complex distributions. In the paper these are described as tools to model underlying scientific-data distributions, create synthetic training examples, produce candidate molecular/protein designs, and accelerate simulations by sampling equilibrium states.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>generative-models</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>chemistry, biology, materials science, physics, imaging</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Sample from learned generative distribution (unconditional or conditional); map discrete objects into continuous latent spaces (VAEs) and decode to candidate hypotheses; combine with surrogate predictors or reward functions to bias generated samples toward desired properties.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Screening using black-box predictors or surrogate models (high-fidelity simulations or pretrained property predictors), synthesizability filters and experimental/simulation feedback via active learning/Bayesian optimization (general approach described in the review).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Computational screening using surrogate predictors, in silico simulation (e.g., molecular simulations), and downstream laboratory validation when available (review describes pipeline but not a single instantiated experiment within this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td>Often combined with Bayesian or ensemble surrogates, or with density-estimation methods (normalizing flows give likelihoods); the review advocates well-calibrated uncertainty estimators but does not prescribe a single implementation here.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Extrapolation in sparsely explored regions can be poor; requires careful validity checks for synthesizability and physical plausibility; may need large datasets or strong inductive biases to produce scientific-grade hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Scientific discovery in the age of artificial intelligence', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2505.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2505.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reinforcement-learning symbolic regression</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reinforcement-learning approaches for symbolic regression and hypothesis search</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>RL agents trained to sequentially generate symbolic expressions or discrete hypotheses by maximizing a reward that measures fit to data and other design criteria, enabling automated discovery of mathematical laws and refutations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deep symbolic regression: recovering mathematical expressions from data via risk-seeking policy gradients</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Reinforcement-learning symbolic regression</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A neural policy (often an RNN or transformer) grows mathematical expressions token-by-token or expands parse-tree nodes; it is trained with RL (policy gradients, risk-seeking objectives) to maximize a reward defined by how well the expression fits observed data and optional parsimony/prior criteria. The review describes parse-tree-based RL and sequence-based RL for discovering formulas and for designing searches aiming to refute conjectures.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>neural-symbolic (RL + sequence/parse models)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>physics, mathematics, general scientific modeling</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Sequential construction of symbolic expressions using an RL policy that selects grammar tokens or tree-expansion actions; exploration guided by reward signals (fit-to-data, information gain, simplicity).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td>Implicit via reward shaping (Occam's razor / parsimony) and by penalizing previously known forms in the reward; explicit novelty metrics not detailed in review.</td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Data fit (residual/error), parsimony constraints, and possible downstream experimental/simulation checks; plausibility is enforced via reward and grammar constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td>Handled by multi-term reward functions combining goodness-of-fit with penalties for complexity (Occam's razor) to prefer simpler plausible laws over overly novel but implausible ones.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Reward functions based on fit-to-data (e.g., error/residual), syntactic validity; no standardized numerical metrics specified in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Evaluate candidate symbolic laws against held-out data, simulation, or follow-up experiments; the review cites examples where refutations or matches to observations are used as validation.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td>Review cites work where RL-based symbolic search found counterexamples or formulae consistent with data (references provided), but does not itself present a new experimentally validated discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>RL methods can get stuck in local optima and may not generalize well to regimes outside training/search experience; exploration strategies are needed to improve generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Scientific discovery in the age of artificial intelligence', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2505.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2505.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GFlowNet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Flow Network (GFlowNet)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A generative framework that learns stochastic policies to sample complex discrete objects with probability proportional to a provided reward, promoting diverse high-reward hypothesis generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>GFlowNet foundations</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Generative Flow Networks (GFlowNets)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>GFlowNets learn a flow (policy) over compositional generation trajectories so that final objects are sampled with probability proportional to a scalar reward; they balance exploration and mode-covering behavior by training trajectory flows using objectives like flow matching or trajectory balance.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>generative-flow-network</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>drug discovery, biological sequence design, chemistry</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Sequential generative process guided by a learned policy that samples objects (molecules, sequences) with probabilities proportional to a reward function reflecting desired properties, enabling multimodal sampling instead of single-mode optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td>Implicit â€” the mode-covering sampling objective encourages discovering diverse (novel) high-reward hypotheses across modes; explicit novelty metrics not specified in review.</td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Reward functions typically include surrogate property predictors and constraints (e.g., synthesizability); plausibility assessed by downstream screening and experimental testing.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td>Achieved by designing reward functions that combine property scores (plausibility) with objectives that encourage diversity (novelty), e.g., via entropy-encouraging components.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Computational screening and, in referenced works, experimental validation of selected candidates; review cites biological sequence design and molecule design applications.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td>Referenced work applied GFlowNets to biological sequence and drug-design search spaces; the review notes successful applications but does not detail a particular novel experimentally validated finding within this review.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Requires carefully specified reward functions; computational cost and surrogate-model fidelity limit real-world success; balancing exploration and exploitation remains challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Scientific discovery in the age of artificial intelligence', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2505.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2505.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Grammar-VAE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Grammar Variational Autoencoder</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A VAE variant that embeds discrete structured objects (e.g., parse trees, SMILES strings) into a continuous latent space constrained by a context-free grammar, enabling syntactically valid latent optimization for symbolic laws or molecules.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Grammar variational autoencoder</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Grammar Variational Autoencoder (Grammar-VAE)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Maps discrete grammar-constrained structures into a continuous latent space; decoder constrained by grammar ensures syntactic validity; latent-space optimization (e.g., Bayesian optimization) then searches for high-scoring symbolic expressions or molecules.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>latent-variable VAE (grammar-constrained)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>symbolic regression (physics/mathematics), molecular design (chemistry)</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Encode candidate symbolic parse trees into latent vectors, optimize latent vectors for desired objectives using Bayesian optimization or gradient methods, decode to syntactically valid candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Data fit and reward evaluation in latent space; decoded candidates evaluated by fit-to-data metrics or surrogate predictors.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Evaluate decoded symbolic expressions or molecules against data or property predictors; possible downstream experimental validation referenced in the literature.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Latent-space optimization can produce decoded outputs that, while syntactically valid, may be implausible or out-of-distribution; relies on quality of decoder and latent representation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Scientific discovery in the age of artificial intelligence', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2505.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2505.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SMILES-VAE / Latent molecular optimization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SMILES VAE and continuous molecular representations for optimization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>VAEs trained on molecular string representations (SMILES) that embed molecules in continuous latent spaces to enable gradient-based or Bayesian optimization for property maximization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Automatic chemical design using a data-driven continuous representation of molecules</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SMILES-VAE (latent molecular optimization)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Encoder-decoder VAE mapping SMILES strings to continuous latent vectors; property predictors on latent space and Bayesian optimization or gradient-based search used to find latent points decoding to molecules with optimized properties; decoder returns discrete SMILES.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>latent-variable VAE</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>chemistry, drug discovery, materials design</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Latent-space optimization (Bayesian optimization, gradient ascent) guided by surrogate property predictors; decode optimized latent vectors to candidate molecules.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Surrogate predictors, synthesizability filters and experimental follow-up as available; the review notes synthesizability is an important post-check.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>In-silico property prediction and ranking, then experimental synthesis/assay for selected candidates in referenced works.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td>Referenced literature applied continuous molecular representations to propose candidate molecules; the review mentions applications to small-molecule and protein design pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Decoder imperfections and mismatch between latent-space objectives and true experimental feasibility (synthesizability) can limit real-world success.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Scientific discovery in the age of artificial intelligence', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2505.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2505.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AlphaFold2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AlphaFold2</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A deep learning system that predicts 3D protein structures from amino-acid sequences with near-experimental accuracy, enabling generation and validation of structural hypotheses at scale.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Highly accurate protein structure prediction with AlphaFold</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AlphaFold2</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>End-to-end deep neural network (attention-based architecture with multiple modules and extensive use of MSAs and templates) that predicts atomic coordinates and confidence metrics for protein structures from sequence input.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>sequence-to-structure deep learning (transformer-like / attention-based)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>structural biology, protein engineering, bioinformatics</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Predicts structure hypotheses (3D coordinates) directly from amino-acid sequence using trained model; can be used to generate candidate folds for unexplored sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Built-in per-residue confidence scores (e.g., pLDDT) and predicted alignment error allow assessment of structural plausibility; review references its atomic accuracy but does not detail metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Model-internal confidence metrics referenced generally in literature (not enumerated here); review emphasizes near-experimental accuracy but does not quantify within this text.</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Comparison to experimentally determined protein structures (X-ray, cryo-EM) in cited AlphaFold work; used as computational validation of structural hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td>Internal confidence scores (discussed in primary AlphaFold literature; review calls for well-calibrated uncertainty estimators generally).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>The review states AlphaFold2 predicts 3D coordinates with atomic accuracy (citing the AlphaFold paper) but does not reproduce numeric metrics here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td>Cited as key breakthrough enabling structural hypotheses at scale; review references its large impact on structural biology.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>May not generalize equally across all protein classes and can struggle with multi-protein complexes or low-homology sequences; review emphasizes need to measure uncertainty when deploying AI tools.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Scientific discovery in the age of artificial intelligence', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2505.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2505.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bayesian machine scientist</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>A Bayesian machine scientist to aid in the solution of challenging scientific problems</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system that learns a posterior distribution over symbolic hypotheses, allowing sampling of hypotheses consistent with data and prior scientific knowledge for downstream testing.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A Bayesian machine scientist to aid in the solution of challenging scientific problems</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Bayesian machine scientist</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Combines symbolic hypothesis representations with Bayesian inference to learn a posterior over candidate symbolic models; enables sampling and ranking of hypotheses compatible with observations and priors.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Bayesian symbolic / probabilistic programming</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>general scientific modeling, symbolic discovery</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Bayesian posterior sampling over symbolic model space â€” propose symbolic models and weight them according to posterior probability conditioned on observed data and priors.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Posterior probability naturally reflects plausibility given data and priors; low-posterior candidates are less plausible.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Posterior probability (Bayesian evidence) used as an implicit quality metric.</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Compare sampled high-posterior models against held-out data and propose experiments to discriminate among posterior hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td>Bayesian framework prioritizes posterior probabilities over p-value hypothesis testing; specific tests not detailed in review.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td>Full posterior over hypotheses (Bayesian uncertainty) enabling quantification of model uncertainty and hypothesis ensembles.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Scalability â€” Bayesian posterior exploration over large symbolic spaces is computationally challenging; requires careful priors and efficient sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Scientific discovery in the age of artificial intelligence', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2505.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2505.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Adaptive reinforced dynamics (neural uncertainty estimator)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adaptive reinforced dynamics with neural-network-based uncertainty estimator for molecular dynamics</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neural-network-guided sampling framework that uses an uncertainty estimator to add compensating potentials, enabling efficient escape from local minima and accelerated exploration of free-energy landscapes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Efficient sampling of high-dimensional free energy landscapes using adaptive reinforced dynamics</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Adaptive reinforced dynamics (neural uncertainty estimator)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Uses a neural network to estimate uncertainty in sampled molecular configurations; where uncertainty is high, the algorithm adaptively adds potentials to flatten barriers and promote transitions, iteratively improving sampling coverage of configuration space.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>neural-guided enhanced-sampling / uncertainty-driven dynamics</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>molecular dynamics, computational chemistry, biophysics</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Not primarily generative of hypotheses; used to accelerate simulation-based evaluation of mechanistic hypotheses by exploring configurational landscapes more efficiently.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Improved sampling yields better estimates of free-energy barriers and transition rates, improving plausibility assessment of mechanistic hypotheses via simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Computational (molecular dynamics) validation: better sampling leads to more reliable computed observables; review cites improvement in sampling efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td>Neural-network-based uncertainty estimator used in the algorithm to locate regions of high epistemic uncertainty and guide adaptive potentials.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Review notes orders-of-magnitude improvements in sampling efficiency for some settings (qualitative); no precise numeric metrics reproduced here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td>Used to accelerate exploration of protein conformational space and identify transitions more efficiently in cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Effectiveness depends on quality/calibration of uncertainty estimator; may introduce bias if compensating potentials are not carefully controlled.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Scientific discovery in the age of artificial intelligence', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2505.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e2505.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Conditional VAE for Bayesian parameter estimation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bayesian parameter estimation using conditional variational autoencoders</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A conditional VAE-based approach that approximates posterior distributions over model parameters for rapid Bayesian inference (example application: gravitational-wave parameter estimation).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Bayesian parameter estimation using conditional variational autoencoders for gravitational-wave astronomy</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Conditional Variational Autoencoder (CVAE) for Bayesian parameter estimation</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Train a CVAE to map observed data to an approximate posterior over parameters by conditioning the encoder/decoder on observables; enables fast amortized Bayesian inference approximating full posterior distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>amortized Bayesian inference / conditional VAE</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>astrophysics (gravitational-wave astronomy), general parameter estimation</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Generates posterior samples (parameter hypotheses) conditioned on observed data via the trained conditional generative model.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Plausibility given data is encoded by posterior probability approximations; plausibility assessed by posterior density and comparison to standard Bayesian samplers.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Posterior approximation quality evaluated by closeness to MCMC/ground-truth posteriors in cited work (not numerically reproduced in review).</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Compare CVAE posterior approximations with gold-standard Bayesian samplers; used to produce fast parameter posteriors for real events.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td>Direct posterior approximation (provides uncertainty estimates over parameters).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Cited as massively faster than traditional samplers in referenced work (orders-of-magnitude speed-ups) but exact numbers not given in the review text.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Compared against traditional Bayesian samplers in cited gravitational-wave literature (improved speed with posterior-approximation fidelity), specifics not enumerated here.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Amortized inference may produce biased posterior approximations if model is mis-specified or training data insufficient.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Scientific discovery in the age of artificial intelligence', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2505.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e2505.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Boltzmann generator / Normalizing flows</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Boltzmann generators (normalizing flows for equilibrium sampling)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Invertible deep networks (normalizing flows) trained to map simple priors to complex equilibrium distributions, enabling direct sampling of equilibrium states and density evaluation for many-body systems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Boltzmann generators: sampling equilibrium states of many-body systems with deep learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Boltzmann generator (normalizing flows)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Series of invertible neural-network transformations mapping simple prior (e.g., Gaussian) to target equilibrium distribution; enables sampling and exact-density evaluation facilitating efficient generation of equilibrium configurations for simulation and hypothesis testing.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>normalizing-flows / generative-density-estimation</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>statistical physics, molecular simulation, lattice field theory</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Generates equilibrium-state samples that can be used to evaluate thermodynamic observables or to test mechanistic hypotheses by sampling rare configurations directly.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Exact density evaluation allows importance-weighted estimates of observables and comparison to conventional MCMC; plausibility assessed via statistical consistency with physics-based observables.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Statistical comparison to ground-truth sampling (MCMC) and computation of thermodynamic observables; cited works report improved sampling of multimodal distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td>Exact density evaluation gives principled likelihoods and enables uncertainty estimation on observables via importance weighting; review highlights this advantage.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td>Applied to equilibrium-sampling tasks in many-body systems and lattice gauge theory in cited literature; review notes improved sampling efficiency in those contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Often computationally expensive to train; may require many flows/layers and care to ensure invertibility and coverage of all modes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Scientific discovery in the age of artificial intelligence', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2505.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e2505.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Autoencoder anomaly detection</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deep autoencoder-based unsupervised anomaly detection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Unsupervised autoencoders trained to reconstruct background data produce higher reconstruction losses for out-of-distribution (rare) events, enabling real-time detection of anomalous scientific signals.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Anomaly detection with robust deep autoencoders</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Autoencoder anomaly detection</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Train an autoencoder on predominant (background) event data; during deployment, compute reconstruction loss (anomaly score); events with high loss are flagged as anomalous candidates for further analysis or experimental follow-up.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>unsupervised anomaly detection (autoencoder)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>high-energy physics, astronomy, Earth science, neuroscience</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Not primarily generative of hypotheses; used to detect rare/novel events that may represent new phenomena and therefore seed hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td>Anomaly score (reconstruction loss) identifies deviation from background distribution as a proxy for novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Subsequent expert analysis, simulation, or downstream classifiers evaluate whether anomalies correspond to plausible new phenomena.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Anomaly score distributions and false-positive/false-negative trade-offs are used to tune thresholds (specific metrics not enumerated in review).</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Retrospective validation on known rare-event datasets and hardware deployment tests (e.g., FPGA implementations cited for LHC use).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td>High anomaly scores flag candidate hallucinated/novel events for human/expert follow-up to filter false positives.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td>Anomaly score as a proxy for epistemic unfamiliarity; no full probabilistic uncertainty model described in review.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td>Used in high-energy physics and other domains to identify rare events for follow-up; cited FPGA deployment for real-time LHC anomaly detection.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>High false-positive rate possible; anomaly score does not by itself determine scientific significance â€” requires downstream interpretation and validation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Scientific discovery in the age of artificial intelligence', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2505.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e2505.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Active learning + Bayesian optimization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Active learning combined with Bayesian optimization for experimental design and hypothesis testing</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Closed-loop strategies that use model uncertainty to select the most informative experiments or candidates, minimizing experiments needed while improving model accuracy and identifying promising hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Active learning + Bayesian optimization</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Active learning selects datapoints (experiments) where model uncertainty is high or expected information gain is maximal; Bayesian optimization uses surrogate models (often Gaussian processes or neural surrogates) to propose next experiments that maximize expected improvement or acquisition functions under cost constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>uncertainty-aware sequential decision-making / Bayesian optimization</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>materials discovery, chemistry synthesis, protein engineering, accelerator tuning, general experimental sciences</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Not direct hypothesis generation; selects and refines hypotheses/candidates by iteratively querying experiments that are most informative about model predictions and parameter spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Uses model uncertainty and predictive distributions to prioritize plausible, informative experiments; acquisition functions encode trade-offs between exploration (novelty) and exploitation (plausibility).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td>Encoded in acquisition functions (e.g., expected improvement, upper confidence bound) that trade off exploration and exploitation.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Acquisition function values, expected information gain, predictive uncertainty â€” specifics are method-dependent and not numerically enumerated in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Closed-loop experimental campaigns where selected candidates are experimentally tested and results are fed back to update models; review cites many practical applications.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>Using uncertainty-aware acquisition helps avoid overconfident selection of false-positive predictions (implicit prevention).</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td>Explicit: surrogate-model predictive uncertainties (GP predictive variance, Bayesian neural net/posterior approximations, ensembles) used to drive selection.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td>Many cited domain-specific successes in the review (materials, proteins, synthesis planning) from active-learning-guided studies.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Quality depends on uncertainty calibration of surrogate models and fidelity of acquisition functions; experimental noise and cost constraints limit efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Scientific discovery in the age of artificial intelligence', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2505.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e2505.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>De novo network hallucination (protein design)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>De novo protein design by deep network hallucination</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A generative procedure using deep networks to â€˜hallucinateâ€™ protein sequences predicted to adopt novel, stable structures â€” a method for automated hypothesis generation in protein design.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>De novo protein design by deep network hallucination</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Deep network hallucination (protein design)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Optimize sequence inputs to a structure-predicting network (or use generative networks) to produce sequences that the network predicts will fold into desired structures; coined 'hallucination' as networks output plausible but novel designs.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>generative/optimization with structure predictor-in-the-loop</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>protein engineering, structural biology</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Gradient-based or generative optimization of sequences to maximize structure-confidence or other predicted properties according to a pretrained structure or property predictor.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Network-predicted structure confidence and downstream experimental characterization of designed proteins (in cited works) serve as plausibility checks.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Experimental synthesis and structural/functional assays in cited studies; review references this work as an example of AI-driven design.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td>Cited as enabling novel protein assemblies and designs validated experimentally in referenced literature.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Risk of producing designs that are artifacts of predictor biases (i.e., model hallucinations); requires experimental validation to confirm real-world fold and function.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Scientific discovery in the age of artificial intelligence', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Deep symbolic regression: recovering mathematical expressions from data via risk-seeking policy gradients <em>(Rating: 2)</em></li>
                <li>GFlowNet foundations <em>(Rating: 2)</em></li>
                <li>Grammar variational autoencoder <em>(Rating: 2)</em></li>
                <li>Automatic chemical design using a data-driven continuous representation of molecules <em>(Rating: 2)</em></li>
                <li>Highly accurate protein structure prediction with AlphaFold <em>(Rating: 2)</em></li>
                <li>A Bayesian machine scientist to aid in the solution of challenging scientific problems <em>(Rating: 2)</em></li>
                <li>Efficient sampling of high-dimensional free energy landscapes using adaptive reinforced dynamics <em>(Rating: 2)</em></li>
                <li>Bayesian parameter estimation using conditional variational autoencoders for gravitational-wave astronomy <em>(Rating: 2)</em></li>
                <li>Boltzmann generators: sampling equilibrium states of many-body systems with deep learning <em>(Rating: 2)</em></li>
                <li>Anomaly detection with robust deep autoencoders <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2505",
    "paper_id": "paper-260384616",
    "extraction_schema_id": "extraction-schema-68",
    "extracted_data": [
        {
            "name_short": "Generative models (VAE/GAN/Flows/Diffusion)",
            "name_full": "Generative models (variational autoencoders, generative adversarial networks, normalizing flows, diffusion models)",
            "brief_description": "Probabilistic neural architectures that learn an approximate data distribution and can sample new candidate hypotheses (molecules, protein sequences, images, symbolic expressions) from that learned distribution for downstream screening or optimization.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Generative models (VAE / GAN / Normalizing flows / Diffusion)",
            "system_description": "Class of neural generative models: VAEs learn latent continuous representations and decoders to sample; GANs use adversarial generator/discriminator to synthesize realistic samples; normalizing flows model exact densities via invertible transformations; diffusion models learn denoising reverse processes to sample complex distributions. In the paper these are described as tools to model underlying scientific-data distributions, create synthetic training examples, produce candidate molecular/protein designs, and accelerate simulations by sampling equilibrium states.",
            "system_type": "generative-models",
            "scientific_domain": "chemistry, biology, materials science, physics, imaging",
            "hypothesis_generation_method": "Sample from learned generative distribution (unconditional or conditional); map discrete objects into continuous latent spaces (VAEs) and decode to candidate hypotheses; combine with surrogate predictors or reward functions to bias generated samples toward desired properties.",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Screening using black-box predictors or surrogate models (high-fidelity simulations or pretrained property predictors), synthesizability filters and experimental/simulation feedback via active learning/Bayesian optimization (general approach described in the review).",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": null,
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Computational screening using surrogate predictors, in silico simulation (e.g., molecular simulations), and downstream laboratory validation when available (review describes pipeline but not a single instantiated experiment within this paper).",
            "reproducibility_measures": null,
            "hallucination_prevention_method": null,
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": "Often combined with Bayesian or ensemble surrogates, or with density-estimation methods (normalizing flows give likelihoods); the review advocates well-calibrated uncertainty estimators but does not prescribe a single implementation here.",
            "benchmark_dataset": null,
            "performance_metrics": null,
            "comparison_with_baseline": null,
            "validated_on_real_science": null,
            "novel_discoveries": null,
            "limitations": "Extrapolation in sparsely explored regions can be poor; requires careful validity checks for synthesizability and physical plausibility; may need large datasets or strong inductive biases to produce scientific-grade hypotheses.",
            "uuid": "e2505.0",
            "source_info": {
                "paper_title": "Scientific discovery in the age of artificial intelligence",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Reinforcement-learning symbolic regression",
            "name_full": "Reinforcement-learning approaches for symbolic regression and hypothesis search",
            "brief_description": "RL agents trained to sequentially generate symbolic expressions or discrete hypotheses by maximizing a reward that measures fit to data and other design criteria, enabling automated discovery of mathematical laws and refutations.",
            "citation_title": "Deep symbolic regression: recovering mathematical expressions from data via risk-seeking policy gradients",
            "mention_or_use": "mention",
            "system_name": "Reinforcement-learning symbolic regression",
            "system_description": "A neural policy (often an RNN or transformer) grows mathematical expressions token-by-token or expands parse-tree nodes; it is trained with RL (policy gradients, risk-seeking objectives) to maximize a reward defined by how well the expression fits observed data and optional parsimony/prior criteria. The review describes parse-tree-based RL and sequence-based RL for discovering formulas and for designing searches aiming to refute conjectures.",
            "system_type": "neural-symbolic (RL + sequence/parse models)",
            "scientific_domain": "physics, mathematics, general scientific modeling",
            "hypothesis_generation_method": "Sequential construction of symbolic expressions using an RL policy that selects grammar tokens or tree-expansion actions; exploration guided by reward signals (fit-to-data, information gain, simplicity).",
            "novelty_assessment_method": "Implicit via reward shaping (Occam's razor / parsimony) and by penalizing previously known forms in the reward; explicit novelty metrics not detailed in review.",
            "plausibility_assessment_method": "Data fit (residual/error), parsimony constraints, and possible downstream experimental/simulation checks; plausibility is enforced via reward and grammar constraints.",
            "novelty_plausibility_balance": "Handled by multi-term reward functions combining goodness-of-fit with penalties for complexity (Occam's razor) to prefer simpler plausible laws over overly novel but implausible ones.",
            "hypothesis_quality_metrics": "Reward functions based on fit-to-data (e.g., error/residual), syntactic validity; no standardized numerical metrics specified in the review.",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Evaluate candidate symbolic laws against held-out data, simulation, or follow-up experiments; the review cites examples where refutations or matches to observations are used as validation.",
            "reproducibility_measures": null,
            "hallucination_prevention_method": null,
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": null,
            "performance_metrics": null,
            "comparison_with_baseline": null,
            "validated_on_real_science": null,
            "novel_discoveries": "Review cites work where RL-based symbolic search found counterexamples or formulae consistent with data (references provided), but does not itself present a new experimentally validated discovery.",
            "limitations": "RL methods can get stuck in local optima and may not generalize well to regimes outside training/search experience; exploration strategies are needed to improve generalization.",
            "uuid": "e2505.1",
            "source_info": {
                "paper_title": "Scientific discovery in the age of artificial intelligence",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "GFlowNet",
            "name_full": "Generative Flow Network (GFlowNet)",
            "brief_description": "A generative framework that learns stochastic policies to sample complex discrete objects with probability proportional to a provided reward, promoting diverse high-reward hypothesis generation.",
            "citation_title": "GFlowNet foundations",
            "mention_or_use": "mention",
            "system_name": "Generative Flow Networks (GFlowNets)",
            "system_description": "GFlowNets learn a flow (policy) over compositional generation trajectories so that final objects are sampled with probability proportional to a scalar reward; they balance exploration and mode-covering behavior by training trajectory flows using objectives like flow matching or trajectory balance.",
            "system_type": "generative-flow-network",
            "scientific_domain": "drug discovery, biological sequence design, chemistry",
            "hypothesis_generation_method": "Sequential generative process guided by a learned policy that samples objects (molecules, sequences) with probabilities proportional to a reward function reflecting desired properties, enabling multimodal sampling instead of single-mode optimization.",
            "novelty_assessment_method": "Implicit â€” the mode-covering sampling objective encourages discovering diverse (novel) high-reward hypotheses across modes; explicit novelty metrics not specified in review.",
            "plausibility_assessment_method": "Reward functions typically include surrogate property predictors and constraints (e.g., synthesizability); plausibility assessed by downstream screening and experimental testing.",
            "novelty_plausibility_balance": "Achieved by designing reward functions that combine property scores (plausibility) with objectives that encourage diversity (novelty), e.g., via entropy-encouraging components.",
            "hypothesis_quality_metrics": null,
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Computational screening and, in referenced works, experimental validation of selected candidates; review cites biological sequence design and molecule design applications.",
            "reproducibility_measures": null,
            "hallucination_prevention_method": null,
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": null,
            "performance_metrics": null,
            "comparison_with_baseline": null,
            "validated_on_real_science": true,
            "novel_discoveries": "Referenced work applied GFlowNets to biological sequence and drug-design search spaces; the review notes successful applications but does not detail a particular novel experimentally validated finding within this review.",
            "limitations": "Requires carefully specified reward functions; computational cost and surrogate-model fidelity limit real-world success; balancing exploration and exploitation remains challenging.",
            "uuid": "e2505.2",
            "source_info": {
                "paper_title": "Scientific discovery in the age of artificial intelligence",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Grammar-VAE",
            "name_full": "Grammar Variational Autoencoder",
            "brief_description": "A VAE variant that embeds discrete structured objects (e.g., parse trees, SMILES strings) into a continuous latent space constrained by a context-free grammar, enabling syntactically valid latent optimization for symbolic laws or molecules.",
            "citation_title": "Grammar variational autoencoder",
            "mention_or_use": "mention",
            "system_name": "Grammar Variational Autoencoder (Grammar-VAE)",
            "system_description": "Maps discrete grammar-constrained structures into a continuous latent space; decoder constrained by grammar ensures syntactic validity; latent-space optimization (e.g., Bayesian optimization) then searches for high-scoring symbolic expressions or molecules.",
            "system_type": "latent-variable VAE (grammar-constrained)",
            "scientific_domain": "symbolic regression (physics/mathematics), molecular design (chemistry)",
            "hypothesis_generation_method": "Encode candidate symbolic parse trees into latent vectors, optimize latent vectors for desired objectives using Bayesian optimization or gradient methods, decode to syntactically valid candidates.",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Data fit and reward evaluation in latent space; decoded candidates evaluated by fit-to-data metrics or surrogate predictors.",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": null,
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Evaluate decoded symbolic expressions or molecules against data or property predictors; possible downstream experimental validation referenced in the literature.",
            "reproducibility_measures": null,
            "hallucination_prevention_method": null,
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": null,
            "performance_metrics": null,
            "comparison_with_baseline": null,
            "validated_on_real_science": null,
            "novel_discoveries": null,
            "limitations": "Latent-space optimization can produce decoded outputs that, while syntactically valid, may be implausible or out-of-distribution; relies on quality of decoder and latent representation.",
            "uuid": "e2505.3",
            "source_info": {
                "paper_title": "Scientific discovery in the age of artificial intelligence",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "SMILES-VAE / Latent molecular optimization",
            "name_full": "SMILES VAE and continuous molecular representations for optimization",
            "brief_description": "VAEs trained on molecular string representations (SMILES) that embed molecules in continuous latent spaces to enable gradient-based or Bayesian optimization for property maximization.",
            "citation_title": "Automatic chemical design using a data-driven continuous representation of molecules",
            "mention_or_use": "mention",
            "system_name": "SMILES-VAE (latent molecular optimization)",
            "system_description": "Encoder-decoder VAE mapping SMILES strings to continuous latent vectors; property predictors on latent space and Bayesian optimization or gradient-based search used to find latent points decoding to molecules with optimized properties; decoder returns discrete SMILES.",
            "system_type": "latent-variable VAE",
            "scientific_domain": "chemistry, drug discovery, materials design",
            "hypothesis_generation_method": "Latent-space optimization (Bayesian optimization, gradient ascent) guided by surrogate property predictors; decode optimized latent vectors to candidate molecules.",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Surrogate predictors, synthesizability filters and experimental follow-up as available; the review notes synthesizability is an important post-check.",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": null,
            "pre_experiment_evaluation": true,
            "validation_mechanism": "In-silico property prediction and ranking, then experimental synthesis/assay for selected candidates in referenced works.",
            "reproducibility_measures": null,
            "hallucination_prevention_method": null,
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": null,
            "performance_metrics": null,
            "comparison_with_baseline": null,
            "validated_on_real_science": true,
            "novel_discoveries": "Referenced literature applied continuous molecular representations to propose candidate molecules; the review mentions applications to small-molecule and protein design pipelines.",
            "limitations": "Decoder imperfections and mismatch between latent-space objectives and true experimental feasibility (synthesizability) can limit real-world success.",
            "uuid": "e2505.4",
            "source_info": {
                "paper_title": "Scientific discovery in the age of artificial intelligence",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "AlphaFold2",
            "name_full": "AlphaFold2",
            "brief_description": "A deep learning system that predicts 3D protein structures from amino-acid sequences with near-experimental accuracy, enabling generation and validation of structural hypotheses at scale.",
            "citation_title": "Highly accurate protein structure prediction with AlphaFold",
            "mention_or_use": "mention",
            "system_name": "AlphaFold2",
            "system_description": "End-to-end deep neural network (attention-based architecture with multiple modules and extensive use of MSAs and templates) that predicts atomic coordinates and confidence metrics for protein structures from sequence input.",
            "system_type": "sequence-to-structure deep learning (transformer-like / attention-based)",
            "scientific_domain": "structural biology, protein engineering, bioinformatics",
            "hypothesis_generation_method": "Predicts structure hypotheses (3D coordinates) directly from amino-acid sequence using trained model; can be used to generate candidate folds for unexplored sequences.",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Built-in per-residue confidence scores (e.g., pLDDT) and predicted alignment error allow assessment of structural plausibility; review references its atomic accuracy but does not detail metrics.",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": "Model-internal confidence metrics referenced generally in literature (not enumerated here); review emphasizes near-experimental accuracy but does not quantify within this text.",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Comparison to experimentally determined protein structures (X-ray, cryo-EM) in cited AlphaFold work; used as computational validation of structural hypotheses.",
            "reproducibility_measures": null,
            "hallucination_prevention_method": null,
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": "Internal confidence scores (discussed in primary AlphaFold literature; review calls for well-calibrated uncertainty estimators generally).",
            "benchmark_dataset": null,
            "performance_metrics": "The review states AlphaFold2 predicts 3D coordinates with atomic accuracy (citing the AlphaFold paper) but does not reproduce numeric metrics here.",
            "comparison_with_baseline": null,
            "validated_on_real_science": true,
            "novel_discoveries": "Cited as key breakthrough enabling structural hypotheses at scale; review references its large impact on structural biology.",
            "limitations": "May not generalize equally across all protein classes and can struggle with multi-protein complexes or low-homology sequences; review emphasizes need to measure uncertainty when deploying AI tools.",
            "uuid": "e2505.5",
            "source_info": {
                "paper_title": "Scientific discovery in the age of artificial intelligence",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Bayesian machine scientist",
            "name_full": "A Bayesian machine scientist to aid in the solution of challenging scientific problems",
            "brief_description": "A system that learns a posterior distribution over symbolic hypotheses, allowing sampling of hypotheses consistent with data and prior scientific knowledge for downstream testing.",
            "citation_title": "A Bayesian machine scientist to aid in the solution of challenging scientific problems",
            "mention_or_use": "mention",
            "system_name": "Bayesian machine scientist",
            "system_description": "Combines symbolic hypothesis representations with Bayesian inference to learn a posterior over candidate symbolic models; enables sampling and ranking of hypotheses compatible with observations and priors.",
            "system_type": "Bayesian symbolic / probabilistic programming",
            "scientific_domain": "general scientific modeling, symbolic discovery",
            "hypothesis_generation_method": "Bayesian posterior sampling over symbolic model space â€” propose symbolic models and weight them according to posterior probability conditioned on observed data and priors.",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Posterior probability naturally reflects plausibility given data and priors; low-posterior candidates are less plausible.",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": "Posterior probability (Bayesian evidence) used as an implicit quality metric.",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Compare sampled high-posterior models against held-out data and propose experiments to discriminate among posterior hypotheses.",
            "reproducibility_measures": null,
            "hallucination_prevention_method": null,
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": "Bayesian framework prioritizes posterior probabilities over p-value hypothesis testing; specific tests not detailed in review.",
            "uncertainty_quantification_method": "Full posterior over hypotheses (Bayesian uncertainty) enabling quantification of model uncertainty and hypothesis ensembles.",
            "benchmark_dataset": null,
            "performance_metrics": null,
            "comparison_with_baseline": null,
            "validated_on_real_science": null,
            "novel_discoveries": null,
            "limitations": "Scalability â€” Bayesian posterior exploration over large symbolic spaces is computationally challenging; requires careful priors and efficient sampling.",
            "uuid": "e2505.6",
            "source_info": {
                "paper_title": "Scientific discovery in the age of artificial intelligence",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Adaptive reinforced dynamics (neural uncertainty estimator)",
            "name_full": "Adaptive reinforced dynamics with neural-network-based uncertainty estimator for molecular dynamics",
            "brief_description": "A neural-network-guided sampling framework that uses an uncertainty estimator to add compensating potentials, enabling efficient escape from local minima and accelerated exploration of free-energy landscapes.",
            "citation_title": "Efficient sampling of high-dimensional free energy landscapes using adaptive reinforced dynamics",
            "mention_or_use": "mention",
            "system_name": "Adaptive reinforced dynamics (neural uncertainty estimator)",
            "system_description": "Uses a neural network to estimate uncertainty in sampled molecular configurations; where uncertainty is high, the algorithm adaptively adds potentials to flatten barriers and promote transitions, iteratively improving sampling coverage of configuration space.",
            "system_type": "neural-guided enhanced-sampling / uncertainty-driven dynamics",
            "scientific_domain": "molecular dynamics, computational chemistry, biophysics",
            "hypothesis_generation_method": "Not primarily generative of hypotheses; used to accelerate simulation-based evaluation of mechanistic hypotheses by exploring configurational landscapes more efficiently.",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Improved sampling yields better estimates of free-energy barriers and transition rates, improving plausibility assessment of mechanistic hypotheses via simulation.",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": null,
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Computational (molecular dynamics) validation: better sampling leads to more reliable computed observables; review cites improvement in sampling efficiency.",
            "reproducibility_measures": null,
            "hallucination_prevention_method": null,
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": "Neural-network-based uncertainty estimator used in the algorithm to locate regions of high epistemic uncertainty and guide adaptive potentials.",
            "benchmark_dataset": null,
            "performance_metrics": "Review notes orders-of-magnitude improvements in sampling efficiency for some settings (qualitative); no precise numeric metrics reproduced here.",
            "comparison_with_baseline": null,
            "validated_on_real_science": true,
            "novel_discoveries": "Used to accelerate exploration of protein conformational space and identify transitions more efficiently in cited work.",
            "limitations": "Effectiveness depends on quality/calibration of uncertainty estimator; may introduce bias if compensating potentials are not carefully controlled.",
            "uuid": "e2505.7",
            "source_info": {
                "paper_title": "Scientific discovery in the age of artificial intelligence",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Conditional VAE for Bayesian parameter estimation",
            "name_full": "Bayesian parameter estimation using conditional variational autoencoders",
            "brief_description": "A conditional VAE-based approach that approximates posterior distributions over model parameters for rapid Bayesian inference (example application: gravitational-wave parameter estimation).",
            "citation_title": "Bayesian parameter estimation using conditional variational autoencoders for gravitational-wave astronomy",
            "mention_or_use": "mention",
            "system_name": "Conditional Variational Autoencoder (CVAE) for Bayesian parameter estimation",
            "system_description": "Train a CVAE to map observed data to an approximate posterior over parameters by conditioning the encoder/decoder on observables; enables fast amortized Bayesian inference approximating full posterior distributions.",
            "system_type": "amortized Bayesian inference / conditional VAE",
            "scientific_domain": "astrophysics (gravitational-wave astronomy), general parameter estimation",
            "hypothesis_generation_method": "Generates posterior samples (parameter hypotheses) conditioned on observed data via the trained conditional generative model.",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Plausibility given data is encoded by posterior probability approximations; plausibility assessed by posterior density and comparison to standard Bayesian samplers.",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": "Posterior approximation quality evaluated by closeness to MCMC/ground-truth posteriors in cited work (not numerically reproduced in review).",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Compare CVAE posterior approximations with gold-standard Bayesian samplers; used to produce fast parameter posteriors for real events.",
            "reproducibility_measures": null,
            "hallucination_prevention_method": null,
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": "Direct posterior approximation (provides uncertainty estimates over parameters).",
            "benchmark_dataset": null,
            "performance_metrics": "Cited as massively faster than traditional samplers in referenced work (orders-of-magnitude speed-ups) but exact numbers not given in the review text.",
            "comparison_with_baseline": "Compared against traditional Bayesian samplers in cited gravitational-wave literature (improved speed with posterior-approximation fidelity), specifics not enumerated here.",
            "validated_on_real_science": true,
            "novel_discoveries": null,
            "limitations": "Amortized inference may produce biased posterior approximations if model is mis-specified or training data insufficient.",
            "uuid": "e2505.8",
            "source_info": {
                "paper_title": "Scientific discovery in the age of artificial intelligence",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Boltzmann generator / Normalizing flows",
            "name_full": "Boltzmann generators (normalizing flows for equilibrium sampling)",
            "brief_description": "Invertible deep networks (normalizing flows) trained to map simple priors to complex equilibrium distributions, enabling direct sampling of equilibrium states and density evaluation for many-body systems.",
            "citation_title": "Boltzmann generators: sampling equilibrium states of many-body systems with deep learning",
            "mention_or_use": "mention",
            "system_name": "Boltzmann generator (normalizing flows)",
            "system_description": "Series of invertible neural-network transformations mapping simple prior (e.g., Gaussian) to target equilibrium distribution; enables sampling and exact-density evaluation facilitating efficient generation of equilibrium configurations for simulation and hypothesis testing.",
            "system_type": "normalizing-flows / generative-density-estimation",
            "scientific_domain": "statistical physics, molecular simulation, lattice field theory",
            "hypothesis_generation_method": "Generates equilibrium-state samples that can be used to evaluate thermodynamic observables or to test mechanistic hypotheses by sampling rare configurations directly.",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Exact density evaluation allows importance-weighted estimates of observables and comparison to conventional MCMC; plausibility assessed via statistical consistency with physics-based observables.",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": null,
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Statistical comparison to ground-truth sampling (MCMC) and computation of thermodynamic observables; cited works report improved sampling of multimodal distributions.",
            "reproducibility_measures": null,
            "hallucination_prevention_method": null,
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": "Exact density evaluation gives principled likelihoods and enables uncertainty estimation on observables via importance weighting; review highlights this advantage.",
            "benchmark_dataset": null,
            "performance_metrics": null,
            "comparison_with_baseline": null,
            "validated_on_real_science": true,
            "novel_discoveries": "Applied to equilibrium-sampling tasks in many-body systems and lattice gauge theory in cited literature; review notes improved sampling efficiency in those contexts.",
            "limitations": "Often computationally expensive to train; may require many flows/layers and care to ensure invertibility and coverage of all modes.",
            "uuid": "e2505.9",
            "source_info": {
                "paper_title": "Scientific discovery in the age of artificial intelligence",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Autoencoder anomaly detection",
            "name_full": "Deep autoencoder-based unsupervised anomaly detection",
            "brief_description": "Unsupervised autoencoders trained to reconstruct background data produce higher reconstruction losses for out-of-distribution (rare) events, enabling real-time detection of anomalous scientific signals.",
            "citation_title": "Anomaly detection with robust deep autoencoders",
            "mention_or_use": "mention",
            "system_name": "Autoencoder anomaly detection",
            "system_description": "Train an autoencoder on predominant (background) event data; during deployment, compute reconstruction loss (anomaly score); events with high loss are flagged as anomalous candidates for further analysis or experimental follow-up.",
            "system_type": "unsupervised anomaly detection (autoencoder)",
            "scientific_domain": "high-energy physics, astronomy, Earth science, neuroscience",
            "hypothesis_generation_method": "Not primarily generative of hypotheses; used to detect rare/novel events that may represent new phenomena and therefore seed hypotheses.",
            "novelty_assessment_method": "Anomaly score (reconstruction loss) identifies deviation from background distribution as a proxy for novelty.",
            "plausibility_assessment_method": "Subsequent expert analysis, simulation, or downstream classifiers evaluate whether anomalies correspond to plausible new phenomena.",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": "Anomaly score distributions and false-positive/false-negative trade-offs are used to tune thresholds (specific metrics not enumerated in review).",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Retrospective validation on known rare-event datasets and hardware deployment tests (e.g., FPGA implementations cited for LHC use).",
            "reproducibility_measures": null,
            "hallucination_prevention_method": null,
            "hallucination_detection_method": "High anomaly scores flag candidate hallucinated/novel events for human/expert follow-up to filter false positives.",
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": "Anomaly score as a proxy for epistemic unfamiliarity; no full probabilistic uncertainty model described in review.",
            "benchmark_dataset": null,
            "performance_metrics": null,
            "comparison_with_baseline": null,
            "validated_on_real_science": true,
            "novel_discoveries": "Used in high-energy physics and other domains to identify rare events for follow-up; cited FPGA deployment for real-time LHC anomaly detection.",
            "limitations": "High false-positive rate possible; anomaly score does not by itself determine scientific significance â€” requires downstream interpretation and validation.",
            "uuid": "e2505.10",
            "source_info": {
                "paper_title": "Scientific discovery in the age of artificial intelligence",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Active learning + Bayesian optimization",
            "name_full": "Active learning combined with Bayesian optimization for experimental design and hypothesis testing",
            "brief_description": "Closed-loop strategies that use model uncertainty to select the most informative experiments or candidates, minimizing experiments needed while improving model accuracy and identifying promising hypotheses.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Active learning + Bayesian optimization",
            "system_description": "Active learning selects datapoints (experiments) where model uncertainty is high or expected information gain is maximal; Bayesian optimization uses surrogate models (often Gaussian processes or neural surrogates) to propose next experiments that maximize expected improvement or acquisition functions under cost constraints.",
            "system_type": "uncertainty-aware sequential decision-making / Bayesian optimization",
            "scientific_domain": "materials discovery, chemistry synthesis, protein engineering, accelerator tuning, general experimental sciences",
            "hypothesis_generation_method": "Not direct hypothesis generation; selects and refines hypotheses/candidates by iteratively querying experiments that are most informative about model predictions and parameter spaces.",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Uses model uncertainty and predictive distributions to prioritize plausible, informative experiments; acquisition functions encode trade-offs between exploration (novelty) and exploitation (plausibility).",
            "novelty_plausibility_balance": "Encoded in acquisition functions (e.g., expected improvement, upper confidence bound) that trade off exploration and exploitation.",
            "hypothesis_quality_metrics": "Acquisition function values, expected information gain, predictive uncertainty â€” specifics are method-dependent and not numerically enumerated in the review.",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Closed-loop experimental campaigns where selected candidates are experimentally tested and results are fed back to update models; review cites many practical applications.",
            "reproducibility_measures": null,
            "hallucination_prevention_method": "Using uncertainty-aware acquisition helps avoid overconfident selection of false-positive predictions (implicit prevention).",
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": "Explicit: surrogate-model predictive uncertainties (GP predictive variance, Bayesian neural net/posterior approximations, ensembles) used to drive selection.",
            "benchmark_dataset": null,
            "performance_metrics": null,
            "comparison_with_baseline": null,
            "validated_on_real_science": true,
            "novel_discoveries": "Many cited domain-specific successes in the review (materials, proteins, synthesis planning) from active-learning-guided studies.",
            "limitations": "Quality depends on uncertainty calibration of surrogate models and fidelity of acquisition functions; experimental noise and cost constraints limit efficiency.",
            "uuid": "e2505.11",
            "source_info": {
                "paper_title": "Scientific discovery in the age of artificial intelligence",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "De novo network hallucination (protein design)",
            "name_full": "De novo protein design by deep network hallucination",
            "brief_description": "A generative procedure using deep networks to â€˜hallucinateâ€™ protein sequences predicted to adopt novel, stable structures â€” a method for automated hypothesis generation in protein design.",
            "citation_title": "De novo protein design by deep network hallucination",
            "mention_or_use": "mention",
            "system_name": "Deep network hallucination (protein design)",
            "system_description": "Optimize sequence inputs to a structure-predicting network (or use generative networks) to produce sequences that the network predicts will fold into desired structures; coined 'hallucination' as networks output plausible but novel designs.",
            "system_type": "generative/optimization with structure predictor-in-the-loop",
            "scientific_domain": "protein engineering, structural biology",
            "hypothesis_generation_method": "Gradient-based or generative optimization of sequences to maximize structure-confidence or other predicted properties according to a pretrained structure or property predictor.",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Network-predicted structure confidence and downstream experimental characterization of designed proteins (in cited works) serve as plausibility checks.",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": null,
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Experimental synthesis and structural/functional assays in cited studies; review references this work as an example of AI-driven design.",
            "reproducibility_measures": null,
            "hallucination_prevention_method": null,
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": null,
            "performance_metrics": null,
            "comparison_with_baseline": null,
            "validated_on_real_science": true,
            "novel_discoveries": "Cited as enabling novel protein assemblies and designs validated experimentally in referenced literature.",
            "limitations": "Risk of producing designs that are artifacts of predictor biases (i.e., model hallucinations); requires experimental validation to confirm real-world fold and function.",
            "uuid": "e2505.12",
            "source_info": {
                "paper_title": "Scientific discovery in the age of artificial intelligence",
                "publication_date_yy_mm": "2023-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Deep symbolic regression: recovering mathematical expressions from data via risk-seeking policy gradients",
            "rating": 2,
            "sanitized_title": "deep_symbolic_regression_recovering_mathematical_expressions_from_data_via_riskseeking_policy_gradients"
        },
        {
            "paper_title": "GFlowNet foundations",
            "rating": 2,
            "sanitized_title": "gflownet_foundations"
        },
        {
            "paper_title": "Grammar variational autoencoder",
            "rating": 2,
            "sanitized_title": "grammar_variational_autoencoder"
        },
        {
            "paper_title": "Automatic chemical design using a data-driven continuous representation of molecules",
            "rating": 2,
            "sanitized_title": "automatic_chemical_design_using_a_datadriven_continuous_representation_of_molecules"
        },
        {
            "paper_title": "Highly accurate protein structure prediction with AlphaFold",
            "rating": 2,
            "sanitized_title": "highly_accurate_protein_structure_prediction_with_alphafold"
        },
        {
            "paper_title": "A Bayesian machine scientist to aid in the solution of challenging scientific problems",
            "rating": 2,
            "sanitized_title": "a_bayesian_machine_scientist_to_aid_in_the_solution_of_challenging_scientific_problems"
        },
        {
            "paper_title": "Efficient sampling of high-dimensional free energy landscapes using adaptive reinforced dynamics",
            "rating": 2,
            "sanitized_title": "efficient_sampling_of_highdimensional_free_energy_landscapes_using_adaptive_reinforced_dynamics"
        },
        {
            "paper_title": "Bayesian parameter estimation using conditional variational autoencoders for gravitational-wave astronomy",
            "rating": 2,
            "sanitized_title": "bayesian_parameter_estimation_using_conditional_variational_autoencoders_for_gravitationalwave_astronomy"
        },
        {
            "paper_title": "Boltzmann generators: sampling equilibrium states of many-body systems with deep learning",
            "rating": 2,
            "sanitized_title": "boltzmann_generators_sampling_equilibrium_states_of_manybody_systems_with_deep_learning"
        },
        {
            "paper_title": "Anomaly detection with robust deep autoencoders",
            "rating": 2,
            "sanitized_title": "anomaly_detection_with_robust_deep_autoencoders"
        }
    ],
    "cost": 0.027792249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Anima Anandkumar 2,13 , Karianne Bergen 11,12 , Carla P. Gomes 4 , Shirley Ho 14
Connor W. ColeyCopyright Connor W. Coley3 August 2023 | 47. Max Welling 30,31</p>
<p>Hanchen Wang 
Department of Engineering
University of Cambridge
CambridgeUK</p>
<p>Department of Computing and Mathematical Sciences
California Institute of Technology
PasadenaCAUSA</p>
<p>Department of Research and Early Development
Genentech Inc
South San FranciscoCAUSA</p>
<p>Department of Computer Science
Stanford University
StanfordCAUSA</p>
<p>Tianfan Fu 
Department of Computational Science and Engineering
Georgia Institute of Technology
AtlantaGAUSA</p>
<p>Yuanqi Du 
Department of Computer Science
Cornell University
IthacaNYUSA</p>
<p>Wenhao Gao 
Department of Chemical Engineering
Massachusetts Institute of Technology
CambridgeMAUSA</p>
<p>Kexin Huang 
Department of Computer Science
Stanford University
StanfordCAUSA</p>
<p>Ziming Liu 
Department of Physics
Massachusetts Institute of Technology
CambridgeMAUSA</p>
<p>Payal Chandak 
Harvard-MIT Program in Health Sciences and Technology
CambridgeMAUSA</p>
<p>Shengchao Liu 
Mila -Quebec AI Institute
MontrealQuebecCanada</p>
<p>UniversitÃ© de MontrÃ©al
MontrealQuebecCanada</p>
<p>Department of Earth, Environmental and Planetary Sciences
Brown University
ProvidenceRIUSA</p>
<p>Data Science Institute
Brown University
ProvidenceRIUSA</p>
<p>NVIDIA
Santa ClaraCAUSA</p>
<p>Center for Computational Astrophysics
Flatiron Institute
New YorkNYUSA</p>
<p>Department of Astrophysical Sciences
Princeton University
PrincetonNJUSA</p>
<p>Department of Physics
Carnegie Mellon University
PittsburghPAUSA</p>
<p>Department of Physics and Center for Data Science
New York University
New YorkNYUSA</p>
<p>Google DeepMind
LondonUK</p>
<p>Microsoft Research
BeijingChina</p>
<p>Department of Biomedical Informatics
Harvard Medical School
BostonMAUSA</p>
<p>Department of Systems Biology
Harvard Medical School
BostonMAUSA</p>
<p>Broad Institute of MIT and Harvard
CambridgeMAUSA</p>
<p>Deep Forest Sciences
Palo AltoCAUSA</p>
<p>BioMap
BeijingChina</p>
<p>Mohamed bin Zayed University of Artificial Intelligence
Abu DhabiUnited Arab Emirates</p>
<p>University of Illinois at Urbana-Champaign
ChampaignILUSA</p>
<p>HEC MontrÃ©al
MontrealQuebecCanada</p>
<p>CIFAR AI Chair
TorontoOntarioCanada</p>
<p>Department of Computer Science and Technology
University of Cambridge
CambridgeUK</p>
<p>University of Amsterdam
AmsterdamNetherlands</p>
<p>Microsoft Research Amsterdam
AmsterdamNetherlands</p>
<p>DP Technology
BeijingChina</p>
<p>AI for Science Institute
BeijingChina</p>
<p>Department of Electrical Engineering and Computer Science
Massachusetts Institute of Technology
CambridgeMAUSA</p>
<p>Harvard Data Science Initiative
CambridgeMAUSA</p>
<p>Study of Natural and Artificial Intelligence
Kempner Institute
Harvard University
CambridgeMAUSA</p>
<p>Anima Anandkumar 2,13 , Karianne Bergen 11,12 , Carla P. Gomes 4 , Shirley Ho 14</p>
<p>Bharath Ramsundar
Connor W. Coley6209363 August 2023 | 47. Max Welling 30,3110.1038/s41586-023-06221-2Review
Artificial intelligence (AI) is being increasingly integrated into scientific discovery to augment and accelerate research, helping scientists to generate hypotheses, design experiments, collect and interpret large datasets, and gain insights that might not have been possible using traditional scientific methods alone. Here we examine breakthroughs over the past decade that include self-supervised learning, which allows models to be trained on vast amounts of unlabelled data, and geometric deep learning, which leverages knowledge about the structure of scientific data to enhance model accuracy and efficiency. Generative AI methods can create designs, such as small-molecule drugs and proteins, by analysing diverse data modalities, including images and sequences. We discuss how these methods can help scientists throughout the scientific process and the central issues that remain despite such advances. Both developers and users of AI toolsneed a better understanding of when such approaches need improvement, and challenges posed by poor data quality and stewardship remain. These issues cut across scientific disciplines and require developing foundational algorithmic approaches that can contribute to scientific understanding or acquire it autonomously, making them critical areas of focus for AI innovation.</p>
<p>Review</p>
<p>indispensable tools for researchers by optimizing parameters and functions 4 , automating procedures to collect, visualize, and process data 5 , exploring vast spaces of candidate hypotheses to form theories 6 , and generating hypotheses and estimating their uncertainty to suggest relevant experiments 7 .</p>
<p>The power of AI methods has vastly increased since the early 2010s because of the availability of large datasets, aided by fast and massively parallel computing and storage hardware (graphics processing units and supercomputers) and coupled with new algorithms. The latter includes deep representation learning (Box 1), particularly multilayered neural networks capable of identifying essential, compact features that can simultaneously solve many tasks that underlie a scientific problem. Of these, geometric deep learning (Box 1) has proved to be helpful in integrating scientific knowledge, presented as compact mathematical statements of physical relationships, prior distributions, constraints and other complex descriptors, such as the geometry of atoms in molecules. Self-supervised learning (Box 1) has enabled neural networks trained on labelled or unlabelled data to transfer learned representations to a different domain with few labelled examples, for example, by pre-training large foundation models 8 and adapting them to solve diverse tasks across different domains. In addition, generative models (Box 1) can estimate the underlying data distribution of a complex system and support new designs. Distinct from other uses of AI, reinforcement-learning methods (Box 1) find optimal strategies for an environment by exploring many possible scenarios and assigning rewards to different actions based on metrics such as the information gain expected from a considered experiment.</p>
<p>In AI-driven scientific discovery, scientific knowledge can be incorporated into AI models using appropriate inductive biases (Box 1), which are assumptions representing structure, symmetry, constraints and prior knowledge as compact mathematical statements. However, applying these laws can lead to equations that are too complex for humans to solve, even with traditional numerical methods 9 . An emerging approach is incorporating scientific knowledge into AI models by including information about fundamental equations, such as the laws of physics or principles of molecular structure and binding in protein folding. Such inductive biases can enhance AI models by reducing the number of training examples needed to achieve the same level of accuracy 10 and scaling analyses to a vast space of unexplored scientific hypotheses 11 .</p>
<p>Using AI for scientific innovation and discovery presents unique challenges compared with other areas of human endeavour where AI is utilized. One of the biggest challenges is the vastness of hypothesis spaces in scientific problems, making systematic exploration infeasible. For instance, in biochemistry, an estimated 10 60 drug-like molecules exist to explore 12 . AI systems have the potential to revolutionize scientific workflows by accelerating processes and providing predictions with near-experimental accuracy. However, there are challenges to obtaining reliably annotated datasets for AI models, which can involve time-consuming and resource-intensive experimentation and simulations 13 . Despite these challenges, AI systems can enable efficient, intelligent and highly autonomous experimental design and data collection, where AI systems can operate under human supervision to assess, evaluate and act on results. Such capabilities have facilitated the development of artificially intelligent agents that continuously interact in dynamic environments and can, for example, make real-time decisions to navigate stratospheric balloons 14 . AI systems can play a valuable role in interpreting scientific datasets and extracting relationships and knowledge from scientific literature in a generalized manner. Recent findings demonstrate the potential for unsupervised language AI models to capture complex scientific concepts 15 , such as the periodic table, and predict applications of functional materials years before their discovery, suggesting that latent knowledge regarding future discoveries may be embedded in past publications.</p>
<p>Recent advances, including the successful unraveling of the 50-year-old protein-folding problem 10 and AI-driven simulations of molecular systems with millions of particles 16 , demonstrate the potential of AI to address challenging scientific problems. However, the remarkable promise of discovery is accompanied by significant challenges for the emerging field of 'AI for Science' (AI4Science). As with any new technology, the success of AI4Science depends on our ability to integrate it into routine practices and understand its potential and limitations. Barriers to the widespread adoption of AI in scientific discovery include internal and external factors specific to each stage of the discovery process and concerns regarding the utility of methods, theory, software and hardware, as well as potential misuse. We explore the developments and address critical questions in AI4Science, including the conduct of science, traditional scepticism and implementation challenges.</p>
<p>AI-aided data collection and curation for scientific research</p>
<p>The ever-increasing scale and complexity of datasets collected by experimental platforms have led to a growing dependence on real-time processing and high-performance computing in scientific research to selectively store and analyse data generated at high rates 17 .</p>
<p>Data selection</p>
<p>A typical particle collision experiment generates over 100 terabytes of data every second 18 . Such scientific experiments are pushing the limits of existing data transmission and storage technologies. In these physics experiments, over 99.99% of raw instrument data represents background events that must be detected in real time and discarded to manage the data rates 18 . To identify rare events for future scientific enquiry, deep-learning methods 18   Scientific discovery is a multifaceted process that involves several interconnected stages, including hypothesis formation, experimental design, data collection and analysis. AI is poised to reshape scientific discovery by augmenting and accelerating research at each stage of this process. The principles and illustrative studies shown here highlight the contributions to enhance scientific understanding and discovery.</p>
<p>be missed during compression. The background processes can be modelled generatively using a deep autoencoder 19 (Box 1). The autoencoder 20 returns a higher loss value (anomaly score) for previously unseen signals (rare events) that are out of the background distribution. Unlike supervised anomaly detection, unsupervised anomaly detection does not require annotations and has been widely used in physics 21,22 , neuroscience 23 , Earth science 24 , oceanography 25 and astronomy 26 .</p>
<p>Data annotation</p>
<p>Training supervised models requires datasets with annotated labels that provide supervised information to guide model training and estimate a function or a conditional distribution over target variables from inputs. Pseudo-labelling 27 and label propagation 28 are enticing alternatives to laborious data labelling, allowing automatic annotation of massive unlabelled datasets based on only a small set of accurate annotations. In biology, techniques that assign functional and structural labels to newly characterized molecules are vital for downstream training of supervised models owing to the difficulty of experimentally generating labels. For instance, less than 1% of sequenced proteins is annotated with biological functions despite the proliferation of next-generation sequencing 29 . Another strategy for data labelling leverages surrogate models trained on manually labelled data to annotate unlabelled samples and uses these predicted pseudo-labels to supervise downstream predictive models. In contrast, label propagation diffuses labels to unlabelled samples via similarity graphs constructed based on feature embeddings 13,30 (Box 1). In addition to automatic labelling, active learning 31-33 (Box 1) can identify the most informative data points to be labelled by humans or the most informative experiments to be performed. This approach allows models to be trained with fewer expert-provided labels. Another strategy in data annotation is to develop labelling rules that leverage domain knowledge 34,35 .</p>
<p>Box 1</p>
<p>Glossary Active learning can improve AI models by selecting the most informative training points when data labelling is costly. Bayesian optimization is a sequential strategy used for optimizing expensive black-box functions and often works with active learning to determine the next query to the black-box function.</p>
<p>An autoencoder is a neural architecture that learns a compressed representation of unlabelled data, consisting of an encoder (which maps data to a representation) and a decoder (which reconstructs data from the representation).</p>
<p>Data augmentation is a strategy that enhances model robustness and generalizability by creating new data samples from existing ones. This process can involve substituting tokens in a sequence, altering the visual aspects of images, or changing atomic positions, always to preserve the essential information. This technique not only increases the diversity of the data but also its volume, thereby aiding in the training of models.</p>
<p>Distribution shift is a prevalent issue in the application of AI methods, whereby the underlying data distribution that an algorithm was initially trained on differs from the distribution of the data it encounters during implementation.</p>
<p>End-to-end learning uses differentiable components, such as neural network modules, to directly connect raw inputs to outputs, avoiding the need for handcrafted input features and enabling direct generation of predictions from inputs.</p>
<p>Generative models estimate a probability distribution of the underlying data and can then generate new samples from that distribution. Examples include variational autoencoders, generative adversarial networks, normalizing flows, diffusion models and generative pretrained transformers.</p>
<p>Geometric deep learning is a field of machine learning that deals with geometric data, such as graphs or manifolds. It typically preserves the invariance of geometric data under transformations and can be applied to 3D structures.</p>
<p>Inductive bias refers to a set of assumptions or preferences that guide the decision-making process of AI models, such as translation equivariance in convolutional networks.</p>
<p>An inverse problem is a scientific or mathematical challenge where the goal is to decipher the underlying causes or parameters that resulted in a specific observation or dataset. Instead of a direct, forward prediction from cause to effect, inverse problems operate in the opposite direction, seeking to deduce the original conditions from the resulting observation. These problems are often complicated due to non-uniqueness and instability, where multiple sets of causes can lead to similar outcomes and minor changes in data can drastically alter the solution.</p>
<p>Physics-informed AI refers to techniques that incorporate physical laws into AI models as a form of prior knowledge.</p>
<p>Reinforcement learning involves sequential decision-making and is represented as a Markov decision process comprising an agent, a set of states, a space of actions, an environment (which determines how the state changes with actions) and a reward function. The reinforcement-learning agent is trained to choose optimal actions based on a state that results in the maximum expected cumulative reward.</p>
<p>Representation learning techniques automatically generate representations of data such as images, documents, sequences or graphs. These representations are typically dense, compact vectors, referred to as embeddings or latent vectors, optimized to capture essential features of input data.</p>
<p>Self-supervised learning is a training strategy for learning from unlabelled data. Generative self-supervised learning, for example, involves predicting a part of the raw data based on the rest, whereas contrastive self-supervised learning involves defining positive and negative views of the input and then aligning the positives and separating the negatives. Both approaches aim to enhance the model's ability to learn meaningful features without needing labelled data.</p>
<p>Surrogate models are analytically tractable models to approximate properties of complex systems.</p>
<p>Symmetries. Equivariance, also known as covariance in physics, characterizes the symmetry of functions. An equivariant function transforms the input equivalently under an operation from a particular group. Invariance is another form of symmetry where a function is invariant to a group of transformations if the output remains unchanged when the inputs are transformed.</p>
<p>A transformer is a neural architecture that uses attention for parallel processing of sequential data via a series of steps. At every step, the attention mechanism selects and combines elements from the previous-step sequence, forming a new representation for each position in the sequence in a differentiable and soft manner.</p>
<p>Weakly supervised learning leverages imperfect, partial or noisy forms of supervision, such as biased or imprecise labels, to train AI models.</p>
<p>Review</p>
<p>Data generation</p>
<p>Deep-learning performance improves with increased quality, diversity and scale 36 of training datasets 37,38 . A fruitful approach to creating better models is to augment training datasets by generating additional synthetic data points through automatic data augmentation and deep generative models. In addition to manually designing such data augmentations (Box 1), reinforcement-learning methods 39 can discover a policy for automatic data augmentation 40,41 that is flexible and agnostic of downstream models. Deep generative models, including variational autoencoders, generative adversarial networks, normalizing flows and diffusion models, learn the underlying data distribution and can sample training points from the optimized distribution. Generative adversarial networks (Box 1) have proven to be beneficial for scientific images because they synthesize realistic images in many domains ranging from particle collision events 42 , pathology slides 43 , chest X-rays 44 , magnetic resonance contrasts 45 , three-dimensional (3D) material microstructure 46 , protein functions 47,48 to genetic sequences 49 . An emerging technique in generative modelling is probabilistic programming 50 , in which data generation models are expressed as computer programs.</p>
<p>Data refinements</p>
<p>Precision instruments such as ultrahigh-resolution lasers and non-invasive microscopy systems enable direct measurement of physical quantities or indirect measurement by calculating real-world objects, producing highly accurate results. AI techniques have significantly increased measurement resolution, reduced noise and eliminated errors in measuring roundness, resulting in high precision consistent across sites. Examples of AI applications in scientific experiments include visualization regions of spacetime such as black holes 5 , capturing a physics particle collision 51 , improving the resolution of live-cell images 52 and better detection of cell types across biological contexts 53 . Deep convolutional methods, which utilize algorithmic advances such as spectral deconvolution 54,55 , flexible sparsity 52 and generative capability 56 , can transform poor spatiotemporally resolved measurements into high-quality, super-resolved and structured images. An important AI task in various scientific disciplines is denoising, which involves differentiating relevant signals from noise and learning to remove noise. Denoising autoencoders 57 can project high-dimensional input data into more compact representations of essential features. These autoencoders minimize the difference between uncorrupted input data points and their reconstruction from the compressed representation of their noise-corrupted version. Other forms of distribution-learning autoencoders, such as variational autoencoders (VAEs; Box 1) 58 , are also frequently used. VAEs learn a stochastic representation via latent autoencoding that retains essential data features while ignoring non-essential sources of variation, probably representing random noise. For example, in single-cell genomics, autoencoders optimizing count-based vectors of gene activation across millions of cells 59 are routinely used to improve protein-RNA expression analyses.</p>
<p>Learning meaningful representations of scientific data</p>
<p>Deep learning can extract meaningful representations of scientific data at various levels of abstraction and optimize them to guide research, often through end-to-end learning (Box 1). A high-quality representation should retain as much information about the data as possible while remaining simple and accessible 60 . Scientifically meaningful representations are compact 21 , discriminative 61 , disentangle underlying factors of variation 62 and encode underlying mechanisms that generalize across numerous tasks 63,64 . Here we introduce three emerging strategies that fulfil these requirements: geometric priors, self-supervised learning and language modelling.</p>
<p>Geometric priors</p>
<p>Integrating geometric priors 65 in learned representations has proved effective as geometry and structure play a central role in scientific domains [66][67][68] . Symmetry is a widely studied concept in geometry 69 . It can be described in terms of invariance and equivariance (Box 1) to represent the behaviour of a mathematical function, such as a neural feature encoder, under a group of transformations, such as the SE(3) group in rigid body dynamics. Important structural properties, such as the secondary structural content of molecular systems, solvent accessibility, residue compactness and hydrogen-bonding patterns, are invariant to spatial orientations. In the analysis of scientific images, objects do not change when translated in the image, meaning that image segmentation masks are translationally equivariant as they change equivalently when input pixels are translated. Incorporating symmetry into models can benefit using AI with limited labelled datasets, such as 3D RNA and protein structures 70,71 , by augmenting training samples, and can improve extrapolative prediction to inputs markedly different than those encountered during model training.</p>
<p>Geometric deep learning</p>
<p>Graph neural networks have emerged as a leading approach for deep learning on datasets with underlying geometric and relational structure [72][73][74][75][76] (Fig. 2a). In a broader sense, geometric deep learning involves discovering relational patterns 65 and equipping neural network models with inductive biases that explicitly make use of localized information encoded in the form of graphs and transformation groups [77][78][79] through neural message-passing algorithms [80][81][82][83][84] . Depending on the scientific problem, various graph representations were developed to capture complex systems [85][86][87] . Directional edges can facilitate the physical modelling of glassy systems 88 , hypergraphs with edges connecting multiple nodes are used in chromatin structure understanding 89 , models trained on multimodal graphs are used to create predictive models in genomics 90 , and sparse, irregular and highly relational graphs have been applied to a number of Large Hadron Collider physics tasks, including the reconstruction of particles from detector readouts and the discrimination of physics signals against background processes 91 .</p>
<p>Self-supervised learning</p>
<p>Supervised learning may be insufficient when only a few labelled samples are available for model training or when labelling data for a specific task is prohibitively expensive. In such cases, leveraging both labelled and unlabelled data can improve model performance and learning capacity. Self-supervised learning is a technique that enables models to learn the general features of a dataset without relying on explicit labels. Effective self-supervised strategies include predicting occluded regions of images, forecasting past or future frames in a video, and using contrastive learning to teach the model to distinguish between similar and dissimilar data points 92 (Fig. 2b). Self-supervised learning can be a crucial pre-processing step to learn transferable features in large unlabelled datasets 92-95 before fine-tuning models on small labelled datasets to perform downstream tasks. Such pretrained models [96][97][98] with a broad understanding of a scientific domain are general-purpose predictors that can be adapted for various tasks, thereby improving label efficiency and surpassing purely supervised methods 8 .</p>
<p>Language modelling</p>
<p>Masked-language modelling is a popular method for self-supervised learning of both natural language and biological sequences (Fig. 2c). The arrangement of atoms or amino acids (tokens) into structures to produce molecular and biological function is similar to how letters form words and sentences to define the meaning of a document. As both natural language and biological sequence processing continue to evolve, they inform the development of each other. In the training process, the goal is to predict the next token in a sequence, whereas in masked-based training 99 , the self-supervised task is to recover a masked token in a sequence using a bidirectional sequence context. Protein language models can encode amino acid sequences to capture structural and functional properties 100,101 and evaluate the evolutionary fitness of viral variants 102 . Such representations are transferable across various tasks, ranging from sequence design 103-105 to structure prediction 10,106 . In handling biochemical sequences 107-109 , chemical language models facilitate efficient exploration of vast chemical space 110,111 . They have been used to predict properties 112 , plan multi-step syntheses 113,114 and explore the space of chemical reactions [115][116][117] .</p>
<p>Transformer architectures</p>
<p>Transformers (Box 1) 118 are neural architecture models that can process sequences of tokens by flexibly modelling interactions between arbitrary token pairs, surpassing earlier efforts using recurrent neural networks for sequential modelling. Transformers dominate natural language processing 37,99 and have been successfully applied to a range of problems, including earthquake signal detection 119 , DNA and protein sequence modelling 10,120 , modelling the effect of sequence variation on biological function 100,121 , and symbolic regression 122 . Although transformers unify graph neural networks and language models 123-125 , the run-time and memory footprint of transformers can scale quadratically with the length of sequences, leading to efficiency challenges addressed by long-range modelling 120 and linearized attention mechanisms 126 . As a result, unsupervised or self-supervised generative pretrained transformers, followed by parameter-efficient fine-tuning, are widely used.</p>
<p>Neural operators</p>
<p>Standard neural network models can be inadequate for scientific applications as they assume a fixed data discretization. This approach is unsuitable for many scientific datasets collected at varying resolutions and grids. Moreover, data are often sampled from an underlying physical phenomenon in a continuous domain, such as seismic activity or fluid flow. Neural operators learn representations invariant to discretization by learning mappings between function spaces 127,128 . Neural operators are guaranteed to be discretization invariant, meaning that they can work on any discretization of inputs and converge to a limit upon mesh refinement. Once neural operators are trained, they can be evaluated at any resolution without the need for re-training. In contrast, the performance of standard neural networks can degrade when data resolution during deployment changes from model training.</p>
<p>AI-based generation of scientific hypotheses</p>
<p>Testable hypotheses are central to scientific discovery. They can take many forms, from symbolic expressions in mathematics to molecules in chemistry and genetic variants in biology. Formulating meaningful hypotheses can be a laborious process, as exemplified by Johannes Kepler, who spent four years analysing stellar and planetary data before arriving at a hypothesis that led to the discovery of the laws of planetary motion 129 . AI methods can be helpful at several stages of this process. They can generate hypotheses by identifying candidate symbolic expressions from noisy observations. They can help design objects, such as a molecule that binds to a therapeutic target 130  and symmetry, such as molecules and materials, by leveraging graphs and employing neural message-passing strategies. This approach generates latent representations (embeddings) by exchanging neural messages along edges within graphs while considering other geometric priors, such as invariance and equivariance constraints. As a result, geometric deep learning can incorporate complex structural information into deep-learning models, allowing for a better understanding and manipulation of the underlying geometric datasets. b, To effectively represent diverse samples such as satellite images, it is crucial to capture both their similarities and differences. Self-supervised learning strategies, such as contrastive learning, achieve this by generating augmented counterparts and aligning positive while separating negative pairs. This iterative process enhances the embeddings, leading to informative latent representations and better performance on downstream prediction tasks. c, Masked-language modelling effectively captures the semantics of sequential data, such as natural language and biological sequences. This approach involves feeding masked elements of the input into a transformer block, which includes pre-processing steps, such as positional encodings. The self-attention mechanism, represented by grey lines with colour intensity reflecting the magnitude of attention weights, combines representations of non-masked input to accurately predict the masked input. This approach produces high-quality representations of sequences by repeating this autocompletion process across many elements of the input.</p>
<p>Review</p>
<p>counterexample that contradicts a mathematical conjecture 9 , suggesting experimental evaluation in the laboratory. Moreover, AI systems can learn a Bayesian posterior distribution (Box 1) of hypotheses and use it to generate hypotheses compatible with scientific data and knowledge 131 .</p>
<p>Black-box predictors of scientific hypotheses</p>
<p>Identifying promising hypotheses for scientific enquiry requires efficiently examining many candidates and selecting those that can maximize the yield of downstream simulations and experimentation.</p>
<p>In drug discovery, high-throughput screening can assess thousands to millions of molecules, and algorithms can prioritize which molecules to investigate experimentally 132 . Models can be trained to anticipate the utility of an experiment, such as relevant molecular properties 133,134 or symbolic formulas that fit the observations 122 . However, experimental ground-truth data for these predictors may be unavailable for many molecules. Thus, weak supervision-learning approaches (Box 1) can be used to train these models, where noisy, limited or imprecise supervision is used as a training signal. These serve as a cost-effective proxy for annotations from human experts, expensive in silico calculations or higher-fidelity experiments (Fig. 3a). AI methods trained on high-fidelity simulations have been used to efficiently screen large libraries of molecules, such as 1.6 million organic-light-emitting-diode material candidates 133 and 11 billion synthon-based ligand candidates 134 . In genomics, transformer architectures trained to predict gene expression values from DNA sequences can help prioritize genetic variants 120 . In particle physics, identifying intrinsic charm quarks in protons involves screening all possible structures and fitting experimental data on each candidate structure 135 . To further increase the efficiency of these processes, AI-selected candidates can be sent to medium or low-throughput experiments for continual refinement of candidates using experimental feedback. The results can be fed back into the AI models using active learning 136 and Bayesian optimization 137 (Box 1), allowing the algorithms to refine their predictions and focus on the most promising candidates.</p>
<p>AI methods have become invaluable when hypotheses involve complex objects such as molecules. For instance, in protein folding, Alpha-Fold2 10 can predict the 3D atom coordinates of proteins from amino acid sequences with atomic accuracy, even for proteins whose structure is unlike any of the proteins in the training dataset. This breakthrough has led to the development of various AI-driven protein-folding methods, such as RoseTTAFold 106 . In addition to forward problems, AI approaches are increasingly used for inverse problems that aim to understand the causal factors that produced a set of observations. Inverse problems, such as inverse folding or fixed backbone design, can predict the amino acid sequence from the protein's backbone 3D atom coordinates using a black-box predictor trained on millions of protein structures 105 . However, such black-box AI predictors require large training datasets and offer limited interpretability despite reducing the dependence on the availability of prior scientific knowledge.</p>
<p>Navigating combinatorial hypothesis spaces</p>
<p>Although sampling all the hypotheses compatible with the data is daunting, a manageable goal is to search for a single good one, which </p>
<p>Fig. 3 | AI-guided generation of scientific hypotheses. a,</p>
<p>High-throughput screening involves using AI predictors trained on experimentally generated datasets to select a small number of screened objects with desirable properties, thus reducing the size of the total candidate pool by orders of magnitude. This approach can leverage self-supervised learning to pre-train predictors on vast amounts of unscreened objects, followed by fine-tuning predictors on datasets of screened objects with labelled readouts. Laboratory evaluations and uncertainty quantification can refine this approach to streamline the screening process, making it more cost effective and time efficient, ultimately accelerating the identification of candidate chemical compounds, materials and biomolecules. b, The AI navigator employs rewards predicted by reinforcementlearning agents and design criteria, such as Occam's razor, to focus on the most promising elements of a candidate hypothesis during symbolic regression. Shown is an example illustrating the inference of the mathematical expression representing Newton's gravitational law. The low-scoring search routes are shown as grey branches in the symbolic expression tree. Guided by actions associated with the highest predicted rewards, this iterative process converges towards mathematical expressions consistent with the data and satisfying other design criteria. c, AI differentiators are autoencoder models that map discrete objects, such as chemical compounds, to points in a differentiable, continuous latent space. This space allows for the optimization of the objects, such as selecting compounds from a vast chemical library that maximize a specific biochemical endpoint. The idealized landscape plot depicts the learned latent space, with deeper colours indicating regions enriched for objects with higher predicted scores. By leveraging this latent space, the AI differentiator can efficiently identify objects that maximize the desired property indicated by the red star.</p>
<p>can be formulated as an optimization problem. Instead of traditional methods that rely on manually engineered rules 138 , AI policies can be used to estimate the reward of each search and prioritize search directions with higher values. An agent trained by a reinforcement-learning algorithm is typically employed to learn the policy. The agent learns to take actions in the search space that maximize a reward signal, which can be defined to reflect the quality of the generated hypotheses or other relevant criteria.</p>
<p>To solve the optimization problem, a symbolic regression task can be solved using evolutionary algorithms, which generate random symbolic laws as the initial set of solutions. Within each generation, slight variations are imposed on candidate solutions. The algorithm checks whether any modification produced a symbolic law that fits the observations better than prior solutions, keeping the best ones for the next generation 139 . However, reinforcement-learning approaches are increasingly replacing this standard strategy. Reinforcement learning uses neural networks to generate a mathematical expression sequentially by adding mathematical symbols from a predefined vocabulary and using the learned policy to decide which notation symbol to be added next 140 . The mathematical formula is represented as a parse tree. The learned policy takes the parse tree as input to determine what leaf node to expand and what notation (from the vocabulary) to add (Fig. 3b). Another approach for using neural networks to solve mathematical problems is transforming a mathematical formula into a binary sequence of symbols. A neural network policy can then probabilistically and sequentially grow the sequence one binary character at a time 6 . By designing a reward that measures the ability to refute the conjecture, this approach can find a refutation to a mathematical conjecture without prior knowledge about the mathematical problem.</p>
<p>Combinatorial optimization also applies to tasks such as discovering molecules with desirable pharmaceutical properties, where each step in molecular design is a discrete decision-making process. In this process, a partially generated molecular graph is given as input to the learned policy, making discrete choices on where to add a new atom and which atom to add at the selected position in the molecule. By iteratively performing this process, the policy can generate a series of possible molecular structures evaluated based on their fitness to the target properties. The search space is too vast to explore all possible combinations, but reinforcement learning can efficiently guide the search by prioritizing the most promising branches worth investigating [141][142][143][144][145] . Reinforcement-learning methods can be trained with a training objective that encourages the resulting policy to sample from all reasonable solutions (with a high reward) rather than to focus on a single good solution, as is the case with standard reward maximization in reinforcement learning [144][145][146] . These reinforcement-learning approaches have been successfully applied to various optimization problems, including maximizing protein expression 147 , planning hydropower to reduce adverse impact in the Amazon Basin 148 and exploring the parameter space of particle accelerators 33 .</p>
<p>Policies learned by AI agents have foresighted actions that seemed unconventional initially but proved to be effective 149 . For instance, in mathematics, supervised models can identify patterns and relations between mathematical objects and help guide intuition and propose conjectures 9 . These analyses have pointed to previously unknown patterns or even new models of the world. However, reinforcement-learning methods may not generalize well to unseen data during model training, as the agent may get stuck in a local optimum once it finds a sequence of actions that work well. To improve generalization, some exploration strategy is required to collect broader search trajectories that could help the agent perform better in new and modified settings.</p>
<p>Optimizing differentiable hypothesis spaces</p>
<p>Scientific hypotheses often take the form of discrete objects, such as symbolic formulas in physics or chemical compounds in pharmaceutical and materials science. Although combinatorial optimization techniques have been successful for some of these problems, a differentiable space can also be used for optimization as it is amenable to gradient-based methods, which can efficiently find local optima. To enable the use of gradient-based optimization, two approaches are frequently used. The first is to use models such as VAEs to map discrete candidate hypotheses to points in a latent differentiable space. The second approach is to relax discrete hypotheses into differentiable objects that can be optimized in the differentiable space. This relaxation can take different forms, such as replacing discrete variables with continuous ones or using a soft version of the original constraints.</p>
<p>Applications of symbolic regression in physics use grammar VAEs 150 . These models represent discrete symbolic expressions as parse trees using context-free grammar and map the trees into a differentiable latent space. Bayesian optimization is then employed to optimize the latent space for symbolic laws while ensuring that the expressions are syntactically valid. In a related study, Brunton and colleagues 151 introduced a method for differentiating symbolic rules by assigning trainable weights to predefined basis functions. Sparse regression was used to select a linear combination of the basis functions that accurately represented the dynamic system while maintaining compactness. Unlike equivariant neural networks, which use a predefined inductive bias to enforce symmetry, symmetry can be discovered as the characteristic behaviour of a domain. For instance, Liu and Tegmark 152 described asymmetry as a smooth loss function and minimized the loss function to extract previously unknown symmetries. This approach was applied to uncover hidden symmetries in black-hole waveform datasets, revealing unexpected space-time structures that were historically challenging to find.</p>
<p>In astrophysics, VAEs have been used to estimate gravitational-wave detector parameters based on pretrained black-hole waveform models. This method is up to six orders of magnitude faster than traditional methods, making it practical to capture transient gravitational-wave events 153 . In materials science, thermodynamic rules are combined with an autoencoder to design an interpretable latent space for identifying phase maps of crystal structures 154 . In chemistry, models such as simplified molecular-input line-entry system (SMILES)-VAE 155 can transform SMILES strings, which are molecular notations of chemical structures in the form of a discrete series of symbols that computers can easily understand, into a differentiable latent space that can be optimized using Bayesian optimization techniques (Fig. 3c). By representing molecular structures as points in the latent space, we can design differentiable objectives and optimize them using self-supervised learning to predict molecular properties based on latent representations of molecules. This means that we can optimize discrete molecular structures by backpropagating gradients of the AI predictor all the way to the continuous-valued representation of molecular inputs. A decoder can turn these molecular representations into approximately corresponding discrete inputs. This approach is used in the design of proteins 156 and small molecules 157,158 .</p>
<p>Performing optimization in the latent space can more flexibly model underlying data distributions than mechanistic approaches in the original hypothesis space. However, extrapolative prediction in sparsely explored regions of the hypothesis space can be poor. In many scientific disciplines, hypothesis spaces can be vastly larger than what can be examined through experimentation. For instance, it is estimated that there are approximately 10 60 molecules, whereas even the largest chemical libraries contain fewer than 10 10 molecules 12,159 . Therefore, there is a pressing need for methods to efficiently search through and identify high-quality candidate solutions in these largely unexplored regions.</p>
<p>AI-driven experimentation and simulation</p>
<p>Evaluating scientific hypotheses through experimentation is critical to scientific discovery. However, laboratory experiments can be costly and impractical. Computer simulations have emerged as a promising alternative, offering the potential for more efficient and flexible experimentation. While simulations rely on handcrafted parameters and heuristics to imitate real-world scenarios, they require a trade-off between accuracy and speed compared with physical experimentation, necessitating understanding the underlying mechanisms. However, with the advent of deep learning, these challenges are being addressed by identifying and optimizing hypotheses for efficient testing and empowering computer simulations to link observations with hypotheses.</p>
<p>Efficient evaluation of scientific hypotheses</p>
<p>AI systems have provided experimental design and optimization tools, which can enhance traditional scientific methods, decrease the number of experiments needed and save resources. Specifically, AI systems can assist with two essential steps of experimental testing: planning and steering. In traditional approaches, these steps often require trial and error, which can be inefficient, costly and even life-threatening at times 160 . AI planning provides a systematic approach to designing experiments, optimizing their efficiency and exploring uncharted territory. At the same time, AI steering directs experimental processes towards high-yield hypotheses, allowing the system to learn from previous observations and adjust the course of experiments. These AI approaches can be model based, using simulations and prior knowledge, or model free, based on machine-learning algorithms alone.</p>
<p>AI systems can aid in the planning of experiments by optimizing the use of resources and reducing unnecessary investigations. Unlike hypothesis searching, experimental planning pertains to the procedures and steps involved in the design of scientific experiments.</p>
<p>One example is synthesis planning in chemistry. Synthesis planning involves finding a sequence of steps by which a target chemical compound can be synthesized from available chemicals. AI systems can design synthetic routes to a desired chemical compound, reducing the need for human intervention 161,162 . Active learning has also been employed in materials discovery and synthesis 32,[163][164][165] . Active learning involves iteratively interacting with and learning from experimental feedback to refine hypotheses. Material synthesis is a complex and resource-intensive process that requires efficient exploration of high-dimensional parameter space. Active learning uses uncertainty estimation to explore the parameter space and reduce uncertainty with as few steps as possible 165 .</p>
<p>During an ongoing experiment, decision-making must often be adapted in real time. However, this process can be difficult and error prone when driven solely by human experience and intuition. Reinforcement learning provides an alternative approach that can continually react to the evolving environment and maximize the safety and success of the experiments. For example, reinforcement-learning approaches have proven to be effective for magnetic control of tokamak plasmas, where the algorithm interacts with the tokamak simulator to optimize a policy for controlling the process 166 (Fig. 4a). In another study, a reinforcement-learning agent used real-time feedback such as wind speed and solar elevation to control a stratospheric balloon and find favourable wind currents for navigation 14 . In quantum physics, experiment design needs to be dynamically adjusted as the best choice for a future materialization of a complex experiment can be counterintuitive. Reinforcement-learning methods can overcome this issue by iteratively designing the experiment and receiving feedback from it. For instance, reinforcement-learning algorithms have been  166 developed an AI controller to regulate nuclear fusion through magnetic fields in a tokamak reactor. The AI agent receives real-time measurements of electrical voltage levels and plasma configurations and takes actions to control the magnetic field and meet experimental targets, such as maintaining a functional power supply. The controller is trained using simulations with a reward function to update model parameters. b, In computational simulations of complex systems, AI systems can accelerate the detection of rare events, such as transitions between different conformational structures of a protein. Wang et al. 169 used a neural-network-based uncertainty estimator to guide the addition of potentials that compensate for the original potential energy, allowing the system to escape local minima (in grey) and explore a configuration space more rapidly. This approach, illustrated here, can enhance the efficiency and accuracy of simulations, leading to a deeper understanding of complex biological phenomena. c, A neural framework for solving partial differential equations, where the AI solver is a physics-informed neural network trained to estimate target function f. The derivative of variable x is calculated by automatically differentiating the neural network's outputs. When the expression for the differential equation is unknown (parameterized by Î·), it can be estimated by solving a multi-objective loss that optimizes both the functional form of the equation and its fit to observations y. Credit: Nuclear fusion icon in a, iStockphoto/VectorMine. used to optimize the measurement and control of quantum systems, where they improve experimental efficiency and accuracy 167 .</p>
<p>Deducing observables from hypotheses using simulations</p>
<p>Computer simulation is a powerful tool to deduce observables from hypotheses, enabling the evaluation of hypotheses that are not directly testable. However, existing simulation techniques heavily rely on human understanding and knowledge about the underlying mechanisms of the studied systems, which can be suboptimal and inefficient. AI systems can enhance computer simulation with more accurate and efficient learning by better fitting key parameters of complex systems, solving differential equations that govern complex systems and modelling states in complex systems.</p>
<p>Scientists often study complex systems by creating a model that involves parameterized forms, which requires domain knowledge to identify initial symbolic expressions for the parameters. An example is molecular force fields, which are interpretable but limited in their ability to represent a wide range of functions and require strong inductive biases or scientific knowledge to generate. To improve the accuracy of molecular simulations, an AI-based neural potential that fits expensive yet accurate quantum-mechanical data has been developed to replace traditional force fields 16,168 . In addition, uncertainty quantification has been used to locate the energy barrier in the high-dimensional free-energy surface, thereby improving the efficiency of molecular dynamics 169 (Fig. 4b). For coarse-grained molecular dynamics, AI models have been utilized to reduce the computational cost for large systems by determining the degree to which the system needs to be coarsened from the learned hidden complex structures 170 . In quantum physics, neural networks have replaced manually estimated symbolic forms in parameterizing wave functions or density functionals due to their flexibility and ability to fit the data accurately 171,172 .</p>
<p>Differential equations are crucial for modelling complex systems' dynamics in space and time. In contrast to numerical algebra solvers, AI-based neural solvers integrate data and physics more seamlessly 173,174 . These neural solvers combine physics with the flexibility of deep learning by grounding neural networks in domain knowledge (Fig. 4c). AI methods have been applied to solve differential equations in various fields, including computational fluid dynamics 175 , predicting the structures of glassy systems 88 , solving stiff chemical kinetic problems 176 and solving the Eikonal equation to characterize the travel times of seismic waves 177,178 . In dynamics modelling, continuous time can be modelled by neural ordinary differential equations 179 . Neural networks can parameterize solutions of Navier-Stokes equations in a spatiotemporal domain using physics-informed losses 180 . However, standard convolutional neural networks have limited ability to model fine-structured characteristics of the solution. This issue can be addressed by learning operators that model mappings between functions using neural networks 127,181 . In addition, solvers must be able to adapt to different domains and boundary conditions. This can be achieved by combining neural differential equations with graph neural networks to discretize arbitrary by graph partitioning 182 .</p>
<p>Statistical modelling is a powerful tool to provide a full quantitative description of complex systems by modelling the distributions of states in those systems. Owing to its capability to capture highly complex distributions, deep generative modelling has recently emerged as a valuable approach in complex system simulations. One well known example is the Boltzmann generator 183 based on normalizing flows 184,185 (Box 1). Normalizing flows can map any complex distribution to a prior distribution (for example, a simple Gaussian) and back using a series of invertible neural networks. Although computationally expensive (often requiring hundreds or thousands of neural layers), normalizing flows provide an exact density function, which enables sampling and training. Unlike conventional simulations, normalizing flows can generate equilibrium states by directly sampling from the prior distribution and applying the neural network, which has a fixed computational cost. This enhances sampling in the lattice field 186 and gauge theories 187 and improves Markov chain Monte Carlo methods 188 that otherwise might not converge due to mode mixing [189][190][191] .</p>
<p>Grand challenges</p>
<p>To harness scientific data, models must be built and employed with simulation and human expertise. Such integration has opened up opportunities for scientific discovery. However, to further enhance the impact of AI across scientific disciplines, significant progress is needed in theory, methods, software and hardware infrastructure. Cross-disciplinary collaborations are crucial to realize a comprehensive and practical approach towards advancing science through AI.</p>
<p>Practical considerations</p>
<p>Scientific datasets are often not directly amenable to AI analyses because of measurement technology limitations that produce incomplete datasets and biased or conflicting readouts, and limited accessibility owing to privacy and safety concerns. Standardized and transparent formats are required to alleviate the workload of data processing 159,[192][193][194][195][196] . Model cards 197 and datasheets 198 are examples of efforts to document the operating characteristics of scientific datasets and models. In addition, federated learning 199,200 and cryptographic 201 algorithms can be used to prevent releasing sensitive data with high commercial value to the public domain. Leveraging open scientific literature, natural language processing and knowledge graph techniques can facilitate literature mining to support material discovery 15 , chemistry synthesis 202 and therapeutic science 203 .</p>
<p>The use of deep learning poses a complex challenge for humanin-the-loop AI-driven design, discovery and evaluation. To automate scientific workflows, optimize large-scale simulation codes and operate instruments, autonomous robotic control can leverage predictions and conduct experiments on high-throughput synthesis and testing lines, creating self-driving laboratories. The early application of generative models in materials exploration suggests that millions of possible materials could be identified with desired properties and functions and evaluated for synthesizability. For instance, King et al. 204 combined logical AI and robotics to autonomously generate functional genomics hypotheses about yeast and experimentally test the hypotheses using laboratory automation. In chemical synthesis, AI optimizes candidate synthesis routes, followed by robots steering chemical reactions in predicted synthesis routes 7 .</p>
<p>The practical implementation of an AI system involves complex software and hardware engineering, requiring a series of interdependent steps that go from data curation and processing to algorithm implementation and design of user and application interfaces. Minor variations in implementation can lead to considerable changes in performance and impact the success of integrating AI models within scientific practice. Therefore, both data and model standardization needs to be considered. AI approaches can suffer from reproducibility due to the stochastic nature of model training, varying model parameters and evolving training datasets, which are both data dependent and task dependent. Standardized benchmarks and experimental design can alleviate such issues 205 . Another direction towards improving reproducibility is through open-source initiatives that release open models, datasets and education programmes 4,130,206,207 .</p>
<p>Algorithmic innovations</p>
<p>To contribute to scientific understanding or acquire it autonomously, algorithmic innovation is required to establish a foundational ecosystem with the most appropriate algorithms for use throughout the scientific process.</p>
<p>The question of out-of-distribution generalization is at the frontier of AI research. A neural network trained on data from a specific regime may discover regularities that do not generalize in a different regime Review whose underlying distribution has shifted (Box 1). Although many scientific laws are not universal, their applicability is generally broad. Compared with state-of-the-art AI, human brains can better and faster generalize to modified settings. An attractive hypothesis is that this is because humans build not just a statistical model of what they observe but a causal model, that is, a family of statistical models indexed by all possible interventions (for example, different initial states, actions of agents or different regimes). Incorporating causality in AI is still a young field [208][209][210][211][212] where much remains to be done. Techniques such as self-supervised learning have great potential for scientific problems because they can leverage massive unlabelled data and transfer their knowledge to low-data regimes. However, current transfer-learning schemes can be ad hoc, lack theoretical guidance 213 and are vulnerable to shifts in underlying distributions 214 . Although preliminary attempts have addressed this challenge 215,216 , more exploration is needed to systematically measure transferability across domains and prevent negative transfer. Moreover, to address the difficulties that scientists care about, the development and evaluation of AI methods must be done in real-world scenarios, such as plausibly realizable synthesis paths in drug design 217,218 , and include well calibrated uncertainty estimators to assess the model's reliability before transitioning it to real-world implementation.</p>
<p>Scientific data are multimodal and include images (such as black-hole images in cosmology), natural language (such as scientific literature), time series (such as thermal yellowing of materials), sequences (such as biological sequences), graphs (such as complex systems) and structures (such as 3D protein-ligand conformations). For instance, in high-energy physics, jets are collimated sprays of particles produced from quarks and gluons at high energy. Identifying their substructures from radiation patterns can aid in the search for new physics. The jet substructures can be described by images, sequences, binary trees, generic graphs and sets of tensors 18 . Although using neural networks to process images has been extensively researched, processing particle images alone is insufficient. Similarly, using other representations of jet substructures in isolation cannot give a holistic and integrated systems view of the complex system 219 . Although integrating multimodal observations remains a challenge, the modular nature of neural networks implies that distinct neural modules can transform diverse data modalities into universal vector representations 220,221 .</p>
<p>Scientific knowledge, such as rotational equivariance in molecules 77 , equality constraints in mathematics 182 , disease mechanisms in biology 222 and multi-scale structures in complex systems 223,224 , can be incorporated into AI models. However, which principles and knowledge are most helpful and practical to implement is still unclear. As AI models require massive data to fit, incorporating scientific knowledge into models can aid learning when datasets are small or sparsely annotated. Therefore, research must establish principled methods for integrating knowledge into AI models and understanding the trade-offs between domain knowledge and learning from measured data.</p>
<p>AI methods often operate as black boxes, meaning that users cannot fully explain how outputs have been generated and what inputs have been critical in producing the outputs. Black-box models can decrease user trust in predictions and have limited applicability in areas where model outputs must be understood before real-world implementation [225][226][227] , such as in human space exploration 228 , and where predictions inform policy, such as in climate science 229 . Transparent deep-learning models remain elusive 230 despite a plethora of explainability techniques [231][232][233] . However, the fact that human brains can synthesize high-level explanations, even if imperfect, that can convince other humans offers hope that by modelling phenomena at similarly high levels of abstraction, future AI models will provide interpretable explanations at least as valuable as those offered by human brains. This also suggests that studying higher-level cognition could potentially inspire future deep-learning models to incorporate both current deep-learning abilities and the abilities to manipulate verbalizable abstractions, reason causally, and generalize out of distribution.</p>
<p>Conduct of science and scientific enterprise</p>
<p>Looking towards the future, the demand for AI expertise will be influenced by two forces. First, the existence of problems that are on the verge of benefiting from the application of AI-such as self-driving labs. Second, the ability of intelligent tools to enhance the state-of-the-art and create new opportunities-such as examining biological, chemical or physical processes that happen at length and time scales not accessible in experiments. On the basis of these two forces, we anticipate that research teams will change in composition to include AI specialists, software and hardware engineers, and novel forms of collaboration involving government at all levels, educational institutions and corporations. Recent state-of-the-art deep-learning models continue to grow in size 10,234 . These models consist of millions or even billions of parameters and have experienced a tenfold increase in size year on year. Training these models involves transmitting data through complex parameterized mathematical operations, with parameters updated to nudge the model outputs towards the desired values. However, the computational and data requirements to calculate these updates are colossal, resulting in a large energy footprint and high computational costs. As a result, big tech companies have heavily invested in computational infrastructure and cloud services, pushing the limits on scale and efficiency. Although for-profit and non-academic organizations have access to vast computational infrastructure, higher-education institutions can be better integrated across multiple disciplines. Furthermore, academic institutions tend to host unique historical databases and measurement technology that might not exist elsewhere but are necessary for AI4Science. These complementary assets have facilitated new modes of industry-academia partnerships, which can impact the selection of research questions pursued.</p>
<p>As AI systems approach performance that rivals and surpasses humans, employing it as a drop-in replacement for routine laboratory work is becoming feasible. This approach enables researchers to develop predictive models from experimental data iteratively and select experiments to improve them without manually performing laborious and repetitive tasks 217,235 . To support this paradigm shift, educational programmes are emerging to train scientists in designing, implementing and applying laboratory automation and AI in scientific research. These programmes help scientists understand when the use of AI is appropriate and to prevent misinterpreted conclusions from AI analyses.</p>
<p>The misapplication of AI tools and misinterpretation of their results can have significant negative impacts 236 . The broad range of applications compounds these risks 237 . However, the misuse of AI is not solely a technological problem; it also depends on the incentives of those leading AI innovation and investing in AI implementation. Establishing ethics review processes and responsible implementation tactics is essential, including a comprehensive overview of the scope and applicability of AI 238 . Furthermore, security risks associated with AI must be considered, as it has become easier to repurpose algorithm implementations for dual use 237 . As algorithms are adaptable to a broad range of applications, they can be developed for one purpose but used for another, creating vulnerabilities to threats and manipulation.</p>
<p>Conclusion</p>
<p>AI systems can contribute to scientific understanding, enable the investigation of processes and objects that cannot be visualized or probed in any other way, and systematically inspire ideas by building models from data and combining them with simulation and scalable computing. To realize this potential, safety and security concerns that come with the use of AI must be addressed through responsible and thoughtful deployment of the technology. To use AI responsibly in scientific research, we need to measure the levels of uncertainty, error, and utility of AI systems. This understanding is essential for accurately interpreting AI outputs and ensuring that we do not rely too heavily on potentially flawed results. As AI systems continue to evolve, prioritizing reliable implementation with proper safeguards in place is key to minimizing risks and maximizing benefits. AI has the potential to unlock scientific discoveries that were previously out of reach.</p>
<p>Fig. 1 |
1Science in the age of artificial intelligence.</p>
<p>Fig. 4 |
4Integration of AI with scientific experiments and simulation. a, Leveraging AI for nuclear fusion control of complex and dynamic systems: Degrave et al.</p>
<p>replace pre-programmed hardware event triggers with algorithms that search for outlying signals to detect unforeseen or rare phenomena that may otherwiseExperiments </p>
<p>Hypotheses </p>
<p>Rare event selection 
in particle collisions </p>
<p>Language modelling for 
biomedical sequences </p>
<p>High-throughput 
virtual screening </p>
<p>Navigation in the 
hypothesis space </p>
<p>Battery design 
optimization </p>
<p>Weather forecasting </p>
<p>Magnetic control of 
nuclear fusion reactors </p>
<p>Planning chemical 
synthesis pathway </p>
<p>Neural solvers of 
differential equations </p>
<p>Observations </p>
<p>Super-resolution 3D 
live-cell imaging </p>
<p>Synthetic electronic 
health record generation </p>
<p>Symbolic regression </p>
<p>Hydropower station 
location planning </p>
<p>AI for science </p>
<p>or a a Geometric deep learning Mask parts out c Masked-language modellingFig. 2 | Learning meaningful representations of scientific data. a, Geometric deep learning integrates information about scientific data's geometry, structureRepel </p>
<p>Attract </p>
<p>Attract </p>
<p>Extract 
embeddings </p>
<p>Augmentation </p>
<p>Embedding space </p>
<p>b Self-supervised learning </p>
<p>Predict the 
masked parts </p>
<p>Encode with 
self-attention </p>
<p>Extract 
embeddings </p>
<p>Graph 
abstraction </p>
<p>Message passing 
Geometric prior </p>
<p>Labelled parts 
Masked parts </p>
<p>Â© Springer Nature Limited 2023
Author contributions All authors contributed to the design and writing of the paper, helped shape the research, provided critical feedback, and commented on the paper and its revisions. H.W., T.F., Y.D. and M.Z conceived the study and were responsible for overall direction and planning. W.G., K.H. and Z.L. contributed equally to this work (equal second authorship) and are listed alphabetically.Competing interestsThe authors declare no competing interests.Additional informationCorrespondence and requests for materials should be addressed to Marinka Zitnik. Peer review information Nature thanks Brian Gallagher and Benjamin Nachman for their contribution to the peer review of this work. Reprints and permissions information is available at http://www.nature.com/reprints. Publisher's note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.Springer Nature or its licensor (e.g. a society or other partner) holds exclusive rights to this article under a publishing agreement with the author(s) or other rightsholder(s); author self-archiving of the accepted manuscript version of this article is solely governed by the terms of such publishing agreement and applicable law.
Deep learning. Y Lecun, Y Bengio, G Hinton, Nature. 521LeCun, Y., Bengio, Y. &amp; Hinton, G. Deep learning. Nature 521, 436-444 (2015).</p>
<p>This survey summarizes key elements of deep learning and its development in speech recognition, computer vision and and natural language processing. This survey summarizes key elements of deep learning and its development in speech recognition, computer vision and and natural language processing.</p>
<p>Understanding, values, and the aims of science. H W De Regt, Phil. Sci. 87de Regt, H. W. Understanding, values, and the aims of science. Phil. Sci. 87, 921-932 (2020).</p>
<p>J V Pickstone, Ways of Knowing: A New History of Science, Technology, and Medicine. Univ. Chicago PressPickstone, J. V. Ways of Knowing: A New History of Science, Technology, and Medicine (Univ. Chicago Press, 2001).</p>
<p>Deep potential: a general representation of a many-body potential energy surface. J Han, Commun. Comput. Phys. 23Han, J. et al. Deep potential: a general representation of a many-body potential energy surface. Commun. Comput. Phys. 23, 629-639 (2018).</p>
<p>This paper introduced a deep neural network architecture that learns the potential energy surface of many-body systems while respecting the underlying symmetries of the system by incorporating group theory. This paper introduced a deep neural network architecture that learns the potential energy surface of many-body systems while respecting the underlying symmetries of the system by incorporating group theory.</p>
<p>First M87 Event Horizon Telescope results. IV. Imaging the central supermassive black hole. K Akiyama, Astrophys. J. Lett. 8754Akiyama, K. et al. First M87 Event Horizon Telescope results. IV. Imaging the central supermassive black hole. Astrophys. J. Lett. 875, L4 (2019).</p>
<p>Constructions in combinatorics via neural networks. A Z Wagner, Preprint atWagner, A. Z. Constructions in combinatorics via neural networks. Preprint at https:// arxiv.org/abs/2104.14516 (2021).</p>
<p>A robotic platform for flow synthesis of organic compounds informed by AI planning. C W Coley, Science. 3651566Coley, C. W. et al. A robotic platform for flow synthesis of organic compounds informed by AI planning. Science 365, eaax1566 (2019).</p>
<p>On the opportunities and risks of foundation models. R Bommasani, Preprint atBommasani, R. et al. On the opportunities and risks of foundation models. Preprint at https://arxiv.org/abs/2108.07258 (2021).</p>
<p>Advancing mathematics by guiding human intuition with AI. A Davies, Nature. 600Davies, A. et al. Advancing mathematics by guiding human intuition with AI. Nature 600, 70-74 (2021).</p>
<p>This paper explores how AI can aid the development of pure mathematics by guiding mathematical intuition. This paper explores how AI can aid the development of pure mathematics by guiding mathematical intuition.</p>
<p>Highly accurate protein structure prediction with AlphaFold. J Jumper, Nature. 596Jumper, J. et al. Highly accurate protein structure prediction with AlphaFold. Nature 596, 583-589 (2021).</p>
<p>This study was the first to demonstrate the ability to predict protein folding structures using AI methods with a high degree of accuracy. achieving results that are at or near the experimental resolution. This accomplishment is particularly noteworthy, as predicting protein folding has been a grand challenge in the field of molecular biology for over 50 yearsThis study was the first to demonstrate the ability to predict protein folding structures using AI methods with a high degree of accuracy, achieving results that are at or near the experimental resolution. This accomplishment is particularly noteworthy, as predicting protein folding has been a grand challenge in the field of molecular biology for over 50 years.</p>
<p>A deep learning approach to antibiotic discovery. J M Stokes, Cell. 180Stokes, J. M. et al. A deep learning approach to antibiotic discovery. Cell 180, 688-702 (2020).</p>
<p>The art and practice of structure-based drug design: a molecular modeling perspective. R S Bohacek, C Mcmartin, W C Guida, Med. Res. Rev. 16Bohacek, R. S., McMartin, C. &amp; Guida, W. C. The art and practice of structure-based drug design: a molecular modeling perspective. Med. Res. Rev. 16, 3-50 (1996).</p>
<p>Using deep learning to annotate the protein universe. M L Bileschi, Nat. Biotechnol. 40Bileschi, M. L. et al. Using deep learning to annotate the protein universe. Nat. Biotechnol. 40, 932-937 (2022).</p>
<p>This paper describes a reinforcement-learning algorithm for navigating a superpressure balloon in the stratosphere, making real-time decisions in the changing environment. M G Bellemare, Nature. 588Autonomous navigation of stratospheric balloons using reinforcement learningBellemare, M. G. et al. Autonomous navigation of stratospheric balloons using reinforcement learning. Nature 588, 77-82 (2020). This paper describes a reinforcement-learning algorithm for navigating a super- pressure balloon in the stratosphere, making real-time decisions in the changing environment.</p>
<p>Unsupervised word embeddings capture latent knowledge from materials science literature. V Tshitoyan, Nature. 571Tshitoyan, V. et al. Unsupervised word embeddings capture latent knowledge from materials science literature. Nature 571, 95-98 (2019).</p>
<p>Deep potential molecular dynamics: a scalable model with the accuracy of quantum mechanics. L Zhang, Phys. Rev. Lett. 120143001Zhang, L. et al. Deep potential molecular dynamics: a scalable model with the accuracy of quantum mechanics. Phys. Rev. Lett. 120, 143001 (2018).</p>
<p>Applications and techniques for fast machine learning in science. A M Deiana, Front. Big Data. 5787421Deiana, A. M. et al. Applications and techniques for fast machine learning in science. Front. Big Data 5, 787421 (2022).</p>
<p>Machine learning in the search for new fundamental physics. G Karagiorgi, Nat. Rev. Phys. 4Karagiorgi, G. et al. Machine learning in the search for new fundamental physics. Nat. Rev. Phys. 4, 399-412 (2022).</p>
<p>Anomaly detection with robust deep autoencoders. C Zhou, R C Paffenroth, ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. Zhou, C. &amp; Paffenroth, R. C. Anomaly detection with robust deep autoencoders. In ACM SIGKDD International Conference on Knowledge Discovery and Data Mining 665-674 (2017).</p>
<p>Reducing the dimensionality of data with neural networks. G E Hinton, R R Salakhutdinov, Science. 313Hinton, G. E. &amp; Salakhutdinov, R. R. Reducing the dimensionality of data with neural networks. Science 313, 504-507 (2006).</p>
<p>The LHC Olympics 2020 a community challenge for anomaly detection in high energy physics. G Kasieczka, Rep. Prog. Phys. 84124201Kasieczka, G. et al. The LHC Olympics 2020 a community challenge for anomaly detection in high energy physics. Rep. Prog. Phys. 84, 124201 (2021).</p>
<p>Autoencoders on field-programmable gate arrays for real-time, unsupervised new physics detection at 40 MHz at the Large Hadron Collider. E Govorkova, Nat. Mach. Intell. 4Govorkova, E. et al. Autoencoders on field-programmable gate arrays for real-time, unsupervised new physics detection at 40 MHz at the Large Hadron Collider. Nat. Mach. Intell. 4, 154-161 (2022).</p>
<p>Detecting microstructural deviations in individuals with deep diffusion MRI tractometry. M Chamberland, Nat. Comput. Sci. 1Chamberland, M. et al. Detecting microstructural deviations in individuals with deep diffusion MRI tractometry. Nat. Comput. Sci. 1, 598-606 (2021).</p>
<p>Delegated regressor, a robust approach for automated anomaly detection in the soil radon time series data. M Rafique, Sci. Rep. 103004Rafique, M. et al. Delegated regressor, a robust approach for automated anomaly detection in the soil radon time series data. Sci. Rep. 10, 3004 (2020).</p>
<p>Annotation-free learning of plankton for classification and anomaly detection. V P Pastore, Sci. Rep. 1012142Pastore, V. P. et al. Annotation-free learning of plankton for classification and anomaly detection. Sci. Rep. 10, 12142 (2020).</p>
<p>A recurrent neural network for classification of unevenly sampled variable stars. B Naul, Nat. Astron. 2Naul, B. et al. A recurrent neural network for classification of unevenly sampled variable stars. Nat. Astron. 2, 151-155 (2018).</p>
<p>Pseudo-label: the simple and efficient semi-supervised learning method for deep neural networks. D.-H Lee, In ICML Workshop on Challenges in Representation Learning. Lee, D.-H. et al. Pseudo-label: the simple and efficient semi-supervised learning method for deep neural networks. In ICML Workshop on Challenges in Representation Learning (2013).</p>
<p>Learning with local and global consistency. D Zhou, Advances in Neural Information Processing Systems. 16Zhou, D. et al. Learning with local and global consistency. In Advances in Neural Information Processing Systems 16, 321-328 (2003).</p>
<p>A large-scale evaluation of computational protein function prediction. P Radivojac, Nat. Methods. 10Radivojac, P. et al. A large-scale evaluation of computational protein function prediction. Nat. Methods 10, 221-227 (2013).</p>
<p>Joint analysis of heterogeneous single-cell RNA-seq dataset collections. N Barkas, Nat. Methods. 16Barkas, N. et al. Joint analysis of heterogeneous single-cell RNA-seq dataset collections. Nat. Methods 16, 695-698 (2019).</p>
<p>Active learning across intermetallics to guide discovery of electrocatalysts for CO 2 reduction and H 2 evolution. K Tran, Z W Ulissi, Nat. Catal. 1Tran, K. &amp; Ulissi, Z. W. Active learning across intermetallics to guide discovery of electrocatalysts for CO 2 reduction and H 2 evolution. Nat. Catal. 1, 696-703 (2018).</p>
<p>Bias free multiobjective active learning for materials design and discovery. K M Jablonka, Nat. Commun. 122312Jablonka, K. M. et al. Bias free multiobjective active learning for materials design and discovery. Nat. Commun. 12, 2312 (2021).</p>
<p>Turn-key constrained parameter space exploration for particle accelerators using Bayesian active learning. R Roussel, Nat. Commun. 125612Roussel, R. et al. Turn-key constrained parameter space exploration for particle accelerators using Bayesian active learning. Nat. Commun. 12, 5612 (2021).</p>
<p>Data programming: creating large training sets, quickly. A J Ratner, Advances in Neural Information Processing Systems. 29Ratner, A. J. et al. Data programming: creating large training sets, quickly. In Advances in Neural Information Processing Systems 29, 3567-3575 (2016).</p>
<p>This paper presents a weakly-supervised AI system designed to annotate massive amounts of data using labeling functions. A Ratner, International Conference on Very Large Data Bases. 11Snorkel: rapid training data creation with weak supervisionRatner, A. et al. Snorkel: rapid training data creation with weak supervision. In International Conference on Very Large Data Bases 11, 269-282 (2017). This paper presents a weakly-supervised AI system designed to annotate massive amounts of data using labeling functions.</p>
<p>GANplifying event samples. A Butter, SciPost Phys. 10139Butter, A. et al. GANplifying event samples. SciPost Phys. 10, 139 (2021).</p>
<p>Language models are few-shot learners. T Brown, Advances in Neural Information Processing Systems. 33Brown, T. et al. Language models are few-shot learners. In Advances in Neural Information Processing Systems 33, 1877-1901 (2020).</p>
<p>Zero-shot text-to-image generation. A Ramesh, International Conference on Machine Learning. 139Ramesh, A. et al. Zero-shot text-to-image generation. In International Conference on Machine Learning 139, 8821-8831 (2021).</p>
<p>Reinforcement learning improves behaviour from evaluative feedback. M L Littman, Nature. 521Littman, M. L. Reinforcement learning improves behaviour from evaluative feedback. Nature 521, 445-451 (2015).</p>
<p>Autoaugment: learning augmentation strategies from data. E D Cubuk, IEEE Conference on Computer Vision and Pattern Recognition. Cubuk, E. D. et al. Autoaugment: learning augmentation strategies from data. In IEEE Conference on Computer Vision and Pattern Recognition 113-123 (2019).</p>
<p>Selfaugment: automatic augmentation policies for self-supervised learning. C J Reed, IEEE Conference on Computer Vision and Pattern Recognition. Reed, C. J. et al. Selfaugment: automatic augmentation policies for self-supervised learning. In IEEE Conference on Computer Vision and Pattern Recognition 2674-2683 (2021).</p>
<p>Deep generative models for fast photon shower simulation in ATLAS. Preprint atATLAS Collaboration et al. Deep generative models for fast photon shower simulation in ATLAS. Preprint at https://arxiv.org/abs/2210.06204 (2022).</p>
<p>Deep adversarial training for multi-organ nuclei segmentation in histopathology images. F Mahmood, IEEE Trans. Med. Imaging. 39Mahmood, F. et al. Deep adversarial training for multi-organ nuclei segmentation in histopathology images. IEEE Trans. Med. Imaging 39, 3257-3267 (2019).</p>
<p>Generating synthetic X-ray images of a person from the surface geometry. B Teixeira, IEEE Conference on Computer Vision and Pattern Recognition. Teixeira, B. et al. Generating synthetic X-ray images of a person from the surface geometry. In IEEE Conference on Computer Vision and Pattern Recognition 9059-9067 (2018).</p>
<p>Assessing the importance of magnetic resonance contrasts using collaborative generative adversarial networks. D Lee, W.-J Moon, J C Ye, Nat. Mach. Intell. 2Lee, D., Moon, W.-J. &amp; Ye, J. C. Assessing the importance of magnetic resonance contrasts using collaborative generative adversarial networks. Nat. Mach. Intell. 2, 34-42 (2020).</p>
<p>Generating three-dimensional structures from a twodimensional slice with generative adversarial network-based dimensionality expansion. S Kench, S J Cooper, Nat. Mach. Intell. 3Kench, S. &amp; Cooper, S. J. Generating three-dimensional structures from a two- dimensional slice with generative adversarial network-based dimensionality expansion. Nat. Mach. Intell. 3, 299-305 (2021).</p>
<p>Protein function prediction is improved by creating synthetic feature samples with generative adversarial networks. C Wan, D T Jones, Nat. Mach. Intell. 2Wan, C. &amp; Jones, D. T. Protein function prediction is improved by creating synthetic feature samples with generative adversarial networks. Nat. Mach. Intell. 2, 540-550 (2020).</p>
<p>Expanding functional protein sequence spaces using generative adversarial networks. D Repecka, Nat. Mach. Intell. 3Repecka, D. et al. Expanding functional protein sequence spaces using generative adversarial networks. Nat. Mach. Intell. 3, 324-333 (2021).</p>
<p>Realistic in silico generation and augmentation of single-cell RNA-seq data using generative adversarial networks. M Marouf, Nat. Commun. 11166Marouf, M. et al. Realistic in silico generation and augmentation of single-cell RNA-seq data using generative adversarial networks. Nat. Commun. 11, 166 (2020).</p>
<p>Probabilistic machine learning and artificial intelligence. Z Ghahramani, Nature. 521Ghahramani, Z. Probabilistic machine learning and artificial intelligence. Nature 521, 452-459 (2015).</p>
<p>This survey provides an introduction to probabilistic machine learning, which involves the representation and manipulation of uncertainty in models and predictions, playing a central role in scientific data analysis. This survey provides an introduction to probabilistic machine learning, which involves the representation and manipulation of uncertainty in models and predictions, playing a central role in scientific data analysis.</p>
<p>Jet-images: computer vision inspired techniques for jet tagging. J Cogan, J. High Energy Phys. 2015118Cogan, J. et al. Jet-images: computer vision inspired techniques for jet tagging. J. High Energy Phys. 2015, 118 (2015).</p>
<p>Sparse deconvolution improves the resolution of live-cell super-resolution fluorescence microscopy. W Zhao, Nat. Biotechnol. 40Zhao, W. et al. Sparse deconvolution improves the resolution of live-cell super-resolution fluorescence microscopy. Nat. Biotechnol. 40, 606-617 (2022).</p>
<p>MARS: discovering novel cell types across heterogeneous single-cell experiments. M BrbiÄ‡, Nat. Methods. 17BrbiÄ‡, M. et al. MARS: discovering novel cell types across heterogeneous single-cell experiments. Nat. Methods 17, 1200-1206 (2020).</p>
<p>Evaluation and development of deep neural networks for image superresolution in optical microscopy. C Qiao, Nat. Methods. 18Qiao, C. et al. Evaluation and development of deep neural networks for image super- resolution in optical microscopy. Nat. Methods 18, 194-202 (2021).</p>
<p>OmniFold: a method to simultaneously unfold all observables. A Andreassen, Phys. Rev. Lett. 124182001Andreassen, A. et al. OmniFold: a method to simultaneously unfold all observables. Phys. Rev. Lett. 124, 182001 (2020).</p>
<p>Super-resolved spatial transcriptomics by deep data fusion. L BergenstrÃ¥hle, Nat. Biotechnol. 40BergenstrÃ¥hle, L. et al. Super-resolved spatial transcriptomics by deep data fusion. Nat. Biotechnol. 40, 476-479 (2021).</p>
<p>Extracting and composing robust features with denoising autoencoders. P Vincent, International Conference on Machine Learning. Vincent, P. et al. Extracting and composing robust features with denoising autoencoders. In International Conference on Machine Learning 1096-1103 (2008).</p>
<p>Auto-encoding variational Bayes. D P Kingma, M Welling, International Conference on Learning Representations. Kingma, D. P. &amp; Welling, M. Auto-encoding variational Bayes. In International Conference on Learning Representations (2014).</p>
<p>Single-cell RNA-seq denoising using a deep count autoencoder. G Eraslan, Nat. Commun. 10390Eraslan, G. et al. Single-cell RNA-seq denoising using a deep count autoencoder. Nat. Commun. 10, 390 (2019).</p>
<p>. I Goodfellow, Y Bengio, A Courville, Learning, MIT PressGoodfellow, I., Bengio, Y. &amp; Courville, A. Deep Learning (MIT Press, 2016).</p>
<p>Emergence of simple-cell receptive field properties by learning a sparse code for natural images. B A Olshausen, D J Field, Nature. 381Olshausen, B. A. &amp; Field, D. J. Emergence of simple-cell receptive field properties by learning a sparse code for natural images. Nature 381, 607-609 (1996).</p>
<p>Deep learning of representations for unsupervised and transfer learning. Y Bengio, ICML Workshop on Unsupervised and Transfer Learning. Bengio, Y. Deep learning of representations for unsupervised and transfer learning. In ICML Workshop on Unsupervised and Transfer Learning (2012).</p>
<p>Learning meaningful representations of protein sequences. N S Detlefsen, S Hauberg, W Boomsma, Nat. Commun. 131914Detlefsen, N. S., Hauberg, S. &amp; Boomsma, W. Learning meaningful representations of protein sequences. Nat. Commun. 13, 1914 (2022).</p>
<p>Dimensionality reduction for visualizing single-cell data using UMAP. E Becht, Nat. Biotechnol. 37Becht, E. et al. Dimensionality reduction for visualizing single-cell data using UMAP. Nat. Biotechnol. 37, 38-44 (2019).</p>
<p>Geometric deep learning: going beyond euclidean data. M M Bronstein, IEEE Signal Process Mag. 34Bronstein, M. M. et al. Geometric deep learning: going beyond euclidean data. IEEE Signal Process Mag. 34, 18-42 (2017).</p>
<p>More is different: broken symmetry and the nature of the hierarchical structure of science. P W Anderson, Science. 177Anderson, P. W. More is different: broken symmetry and the nature of the hierarchical structure of science. Science 177, 393-396 (1972).</p>
<p>Informing geometric deep learning with electronic interactions to accelerate quantum chemistry. Z Qiao, Proc. Natl Acad. Sci. USA. 1192205221119Qiao, Z. et al. Informing geometric deep learning with electronic interactions to accelerate quantum chemistry. Proc. Natl Acad. Sci. USA 119, e2205221119 (2022).</p>
<p>Symmetry group equivariant architectures for physics. A Bogatskiy, Bogatskiy, A. et al. Symmetry group equivariant architectures for physics. Preprint at https://arxiv.org/abs/2203.06153 (2022).</p>
<p>Geometric deep learning: grids, groups, graphs, geodesics, and gauges. M M Bronstein, Preprint atBronstein, M. M. et al. Geometric deep learning: grids, groups, graphs, geodesics, and gauges. Preprint at https://arxiv.org/abs/2104.13478 (2021).</p>
<p>Geometric deep learning of RNA structure. R J L Townshend, Science. 373Townshend, R. J. L. et al. Geometric deep learning of RNA structure. Science 373, 1047-1051 (2021).</p>
<p>Hallucinating symmetric protein assemblies. B I M Wicky, Science. 378Wicky, B. I. M. et al. Hallucinating symmetric protein assemblies. Science 378, 56-61 (2022).</p>
<p>Semi-supervised classification with graph convolutional networks. T N Kipf, M Welling, International Conference on Learning Representations. Kipf, T. N. &amp; Welling, M. Semi-supervised classification with graph convolutional networks. In International Conference on Learning Representations (2017).</p>
<p>Graph attention networks. P VeliÄkoviÄ‡, International Conference on Learning Representations. VeliÄkoviÄ‡, P. et al. Graph attention networks. In International Conference on Learning Representations (2018).</p>
<p>Inductive representation learning on large graphs. W L Hamilton, Z Ying, J Leskovec, Advances in Neural Information Processing Systems. 30Hamilton, W. L., Ying, Z. &amp; Leskovec, J. Inductive representation learning on large graphs. In Advances in Neural Information Processing Systems 30, 1024-1034 (2017).</p>
<p>Neural message passing for quantum chemistry. J Gilmer, International Conference on Machine Learning. Gilmer, J. et al. Neural message passing for quantum chemistry. In International Conference on Machine Learning 1263-1272 (2017).</p>
<p>Graph representation learning in biomedicine and healthcare. M M Li, K Huang, M Zitnik, Nat. Biomed. Eng. 6Li, M. M., Huang, K. &amp; Zitnik, M. Graph representation learning in biomedicine and healthcare. Nat. Biomed. Eng. 6, 1353-1369 (2022).</p>
<p>E(n) equivariant graph neural networks. V G Satorras, E Hoogeboom, M Welling, International Conference on Machine Learning. Satorras, V. G., Hoogeboom, E. &amp; Welling, M. E(n) equivariant graph neural networks. In International Conference on Machine Learning 9323-9332 (2021).</p>
<p>This study incorporates principles of physics into the design of neural models, advancing the field of equivariant machine learning. This study incorporates principles of physics into the design of neural models, advancing the field of equivariant machine learning.</p>
<p>Tensor field networks: rotation-and translation-equivariant neural networks for 3D point clouds. N Thomas, Preprint atThomas, N. et al. Tensor field networks: rotation-and translation-equivariant neural networks for 3D point clouds. Preprint at https://arxiv.org/abs/1802.08219 (2018).</p>
<p>Generalizing convolutional neural networks for equivariance to lie groups on arbitrary continuous data. M Finzi, International Conference on Machine Learning. Finzi, M. et al. Generalizing convolutional neural networks for equivariance to lie groups on arbitrary continuous data. In International Conference on Machine Learning 3165-3176 (2020).</p>
<p>SE(3)-transformers: 3D roto-translation equivariant attention networks. F Fuchs, Advances in Neural Information Processing Systems. 33Fuchs, F. et al. SE(3)-transformers: 3D roto-translation equivariant attention networks. In Advances in Neural Information Processing Systems 33, 1970-1981 (2020).</p>
<p>Deep sets. M Zaheer, Advances in Neural Information Processing Systems. 30Zaheer, M. et al. Deep sets. In Advances in Neural Information Processing Systems 30, 3391-3401 (2017).</p>
<p>This paper is an early study that explores the use of deep neural architectures on set data, which consists of an unordered list of elements. This paper is an early study that explores the use of deep neural architectures on set data, which consists of an unordered list of elements.</p>
<p>Spherical CNNs. T S Cohen, International Conference on Learning Representations. Cohen, T. S. et al. Spherical CNNs. In International Conference on Learning Representations (2018).</p>
<p>Permutation equivariant models for compositional generalization in language. J Gordon, International Conference on Learning Representations. Gordon, J. et al. Permutation equivariant models for compositional generalization in language. In International Conference on Learning Representations (2019).</p>
<p>A practical method for constructing equivariant multilayer perceptrons for arbitrary matrix groups. M Finzi, M Welling, A G Wilson, International Conference on Machine Learning. Finzi, M., Welling, M. &amp; Wilson, A. G. A practical method for constructing equivariant multilayer perceptrons for arbitrary matrix groups. In International Conference on Machine Learning 3318-3328 (2021).</p>
<p>Recovering gene interactions from single-cell data using data diffusion. D V Dijk, Cell. 174Dijk, D. V. et al. Recovering gene interactions from single-cell data using data diffusion. Cell 174, 716-729 (2018).</p>
<p>Deciphering interaction fingerprints from protein molecular surfaces using geometric deep learning. P Gainza, Nat. Methods. 17Gainza, P. et al. Deciphering interaction fingerprints from protein molecular surfaces using geometric deep learning. Nat. Methods 17, 184-192 (2020).</p>
<p>The data-driven future of high-energy-density physics. P W Hatfield, Nature. 593Hatfield, P. W. et al. The data-driven future of high-energy-density physics. Nature 593, 351-361 (2021).</p>
<p>Unveiling the predictive power of static structure in glassy systems. V Bapst, Nat. Phys. 16Bapst, V. et al. Unveiling the predictive power of static structure in glassy systems. Nat. Phys. 16, 448-454 (2020).</p>
<p>Multiscale and integrative single-cell Hi-C analysis with Higashi. R Zhang, T Zhou, J Ma, Nat. Biotechnol. 40Zhang, R., Zhou, T. &amp; Ma, J. Multiscale and integrative single-cell Hi-C analysis with Higashi. Nat. Biotechnol. 40, 254-261 (2022).</p>
<p>Multi-omic machine learning predictor of breast cancer therapy response. S.-J Sammut, Nature. 601Sammut, S.-J. et al. Multi-omic machine learning predictor of breast cancer therapy response. Nature 601, 623-629 (2022).</p>
<p>Graph neural networks at the Large Hadron Collider. G Dezoort, Nat. Rev. Phys. 5DeZoort, G. et al. Graph neural networks at the Large Hadron Collider. Nat. Rev. Phys. 5, 281-303 (2023).</p>
<p>Pre-training molecular graph representation with 3D geometry. S Liu, International Conference on Learning Representations. Liu, S. et al. Pre-training molecular graph representation with 3D geometry. In International Conference on Learning Representations (2022).</p>
<p>A gravitational-wave standard siren measurement of the Hubble constant. Nature. 551The LIGO Scientific Collaboration. et al. A gravitational-wave standard siren measurement of the Hubble constant. Nature 551, 85-88 (2017).</p>
<p>Deep learning and process understanding for data-driven Earth system science. M Reichstein, Nature. 566Reichstein, M. et al. Deep learning and process understanding for data-driven Earth system science. Nature 566, 195-204 (2019).</p>
<p>Accelerated identification of disease-causing variants with ultra-rapid nanopore genome sequencing. S D Goenka, Nat. Biotechnol. 40Goenka, S. D. et al. Accelerated identification of disease-causing variants with ultra-rapid nanopore genome sequencing. Nat. Biotechnol. 40, 1035-1041 (2022).</p>
<p>Greedy layer-wise training of deep networks. Y Bengio, Advances in Neural Information Processing Systems. 19Bengio, Y. et al. Greedy layer-wise training of deep networks. In Advances in Neural Information Processing Systems 19, 153-160 (2006).</p>
<p>A fast learning algorithm for deep belief nets. G E Hinton, S Osindero, Y.-W Teh, Neural Comput. 18Hinton, G. E., Osindero, S. &amp; Teh, Y.-W. A fast learning algorithm for deep belief nets. Neural Comput. 18, 1527-1554 (2006).</p>
<p>Machine learning: trends, perspectives, and prospects. M I Jordan, T M Mitchell, Science. 349Jordan, M. I. &amp; Mitchell, T. M. Machine learning: trends, perspectives, and prospects. Science 349, 255-260 (2015).</p>
<p>BERT: pre-training of deep bidirectional transformers for language understanding. J Devlin, North American Chapter of the Association for Computational Linguistics. Devlin, J. et al. BERT: pre-training of deep bidirectional transformers for language understanding. In North American Chapter of the Association for Computational Linguistics 4171-4186 (2019).</p>
<p>Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences. A Rives, Proc. Natl Acad. Sci. USA. 1182016239118Rives, A. et al. Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences. Proc. Natl Acad. Sci. USA 118, e2016239118 (2021).</p>
<p>ProtTrans: rowards cracking the language of lifes code through selfsupervised deep learning and high performance computing. A Elnaggar, IEEE Transactions on Pattern Analysis and Machine Intelligence. Elnaggar, A. et al. ProtTrans: rowards cracking the language of lifes code through self- supervised deep learning and high performance computing. In IEEE Transactions on Pattern Analysis and Machine Intelligence (2021).</p>
<p>Learning the language of viral evolution and escape. B Hie, Science. 371Hie, B. et al. Learning the language of viral evolution and escape. Science 371, 284-288 (2021).</p>
<p>This paper modeled viral escape with machine learning algorithms originally developed for human natural language. This paper modeled viral escape with machine learning algorithms originally developed for human natural language.</p>
<p>Low-N protein engineering with data-efficient deep learning. S Biswas, Nat. Methods. 18Biswas, S. et al. Low-N protein engineering with data-efficient deep learning. Nat. Methods 18, 389-396 (2021).</p>
<p>Controllable protein design with language models. N Ferruz, B HÃ¶cker, Nat. Mach. Intell. 4Ferruz, N. &amp; HÃ¶cker, B. Controllable protein design with language models. Nat. Mach. Intell. 4, 521-532 (2022).</p>
<p>Learning inverse folding from millions of predicted structures. C Hsu, International Conference on Machine Learning. Hsu, C. et al. Learning inverse folding from millions of predicted structures. In International Conference on Machine Learning 8946-8970 (2022).</p>
<p>Accurate prediction of protein structures and interactions using a threetrack neural network. M Baek, Science. 373Baek, M. et al. Accurate prediction of protein structures and interactions using a three- track neural network. Science 373, 871-876 (2021).</p>
<p>Inspired by AlphaFold2, this study reported RoseTTAFold, a novel three-track neural module capable of simultaneously processing protein's sequence, distance and coordinates. Inspired by AlphaFold2, this study reported RoseTTAFold, a novel three-track neural module capable of simultaneously processing protein's sequence, distance and coordinates.</p>
<p>SMILES, a chemical language and information system. 1. Introduction to methodology and encoding rules. D Weininger, J. Chem. Inf. Comput. Sci. 28Weininger, D. SMILES, a chemical language and information system. 1. Introduction to methodology and encoding rules. J. Chem. Inf. Comput. Sci. 28, 31-36 (1988).</p>
<p>BigSMILES: a structurally-based line notation for describing macromolecules. T.-S Lin, ACS Cent. Sci. 5Lin, T.-S. et al. BigSMILES: a structurally-based line notation for describing macromolecules. ACS Cent. Sci. 5, 1523-1531 (2019).</p>
<p>SELFIES and the future of molecular string representations. M Krenn, Patterns. 3100588Krenn, M. et al. SELFIES and the future of molecular string representations. Patterns 3, 100588 (2022).</p>
<p>Language models can learn complex molecular distributions. D Flam-Shepherd, K Zhu, A Aspuru-Guzik, Nat. Commun. 133293Flam-Shepherd, D., Zhu, K. &amp; Aspuru-Guzik, A. Language models can learn complex molecular distributions. Nat. Commun. 13, 3293 (2022).</p>
<p>Chemical language models enable navigation in sparsely populated chemical space. M A Skinnider, Nat. Mach. Intell. 3Skinnider, M. A. et al. Chemical language models enable navigation in sparsely populated chemical space. Nat. Mach. Intell. 3, 759-770 (2021).</p>
<p>ChemBERTa: large-scale self-supervised pretraining for molecular property prediction. S Chithrananda, G Grand, B Ramsundar, Machine Learning for Molecules Workshop at NeurIPS. Chithrananda, S., Grand, G. &amp; Ramsundar, B. ChemBERTa: large-scale self-supervised pretraining for molecular property prediction. In Machine Learning for Molecules Workshop at NeurIPS (2020).</p>
<p>Predicting retrosynthetic pathways using transformer-based models and a hyper-graph exploration strategy. P Schwaller, Chem. Sci. 11Schwaller, P. et al. Predicting retrosynthetic pathways using transformer-based models and a hyper-graph exploration strategy. Chem. Sci. 11, 3316-3325 (2020).</p>
<p>State-of-the-art augmented NLP transformer models for direct and single-step retrosynthesis. I V Tetko, Nat. Commun. 115575Tetko, I. V. et al. State-of-the-art augmented NLP transformer models for direct and single-step retrosynthesis. Nat. Commun. 11, 5575 (2020).</p>
<p>Mapping the space of chemical reactions using attention-based neural networks. P Schwaller, Nat. Mach. Intell. 3Schwaller, P. et al. Mapping the space of chemical reactions using attention-based neural networks. Nat. Mach. Intell. 3, 144-152 (2021).</p>
<p>Quantitative interpretation explains machine learning models for chemical reaction prediction and uncovers bias. D P KovÃ¡cs, W Mccorkindale, A A Lee, Nat. Commun. 121695KovÃ¡cs, D. P., McCorkindale, W. &amp; Lee, A. A. Quantitative interpretation explains machine learning models for chemical reaction prediction and uncovers bias. Nat. Commun. 12, 1695 (2021).</p>
<p>Transfer learning enables the molecular transformer to predict regio-and stereoselective reactions on carbohydrates. G Pesciullesi, Nat. Commun. 114874Pesciullesi, G. et al. Transfer learning enables the molecular transformer to predict regio-and stereoselective reactions on carbohydrates. Nat. Commun. 11, 4874 (2020).</p>
<p>Attention is all you need. A Vaswani, Advances in Neural Information Processing Systems. 30Vaswani, A. et al. Attention is all you need. In Advances in Neural Information Processing Systems 30, 5998-6008 (2017).</p>
<p>This paper introduced the transformer, a modern neural network architecture that can process sequential data in parallel, revolutionizing natural language processing and sequence modeling. This paper introduced the transformer, a modern neural network architecture that can process sequential data in parallel, revolutionizing natural language processing and sequence modeling.</p>
<p>Earthquake transformer-an attentive deep-learning model for simultaneous earthquake detection and phase picking. S M Mousavi, Nat. Commun. 113952Mousavi, S. M. et al. Earthquake transformer-an attentive deep-learning model for simultaneous earthquake detection and phase picking. Nat. Commun. 11, 3952 (2020).</p>
<p>Effective gene expression prediction from sequence by integrating long-range interactions. Å½ Avsec, Nat. Methods. 18Avsec, Å½. et al. Effective gene expression prediction from sequence by integrating long-range interactions. Nat. Methods 18, 1196-1203 (2021).</p>
<p>Language models enable zero-shot prediction of the effects of mutations on protein function. J Meier, Advances in Neural Information Processing Systems. 34Meier, J. et al. Language models enable zero-shot prediction of the effects of mutations on protein function. In Advances in Neural Information Processing Systems 34, 29287-29303 (2021).</p>
<p>End-to-end symbolic regression with transformers. P.-A Kamienny, Advances in Neural Information Processing Systems. 35Kamienny, P.-A. et al. End-to-end symbolic regression with transformers. In Advances in Neural Information Processing Systems 35, 10269-10281 (2022).</p>
<p>Perceiver: general perception with iterative attention. A Jaegle, International Conference on Machine Learning. Jaegle, A. et al. Perceiver: general perception with iterative attention. In International Conference on Machine Learning 4651-4664 (2021).</p>
<p>Decision transformer: reinforcement learning via sequence modeling. L Chen, Advances in Neural Information Processing Systems. 34Chen, L. et al. Decision transformer: reinforcement learning via sequence modeling. In Advances in Neural Information Processing Systems 34, 15084-15097 (2021).</p>
<p>An image is worth 16x16 words: transformers for image recognition at scale. A Dosovitskiy, International Conference on Learning Representations. Dosovitskiy, A. et al. An image is worth 16x16 words: transformers for image recognition at scale. In International Conference on Learning Representations (2020).</p>
<p>Rethinking attention with performers. K Choromanski, International Conference on Learning Representations. Choromanski, K. et al. Rethinking attention with performers. In International Conference on Learning Representations (2021).</p>
<p>Fourier neural operator for parametric partial differential equations. Z Li, International Conference on Learning Representations. Li, Z. et al. Fourier neural operator for parametric partial differential equations. In International Conference on Learning Representations (2021).</p>
<p>Neural operator: learning maps between function spaces. N Kovachki, J. Mach. Learn. Res. 24Kovachki, N. et al. Neural operator: learning maps between function spaces. J. Mach. Learn. Res. 24, 1-97 (2023).</p>
<p>Kepler's laws of planetary motion: 1609-1666. J L Russell, Br. J. Hist. Sci. 2Russell, J. L. Kepler's laws of planetary motion: 1609-1666. Br. J. Hist. Sci. 2, 1-24 (1964).</p>
<p>Artificial intelligence foundation for therapeutic science. K Huang, Nat. Chem. Biol. 18Huang, K. et al. Artificial intelligence foundation for therapeutic science. Nat. Chem. Biol. 18, 1033-1036 (2022).</p>
<p>A Bayesian machine scientist to aid in the solution of challenging scientific problems. R GuimerÃ , Sci. Adv. 66971GuimerÃ , R. et al. A Bayesian machine scientist to aid in the solution of challenging scientific problems. Sci. Adv. 6, eaav6971 (2020).</p>
<p>Deep learning-guided discovery of an antibiotic targeting Acinetobacter baumannii. G Liu, 10.1038/s41589-023-01349-8Nat. Chem. Biol. Liu, G. et al. Deep learning-guided discovery of an antibiotic targeting Acinetobacter baumannii. Nat. Chem. Biol. https://doi.org/10.1038/s41589-023-01349-8 (2023).</p>
<p>Design of efficient molecular organic light-emitting diodes by a high-throughput virtual screening and experimental approach. R GÃ³mez-Bombarelli, Nat. Mater. 15This paper proposes using a black-box AI predictor to accelerate high-throughput screening of molecules in materials scienceGÃ³mez-Bombarelli, R. et al. Design of efficient molecular organic light-emitting diodes by a high-throughput virtual screening and experimental approach. Nat. Mater. 15, 1120-1127 (2016). This paper proposes using a black-box AI predictor to accelerate high-throughput screening of molecules in materials science.</p>
<p>Synthon-based ligand discovery in virtual libraries of over 11 billion compounds. A A Sadybekov, Nature. 601Sadybekov, A. A. et al. Synthon-based ligand discovery in virtual libraries of over 11 billion compounds. Nature 601, 452-459 (2022).</p>
<p>The NNPDF Collaboration Evidence for intrinsic charm quarks in the proton. Nature. 606The NNPDF Collaboration Evidence for intrinsic charm quarks in the proton. Nature 606, 483-487 (2022).</p>
<p>Accelerating high-throughput virtual screening through molecular pool-based active learning. D E Graff, E I Shakhnovich, C W Coley, Chem. Sci. 12Graff, D. E., Shakhnovich, E. I. &amp; Coley, C. W. Accelerating high-throughput virtual screening through molecular pool-based active learning. Chem. Sci. 12, 7866-7881 (2021).</p>
<p>Accurate multiobjective design in a space of millions of transition metal complexes with neural-network-driven efficient global optimization. J P Janet, ACS Cent. Sci. 6Janet, J. P. et al. Accurate multiobjective design in a space of millions of transition metal complexes with neural-network-driven efficient global optimization. ACS Cent. Sci. 6, 513-524 (2020).</p>
<p>. F Bacon, Novum Organon, 1620Bacon, F. Novum Organon Vol. 1620 (2000).</p>
<p>Distilling free-form natural laws from experimental data. M Schmidt, H Lipson, Science. 324Schmidt, M. &amp; Lipson, H. Distilling free-form natural laws from experimental data. Science 324, 81-85 (2009).</p>
<p>Deep symbolic regression: recovering mathematical expressions from data via risk-seeking policy gradients. B K Petersen, International Conference on Learning Representations. Petersen, B. K. et al. Deep symbolic regression: recovering mathematical expressions from data via risk-seeking policy gradients. In International Conference on Learning Representations (2020).</p>
<p>This paper describes a reinforcement-learning algorithm for navigating molecular combinatorial spaces, and it validates generated molecules using wet-lab experiments. A Zhavoronkov, Nat. Biotechnol. 37Deep learning enables rapid identification of potent DDR1 kinase inhibitorsZhavoronkov, A. et al. Deep learning enables rapid identification of potent DDR1 kinase inhibitors. Nat. Biotechnol. 37, 1038-1040 (2019). This paper describes a reinforcement-learning algorithm for navigating molecular combinatorial spaces, and it validates generated molecules using wet-lab experiments.</p>
<p>Optimization of molecules via deep reinforcement learning. Z Zhou, Sci. Rep. 910752Zhou, Z. et al. Optimization of molecules via deep reinforcement learning. Sci. Rep. 9, 10752 (2019).</p>
<p>Graph convolutional policy network for goal-directed molecular graph generation. J You, Advances in Neural Information Processing Systems. 31You, J. et al. Graph convolutional policy network for goal-directed molecular graph generation. In Advances in Neural Information Processing Systems 31, 6412-6422 (2018).</p>
<p>GFlowNet foundations. Y Bengio, Preprint atBengio, Y. et al. GFlowNet foundations. Preprint at https://arxiv.org/abs/2111.09266 (2021).</p>
<p>This paper describes a generative flow network that generates objects by sampling them from a distribution optimized for drug design. This paper describes a generative flow network that generates objects by sampling them from a distribution optimized for drug design.</p>
<p>Biological sequence design with GFlowNets. M Jain, International Conference on Machine Learning. Jain, M. et al. Biological sequence design with GFlowNets. In International Conference on Machine Learning 9786-9801 (2022).</p>
<p>Trajectory balance: improved credit assignment in GFlowNets. N Malkin, Advances in Neural Information Processing Systems. 35Malkin, N. et al. Trajectory balance: improved credit assignment in GFlowNets. In Advances in Neural Information Processing Systems 35, 5955-5967 (2022).</p>
<p>This study introduced a dynamic programming approach to determine the optimal locations and capacities of hydropower dams in the Amazon Basin. O Borkowski, Nat. Commun. 111872Large scale active-learning-guided exploration for in vitro protein production optimization. balancing between energy production and environmental impactBorkowski, O. et al. Large scale active-learning-guided exploration for in vitro protein production optimization. Nat. Commun. 11, 1872 (2020). This study introduced a dynamic programming approach to determine the optimal locations and capacities of hydropower dams in the Amazon Basin, balancing between energy production and environmental impact.</p>
<p>This study introduced a dynamic programming approach to determine the optimal locations and capacities of hydropower dams in the Amazon basin. A S Flecker, Science. 375Reducing adverse impacts of Amazon hydropower expansion. achieving a balance between the benefits of energy production and the potential environmental impactsFlecker, A. S. et al. Reducing adverse impacts of Amazon hydropower expansion. Science 375, 753-760 (2022). This study introduced a dynamic programming approach to determine the optimal locations and capacities of hydropower dams in the Amazon basin, achieving a balance between the benefits of energy production and the potential environmental impacts.</p>
<p>Learning from learning machines: a new generation of AI technology to meet the needs of science. L Pion-Tonachini, Preprint atPion-Tonachini, L. et al. Learning from learning machines: a new generation of AI technology to meet the needs of science. Preprint at https://arxiv.org/abs/2111.13786 (2021).</p>
<p>This paper describes a grammar variational autoencoder that generates novel symbolic laws and drug molecules. M J Kusner, B Paige, J M HernÃ¡ndez-Lobato, International Conference on Machine Learning. Grammar variational autoencoderKusner, M. J., Paige, B. &amp; HernÃ¡ndez-Lobato, J. M. Grammar variational autoencoder. In International Conference on Machine Learning 1945-1954 (2017). This paper describes a grammar variational autoencoder that generates novel symbolic laws and drug molecules.</p>
<p>Discovering governing equations from data by sparse identification of nonlinear dynamical systems. S L Brunton, J L Proctor, J N Kutz, Proc. Natl Acad. Sci. USA. 113Brunton, S. L., Proctor, J. L. &amp; Kutz, J. N. Discovering governing equations from data by sparse identification of nonlinear dynamical systems. Proc. Natl Acad. Sci. USA 113, 3932-3937 (2016).</p>
<p>Machine learning hidden symmetries. Z Liu, M Tegmark, Phys. Rev. Lett. 128180201Liu, Z. &amp; Tegmark, M. Machine learning hidden symmetries. Phys. Rev. Lett. 128, 180201 (2022).</p>
<p>Bayesian parameter estimation using conditional variational autoencoders for gravitational-wave astronomy. H Gabbard, Nat. Phys. 18Gabbard, H. et al. Bayesian parameter estimation using conditional variational autoencoders for gravitational-wave astronomy. Nat. Phys. 18, 112-117 (2022).</p>
<p>Automating crystal-structure phase mapping by combining deep learning with constraint reasoning. D Chen, Nat. Mach. Intell. 3Chen, D. et al. Automating crystal-structure phase mapping by combining deep learning with constraint reasoning. Nat. Mach. Intell. 3, 812-822 (2021).</p>
<p>Automatic chemical design using a data-driven continuous representation of molecules. R GÃ³mez-Bombarelli, ACS Cent. Sci. 4GÃ³mez-Bombarelli, R. et al. Automatic chemical design using a data-driven continuous representation of molecules. ACS Cent. Sci. 4, 268-276 (2018).</p>
<p>De novo protein design by deep network hallucination. I Anishchenko, Nature. 600Anishchenko, I. et al. De novo protein design by deep network hallucination. Nature 600, 547-552 (2021).</p>
<p>Differentiable scaffolding tree for molecular optimization. T Fu, International Conference on Learning Representations. Fu, T. et al. Differentiable scaffolding tree for molecular optimization. In International Conference on Learning Representations (2021).</p>
<p>Inverse molecular design using machine learning: generative models for matter engineering. B Sanchez-Lengeling, A Aspuru-Guzik, Science. 361Sanchez-Lengeling, B. &amp; Aspuru-Guzik, A. Inverse molecular design using machine learning: generative models for matter engineering. Science 361, 360-365 (2018).</p>
<p>This study describes an initiative with open AI models, datasets and education programmes to facilitate advances in therapeutic science across all stages of drug discovery and development. K Huang, NeurIPS Datasets and Benchmarks. Therapeutics Data Commons: machine learning datasets and tasks for drug discovery and developmentHuang, K. et al. Therapeutics Data Commons: machine learning datasets and tasks for drug discovery and development. In NeurIPS Datasets and Benchmarks (2021). This study describes an initiative with open AI models, datasets and education programmes to facilitate advances in therapeutic science across all stages of drug discovery and development.</p>
<p>Lab hazard. A Dance, Nature. 458Dance, A. Lab hazard. Nature 458, 664-665 (2009).</p>
<p>This paper describes an approach that combines deep neural networks with Monte Carlo tree search to plan chemical synthesis. M H S Segler, M Preuss, M P Waller, Nature. 555Planning chemical syntheses with deep neural networks and symbolic AISegler, M. H. S., Preuss, M. &amp; Waller, M. P. Planning chemical syntheses with deep neural networks and symbolic AI. Nature 555, 604-610 (2018). This paper describes an approach that combines deep neural networks with Monte Carlo tree search to plan chemical synthesis.</p>
<p>Autonomous platforms for data-driven organic synthesis. W Gao, P Raghavan, C W Coley, Nat. Commun. 131075Gao, W., Raghavan, P. &amp; Coley, C. W. Autonomous platforms for data-driven organic synthesis. Nat. Commun. 13, 1075 (2022).</p>
<p>On-the-fly closed-loop materials discovery via Bayesian active learning. A G Kusne, Nat. Commun. 115966Kusne, A. G. et al. On-the-fly closed-loop materials discovery via Bayesian active learning. Nat. Commun. 11, 5966 (2020).</p>
<p>Machine learning in combinatorial polymer chemistry. A J Gormley, M A Webb, Nat. Rev. Mater. 6Gormley,A. J. &amp; Webb, M. A. Machine learning in combinatorial polymer chemistry. Nat. Rev. Mater. 6, 642-644 (2021).</p>
<p>Autonomous materials synthesis via hierarchical active learning of nonequilibrium phase diagrams. S Ament, Sci. Adv. 74930Ament, S. et al. Autonomous materials synthesis via hierarchical active learning of nonequilibrium phase diagrams. Sci. Adv. 7, eabg4930 (2021).</p>
<p>This paper describes an approach for controlling tokamak plasmas, using a reinforcement-learning agent to command-control coils and satisfy physical and operational constraints. J Degrave, Nature. 602Magnetic control of tokamak plasmas through deep reinforcement learningDegrave, J. et al. Magnetic control of tokamak plasmas through deep reinforcement learning. Nature 602, 414-419 (2022). This paper describes an approach for controlling tokamak plasmas, using a reinforcement-learning agent to command-control coils and satisfy physical and operational constraints.</p>
<p>Active learning machine learns to create new quantum experiments. A A Melnikov, Proc. Natl Acad. Sci. USA. 115Melnikov, A. A. et al. Active learning machine learns to create new quantum experiments. Proc. Natl Acad. Sci. USA 115, 1221-1226 (2018).</p>
<p>ANI-1: an extensible neural network potential with DFT accuracy at force field computational cost. J S Smith, O Isayev, A E Roitberg, Chem. Sci. 8Smith, J. S., Isayev, O. &amp; Roitberg, A. E. ANI-1: an extensible neural network potential with DFT accuracy at force field computational cost. Chem. Sci. 8, 3192-3203 (2017).</p>
<p>This paper describes a neural network for reliable uncertainty estimations in molecular dynamics, enabling efficient sampling of high-dimensional free energy landscapes. D Wang, Nat. Comput. Sci. 2Efficient sampling of high-dimensional free energy landscapes using adaptive reinforced dynamicsWang, D. et al. Efficient sampling of high-dimensional free energy landscapes using adaptive reinforced dynamics. Nat. Comput. Sci. 2, 20-29 (2022). This paper describes a neural network for reliable uncertainty estimations in molecular dynamics, enabling efficient sampling of high-dimensional free energy landscapes.</p>
<p>Coarse-graining auto-encoders for molecular dynamics. W Wang, R GÃ³mez-Bombarelli, Comput. Mater. 5125Wang, W. &amp; GÃ³mez-Bombarelli, R. Coarse-graining auto-encoders for molecular dynamics. npj Comput. Mater. 5, 125 (2019).</p>
<p>This paper describes a method to learn the wavefunction of quantum systems using deep neural networks in conjunction with variational quantum Monte Carlo. J Hermann, Z SchÃ¤tzle, F NoÃ©, Nat. Chem. 12Deep-neural-network solution of the electronic SchrÃ¶dinger equationHermann, J., SchÃ¤tzle, Z. &amp; NoÃ©, F. Deep-neural-network solution of the electronic SchrÃ¶dinger equation. Nat. Chem. 12, 891-897 (2020). This paper describes a method to learn the wavefunction of quantum systems using deep neural networks in conjunction with variational quantum Monte Carlo.</p>
<p>Solving the quantum many-body problem with artificial neural networks. G Carleo, M Troyer, Science. 355Carleo, G. &amp; Troyer, M. Solving the quantum many-body problem with artificial neural networks. Science 355, 602-606 (2017).</p>
<p>Physics-informed machine learning. Em Karniadakis, G , Nat. Rev. Phys. 3Em Karniadakis, G. et al. Physics-informed machine learning. Nat. Rev. Phys. 3, 422-440 (2021).</p>
<p>Physics-informed neural operator for learning partial differential equations. Z Li, Preprint atLi, Z. et al. Physics-informed neural operator for learning partial differential equations. Preprint at https://arxiv.org/abs/2111.03794 (2021).</p>
<p>This paper describes an approach to accelerating computational fluid dynamics by training a neural network to interpolate from coarse to fine grids and generalize to varying forcing functions and Reynolds numbers. D Kochkov, Proc. Natl Acad. Sci. USA. 1182101784118Machine learning-accelerated computational fluid dynamicsKochkov, D. et al. Machine learning-accelerated computational fluid dynamics. Proc. Natl Acad. Sci. USA 118, e2101784118 (2021). This paper describes an approach to accelerating computational fluid dynamics by training a neural network to interpolate from coarse to fine grids and generalize to varying forcing functions and Reynolds numbers.</p>
<p>Stiff-PINN: physics-informed neural network for stiff chemical kinetics. W Ji, J. Phys. Chem. A. 125Ji, W. et al. Stiff-PINN: physics-informed neural network for stiff chemical kinetics. J. Phys. Chem. A 125, 8098-8106 (2021).</p>
<p>EikoNet: solving the Eikonal equation with deep neural networks. J D Smith, K Azizzadenesheli, Z E Ross, IEEE Trans. Geosci. Remote Sens. 59Smith, J. D., Azizzadenesheli, K. &amp; Ross, Z. E. EikoNet: solving the Eikonal equation with deep neural networks. IEEE Trans. Geosci. Remote Sens. 59, 10685-10696 (2020).</p>
<p>PINNeik: Eikonal solution using physics-informed neural networks. U B Waheed, Comput. Geosci. 155104833Waheed, U. B. et al. PINNeik: Eikonal solution using physics-informed neural networks. Comput. Geosci. 155, 104833 (2021).</p>
<p>This paper established a connection between neural networks and differential equations by introducing the adjoint method to learn continuous-time dynamical systems from data. R T Q Chen, Advances in Neural Information Processing Systems. 31Neural ordinary differential equations. replacing backpropagationChen, R. T. Q. et al. Neural ordinary differential equations. In Advances in Neural Information Processing Systems 31, 6572-6583 (2018). This paper established a connection between neural networks and differential equations by introducing the adjoint method to learn continuous-time dynamical systems from data, replacing backpropagation.</p>
<p>This paper describes a deep-learning approach for solving forwards and inverse problems in nonlinear partial differential equations and can find solutions to differential equations from data. M Raissi, P Perdikaris, G E Karniadakis, J. Comput. Phys. 378Physics-informed neural networks: a deep learning framework for solving forward and inverse problems involving nonlinear partial differential equationsRaissi, M., Perdikaris, P. &amp; Karniadakis, G. E. Physics-informed neural networks: a deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. J. Comput. Phys. 378, 686-707 (2019). This paper describes a deep-learning approach for solving forwards and inverse problems in nonlinear partial differential equations and can find solutions to differential equations from data.</p>
<p>Learning nonlinear operators via DeepONet based on the universal approximation theorem of operators. L Lu, Nat. Mach. Intell. 3Lu, L. et al. Learning nonlinear operators via DeepONet based on the universal approximation theorem of operators. Nat. Mach. Intell. 3, 218-229 (2021).</p>
<p>Message passing neural PDE solvers. J Brandstetter, D Worrall, M Welling, International Conference on Learning Representations. Brandstetter, J., Worrall, D. &amp; Welling, M. Message passing neural PDE solvers. In International Conference on Learning Representations (2022).</p>
<p>This paper presents an efficient sampling algorithm using normalizing flows to simulate equilibrium states in many-body systems. F NoÃ©, Science. 3651147Boltzmann generators: sampling equilibrium states of many-body systems with deep learningNoÃ©, F. et al. Boltzmann generators: sampling equilibrium states of many-body systems with deep learning. Science 365, eaaw1147 (2019). This paper presents an efficient sampling algorithm using normalizing flows to simulate equilibrium states in many-body systems.</p>
<p>Variational inference with normalizing flows. D Rezende, S Mohamed, International Conference on Machine Learning. 37Rezende, D. &amp; Mohamed, S. Variational inference with normalizing flows. In International Conference on Machine Learning 37, 1530-1538, (2015).</p>
<p>Density estimation using real NVP. L Dinh, J Sohl-Dickstein, S Bengio, International Conference on Learning Representations. Dinh, L., Sohl-Dickstein, J. &amp; Bengio, S. Density estimation using real NVP. In International Conference on Learning Representations (2017).</p>
<p>Estimation of thermodynamic observables in lattice field theories with deep generative models. K A Nicoli, Phys. Rev. Lett. 12632001Nicoli, K. A. et al. Estimation of thermodynamic observables in lattice field theories with deep generative models. Phys. Rev. Lett. 126, 032001 (2021).</p>
<p>Equivariant flow-based sampling for lattice gauge theory. G Kanwar, Phys. Rev. Lett. 125121601Kanwar, G. et al. Equivariant flow-based sampling for lattice gauge theory. Phys. Rev. Lett. 125, 121601 (2020).</p>
<p>Adaptive Monte Carlo augmented with normalizing flows. M GabriÃ©, G M Rotskoff, E Vanden-Eijnden, Proc. Natl Acad. Sci. USA. 1192109420119GabriÃ©, M., Rotskoff, G. M. &amp; Vanden-Eijnden, E. Adaptive Monte Carlo augmented with normalizing flows. Proc. Natl Acad. Sci. USA 119, e2109420119 (2022).</p>
<p>Markov chain Monte Carlo methods and the label switching problem in Bayesian mixture modeling. A Jasra, C C Holmes, D A Stephens, Stat. Sci. 20Jasra, A., Holmes, C. C. &amp; Stephens, D. A. Markov chain Monte Carlo methods and the label switching problem in Bayesian mixture modeling. Stat. Sci. 20, 50-67 (2005).</p>
<p>Better mixing via deep representations. Y Bengio, International Conference on Machine Learning. Bengio, Y. et al. Better mixing via deep representations. In International Conference on Machine Learning 552-560 (2013).</p>
<p>A framework for adaptive MCMC targeting multimodal distributions. E Pompe, C Holmes, K ÅatuszyÅ„ski, Ann. Stat. 48Pompe, E., Holmes, C. &amp; ÅatuszyÅ„ski, K. A framework for adaptive MCMC targeting multimodal distributions. Ann. Stat. 48, 2930-2952 (2020).</p>
<p>ATOM3D: tasks on molecules in three dimensions. R J L Townshend, NeurIPS Datasets and Benchmarks. Townshend, R. J. L. et al. ATOM3D: tasks on molecules in three dimensions. In NeurIPS Datasets and Benchmarks (2021).</p>
<p>The open reaction database. S M Kearnes, J. Am. Chem. Soc. 143Kearnes, S. M. et al. The open reaction database. J. Am. Chem. Soc. 143, 18820-18826 (2021).</p>
<p>Open Catalyst 2020 (OC20) dataset and community challenges. L Chanussot, ACS Catal. 11Chanussot, L. et al. Open Catalyst 2020 (OC20) dataset and community challenges. ACS Catal. 11, 6059-6072 (2021).</p>
<p>GuacaMol: benchmarking models for de novo molecular design. N Brown, J. Chem. Inf. Model. 59Brown, N. et al. GuacaMol: benchmarking models for de novo molecular design. J. Chem. Inf. Model. 59, 1096-1108 (2019).</p>
<p>Tranception: protein fitness prediction with autoregressive transformers and inference-time retrieval. P Notin, International Conference on Machine Learning. Notin, P. et al. Tranception: protein fitness prediction with autoregressive transformers and inference-time retrieval. In International Conference on Machine Learning 16990-17017 (2022).</p>
<p>Model cards for model reporting. M Mitchell, Conference on Fairness, Accountability, and. Mitchell, M. et al. Model cards for model reporting. In Conference on Fairness, Accountability, and Transparency220-229 (2019).</p>
<p>Datasheets for datasets. T Gebru, Commun. ACM. 64Gebru, T. et al. Datasheets for datasets. Commun. ACM 64, 86-92 (2021).</p>
<p>Advancing COVID-19 diagnosis with privacy-preserving collaboration in artificial intelligence. X Bai, Nat. Mach. Intell. 3Bai, X. et al. Advancing COVID-19 diagnosis with privacy-preserving collaboration in artificial intelligence. Nat. Mach. Intell. 3, 1081-1089 (2021).</p>
<p>Swarm learning for decentralized and confidential clinical machine learning. S Warnat-Herresthal, Nature. 594Warnat-Herresthal, S. et al. Swarm learning for decentralized and confidential clinical machine learning. Nature 594, 265-270 (2021).</p>
<p>Realizing private and practical pharmacological collaboration. B Hie, H Cho, B Berger, Science. 362Hie, B., Cho, H. &amp; Berger, B. Realizing private and practical pharmacological collaboration. Science 362, 347-350 (2018).</p>
<p>Digitization and validation of a chemical synthesis literature database in the ChemPU. S Rohrbach, Science. 377Rohrbach, S. et al. Digitization and validation of a chemical synthesis literature database in the ChemPU. Science 377, 172-180 (2022).</p>
<p>Network medicine framework for identifying drug-repurposing opportunities for COVID-19. D M Gysi, Proc. Natl Acad. Sci. USA. 1182025581118Gysi, D. M. et al. Network medicine framework for identifying drug-repurposing opportunities for COVID-19. Proc. Natl Acad. Sci. USA 118, e2025581118 (2021).</p>
<p>The automation of science. R D King, Science. 324King, R. D. et al. The automation of science. Science 324, 85-89 (2009).</p>
<p>ColabFold: making protein folding accessible to all. M Mirdita, Nat. Methods. 19Mirdita, M. et al. ColabFold: making protein folding accessible to all. Nat. Methods 19, 679-682 (2022).</p>
<p>TorchMD: a deep learning framework for molecular simulations. S Doerr, J. Chem. Theory Comput. 17Doerr, S. et al. TorchMD: a deep learning framework for molecular simulations. J. Chem. Theory Comput. 17, 2355-2363 (2021).</p>
<p>S S Schoenholz, E D Cubuk, Md, Advances in Neural Information Processing Systems. 33Schoenholz, S. S. &amp; Cubuk, E. D. JAX MD: a framework for differentiable physics. In Advances in Neural Information Processing Systems 33, 11428-11441 (2020).</p>
<p>Elements of Causal Inference: Foundations and Learning Algorithms. J Peters, D Janzing, B SchÃ¶lkopf, MIT PressPeters, J., Janzing, D. &amp; SchÃ¶lkopf, B. Elements of Causal Inference: Foundations and Learning Algorithms (MIT Press, 2017).</p>
<p>A meta-transfer objective for learning to disentangle causal mechanisms. Y Bengio, International Conference on Learning Representations. Bengio, Y. et al. A meta-transfer objective for learning to disentangle causal mechanisms. In International Conference on Learning Representations (2020).</p>
<p>Toward causal representation learning. B SchÃ¶lkopf, Proc. IEEE. IEEE109SchÃ¶lkopf, B. et al. Toward causal representation learning. Proc. IEEE 109, 612-634 (2021).</p>
<p>Inductive biases for deep learning of higher-level cognition. A Goyal, Y Bengio, Proc. R. Soc. A. 47820210068Goyal, A. &amp; Bengio, Y. Inductive biases for deep learning of higher-level cognition. Proc. R. Soc. A 478, 20210068 (2022).</p>
<p>Bayesian structure learning with generative flow networks. T Deleu, Conference on Uncertainty in Artificial Intelligence. Deleu, T. et al. Bayesian structure learning with generative flow networks. In Conference on Uncertainty in Artificial Intelligence 518-528 (2022).</p>
<p>Shortcut learning in deep neural networks. R Geirhos, Nat. Mach. Intell. 2Geirhos, R. et al. Shortcut learning in deep neural networks. Nat. Mach. Intell. 2, 665-673 (2020).</p>
<p>WILDS: a benchmark of in-the-wild distribution shifts. P W Koh, International Conference on Machine Learning. Koh, P. W. et al. WILDS: a benchmark of in-the-wild distribution shifts. In International Conference on Machine Learning 5637-5664 (2021).</p>
<p>Label efficient learning of transferable representations across domains and tasks. Z Luo, Advances in Neural Information Processing Systems. 30Luo, Z. et al. Label efficient learning of transferable representations across domains and tasks. In Advances in Neural Information Processing Systems 30, 165-177 (2017).</p>
<p>How much more data do I need? estimating requirements for downstream tasks. R Mahmood, IEEE Conference on Computer Vision and Pattern Recognition. Mahmood, R. et al. How much more data do I need? estimating requirements for downstream tasks. In IEEE Conference on Computer Vision and Pattern Recognition 275-284 (2022).</p>
<p>Autonomous discovery in the chemical sciences part II: outlook. C W Coley, N S Eyke, K F Jensen, Angew. Chem. Int. Ed. 59Coley, C. W., Eyke, N. S. &amp; Jensen, K. F. Autonomous discovery in the chemical sciences part II: outlook. Angew. Chem. Int. Ed. 59, 23414-23436 (2020).</p>
<p>The synthesizability of molecules proposed by generative models. W Gao, C W Coley, J. Chem. Inf. Model. 60Gao, W. &amp; Coley, C. W. The synthesizability of molecules proposed by generative models. J. Chem. Inf. Model. 60, 5714-5723 (2020).</p>
<p>Jet substructure at the Large Hadron Collider. R Kogler, Rev. Mod. Phys. 9145003Kogler, R. et al. Jet substructure at the Large Hadron Collider. Rev. Mod. Phys. 91, 045003 (2019).</p>
<p>Multimodal biomedical AI. J N Acosta, Nat. Med. 28Acosta, J. N. et al. Multimodal biomedical AI. Nat. Med. 28, 1773-1784 (2022).</p>
<p>Flamingo: a visual language model for few-shot learning. J.-B Alayrac, Advances in Neural Information Processing Systems. 35Alayrac, J.-B. et al. Flamingo: a visual language model for few-shot learning. In Advances in Neural Information Processing Systems 35, 23716-23736 (2022).</p>
<p>Biologically informed deep neural network for prostate cancer discovery. H A Elmarakeby, Nature. 598Elmarakeby, H. A. et al. Biologically informed deep neural network for prostate cancer discovery. Nature 598, 348-352 (2021).</p>
<p>A multi-scale map of cell structure fusing protein images and interactions. Y Qin, Nature. 600Qin, Y. et al. A multi-scale map of cell structure fusing protein images and interactions. Nature 600, 536-542 (2021).</p>
<p>Mapping the multiscale structure of biological systems. L V Schaffer, T Ideker, Cell Systems. 12Schaffer, L. V. &amp; Ideker, T. Mapping the multiscale structure of biological systems. Cell Systems 12, 622-635 (2021).</p>
<p>Interpretability of machine learning-based prediction models in healthcare. G Stiglic, Wiley Interdiscip. Rev. Data Min. Knowl. Discov. 101379Stiglic, G. et al. Interpretability of machine learning-based prediction models in healthcare. Wiley Interdiscip. Rev. Data Min. Knowl. Discov. 10, e1379 (2020).</p>
<p>A cost-aware framework for the development of AI models for healthcare applications. G Erion, Nat. Biomed. Eng. 6Erion, G. et al. A cost-aware framework for the development of AI models for healthcare applications. Nat. Biomed. Eng. 6, 1384-1398 (2022).</p>
<p>Explainable machine-learning predictions for the prevention of hypoxaemia during surgery. S M Lundberg, Nat. Biomed. Eng. 2Lundberg, S. M. et al. Explainable machine-learning predictions for the prevention of hypoxaemia during surgery. Nat. Biomed. Eng. 2, 749-760 (2018).</p>
<p>Beyond low Earth orbit: biological research, artificial intelligence, and self-driving labs. L M Sanders, Preprint atSanders, L. M. et al. Beyond low Earth orbit: biological research, artificial intelligence, and self-driving labs. Preprint at https://arxiv.org/abs/2112.12582 (2021).</p>
<p>Interpretable deep learning for spatial analysis of severe hailstorms. D J Gagne, Ii, Mon. Weather Rev. 147Gagne, D. J. II et al. Interpretable deep learning for spatial analysis of severe hailstorms. Mon. Weather Rev. 147, 2827-2845 (2019).</p>
<p>Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. C Rudin, Nat. Mach. Intell. 1Rudin, C. Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. Nat. Mach. Intell. 1, 206-215 (2019).</p>
<p>Understanding black-box predictions via influence functions. P W Koh, P Liang, International Conference on Machine Learning. Koh, P. W. &amp; Liang, P. Understanding black-box predictions via influence functions. In International Conference on Machine Learning 1885-1894 (2017).</p>
<p>Coresets for data-efficient training of machine learning models. B Mirzasoleiman, J Bilmes, J Leskovec, International Conference on Machine Learning. Mirzasoleiman, B., Bilmes, J. &amp; Leskovec, J. Coresets for data-efficient training of machine learning models. In International Conference on Machine Learning 6950-6960 (2020).</p>
<p>Interpretability beyond feature attribution: quantitative testing with concept activation vectors (TCAV). B Kim, International Conference on Machine Learning. Kim, B. et al. Interpretability beyond feature attribution: quantitative testing with concept activation vectors (TCAV). In International Conference on Machine Learning 2668-2677 (2018).</p>
<p>Mastering the game of go without human knowledge. D Silver, Nature. 550Silver, D. et al. Mastering the game of go without human knowledge. Nature 550, 354-359 (2017).</p>
<p>Artificial intelligence in chemistry: current trends and future directions. Z J Baum, J. Chem. Inf. Model. 61Baum, Z. J. et al. Artificial intelligence in chemistry: current trends and future directions. J. Chem. Inf. Model. 61, 3197-3212 (2021).</p>
<p>Adversarial attacks on medical machine learning. S G Finlayson, Science. 363Finlayson, S. G. et al. Adversarial attacks on medical machine learning. Science 363, 1287-1289 (2019).</p>
<p>Dual use of artificial-intelligence-powered drug discovery. F Urbina, Nat. Mach. Intell. 4Urbina, F. et al. Dual use of artificial-intelligence-powered drug discovery. Nat. Mach. Intell. 4, 189-191 (2022).</p>
<p>Minimum information about clinical artificial intelligence modeling: the MI-CLAIM checklist. B Norgeot, Nat. Med. 26Norgeot, B. et al. Minimum information about clinical artificial intelligence modeling: the MI-CLAIM checklist. Nat. Med. 26, 1320-1324 (2020).</p>            </div>
        </div>

    </div>
</body>
</html>
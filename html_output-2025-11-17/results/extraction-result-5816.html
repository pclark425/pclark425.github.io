<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5816 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5816</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5816</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-116.html">extraction-schema-116</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <p><strong>Paper ID:</strong> paper-dfa0fd70a186b0f7f44e833b4173ed97a4f5de31</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/dfa0fd70a186b0f7f44e833b4173ed97a4f5de31" target="_blank">Understanding Causality with Large Language Models: Feasibility and Opportunities</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> Assessment of the ability of large language models to answer causal questions by analyzing their strengths and weaknesses against three types of causal question and possible future directions and opportunities are discussed.</p>
                <p><strong>Paper Abstract:</strong> We assess the ability of large language models (LLMs) to answer causal questions by analyzing their strengths and weaknesses against three types of causal question. We believe that current LLMs can answer causal questions with existing causal knowledge as combined domain experts. However, they are not yet able to provide satisfactory answers for discovering new knowledge or for high-stakes decision-making tasks with high precision. We discuss possible future directions and opportunities, such as enabling explicit and implicit causal modules as well as deep causal-aware LLMs. These will not only enable LLMs to answer many different types of causal questions for greater impact but also enable LLMs to be more trustworthy and efficient in general.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5816.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5816.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Rephrasing stability</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Stability to rephrasing / prompt wording (anecdotal)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper reports that small rephrasings of prompts (e.g., different names for the same anatomical region) produced stable performance for Type 1 causal questions in their GPT-4 (new Bing) examples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (new Bing)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Type 1 causal question answering (known-domain causal)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Answering causal questions that rely on existing domain/common knowledge (e.g., medical causation, physical consequences of simple actions).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Rephrasing/alternative wording of the same question (e.g., 'shoulder area' vs 'muscles close to the upper front collarbone area') presented in the default balanced conversation setting.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Original phrasing vs rephrased wording of the same question.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative: performance remained stable across these rephrasings; LLM provided correct and clear explanations for Type 1 examples.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Qualitative: no observed drop in answer quality when region names were rephrased.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>no effect</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors note that for Type 1 (known-domain) questions, LLMs' large knowledge stores lead to stable answers across small wording changes; this may be due to robust retrieval of common-domain facts in the model.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>Figure 2 explicitly shows stable performance under rephrasing; authors report nondeterminism but no change in answer quality across trials for these cases.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Understanding Causality with Large Language Models: Feasibility and Opportunities', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5816.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5816.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Providing assumptions / added context</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Provision of explicit assumptions and contextual details</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper shows that adding explicit assumptions or contextual details changes what GPT-4 can do: with more assumptions it can explain meanings but still often cannot solve Type 2 causal discovery problems without APIs or algorithmic execution.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (new Bing)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Type 2 causal discovery from data</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Determining causal directionality or discovering causal relationships from (novel) numerical/observational data.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Providing additional problem assumptions and context in the prompt (e.g., linearity assumption, time-series indication, scale of data) and asking for method recommendation or causal inference.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Without explicit assumptions vs with explicit assumptions added to the prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative: without API access or algorithm execution, GPT-4 cannot solve the causal discovery task; with extra assumptions it can explain the meaning of assumptions but still fails to produce correct causal discovery solutions or recommend the most suitable methods.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Qualitative: added assumptions improved the model's ability to explicate assumptions, but did not enable correct causal discovery or selection of optimal methods.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved (explanatory ability) but insufficient for correct solving</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors suggest that explicit assumptions are necessary for causal-method selection and trustworthy conclusions; LLMs can surface required method types when assumptions are provided but lack the ability to perform deep numerical causal discovery without executing specialized algorithms.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Understanding Causality with Large Language Models: Feasibility and Opportunities', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5816.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5816.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Algorithmic prompting / API-call prompts</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompting to generate/execute algorithms or call external causal APIs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The authors report that prompting LLMs to generate code/algorithms or to call causal-analysis APIs enables procedural attempts at Type 2/3 tasks, but GPT-4 often chooses suboptimal methods and inherits limitations of external causal tools.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (new Bing) / ChatGPT (discussed)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Type 2 and Type 3 causal tasks requiring algorithmic solution or external computation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks that require executing causal-discovery algorithms or large-scale numerical estimation of intervention effects (e.g., recommending which customers to target to maximize revenue).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Prompts that ask the model to generate and execute algorithms, produce code, or call external causal-analysis APIs (i.e., procedural/code-generation prompting and API-invocation framing).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Natural-language-only prompt vs prompts that instruct the model to generate/execute algorithms or call APIs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative: LLMs can generate algorithms/code and identify broad method classes, but often fail to choose the most suitable specific method; when API access is used, the model's output is bounded by the strengths/limitations of external causal tools.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Qualitative: algorithmic/API-enabled prompting allows procedural progress toward solving tasks but does not guarantee correct method selection or high-precision results; natural-language-only prompting is less capable of producing executable solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved ability to attempt procedural solutions, but did not reliably produce correct or optimal results (mixed/insufficient)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>The paper hypothesizes that while LLMs can produce code and orchestrate APIs, they lack deep symbolic/mathematical reasoning to choose optimal causal methods; furthermore, using external APIs makes the system inherit the limitations of the causal methods available.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Understanding Causality with Large Language Models: Feasibility and Opportunities', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5816.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5816.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>In-context memory augmentation (ITE)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Using in-context example (ITE) / memory augmentation to provide per-case information</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>For Type 3 high-precision individualized treatment estimation tasks, the authors report that providing correct context via ITE (in-context memory augmentation) enables GPT to give useful answers, whereas without it the model struggles or hallucinates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (new Bing)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Type 3 individualized treatment effect (ITE) / high-precision decision tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>High-precision interventional decision-making requiring estimation of individual treatment effects or selection of optimal individuals from a population.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>In-context memory augmentation: supplying per-instance ITE data or structured contextual memory within the prompt to ground the model's decision-making.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Without ITE/contextual augmentation vs with ITE provided in context.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative: With correct ITE/context provided, GPT-4 can provide useful, correct-seeming recommendations; without it, the model fails to answer reliably and shows hallucinations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Qualitative: clear improvement when ITE/context is provided; without it performance is poor or hallucinatory.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors indicate that providing grounded per-case numerical/contextual information (memory augmentation) supplies what the model lacks internally, enabling correct selection/estimation; this suggests context grounding is crucial for high-precision causal tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Understanding Causality with Large Language Models: Feasibility and Opportunities', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5816.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5816.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Extra prompting / further follow-up prompts</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of iterative or further prompting (multi-turn prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper documents that further prompting in Type 3 scenarios often causes hallucinations or inconsistent intermediate steps; LLMs may produce confident but incorrect statements when pushed with additional prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (new Bing)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Type 3 high-precision causal decision tasks (multi-turn)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>High-stakes individualized or high-precision optimization questions requiring stable, stepwise reasoning over multiple turns.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Multi-turn follow-up prompting (asking for more detail, pushing for exact numeric recommendations after initial answer).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Single-turn initial prompt vs continued/further prompting for more details or recalculations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative: continued/further prompting often led to hallucinations or confidently incorrect intermediate steps; model produced inconsistent or confusing additional content in some examples.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Qualitative: multi-turn follow-ups decreased reliability in presented examples, producing hallucinations not present in single-turn initial answers.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>reduced</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors connect this to known LLM weaknesses in multi-step mathematical/symbolic reasoning and hallucination in intermediate steps; pushing the model further without grounded computation amplifies these issues.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Understanding Causality with Large Language Models: Feasibility and Opportunities', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5816.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5816.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Conversation mode / nondeterminism</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Default balanced conversation setting and model nondeterminism</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The experiments used GPT-4 via the new Bing in the default 'balanced' conversation mode; the authors note LLM nondeterminism but report that answer quality across trials did not change for their example set.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (new Bing, balanced mode)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Demonstrations across Type 1/2/3 causal question examples</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Qualitative demonstrations of LLM behaviour on different causal question types under a particular conversation/temperature setting.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Using the default balanced conversation setting of the new Bing interface (authors' experimental setup).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative: despite model nondeterminism, authors observed no change in overall answer quality across multiple trials for the presented examples.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>no effect</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors remark that while generation is stochastic, for the tested prompts and examples the qualitative answer quality remained consistent; they nevertheless acknowledge nondeterminism as a property of LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Understanding Causality with Large Language Models: Feasibility and Opportunities', 'publication_date_yy_mm': '2023-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Sparks of artificial general intelligence: Early experiments with gpt-4 <em>(Rating: 2)</em></li>
                <li>Training language models to follow instructions with human feedback <em>(Rating: 2)</em></li>
                <li>Complexity-based prompting for multi-step reasoning <em>(Rating: 2)</em></li>
                <li>GPT-4 technical report <em>(Rating: 2)</em></li>
                <li>How close is chatgpt to human experts? comparison corpus, evaluation, and detection <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5816",
    "paper_id": "paper-dfa0fd70a186b0f7f44e833b4173ed97a4f5de31",
    "extraction_schema_id": "extraction-schema-116",
    "extracted_data": [
        {
            "name_short": "Rephrasing stability",
            "name_full": "Stability to rephrasing / prompt wording (anecdotal)",
            "brief_description": "The paper reports that small rephrasings of prompts (e.g., different names for the same anatomical region) produced stable performance for Type 1 causal questions in their GPT-4 (new Bing) examples.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4 (new Bing)",
            "model_size": null,
            "task_name": "Type 1 causal question answering (known-domain causal)",
            "task_description": "Answering causal questions that rely on existing domain/common knowledge (e.g., medical causation, physical consequences of simple actions).",
            "problem_format": "Rephrasing/alternative wording of the same question (e.g., 'shoulder area' vs 'muscles close to the upper front collarbone area') presented in the default balanced conversation setting.",
            "comparison_format": "Original phrasing vs rephrased wording of the same question.",
            "performance": "Qualitative: performance remained stable across these rephrasings; LLM provided correct and clear explanations for Type 1 examples.",
            "performance_comparison": "Qualitative: no observed drop in answer quality when region names were rephrased.",
            "format_effect_size": null,
            "format_effect_direction": "no effect",
            "explanation_or_hypothesis": "Authors note that for Type 1 (known-domain) questions, LLMs' large knowledge stores lead to stable answers across small wording changes; this may be due to robust retrieval of common-domain facts in the model.",
            "counterexample_or_null_result": "Figure 2 explicitly shows stable performance under rephrasing; authors report nondeterminism but no change in answer quality across trials for these cases.",
            "uuid": "e5816.0",
            "source_info": {
                "paper_title": "Understanding Causality with Large Language Models: Feasibility and Opportunities",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "Providing assumptions / added context",
            "name_full": "Provision of explicit assumptions and contextual details",
            "brief_description": "The paper shows that adding explicit assumptions or contextual details changes what GPT-4 can do: with more assumptions it can explain meanings but still often cannot solve Type 2 causal discovery problems without APIs or algorithmic execution.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4 (new Bing)",
            "model_size": null,
            "task_name": "Type 2 causal discovery from data",
            "task_description": "Determining causal directionality or discovering causal relationships from (novel) numerical/observational data.",
            "problem_format": "Providing additional problem assumptions and context in the prompt (e.g., linearity assumption, time-series indication, scale of data) and asking for method recommendation or causal inference.",
            "comparison_format": "Without explicit assumptions vs with explicit assumptions added to the prompt.",
            "performance": "Qualitative: without API access or algorithm execution, GPT-4 cannot solve the causal discovery task; with extra assumptions it can explain the meaning of assumptions but still fails to produce correct causal discovery solutions or recommend the most suitable methods.",
            "performance_comparison": "Qualitative: added assumptions improved the model's ability to explicate assumptions, but did not enable correct causal discovery or selection of optimal methods.",
            "format_effect_size": null,
            "format_effect_direction": "improved (explanatory ability) but insufficient for correct solving",
            "explanation_or_hypothesis": "Authors suggest that explicit assumptions are necessary for causal-method selection and trustworthy conclusions; LLMs can surface required method types when assumptions are provided but lack the ability to perform deep numerical causal discovery without executing specialized algorithms.",
            "counterexample_or_null_result": null,
            "uuid": "e5816.1",
            "source_info": {
                "paper_title": "Understanding Causality with Large Language Models: Feasibility and Opportunities",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "Algorithmic prompting / API-call prompts",
            "name_full": "Prompting to generate/execute algorithms or call external causal APIs",
            "brief_description": "The authors report that prompting LLMs to generate code/algorithms or to call causal-analysis APIs enables procedural attempts at Type 2/3 tasks, but GPT-4 often chooses suboptimal methods and inherits limitations of external causal tools.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4 (new Bing) / ChatGPT (discussed)",
            "model_size": null,
            "task_name": "Type 2 and Type 3 causal tasks requiring algorithmic solution or external computation",
            "task_description": "Tasks that require executing causal-discovery algorithms or large-scale numerical estimation of intervention effects (e.g., recommending which customers to target to maximize revenue).",
            "problem_format": "Prompts that ask the model to generate and execute algorithms, produce code, or call external causal-analysis APIs (i.e., procedural/code-generation prompting and API-invocation framing).",
            "comparison_format": "Natural-language-only prompt vs prompts that instruct the model to generate/execute algorithms or call APIs.",
            "performance": "Qualitative: LLMs can generate algorithms/code and identify broad method classes, but often fail to choose the most suitable specific method; when API access is used, the model's output is bounded by the strengths/limitations of external causal tools.",
            "performance_comparison": "Qualitative: algorithmic/API-enabled prompting allows procedural progress toward solving tasks but does not guarantee correct method selection or high-precision results; natural-language-only prompting is less capable of producing executable solutions.",
            "format_effect_size": null,
            "format_effect_direction": "improved ability to attempt procedural solutions, but did not reliably produce correct or optimal results (mixed/insufficient)",
            "explanation_or_hypothesis": "The paper hypothesizes that while LLMs can produce code and orchestrate APIs, they lack deep symbolic/mathematical reasoning to choose optimal causal methods; furthermore, using external APIs makes the system inherit the limitations of the causal methods available.",
            "counterexample_or_null_result": null,
            "uuid": "e5816.2",
            "source_info": {
                "paper_title": "Understanding Causality with Large Language Models: Feasibility and Opportunities",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "In-context memory augmentation (ITE)",
            "name_full": "Using in-context example (ITE) / memory augmentation to provide per-case information",
            "brief_description": "For Type 3 high-precision individualized treatment estimation tasks, the authors report that providing correct context via ITE (in-context memory augmentation) enables GPT to give useful answers, whereas without it the model struggles or hallucinates.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4 (new Bing)",
            "model_size": null,
            "task_name": "Type 3 individualized treatment effect (ITE) / high-precision decision tasks",
            "task_description": "High-precision interventional decision-making requiring estimation of individual treatment effects or selection of optimal individuals from a population.",
            "problem_format": "In-context memory augmentation: supplying per-instance ITE data or structured contextual memory within the prompt to ground the model's decision-making.",
            "comparison_format": "Without ITE/contextual augmentation vs with ITE provided in context.",
            "performance": "Qualitative: With correct ITE/context provided, GPT-4 can provide useful, correct-seeming recommendations; without it, the model fails to answer reliably and shows hallucinations.",
            "performance_comparison": "Qualitative: clear improvement when ITE/context is provided; without it performance is poor or hallucinatory.",
            "format_effect_size": null,
            "format_effect_direction": "improved",
            "explanation_or_hypothesis": "Authors indicate that providing grounded per-case numerical/contextual information (memory augmentation) supplies what the model lacks internally, enabling correct selection/estimation; this suggests context grounding is crucial for high-precision causal tasks.",
            "counterexample_or_null_result": null,
            "uuid": "e5816.3",
            "source_info": {
                "paper_title": "Understanding Causality with Large Language Models: Feasibility and Opportunities",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "Extra prompting / further follow-up prompts",
            "name_full": "Effect of iterative or further prompting (multi-turn prompting)",
            "brief_description": "The paper documents that further prompting in Type 3 scenarios often causes hallucinations or inconsistent intermediate steps; LLMs may produce confident but incorrect statements when pushed with additional prompts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4 (new Bing)",
            "model_size": null,
            "task_name": "Type 3 high-precision causal decision tasks (multi-turn)",
            "task_description": "High-stakes individualized or high-precision optimization questions requiring stable, stepwise reasoning over multiple turns.",
            "problem_format": "Multi-turn follow-up prompting (asking for more detail, pushing for exact numeric recommendations after initial answer).",
            "comparison_format": "Single-turn initial prompt vs continued/further prompting for more details or recalculations.",
            "performance": "Qualitative: continued/further prompting often led to hallucinations or confidently incorrect intermediate steps; model produced inconsistent or confusing additional content in some examples.",
            "performance_comparison": "Qualitative: multi-turn follow-ups decreased reliability in presented examples, producing hallucinations not present in single-turn initial answers.",
            "format_effect_size": null,
            "format_effect_direction": "reduced",
            "explanation_or_hypothesis": "Authors connect this to known LLM weaknesses in multi-step mathematical/symbolic reasoning and hallucination in intermediate steps; pushing the model further without grounded computation amplifies these issues.",
            "counterexample_or_null_result": null,
            "uuid": "e5816.4",
            "source_info": {
                "paper_title": "Understanding Causality with Large Language Models: Feasibility and Opportunities",
                "publication_date_yy_mm": "2023-04"
            }
        },
        {
            "name_short": "Conversation mode / nondeterminism",
            "name_full": "Default balanced conversation setting and model nondeterminism",
            "brief_description": "The experiments used GPT-4 via the new Bing in the default 'balanced' conversation mode; the authors note LLM nondeterminism but report that answer quality across trials did not change for their example set.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4 (new Bing, balanced mode)",
            "model_size": null,
            "task_name": "Demonstrations across Type 1/2/3 causal question examples",
            "task_description": "Qualitative demonstrations of LLM behaviour on different causal question types under a particular conversation/temperature setting.",
            "problem_format": "Using the default balanced conversation setting of the new Bing interface (authors' experimental setup).",
            "comparison_format": null,
            "performance": "Qualitative: despite model nondeterminism, authors observed no change in overall answer quality across multiple trials for the presented examples.",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "no effect",
            "explanation_or_hypothesis": "Authors remark that while generation is stochastic, for the tested prompts and examples the qualitative answer quality remained consistent; they nevertheless acknowledge nondeterminism as a property of LLMs.",
            "counterexample_or_null_result": null,
            "uuid": "e5816.5",
            "source_info": {
                "paper_title": "Understanding Causality with Large Language Models: Feasibility and Opportunities",
                "publication_date_yy_mm": "2023-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Sparks of artificial general intelligence: Early experiments with gpt-4",
            "rating": 2
        },
        {
            "paper_title": "Training language models to follow instructions with human feedback",
            "rating": 2
        },
        {
            "paper_title": "Complexity-based prompting for multi-step reasoning",
            "rating": 2
        },
        {
            "paper_title": "GPT-4 technical report",
            "rating": 2
        },
        {
            "paper_title": "How close is chatgpt to human experts? comparison corpus, evaluation, and detection",
            "rating": 1
        }
    ],
    "cost": 0.0105045,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Understanding Causality with Large Language Models: Feasibility and Opportunities</h1>
<p>Cheng Zhang, Stefan Bauer, Paul Bennett, Jiangfeng Gao, Wenbo Gong, Agrin Hilmkil, Joel Jennings, Chao Ma, Tom Minka, Nick Pawlowski, James Vaughan</p>
<h4>Abstract</h4>
<p>We assess the ability of large language models (LLMs) to answer causal questions by analyzing their strengths and weaknesses against three types of causal question. We believe that current LLMs can answer causal questions with existing causal knowledge as combined domain experts. However, they are not yet able to provide satisfactory answers for discovering new knowledge or for high-stakes decision-making tasks with high precision. We discuss possible future directions and opportunities, such as enabling explicit and implicit causal modules as well as deep causal-aware LLMs. These will not only enable LLMs to answer many different types of causal questions for greater impact but also enable LLMs to be more trustworthy and efficient in general.</p>
<h2>1 Introduction</h2>
<p>The development of large language models Brown et al. (2020); OpenAI (2022; 2023); Ouyang et al. (2022); Radford et al. (2018; 2019); Thoppilan et al. (2022); Vaswani et al. (2017) has been extremely rapid. Recently, ChatGPT OpenAI (2022; 2023); Ouyang et al. (2022) has disrupted many domains, such as search, AI-aided content generation and editing, and is making great strides towards artificial general intelligence (AGI) Bubeck et al. (2023). On the other hand, LLMs have been shown to have some limitations in formal reasoning Bubeck et al. (2023); Mahowald et al. (2023); Wolfram (2023).</p>
<p>One of the most commonly asked question types are causal questions (several examples of such questions are given in Section 2). As humans actively interact with the environment, causal questions are fundamental to our nature. Besides its ultimate importance, understanding causal questions will likely be important for AGI and beyond. Answering such questions requires a combination of symbolic reasoning and high precision prediction; efficient interaction with humans to understand and verify assumptions; discovery of unknown knowledge; and interfacing of real-world physical interactions with safety awareness. Thus, in this article, we discuss LLMs in terms of their causal reasoning capabilities and the opportunities going forward.</p>
<p>In particular, we want to discuss and examine to what extent LLMs can be used to answer questions of causal nature. We will initially focus on current LLMs, a type of auto-regressive generative model Brown et al. (2020); Radford et al. (2019), characterized by the next token generation-based training. ChatGPT OpenAI (2022; 2023); Ouyang et al. (2022) introduces a new training paradigm where human feedback is utilized to enhance the alignment to the human objective regarding usefulness. The current usage of such human interventions is in its infancy, and we will discuss its potential in Section 3 Using causal reasoning as an example, our discussion perhaps generalizes beyond causal questions towards reasoning skills.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>LLMs have obtained impressive language skills, providing a generic and natural way for human to interact with AI Mahowald et al. (2023). It has also obtained a large volume of knowledge about the world which in many cases appear as a 'conservative team of experts' Guo et al. (2023).</p>
<p>Most impressively, it demonstrated strong understanding beyond memorization in a large range of scenarios including correct tool use, basic mathematical reasoning and understanding the mental state of human and agents Bubeck et al. (2023). LLMs can answer causal questions that are rooted in common knowledge or using known tools (as Type 1). However, when deep and high precision reasoning required (as Type 2 and Type 3), for instance, when optimizing decisions about future actions or expanding the existing knowledge boundary, the current capability of LLMs is insufficient, especially considering the limitations of existing causal methods.</p>
<p>This is aligned with the current observation of its limitations in advanced mathematical reasoning ability as shown in Bubeck et al. (2023). This opens great opportunities to enable LLMs to answer deeper causal questions, while requiring the introduction of new paradigms in the machine learning model itself (Section 3). Research enabling all type of causal questions to be answered by LLMs will be a great step to further scale the impact, augmenting human skill and empowering a broader audience to utilize AI for real-world actions.</p>
<h1>2 Can current LLMs answer causal questions?</h1>
<h3>2.1 Causal questions</h3>
<p>Any questions regarding the understanding of the effect of (imagined) interventions are causal questions. Causal methods are designed not only to understand causality like humans but also to augment human ability to optimize decisions with high precision at large scale. For example, decision-making scenarios requiring quantitative understanding of the effect of actions leading to the desired outcome, especially when the decision space is large. In science, causality is used to expand human knowledge to discover new causal relationships from data, such as bio-chemical or physical processes Meinshausen et al. (2016); Mooij et al. (2020); Runge et al. (2019); Sanchez-Romero et al. (2018). The questions below are typical causal questions, and answering them correctly requires an AI to have some degree of understanding of causality.</p>
<p>Type 1 Identifying causal relationships using domain knowledge (See App. A.1)
Example 1: Patient: Will my minor spine injury cause numbness in my shoulder?
Example 2: Person: I am balancing a glass of water on my head. Suppose I take a quick step to the right. What will happen to the glass?</p>
<p>Type 2 Discovering new knowledge from data (See App. A.2)
Example 1: Scientist: In a new scientific experiment. I observe two variables A and B which were from climate data. The observations are as follows: A:[...], B:[...]. Please let me know if A cause B or B causes A.
Example 2: Marketing specialist: I plan to launch a new membership program different from our competitors X and Y . There are two ways to design the benefit as members. The first is 'buy four and get a fifth one for free,' and the other is 'get 20 dollar cash return for every 100 dollar spend'. Which one should I choose?</p>
<p>Type 3 Quantitative estimating of the consequences of actions (See App. A.3)
Example 1: Sales manager: I have 1000 dealers with the following information about them [...]. I can only give membership to 100 of them next year. I want the membership program provides the highest revenue growth. Which 100 dealers should I choose?
Example 2: Medical doctor: This is the third time that this patient has returned with lumbago. The epidural steroid injections helped him before, but not for long. I injected 12 mn betamethasone the last two times. What is the dose that I should use this time?</p>
<p>Feasibility Current LLMs OpenAI (2022; 2023) can answer Type 1 well thanks to their large collection of knowledge. Thus, even though the answers are not $100 \%$ reliable today, we believe that such causal questions will be solved by LLMs.</p>
<p>However, the current token generation approaches limit the ability of LLMs to answer Type 2 and Type 3 questions. To answer these questions, the model needs to understand the fundamental causal mechanisms to discover new knowledge from data beyond the current human knowledge (Type 2), and provide intervention recommendations with an understanding of the quantitative effect on a large population (Type 3 Example 1) or for personalized high-stake decisions (Type 3 Example 2). Currently, LLMs fail to answer these questions directly without extra prompting as shown in App. A. 2 A.3. With prompting, LLMs can generate and execute algorithms (Section 3 in Bubeck et al. (2023)), but often fail to choose the most suitable ones. Although the ability to choose the correct method will be improved in future LLMs, the limitations of existing causal methods that the model has at its disposal to answer these questions remains.</p>
<h1>2.2 Challenges</h1>
<p>Discovering new insight beyond human knowledge One of the key goals for using machine learning to answer causal questions, especially causal discovery Glymour et al. (2019); Spirtes et al. (2000), is to discover unknown causal relationships from data. Thus, it is targeted towards expanding existing human knowledge. In general, new scientific knowledge is discovered by collecting data, forming hypotheses and testing hypotheses with more data. Nowadays, causal machine learning methods can be used to discover new relationships from observational data to suggest possible new insights into a given process.</p>
<p>As shown in Section 4 in Bubeck et al. (2023), LLMs in their current form are limited in advanced symbolic and mathematical reasoning. We also observe that they often make mistakes understanding mathematical assumptions which are required to find the suitable existing causal solutions or build new ones.</p>
<p>Earning trust Assumptions are needed to build any machine learning model. In causality, they play a pivotal role in method design and creating trust in the results from the users. A model with explicit assumptions for the different steps, that is able to provide step-by-step explanations regarding how the conclusion is reached from the assumptions is more trustworthy.</p>
<p>Currently, LLMs tend to make mistakes (hallucinations) in intermediate steps or skip steps in advanced math reasoning Bubeck et al. (2023); Fu et al. (2022) with seeming confidence.</p>
<p>Answers in their current form are difficult to trust as the reasoning is not always clearly broken down or is flawed. It could be dangerous in high stakes decision-making tasks to rely on the LLM's answers. Some method of earning the trust when answering high-stake causal questions is needed.</p>
<p>Precision and in-context requirements When it comes to interventional decision-making, high precision for numerical optimization in a highly contextual setting may be required, such as in Type 3. This is a hard or impossible task for humans to perform. To answer Type 3 example 1, one needs to know the effect of the membership program on every customer and recommend the top ones. To answer Type 3 example 2, although it is an individual patient, we require high precision and the exact treatment and dose with an accurate understanding of the effect.</p>
<p>Both applications require large-scale estimation of individual treatment effects with high-numerical precision in addition to the assumptions discussed before. The limitation shown in Section 4 in Bubeck et al. (2023) of LLMs demonstrates GPT4's inability to answer such a question directly. Utilizing external APIs makes LLMs inherits the limitation of existing causal methods to answer these questions. Moreover, such applications require highly contextual reasoning. The symptoms of a particular patient can indicate different things compared to the same symptoms in another. The requirement for complete information induces further questions regarding privacy and continual learning that are shared by other machine learning models as well Zanella-Beguelin et al. (2021); Zhang et al. (2022); Bubeck et al. (2023).</p>
<h1>3 Opportunities for enlarged impact</h1>
<p>Natural language is the most intuitive way for humans to communicate. The ability to bring causality to LLMs is a huge opportunity for greater impact.</p>
<p>For LLMs, the impact can be twofold. Firstly, having the ability to answer questions of causal nature will significantly enlarge its impact on new domains (Impact 1). Secondly, new paradigms (RLHF) to train LLMs require human interventions, which is naturally a causal paradigm. Advancing LLM research with rigorous consideration of the underlying causal mechanism has great potential to even further improve its language question-answering ability, considering generalization, fairness, interpretability computation efficiency, and sample efficiency <em>Jin et al. (2022); Ortega et al. (2021); Zhou et al. (2023)</em> (Impact 2).</p>
<p>Although causality has had an impact in specific domains with expert involvement such as in economics, the long-standing research in the field has yet to realize its full potential at scale <em>Heinze-Deml et al. (2018); Geffner et al. (2022)</em>. Further advances are needed in causal machine learning methods with respect to real-world considerations such as scalability, flexibility, and generality. Current causal methods also suffer from difficulties of obtaining domain knowledge, communicating assumptions, and communicating results. LLMs certainly can help with obvious advantages in communication with humans (Impact 3). Although more challenging, LLMs have the potential to advance the causal method as well (Impact 4).</p>
<p>With these great potentials, we believe that we can either introduce modularity (causal modules that can interact with LLMs) or further enlarge the reasoning capacity with new training paradigms while keeping the model foundational. These two research directions are not limited to causality. This is aligned with symbolic reasoning <em>Wolfram (2023)</em> and other functional competences <em>Mahowald et al. (2023)</em>.</p>
<p>Causal Modules Should we train LLMs to do floating point multiplication or just allow them to call Python? The same question applies to causal reasoning and other functional competencies as well. Rather than training LLMs to understand causality intrinsically, an efficient way to enlarge the impact is to create causal modules that can be used by the LLM.</p>
<p>There are two clear ways to create causal modules. Use existing methods such as create different causal APIs or utilize LLMs ability to generate such modules themselves (LLMs has shown impressive ability to code <em>Bubeck et al. (2023); OpenAI (2023)</em>).</p>
<p>This is non-trivial and requires both research and engineering effort, even with the assumption that all causal methods have an available API that can be easily interfaced with. First, causal modelling requires communicating with users to identify appropriate assumptions needed as well as further identification of the most suitable causal models meeting these assumptions to choose the correct API. (See Fig. 5) Secondly, to develop trust, summarizing the causal module's intermediate and final outputs with step-wise assumptions to provide a final answer in a trustworthy way is required.</p>
<p>With either direct access to API or generating the module themselves, LLMs with causal modules are limited to existing causal machine learning methods. Causal machine learning is still at its early stage. Thus, advances in the efficiency of causal machine learning is key to the user experience so that waiting times for answers are short. The scalability of such techniques to allow them to be applied to dataset sizes seen in the real world. The generality of such techniques, allowing them to realistic assumptions for different applications.</p>
<p>LLMs with new model paradigm to understand causality To go beyond current limitations, exploring a new training paradigm that enables a single foundational model to understand causality and perform causal reasoning directly or through generating novel methods is an exciting direction. The new RL-based paradigm <em>OpenAI (2022); Ouyang et al. (2022)</em> already introduces human interventions, by incorporating causality into this design may allow LLMs to be trained more efficient and have the possibility to answer many different types of causal questions. However, only advancing LLMs through RL would not be sufficient as naive RL methods only thrive when cheap interventions such as in simulators are. In the real world, most interventions and experiments are costly or even infeasible, which is one of the key motivations of causal machine learning research. Possible next steps include explicitly modeling the interventions and</p>
<p>consequences using LLMs; distilling causal models in LLMs to allow LLMs to gain the ability to internally build new methods; introducing assumption understanding and model verification to produce trustworthy causal answers.</p>
<h1>4 Conclusion</h1>
<p>To conclude, we would like to quote Scott and Shaw (2021), " The ability to use AI to remove constraints and create abundance in pursuit of solutions to our most important problems, and the opportunity to do this in ways where we pair the strength of AI with the strength of humans, is the real answer to 'Why AI?'. "</p>
<p>Large language models represent a remarkable advancement in AI research, bringing us closer than ever to achieving human-level language capabilities. We envision a future where AI is empowered by the ability to solve causal questions beyond human's current ability and provide insights to augment human's ability, enabling new discoveries and optimizing decision making in the (physical) real world. This paradigm shift has the potential to revolutionize and unlock the full potential of AI, and pave the way for more unprecedented progress in the field.</p>
<h2>References</h2>
<p>T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, $33: 1877-1901,2020$.
S. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar, P. Lee, Y. T. Lee, Y. Li, S. Lundberg, H. Nori, H. Palangi, M. T. Ribeiro, and Y. Zhang. Sparks of artificial general intelligence: Early experiments with gpt-4, 2023.
Y. Fu, H. Peng, A. Sabharwal, P. Clark, and T. Khot. Complexity-based prompting for multi-step reasoning. arXiv preprint arXiv:2210.00720, 2022.
T. Geffner, J. Antoran, A. Foster, W. Gong, C. Ma, E. Kiciman, A. Sharma, A. Lamb, M. Kukla, N. Pawlowski, et al. Deep end-to-end causal inference. arXiv preprint arXiv:2202.02195, 2022.
C. Glymour, K. Zhang, and P. Spirtes. Review of causal discovery methods based on graphical models. Frontiers in genetics, 10:524, 2019.
B. Guo, X. Zhang, Z. Wang, M. Jiang, J. Nie, Y. Ding, J. Yue, and Y. Wu. How close is chatgpt to human experts? comparison corpus, evaluation, and detection. arXiv preprint arXiv:2301.07597, 2023.
C. Heinze-Deml, M. H. Maathuis, and N. Meinshausen. Causal structure learning. Annual Review of Statistics and Its Application, 5:371-391, 2018.
Z. Jin, A. Feder, and K. Zhang. Causalnlp tutorial: An introduction to causality for natural language processing. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: Tutorial Abstracts, pages 17-22, 2022.
K. Mahowald, A. A. Ivanova, I. A. Blank, N. Kanwisher, J. B. Tenenbaum, and E. Fedorenko. Dissociating language and thought in large language models: a cognitive perspective. arXiv preprint arXiv:2301.06627, 2023.
N. Meinshausen, A. Hauser, J. M. Mooij, J. Peters, P. Versteeg, and P. Bhlmann. Methods for causal inference from gene perturbation experiments and validation. Proceedings of the National Academy of Sciences, 113(27):7361-7368, 2016.
J. M. Mooij, S. Magliacane, and T. Claassen. Joint causal inference from multiple contexts. The Journal of Machine Learning Research, 21(1):3919-4026, 2020.</p>
<p>OpenAI. Chatgpt. https://chat.openai.com/chat/, 2022.
OpenAI. Gpt-4 technical report, 2023.
P. A. Ortega, M. Kunesch, G. Deltang, T. Genewein, J. Grau-Moya, J. Veness, J. Buchli, J. Degrave, B. Piot, J. Perolat, et al. Shaking the foundations: delusions in sequence models for interaction and control. arXiv preprint arXiv:2110.10819, 2021.
L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155, 2022.
A. Radford, K. Narasimhan, T. Salimans, I. Sutskever, et al. Improving language understanding by generative pre-training. 2018.
A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.
J. Runge, S. Bathiany, E. Bollt, G. Camps-Valls, D. Coumou, E. Deyle, C. Glymour, M. Kretschmer, M. D. Mahecha, J. Muoz-Mar, et al. Inferring causation from time series in earth system sciences. Nature communications, 10(1):2553, 2019.
R. Sanchez-Romero, J. D. Ramsey, K. Zhang, M. K. Glymour, B. Huang, and C. Glymour. Causal discovery of feedback networks with functional magnetic resonance imaging. bioRxiv, page 245936, 2018.
K. Scott and G. Shaw. Reprogramming the American Dream: From Rural America to Silicon Valley-Making AI Serve Us All. 2021.
P. Spirtes, C. N. Glymour, R. Scheines, and D. Heckerman. Causation, prediction, and search. MIT press, 2000 .
R. Thoppilan, D. De Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, H.-T. Cheng, A. Jin, T. Bos, L. Baker, Y. Du, et al. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239, 2022.
A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, . Kaiser, and I. Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.
S. Wolfram. Wolfram|alpha as the way to bring computational knowledge superpowers to chatgpt. https://writings.stephenwolfram.com/2023/01/ wolframalpha-as-the-way-to-bring-computational-knowledge-superpowers-to-chatgpt/, 2023.
S. Zanella-Beguelin, S. Tople, A. Paverd, and B. Kpf. Grey-box extraction of natural language models. In International Conference on Machine Learning, pages 12278-12286. PMLR, 2021.
X. Zhang, B. Peng, J. Gao, and H. Meng. Toward self-learning end-to-end task-oriented dialog systems. In Proceedings of the 23rd Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages $516-530,2022$.
G. Zhou, L. Yao, X. Xu, C. Wang, L. Zhu, and K. Zhang. On the opportunity of causal deep generative models: A survey and future directions. arXiv preprint arXiv:2301.12351, 2023.</p>
<h1>A Appendix</h1>
<p>In this appendix, we demonstrate the GPT4 behaviour using the new Bing with the default balanced conversation setting. All the examples below are generated in March 2023. The examples where LLMs can provide correct answers have green captions and the ones with limitations have orange captions. As LLMs are nondeterministic, the generated answers varies. However, based on multiple trials, we found that the answer quality for different types of questions does not change. Thus, we only present one exemplar answer per question and discuss the performance.</p>
<h2>A. 1 Type 1 question behaviours</h2>
<p>With Fig 1, 2, 3, we show that questions requiring known causal knowledge can be answered by LLMs quite well. These questions ranging from identify if a causal relationship exist or not in medical setting to reason about the potential consequence of an action/intervention. Although the current performance may be associated existing dataset bias Bubeck et al. (2023). Looking to the future, we should expect LLMs be able to answer causal questions based on known domain knowledge sufficiently well.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: An example of a good answer to Type 1 question. The answer is correct and clear explanations are provided. It can clearly identify if a causal relationship exisit from domain knowledge.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: In addition to 1, we tested it with different names of the same region, for example "muscles close to the upper front collarbone area" instead of "shoulder area". We observe that the perfromance is quite stable.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: With this example, we can see that LLMs can understand the consequence of an action which is a basic causal inference task in a domain with common knowledge.</p>
<h1>A. 2 Type 2 question behaviours</h1>
<p>This type of question requires discovering new knowledge from the data. LLMs in their current form (without access to an API) cannot answer such questions as shown in Figure 4 and 6. This is expected as current LLMs learned from text data are not designed to discover new knowledge from numerical observations using complex reasoning. An encouraging sign is that the LLMs can identify the required method type (causal discovery) to answer such questions. However, at the current stage, for causal tasks requiring a deep understanding of different methods, LLMs fail to recommend the most suitable causal discovery method given the question description as shown in Figure 5. Thus, combining advanced research in causal machine learning and LLMs holds promising future.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: GPT4 cannot answer this type of question. With more assumptions given, it can only explain the meaning of these assumptions without answering the causal question.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: GPT4 can identify that such a question requires causal discovery methods and tries to recommend a causal discovery toolbox. This is a great step as this provides the possibility to answer such questions with API access and they can take advantage of the advances of the causal machine learning research. However, the recommended methods are not suitable. For the first question, I have already provided the assumption that the relationships are linear and the LLM recommends two non-linear methods, one of which is designed for time-series. For the second question, the recommended method is OK in theory but it is not the most efficient one to handle such scale data.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Lego to launch a new membership program different from our competitors 4 and 7. There are two ways to design the benefit as members. The first is "buy four and get a fifth one for free," and the other is "get an dollar cash return for every 100 dollar spend." Which one should I choose?</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$\checkmark$ Searching for: buy four get fifth free vs cashback</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$\checkmark$ Generating answers for you...</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Both options have their pros and cons, so it depends on what you want to achieve with your membership program.</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">The "buy four and get a fifth one for free" option is a great way to encourage customers to buy more products from you. It can also help you increase customer loyalty by rewarding them for their purchases. However, this option may not be as effective if your customers don't buy products from you frequently.</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">On the other hand, the "get $\$ 20$ cash return for every $\$ 100$ spend" option can be a great way to attract new customers and encourage them to spend more money with you. This option can also help you increase customer loyalty by rewarding them for their purchases. However, this option may not be as effective if your customers don't spend a lot of money with you.</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Ultimately, the best option for your membership program will depend on your business goals and your target audience. You may want to consider conducting market research or surveying your customers to determine which option would be most effective for your business.</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">I hope this helps! Let me know if you have any other questions.</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Learn more: 1. thepointgup.com 2. moneysavingexpert.com 3. quora.com 1 of 20 0</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Figure 6: GPT can give generic explains for the context. However, it does not provide concrete suggestions which require causal inference and more real-world evidence. For this question, the answers are not wrong but it is not sufficient. In a desired world, the answer should identify that one need to know the impact of these two programs on revenue to make the recommendation (a causal inference question) and more information are required.</p>
<h1>A. 3 Type 3 question behaviours</h1>
<p>GPT currently fails to answer high precision causal reasoning questions and shows hallucinatory behaviour with further prompting. However, given correct context (using ITE as memory augmentation), GPT can provide useful answers. Type 3 Example 1 questions are shown in Figure ?? to Figure 9. Type 3 Example 2 are shown in Figure ?? and 10.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 7: An example of decision making requiring causal reasoning with Bing. The computation of the revenue growth was revenue multiplied by discount. One hypothesis is that it may have not understood the header.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 8: Correct example of decision making to pick the highest ones. Following the question in Fig 7. However, as of March 2023, the performance is not stable as shown in Fig 9.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Company A will buy 65 K product more from us if I ask them to join our membership program. Company <br> B will buy 105 K product more from us if I ask them to join our membership program. Company C will <br> buy 95 K product more from us if I ask them to join our membership program. I can only give out 2 <br> membership cards. Which 2 company should I choose?</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">It's great to hear that you have potential customers interested in your membership program. It's a tough <br> decision to make when you have to choose between them.</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">If you want to maximize the total amount of product sold, you should choose Company B and Company <br> C. This way, you will sell a total of 200K more products.</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">If you want to maximize the number of companies that join your membership program, you should <br> choose Company A and Company B. This way, two companies will join your membership program.</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Which one do you prefer?</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<p>Figure 9: Wrong example of the same task as above. In this particular example, the third sentence seems out of the place. One explanation is that it understand the objective as to choose two companies. Instead of answering that any two companies will be fine if the objective is to choose just two companies. It give a specific example with confidence. This sentence is rather confusing to human audience. LLMs are stochastic but such mistakes are undesirable from a user trust perspective.</p>
<p>This is the third time that this patient returns with lunbago. The epidural steroid injections helped him before but not long. I injected 12 mg betamethasone last two times. What is the dose that I should use this time?
$\checkmark$ Searching for: betamethasone dose for epidural steroid injection
$\checkmark$ Generating answers for you...
According to a study published in 2015, administration of the lowest possible dose of epidural betamethasone is desired to minimize side effects while maintaining efficacy. The study hypothesized that a 3 mg or a 6 mg dose of betamethasone will demonstrate equivalent analgesic properties ${ }^{(8)}$.</p>
<p>Another source suggests that the usual adult dose for epidural steroid injection is $6-12 \mathrm{mg}(8)$.
It's important to note that the dose may vary depending on the patient's condition and other factors. It's best to consult with a doctor before administering any medication.
I hope this helps!
Learn more: 1. pubmed.ncbi.nlm.nih.gov 2. drugs.com 3. brf.nice.org.uk +1 more 1 of 15</p>
<p>The pain relief will last 1 month if I give 18 mg , 2 weeks if I give 12 mg and may not be so helpful if I give under 10 mg . There is no other side effect. Which does should I use?</p>
<p>It's important to note that the dose may vary depending on the patient's condition and other factors. It's best to consult with a doctor before administering any medication.</p>
<p>However, based on the information you provided, it seems that 18 mg would be the most effective dose for pain relief.
I hope this helps!
2 of 15</p>
<p>Figure 10: With this medical example, compared to ChatGPT77, GPT 4 shows appropriate referrals on allowed categories as discussed in OpenAI (2023) as it gives a general answer on the dose guidance. It shows stronger safety guards, even when ITE information provided.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>Correspond to: cheng.zhang@microsoft.com; the rest of the authors are ordered by family name initials.
Claim: This commentary are based only on publicly available information such as public APIs and technical articles by March 2023.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2051 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2051</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2051</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-50.html">extraction-schema-50</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using large language models (LLMs) to generate curricula, tasks, or goals for training agents, including comparisons with manual or heuristic curriculum approaches, performance results, and domain characteristics.</div>
                <p><strong>Paper ID:</strong> paper-279250436</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2506.07915v1.pdf" target="_blank">LUCIFER: Language Understanding and Context-Infused Framework for Exploration and Behavior Refinement</a></p>
                <p><strong>Paper Abstract:</strong> In dynamic environments, the rapid obsolescence of pre-existing environmental knowledge creates a gap between an agent's internal model and the evolving reality of its operational context. This disparity between prior and updated environmental valuations fundamentally limits the effectiveness of autonomous decision-making. To bridge this gap, the contextual bias of human domain stakeholders, who naturally accumulate insights through direct, real-time observation, becomes indispensable. However, translating their nuanced, and context-rich input into actionable intelligence for autonomous systems remains an open challenge. To address this, we propose LUCIFER (Language Understanding and Context-Infused Framework for Exploration and Behavior Refinement), a domain-agnostic framework that integrates a hierarchical decision-making architecture with reinforcement learning (RL) and large language models (LLMs) into a unified system. This architecture mirrors how humans decompose complex tasks, enabling a high-level planner to coordinate specialised sub-agents, each focused on distinct objectives and temporally interdependent actions. Unlike traditional applications where LLMs are limited to single role, LUCIFER integrates them in two synergistic roles: as context extractors, structuring verbal stakeholder input into domain-aware representations that influence decision-making through an attention space mechanism aligning LLM-derived insights with the agent's learning process, and as zero-shot exploration facilitators guiding the agent's action selection process during exploration. We benchmark various LLMs in both roles and demonstrate that LUCIFER improves exploration efficiency and decision quality, outperforming flat, goal-conditioned policies. Our findings show the potential of context-driven decision-making, where autonomous systems leverage human contextual knowledge for operational success.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2051.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2051.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using large language models (LLMs) to generate curricula, tasks, or goals for training agents, including comparisons with manual or heuristic curriculum approaches, performance results, and domain characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HierQ-LLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hierarchical Q-learning with LLM-guided exploration (LUCIFER variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A variant of the LUCIFER hierarchical agent that integrates LLMs in two roles: (1) a Context Extractor that maps verbal stakeholder inputs into structured spatial/semantic entities used by an attention-space, and (2) an Exploration Facilitator that issues zero-shot action predictions to a specialised worker (information-collection worker). Used end-to-end in a simulated post-earthquake SAR domain.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generator_type</strong></td>
                            <td>LLM-assisted task guidance (zero-shot exploration & context-driven shaping); not an explicit curriculum generator</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Gemma2 (context extractor); Hermes3 (exploration facilitator in 3-info); Llama3.1 (exploration facilitator in 6-info); Tulu3 and others evaluated</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_size</strong></td>
                            <td>Gemma2 (9B); Hermes3 (8B); Llama3.1 (8B); Tulu3 (8B); additional models ranged 3B-9B</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>LLMs are not used to produce an explicit multi-episode curriculum schedule. Instead, they provide two complementary functions that influence task execution and exploration: (a) Context Extractor: a RAG-augmented LLM ingests verbal inputs V from stakeholders and outputs structured contextual representations C = {(entity, category, coordinates)} mapped into an Information Space I; these C items are injected into an attention-space Ψ that shapes policy (π'), reward (R'') and action space (A') dynamically (policy shaping, potential-based reward shaping plus semantic shaping, and action masking/expansion). (b) Exploration Facilitator: at exploratory decision points the agent queries the LLM with the current state s_t, a short-term trajectory buffer ξ, and a long-term memory D; the LLM returns a zero-shot predicted action a* = argmax_a P(a | s_t, ξ, D) which supplements or replaces standard exploration (ε-greedy). This mechanism effectively biases which sub-tasks/short-term goals (e.g., which information points to probe next) are attempted, but does not output an explicit curriculum schedule spanning episodes.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_name</strong></td>
                            <td>Simulated post-earthquake urban search-and-rescue (SAR) — 2D OpenAI Gym gridworld; 3D ROS2 Gazebo proof-of-concept</td>
                        </tr>
                        <tr>
                            <td><strong>domain_characteristics</strong></td>
                            <td>Multi-objective, long-horizon and temporally coupled tasks (navigation, information collection, triage); configurable information complexity (3-info vs 6-info); reward density variants (sparse vs non-sparse); presence of hazards (HAZ), points-of-interest (POI), obstacles and final rescue targets; requires spatial grounding and domain-aware prioritisation.</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning_details</strong></td>
                            <td>LLM exploration queries condition on current state s_t, short-term trajectory buffer ξ (recent transitions within the episode), and long-term memory buffer D (summaries of past interactions and success/failure at semantically meaningful locations). Context extractor maps verbal inputs to spatial states s_j and semantic categories cat_j which are fed into attention-space shaping.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complementary_systems</strong></td>
                            <td>Hierarchical Strategic Decision Engine (SDE) and specialised Worker modules (navigation w_TN, information collection w_TI, triage w_TT); Attention Space mechanism for policy/reward/action shaping; Retrieval-Augmented Generation (RAG) knowledge base; trajectory and memory buffers (ξ, D).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_llm_curriculum</strong></td>
                            <td>LLM-guided agent variants (HierQ-LLM) substantially outperform flat, non-hierarchical agents in the SAR gridworld. Reported high-level outcomes: HierQ-LLM variants achieve substantially higher mission success and information-collection rates than flat Q agents; in reported experiments HierQ-LLM reaches mission-success rates in the range shown as much higher than flat baselines (example reported value in paper: HierQ-LLM MSR reported as 83.4% in a 3-info non-sparse configuration in Table III; overall HierQ-LLM frequently achieves ≈60%+ MSR in many settings). As exploration facilitators, model-specific accuracy on action predictions varies by setting: Hermes3 (8B) attains near-perfect guidance in 3-info settings (≈99.7–99.8% accuracy for 3-info sparse/non-sparse), Llama3.1 (8B) leads in 6-info (≈96.4–98.9%), while Gemma2 (9B) is an excellent context extractor (100% accuracy, zero errors on a 14-sample test set) but shows degraded zero-shot exploration accuracy under sparse/6-info conditions (drops to ~69.8% in sparse 6-info).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_manual_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_heuristic_curriculum</strong></td>
                            <td>Heuristic/manual shaping baselines (policy shaping, reward shaping, action shaping) and hierarchical agents without LLM guidance are included for comparison. Flat agents (Q, Q-PS, Q-RS, Q-AS) exhibit very low MSR (reported 'below 15%' across scenarios). Hierarchical agents with heuristic shaping (e.g., HierQ-PS, HierQ-RS, HierQ-AS) substantially outperform flat agents; e.g., HierQ-PS reported MSR ≈62.0% in the non-sparse 3-info setting (Table III), showing that heuristic policy shaping alone is effective. The best results arise from hybrid combinations (HierQ-LLM + policy shaping).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_no_curriculum</strong></td>
                            <td>Flat Q-learning agents without hierarchy or shaping show near-zero mission success (MSR reported as ~0% in many settings in Table III).</td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Per-model response times reported for context-extraction accuracy benchmarking: Gemma2 (9B) avg ≈15.3 s per query; Tulu3 (8B) ≈9.8 s; Llama3.1 (8B) ≈13.1 s; Hermes3 (8B) ≈10.7 s; smaller models (Gemma3 4B ≈4.9 s; Llama3.2 3B ≈2.7 s; Qwen2.5 3B ≈2.3 s). No dollar-cost accounting provided.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes_limitations</strong></td>
                            <td>Noted failure modes include: (1) Dependency on LLM output quality — misclassification or retrieval mismatches from RAG can mislead shaping (e.g., retrieving irrelevant domain manuals); (2) Model-specific generalisation limits — Gemma2, despite perfect context extraction, degrades as an exploration facilitator in sparse/long-horizon (6-info) settings; (3) Partial pipeline substitution bottleneck — replacing only the information-collection worker with LLM guidance leaves navigation and triage under RL, so poor RL subcomponents can limit overall mission success; (4) Human language variability — framework assumes reasonably clear verbal inputs; vague/ambiguous language may require explicit interactive clarification; (5) Computational latency — LLM response times (several seconds) may affect real-time applicability.</td>
                        </tr>
                        <tr>
                            <td><strong>long_horizon_performance</strong></td>
                            <td>Performance degrades when moving from 3-info to 6-info (longer-horizon) tasks; LLM facilitators demonstrate different robustness by horizon: Hermes3 excels in short-horizon (3-info) sparse and non-sparse settings (near-perfect), whereas Llama3.1 generalises better to longer 6-info missions (high accuracy ~96–99%). Overall, hierarchical shaping + LLM guidance mitigates but does not fully remove long-horizon difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>specialized_domain_performance</strong></td>
                            <td>Domain is SAR (requires spatial reasoning and safety prioritisation); LLM context extraction effectively maps verbal spatial reports into POI/HAZ entities (Gemma2 achieved 100% accuracy on a 14-input test set). No results reported for domains requiring deep technical scientific expertise beyond SAR-style spatial/operational knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size_scaling</strong></td>
                            <td>Paper benchmarks 13 pre-trained LLMs across sizes (3B–9B). Larger models (Gemma2 9B) gave best context-extraction accuracy (100%), while smaller models traded speed for reduced accuracy (e.g., Llama3.2 3B ≈92.7% accuracy, Qwen2.5 3B ≈73.6% accuracy). As exploration facilitators, 8B models (Hermes3, Llama3.1, Tulu3) performed best for respective horizon settings. Overall, performance trends suggest quality improves with model capacity in context extraction, but exploration robustness depends on model architecture/variant, not strictly size alone.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_curriculum_effectiveness</strong></td>
                            <td>Key findings: (1) LLMs can effectively supply structured, spatially-grounded context that when injected via an attention-space to shape policy, reward and action space, improves hierarchical agent performance; (2) LLMs used as zero-shot exploration facilitators reduce inefficient trial-and-error and improve information-collection worker performance, but their effectiveness depends on horizon and feedback density (short-horizon/dense settings show near-perfect results, long-horizon/sparse settings expose generalisation limits); (3) Hierarchical decomposition is essential — flat agents, even when augmented with LLM context, fail on complex multi-phase tasks; (4) Hybrid approaches (hierarchy + LLM guidance + policy shaping) yield the best results, demonstrating complementarity between heuristic shaping and LLM reasoning; (5) computational latency and dependence on LLM retrieval/hallucination robustness are important practical constraints.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2051.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2051.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using large language models (LLMs) to generate curricula, tasks, or goals for training agents, including comparisons with manual or heuristic curriculum approaches, performance results, and domain characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Policy/Reward/Action Shaping (Heuristic curricula)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Policy shaping, Reward shaping and Action-space shaping (attention-space heuristic methods)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Heuristic shaping methods embedded in the LUCIFER attention-space that bias the agent's Q-values, modify rewards via potential-based shaping and semantic signals, and restrict/expand action spaces based on LLM-processed contextual insights or designer choices.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generator_type</strong></td>
                            <td>heuristic-based / manual shaping (no explicit curriculum generator)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>These methods do not produce curricula but act as per-step heuristics to bias learning: Policy shaping biases Q(s,a) via Ψ(s,a) penalties/incentives for actions leading to undesirable/desirable/critical states; Reward shaping uses potential-based reward shaping (Φ(s)) combined with semantic immediate shaping Φ_Ψ(s) derived from contextual categories to produce R''(s,a); Action-space shaping toggles available actions A(s)→A'(s) by removing or adding actions based on detected semantic states (s_u, s_d, s_o). When combined with hierarchy, these heuristics serve to prioritise intermediate goals and improve safety.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_name</strong></td>
                            <td>Same SAR gridworld as above (2D OpenAI Gym; 3D Gazebo proof-of-concept)</td>
                        </tr>
                        <tr>
                            <td><strong>domain_characteristics</strong></td>
                            <td>Multi-stage mission with interdependent subtasks, sparse vs dense reward regimes, and spatial hazards requiring safety-aware action masking.</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning_details</strong></td>
                            <td>Shaping conditions on detected contextual entities c_j mapped to states s_j and semantic categories (undesirable s_u, desirable s_d, objective s_o) derived either from LLM context extraction or domain knowledge; reward shaping uses spatial potential over S_ref and semantic flags.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complementary_systems</strong></td>
                            <td>Used with hierarchical planner (SDE), workers, and optionally with LLM context extractor; integrates with Q-learning updates (modifies maximisation set to A').</td>
                        </tr>
                        <tr>
                            <td><strong>performance_llm_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_manual_curriculum</strong></td>
                            <td>Heuristic shaping (policy, reward, action shaping) significantly improves performance over no-shaping on hierarchical agents. Example: HierQ-PS reported MSR ~62.0% in non-sparse 3-info (Table III), and heuristic shaping generally produced safety improvements (MSWC ≈ MSR in many shaped configurations).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_heuristic_curriculum</strong></td>
                            <td>See above (heuristic shaping is the curriculum-like baseline). Heuristic policy shaping and action shaping both boost mission success and safety; their combination with hierarchy yields robust gains compared to flat baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_no_curriculum</strong></td>
                            <td>Flat agents without hierarchy or shaping had near-zero mission success (MSR ≈ 0% in many settings).</td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Heuristic shaping imposes negligible online computational cost compared to LLM inference; no explicit cost numbers provided.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes_limitations</strong></td>
                            <td>Heuristic shaping depends on correct identification of semantic states; overly restrictive action masking can remove necessary exploratory actions; policy-shaping integration into function-approximation (DRL) is non-trivial and was not addressed in this tabular-RL study.</td>
                        </tr>
                        <tr>
                            <td><strong>long_horizon_performance</strong></td>
                            <td>Shaping helps but does not fully resolve long-horizon (6-info) degradation; hierarchical decomposition combined with shaping is necessary to maintain acceptable performance as horizon increases.</td>
                        </tr>
                        <tr>
                            <td><strong>specialized_domain_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size_scaling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_curriculum_effectiveness</strong></td>
                            <td>Heuristic shaping acts as an effective complementary mechanism to hierarchy and LLM guidance: it improves safety and mission success, and when combined with LLM-guided exploration yields the best empirical results. However, heuristic shaping alone (without hierarchy) fails to salvage flat agents.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2051.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2051.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using large language models (LLMs) to generate curricula, tasks, or goals for training agents, including comparisons with manual or heuristic curriculum approaches, performance results, and domain characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Related LLM-based curriculum/goal-generation works</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prior literature that uses LLMs to generate sub-goals, intrinsic goals, or curriculum schedules (mentioned in related work)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper cites multiple prior works that apply LLMs to design rewards, decompose tasks into subtasks, or generate intrinsic goals/curricula for RL agents, e.g., papers on LLMs as reward designers, LLM planners, autotelic LLM exploration, and automated policy learning workflows (Learningflow) for curriculum RL.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generator_type</strong></td>
                            <td>LLM-generated (in cited works) — includes intrinsic goal generation, high-level planning, and curriculum RL workflows</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td>Cited prior work includes multiple paradigms: LLMs as reward designers (design dense/nuanced rewards), LLMs as high-level planners that break tasks into sub-tasks and select subgoals dynamically, and LLM-driven generation of intrinsic goals to promote open-ended exploration; additionally, collaborative multi-LLM workflows for curriculum RL in urban driving (Learningflow) are mentioned. These works use LLMs to produce sub-goals or curricula that shape agent training dynamics across episodes.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_name</strong></td>
                            <td>Various domains in cited literature: robotics manipulation, urban driving, open-ended goal-conditioned RL, Minecraft, embodied agents; not experiments in the current paper.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_characteristics</strong></td>
                            <td>Often open-ended or multi-task domains, requiring hierarchical decomposition, long-horizon planning, and domain grounding; some works focus on automated curriculum for policy pretraining or for structured multi-stage tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>state_conditioning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_mechanism_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>complementary_systems</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_llm_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_manual_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_heuristic_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_no_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes_limitations</strong></td>
                            <td>Paper notes general risks from literature: hallucinations, retrieval mismatches, and domain-mismatch risks when grounding LLM outputs into control systems.</td>
                        </tr>
                        <tr>
                            <td><strong>long_horizon_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>specialized_domain_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size_scaling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_curriculum_effectiveness</strong></td>
                            <td>The paper positions these prior works as motivation: LLMs can generate rewards, decompose tasks, and propose intrinsic goals (i.e., provide curricula), but the present contribution focuses on integrating LLM outputs as guided heuristics (attention-space) within a hierarchical RL loop rather than wholesale curriculum substitution.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Learningflow: Automated policy learning workflow for urban driving with large language models <em>(Rating: 2)</em></li>
                <li>Augmenting autotelic agents with large language models <em>(Rating: 2)</em></li>
                <li>Autotelic llm-based exploration for goal-conditioned rl <em>(Rating: 2)</em></li>
                <li>Latent reward: Llm-empowered credit assignment in episodic reinforcement learning <em>(Rating: 1)</em></li>
                <li>Motif: Intrinsic motivation from artificial intelligence feedback <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2051",
    "paper_id": "paper-279250436",
    "extraction_schema_id": "extraction-schema-50",
    "extracted_data": [
        {
            "name_short": "HierQ-LLM",
            "name_full": "Hierarchical Q-learning with LLM-guided exploration (LUCIFER variant)",
            "brief_description": "A variant of the LUCIFER hierarchical agent that integrates LLMs in two roles: (1) a Context Extractor that maps verbal stakeholder inputs into structured spatial/semantic entities used by an attention-space, and (2) an Exploration Facilitator that issues zero-shot action predictions to a specialised worker (information-collection worker). Used end-to-end in a simulated post-earthquake SAR domain.",
            "citation_title": "here",
            "mention_or_use": "use",
            "curriculum_generator_type": "LLM-assisted task guidance (zero-shot exploration & context-driven shaping); not an explicit curriculum generator",
            "llm_model_name": "Gemma2 (context extractor); Hermes3 (exploration facilitator in 3-info); Llama3.1 (exploration facilitator in 6-info); Tulu3 and others evaluated",
            "llm_model_size": "Gemma2 (9B); Hermes3 (8B); Llama3.1 (8B); Tulu3 (8B); additional models ranged 3B-9B",
            "curriculum_description": "LLMs are not used to produce an explicit multi-episode curriculum schedule. Instead, they provide two complementary functions that influence task execution and exploration: (a) Context Extractor: a RAG-augmented LLM ingests verbal inputs V from stakeholders and outputs structured contextual representations C = {(entity, category, coordinates)} mapped into an Information Space I; these C items are injected into an attention-space Ψ that shapes policy (π'), reward (R'') and action space (A') dynamically (policy shaping, potential-based reward shaping plus semantic shaping, and action masking/expansion). (b) Exploration Facilitator: at exploratory decision points the agent queries the LLM with the current state s_t, a short-term trajectory buffer ξ, and a long-term memory D; the LLM returns a zero-shot predicted action a* = argmax_a P(a | s_t, ξ, D) which supplements or replaces standard exploration (ε-greedy). This mechanism effectively biases which sub-tasks/short-term goals (e.g., which information points to probe next) are attempted, but does not output an explicit curriculum schedule spanning episodes.",
            "domain_name": "Simulated post-earthquake urban search-and-rescue (SAR) — 2D OpenAI Gym gridworld; 3D ROS2 Gazebo proof-of-concept",
            "domain_characteristics": "Multi-objective, long-horizon and temporally coupled tasks (navigation, information collection, triage); configurable information complexity (3-info vs 6-info); reward density variants (sparse vs non-sparse); presence of hazards (HAZ), points-of-interest (POI), obstacles and final rescue targets; requires spatial grounding and domain-aware prioritisation.",
            "state_conditioning": true,
            "state_conditioning_details": "LLM exploration queries condition on current state s_t, short-term trajectory buffer ξ (recent transitions within the episode), and long-term memory buffer D (summaries of past interactions and success/failure at semantically meaningful locations). Context extractor maps verbal inputs to spatial states s_j and semantic categories cat_j which are fed into attention-space shaping.",
            "novelty_mechanism": false,
            "novelty_mechanism_details": null,
            "complementary_systems": "Hierarchical Strategic Decision Engine (SDE) and specialised Worker modules (navigation w_TN, information collection w_TI, triage w_TT); Attention Space mechanism for policy/reward/action shaping; Retrieval-Augmented Generation (RAG) knowledge base; trajectory and memory buffers (ξ, D).",
            "performance_llm_curriculum": "LLM-guided agent variants (HierQ-LLM) substantially outperform flat, non-hierarchical agents in the SAR gridworld. Reported high-level outcomes: HierQ-LLM variants achieve substantially higher mission success and information-collection rates than flat Q agents; in reported experiments HierQ-LLM reaches mission-success rates in the range shown as much higher than flat baselines (example reported value in paper: HierQ-LLM MSR reported as 83.4% in a 3-info non-sparse configuration in Table III; overall HierQ-LLM frequently achieves ≈60%+ MSR in many settings). As exploration facilitators, model-specific accuracy on action predictions varies by setting: Hermes3 (8B) attains near-perfect guidance in 3-info settings (≈99.7–99.8% accuracy for 3-info sparse/non-sparse), Llama3.1 (8B) leads in 6-info (≈96.4–98.9%), while Gemma2 (9B) is an excellent context extractor (100% accuracy, zero errors on a 14-sample test set) but shows degraded zero-shot exploration accuracy under sparse/6-info conditions (drops to ~69.8% in sparse 6-info).",
            "performance_manual_curriculum": null,
            "performance_heuristic_curriculum": "Heuristic/manual shaping baselines (policy shaping, reward shaping, action shaping) and hierarchical agents without LLM guidance are included for comparison. Flat agents (Q, Q-PS, Q-RS, Q-AS) exhibit very low MSR (reported 'below 15%' across scenarios). Hierarchical agents with heuristic shaping (e.g., HierQ-PS, HierQ-RS, HierQ-AS) substantially outperform flat agents; e.g., HierQ-PS reported MSR ≈62.0% in the non-sparse 3-info setting (Table III), showing that heuristic policy shaping alone is effective. The best results arise from hybrid combinations (HierQ-LLM + policy shaping).",
            "performance_no_curriculum": "Flat Q-learning agents without hierarchy or shaping show near-zero mission success (MSR reported as ~0% in many settings in Table III).",
            "has_curriculum_comparison": true,
            "task_diversity_metrics": null,
            "transfer_generalization_results": null,
            "computational_cost": "Per-model response times reported for context-extraction accuracy benchmarking: Gemma2 (9B) avg ≈15.3 s per query; Tulu3 (8B) ≈9.8 s; Llama3.1 (8B) ≈13.1 s; Hermes3 (8B) ≈10.7 s; smaller models (Gemma3 4B ≈4.9 s; Llama3.2 3B ≈2.7 s; Qwen2.5 3B ≈2.3 s). No dollar-cost accounting provided.",
            "failure_modes_limitations": "Noted failure modes include: (1) Dependency on LLM output quality — misclassification or retrieval mismatches from RAG can mislead shaping (e.g., retrieving irrelevant domain manuals); (2) Model-specific generalisation limits — Gemma2, despite perfect context extraction, degrades as an exploration facilitator in sparse/long-horizon (6-info) settings; (3) Partial pipeline substitution bottleneck — replacing only the information-collection worker with LLM guidance leaves navigation and triage under RL, so poor RL subcomponents can limit overall mission success; (4) Human language variability — framework assumes reasonably clear verbal inputs; vague/ambiguous language may require explicit interactive clarification; (5) Computational latency — LLM response times (several seconds) may affect real-time applicability.",
            "long_horizon_performance": "Performance degrades when moving from 3-info to 6-info (longer-horizon) tasks; LLM facilitators demonstrate different robustness by horizon: Hermes3 excels in short-horizon (3-info) sparse and non-sparse settings (near-perfect), whereas Llama3.1 generalises better to longer 6-info missions (high accuracy ~96–99%). Overall, hierarchical shaping + LLM guidance mitigates but does not fully remove long-horizon difficulty.",
            "specialized_domain_performance": "Domain is SAR (requires spatial reasoning and safety prioritisation); LLM context extraction effectively maps verbal spatial reports into POI/HAZ entities (Gemma2 achieved 100% accuracy on a 14-input test set). No results reported for domains requiring deep technical scientific expertise beyond SAR-style spatial/operational knowledge.",
            "ablation_studies": null,
            "model_size_scaling": "Paper benchmarks 13 pre-trained LLMs across sizes (3B–9B). Larger models (Gemma2 9B) gave best context-extraction accuracy (100%), while smaller models traded speed for reduced accuracy (e.g., Llama3.2 3B ≈92.7% accuracy, Qwen2.5 3B ≈73.6% accuracy). As exploration facilitators, 8B models (Hermes3, Llama3.1, Tulu3) performed best for respective horizon settings. Overall, performance trends suggest quality improves with model capacity in context extraction, but exploration robustness depends on model architecture/variant, not strictly size alone.",
            "key_findings_curriculum_effectiveness": "Key findings: (1) LLMs can effectively supply structured, spatially-grounded context that when injected via an attention-space to shape policy, reward and action space, improves hierarchical agent performance; (2) LLMs used as zero-shot exploration facilitators reduce inefficient trial-and-error and improve information-collection worker performance, but their effectiveness depends on horizon and feedback density (short-horizon/dense settings show near-perfect results, long-horizon/sparse settings expose generalisation limits); (3) Hierarchical decomposition is essential — flat agents, even when augmented with LLM context, fail on complex multi-phase tasks; (4) Hybrid approaches (hierarchy + LLM guidance + policy shaping) yield the best results, demonstrating complementarity between heuristic shaping and LLM reasoning; (5) computational latency and dependence on LLM retrieval/hallucination robustness are important practical constraints.",
            "uuid": "e2051.0"
        },
        {
            "name_short": "Policy/Reward/Action Shaping (Heuristic curricula)",
            "name_full": "Policy shaping, Reward shaping and Action-space shaping (attention-space heuristic methods)",
            "brief_description": "Heuristic shaping methods embedded in the LUCIFER attention-space that bias the agent's Q-values, modify rewards via potential-based shaping and semantic signals, and restrict/expand action spaces based on LLM-processed contextual insights or designer choices.",
            "citation_title": "here",
            "mention_or_use": "use",
            "curriculum_generator_type": "heuristic-based / manual shaping (no explicit curriculum generator)",
            "llm_model_name": null,
            "llm_model_size": null,
            "curriculum_description": "These methods do not produce curricula but act as per-step heuristics to bias learning: Policy shaping biases Q(s,a) via Ψ(s,a) penalties/incentives for actions leading to undesirable/desirable/critical states; Reward shaping uses potential-based reward shaping (Φ(s)) combined with semantic immediate shaping Φ_Ψ(s) derived from contextual categories to produce R''(s,a); Action-space shaping toggles available actions A(s)→A'(s) by removing or adding actions based on detected semantic states (s_u, s_d, s_o). When combined with hierarchy, these heuristics serve to prioritise intermediate goals and improve safety.",
            "domain_name": "Same SAR gridworld as above (2D OpenAI Gym; 3D Gazebo proof-of-concept)",
            "domain_characteristics": "Multi-stage mission with interdependent subtasks, sparse vs dense reward regimes, and spatial hazards requiring safety-aware action masking.",
            "state_conditioning": true,
            "state_conditioning_details": "Shaping conditions on detected contextual entities c_j mapped to states s_j and semantic categories (undesirable s_u, desirable s_d, objective s_o) derived either from LLM context extraction or domain knowledge; reward shaping uses spatial potential over S_ref and semantic flags.",
            "novelty_mechanism": false,
            "novelty_mechanism_details": null,
            "complementary_systems": "Used with hierarchical planner (SDE), workers, and optionally with LLM context extractor; integrates with Q-learning updates (modifies maximisation set to A').",
            "performance_llm_curriculum": null,
            "performance_manual_curriculum": "Heuristic shaping (policy, reward, action shaping) significantly improves performance over no-shaping on hierarchical agents. Example: HierQ-PS reported MSR ~62.0% in non-sparse 3-info (Table III), and heuristic shaping generally produced safety improvements (MSWC ≈ MSR in many shaped configurations).",
            "performance_heuristic_curriculum": "See above (heuristic shaping is the curriculum-like baseline). Heuristic policy shaping and action shaping both boost mission success and safety; their combination with hierarchy yields robust gains compared to flat baselines.",
            "performance_no_curriculum": "Flat agents without hierarchy or shaping had near-zero mission success (MSR ≈ 0% in many settings).",
            "has_curriculum_comparison": true,
            "task_diversity_metrics": null,
            "transfer_generalization_results": null,
            "computational_cost": "Heuristic shaping imposes negligible online computational cost compared to LLM inference; no explicit cost numbers provided.",
            "failure_modes_limitations": "Heuristic shaping depends on correct identification of semantic states; overly restrictive action masking can remove necessary exploratory actions; policy-shaping integration into function-approximation (DRL) is non-trivial and was not addressed in this tabular-RL study.",
            "long_horizon_performance": "Shaping helps but does not fully resolve long-horizon (6-info) degradation; hierarchical decomposition combined with shaping is necessary to maintain acceptable performance as horizon increases.",
            "specialized_domain_performance": null,
            "ablation_studies": null,
            "model_size_scaling": null,
            "key_findings_curriculum_effectiveness": "Heuristic shaping acts as an effective complementary mechanism to hierarchy and LLM guidance: it improves safety and mission success, and when combined with LLM-guided exploration yields the best empirical results. However, heuristic shaping alone (without hierarchy) fails to salvage flat agents.",
            "uuid": "e2051.1"
        },
        {
            "name_short": "Related LLM-based curriculum/goal-generation works",
            "name_full": "Prior literature that uses LLMs to generate sub-goals, intrinsic goals, or curriculum schedules (mentioned in related work)",
            "brief_description": "The paper cites multiple prior works that apply LLMs to design rewards, decompose tasks into subtasks, or generate intrinsic goals/curricula for RL agents, e.g., papers on LLMs as reward designers, LLM planners, autotelic LLM exploration, and automated policy learning workflows (Learningflow) for curriculum RL.",
            "citation_title": "",
            "mention_or_use": "mention",
            "curriculum_generator_type": "LLM-generated (in cited works) — includes intrinsic goal generation, high-level planning, and curriculum RL workflows",
            "llm_model_name": null,
            "llm_model_size": null,
            "curriculum_description": "Cited prior work includes multiple paradigms: LLMs as reward designers (design dense/nuanced rewards), LLMs as high-level planners that break tasks into sub-tasks and select subgoals dynamically, and LLM-driven generation of intrinsic goals to promote open-ended exploration; additionally, collaborative multi-LLM workflows for curriculum RL in urban driving (Learningflow) are mentioned. These works use LLMs to produce sub-goals or curricula that shape agent training dynamics across episodes.",
            "domain_name": "Various domains in cited literature: robotics manipulation, urban driving, open-ended goal-conditioned RL, Minecraft, embodied agents; not experiments in the current paper.",
            "domain_characteristics": "Often open-ended or multi-task domains, requiring hierarchical decomposition, long-horizon planning, and domain grounding; some works focus on automated curriculum for policy pretraining or for structured multi-stage tasks.",
            "state_conditioning": null,
            "state_conditioning_details": null,
            "novelty_mechanism": null,
            "novelty_mechanism_details": null,
            "complementary_systems": null,
            "performance_llm_curriculum": null,
            "performance_manual_curriculum": null,
            "performance_heuristic_curriculum": null,
            "performance_no_curriculum": null,
            "has_curriculum_comparison": null,
            "task_diversity_metrics": null,
            "transfer_generalization_results": null,
            "computational_cost": null,
            "failure_modes_limitations": "Paper notes general risks from literature: hallucinations, retrieval mismatches, and domain-mismatch risks when grounding LLM outputs into control systems.",
            "long_horizon_performance": null,
            "specialized_domain_performance": null,
            "ablation_studies": null,
            "model_size_scaling": null,
            "key_findings_curriculum_effectiveness": "The paper positions these prior works as motivation: LLMs can generate rewards, decompose tasks, and propose intrinsic goals (i.e., provide curricula), but the present contribution focuses on integrating LLM outputs as guided heuristics (attention-space) within a hierarchical RL loop rather than wholesale curriculum substitution.",
            "uuid": "e2051.2"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Learningflow: Automated policy learning workflow for urban driving with large language models",
            "rating": 2
        },
        {
            "paper_title": "Augmenting autotelic agents with large language models",
            "rating": 2
        },
        {
            "paper_title": "Autotelic llm-based exploration for goal-conditioned rl",
            "rating": 2
        },
        {
            "paper_title": "Latent reward: Llm-empowered credit assignment in episodic reinforcement learning",
            "rating": 1
        },
        {
            "paper_title": "Motif: Intrinsic motivation from artificial intelligence feedback",
            "rating": 1
        }
    ],
    "cost": 0.019950999999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>LUCIFER: Language Understanding and Context-Infused Framework for Exploration and Behavior Refinement</p>
<p>Dimitris Panagopoulos d.panagopoulos@cranfield.ac.uk 
Faculty of Engineering and Applied Science
Cranfield University
MK43 0ALCranfieldUK</p>
<p>Adolfo Perrusquía adolfo.perrusquia-guzman@cranfield.ac.uk 
Faculty of Engineering and Applied Science
Cranfield University
MK43 0ALCranfieldUK</p>
<p>Weisi Guo weisi.guo@cranfield.ac.uk 
Faculty of Engineering and Applied Science
Cranfield University
MK43 0ALCranfieldUK</p>
<p>LUCIFER: Language Understanding and Context-Infused Framework for Exploration and Behavior Refinement
B78799BEEBF880EEB911E35C4F5FD30FHierarchical Decision-MakingLarge Language ModelsContext-Aware AgentsHuman-AI Collaboration
In dynamic environments, the rapid obsolescence of pre-existing environmental knowledge creates a gap between an agent's internal model and the evolving reality of its operational context.This disparity between prior and updated environmental valuations fundamentally limits the effectiveness of autonomous decision-making.To bridge this gap, the contextual bias of human domain stakeholders, who naturally accumulate insights through direct, real-time observation, becomes indispensable.However, translating their nuanced, and context-rich input into actionable intelligence for autonomous systems remains an open challenge.To address this, we propose LUCIFER (Language Understanding and Context-Infused Framework for Exploration and Behavior Refinement), a domain-agnostic framework that integrates a hierarchical decision-making architecture with reinforcement learning (RL) and large language models (LLMs) into a unified system.This architecture mirrors how humans decompose complex tasks, enabling a high-level planner to coordinate specialised sub-agents, each focused on distinct objectives and temporally interdependent actions.Unlike traditional applications where LLMs are limited to single role, LUCIFER integrates them in two synergistic roles: as context extractors, structuring verbal stakeholder input into domain-aware representations that influence decision-making through an attention space mechanism aligning LLM-derived insights with the agent's learning process, and as zero-shot exploration facilitators guiding the agent's action selection process during exploration.We benchmark various LLMs in both roles and demonstrate that LUCIFER improves exploration efficiency and decision quality, outperforming flat, goalconditioned policies.Our findings show the potential of contextdriven decision-making, where autonomous systems leverage human contextual knowledge for operational success.</p>
<p>I. INTRODUCTION</p>
<p>A. Background &amp; Motivation</p>
<p>M ODERN autonomous systems are increasingly de- ployed in dynamic and high-stakes settings, ranging from industrial inspection to rescue efforts in the aftermath of man-made or natural disasters [1]- [4].In these complex operational contexts, information about key environmental elements (e.g., navigable routes, safe operating zones, or key inspection points) can become outdated, misleading both human teams and autonomous systems.As conditions evolve over time, prior knowledge can deviate from the objective reality on the ground.This rapid obsolescence introduces knowledge gaps between an agent's (i.e., human or robot) contemporary understanding of the world and the updated valuation of environmental features moments later, leading to poor situational awareness [5], [6].</p>
<p>Human stakeholders operating in these environments [7], [8], whether technical experts or individuals on-site, naturally gather situation-specific clues through direct observation.This continuous exposure offers localised bias that can plug knowledge gaps left by obsolete environmental information [9]- [11].However, their verbalised input, often rich and composite in nature, is rarely integrated into planning or decision-making pipelines.Although current solutions operate under a humanon-the-loop paradigm, where a centralised command center supervises robotic decisions, they fall short in leveraging the real-time verbalised input from on-site stakeholders [12].This reflects a tendency in autonomous systems design to neglect meaningful human-in-the-loop engagement in highstakes environments, even though methodologies already allow for human influence in the decision-making process [13].Bridging this communication barrier, so that agents can effectively integrate multifaceted language-based descriptions into a structured learning mechanism, remains a critical challenge, highlighting the need for adaptive decision-making frameworks capable of leveraging real-time insights.</p>
<p>Reinforcement learning (RL) offers a promising approach to enable agents to iteratively refine their policies based on environmental feedback [14].However, conventional RL approaches, which rely on conventional flat policies operating over entire state spaces, struggle to handle the complexity of environments with distant or interdependent goals.In goal-conditioned RL, for instance, accurately estimating state values for distant goals remains challenging, necessitating a structured approach that prioritises reaching intermediate objectives first.Developing a framework that embraces a hierarchical structure becomes essential, as it enables agents to plan policies over manageable subsets of the state space [15].Through task decomposition, which mirrors human cognitive resource allocation [16], agents can better coordinate multiple interdependent tasks [17], where decisions in one task influence the success of others.This hierarchical approach, when enhanced with contextual insights from stakeholders, forms the foundation of our proposed solution.</p>
<p>In this article, we propose LUCIFER (Language Understanding and Context-Infused Framework for Exploration and Behavior Refinement) to address these arXiv:2506.07915v1[cs.AI] 9 Jun 2025 Fig. 1.Overview of LUCIFER's workflow.The Strategic Decision Engine (SDE) assigns tasks based on environmental observations and Information Space, while Worker agents interact with the Environment by executing primitive actions.The Context Extractor processes verbal inputs into structured representations, which refine decision-making via the Attention Space.The Exploration Facilitator leverages past interactions and learning status to guide exploration by making zero-shot predictions.</p>
<p>challenges.First, LUCIFER systematically integrates contextual knowledge by leveraging LLMs as context extractors.This component transforms linguistic inputs from ground-level stakeholders into structured, actionable insights.These insights are then embedded into the agent's decisionmaking process via an attention space mechanism to ensure that agent behaviors are grounded in up-to-date contextual knowledge rather than obsolete priors.Second, to effectively handle temporal coupling and long-horizon task complexity, LUCIFER adopts a hierarchical policy structure operating at multiple temporal scales.This approach, mirroring how humans decompose tasks to manage limited cognitive resources, enables specialised agents to develop task-specific behaviors and operate across distinct tasks, while accounting for temporal coupling, where one decision can impact the success of another.Finally, recognising the challenges of randomly exploring such environments, LUCIFER employs LLMs in a complementary role as exploration facilitators.</p>
<p>Here, we exploit the advanced reasoning capabilities of LLMs to make predictions during the action selection process of specialised agents and reduce reliance on traditional RL-driven trial-and-error exploration.</p>
<p>Although originally inspired by search and rescue (SAR) scenarios, where timely updates about hazardous zones or trapped survivors are critical, the framework is readily adaptable to a broad range of domains (e.g., manufacturing lines, agricultural robotics, assistive robotics, environmental monitoring).The underlying principles of dynamic contextual knowledge integration and hierarchical coordination equally apply to other fields where autonomy and human expertise intersect.As an illustrative example, we demonstrate how LUCIFER supports multi-objective missions in a simulated SAR environment, highlighting notable gains in exploration efficiency and overall decision quality.The proposed framework is motivated by the analysis of UAV-supported SAR operations [18] showing that effective decisions rely on continuously updated knowledge maps that incorporate live field data from both autonomous agents and human stakeholders.Bearing that in mind, we emphasise the need for systems capable of continuous information integration, processing, and adaptation to ground truths in order to improve real-time, context-driven decision-making.</p>
<p>B. Contributions</p>
<p>In summary, this article makes the following contributions: 1) We introduce LUCIFER, a scalable, hierarchical domain-agnostic framework that enables the systematic fusion of situational knowledge into autonomous decision-making systems.This addresses a fundamental challenge particularly in human-AI collaboration by providing a generalisable approach to leveraging domainrelevant cues in dynamic environments.</p>
<p>2) We demonstrate the dual functionality of off-the-shelf LLMs highlighting their versatility in emulating humanlike extraction process and exhibiting zero-shot, trainingfree predictive capabilities.3) We develop an attention space mechanism that bridges the gap between LLM-processed human insights with RL, providing a structured approach to representing contextual bias integration.4) We compare different LLM variants using a SAR environment as a testbed and demonstrate that LUCIFER outperforms conventional single-policy baselines.Our results provide valuable insights into the practical implementation of LLM-assisted intelligent systems.
B : V → C 2: Set C ← ∅ 3: for all v i ∈ V do 4:
Extract key entities E i from v i using L B</p>
<p>5:</p>
<p>for all e j ∈ E i do 6:</p>
<p>Infer contextual meaning of e j based on I</p>
<p>7:</p>
<p>Assign category label cat j using I priorities 8:</p>
<p>Structure contextual representation: c j = (e j , cat j )</p>
<p>Add c j to C</p>
<p>II. RELATED WORK</p>
<p>The integration of LLMs into computational systems has opened important directions in current AI research, especially since the advert of ChatGPT and similar large-scale models era.This section reviews key paradigms of LLM combined with learning schemes and their role in environmental information processing.We build upon the comprehensive taxonomies and analyses established in recent survey works [19]- [22].</p>
<p>A. Integration Paradigms of LLMs and RL</p>
<p>Following Cao et al.'s [21] taxonomy, LLMs can serve multiple roles in RL systems.One prominent role is as reward designers, where they have the potential to design or shape reward functions.For instance, [23] and [24] utilise LLMs to generate nuanced reward signals aligned with human preferences.Similarly, based on the characteristics of the environment, task, or agent's behavior, existing works leverage LLMs to design reward functions [25]- [30].Extending this, [31] employs LLMs to empower credit assignment, bridging a critical challenge in episodic RL tasks.</p>
<p>Due to the credit assignment problem, LLMs can break down complex tasks into structured sub-tasks, acting as highlevel planners [32]- [34].For example [35], [36] demonstrate how LLMs can dynamically select appropriate sub-goals during task execution.Recent works such as [37] emphasise using LLMs to generate intrinsic goals to promote open-ended exploration for RL agents.Similarly, the work in [38] highlights interactive planning systems where LLMs generate, explain, and prioritise sub-goals to allow RL agents to handle diverse tasks seamlessly.In urban driving settings, [39] illustrates a novel collaborative automated policy training workflow, which consists of multiple LLM agents for curriculum RL.</p>
<p>A fundamental distinction established by Luketina et al. [19] is between language-conditional and language-assisted RL.In the former scheme agents directly interact with the environment through language instructions [40], [41].Conversely, in the latter, language serves as a medium of communicating domain knowledge without being integral to the core task [42]- [44].Our work aligns with the language-assisted RL paradigm.Here, language is used to convey broader descriptive information about the environment's structure, properties, and dynamics, rather than just surface-level instructions about what actions an agent should take.</p>
<p>B. Environmental Information Processing</p>
<p>Another crucial role of LLMs when integrated with RL systems, highlighted by Cao et al. [21], is serving as information processors.This bridges the gap between descriptive environmental information and structured agent inputs through feature representation extraction or language translation.In feature representation extraction, it is shown how frozen or fine-tuned LLMs can extract meaningful representations from environment observations [45]- [48].In language translation, environmental and task instruction information is grounded into formal task-specific language to reduce learning complexity [49]- [51].Recent studies also demonstrate the use of generative models for direct entity and relation extraction from natural language inputs.Models like REBEL [52] and Instruc-tUIE [53] reformulate extraction as a generative task, enabling models to identify key elements and classify them into relevant categories.These works, however, typically operate in isolation from downstream control systems, and are rarely embedded into interactive agents that act upon the extracted knowledge.In contrast, LLMs have demonstrated potential in translating abstract ethical principles into actionable behaviors such as avoiding socially sensitive areas during autonomous navigation [54].LUCIFER extends this direction by grounding context into spatially structured constraints that adapts agent behavior in real time.To the best of our knowledge, no existing studies have explored the direct integration of LLMdriven entity classification into a hierarchical control loop, where language serves as an input channel for contextual information and the extracted knowledge continuously shapes agent behavior.</p>
<p>Our proposed framework aligns naturally with the taxonomy proposed in [55], which categorises informed machine learning approaches along three dimensions: knowledge source, knowledge representation, and knowledge integration.Knowledge source in LUCIFER originates from human stakeholders who possess domain-relevant contextual biases over the operational environment.Knowledge representation is encoded through LLMs, further refined via a Retrieval-Augmented Generation (RAG) pipeline [56], that turns linguistic verbal inputs into actionable, spatially-relevant insights.Finally, knowledge integration is facilitated by an attention space mechanism that supports decision-making and guides exploration to address the challenges typically associated with exploratory processes.</p>
<p>III. METHODOLOGY</p>
<p>LUCIFER extends the framework introduced in [57], by introducing a novel dual-role for LLMs enabling both context extraction and exploration guidance and enhancing the attention-space mechanism to shape policy, reward, and action space.Additionally, it includes a comprehensive benchmarking of multiple LLMs.These advancements transform the original framework into a domain-agnostic and scalable system capable of generalising across diverse operational domains that require structured task execution and real-time adaptation, while offering a more robust and detailed evaluation of its core components (see Fig. 1).</p>
<p>A. Markov Decision Process Configuration</p>
<p>The foundation of our proposed framework, LUCIFER, is a Markov Decision Process (MDP) designed to model the hierarchical decision-making problem.We define the MDP as a tuple ⟨S, A, T, R, γ⟩.Here, S is a finite set of states and A is a finite set of primitive actions, T :
S × A × S → [0, 1] is the state transition probability function, R : S × A → R is the reward function, γ ∈ [0, 1]
is the discount factor balancing immediate and future rewards.</p>
<p>B. Hierarchical Task Decomposition</p>
<p>To address the complexity of long-horizon tasks, the framework employs a hierarchical structure that decomposes complex decision-making problems into simpler, distinct subproblems [15].In our hierarchical two-layer structure, a highlevel planner assigns structured tasks (i.e., goals), while specialised workers produce primitive actions within the environment to execute them.This ensures effective policy switching between intermediate goals that must be achieved en route to the final goal.</p>
<p>Formally, let U = {U 1 , U 2 , . . ., U n } represent the set of tasks, where each task U i is delegated to a worker w i ∈ W responsible for executing it.Specifically, at the high-level, a Strategic Decision Engine (SDE) produces tasks U i ∈ U that specify desired changes in state observations.The SDE selects tasks by either sampling from its policy π U : S → U or using a pre-defined task transition process.At the low-level, a Worker module is activated, where each worker operates under a policy π W : S → A tailored to its assigned task U i .Task transitions are governed by a termination condition β : S → {0, 1}, defined for each task U i .Specifically, β(s) = 1 if the current state s ∈ S satisfies the completion criteria for the active task U i (e.g., completing a target state or sub-goal), and β(s) = 0 otherwise.This ensures that a worker continues executing actions under π W until the intermediate goal is met, at which point the SDE advances to the next task U i+1 .This process repeats until the final goal is achieved, aligning the hierarchical execution with the underlying MDP dynamics.</p>
<p>C. Information Space</p>
<p>The Information Space serves as a reference map to the SDE, ensuring that the agent operates within a structured, mission-relevant knowledge domain, particularly in environments where contextual information is critical.It is defined as a collection I = {I 1 , I 2 , . . ., I m }, where each I j represents a distinct category, type, or unit of information pertinent to the agent's operational objectives.These elements such as mission priorities, constraints, and safety considerations define the scope of meaningful information for the system, thus guiding its perception, reasoning, and actions.</p>
<p>D. LLM as Context Extractors</p>
<p>The Context Extractor component leverages LLMs to convert verbal inputs into structured representations that can be directly utilised within the decision-making pipeline.This transformation is essential, as raw linguistic inputs often lack the structured format required by autonomous systems.The LLM processes these inputs, extracts key entities, and maps them to predefined categories within the Information Space I.The resulting structured output, denoted as C, informs and shapes the agent's situational understanding (see Section III-F).</p>
<p>Formally, given a verbal input space V = {v 1 , . . ., v m }, the LLM acts as a transformation function L B : V → C enhanced by a RAG pipeline.This pipeline augments the LLM's domain-specific expertise through integration with a knowledge base B, enabling context-aware interpretation without the need for extensive retraining or fine-tuning.For each input v i ∈ V , the LLM identifies a set of relevant entities E i and infers their contextual meaning based on the structured defined in Information Space I.Each extracted entity e j is then classified into a category cat j according to I.This process, outlined in Algorithm 1, ensures that the agent receives precise, spatially-grounded insights C = {c 1 , . . ., c n } aligned with its operational objectives and optimised for downstream decision-making.</p>
<p>E. LLM as Exploration Facilitator</p>
<p>The Exploration Facilitator uses LLMs to improve exploration by predicting plausible actions at decision points, leveraging both short-term learning behavior and long-term experience.Rather than relying solely on trial-and-error learning, this approach supplements conventional action selection strategies, such as ε-greedy exploration, with informed, zeroshot predictions.</p>
<p>Formally, the agent maintains two types of buffers.The trajectory buffer ξ captures short-term, episodic behavior by storing transitions of the form {(s t , a t , s t+1 , r t )} within a single episode.This buffer is reset at the start of each new episode and provides the LLM with a local view of recent agent behavior and outcomes.In contrast, the memory buffer D is a persistent, cross-episodic record that accumulates structured summaries of past experiences across tasks.It captures past interactions at semantically meaningful locations (e.g., visited regions, task-relevant states), along with associated action attempts and their success or failure.Both buffers are maintained and encoded as structured, text-based representations.This design choice let the LLM leverage its reasoning capabilities as it is enabled to better reason over the agent's cumulative experience and reuse historical knowledge across episodes.At each exploratory decision point, the agent queries the LLM using the current state s t , the recent trajectory ξ, and the long-term memory D. The LLM returns a predicted action with the highest probability a * = arg max a∈A P (a | s t , ξ, D).This zero-shot, training-free predictive mechanism, shown in Algorithm 2, reframes exploration as a structured reasoning task, where the LLM fuses immediate context with longterm patterns to guide the agent toward informative and goalaligned behaviors.</p>
<p>F. Attention Space</p>
<p>The attention space mechanism-distinct fundamentally from transformer-based self-attention that computes token for all a ∈ A(s) leading to s j do 5:</p>
<p>Compute Ψ(s, a) using Eq. (</p>
<p>Overwrite Q ′ (s, a) ← Ψ(s, a) Extract relevant state s j and category cat j from c j 4:</p>
<p>Update Φ Ψ (s j ) using Eq. ( 6)</p>
<p>5:</p>
<p>for all (s, a, s ′ ) where s ′ = s j do 6:</p>
<p>Compute F (s, s ′ ) using Eq. (</p>
<p>Compute R ′ (s, a) using Eq. (</p>
<p>Compute R ′′ (s, a) using Eq. ( 7)
9:
end for 10: end for 11: return R ′′ (s, a)</p>
<p>Algorithm 5 Attention Space Mechanism for Action Space Adjustment Require: action space A(s), spatially insights C Ensure: updated action space A ′ (s)
1: Initialise A ′ (s) ← A(s) 2: for all c j ∈ C do 3:
Extract relevant state s j and category cat j from c j 4:</p>
<p>Update A ′ (s) using Eq.(8) 5: end for 6: return A ′ (s) relationships-serves as an intermediary between the structured output generated by the LLM (Section III-D) and downstream agents in the hierarchical framework.Unlike token-level attention in language models, this mechanism dynamically refines the agent's decision-making process by embedding contextual structured representation C into the learning process.Unlike arbitrary biasing, this influence is a form of heuristic refinement that leverages environmental structure, aligning with the notion of ecological rationality [58].To do so, it influences key core components, including the policy (π), reward function (R), and action space (A), as illustrated in Fig. 3. Formally, this transformation is defined as:
Ψ : C → (π ′ , R ′ , A ′ )(1)
where C denotes the structured contextual insights (e.g., identified constraints, operational priorities, or task-relevant objectives), π ′ represents a context-aware policy that prioritises relevant actions through modified state-action mappings, R ′ is an adapted reward function that introduces contextual preferences, and A ′ reflects a modified action space that promotes or restricts certain behaviors based on situational relevance.</p>
<p>1
Ψ(s, a) =      −λ u , ∀a leading to s u λ d , ∀a leading to s d λ o , ∀a leading to s o(2)
where λ u , λ d , λ o ∈ R are scalar parameters that penalise or incentivise transitions towards s u , s d , and s o , respectively.The modified Q-values induce a refined policy π ′ , where action preferences are systematically guided by contextual relevance through Ψ(s, a).</p>
<p>2) Reward Shaping: Reward shaping enhances learning efficiency by modifying the reward signal using contextual and spatial information.The final shaped reward function R ′′ (s, a) combines potential-based reward shaping (PBRS), a theoretically sound approach that preserves the optimal policy of the original MDP, with immediate context-sensitive reward adjustments.The potential function Φ(s) encodes spatial preferences over states and is adaptable based on the environment's operational goals.It can be formulated to either encourage proximity to desirable states (e.g., goals, points-of-interest) or discourage proximity to undesirable states (e.g., hazards, unsafe zones).It is defined as:
Φ(s) = f min sr∈Sref dist(s, s r )(3)
where S ref ⊂ S crit , and dist(s, s r ) is a domain-relevant distance metric (e.g., L1, L2 norm, etc.).The transformation function f (•) maps this distance into a scalar potential depending on whether proximity is rewarded or penalised.The potential-based shaping term F (s, s ′ ) is then calculated as the discounted difference in potential between the next state s ′ and the current state s:
F (s, s ′ ) = γΦ(s ′ ) − Φ(s)(4)
This term is added to the original reward to produce the shaped reward:
R ′ (s, a) = R(s, a) + F (s, s ′ )(5)
To incorporate real-time contextual updates from the LLM, an additional immediate shaping signal is applied based on the semantic category of the current state:
Φ Ψ (s) =      −β u , if s = s u β d , if s = s d β o , if s = s o(6)
where β u , β d , β o ∈ R are parameters that modulate penalties or incentives based on semantic state relevance.</p>
<p>The final shaped reward function combines both spatial and semantic shaping:
R ′′ (s, a) = R ′ (s, a) + Φ Ψ (s)(7)
3) Action Space Shaping/Biasing: Beyond policy and reward shaping, the attention space mechanism allows for dynamic toggling of the agent's action space A(s) based on contextual insights.Formally, the adjusted action space is defined as:
A ′ (s) =          A(s) \ {a | a → s u }, if s u detected {a | a → s d , s o }, if s d ors o prioritised A(s) ∪ {a | a → s d , s o }, if expanding exploration A(s), otherwise(8)
First, if an undesirable state s u is present, any action that leads the agent to it is removed from its available action space.Second, in some cases, depending on the problem settings, the system designer may choose to further restrict the action space by keeping only the action that leads to desired states s d or s o .Third, the system designer, alternatively, can expand the action space by adding new actions that move the agent toward desired states s d or s o .For instance, in exploration phases, actions leading to s d or s o might be added to encourage systematic probing of under-explored regions.If none of these conditions apply, the agent's action space remains unchanged.This adjustment effectively transforms the underlying MDP and necessitates consistent application across all aspects of learning.Crucially, the computation of TD targets must also respect this modified action space.For off-policy Q-learning, the standard update:
Q(s t , a t ) ← Q(s t , a t )+α<a href="9">r t+1 +γ max a ′ ∈A Q(s t+1 , a ′ )−Q(s t , a t )</a>
Must be formulated to consider only valid actions in the maximisation operation:
Q(s t , a t ) ← Q(s t , a t )+α<a href="10">r t+1 +γ max a ′ ∈A ′ Q(s t+1 , a ′ )−Q(s t , a t )</a> Similarly, on-policy methods like SARSA must select a ′ from A ′ rather than A. Inconsistent application, where actions are restricted during selection but not during target computation, creates fundamental learning conflicts that prevent convergence, as the agent would simultaneously pursue different optimal policies during action selection versus value updating.</p>
<p>IV. EXPERIMENTAL SETUP</p>
<p>A. Problem Setting</p>
<p>To validate the adaptability of our proposed framework, we consider a post-earthquake urban SAR scenario.The mission involves locating and assisting potential victims, while continuously gathering information about environmental hazards ("HAZ") and safe zones ("POI").Following the hierarchical decomposition introduced in Section III-B, the mission is divided into three specialised sub-tasks.Handled by worker w T N , the navigation sub-task U T N involves moving the agent to designated location using four directional actions.Carried out by worker w T I , the information collection sub-task U T I focuses on collecting critical situational data.This data is delivered directly via verbal inputs from human stakeholders, necessitating robust interpretation of their inputs.Executed by worker w T T , the operational sub-task U T T encompasses rescue activities, debris removal, and triage procedures.A high-level planner orchestrates the sequence of these subtasks, executed in an order that respects domain-specific dependencies captured by the Information Space I.For example, the triage sub-task U T T is deferred until sufficient environmental information is gathered, ensuring safety and operational readiness.Initially, the planner assigns w T N to navigate toward designated collection points.Once reached, w T I is activated to collect relevant information types.This cycle may repeat multiple times across different locations until the planner determines that conditions are met (dictated again by I) to move and initiate the final triage phase.This process is illustrated in Fig. 2.</p>
<p>B. Simulated Environment</p>
<p>We employ a 2D gridworld environment implemented with OpenAI Gym to simulate a disaster area.Each grid cell may represent an empty space, obstacle, information point, hazard, safe zone, or victim location (final location).This setup (Fig. 4) allows for rapid training and fine-grained evaluation of LUCIFER's core modules, including hierarchical decisionmaking, LLM-guided reasoning, and attention-based shaping.Additionally, we develop a 3D ROS2 Gazebo simulation as a proof-of-concept to demonstrate real-world applicability.While no quantitative results are reported from this environment, it showcases the framework's potential for realistic robotic deployment.We assess environmental difficulty along two key dimensions.First, we define two levels of information complexity: 3-info, where the agent must collect three types of information, and 6-info, where six distinct information types must be collected before initiating rescue efforts.Second, we control the reward density by using both sparse and nonsparse reward configurations.In the sparse setting, rewards are only granted upon completing the final objective (e.g., saving the victim), whereas in the non-sparse variant, intermediate rewards guide exploration and collection behavior.All combinations of these settings are tested across the full set of agents presented in this paper.Quantitative results are reported in Table III.Code and data will be made publicly available upon acceptance at https://github.com/dimipan/journalLLM RL.</p>
<p>C. Implementation Details</p>
<p>The simulation environment is implemented in Python using OpenAI Gym.Evaluations were conducted on a system with an Intel i7-12700 CPU, 32 GB RAM, and an Nvidia RTX A2000 GPU (6 GB VRAM).We train RL agents using offpolicy Q-learning with learnign rate α = 0.1, discount factor γ = 0.99, and a decaying ε-greedy exploration strategy.Each experiment is run for 1000 episodes, averaged over 50 independent trials for statistical reliability.The state space includes the agent's position, number of information elements collected, and victim rescue status.The action space consists of system 34 actions in total, distributed across three taskspecific workers in the hierarchical framework.For navigation 4 directional movement actions, for information collection 26 actions, and for triage 4 rescue-related actions).We compare a total of 13 pre-trained LLMs integrated via a local RAG setup (Ollama, Chroma vector store, and LangChain).These models are assessed on their ability to interpret verbal inputs and generate structured outputs as described in Section III-D.The output consists of identified locations and their classified categories (e.g., POI or HAZ) paired with spatial coordinates, which are matched against the domain knowledge base B to support downstream decision-making.A subset of 5 LLMs is further evaluated in their role as exploration facilitators (Section III-E), specifically in guiding worker w T I during information collection by generating zero-shot action predictions.For agents employing the attention-based shaping mechanism, we employ Gemma2 (9B) as the context extractor across all tested scenarios.As exploration facilitators, Hermes3 (8B) is deployed in the 3-info configuration and Llama3.1 (8B) is used in the 6-info setting.</p>
<p>V. RESULTS AND ANALYSIS A. Comparative Performance of LLMs</p>
<p>How do different LLMs perform as Context Extractors for SAR-related inputs?We evaluate the effectiveness of various LLMs in processing verbal inputs relevant to SAR tasks, focusing on their ability to convert natural language into actionable, structured representations.Table I presents a comparative analysis of model performance across a standardised test set of 14 verbal inputs, comprised of 7 simple and 7 complex scenarios, where complexity is defined by the number of locations referenced per input.Each model is evaluated over 80 independent runs.Accuracy (Acc.) is computed via a pointbased scoring system: each correctly identified location earns 1 point, and each correct classification ("POI" vs. "HAZ") earns an additional point, with a maximum of 2 points per location.We track three distinct error types: Location Errors, indicating missed or incorrectly identified locations; Classification Errors, indicating correct locations misclassified as "POI" or "HAZ"; Hallucination Errors indicating identified locations not present in the original input.A model achieves a Success Rate of 100% only if it scores 100% accuracy and produces zero errors in all categories.This metric ensures correctness and reliability that is crucial for safety-critical applications like SAR.Response time is also recorded to evaluate realtime feasibility.</p>
<p>Among all models tested, Gemma2 (9B) delivers the best overall performance, achieving 100.0%accuracy, zero errors, and a 100.0%success rate.However, this comes at the cost of the second longest response time, averaging 15.3 seconds.Notably, none of the models produced hallucination errors, highlighting their reliability in avoiding fabricated or misleading content, which is vital for such high-stakes decisionmaking.</p>
<p>1) 8B Models: The 8B-parameter models exhibit varied performance.Tulu3 (8B) leads this group with 98.5% accuracy, no location errors, 0.3 classification errors, and an 86.3% success rate, paired with a relatively fast response time of 9.8 seconds.Llama3.1 (8B) follows closely with 97.3% accuracy, minimal errors (0.1 location errors and 0.1 classification errors), and an 81.3% success rate, though its response time is longer at 13.1 seconds.Hermes3 (8B) achieves 97.0% accuracy, with 0.1 location errors and 0.4 classification errors, resulting in a 78.8% success rate and a response time of 10.7 seconds.Dolphin3 (8B) records 95.3% accuracy, no location errors, but a higher 0.9 classification errors, leading to a 68.8% success rate and a response time of 9.5 seconds.Llama3: instruct (8B) has the lowest accuracy in this group at 91.4%, with 0.5 location errors and 0.4 classification errors, yielding a 62.5% success rate and a response time of 10.1 seconds.</p>
<p>2) 7B Models: These models generally show slightly lower accuracy and success rates compared to the 8B models.In particular, deepseek-r1 (7B) stands out with 95.1% accuracy (based on 74 successful runs out of 80), 0.3 location errors, 0.2 classification errors, and a 73.0%success rate, but it has the longest response time in this category at 21.6 seconds.Zephyr (7B) achieves 94.5% accuracy (based on 70 successful runs out of 80), with 0.2 location errors and 0.6 classification errors, resulting in a 60.0% success rate and a response time of 12.6 seconds.Mistral (7B) records 93.0% accuracy, 0.4 location errors, no classification errors, and a 58.8% success rate, with a fast response time of 8.2 seconds.Qwen2.5 (7B) has 91.3% accuracy, 0.5 location errors, 0.4 classification errors, and a 63.8% success rate, with a response time of 9.2 seconds.</p>
<p>3) Smaller Models (4B and 3B): Smaller models demonstrate trade-offs between accuracy and response time.As shown, Gemma3 (4B) achieves 93.9% accuracy, with 0.4 location errors and 0.4 classification errors, a 60.0% success rate, and a fast response time of 4.9 seconds.Llama3.2(3B) records 92.7% accuracy, 0.3 location errors, 0.3 classification errors, and a 47.5% success rate, with a very fast response time of 2.7 seconds.Qwen2.5 (3B) has the lowest performance, with 73.6% accuracy, 1.6 location errors, 0.9 classification errors, and a 37.5% success rate, but it offers the fastest response time at 2.3 seconds.How do different LLMs perform as Exploration Facilitators?Table II presents the performance of selected LLMs in their role as Exploration Facilitators, specifically guiding the w T I (information collection) worker in the HierQ-LLM agent across both sparse and non-sparse reward settings, and under 3-info and 6-info configurations.Accuracy and response time are reported for each condition.Three key trends emerge from this evaluation.First, under non-sparse 3-info conditions, all models achieve over 95% accuracy.This suggests that in less complex environments with dense feedback, exploration guidance is straightforward and well-handled by all evaluated models.Second, Gemma2 (9B) shows unexpected performance degradation under both non-sparse 6-info and sparse settings.Accuracy drops to 86.5% in non-sparse 6-info, 72.8% in sparse 3-info, and 69.8% in sparse 6-info.While Tulu3 (8B) also sees a drop in sparse 6-info (81.0%), it still outperforms Gemma2 under these conditions.This indicates potential limitations in Gemma2's generalization to longer-horizon or low-feedback scenarios despite its strong performance as a context extractor.Third, Hermes3 (8B) achieves the highest accuracy under the 3-info configuration, with near-perfect performance in both sparse (99.7%) and non-sparse (99.8%) settings, whereas Llama3.1 (8B) leads under the more complex 6-info settings, with top accuracy in both sparse (96.4%) and non-sparse (98.9%) environments.These patterns suggest Hermes3 is best suited for short-horizon guidance, while Llama3.1 generalises better to longer, more complex missions.Overall, these results highlight that while most models perform well in easy scenarios, sparsity and task length expose important differences in robustness and generalisation.</p>
<p>B. Comparison of Shaping Methods</p>
<p>To quantitatively evaluate agent performance under varying levels of complexity and uncertainty, we employ five core metrics that capture both task effectiveness and decision quality.Mission Success Rate (MSR): The percentage of episodes in which the agent completes the full mission by collecting all required information types and executing the correct rescue action at the designated target location.Information Collection Success Rate (ICSR): The percentage of episodes where the agent successfully collects all required information types, regardless of whether the final rescue action is completed.Predictor Success Rate (PSR): For agents using an LLM to infer the correct information type, this metric reflects the accuracy of those predictions.For agents not using an LLM, it is defined as the accuracy of collecting the correct information through random action sampling.Mission Success Without Collisions (MSWC): A safety-oriented variant of MSR, counting only missions completed without encountering hazardous states.Average Reward (AR): The mean cumulative reward obtained per episode, indicating overall task efficiency.Table III summarises performance across both sparse and non-sparse reward environments and under 3-info and 6-info task configurations.Results show that flat agents (Q, Q-PS, Q-RS, Q-AS), even when enhanced with LLM-based context extraction, consistently exhibit low MSR-remaining below 15% across all scenarios.This demonstrates the limitations of non-hierarchical architectures in managing complex, multi-phase tasks.Hierarchical agents outperform their flat counterparts, demonstrating higher MSR, ICSR, and MSWC scores across the board.Shaping techniques, especially policy shaping (PS) and action shaping (AS), further boost mission success and safety.As expected, moving from 3-info to 6-info configurations results in noticeable performance degradation, especially in sparse environments.This trend confirms the increased difficulty of longer-horizon missions requiring greater contextual understanding and planning.Nonetheless, shaped hierarchical agents maintain comparatively high performance across all metrics, showcasing their adaptability and robustness in uncertain, high-stakes scenarios.A detailed discussion and interpretation of these performance trends is provided in Section VI.</p>
<p>VI. DISCUSSION &amp; LIMITATIONS</p>
<p>Taken together, the results validate the effectiveness of combining hierarchical task structure with targeted shaping mechanisms, particularly in sparse and complex environments where flat agents consistently underperform.Hierarchical decomposition, when paired with shaping, supports more effiecient and safe behavior.Beyond these broad trends, closer inspection of agent-specific outcomes reveals how different components of LUCIFER affect performance.Below, we unpack these effects and discuss key limitations, along with directions for future improvement.</p>
<p>A. Interpretation of Shaping and LLM Effects</p>
<p>A closer inspection of Table III reveals several key insights into the effects of shaping mechanisms and LLM integration.</p>
<p>Shaping promotes safety: For flat agents utilising policy or action shaping (e.g., Q-PS, Q-AS), successful missions are always completed safely as shown by MSWC equaling MSR in the non-sparse 3-info condition.This suggests that even with limited success overall, it inherently promotes policy safety.For hierarchical agents, this safety pattern generalises across all configurations, demonstrating the synergy between structured decomposition and shaping.Exploitation drives success: The consistently high MSR achieved by HierQ-PS across all settings may be attributed to its ability to quickly switch to exploitation once all required information is collected.With a shaped Q-table, the agent can complete the mission efficiently and reduce unnecessary exploration and exposure to risk.LLM Integration is bottlenecked by RL components: While HierQ-LLM performs competitively, it may not outperform across the board as expected.This is because the LLM only controls the information collection worker (w T I ); navigation (w T N ) and rescue (w T T ) are still governed by standard RL policies.Thus, poor navigation performance can limit the effectiveness of even perfect LLM-driven guidance.Formally, replacing the RL-based action selection process for a single worker affects only a subset of the entire mission pipeline.In larger environments or tasks requiring more diverse information types, we expect the exploration facilitator's impact to grow.Hybrid methods perform best: Finally, as expected, HierQ-LLM with policy shaping (HierQ-LLM-PS) yields the best overall performance across all configurations showing that attention space and LLM-guided reasoning complement each other.</p>
<p>B. Complete or Sufficient Information</p>
<p>A central design choice in LUCIFER is transitioning from exploration to exploitation once sufficient information, as defined by the Information Space, has been gathered.This switching to a "do" mode, reflecting the behavior of a rescuer who, after a preliminary survey, shifts from scouting to executing a plan, supports rapid mission execution, which is a critical requirement in real-world SAR contexts.However, in practice, information may be incomplete or dynamic.Blindly switching to exploitation based on early inputs could yield suboptimal outcomes if new or contradictory data emerges mid-mission.Future work could explore adaptive mechanisms that allow the agent to reassess confidence or re-enter information-gathering mode when contextual uncertainty increases.</p>
<p>C. Learning Scalability</p>
<p>Our evaluation employs tabular RL agents to make shaping effects interpretable and easy to implement.Although reward and action space shaping techniques are readily transferable to deep reinforcement learning (DRL) settings as they operate on universal RL components, policy shaping is more challenging due to the absence of explicit Q-tables in functionapproximated policies.Future work could explore embedding LLM-derived signals into policy networks.Second, while LUCIFER employs a fixed task decomposition based on domain knowledge, learning high-level policies dynamically, such as through a Semi-Markov Decision Process (SMDP), could enhance adaptability by enabling the agent to select tasks autonomously.</p>
<p>D. Dependence on LLMs</p>
<p>LUCIFER's context extractor and exploration facilitator rely on the quality of LLM outputs, which introduces risks.Even with RAG-based augmentation (e.g., with domain manuals or maps), LLMs may hallucinate, misclassify, or suffer from retrieval mismatches.For instance, retrieving data from a flood manual in a wildfire scenario could destabilise behavior.Mitigating this risk requires improving domain adaptation and retrieval accuracy.Crucially, our use of LLMs aligns with prior work advocating for heuristic use of LLM outputs rather than direct policy substitution [33].LUCIFER extends this principle by incorporating LLM-derived knowledge as a guiding heuristic through an attention space to ensure that planning remains adaptive rather than rigidly dictated by the LLM.Additionally, in our framework, the context extractor is integrated into the agent's reasoning and decision-making pipeline.However, it can easily be adapted as a standalone module to automate the translation of complex linguistic reports into structured insights.This could reduce cognitive load on human commanders in SAR command-and-control centers.Future work could explicitly investigate and quantify this operational benefit.</p>
<p>E. Human Language Variability &amp; Communication Gaps</p>
<p>Finally, from the human-in-the-loop perspective, a further limitation arises from the nature of human language [59].Communication in high-pressure situations is often vague or unclear, yet current implementation assumes a reasonable level of clarity in verbal inputs to function effectively.During controlled proof-of-concept experiments, this assumption held allowing us to test the core hypothesis: that spatial insights derived from human linguistic descriptions can steer an agent more effectively than purely autonomous exploration.However, real-world deployments may violate this assumption requiring agents to proactively query human stakeholders to elicit missing or ambiguous information.Developing interactive dialogue mechanisms, driven by uncertainty or task relevance, would allow the agent to close knowledge gaps in real time reframing the RL objective to include informationseeking behaviors.Addressing these limitations represents a promising next step toward more adaptive, scalable, and human-aware learning systems.</p>
<p>VII. CONCLUSION</p>
<p>In summary, we introduce LUCIFER, a domain-agnostic framework that lays the foundation for decision-making systems capable of integrating human knowledge while maintaining algorithmic rigor.It represents a substantial advancement in the synthesis of human and machine intelligence, resulting in an integrated system that is computationally robust and inherently human-centric.Unlike traditional systems that treat humans as passive supervisors, our proposed framework positions stakeholders as active knowledge providers, underscoring the value of human informational processing within computational systems.By unifying dual-role LLMs with a hierarchical architecture, the framework supports context-aware reasoning and behavior in high-stakes environments.Looking ahead, extending the system to support interactive, bidirectional communication between humans and agents presents a compelling direction for future research and a paradigm shift in how intelligent systems are designed and deployed into real-world operations.</p>
<p>Fig. 2 .
2
Fig. 2. Illustration of LUCIFER in a post-earthquake SAR Case Study: The high-level planner decomposes the mission into three sub-tasks-navigation U T N , information collection U T I , and triage U T T -executed by specialised workers.At T 2 , worker w T N navigates to key locations, while w T I gathers critical data, dictated by the Information Space I.An LLM-based Context Extractor processes human inputs to provide structured insights (⋆), and a separate LLM-based Exploration Facilitator guides w T I 's action selection process.These components reduce the agent's reliance on inefficient, high-effort exploratory trajectories (red) and instead enable optimised decision-making pathways (green), enhancing progress toward mission goals.</p>
<p>Algorithm 3
3
Attention Space Mechanism for Policy Shaping Require: spatially insights C, learned table Q(s, a), Attention Space Ψ(s, a), critical states S crit Ensure: updated Q ′ (s, a) 1: Set Ψ(s, a) ← 0, Q ′ (s, a) ← Q(s, a), ∀(s, a) 2: for all c j ∈ C do 3:Extract relevant state s j and category cat j from c j 4:</p>
<p>end for 9: return Q ′ (s, a) Algorithm 4 Attention Space Mechanism for Reward Shaping Require: reward function R(s, a), spatially insights C, discount factor γ, potential function Φ(s) Ensure: final shaped reward function R ′′ (s, a) 1: Initialise Φ Ψ (s) ← 0, ∀s 2: for all c j ∈ C do 3:</p>
<p>Fig. 4 .
4
Fig. 4. 2D and 3D environments used for 3-info configuration.</p>
<p>) Policy Refinement/Shaping: Policy shaping adjusts the agent's decision-making by biasing Q-values of state-action pairs linked to contextually critical states.Let S crit = {s u , s d , s o } represent a set of generalised state categories derived from C: Undesirable states (e.g., states violating constraints or safety thresholds) • s d : Desirable states (e.g., states that fulfill intermediate objectives) • s o : Critical objective states (e.g., states achieving mission-critical goals) The attention space introduces biases into the Q-values as follows:</p>
<p>• s u :</p>
<p>Illustration of the attention space mechanism.The LLM-processed output (i.e., structured into C) guides policy shaping, reward shaping, and action biasing.By embedding contextual insights into these key components, the agent's behavior is adaptively refined to reflect real-time environmental knowledge.
Attention Spacestate-action pair 1Q-valueadjustmentstate-action pair 2Policy Shapingstate-action pair ...state-action pair n[space ]LLM processed outputReward Shapingpotential-based functionUpdated Behavior[ Reward Space][ Reward Space]Action SpaceAction Biasingtoggling[ Action Space A ][ Action Space]Fig. 3.</p>
<p>TABLE III COMPARISON
III
OF AGENTS UNDER NON-SPARSE (LEFT) AND SPARSE (RIGHT) ENVIRONMENTS IN 3 AND 6 INFORMATION SETTING.THE BEST IS MARKED IN BOLD, AND THE SECOND-BEST IN UNDERLINE.
Non-Sparse EnvironmentSparse EnvironmentAgentSetting MSR ICSRPSRMSWCARAgentSetting MSR ICSRPSRMSWCAR(%)(%)(%)(%)(%)(%)(%)(%)Q3-info 6-info0 01.2 1.1≤ 5.0 ≤ 5.00 0-4.9 -4.6Q3-info 6-info0 00 0≤ 5.0 ≤ 5.00 0-32.4 -22.8Q-PS3-info 6-info9.2 043.8 1.8≤ 5.0 ≤ 5.09.2 0-0.7 -3.7Q-PS3-info 6-info0 00 0≤ 5.0 ≤ 5.00 0-32.4 -22.8Q-RS3-info 6-info13.9 041.0 1.5≤ 5.0 ≤ 5.00.9 04.3 -5.7Q-RS3-info 6-info0 00 0≤ 5.0 ≤ 5.00 0-32.4 -22.7Q-AS3-info 6-info10.8 045.9 2.4≤ 5.0 ≤ 5.010.8 01.4 -3.9Q-AS3-info 6-info0 00 0≤ 5.0 ≤ 5.00 0-32.4 -22.7HierQ3-info 6-info59.4 51.663.7 54.4≤ 5.0 ≤ 5.00.5 0.755.0 56.7HierQ3-info 6-info55.7 37.457.1 39.6≤ 5.0 ≤ 5.00.5 0.731.2 5.8HierQ-LLM3-info 6-info60.4 53.665.1 56.999.8 98.90.5 0.855.8 58.8HierQ-LLM3-info 6-info54.8 35.757.7 37.899.7 96.40.3 0.829.1 2.7HierQ-PS3-info 6-info62.0 52.463.3 54.4≤ 5.0 ≤ 5.062.0 52.457.6 56.9HierQ-PS3-info 6-info56.5 38.257.5 39.4≤ 5.0 ≤ 5.056.5 38.231.8 6.3HierQ-RS3-info 6-info58.1 51.363.2 54.7≤ 5.0 ≤ 5.00.8 1.253.1 56.3HierQ-RS3-info 6-info54.5 37.257.2 39.9≤ 5.0 ≤ 5.00.6 1.029.8 5.3HierQ-AS3-info 6-info57.8 51.363.2 54.4≤ 5.0 ≤ 5.057.8 51.351.8 55.3HierQ-AS3-info 6-info54.9 37.957.6 40.1≤ 5.0 ≤ 5.054.9 37.929.3 5.5HierQ-LLM3-info83.484.999.883.487.6HierQ-LLM3-info75.176.099.775.153.3-PS6-info64.866.998.964.875.6-PS6-info44.746.096.444.712.6</p>
<p>Mobility and sensing demands in usar. R Murphy, J Casper, J Hyams, M Micire, B Minten, 2000 26th Annual Conference of the IEEE Industrial Electronics Society. IECON 2000. 2000 IEEE International Conference on Industrial Electronics, Control and Instrumentation. 21st Century Technologies. IEEE20001</p>
<p>Experience in system design for human-robot teaming in urban search and rescue. G.-J M Kruijff, M Janíček, S Keshavdas, B Larochelle, H Zender, N J Smets, T Mioch, M A Neerincx, J V Diggelen, F Colas, Field and Service Robotics: Results of the 8th International Conference. Springer2014</p>
<p>Aerial view localization with reinforcement learning: Towards emulating search-andrescue. A Pirinen, A Samuelsson, J Backsund, K Åström, arXiv:2209.036942022arXiv preprint</p>
<p>Disaster area coverage optimisation using reinforcement learning. C Gruffeille, A Perrusquía, A Tsourdos, W Guo, 2024 International Conference on Unmanned Aircraft Systems (ICUAS). IEEE2024</p>
<p>Artificial intelligence: a modern approach. S J Russell, P Norvig, 2016Pearson</p>
<p>Planning and acting in partially observable stochastic domains. L P Kaelbling, M L Littman, A R Cassandra, Artificial intelligence. 1011-21998</p>
<p>Role of stakeholders in mitigating disaster prevalence: Theoretical perspective. A.-F Saeed, N Kasim, MATEC Web of Conferences. EDP Sciences20192663008</p>
<p>Stakeholder analysis in the context of natural disaster mitigation: The case of flooding in three us cities. A V Ter-Mkrtchyan, A L Franklin, Sustainability. 1520149452023</p>
<p>Quick response disaster research: Opportunities and challenges for a new funding program. G Oulahen, B Vogel, C Gouett-Hanna, International Journal of Disaster Risk Science. 112020</p>
<p>Toward human-centered simulation modeling for critical infrastructure disaster recovery planning. A Ganji, S Miles, 2018 IEEE Global Humanitarian Technology Conference (GHTC). IEEE2018</p>
<p>Understanding human behavior response to disasters. D Erokhin, N Komendantova, Oxford Research Encyclopedia of Natural Hazard Science. 2024</p>
<p>Trusted autonomy between humans and robots: Toward human-on-the-loop in robotics and autonomous systems. S Nahavandi, IEEE Systems, Man, and Cybernetics Magazine. 312017</p>
<p>Robotic urban search and rescue: A survey from the control perspective. Y Liu, G Nejat, Journal of Intelligent &amp; Robotic Systems. 722013</p>
<p>Reinforcement learning: An introduction. R S Sutton, A G Barto, 1998MIT press Cambridge1</p>
<p>Intelligent problem-solving as integrated hierarchical reinforcement learning. M Eppe, C Gumbsch, M Kerzel, P D Nguyen, M V Butz, S Wermter, Nature Machine Intelligence. 412022</p>
<p>C G Correa, M K Ho, F Callaway, T L Griffiths, arXiv:2007.13862Resourcerational task decomposition to minimize planning costs. 2020arXiv preprint</p>
<p>A multi-robot task assignment framework for search and rescue with heterogeneous teams. H Osooli, 2024University of Massachusetts LowellMaster's thesis</p>
<p>Prospective decision modelling of uncrewed aerial vehicle operators to inform design recommendations for future systems. S Hart, V Steane, M Chattington, 2024 IEEE 4th International Conference on Human-Machine Systems (ICHMS). IEEE2024</p>
<p>A survey of reinforcement learning informed by natural language. J Luketina, N Nardelli, G Farquhar, J Foerster, J Andreas, E Grefenstette, S Whiteson, T Rocktäschel, 2019</p>
<p>The rl/llm taxonomy tree: Reviewing synergies between reinforcement learning and large language models. M Pternea, P Singh, A Chakraborty, Y Oruganti, M Milletari, S Bapat, K Jiang, arXiv:2402.018742024arXiv preprint</p>
<p>Survey on large language model-enhanced reinforcement learning: Concept, taxonomy, and methods. Y Cao, H Zhao, Y Cheng, T Shu, Y Chen, G Liu, G Liang, J Zhao, J Yan, Y Li, IEEE Transactions on Neural Networks and Learning Systems. 2024</p>
<p>A survey on enhancing reinforcement learning in complex environments: Insights from human and llm feedback. A R Laleh, M N Ahmadabadi, arXiv:2411.134102024arXiv preprint</p>
<p>Reward design with language models. M Kwon, S M Xie, K Bullard, D Sadigh, arXiv:2303.000012023arXiv preprint</p>
<p>Eureka: Humanlevel reward design via coding large language models. Y J Ma, W Liang, G Wang, D.-A Huang, O Bastani, D Jayaraman, Y Zhu, L Fan, A Anandkumar, arXiv:2310.129312023arXiv preprint</p>
<p>Text2reward: Reward shaping with language models for reinforcement learning. T Xie, S Zhao, C H Wu, Y Liu, Q Luo, V Zhong, Y Yang, T Yu, arXiv:2309.114892023arXiv preprint</p>
<p>Self-refined large language model as automated reward function designer for deep reinforcement learning in robotics. J Song, Z Zhou, J Liu, C Fang, Z Shu, L Ma, arXiv:2309.066872023arXiv preprint</p>
<p>Accelerating reinforcement learning of robotic manipulations via feedback from large language models. K Chu, X Zhao, C Weber, M Li, S Wermter, arXiv:2311.023792023arXiv preprint</p>
<p>Language reward modulation for pretraining reinforcement learning. A Adeniji, A Xie, C Sferrazza, Y Seo, S James, P Abbeel, arXiv:2308.122702023arXiv preprint</p>
<p>Auto mc-reward: Automated dense reward design with large language models for minecraft. H Li, X Yang, Z Wang, X Zhu, J Zhou, Y Qiao, X Wang, H Li, L Lu, J Dai, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024435</p>
<p>Motif: Intrinsic motivation from artificial intelligence feedback. M Klissarov, P D'oro, S Sodhani, R Raileanu, P.-L Bacon, P Vincent, A Zhang, M Henaff, arXiv:2310.001662023arXiv preprint</p>
<p>Latent reward: Llm-empowered credit assignment in episodic reinforcement learning. Y Qu, Y Jiang, B Wang, Y Mao, C Wang, C Liu, X Ji, arXiv:2412.111202024arXiv preprint</p>
<p>Llm-planner: Few-shot grounded planning for embodied agents with large language models. C H Song, J Wu, C Washington, B M Sadler, W.-L Chao, Y Su, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023</p>
<p>Navigation with large language models: Semantic guesswork as a heuristic for planning. D Shah, M R Equi, B Osiński, F Xia, B Ichter, S Levine, Conference on Robot Learning. PMLR2023</p>
<p>Enabling intelligent interactions between an agent and an llm: A reinforcement learning approach. B Hu, C Zhao, P Zhang, Z Zhou, Y Yang, Z Xu, B Liu, arXiv:2306.036042023arXiv preprint</p>
<p>R Yang, J Chen, Y Zhang, S Yuan, A Chen, K Richardson, Y Xiao, D Yang, arXiv:2406.04784Selfgoal: Your language agents already know how to achieve high-level goals. 2024arXiv preprint</p>
<p>Augmenting autotelic agents with large language models. C Colas, L Teodorescu, P.-Y Oudeyer, X Yuan, M.-A Côté, Conference on Lifelong Learning Agents. PMLR2023</p>
<p>Autotelic llm-based exploration for goal-conditioned rl. G Pourcel, T Carta, G Kovač, P.-Y Oudeyer, 2024in Intrinsically Motivated Openended Learning Workshop at NeurIPS 2024</p>
<p>Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents. Z Wang, S Cai, G Chen, A Liu, X Ma, Y Liang, arXiv:2302.015602023arXiv preprint</p>
<p>Learningflow: Automated policy learning workflow for urban driving with large language models. Z Peng, Y Wang, X Han, L Zheng, J Ma, arXiv:2501.050572025arXiv preprint</p>
<p>Interactive language: Talking to robots in real time. C Lynch, A Wahid, J Tompson, T Ding, J Betker, R Baruch, T Armstrong, P Florence, IEEE Robotics and Automation Letters. 2023</p>
<p>Instruction-following agents with multimodal transformer. H Liu, L Lee, K Lee, P Abbeel, arXiv:2210.134312022arXiv preprint</p>
<p>Human as ai mentor: Enhanced human-in-the-loop reinforcement learning for safe and efficient autonomous driving. Z Huang, Z Sheng, C Ma, S Chen, Communications in Transportation Research. 41001272024</p>
<p>Guiding pretraining in reinforcement learning with large language models. Y Du, O Watkins, Z Wang, C Colas, T Darrell, P Abbeel, A Gupta, J Andreas, International Conference on Machine Learning. PMLR2023</p>
<p>Grounding large language models in interactive environments with online reinforcement learning. T Carta, C Romac, T Wolf, S Lamprier, O Sigaud, P.-Y Oudeyer, International Conference on Machine Learning. PMLR2023</p>
<p>History compression via language models in reinforcement learning. F Paischer, T Adler, V Patil, A Bitto-Nemling, M Holzleitner, S Lehner, H Eghbal-Zadeh, S Hochreiter, International Conference on Machine Learning. PMLR2022185</p>
<p>Semantic helm: A human-readable memory for reinforcement learning. F Paischer, T Adler, M Hofmarcher, S Hochreiter, Advances in Neural Information Processing Systems. 202436</p>
<p>Efficient policy adaptation with contrastive prompt ensemble for embodied agents. W K Kim, S Kim, H Woo, Advances in Neural Information Processing Systems. 202436</p>
<p>Recore: Regularized contrastive representation learning of world model. R P Poudel, H Pandya, S Liwicki, R Cipolla, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition202422913</p>
<p>Natural languageconditioned reinforcement learning with inside-out task language development and translation. J.-C Pang, X.-Y Yang, S.-H Yang, Y Yu, arXiv:2302.093682023arXiv preprint</p>
<p>Starling: Self-supervised training of text-based reinforcement learning agent with large language models. S Basavatia, K Murugesan, S Ratnakar, arXiv:2406.058722024arXiv preprint</p>
<p>Informing reinforcement learning agents by grounding language to markov decision processes. B A Spiegel, Z Yang, W Jurayj, B Bachmann, S Tellex, G Konidaris, Workshop on Training Agents with Foundation Models at RLC 2024. 2024</p>
<p>Rebel: Relation extraction by end-to-end language generation. P.-L H Cabot, R Navigli, Findings of the Association for Computational Linguistics: EMNLP 2021. 2021</p>
<p>Instructuie: Multi-task instruction tuning for unified information extraction. X Wang, W Zhou, C Zu, H Xia, T Chen, Y Zhang, R Zheng, J Ye, Q Zhang, T Gui, arXiv:2304.080852023arXiv preprint</p>
<p>Encoding social &amp; ethical values in autonomous navigation: Philosophies behind an interactive online demonstration. Y Tang, L Moffat, W Guo, C May-Chahal, J Deville, A Tsourdos, Proceedings of the Second International Symposium on Trustworthy Autonomous Systems. the Second International Symposium on Trustworthy Autonomous Systems2024</p>
<p>Informed machine learning-a taxonomy and survey of integrating prior knowledge into learning systems. L Von Rueden, S Mayer, K Beckh, B Georgiev, S Giesselbach, R Heese, B Kirsch, J Pfrommer, A Pick, R Ramamurthy, IEEE Transactions on Knowledge and Data Engineering. 3512021</p>
<p>Retrievalaugmented generation for knowledge-intensive nlp tasks. P Lewis, E Perez, A Piktus, F Petroni, V Karpukhin, N Goyal, H Küttler, M Lewis, W -T. Yih, T Rocktäschel, Advances in neural information processing systems. 202033</p>
<p>Selective exploration and information gathering in search and rescue using hierarchical learning guided by natural language input. D Panagopoulos, A Perrusquia, W Guo, 2024 IEEE International Conference on Systems, Man, and Cybernetics (SMC). IEEE2024</p>
<p>Resource-rational analysis: Understanding human cognition as the optimal use of limited computational resources. F Lieder, T L Griffiths, Behavioral and brain sciences. 43e12020</p>
<p>The communicative function of ambiguity in language. S T Piantadosi, H Tily, E Gibson, Cognition. 12232012</p>            </div>
        </div>

    </div>
</body>
</html>
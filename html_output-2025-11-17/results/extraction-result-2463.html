<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2463 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2463</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2463</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-67.html">extraction-schema-67</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <p><strong>Paper ID:</strong> paper-5680859</p>
                <p><strong>Paper Title:</strong> Entropy-Based Search Algorithm for Experimental Design</p>
                <p><strong>Paper Abstract:</strong> The scientific method relies on the iterated processes of inference and inquiry. The inference phase consists of selecting the most probable models based on the available data; whereas the inquiry phase consists of using what is known about the models to select the most relevant experiment. Optimizing inquiry involves searching the parameterized space of experiments to select the experiment that promises, on average, to be maximally informative. In the case where it is important to learn about each of the model parameters, the relevance of an experiment is quantified by Shannon entropy of the distribution of experimental outcomes predicted by a probable set of models. If the set of potential experiments is described by many parameters, we must search this high-dimensional entropy space. Brute force search methods will be slow and computationally expensive. We present an entropy-based search algorithm, called nested entropy sampling, to select the most informative experiment for efficient experimental design. This algorithm is inspired by Skilling's nested sampling algorithm used in inference and borrows the concept of a rising threshold while a set of experiment samples are maintained. We demonstrate that this algorithm not only selects highly relevant experiments, but also is more efficient than brute force search. Such entropic search techniques promise to greatly benefit autonomous experimental design.</p>
                <p><strong>Cost:</strong> 0.007</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2463.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2463.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EBSA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Entropy Based Search Algorithm for Experimental Design</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An experimental-design/search algorithm that selects experiments by maximizing the information entropy of predictive distributions using a sampling-and-acceptance procedure to reduce the number of inquiry computations required to find optimal experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Entropy Based Search Algorithm for Experimental Design</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Entropy Based Search Algorithm (EBSA)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>EBSA generates an initial set of candidate experiments, computes the predictive entropy for each, and iteratively improves candidates via a stochastic explore loop: select the sample with minimum entropy H* (least informative), generate a trial by randomly selecting and varying another sample, compute its entropy H_trial, and accept the trial if H_trial > H*, replacing the least-informative sample. The algorithm seeks to maximize the entropy of predictive distributions (MaxEnt / expected information) as the utility for selecting the optimal next experiment and is implemented as an automated inquiry engine driven only by a model and likelihood (posterior samples input). It reports reduced number of inquiry computations and a 'compression ratio' that the authors claim is independent of landscape complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>General experimental design / autonomous scientific experimentation (demonstrated on synthetic parameterized landscapes and a circle-detection/light-sensor inference task), applicable to robotic/remote scientific instruments and other active experimental platforms.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocates computational/experimental resources by sampling candidate experiments and preferring those that maximize predictive entropy (expected information). Iteratively replaces the least-informative candidates with trials that increase entropy, effectively directing resources (inquiry computations and experiment proposals) toward high-expected-information experiments. The strategy reduces the total number of inquiry computations by focusing search on regions of parameter space predicted to yield the largest increase in predictive uncertainty (entropy) resolution.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Expressed qualitatively as number of 'inquiry computations' (i.e., predictive-evaluation computations per candidate experiment) and via a reported 'compression ratio' (ratio of search cost reduction); no explicit wall-clock time, FLOPs, or monetary costs are given.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Information entropy of the predictive distribution (MaxEnt objective); the paper frames expected information (expected utility) equivalently to expected utility in Bayesian decision theory (references to expected information as expected utility).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>A stochastic sampling-and-acceptance explore loop: start from a random candidate set (exploration), iteratively propose trials by varying parameters of randomly chosen samples, compute predictive entropy, accept trials only if they increase entropy (greedy uphill move on entropy), and adapt step sizes by monitoring acceptance range. This favors exploration of experiments that increase predictive distribution entropy but does not implement an explicit exploitation step that focuses on refinement of a single promising hypothesis; instead it greedily seeks high-entropy experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Implicit: diversity arises from initial random sampling of candidate experiments and continued random selection/variation of trials; there is no explicit diversity-regularization, diversification penalty, or explicit mechanism to ensure coverage across distinct hypothesis modes documented in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Computational budget framed as number of inquiry computations (i.e., cost of evaluating candidate experiments); no explicit monetary or wall-clock budget is specified.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Handled implicitly by reducing the number of inquiry computations through targeted search (compression ratio). The algorithm reduces search cost by focusing evaluation effort on candidates that increase predictive entropy rather than exhaustively evaluating all candidates; no explicit budget-constrained optimization (e.g., knapsack or constrained expected-utility optimization) is described.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Qualitative: reported reduction in the number of inquiry computations and a 'compression ratio' that is stated to be independent of landscape complexity; no numerical performance metrics (e.g., percent reduction, absolute counts, runtime) are provided in the provided text.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Qualitative claim of reduction in number of inquiry computations; 'compression ratio independent of landscape complexity' is reported but without numerical values in the provided text.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>The paper frames expected information as expected utility (Bayesian decision theory) and advocates maximizing predictive entropy (MaxEnt) as the criterion for optimal experiments, but does not present an explicit quantitative tradeoff analysis balancing computational cost versus information gain, breakthrough potential, and hypothesis diversity in the provided text.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Principal conclusion: the optimal experiment is the one that maximizes the information entropy of the predictive distribution (MaxEnt / expected information as expected utility). Practical recommendation: allocate inquiry computations preferentially to candidate experiments that maximize expected information, using iterative sampling and entropy-increasing acceptance to reduce total computational cost.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2463.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2463.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Nested Entropy Sampling</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Nested Entropy Sampling</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sampling-based search procedure that seeks experiments maximizing predictive entropy by maintaining and updating a population of candidate experiments, replacing less informative samples with higher-entropy trials via a nested/iterative acceptance rule.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Entropy Based Search Algorithm for Experimental Design</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Nested Entropy Sampling</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Described as a nested-sampling-like algorithm applied to the space of candidate experiments, where samples are generated, their predictive entropies computed, and lower-entropy samples are iteratively replaced by higher-entropy trials produced by perturbing other samples. The method borrows the concept of nested sampling (sample replacement under a threshold) but the objective and acceptance rule are based on maximizing predictive information (entropy) rather than evidence/likelihood volumes.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Experimental design / active learning across parameterized landscapes; intended for autonomous scientific instruments and inquiry engines.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocates evaluation (inquiry) effort by maintaining a fixed-size sample set and replacing the least-informative sample(s) with trials that increase predictive entropy, thereby concentrating computational evaluations on regions expected to yield higher information.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Number of inquiry computations per iteration/sample (qualitative); nested replacement aims to reduce total evaluations, but no explicit numeric cost units are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Predictive information entropy (MaxEnt); expected information used as the utility function guiding sample replacement.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Exploration via random sampling and stochastic perturbation of samples; exploitation achieved implicitly by accepting only trials that increase entropy (a greedy ascent on entropy), with adaptive step-size control to tune exploration magnitude.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Implicit via population of samples and random trial generation; no explicit diversity-encouraging objective or constraints are described.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Computational/inquiry-evaluation budget (number of samples/evaluations); not formalized as a constrained optimization problem in the text provided.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Handled by maintaining a finite sample set and replacing samples in a way that reduces unnecessary evaluations (claims of compression ratio improvement); no explicit budgeting algorithm described.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Qualitative claim of fewer inquiry computations and compression-ratio benefits; no explicit numerical performance values provided in the excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Qualitative: reported reduction in inquiry computations and compression ratio independent of landscape complexity; numeric magnitude not provided in the provided text.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>No explicit quantitative tradeoff analysis in the provided text; the method implicitly prioritizes information gain while attempting to reduce computational cost but does not quantify tradeoffs with breakthrough potential or hypothesis diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Using a nested-sampling-style replacement strategy to maximize predictive entropy is an effective approach for concentrating computational effort on experiments with high expected information, yielding fewer required inquiry computations according to the authors' qualitative results.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Expected Information as Expected Utility <em>(Rating: 2)</em></li>
                <li>Bayesian adaptive exploration <em>(Rating: 2)</em></li>
                <li>Maximum Entropy Sampling and Optimal Bayesian Experimental Design <em>(Rating: 2)</em></li>
                <li>Designing intelligent instruments <em>(Rating: 2)</em></li>
                <li>Information-Optimal Selective Data Return for Autonomous Rover Traverse Science and Survey <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2463",
    "paper_id": "paper-5680859",
    "extraction_schema_id": "extraction-schema-67",
    "extracted_data": [
        {
            "name_short": "EBSA",
            "name_full": "Entropy Based Search Algorithm for Experimental Design",
            "brief_description": "An experimental-design/search algorithm that selects experiments by maximizing the information entropy of predictive distributions using a sampling-and-acceptance procedure to reduce the number of inquiry computations required to find optimal experiments.",
            "citation_title": "Entropy Based Search Algorithm for Experimental Design",
            "mention_or_use": "use",
            "system_name": "Entropy Based Search Algorithm (EBSA)",
            "system_description": "EBSA generates an initial set of candidate experiments, computes the predictive entropy for each, and iteratively improves candidates via a stochastic explore loop: select the sample with minimum entropy H* (least informative), generate a trial by randomly selecting and varying another sample, compute its entropy H_trial, and accept the trial if H_trial &gt; H*, replacing the least-informative sample. The algorithm seeks to maximize the entropy of predictive distributions (MaxEnt / expected information) as the utility for selecting the optimal next experiment and is implemented as an automated inquiry engine driven only by a model and likelihood (posterior samples input). It reports reduced number of inquiry computations and a 'compression ratio' that the authors claim is independent of landscape complexity.",
            "application_domain": "General experimental design / autonomous scientific experimentation (demonstrated on synthetic parameterized landscapes and a circle-detection/light-sensor inference task), applicable to robotic/remote scientific instruments and other active experimental platforms.",
            "resource_allocation_strategy": "Allocates computational/experimental resources by sampling candidate experiments and preferring those that maximize predictive entropy (expected information). Iteratively replaces the least-informative candidates with trials that increase entropy, effectively directing resources (inquiry computations and experiment proposals) toward high-expected-information experiments. The strategy reduces the total number of inquiry computations by focusing search on regions of parameter space predicted to yield the largest increase in predictive uncertainty (entropy) resolution.",
            "computational_cost_metric": "Expressed qualitatively as number of 'inquiry computations' (i.e., predictive-evaluation computations per candidate experiment) and via a reported 'compression ratio' (ratio of search cost reduction); no explicit wall-clock time, FLOPs, or monetary costs are given.",
            "information_gain_metric": "Information entropy of the predictive distribution (MaxEnt objective); the paper frames expected information (expected utility) equivalently to expected utility in Bayesian decision theory (references to expected information as expected utility).",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "A stochastic sampling-and-acceptance explore loop: start from a random candidate set (exploration), iteratively propose trials by varying parameters of randomly chosen samples, compute predictive entropy, accept trials only if they increase entropy (greedy uphill move on entropy), and adapt step sizes by monitoring acceptance range. This favors exploration of experiments that increase predictive distribution entropy but does not implement an explicit exploitation step that focuses on refinement of a single promising hypothesis; instead it greedily seeks high-entropy experiments.",
            "diversity_mechanism": "Implicit: diversity arises from initial random sampling of candidate experiments and continued random selection/variation of trials; there is no explicit diversity-regularization, diversification penalty, or explicit mechanism to ensure coverage across distinct hypothesis modes documented in the paper.",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Computational budget framed as number of inquiry computations (i.e., cost of evaluating candidate experiments); no explicit monetary or wall-clock budget is specified.",
            "budget_constraint_handling": "Handled implicitly by reducing the number of inquiry computations through targeted search (compression ratio). The algorithm reduces search cost by focusing evaluation effort on candidates that increase predictive entropy rather than exhaustively evaluating all candidates; no explicit budget-constrained optimization (e.g., knapsack or constrained expected-utility optimization) is described.",
            "breakthrough_discovery_metric": null,
            "performance_metrics": "Qualitative: reported reduction in the number of inquiry computations and a 'compression ratio' that is stated to be independent of landscape complexity; no numerical performance metrics (e.g., percent reduction, absolute counts, runtime) are provided in the provided text.",
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "efficiency_gain": "Qualitative claim of reduction in number of inquiry computations; 'compression ratio independent of landscape complexity' is reported but without numerical values in the provided text.",
            "tradeoff_analysis": "The paper frames expected information as expected utility (Bayesian decision theory) and advocates maximizing predictive entropy (MaxEnt) as the criterion for optimal experiments, but does not present an explicit quantitative tradeoff analysis balancing computational cost versus information gain, breakthrough potential, and hypothesis diversity in the provided text.",
            "optimal_allocation_findings": "Principal conclusion: the optimal experiment is the one that maximizes the information entropy of the predictive distribution (MaxEnt / expected information as expected utility). Practical recommendation: allocate inquiry computations preferentially to candidate experiments that maximize expected information, using iterative sampling and entropy-increasing acceptance to reduce total computational cost.",
            "uuid": "e2463.0"
        },
        {
            "name_short": "Nested Entropy Sampling",
            "name_full": "Nested Entropy Sampling",
            "brief_description": "A sampling-based search procedure that seeks experiments maximizing predictive entropy by maintaining and updating a population of candidate experiments, replacing less informative samples with higher-entropy trials via a nested/iterative acceptance rule.",
            "citation_title": "Entropy Based Search Algorithm for Experimental Design",
            "mention_or_use": "use",
            "system_name": "Nested Entropy Sampling",
            "system_description": "Described as a nested-sampling-like algorithm applied to the space of candidate experiments, where samples are generated, their predictive entropies computed, and lower-entropy samples are iteratively replaced by higher-entropy trials produced by perturbing other samples. The method borrows the concept of nested sampling (sample replacement under a threshold) but the objective and acceptance rule are based on maximizing predictive information (entropy) rather than evidence/likelihood volumes.",
            "application_domain": "Experimental design / active learning across parameterized landscapes; intended for autonomous scientific instruments and inquiry engines.",
            "resource_allocation_strategy": "Allocates evaluation (inquiry) effort by maintaining a fixed-size sample set and replacing the least-informative sample(s) with trials that increase predictive entropy, thereby concentrating computational evaluations on regions expected to yield higher information.",
            "computational_cost_metric": "Number of inquiry computations per iteration/sample (qualitative); nested replacement aims to reduce total evaluations, but no explicit numeric cost units are provided.",
            "information_gain_metric": "Predictive information entropy (MaxEnt); expected information used as the utility function guiding sample replacement.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Exploration via random sampling and stochastic perturbation of samples; exploitation achieved implicitly by accepting only trials that increase entropy (a greedy ascent on entropy), with adaptive step-size control to tune exploration magnitude.",
            "diversity_mechanism": "Implicit via population of samples and random trial generation; no explicit diversity-encouraging objective or constraints are described.",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Computational/inquiry-evaluation budget (number of samples/evaluations); not formalized as a constrained optimization problem in the text provided.",
            "budget_constraint_handling": "Handled by maintaining a finite sample set and replacing samples in a way that reduces unnecessary evaluations (claims of compression ratio improvement); no explicit budgeting algorithm described.",
            "breakthrough_discovery_metric": null,
            "performance_metrics": "Qualitative claim of fewer inquiry computations and compression-ratio benefits; no explicit numerical performance values provided in the excerpt.",
            "comparison_baseline": null,
            "performance_vs_baseline": null,
            "efficiency_gain": "Qualitative: reported reduction in inquiry computations and compression ratio independent of landscape complexity; numeric magnitude not provided in the provided text.",
            "tradeoff_analysis": "No explicit quantitative tradeoff analysis in the provided text; the method implicitly prioritizes information gain while attempting to reduce computational cost but does not quantify tradeoffs with breakthrough potential or hypothesis diversity.",
            "optimal_allocation_findings": "Using a nested-sampling-style replacement strategy to maximize predictive entropy is an effective approach for concentrating computational effort on experiments with high expected information, yielding fewer required inquiry computations according to the authors' qualitative results.",
            "uuid": "e2463.1"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Expected Information as Expected Utility",
            "rating": 2,
            "sanitized_title": "expected_information_as_expected_utility"
        },
        {
            "paper_title": "Bayesian adaptive exploration",
            "rating": 2,
            "sanitized_title": "bayesian_adaptive_exploration"
        },
        {
            "paper_title": "Maximum Entropy Sampling and Optimal Bayesian Experimental Design",
            "rating": 2,
            "sanitized_title": "maximum_entropy_sampling_and_optimal_bayesian_experimental_design"
        },
        {
            "paper_title": "Designing intelligent instruments",
            "rating": 2,
            "sanitized_title": "designing_intelligent_instruments"
        },
        {
            "paper_title": "Information-Optimal Selective Data Return for Autonomous Rover Traverse Science and Survey",
            "rating": 1,
            "sanitized_title": "informationoptimal_selective_data_return_for_autonomous_rover_traverse_science_and_survey"
        }
    ],
    "cost": 0.00727125,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Entropy Based Search Algorithm Entropy Based Search Algorithm for Experimental Design for Experimental Design</p>
<p>Nabin K Malakar 
University at Albany
SUNY</p>
<p>Kevin H Knuth 
University at Albany
SUNY</p>
<p>Entropy Based Search Algorithm Entropy Based Search Algorithm for Experimental Design for Experimental Design</p>
<p>Simulations with Parameterized Landscape
( ) ( ) ( ) ( ) ( ) ( ) ⎥ ⎦ ⎤ ⎢ ⎣ ⎡ ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ ⎟ ⎟ ⎠ ⎞ ⎜ ⎜ ⎝ ⎛ ′ − ′ ′ − ′ ⎟ ⎟ ⎠ ⎞ ⎜ ⎜ ⎝ ⎛ ′ − ′ ′ − ′ − = ′ ′ ∑ k k k k k k k k k k v y u x B C C A v y u x Exp a y , x H 2 1
We Investigated landscapes comprised of 1-100 Gaussians </p>
<p>•-
Infer the position (x, y) and radius of a circle • Using light sensor measure intensity at a given position • Automated inference and inquiry engines • Programmed only with model and likelihood---no search strategyReduces the number of inquiry computations -Compression Ratio independent of landscape complexity • Can be adapted for other Utility Functions</p>
<p>SET UP: Generate a set of s sample experiments randomly and compute the entropy H for each. WHILE samples have different entropy values Select the sample s<em> from the set that has the least entropy, denoted H</em>. Generate a trial experiment s trial by selecting another sample at random from . the set EXPLORE LOOP Explore by varying the parameters of s trial and computing the new value of H trial. Accept the trial if H &gt; H<em> otherwise reject it. Monitor acceptance range and change exploration step size Replace s</em> with s trial END WHILE INPUT: posterior samples from the inference phase( ) 
( 
) 
( 
) </p>
<p>e 
, 
D 
, 
d 
| 
p 
log 
e 
, 
D 
, 
d 
| 
p 
e 
, 
d 
U </p>
<p>e 
e 
e </p>
<p>θ 
θ </p>
<p>∑ </p>
<p>= </p>
<p>Bayesian Decision Theory </p>
<p>• Expected Utility: </p>
<p>• The best action maximizes the expected utility </p>
<p>( ) 
( ) ( ), </p>
<p>d 
p 
e 
, 
d 
U 
e 
EU </p>
<p>e 
d </p>
<p>e </p>
<p>e </p>
<p>∑ </p>
<p>= </p>
<p>( ) </p>
<p>e 
EU 
max 
arg 
ê = </p>
<p>Experiment </p>
<p>Predicted outcome </p>
<p>MaxEnt 2010 </p>
<p>Maximization of Entropy… </p>
<p>( 
) 
( 
) . </p>
<p>M 
e 
, 
| 
d 
p 
log 
M 
, 
e 
, 
| 
d 
p 
C 
max 
arg 
ê </p>
<p>e 
d </p>
<p>e </p>
<p>e </p>
<p>⎟ 
⎟ 
⎠ </p>
<p>⎞ 
⎜ 
⎜ 
⎝ 
⎛ − 
= </p>
<p>∑ </p>
<p>D 
D </p>
<p>• Optimal Experiment: 
maximizes the Information Entropy of the 
predictive distributions </p>
<p>• This can also be shown using the 
Inquiry Calculus (Knuth) 
Nested Entropy Sampling </p>
<p>OUTPUT: prescription of the optimal experiment. </p>
<p>MaxEnt 2010 </p>
<p>MaxEnt 2010 </p>
<p>Expected Information as Expected Utility. J M Bernardo, The Annals of Statistics. 7• Bernardo, J. M. Expected Information as Expected Utility, The Annals of Statistics, 7, 3, 686-690, 1979.</p>
<p>Information Theory and Spectral Information. K A Earle, K H Knuth, D J Schneider, D E Budil, Advanced Electron Spin Resonance Technology Research Center Workshop. Ithaca NYEarle, K.A., Knuth, K.H., Schneider, D.J, Budil, D.E. 2008. Information Theory and Spectral Information, Advanced Electron Spin Resonance Technology Research Center Workshop 2008 (ACERT 2008), Ithaca NY, May 2008.</p>
<p>Designing intelligent instruments. K H Knuth, P M Erner, S Frasso, Bayesian Inference and Maximum Entropy Methods in Science and Engineering. Saratoga Springs, NY, USA• Knuth, K. H., Erner P. M., Frasso S. 2007, Designing intelligent instruments. In Bayesian Inference and Maximum Entropy Methods in Science and Engineering, Saratoga Springs, NY, USA, 2007, edited by K.</p>
<p>. H Knuth, A Caticha, J L Center, A Gi_N, C C Rodriguez, AIP Conference Proceedings. 954American Institute of PhysicsH. Knuth, A. Caticha, J. L. Center, A. Gi_n, C. C. Rodriguez, AIP Conference Proceedings 954, American Institute of Physics, Melville NY, pp. 203-211.</p>
<p>Autonomous Science Platforms and Question-Asking Machines. K H Knuth, J L Center, Cognitive Information Processing. Italy• Knuth, K. H. and Center, J. L., Autonomous Science Platforms and Question-Asking Machines, Cognitive Information Processing 2010 (CIP 2010), Italy, 2010.</p>
<p>Making Decisions Wiley-interscience. D V Lindley, • Lindley, D. V., Making Decisions Wiley-interscience, 1971.</p>
<p>On a Measure of the Information Provided by an Experiment. D V Lindley, Ann. Math. Stat. 27• Lindley, D. V., On a Measure of the Information Provided by an Experiment, Ann. Math. Stat., 27, 4, 986- 1005, 1956.</p>
<p>Bayesian adaptive exploration. T J Loredo, Bayesian Inference and Maximum Entropy Methods in Science and Engineering. G. J. Erickson &amp; Y. ZhaiJackson Hole WY, USA; New YorkAmerican Institute of Physics707330• Loredo, T. J. 2004, Bayesian adaptive exploration. In Bayesian Inference and Maximum Entropy Methods in Science and Engineering, Jackson Hole WY, USA, edited by G. J. Erickson &amp; Y. Zhai, AIP Conference Proceedings 707, American Institute of Physics, New York, 2004, 330.</p>
<p>Maximum Entropy Sampling and Optimal Bayesian Experimental Design. P Sebastiani, H P Wynn, J. Roy. Stat. Soc. B. 62• Sebastiani, P. and Wynn, H. P., Maximum Entropy Sampling and Optimal Bayesian Experimental Design, J. Roy. Stat. Soc. B, 62, pp. 145-157, (2000).</p>
<p>A Mathematical Theory of Communication. C E Shannon, Bell System Technical Journal. 27623656• Shannon, C. E., A Mathematical Theory of Communication, Bell System Technical Journal, 27, 379423, 623656, 1948.</p>
<p>Information-Optimal Selective Data Return for Autonomous Rover Traverse Science and Survey. D S Sivia, J Skilling, J Skilling, S ; • Thrun, W Burgard, D Fox, D R ; • Thompson, T Smith, D Wettergreen, Data Analysis: A Bayesian Tutorial. CambridgeMIT Press4Nested Sampling for General Bayesian Computation. ICRA• Sivia, D. S., Skilling, J., Data Analysis: A Bayesian Tutorial, 2nd Edition, Oxford University Press, 2006. • Skilling, J., Nested Sampling for General Bayesian Computation, Bayesian Analysis 4, pp. 833-860, 2006. • Thrun S., Burgard W., Fox D.,. Probabilistic Robotics, MIT Press:Cambridge, 2005. • Thompson, D. R., Smith, T. and Wettergreen, D., Information-Optimal Selective Data Return for Autonomous Rover Traverse Science and Survey. ICRA 2008.</p>
<p>Bayesian Experimental Design-Studies for Fusion Diagnostics. R Fischer, Bayesian Inference and Maximum Entropy Methods in Science and Engineering. R. Fischer, R. Preuss and U. von Toussaint735• Fischer, R., 2004. Bayesian Experimental Design-Studies for Fusion Diagnostics. In Bayesian Inference and Maximum Entropy Methods in Science and Engineering, edited by R. Fischer, R. Preuss and U. von Toussaint, AIP Conference Proceedings, 735, pp. 76-83.</p>            </div>
        </div>

    </div>
</body>
</html>
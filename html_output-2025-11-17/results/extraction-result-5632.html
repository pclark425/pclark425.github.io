<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5632 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5632</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5632</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-117.html">extraction-schema-117</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to estimate or predict the probability or likelihood of specific future real-world scientific discoveries, including details on methods, evaluation, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-269302484</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2404.14419v2.pdf" target="_blank">Evaluation and Improvement of Fault Detection for Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have recently achieved significant success across various application domains, garnering substantial attention from different communities. Unfortunately, even for the best LLM, many \textit{faults} still exist that LLM cannot properly predict. Such faults will harm the usability of LLMs in general and could introduce safety issues in reliability-critical systems such as autonomous driving systems. How to quickly reveal these faults in real-world datasets that LLM could face is important, but challenging. The major reason is that the ground truth is necessary but the data labeling process is heavy considering the time and human effort. To handle this problem, in the conventional deep learning testing field, test selection methods have been proposed for efficiently evaluating deep learning models by prioritizing faults. However, despite their importance, the usefulness of these methods on LLMs is unclear, and lack of exploration. In this paper, we conduct the first empirical study to investigate the effectiveness of existing fault detection methods for LLMs. Experimental results on four different tasks~(including both code tasks and natural language processing tasks) and four LLMs~(e.g., LLaMA3 and GPT4) demonstrated that simple methods such as Margin perform well on LLMs but there is still a big room for improvement. Based on the study, we further propose \textbf{MuCS}, a prompt \textbf{Mu}tation-based prediction \textbf{C}onfidence \textbf{S}moothing framework to boost the fault detection capability of existing methods. Concretely, multiple prompt mutation techniques have been proposed to help collect more diverse outputs for confidence smoothing. The results show that our proposed framework significantly enhances existing methods with the improvement of test relative coverage by up to 70.53\%.</p>
                <p><strong>Cost:</strong> 0.005</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5632",
    "paper_id": "paper-269302484",
    "extraction_schema_id": "extraction-schema-117",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.0049415,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Evaluation and Improvement of Fault Detection for Large Language Models
5 Nov 2024</p>
<p>Qiang Hu 
Equal Contribution</p>
<p>Equal Contribution</p>
<p>Equal Contribution</p>
<p>Jin Wen 
Equal Contribution</p>
<p>Equal Contribution</p>
<p>Equal Contribution</p>
<p>Yuheng Huang 
Wei Ma 
Lei 2024 Ma 
Maxime Cordy 
Xiaofei Xie 
Evaluation </p>
<p>The University of Tokyo
Japan</p>
<p>University of Luxembourg
Luxembourg</p>
<p>MAXIME CORDY
University of Luxembourg
Luxembourg</p>
<p>The University of Tokyo
Japan</p>
<p>Nanyang Technological University
Singapore</p>
<p>XIAOFEI XIE
Singapore</p>
<p>Management University
Singapore</p>
<p>The University of Tokyo &amp; University of Alberta
Japan, Canada</p>
<p>Improvement of Fault Detection for Large Language Models. In Proceedings of (Conference). ACM
New YorkNYUSA</p>
<p>Evaluation and Improvement of Fault Detection for Large Language Models
5 Nov 2024F9E2F9A6C5C56D25F1D99AD464D0472610.1145/nnnnnnn.nnnnnnnarXiv:2404.14419v2[cs.SE]
Large language models (LLMs) have recently achieved significant success across various application domains, garnering substantial attention from different communities.Unfortunately, even for the best LLM, many faults still exist that LLM cannot properly predict.Such faults will harm the usability of LLMs in general and could introduce safety issues in reliability-critical systems such as autonomous driving systems.How to quickly reveal these faults in real-world datasets that LLM could face is important, but challenging.The major reason is that the ground truth is necessary but the data labeling process is heavy considering the time and human effort.To handle this problem, in the conventional deep learning testing field, test selection methods have been proposed for efficiently evaluating deep learning models by prioritizing faults.However, despite their importance, the usefulness of these methods on LLMs is unclear, and lack of exploration.In this paper, we conduct the first empirical study to investigate the effectiveness of existing fault detection methods for LLMs.Experimental results on four different tasks (including both code tasks and natural language processing tasks) and four LLMs (e.g., LLaMA3 and GPT4) demonstrated that simple methods such as Margin perform well on LLMs but there is still a big room for improvement.Based on the study, we further propose MuCS, a prompt Mutation-based prediction Confidence Smoothing framework to boost the fault detection capability of existing methods.Concretely, multiple prompt mutation techniques have been proposed to help collect more diverse outputs for confidence smoothing.The results show that our proposed framework significantly enhances existing methods with the improvement of test relative coverage by up to 70.53%.</p>
<p>INTRODUCTION</p>
<p>In recent years, the potential of large language models (LLMs) has brought hope to general artificial intelligence (AGI).LLMs achieved promising and even the best results compared to other conventional methods in many areas, such as question answering [44], sentiment analysis [41], and code understanding [25] that make LLMs become the first choice of deep learning models in such tasks.Besides the most famous application ChatGPT1 , multiple LLMs have been proposed, e.g., LLaMA [34], Alpaca [33], and Cerebras-GPT [6].More interestingly, focusing on software engineering (SE) tasks, a series of code-related LLMs with good programming ability have been released to help the software development, such as StarCoder [21] and CodeLLaMA [31].Learning to use LLMs is already a basic skill for us in daily life.</p>
<p>Despite the success, the same as other types of deep learning models, LLMs also suffer from different problems that limit their practical usage.For example, LLMs sometimes generate unrelated outputs to the inputs or incorrect results misaligned with established world knowledge, which is known as the hallucination problem [42].Besides, recent work [7] showed that LLMs can be easily fooled by adversarial attacks and are not robust.Furthermore, considering SE tasks, existing code-related LLMs cannot capture equivalent semantics when the given natural language prompt expresses the same meaning in different languages [29].Those limitations remind us that LLMs are not always trustworthy, and it is necessary to carefully evaluate LLMs and quickly reveal the unreliable outputs of LLMs in practical usage.</p>
<p>The basic way to evaluate LLMs and reveal their weakness is to prepare test data as diverse as possible to assess the performance of LLMs accordingly.However, preparing such test data is heavy work due to the complicated process of data collection, data cleaning, and data labeling.Especially, data labeling requires human effort with domain knowledge which is the most challenging part.Moreover, some LLMs are closed-source which requires funding expenses, e.g., it tasks 0.03$ per 1,000 tokens of inputs for GPT4 using OpenAI API.Therefore, it is almost impossible to thoroughly evaluate LLMs on the target downstream task for people who have limited budgets.As a result, how to efficiently test LLMs and filter the mispredicted data (unreliable outputs) of LLMs with less effort becomes an urgent problem.</p>
<p>To tackle the data labeling issue mentioned above, in the field of deep learning testing, multiple fault detection methods that quickly identify fault data (data the model has incorrect prediction) in the test set without using the labeling information [17] have been proposed.Based on the information required, existing methods can be divided into learning-based methods and outputbased methods.Those methods have been widely studied in the classical deep learning models and have proven to be effective in saving the labeling budget during testing.However, such studied classical deep learning models are constructively different from LLMs, e.g., LLMs are pretrained using a large amount of pre-training data with diverse tasks which makes their prediction confidence (most fault detection methods rely on) unclear given a specific downstream task.Therefore, as no studies have explored the effectiveness of fault detection methods for LLMs, it is unknown whether we can directly employ existing methods to evaluate LLMs and save our effort or not.</p>
<p>To bridge this gap, in this paper, we conduct the first study to investigate the effectiveness of existing fault detection methods for LLMs.The major challenge in the study is that most fault detection methods are proposed for classification tasks and use the output probabilities for data prioritization, but LLMs normally output a sequence of tokens without such a clear output probability for the given task.To solve this, we design the prompt to guide LLMs to produce the output with their confidence.For datasets that have more than two classes, we add examples (fewshot) in each class in the prompt as guidance to ask LLMs to predict new data.In total, our study covers nine fault detection methods including both learning-based methods, e.g., TestRank [22], and output-based methods, e.g., ATS [12].Based on the experiment results collected from four datasets (e.g., sentiment analysis and code clone detection) and four LLMs (e.g., LLaMA3 and GPT4).We found that 1) LLMs are not well-calibrated and overconfident in clone detection, problem classification, and news classification tasks, 2) simple methods such as Margin perform better compared to others, and 3) however, existing fault detection methods perform relatively pool on LLMs compared their performance on classical deep learning models.There is a need to propose methods to further enhance existing fault detection methods.</p>
<p>To do so, inspired by [23,36] which utilize mutation testing techniques to help test deep learning models, we introduce MuCS, a prompt Mutation-based prediction Confidence Smoothing framework to enhance existing fault detection methods.Specifically, we mutate the prompt by transformation methods to generate a set of prompt mutants first.Here, both text augmentation methods and code refactoring methods have been considered in MuCS.Then, we collect the output probabilities of all mutants and compute the average prediction confidence.Finally, we use the averaged confidence to perform fault detection using current fault detection methods.We compare the effectiveness of fault detection methods with and without our confidence smoothing method and found that using MuCS significantly enhances the performance of existing methods by up to 70.53%.</p>
<p>To summarize, the main contributions of this paper are:</p>
<p>‚Ä¢ This is the first study that explores the effectiveness of fault detection methods on LLMs.We found that 1) LLMs are overconfident in clone detection, problem classification, and news classification tasks; 2) the prediction confidence of LLMs is concentrated in a few intervals; and 3) simple methods perform relatively better than others.‚Ä¢ We propose MuCS, a prompt mutation-based method to smooth the confidence of LLMs and enhance the effectiveness of existing fault detection methods.</p>
<p>The rest of this paper is organized as follows.Section 2 reviews the related works.Section 3 presents the design of our empirical study and Section 4 summarizes the empirical results.Section 5 introduces our proposed mutation-based confidence smoothing solution and Section 6 presents its related evaluation.Section 7 discusses the limitation of this work and Section 8 concludes this work.</p>
<p>RELATED WORK</p>
<p>We review the related works from the perspectives of fault detection for deep neural networks and large language model testing.</p>
<p>Fault Detection for Deep Neural Networks</p>
<p>To save the labeling budget, multiple test selection methods [3,39,43,45] have been proposed.A recent survey [17] reviewed test selection methods and divided them into fault detection methods, sampling-based model retraining methods, model selection methods, and performance estimation methods.We focus on the fault detection methods.</p>
<p>The early method DeepGini [10] has been proposed by Feng et al. to reveal inputs that are more likely been mispredicted by the model.DeepGini [10] defined a new Gini score to measure the uncertainty of deep learning models on the inputs.Later on, inspired by the mutation testing in the conventional SE field, Wang et al. [36] proposed to mutate both input samples and deep learning models and identify faults by computing the killing score.Their evaluation on more than 20 tasks demonstrated that mutation testing is useful for helping find bugs in deep learning models.Furthermore, Gao et al. [12] proposed an adaptive test selection (ATS) method.ATS not only considers detecting faults but also tries to find diverse faults (diverse here means faults are from different categories) in deep learning models.More recently, Bao et al. [4] first empirically proved that simple methods (i.e., output probability-based methods) perform well on fault detection.Then, they proposed a new method to compute the uncertainty scores by averaging the basic score of the input and scores from the neighbors of this input.Besides, Ma et al. [26] conducted a comprehensive empirical study to explore the potential of test selection methods and active learning methods on fault detection.They found that MaxP is the best in terms of the correlation between the MaxP score and the misclassification.Besides the SE community, researchers from the ML community also contributed to this field.Dan et al. [15] built the first benchmark for detecting misclassified data and found that just using the confidence score is already a strong baseline for this task.Li et al. [22] proposed TestRank which uses graph neural networks to learn the difference between benign inputs and faults for fault detection.</p>
<p>Different from these works, our work is the first to explore the effectiveness of fault detection methods for large language models.We found that existing methods cannot detect faults in LLMs well and proposed the use of prompt mutation to further enhance these methods.</p>
<p>Large Language Model Testing</p>
<p>Large language models have been the most used deep learning models for most of the tasks.Comprehensively testing or evaluating the performance of LLMs has become one of the most important directions in almost all communities.</p>
<p>Chang et al. surveyed works that focus on evaluating LLMs [5].They classified the evaluation objectives into seven categories, natural language processing, robustness/ethics/biases/trustworthiness, social science, natural science &amp; engineering, medical applications, agent applications, and other applications.Evaluating LLMs for code which we are more interested in lies in the category of natural science &amp; engineering.Xu et al. [38] conducted a comprehensive study to evaluate four series of LLMs on the code generation task using the HumanEval benchmark.They found that the performance of LLMs is affected by the parameter size of models and the training time.Liu et al. [24] propose a new code synthesis evaluation framework EvalPlus to measure the correctness of code generated by LLMs.Based on this framework, they found that the existing benchmark HumanEval is not rigorous enough.The performance (passk) of LLMs drops a lot when using the new benchmark.Besides, Ma et al. [25] evaluated the capability of LLMs to understand code syntax and semantics.The experimental results show that LLMs can understand the syntax structure of code, and perform code static analysis, but cannot approximate code dynamic behavior of code.</p>
<p>More recently, Yuan et al. [40] compared LLMs with conventional code models that are fine-tuned for specific code tasks.They found that in the zero-shot setting, instruction-tuned LLMs have competitive performance to fine-tuned code models.In the one-shot setting, the guidance by oneshot example sometimes harms the performance of LLMs.Du et al. [9] argued that existing works that evaluate the effectiveness of LLMs for code mainly focus on simple code tasks (function-level code synthesis) and built the first class-level code generation benchmark for the LLM evaluation.Based on their benchmark, they found that existing LLMs have significantly lower performance on class-level code generation tasks compared to method-level code generation tasks.There is still a big room to improve the ability of LLMs for code generation.</p>
<p>Existing LLM testing works mainly focus on constructing new and challenging datasets (by manual data collection and adversarial data generation, etc.) to validate the capability of LLMs, which can be seen as test augmentation techniques.In contrast, our work targets test selection from the perspective of software testing for LLM testing.</p>
<p>EMPIRICAL STUDY</p>
<p>Our study focuses on the problem of fault detection of LLMs.We consider both learning-based fault detection methods and output-probability-based methods in the literature.Based on the previous definition [17], given a LLM , an unlabeled test set   , and a labeling budget , fault detection is to select a subset  of   such that   = arg min {  ‚äÜ  ‚àß|  |= }  (,   ,   ) where   are the labels corresponding to   , and  (‚Ä¢) is the performance measurement function.We follow the guidance of previous works [17,18,26] to select nine fault detection methods in our study.We exclude neuron coverage-based methods and surprise adequacy methods in our study since the parameter size of LLMs is too big (&gt;7B) and the coverage and adequacy score are difficult to compute.</p>
<p>Fault Detection Methods</p>
<p>In total, we collect nine fault detection methods.Let X be a set of test inputs and Y be the corresponding label set,  ‚àà X and  ‚àà Y denote a given test sample and its true label, respectively.Let    () be the likelihood of  belonging to class   ‚àà Y produced by the model M.</p>
<p>‚Ä¢ Random Selection is the basic selection criteria that test the model using randomly selected a subset of test data.‚Ä¢ Max Probability (MaxP) prioritizes data based on the maximum output probability, expressed as Max   |  ‚àà Y .Data samples with lower MaxP scores are more likely to be faults.
‚Ä¢ DeepGini defines a Gini score calculated by ùê∫ùëñùëõùëñ (ùë•) = 1 ‚àí ùë¶ ùëñ ‚àà Y ùëù ùë¶ ùëñ 2 (ùë•)
to measure the uncertainty of data.The data with higher Gini scores are treated as faults.</p>
<p>‚Ä¢ Entropy selects data with the minimum Shannon entropy calculated using output probabilities.</p>
<p>‚Ä¢ Margin measures the uncertainty of data based on the difference between the top-1 and top-2 output probabilities.A smaller difference indicates that the model is more difficult to distinguish the data between the two classes and the data should be considered faults.‚Ä¢ Multiple-Boundary Clustering and Prioritization (MCP) contains two steps.First, it divides the data space into different areas based on the decision boundary.Here, the decision boundary is approximated by the top-2 prediction probabilities.For example, give an input  and its top-2 output probabilities are   1 and   3 .Then this input is near the decision boundary (1, 3) and on the side of 1. MCP then selects data from different data spaces based on the score
ùëù ùë¶ ùëì
  , where   and   are the top-1 and top-2 probabilities.</p>
<p>‚Ä¢ Nearest Neighbor Smoothing (NNS) first finds the neighbors and then smooths the output probabilities of each input using the outputs of neighbors.Finally, uncertainty-based fault detection methods such as DeepGini are employed to select the faults.In NNS, neighbors are searched by computing the distance between each extracted representation of the input.NNS requires the intermediate outputs of inputs to conduct data selection and, thus, cannot be used for closed-source LLMs such as GPT3.5.‚Ä¢ Adaptive Test Selection (ATS) first projects the top-3 maximum output probabilities to the space plane.After that, it computes the coverage of each input on the plane.Then, the coverage score is used to identify if the input is a fault or not based on its difference from the coverage of the whole test set.Note that ATS can only be used for classification tasks that have greater than three categories.‚Ä¢ TestRank initially extracts two features from the input data: 1) the output from the logits layer as intrinsic attributes, and 2) the graph information, which includes the cosine distance to other data points and the label of the data.Subsequently, a graph neural network (GNN) model is employed to assimilate the graph information and forecast the contextual attributes of the data.Finally, the contextual attributes and intrinsic attributes are amalgamated and inputted into a binary classification model to acquire proficiency in identifying failures.Comparable to the NNS method, TestRank also necessitates intermediate outputs for data selection and is not applicable to closed-source LLMs.Except for the random selection and TestRank, all the other methods are purely based on the output probability.For the TestRank, the output probability is also a part of the information used for the detection.Thus, the prediction confidence of LLMs is important for fault detection.</p>
<p>Datasets and Models</p>
<p>We consider four datasets including both natural language classification tasks and programming language classification tasks, and four types of LLMs covering both open-sourced models and closed-source models.Due to the high cost of assessing close-source LLMs, we prepare relatively more test data for open-source LLMs than close-source LLMs.</p>
<p>Clone Detection (CD): Utilizing the BigCloneBench [32] dataset, we randomly select 1000 and 200 items to investigate the code clone detection ability of open-source LLMs and closed-source LLMs, respectively.Models are evaluated on their ability to predict the presence of code clones (a probability score from 0 to 1, where 0 indicates no clone and 1 indicates a clone) between code snippet pairs, a critical task for enhancing software maintenance and development practices.Note that, some fault detection methods that require more than two classes such as ATS cannot apply to clone detection tasks.</p>
<p>Problem Classification (PC): Based on the Java250 [30] dataset from the CodeNet Project, problem classification focuses on the classification of programming challenges.1000 samples and 200 samples across 5 types of problems are randomly chosen for evaluating open-source and closed-source LLMs.This task measures the model's proficiency in categorizing coding problems using a singular example for each category, requiring output as a probability distribution across classes that sum to 1.We employ the one-shot setting (one-shot examples are randomly selected from the original dataset) for this task due to the poor performance of zero-shot evaluation (nearly random guess).</p>
<p>Sentiment Analysis (Sentiment): For this task, 1000 samples and 150 samples are extracted from the Sentiment Analysis of IMDB Movie Reviews dataset 2 to evaluate open-source and closedsource LLMs, respectively.The challenge for models is to predict the sentiment of movie reviews, negative, neutral, and positive, ranging from 0 to 1, evaluating the capacity of models for natural language understanding and emotional tone assessment.</p>
<p>TagMyNews (TMN): Using the TagMyNews Dataset, we randomly picked 1400 and 200 articles with seven categories, which are sourced from RSS feeds of popular newspapers [35] for opensource and closed-source LLMs, respectively.This task aims to assess the efficacy of the model in accurately categorizing news articles based on their textual content.Due to the same reason as the problem classification task, we employ the one-shot setting for this task.</p>
<p>LLaMA3 and DeepSeekCoder.LLaMA3 [34] is known for its auto-regressive architecture aimed at chat applications.The parameters of the series of LLaMA3 models range from 8 billion to 70 billion and are learned from publicly available online data amounting to 2 trillion tokens.Besides, LLaMA3 models have been fine-tuned with a combination of supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF), focusing on aligning the models to human preferences for helpfulness and safety.DeepSeeker-Coder consists of a series of code language models, which achieve competitive performance on different code-related tasks [14].DeepSeek-Coder not only beats most open-sourced LLMs but also surpasses existing advancing close-source LLMs like Codex and GPT-3.5.We employ the instruct version (meta-llama/Meta-Llama-3-8B-Instruct and deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct) available on the public Hugging Face platform for our experiments.GPT3.5 and GPT4 [1].GPT3.5 is a fined-tuned version of GPT3 developed in January 2022.It is built on Transformer architecture while emphasizing decoder-only mechanisms.GPT4, which is known as the state-of-the-art LLM, has a substantial leap in capability and complexity compared to GPT-3.5.Both GPT3.5 and GPT4 are closed-source LLMs, we employ gpt-3.5-turbo-0125and gpt-4-0125preview models in our experiments through API access, specifically within text chat application contexts.</p>
<p>Table 1 presents the details of our used datasets and models.We found that in some cases (marked by red), the performance of LLMs is too bad (close to random guess) and too good (with perfect accuracy).For these cases, we only evaluate their prediction confidence (RQ1) and do not conduct fault detection (RQ2) on them.</p>
<p>Prompt Design</p>
<p>The quality of input prompts highly affects the output of LLMs.To obtain the output probabilities of LLMs given the input, we add guidance in the prompt to ask LLMs to produce their prediction confidence of each category.Figure 1 presents the template example of the code clone detection task.For multiple class classification tasks such as problem classification, we use one-shot examples to lead LLMs to produce more accurate prediction confidence.</p>
<p>Research Question</p>
<p>Our study aims to explore the effectiveness of existing fault detection methods on LLMs.Concretely, we answer the following two research questions:</p>
<p>‚Ä¢ RQ1: How confident are LLMs in their prediction?Most (seven out of nine) of the fault detection methods are purely output uncertainty-based.Therefore, checking the prediction confidence (an important indicator to measure the output uncertainty) produced by LLMs before running fault detection methods is necessary.</p>
<p>‚Ä¢ RQ2: How effective are fault detection methods for LLMs?In this research question, we check if the existing methods demonstrated to be useful in classical deep learning models can perform well on LLMs.</p>
<p>Evaluation Metrics.</p>
<p>In RQ1, we follow the previous work [13] to evaluate the confidence of LLMs in their prediction using metrics Prediction Confidence (Confidence) and Expected Calibration Error (ECE).Roughly speaking, Confidence is computed by probabilities associated with the predicted label.ECE first splits the range of the prediction score into  equally-spaced intervals (denoted as   ,  ‚àà {1, . . .,  }) and then computes the weighted average of the bins' accuracy/confidence.A lower ECE score indicates that the model is better calibrated, i.e., the prediction probabilities can better represent the true correctness of models.</p>
<p>Definition 1 (Prediction Confidence (Confidence)).
ùê∂ùëúùëõùëì ùëñùëëùëíùëõùëêùëí (ùë•) = ùëöùëéùë• ({ùëù ùë¶ ùëñ (ùë•)|0 &lt; ùëñ ‚â§ ùëÅ })
where  is the number of classes.</p>
<p>Definition 2 (Expected Calibration Error (ECE)).Given a test set  , let   ‚äÜ  be the inputs whose prediction confidence falls into the  ‚Ñé interval   = ( ‚àí1  ,   ],  (  ) be the accuracy of the inputs   , and
ùê¥ùë£ùëî_ùê∂ùëúùëõùëì (ùêµ ùëö ) = 1 |ùêµ ùëö |
 ‚àà    () be the average confidence within   , then the expected calibration error on  is defined as:
ùê∏ùê∂ùê∏ (ùëã ) = ùëÄ ‚àëÔ∏Å ùëö=1 |ùêµ ùëö | |ùëã | |ùê¥ùëêùëê (ùêµ ùëö ) ‚àí ùê¥ùë£ùëî_ùê∂ùëúùëõùëì (ùêµ ùëö )|
In RQ2, we follow the previous work [22] and evaluate the effectiveness of fault detection methods using the metric of Test Relative Coverage (TRC) which is calculated by the percentage of the number of faults divided by the labeling budget or the total number of faults whichever is minimal.</p>
<p>Definition 3 (Test Relative Coverage (TRC)).Given a test set  and a labeling budget , the test relative coverage on  is defined as:
ùëá ùëÖùê∂ (ùëã ) = | ùëã ùëì ùëéùë¢ùëôùë°ùë† | ùëÄùëñùëõ ùëèùë¢ùëëùëîùëíùë°, |ùëã ùëì ùëéùë¢ùëôùë°ùë† |
where    represents the faults within the selected test set   and    represents the faults in the whole test set  .A higher TRC value indicates the better performance of the fault detection method.</p>
<p>Configurations</p>
<p>Configuration of fault detection methods.NNS and TestRank need to compute the distance between data samples and require the representation of the samples.In this work, we directly use the input embeddings extracted from LLMs as the representation.Besides, TestRank is a learningbased method where training data is required for fault detection.For the clone detection, problem classification, and TagMyNews tasks, we randomly select 200 data from the original datasets as the training data.For the sentiment analysis dataset, as the original dataset only contains 200 data samples and 150 of them are used as test data, we collect the remaining 50 samples as the training data.For the data labeling process, we set the labeling budgets from 10% to 90% with an interval of 10%.</p>
<p>Configuration of LLMs.In our experiments, we set top_p=0.95 for open-sourced LLaMA3 and DeepSeek-Coder, and top_p=1 for closure GPT3.5 and GPT4 which are the default settings used by LLMs.top_p controls the accumulation of token probabilities to select the next word for all LLMs.We set the window size of the token context as max_length= 10k for LLaMA3 and DeepSeek-Coder to match our test data's token length.The window sizes of GPT3.5 and GPT4 are set as 16385 and 128000 respectively, using the default settings.</p>
<p>Implementation and Environment</p>
<p>For the fault detection methods, we reuse the official implementations provided by the original papers and modify them to fit our tasks.For the LLaMA and CodeLLaMA models, we use the resources provided by Hugging Face3 in this work.For GPT3.5 and GPT4 models, we use the OpenAI's API to access the models and collect the results.In total, it costs 267 US dollars to run GPT-related experiments.We conduct all LLaMA-related experiments on a high-performance computer cluster.Each cluster node runs a 2.6 GHz Intel Xeon Gold 6132 CPU with an NVIDIA Tesla V100 16G SXM2 GPU.</p>
<p>EMPIRICAL RESULTS</p>
<p>RQ1: Prediction Confidence</p>
<p>First, as most fault detection methods rely on prediction confidence, we check whether LLMs can confidently predict the data in our studied tasks.Figure 2 depicts the distribution (number of data in each confidence interval) of the prediction confidence produced by LLMs, where we set the number of intervals  as 30.The first conclusion is that GPT4 has the most confidence in its predictions compared to other LLMs.On average, the confidence of DeepSeekCoder, LLaMA3, GPT3.5, and GPT4 on these four datasets are 81.72%,59.15%, 77.01%, and 87.27%, respectively.Comparing the accuracy of LLMs and their average confidence (which is a notion of miscalibration [13]), we found that our LLMs are not well-calibrated to these four tasks.For example, for DeepSeekCoder on the clone detection task, the accuracy and average confidence are 69.4% and 94.61%, respectively.This means DeepSeekCoder is overconfident with its prediction even though it has poor performance.Overall, except for TagMyNews, LLMs are overconfident in the other three tasks with 16.78% higher confidence than the true accuracy.Additionally, the results show that prediction confidence is concentrated in certain ranges for many cases.For example, for the clone detection -GPT3.5, confidence only falls into four ranges.This means the model has similar confidence to many different inputs, in turn, the output-based fault detection methods (e.g., MaxP) could produce similar uncertainty scores to these inputs.</p>
<p>Interestingly, we found that GPT4 has perfect accuracy (100%) on the problem classification task with high average confidence (0.9827 on average) and a low ECE score of 0.0143.We conjecture there is a data leakage problem of our test data, i.e., our used problem classification dataset is a part of the training data of GPT4.Investigating the exact reason behind this could be future work to better understand what has been learned by the pre-trained LLMs.As there are no faults in GPT4 for the problem classification task, we exclude this model from the following fault detection study.</p>
<p>Answer to RQ1: LLMs are not well-calibrated and overconfident in clone detection, problem classification, and news classification tasks.Besides, prediction confidence is concentrated in a few intervals, indicating LLMs have similar confidence to most of the inputs.</p>
<p>RQ2: Effectiveness of Fault Detection</p>
<p>In this RQ, we explore the performance of existing fault detection methods for LLMs.As mentioned in Section 3.2, we exclude models that have perfect performance (100% accuracy) on the studied tasks.Figure 3 depicts the TRC values achieved by fault detection methods in each labeling budget, and Table 2 summarizes the averaged TRC values across all considered labeling budgets.First, considering the averaged results, we can see that all methods have relatively better results than random selection, which indicates the potential of their usefulness.However, we found that the best averaged TRC value on LLMs is only 0.7321, which is far away from the idea detection performance, TRC = 1.Worsely, in some cases, e.g., Sentiment-LLaMA3, some methods have less than 0.2 TRC score, which never happened in conventional deep learning models [22].This means fault detection for LLMs is a more challenging problem compared to fault detection for conventional deep learning models.</p>
<p>Then, comparing each method, interestingly, we found that the simple method, Margin, which just utilizes the top-1 and top-2 output probabilities to measure the uncertainty, performs the best among our considered methods.This phenomenon is similar to the findings by existing works [37] which stated that simple methods perform the best for neural network test prioritization.Specifically, in more than half of cases (6 out of 11 cases), Margin achieved the highest averaged TRC scores.Methods that require embedding information for the data selection (NNS and TestRank) cannot stand out in LLMs.The reason could be that the embeddings extracted by LLMs are too high-dimensional and diverse to be learned by simple clustering methods (used by NNS) and graph neural networks (used by TestRank).</p>
<p>Answer to RQ2: Under our studied objects, the simple method, Margin, performs the best in fault detection for LLMs.However, there is still a big room for improvement, and more effective methods are needed.As analyzed in Section 4.1, the concentrated confidence issue can harm the performance of output-based fault detection methods (those methods will give similar uncertainty scores to the inputs), we need to diversify the prediction confidence of LLMs to enhance fault detection.</p>
<p>MUCS: MUTATION-BASED CONFIDENCE SMOOTHING</p>
<p>It is challenging since we cannot get the internal information (e.g., output logits) of closedsource LLMs to smooth the prediction confidence.Inspired by previous works [23,36] that showed the potential of testing deep learning models by mutation analysis (one uses mutation killing score to detect faults and the other one uses mutation analysis to improve the performance of code models), we propose MuCS, a simple yet effective black-box framework to enhance existing fault detection methods by smoothing the prediction confidence from the input-level using prompt mutation.Figure 4 presents the overall workflow of our proposed framework.It has two main steps, 1) prompt mutation, and 2) confidence smooth.Specifically, given an LLM and an input prompt p, we first utilize transformation methods to mutate the inputs and generate n mutated prompts,  1 ,  2 , ...,   .For NLP tasks, we use text augmentation methods [27] (e.g., token random deletion) to transform prompts into new ones.For the code tasks, we consider both text augmentation methods and code refactoring methods [2] (e.g., add new Print() functions) for the prompt mutation.After that, we feed all the mutants to the LLM and collect the output confidence   1 ,   2 ,. . .,    produced by the LLM.Finally, a confidence smooth method is employed to compute the final confidence of the LLM on the input.In this work, we use the straightforward method, average, to smooth the confidence.Algorithm 1 summarizes our mutation-based confidence smoothing Table 3. Averaged test relative coverage (relative improvement compared to results without using MuSC in Table 2) cross all labeling budgets of each method after prompt mutation-based confidence smoothing.To investigate whether our proposed framework MuCS can help fault detection for LLMs, we first analyze the effectiveness of existing fault detection with MuCS as a plug-in.Then, we explore the reason why MuCS enhances fault detection by analyzing its influences on the prediction confidence of LLMs.Our exploration plans to answer the following research questions.
Gini-M Entropy-M MCP-M MaxP-M Margin-M ATS-M NNS-M TestRank-M BALD DeepSeekCoder-CD 0.
‚Ä¢ RQ3: How effective is MuCS in enhancing fault detection for LLMs?In this research question, we compare the effectiveness of fault detection methods with and without using MuCS to analyze the usefulness of our proposed framework.‚Ä¢ RQ4: How does MuCS affect the final outputs of the LLMs?We want to explore how MuCS influences the outputs of LLMs to figure out why it works on fault detection.</p>
<p>Evaluation Setup</p>
<p>We set the mutation times as 10 for all tasks.That is, we generate 10 mutants for each input prompt.We set  (the number of mutation operators used) as 3 for all the tasks except for the clone detection.The reason is that we found the performance of LLMs on the clone detection task drops significantly (nearly random guesses, e.g., all the predicted labels of GPT3.5 are 0) when  = 3 and  = 2. Therefore, we set  = 1 for the clone detection task.For other tasks, LLMs have similar accuracy on the mutated datasets to the original ones (with less than 5% difference).For the other settings, we use the same settings as Section 3.</p>
<p>RQ3: Effectiveness of MuCS</p>
<p>First, we explore whether the MuCS can enhance the existing fault detection methods or not.Before that, since the input mutation can produce multiple predictions from LLMs which is similar to the dropout uncertainty [11].We introduce another fault detection method baseline which is based on dropout uncertainty, Bayesian Active Learning by Disagreement (BALD) [16].</p>
<p>Bayesian Active Learning by Disagreement (BALD) defines the uncertainty of an input by the disagreement of the outputs produced by LLMs on the mutants of this input.The disagreement score is calculated by 1 ‚àí  ( ( 1 ,...,  ))</p>
<p>ùëá</p>
<p>, where   is the predicted label of mutant   and  is the number of mutants.A higher BALD score indicates the input is more likely to be a fault.</p>
<p>Table 3 summarizes the average TRC scores achieved by different fault detection methods across all considered labeling budgets using MuCS (with -M as a suffix), and the relative difference between the scores with and without using MuCS.The results clearly demonstrate that the MuCS significantly boosts the performance of fault detection.Specifically, in 52 out of 71 cases, the performance of existing fault detection methods has been increased with TRC improvements ranging from 0.48% to 70.58%.Considering the average results (the last row in the Table ), MuCS can enhance all fault detection methods with improvement by up to 13.57%.Besides, we conducted the statistical analysis using two methods Welch's t-test and Wilcoxon signed-rank test to investigate the significance of improvements introduced by MuCS.Concretely, we collect all TRC scores resulting from each method with and without using MuCS and compute the significance.The results are shown in Table 4.The t-test suggests that for four methods (Gini, MCP, MaxP, and ATS), the improvements are significant, and the signed-rank test demonstrates all improvements are significant (with P-value &lt; 0.03).Therefore, we can conclude that MuCS is effective in boosting the existing fault detection methods on LLMs.</p>
<p>Comparing the improvements across different LLMs, we found that improvements in closedsource LLMs are greater than improvements in open-source LLMs.On average, the improvements in DeepSeekCoder, LLaMA3, GPT3.5, and GPT4 are 3.82%, 3.22%, 16.38%, and 7.87%, respectively.This means MuCS is an effective framework to enhance the black-box testing of LLMs.Then, after comparing each method after using MuCS, we found that MaxP, another simple method that directly utilizes the top-output probability as the indicator for data prioritization performs the best.Combining the conclusion in Section 4.2, simple methods perform better than learning-based methods with and without using MuCS.</p>
<p>Finally, even though the mutation-based confidence smooth can enhance the fault detection for LLMs, the best method only has a 0.7641 average TRC score.There is still a big room between the TRC scores achieved by existing methods and the ideal performance.</p>
<p>Answer to RQ3: MuCS significantly enhances the effectiveness of existing fault detection methods by up to 70.53% in terms of the TRC score.</p>
<p>RQ4: MuCS Affected Prediction Confidence</p>
<p>We further study how MuCS affects the prediction of LLMs and whether it diversifies the prediction confidence or not. Figure 5 depicts the confidence of LLMs on the datasets before and after confidence smoothing and Table 5 summarizes the average ECE scores.The results show that even though MuCS can reduce the average ECE scores (i.e., better calibration), there are still multiple pairs that have similar ECE scores, e.g., Sentiment-GPT35 and Sentiment-GPT4.Moreover, previous works [46] stated the effectiveness of fault detection methods is not directly related to the calibration of the model (most confidence calibration methods are useless or harmful for fault detection).It is not clear whether the improvements are introduced by better calibration.</p>
<p>Then, we check if MuCS can increase the diversity of confidence distribution to solve the confidence concentrated issue.We compute the variance of the number of confidence scores in each range to check this diversity.The results presented in Table 5 demonstrated that, except for one case CD-GPT35, LLMs have more diverse predictions on the datasets with an average 55.44% diversity improvement.Taking Figure 5(f) as an example, the confidence scores of GPT3.5 are concentrated in 1.0 which is highly contradictory to its accuracy (36%).After the mutation-based Answer to RQ4: MuCS significantly increases the diversity of confidence distribution of LLMs with an average 55.44% improvement and eliminates the concentrated confidence problem.</p>
<p>DISCUSSION</p>
<p>Confidence vs. Correctness</p>
<p>As mentioned before, all considered fault detection methods rely on the assumption -there is a connection between the prediction confidence and the prediction correctness.However, it is unclear whether the assumption stands or not, especially for LLMs.We design an exploratory study to answer this problem.Specifically, we label the corrected prediction as 1 and the incorrect prediction as 0 and then utilize Pearson correlation coefficient to quantify the relationship between confidence and correctness as shown in Table 6.We can see that there is a clear positive correlation between these two indicators except for the problem classification task before using MuCS.Moreover, after employing MuCS, all correlations become positive with a significance less than 0.03.This exploratory study demonstrates the relationship between confidence and correctness exists, and can be enhanced by MuCS.</p>
<p>Limitation and Future Direction</p>
<p>Limitation.For a given input, our proposed mutation-based confidence smooth solution needs to access LLMs  times, thus, the money consumption is  times than the normal way when we test closed-source LLMs.However, from the view of executing time, the increased cost is negligible since it is possible to perform the prediction of a batch of data at the same time.There is indeed a trade-off between the monetary cost of accessing close-source LLMs and data labeling.This should be considered case by case.Future direction.In this work, we only consider classification tasks since most existing fault detection methods are proposed for these tasks.Some methods, such as PRIMA [36] can be used for none classification tasks directly.Unfortunately, applying it to LLMs is not practical as it needs to mutate deep learning models but LLMs are much larger than classical deep learning models.However, the most important tasks people would use are generation tasks, e.g., chatting using ChatGPT.One of the important directions in this field is to propose fault detection methods for non-classification tasks, especially generation tasks, and then use these methods to help reveal critical faults in LLMs.Some metrics [19,20] have been designed for measuring the uncertainty in LLMs, how to combine these uncertainty metrics with existing methods could be the first try.</p>
<p>Threats to Validity</p>
<p>The internal threat lies in the implementation of fault detection methods, LLMs, and prompt mutation methods.The implementation of each fault detection method comes from the official project provided in the original paper.The implementation of LLMs comes from the famous opensource project Hugging Face.The code of text mutation also comes from the commonly used project TextAugment [27].The implementation of code refactoring methods modified from the previous work [8].</p>
<p>The external threat comes from our studied tasks, datasets, LLMs, and fault detection methods.As mentioned in Section 7.2, we only consider the classification tasks in this work.For the tasks, we consider tasks from both NLP and SE fields.Two traditional NLP tasks and two programming tasks are included in our work.For the studied LLMs, we consider both open-source LLMs, e.g., LLaMA, and closed-source LLMs (GPT4), where GPT4 is recognized as the SOTA LLM currently.Besides, another threat that comes from the datasets and models is that there might be data leakage issues in LLMs.For example, we found GPT4 has 100% accuracy on the problem classification task which means our test set is likely in the training set of GPT4.However, it is difficult to check this issue as GPT3.5 and GPT4 are closed-source LLMs.We can only exclude the cases from the accuracy analysis of LLMs.For the fault detection methods, we follow the previous works [17,18,26] and explore 10 fault detection methods including both learning-based methods and output-based methods in our study.These methods are from different communities, for example, DeepGini is from the SE community and TestRank is from the ML community.We believe our studied objectives are representative and diverse enough and the conclusion drawn from this work can be generalized to other similar cases.</p>
<p>The construct threat could be the hyperparameter settings and the non-determinism of LLMs.We directly use the default settings suggested by Hugging Face and OpenAI's API to build or access LLMs.Besides, we release the datasets and prompts used in our experiments to support reproducing results reported in our work and for future research.</p>
<p>CONCLUSION</p>
<p>In this work, we conducted the first empirical study to explore the potential of fault detection methods for LLMs.Based on the experiments on four tasks including NLP and code tasks, four LLMs, and nine fault detection methods, we found LLMs are overconfident with their predictions and simple methods such as Margin performs the best in revealing faults in LLMs.To enhance the detection performance, we proposed MuCS, which uses mutation analysis to smooth the prediction confidence of LLMs.Specifically, we utilized text transformation and code refactoring techniques to mutate the inputs and collect the averaged output probabilities of all mutants as the final prediction confidence of LLMs.Evaluation results demonstrated that MuCS significantly increased the diversity of output distribution of LLMs and enhanced the effectiveness of fault detection by up to 70.53% in terms of test relative coverage score.</p>
<p>Fig. 2 .
2
Fig. 2. Distribution of prediction confidence and ECE for LLMs.The dashed line indicates the average confidence.</p>
<p>Fig. 3 .
3
Fig. 3. Test relative coverage.</p>
<p>Algorithm 1 : 3 for ùëó = 0 ‚Üí ùêæ do 4 ùëÇùëÉ = RandomSelection (ùëÇùëÉùë†) 5 ùëù ‚Ä≤ = ùëÇùëÉ (ùëù) 6 ùëù = ùëù ‚Ä≤ 7 end
134567
MuCS Input : : input prompt : mutation operators : LLM : perturbation size : number of generated mutants Output : ‚Ñé : smoothed confidence 1  = [] 2 for  = 0 ‚Üí  do /<em> mutants generation </em>/</p>
<p>Table 1 .
1
Datasets and models used in our study.
TaskClass NumberModelTest Size AccuracyDeepSeekCoder100069.4%Clone Detection (CD)2GPT3.520075.5%GPT420077.0%DeepSeekCoder100042.6%Problem Classification (PC)5GPT3.520036.0%GPT4200100.0%LLaMA3100068.8%Sentiment Analysis (Sentiment)3GPT3.515082.0%GPT415090.0%LLaMA3140046.7%TagMyNews (TMN)7GPT3.520060.0%GPT420082.0%</p>
<p>Table 2 .
2
Average test relative coverage of each method.The best method is highlighted in green .
Random GiniEntropy McpMaxp Margin ATSNNSTestRankDeepSeekCoder-CD0.53010.65860.69730.6746 0.6586 0.6586-0.67320.6106GPT3.5-CD0.51210.62020.61320.5774 0.6202 0.6202---GPT4-CD0.55010.67770.65350.6371 0.5424 0.6777---DeepSeekCoder-PC0.66140.66090.67960.7116 0.6631 0.6670 0.7031 0.57540.7169GPT3.5-PC0.71600.77550.69840.7110 0.7774 0.7811 0.7134--LLaMA3-TMN0.61910.72270.75690.6419 0.7627 0.7737 0.7446 0.65410.6683GPT3.5-TMN0.58890.66200.65510.6139 0.6903 0.7042 0.7096--GPT4-TMN0.53950.80310.78890.7265 0.7815 0.7784 0.7944--LLaMA3-Sentiment0.53920.64900.61760.6332 0.8131 0.9377 0.5796 0.61470.5806GPT3.5-Sentiment0.51770.58520.59510.5728 0.6305 0.6543 0.4609--GPT4-Sentiment0.52590.80000.86670.8370 0.8000 0.8000 0.5259--Average0.57270.69230.69290.6670 0.7036 0.7321 0.6540 0.62940.6441</p>
<p>Table 4 .
4
Statistical analysis for comparing fault detection methods with and without using MuCS.
6886 (‚Üë4.55) 0.6890 (‚Üì1.20) 0.6592 (‚Üì2.28) 0.6886 (‚Üë4.55) 0.6886 (‚Üë4.55) -0.6576 (‚Üì2.32) 0.6134 (‚Üë0.46) 0.7067GPT3.5-CD0.6873 (‚Üë10.81) 0.5989 (‚Üì2.33) 0.6369 (‚Üë10.31) 0.6873 (‚Üë10.81) 0.6873 (‚Üë10.81) ---0.5827GPT4-CD0.7550 (‚Üë11.41) 0.7211 (‚Üë10.34) 0.7318 (‚Üë14.87) 0.7550 (‚Üë39.20) 0.5107 (‚Üì24.64) ---0.4380DeepSeekCoder-PC 0.7462 (‚Üë12.91) 0.7134 (‚Üë4.97) 0.6883 (‚Üì3.27) 0.7412 (‚Üë11.77) 0.6516 (‚Üì2.31) 0.7212 (‚Üë2.57) 0.6984 (‚Üë21.37) 0.7425 (‚Üë3.57) 0.7145GPT3.5-PC0.8344 (‚Üë7.59) 0.8346 (‚Üë19.49) 0.7116 (‚Üë0.08) 0.8492 (‚Üë9.24) 0.8343 (‚Üë6.81) 0.7930 (‚Üë11.16) --0.8389LLaMA3-TMN0.7637 (‚Üë5.68) 0.7722 (‚Üë2.02) 0.7073 (‚Üë10.18) 0.7702 (‚Üë0.99) 0.7653 (‚Üì1.09) 0.7162 (‚Üì3.82) 0.7156 (‚Üë9.41) 0.7086 (‚Üë6.03) 0.7434GPT3.5-TMN0.7431 (‚Üë12.24) 0.7319 (‚Üë11.72) 0.6208 (‚Üë1.13) 0.7630 (‚Üë10.54) 0.7481 (‚Üë6.24) 0.7708 (‚Üë8.63) --0.7407GPT4-TMN0.7691 (‚Üì4.23) 0.7327 (‚Üì7.12) 0.7451 (‚Üë2.55) 0.7790 (‚Üì0.32) 0.7821 (‚Üë0.48) 0.7852 (‚Üì1.16) --0.7037LLaMA3-Sentiment 0.6417 (‚Üì1.13) 0.5899 (‚Üì4.48) 0.8423 (‚Üë33.02) 0.7587 (‚Üì6.70) 0.8643 (‚Üì7.83) 0.6211 (‚Üë7.16) 0.6006 (‚Üì2.30) 0.6065 (‚Üë4.46) 0.6199GPT3.5-Sentiment0.7654 (‚Üë30.80) 0.7646 (‚Üë28.49) 0.7959 (‚Üë38.94) 0.7539 (‚Üë19.58) 0.6782 (‚Üë3.65) 0.7860 (‚Üë70.53) --0.6379GPT4-Sentiment0.8593 (‚Üë7.41) 0.8593 (‚Üì0.85) 0.8444 (‚Üë0.88) 0.8593 (‚Üë7.41) 0.8519 (‚Üë6.49) 0.7481 (‚Üë42.24) --0.6667Average0.7503 (‚Üë8.39) 0.7280 (‚Üë5.05) 0.7258 (‚Üë8.81) 0.7641 (‚Üë8.60) 0.7329 (‚Üë0.12) 0.7427 (‚Üë13.57) 0.6681 (‚Üë6.15) 0.6678 (‚Üë3.67) 0.6720GiniEntropyMcpMaxpMarginATSNNSTestRankWelch's t-testSignificance 2.88731.73733.03552.57871.15673.10491.14050.7018P-value4.10E-03 8.31E-02 2.56E-03 1.03E-02 2.48E-01 2.11E-03 2.56E-01 4.84E-01Wilcoxon signed-rank test Significance500.51136.5770.5663.51273.5283148.542.5P-value4.82E-10 2.47E-04 1.30E-06 9.15E-09 1.18E-03 1.95E-06 1.09E-02 2.11E-056 EVALUATION6.1 Research Question</p>
<p>Table 5 .
5
ECE and variance of prediction distribution before and after confidence smoothing.
ECE (‚Üì)Diversity (‚Üì)ModelBefore After BeforeAfterCD-DeepSeekCoder 0.9421 0.9112 11066.62 9535.22CD-GPT350.1915 0.2563 584.96655.29CD-GPT40.1630 0.1638 696.62542.42PC-DeepSeekCoder 0.3110 0.1713 7963.02 1189.08PC-GPT350.4560 0.3449 205.3624.69PC-GPT40.0143-332.00-TMN-LLaMA30.1711 0.0701 12338.18 1641.37TMN-GPT350.0982 0.1030 205.3624.69TMN-GPT40.0407 0.0846 196.5662.29Sentiment-LLaMA3 0.5714 0.2345 8301.62 4898.62Sentiment-GPT350.0917 0.1180 335.87104.87Sentiment-GP40.0787 0.1093 230.6093.33Average change‚Üë 17.60%‚Üë 55.44%</p>
<p>Table 6 .
6
Correlation between prediction confidence and prediction correctness.
Without MuCSWith MuCSModelSignificance P-value Significance P-valueCD-DeepSeekCoder0.27537.43E-190.22431.09E-12CD-GPT3.50.26361.21E-130.30329.41E-18CD-GPT40.29694.71E-170.28231.70E-15PC-DeepSeekCoder-0.05229.92E-020.24021.38E-14PC-GPT3.50.09525.22E-020.25461.07E-07TMN-LLaMA30.11102.53E-050.24261.21E-20TMN-GPT3.50.29042.97E-290.32382.47E-36TMN-GPT40.39091.55E-530.40595.97E-58Sentiment-LLaMA30.46031.34E-530.37302.33E-34Sentiment-GPT3.50.20074.37E-030.21242.53E-03Sentiment-GPT40.42633.10E-100.43321.49E-10
OpenAI, ChatGPT, https://chat.openai.com/
Autolabel, https://github.com/refuel-ai/autolabel
Hugging Face, https://huggingface.co/
ùê∂ ùë†ùëöùëúùëúùë°‚Ñé = Mean (ùê∂ùë†)
solution.MuCS first randomly selects multiple (defined by the perturbation size ) mutation operators  (line 4) to mutate  and then generates  ‚Ä≤ (lines 4, 5).Then, MuCS collects all the prediction confidence produced by LLMs using all mutants  ‚Ä≤ (line 8).Finally, the averaged confidence is returned as the smoothed confidence (lines 10 and 11).‚Ä¢ Synonym Replacement (SP) randomly selects  words in  and replaces them as their synonyms, where  &lt; .Here, the synonyms are searched from the WordNet[28].We use the default setting of TextAugment[27],  = 1 in MuCS.‚Ä¢ Random Deletion (RD) randomly removes words in the  with a probability.Specifically, RD assigns a random probability  uniformed from 0 to 1 to each word in .Given a probability threshold  , if  is less than  , the corresponding word will be removed.We use the default setting of MixCode[8],  = 0.01 in MuCS.‚Ä¢ Random Insertion (RI) randomly selects  words in  first where  &lt; .Then, it finds the synonyms of selected words and inserts them into the random locations of .We use the default setting of MixCode,  = 1 in MuCS.‚Ä¢ Random Swap (RW) randomly swaps two words in .Mutation Operators‚Ä¢ Punctuation Insertion (PI) randomly selects  punctuation (e.g., !) and inserts them into the random locations of , where  &lt; .We use the default setting of MixCode,  = 1 in MuCS.‚Ä¢ Print Adding (PA) randomly selects a line in the code and adds a print function with randomly generated content below it.‚Ä¢ Local Variable Adding (LVA) randomly selects a line in the code and defines an unused local variable below it.‚Ä¢ Dead If Adding (DIA) randomly selects a line in the code and then adds an unreachable if statement below it.‚Ä¢ Duplication randomly copies an assignment in the code and inserts it next to this assignment.During the mutant generation process, we randomly select  mutation operators to mutate  and generate its mutant  ‚Ä≤ .smooth, confidence scores are spread throughout the distribution which is more useful to help understand the capability of LLMs for this task.
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. 2023. Gpt-4 technical report. 2023arXiv preprint</p>
<p>Empirical evaluation of the impact of object-oriented code refactoring on quality attributes: A systematic literature review. Al Jehad, Anas Dallal, Abdin, IEEE Transactions on Software Engineering. 442017. 2017</p>
<p>DeepAbstraction++: Enhancing Test Prioritization Performance via Combined Parameterized Boxes. Hamzah Al-Qadasi, Yli√®s Falcone, Saddek Bensalem, International Conference on Bridging the Gap between AI and Reality. Springer2023</p>
<p>Shenglin Bao, Chaofeng Sha, Bihuan Chen, Xin Peng, Wenyun Zhao, 10.1145/3597926.3598073Defense of Simple Techniques for Neural Network Test Case Selection (ISSTA 2023). New York, NY, USAAssociation for Computing Machinery2023</p>
<p>A survey on evaluation of large language models. Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, ACM Transactions on Intelligent Systems and Technology. 2023. 2023</p>
<p>Cerebras-gpt: Open compute-optimal language models trained on the cerebras wafer-scale cluster. Nolan Dey, Gurpreet Gosal, Hemant Khachane, William Marshall, Ribhu Pathria, Marvin Tom, Joel Hestness, arXiv:2304.032082023. 2023arXiv preprint</p>
<p>. Yinpeng Dong, Huanran Chen, Jiawei Chen, Zhengwei Fang, Xiao Yang, Yichi Zhang, Yu Tian, Hang Su, arXiv:2309.11751Jun Zhu. 2023. 2023How Robust is Google's Bard to Adversarial Image Attacks? arXiv preprint</p>
<p>Mixcode: Enhancing code classification by mixup-based data augmentation. Zeming Dong, Qiang Hu, Yuejun Guo, Maxime Cordy, Mike Papadakis, Zhenya Zhang, Yves Le Traon, Jianjun Zhao, 2023 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER). IEEE2023</p>
<p>Evaluating Large Language Models in Class-Level Code Generation. Xueying Du, Mingwei Liu, Kaixin Wang, Hanlin Wang, Junwei Liu, Yixuan Chen, Jiayi Feng, Chaofeng Sha, Xin Peng, Yiling Lou, 2024 IEEE/ACM 46th International Conference on Software Engineering (ICSE). IEEE Computer Society2024</p>
<p>DeepGini: prioritizing massive tests to enhance the robustness of deep neural networks (ISSTA. Yang Feng, Qingkai Shi, Xinyu Gao, Jun Wan, Chunrong Fang, Zhenyu Chen, 10.1145/3395363.33973572020. 2020Association for Computing MachineryNew York, NY, USA</p>
<p>Dropout as a bayesian approximation: Representing model uncertainty in deep learning. Yarin Gal, Zoubin Ghahramani, 2016</p>
<p>Xinyu Gao, Yang Feng, Yining Yin, Zixi Liu, Zhenyu Chen, Baowen Xu, 10.1145/3510003.3510232Adaptive test selection for deep neural networks (ICSE '22). New York, NY, USAAssociation for Computing Machinery2022</p>
<p>On calibration of modern neural networks. Chuan Guo, Geoff Pleiss, Yu Sun, Kilian Q Weinberger, International conference on machine learning. PMLR2017</p>
<p>DeepSeek-Coder: When the Large Language Model Meets Programming -The Rise of Code Intelligence. Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y Wu, Y K Li, Fuli Luo, Yingfei Xiong, Wenfeng Liang, 10.48550/ARXIV.2401.14196arXiv:2401.141962024. 2024</p>
<p>A baseline for detecting misclassified and out-of-distribution examples in neural networks. Dan Hendrycks, Kevin Gimpel, arXiv:1610.021362016. 2016arXiv preprint</p>
<p>Bayesian active learning for classification and preference learning. Neil Houlsby, Ferenc Husz√°r, Zoubin Ghahramani, M√°t√© Lengyel, arXiv:1112.57452011. 2011arXiv preprint</p>
<p>Test Optimization in DNN Testing: A Survey. Qiang Hu, Yuejun Guo, Xiaofei Xie, Maxime Cordy, Lei Ma, Mike Papadakis, Yves Le Traon, 10.1145/3643678ACM Trans. Softw. Eng. Methodol. 2024. jan 2024</p>
<p>Qiang Hu, Yuejun Guo, Xiaofei Xie, Maxime Cordy, Wei Ma, Mike Papadakis, Yves Le Traon, arXiv:2308.01314Evaluating the robustness of test selection methods for deep neural networks. 2023. 2023arXiv preprint</p>
<p>Look before you leap: An exploratory study of uncertainty measurement for large language models. Yuheng Huang, Jiayang Song, Zhijie Wang, Huaming Chen, Lei Ma, arXiv:2307.102362023. 2023arXiv preprint</p>
<p>Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation. Lorenz Kuhn, Yarin Gal, Sebastian Farquhar, arXiv:2302.096642023. 2023arXiv preprint</p>
<p>Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, arXiv:2305.06161StarCoder: may the source be with you!. 2023. 2023arXiv preprint</p>
<p>TestRank: Bringing Order into Unlabeled Test Instances for Deep Learning Tasks. Y U Li, L I Min, Lai Qiuxia, Yannan Liu, Qiang Xu, Advances in Neural Information Processing Systems. Y Beygelzimer, P S Dauphin, J Wortman Liang, Vaughan, Curran Associates, Inc2021. 20874-2088634</p>
<p>CCTest: Testing and Repairing Code Completion Systems. Zongjie Li, Chaozheng Wang, Zhibo Liu, Haoxuan Wang, Dong Chen, Shuai Wang, Cuiyun Gao, 10.1109/ICSE48619.2023.00110Proceedings of the 45th International Conference on Software Engineering. the 45th International Conference on Software EngineeringMelbourne, Victoria, AustraliaIEEE Press2023</p>
<p>Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation. Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, Lingming Zhang, Advances in Neural Information Processing Systems. 362024. 2024</p>
<p>Wei Ma, Shangqing Liu, Wenhan Wang, Qiang Hu, Ye Liu, Cen Zhang, Liming Nie, Yang Liu, arXiv:2305.12138The Scope of ChatGPT in Software Engineering: A Thorough Investigation. 2023. 2023arXiv preprint</p>
<p>Test selection for deep learning systems. Wei Ma, Mike Papadakis, Anestis Tsakmalis, Maxime Cordy, Yves Le Traon, ACM Transactions on Software Engineering and Methodology. 302021. 2021TOSEM)</p>
<p>Improving short text classification through global augmentation methods. Vukosi Marivate, Tshephisho Sefara, International Cross-Domain Conference for Machine Learning and Knowledge Extraction. Springer2020</p>
<p>WordNet: a lexical database for English. A George, Miller, 10.1145/219717.219748Commun. ACM. 381995. nov 1995</p>
<p>HumanEval-XL: A Multilingual Code Generation Benchmark for Cross-lingual Natural Language Generalization. Qiwei Peng, Yekun Chai, Xuhong Li, arXiv:2402.166942024. 2024arXiv preprint</p>
<p>CodeNet: A Large-Scale AI for Code Dataset for Learning a Diversity of Coding Tasks. Ruchir Puri, David S Kung, Geert Janssen, Wei Zhang, Giacomo Domeniconi, Vladimir Zolotov, Julian Dolby, Jie Chen, Mihir R Choudhury, Lindsey Decker, Veronika Thost, Luca Buratti, Saurabh Pujar, Shyam Ramji, Ulrich Finkler, Susan Malaika, Frederick Reiss, Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021. Joaquin Vanschoren, Sai-Kit Yeung, the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 20212021. December 2021</p>
<p>Jonas Baptiste Roziere, Fabian Gehring, Sten Gloeckle, Itai Sootla, Gat, Ellen Xiaoqing, Yossi Tan, Jingyu Adi, Tal Liu, J√©r√©my Remez, Rapin, arXiv:2308.12950Code llama: Open foundation models for code. 2023. 2023arXiv preprint</p>
<p>Towards a Big Data Curated Benchmark of Inter-project Code Clones. Jeffrey Svajlenko, Judith F Islam, Iman Keivanloo, Chanchal Kumar Roy, Mohammad Mamun, Mia , 10.1109/ICSME.2014.7730th IEEE International Conference on Software Maintenance and Evolution. Victoria, BC, CanadaIEEE Computer Society2014. September 29 -October 3, 2014</p>
<p>Alpaca: A strong, replicable instruction-following model. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, Stanford Center for Research on Foundation Models. 2023. 202337</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timoth√©e Lachaux, Baptiste Lacroix, Naman Rozi√®re, Eric Goyal, Faisal Hambro, Azhar, arXiv:2302.13971Llama: Open and efficient foundation language models. 2023. 2023arXiv preprint</p>
<p>Classification of Short Texts by Deploying Topical Annotations. Daniele Vitale, Paolo Ferragina, Ugo Scaiella, 10.1007/978-3-642-28997-2_32Advances in Information Retrieval -34th European Conference on IR Research. Lecture Notes in Computer Science. Berkant Zaragoza, Vanessa Barla Cambazoglu, Ronny Murdock, Fabrizio Lempel, Silvestri, Barcelona, Spain; Ricardo Baeza-Yates, Arjen P. de VriesSpringer2012. 2012. April 1-5, 20127224</p>
<p>Prioritizing Test Inputs for Deep Neural Networks via Mutation Analysis. Zan Wang, Hanmo You, Junjie Chen, Yingyi Zhang, Xuyuan Dong, Wenbin Zhang, 10.1109/ICSE43902.2021.00046Proceedings of the 43rd International Conference on Software Engineering. the 43rd International Conference on Software EngineeringMadrid, SpainIEEE Press2021ICSE '21)</p>
<p>Simple techniques work surprisingly well for neural network test prioritization and active learning (replicability study). Michael Weiss, Paolo Tonella, Proceedings of the 31st ACM SIGSOFT International Symposium on Software Testing and Analysis. the 31st ACM SIGSOFT International Symposium on Software Testing and Analysis2022</p>
<p>A systematic evaluation of large language models of code. Uri Frank F Xu, Graham Alon, Vincent Neubig, Josua Hellendoorn, Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming. the 6th ACM SIGPLAN International Symposium on Machine Programming2022</p>
<p>Dynamic Data Fault Localization for Deep Neural Networks. Yining Yin, Yang Feng, Shihao Weng, Zixi Liu, Yuan Yao, Yichi Zhang, Zhihong Zhao, Zhenyu Chen, Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering. the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering2023</p>
<p>Evaluating instruction-tuned large language models on code comprehension and generation. Zhiqiang Yuan, Junwei Liu, Qiancheng Zi, Mingwei Liu, Xin Peng, Yiling Lou, arXiv:2308.012402023. 2023arXiv preprint</p>
<p>Wenxuan Zhang, Yue Deng, Bing Liu, Sinno Jialin Pan, Lidong Bing, arXiv:2305.15005Sentiment Analysis in the Era of Large Language Models: A Reality Check. 2023. 2023arXiv preprint</p>
<p>Siren's song in the AI ocean: a survey on hallucination in large language models. Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, arXiv:2309.012192023. 2023arXiv preprint</p>
<p>CertPri: certifiable prioritization for deep neural networks via movement cost in feature space. Haibin Zheng, Jinyin Chen, Haibo Jin, 2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE). IEEE2023</p>
<p>Large language models are human-level prompt engineers. Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, Jimmy Ba, arXiv:2211.019102022. 2022arXiv preprint</p>
<p>Openmix: Exploring outlier samples for misclassification detection. Fei Zhu, Zhen Cheng, Xu-Yao Zhang, Cheng-Lin Liu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>Rethinking confidence calibration for failure prediction. Fei Zhu, Zhen Cheng, Xu-Yao Zhang, Cheng-Lin Liu, arXiv:2303.029702023. 2023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1062 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1062</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1062</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-24.html">extraction-schema-24</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <p><strong>Paper ID:</strong> paper-cf4a1eadf2e1a8e9144446790e563c8a3097f9c3</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/cf4a1eadf2e1a8e9144446790e563c8a3097f9c3" target="_blank">CM3: Cooperative Multi-goal Multi-stage Multi-agent Reinforcement Learning</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> This work restructure the problem into a novel two-stage curriculum, in which single-agent goal attainment is learned prior to learning multi-agent cooperation, and derives a new multi-goal multi- agent policy gradient with a credit function for localized credit assignment.</p>
                <p><strong>Paper Abstract:</strong> A variety of cooperative multi-agent control problems require agents to achieve individual goals while contributing to collective success. This multi-goal multi-agent setting poses difficulties for recent algorithms, which primarily target settings with a single global reward, due to two new challenges: efficient exploration for learning both individual goal attainment and cooperation for others' success, and credit-assignment for interactions between actions and goals of different agents. To address both challenges, we restructure the problem into a novel two-stage curriculum, in which single-agent goal attainment is learned prior to learning multi-agent cooperation, and we derive a new multi-goal multi-agent policy gradient with a credit function for localized credit assignment. We use a function augmentation scheme to bridge value and policy functions across the curriculum. The complete architecture, called CM3, learns significantly faster than direct adaptations of existing algorithms on three challenging multi-goal multi-agent problems: cooperative navigation in difficult formations, negotiating multi-vehicle lane changes in the SUMO traffic simulator, and strategic cooperation in a Checkers environment.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1062.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1062.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CM3 (Cooperative Navigation)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CM3 agent in Cooperative Navigation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A parameter-shared multi-agent actor-critic system trained with a two-stage curriculum (single-agent Stage 1 then multi-agent Stage 2), function augmentation, and a goal-conditioned credit function to enable fast learning of individual goal attainment and localized cooperation in simulated navigation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>CM3 agent (parameter-shared)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Reinforcement learning agent using an actor-critic architecture enhanced by: (1) a Stage-1 single-agent curriculum for learning greedy goal-directed behavior, (2) function augmentation to extend Stage-1 networks to multi-agent inputs, and (3) a goal-specific credit function and corresponding multi-goal policy gradient for localized credit assignment. Training uses centralized training with decentralized execution and replay buffers (off-policy option).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent (virtual)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Cooperative Navigation (Antipodal / Intersection / Merge variants)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Continuous 2D particle world with multiple movable agents and static landmarks. Agents have local observations decomposed into o_self (own absolute position & velocity) and o_others (relative positions & velocities of other agents). Rewards are individual (negative distance to assigned landmark) with collision penalties (-1). Variants increase difficulty via initial and target placements requiring complex coordinated maneuvers (e.g., antipodal starts, intersection crossings, merge scenarios). Episodes terminate when agents are within a small threshold of their goals.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Number of agents (N up to 4 in experiments), continuous state (positions and velocities per agent), joint action space size (|A|^N; |A|=5 actions per agent), presence of collisions (dynamics with contact forces), and trajectory length until termination; rewards are dense per-agent but cooperation region is a small subset of state space.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>medium-high (N=4 experiments; continuous position/velocity state; multi-agent contact dynamics)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Stochastic initial/goal assignment: with 20% probability positions/goals are uniform random in (-1,1)^2, otherwise fixed predefined challenging configurations. Variation manifested as randomized episode starts and a mix of predefined hard layouts.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium (mixture of fixed hard configurations and uniform-random initializations)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Average episodic return (sum of per-agent rewards), convergence speed measured in number of training episodes to reach near-optimal behavior, collision rate as auxiliary metric</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>CM3 reached final performance comparable to best baselines; converged >15k episodes faster than IAC (textual result). Exact numeric final returns not listed in paper tables for these variants.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Yes — the paper explicitly discusses that environment complexity (large joint state/action spaces and regions requiring cooperation) and variation (randomized starts/goals) make exploration hard. They argue and show empirically that reducing complexity via Stage-1 single-agent training (learning individual goal attainment) reduces the effective exploration difficulty when agents are later placed into the multi-agent (higher-variation) environment, thereby accelerating discovery of cooperative solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Curriculum learning (two-stage: single-agent Stage 1 followed by multi-agent Stage 2) + function augmentation + centralized training with decentralized execution</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Stage-1 used 1k episodes in the Cooperative Navigation experiments (paper reports Stage-1 sizes like 1k); CM3 achieved large speedups in Stage-2, converging >15k episodes faster than independent actor-critic baseline in some variants.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Curriculum + function augmentation substantially speeds learning: initializing multi-agent policies with Stage-1 single-agent parameters yields much faster and more stable convergence than training the full multi-agent architecture from scratch. The credit function further improves stability and reduces collisions compared to using only global Q-based advantages.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CM3: Cooperative Multi-goal Multi-stage Multi-agent Reinforcement Learning', 'publication_date_yy_mm': '2018-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1062.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1062.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CM3 (SUMO Double Lane Merge)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CM3 agent in SUMO double lane merge (autonomous driving)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A CM3-trained driving agent fleet in the SUMO microscopic traffic simulator that learns cooperative double-lane-merge maneuvers under limited field-of-view and sparse rewards, using curriculum initialization, convolutional processing of local occupancy/grids, and a goal-conditioned credit function for localized credit assignment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>CM3 driving agents (parameter-shared sedans)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Reinforcement learning agents (actor-critic) trained with CM3: Stage-1 single-agent lane-reaching MDP (policy π^1 and Q^1), then Stage-2 multi-agent augmentation that adds convolutional processing of o_others (discretized occupancy and relative speed grid) and trains goal-conditioned credit functions for action-goal pairs. Training used centralized critics, replay buffers, and discrete 5-action control (no-op, accel, decel, lane-left, lane-right).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent (virtual autonomous vehicle)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>SUMO double lane merge (microscopic traffic simulator)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Straight 200 m road with 4 lanes, sublane discretization (4 sub-lanes per lane, 13x9 occupancy grid per agent with two channels: occupancy and relative speed), other traffic controlled by SUMO; agents have limited field-of-view (15 m forward/backwards) and sparse rewards (collision: -1, timeout: -10, goal reach: +10*(1-Δ) where Δ is sub-lane offset, speed penalty). Agent goals are target lanes at x=190m. Episodes are up to 33 simulation steps (0.2s resolution).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>State dimensionality (agent absolute positions and speed; o_others: 13x9x2 occupancy tensor), number of interacting vehicles (training used N=2 learning agents but environment includes SUMO-controlled vehicles), discrete action set size (5), sparse and delayed rewards, sublane resolution (continuous lateral discretization), and required multi-step coordinated maneuvers (sequence length up to 33 steps).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high (sparse rewards, partial observability via local grids, dynamic interactions with other vehicles and need for multi-step coordinated merges)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Initial and goal lane assignment: with 20% probability goals and initial lanes are uniform random; with 80% a specific difficult pairing is used. Generalization tests included heavy SUMO-controlled traffic and rare initial/goal lane configurations (Table 1: configurations C1–C4).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium-high (mixture of structured hard configurations and uniform random setups; test includes high-traffic scenarios)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Average sum of agents' episodic rewards (per-episode returns), success of merge maneuvers, convergence speed (episodes to reach cooperative behavior)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Generalization test (heavy traffic) average summed rewards (100 test episodes): C1: 16.17, C2: 14.93, C3: 15.85, C4: 16.35 (these are CM3 test-set average sum-of-rewards reported in Table 1). In training, CM3 learned cooperative solutions while COMA and IAC failed to perform necessary merges; exact training-episode counts to convergence not numerically specified except relative statements.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Yes — the paper emphasizes that SUMO's dominant challenge is credit assignment (since initial states force agents into cooperative regions) combined with sparse rewards; variation via different initial/goal lane assignments and traffic density increases the difficulty. CM3's curriculum reduces exploration burden and its credit function addresses the credit-assignment challenge, enabling learning despite high complexity and variation; tests show policies trained with few agents generalize to heavier traffic (higher complexity) with good performance.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td>See generalization test: heavy traffic (high complexity + variation) results C1–C4 (sum rewards 16.17 / 14.93 / 15.85 / 16.35 averaged over 100 episodes).</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Curriculum learning (single-agent Stage-1 lane-reaching MDP) + function augmentation + centralized critics + off-policy replay buffer in some experiments</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Yes — policies trained with N=2 agents on relatively empty roads generalized to scenarios with heavy SUMO-controlled traffic and rare initial/goal lane configurations. CM3 outperformed IAC and COMA on these heavy-traffic tests (Table 1), demonstrating robustness to increased environmental complexity and variation.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Stage-1 episodes unspecified for SUMO in main text (Stage-1 used per-environment settings); paper notes Stage-1 training uses orders of magnitude fewer samples than full multi-agent training. Comparatively, CM3 was sample-efficient enough to outperform baselines in SUMO experiments (learning converged faster than some baselines; exact episodes to convergence not always numeric).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>In SUMO, credit assignment is the main bottleneck (cooperation region is unavoidable due to initial states); CM3's credit function and curriculum enable learning of long, sparse sequences of coordinated maneuvers (double merges) that baselines (COMA, IAC) failed to find. Additionally, centralized training with few learning agents can generalize to heavier traffic (higher-complexity deployment) when local interactions dominate.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CM3: Cooperative Multi-goal Multi-stage Multi-agent Reinforcement Learning', 'publication_date_yy_mm': '2018-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1062.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1062.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CM3 (Checkers)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CM3 agent in strategic Checkers gridworld</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>CM3 applied to a strategic 2-agent gridworld where each agent has an individual, conflicting collection goal (one prefers red tiles, the other yellow) and cooperation (path clearing) is required to reach the global optimum; uses curriculum, function augmentation, and goal-conditioned credit functions to learn cooperative yet individually-goal-directed strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>CM3 gridworld agents (Agent A and Agent B)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Actor-critic reinforcement learners trained with CM3; Stage-1 trains single-agent policies alternating roles (Agent A or B) to achieve greedy goal behavior, Stage-2 augments networks for multi-agent observations and trains centralized critics including the credit function Q_n(s,a^m) for localized credit assignment. Observations are partial (5x5 field-of-view tensors plus other-state info), and rewards are sparse and adversarially-signed (+1 / -0.5 depending on agent and tile color).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent (virtual gridworld agent)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Checkers strategic gridworld (5x13 grid with collectible red/yellow tiles)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Gridworld with a central 3x8 region of red/yellow collectible tiles in a checkered pattern that disappear when collected. Agents have limited 5x5 field-of-view sensory tensors, and different reward preferences (Agent A: +1 red, -0.5 yellow; Agent B: -0.5 red, +1 yellow). Global optimum requires agents to clear paths and coordinate to allow each other to collect preferred tiles; episodes end at 75 steps or when all tiles collected.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Combinatorial joint trajectory space (two agents with 5 discrete actions each), partial observability (limited 5x5 field-of-view), goal conflict (opposing preferences) requiring sequential coordination, and delayed/no-instantaneous-reward cooperative actions; state includes reward tensor and agent positions.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high (combinatorial planning, partial observability, adversarially aligned individual goals requiring cooperation)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Initial positions fixed in experiments (Agent A at (2,8), Agent B at (4,8)); variation is introduced by alternating which role is trained in Stage-1 and by stochasticity in agent actions during learning. No large procedural variability described for this domain.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>low-medium (mostly fixed layout with stochastic behavior during training; role alternation in Stage-1 adds some variation)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Episodic score (sum of per-agent collected rewards), convergence speed in episodes to reach global optimum</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>CM3 converged to global optimum with score 24; CM3 (with 5k episodes Stage-1) converged ~10k episodes faster than COMA and QMIX to this optimum (explicit numeric 'score 24' reported).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Yes — the paper highlights that in Checkers exploration of combinatorially large joint trajectories and sparse/cooperative-required actions make learning hard; CM3's curriculum (learning individual goal attainment first) helps discovery of cooperative sequences and the credit function resolves credit assignment for path-clearing actions.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Curriculum learning (Stage-1 single-agent alternating role training) + function augmentation + centralized critics with credit function; off-policy replay optional</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Stage-1 used 5k episodes for Checkers (paper reports using 5k Stage-1 episodes), and Stage-2 training led CM3 to converge ~10k episodes faster than COMA and QMIX to the global optimum.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>CM3 successfully learns cooperative strategies in environments where individual agents cannot maximize reward alone. The curriculum greatly accelerates exploration toward cooperative states and the credit function enables stable credit assignment for actions that enable others' success; baselines (IAC, COMA, QMIX) were slower or failed except COMA solved Checkers but slower.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CM3: Cooperative Multi-goal Multi-stage Multi-agent Reinforcement Learning', 'publication_date_yy_mm': '2018-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Multi-agent actor-critic for mixed cooperative-competitive environments <em>(Rating: 2)</em></li>
                <li>Counterfactual multi-agent policy gradients <em>(Rating: 2)</em></li>
                <li>QMIX: Monotonic value function factorisation for deep multi-agent reinforcement learning <em>(Rating: 2)</em></li>
                <li>Value-decomposition networks for cooperative multi-agent learning based on team reward <em>(Rating: 2)</em></li>
                <li>Microscopic traffic simulation using SUMO <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1062",
    "paper_id": "paper-cf4a1eadf2e1a8e9144446790e563c8a3097f9c3",
    "extraction_schema_id": "extraction-schema-24",
    "extracted_data": [
        {
            "name_short": "CM3 (Cooperative Navigation)",
            "name_full": "CM3 agent in Cooperative Navigation",
            "brief_description": "A parameter-shared multi-agent actor-critic system trained with a two-stage curriculum (single-agent Stage 1 then multi-agent Stage 2), function augmentation, and a goal-conditioned credit function to enable fast learning of individual goal attainment and localized cooperation in simulated navigation tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "CM3 agent (parameter-shared)",
            "agent_description": "Reinforcement learning agent using an actor-critic architecture enhanced by: (1) a Stage-1 single-agent curriculum for learning greedy goal-directed behavior, (2) function augmentation to extend Stage-1 networks to multi-agent inputs, and (3) a goal-specific credit function and corresponding multi-goal policy gradient for localized credit assignment. Training uses centralized training with decentralized execution and replay buffers (off-policy option).",
            "agent_type": "simulated agent (virtual)",
            "environment_name": "Cooperative Navigation (Antipodal / Intersection / Merge variants)",
            "environment_description": "Continuous 2D particle world with multiple movable agents and static landmarks. Agents have local observations decomposed into o_self (own absolute position & velocity) and o_others (relative positions & velocities of other agents). Rewards are individual (negative distance to assigned landmark) with collision penalties (-1). Variants increase difficulty via initial and target placements requiring complex coordinated maneuvers (e.g., antipodal starts, intersection crossings, merge scenarios). Episodes terminate when agents are within a small threshold of their goals.",
            "complexity_measure": "Number of agents (N up to 4 in experiments), continuous state (positions and velocities per agent), joint action space size (|A|^N; |A|=5 actions per agent), presence of collisions (dynamics with contact forces), and trajectory length until termination; rewards are dense per-agent but cooperation region is a small subset of state space.",
            "complexity_level": "medium-high (N=4 experiments; continuous position/velocity state; multi-agent contact dynamics)",
            "variation_measure": "Stochastic initial/goal assignment: with 20% probability positions/goals are uniform random in (-1,1)^2, otherwise fixed predefined challenging configurations. Variation manifested as randomized episode starts and a mix of predefined hard layouts.",
            "variation_level": "medium (mixture of fixed hard configurations and uniform-random initializations)",
            "performance_metric": "Average episodic return (sum of per-agent rewards), convergence speed measured in number of training episodes to reach near-optimal behavior, collision rate as auxiliary metric",
            "performance_value": "CM3 reached final performance comparable to best baselines; converged &gt;15k episodes faster than IAC (textual result). Exact numeric final returns not listed in paper tables for these variants.",
            "complexity_variation_relationship": "Yes — the paper explicitly discusses that environment complexity (large joint state/action spaces and regions requiring cooperation) and variation (randomized starts/goals) make exploration hard. They argue and show empirically that reducing complexity via Stage-1 single-agent training (learning individual goal attainment) reduces the effective exploration difficulty when agents are later placed into the multi-agent (higher-variation) environment, thereby accelerating discovery of cooperative solutions.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Curriculum learning (two-stage: single-agent Stage 1 followed by multi-agent Stage 2) + function augmentation + centralized training with decentralized execution",
            "generalization_tested": false,
            "generalization_results": "",
            "sample_efficiency": "Stage-1 used 1k episodes in the Cooperative Navigation experiments (paper reports Stage-1 sizes like 1k); CM3 achieved large speedups in Stage-2, converging &gt;15k episodes faster than independent actor-critic baseline in some variants.",
            "key_findings": "Curriculum + function augmentation substantially speeds learning: initializing multi-agent policies with Stage-1 single-agent parameters yields much faster and more stable convergence than training the full multi-agent architecture from scratch. The credit function further improves stability and reduces collisions compared to using only global Q-based advantages.",
            "uuid": "e1062.0",
            "source_info": {
                "paper_title": "CM3: Cooperative Multi-goal Multi-stage Multi-agent Reinforcement Learning",
                "publication_date_yy_mm": "2018-09"
            }
        },
        {
            "name_short": "CM3 (SUMO Double Lane Merge)",
            "name_full": "CM3 agent in SUMO double lane merge (autonomous driving)",
            "brief_description": "A CM3-trained driving agent fleet in the SUMO microscopic traffic simulator that learns cooperative double-lane-merge maneuvers under limited field-of-view and sparse rewards, using curriculum initialization, convolutional processing of local occupancy/grids, and a goal-conditioned credit function for localized credit assignment.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "CM3 driving agents (parameter-shared sedans)",
            "agent_description": "Reinforcement learning agents (actor-critic) trained with CM3: Stage-1 single-agent lane-reaching MDP (policy π^1 and Q^1), then Stage-2 multi-agent augmentation that adds convolutional processing of o_others (discretized occupancy and relative speed grid) and trains goal-conditioned credit functions for action-goal pairs. Training used centralized critics, replay buffers, and discrete 5-action control (no-op, accel, decel, lane-left, lane-right).",
            "agent_type": "simulated agent (virtual autonomous vehicle)",
            "environment_name": "SUMO double lane merge (microscopic traffic simulator)",
            "environment_description": "Straight 200 m road with 4 lanes, sublane discretization (4 sub-lanes per lane, 13x9 occupancy grid per agent with two channels: occupancy and relative speed), other traffic controlled by SUMO; agents have limited field-of-view (15 m forward/backwards) and sparse rewards (collision: -1, timeout: -10, goal reach: +10*(1-Δ) where Δ is sub-lane offset, speed penalty). Agent goals are target lanes at x=190m. Episodes are up to 33 simulation steps (0.2s resolution).",
            "complexity_measure": "State dimensionality (agent absolute positions and speed; o_others: 13x9x2 occupancy tensor), number of interacting vehicles (training used N=2 learning agents but environment includes SUMO-controlled vehicles), discrete action set size (5), sparse and delayed rewards, sublane resolution (continuous lateral discretization), and required multi-step coordinated maneuvers (sequence length up to 33 steps).",
            "complexity_level": "high (sparse rewards, partial observability via local grids, dynamic interactions with other vehicles and need for multi-step coordinated merges)",
            "variation_measure": "Initial and goal lane assignment: with 20% probability goals and initial lanes are uniform random; with 80% a specific difficult pairing is used. Generalization tests included heavy SUMO-controlled traffic and rare initial/goal lane configurations (Table 1: configurations C1–C4).",
            "variation_level": "medium-high (mixture of structured hard configurations and uniform random setups; test includes high-traffic scenarios)",
            "performance_metric": "Average sum of agents' episodic rewards (per-episode returns), success of merge maneuvers, convergence speed (episodes to reach cooperative behavior)",
            "performance_value": "Generalization test (heavy traffic) average summed rewards (100 test episodes): C1: 16.17, C2: 14.93, C3: 15.85, C4: 16.35 (these are CM3 test-set average sum-of-rewards reported in Table 1). In training, CM3 learned cooperative solutions while COMA and IAC failed to perform necessary merges; exact training-episode counts to convergence not numerically specified except relative statements.",
            "complexity_variation_relationship": "Yes — the paper emphasizes that SUMO's dominant challenge is credit assignment (since initial states force agents into cooperative regions) combined with sparse rewards; variation via different initial/goal lane assignments and traffic density increases the difficulty. CM3's curriculum reduces exploration burden and its credit function addresses the credit-assignment challenge, enabling learning despite high complexity and variation; tests show policies trained with few agents generalize to heavier traffic (higher complexity) with good performance.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": "See generalization test: heavy traffic (high complexity + variation) results C1–C4 (sum rewards 16.17 / 14.93 / 15.85 / 16.35 averaged over 100 episodes).",
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Curriculum learning (single-agent Stage-1 lane-reaching MDP) + function augmentation + centralized critics + off-policy replay buffer in some experiments",
            "generalization_tested": true,
            "generalization_results": "Yes — policies trained with N=2 agents on relatively empty roads generalized to scenarios with heavy SUMO-controlled traffic and rare initial/goal lane configurations. CM3 outperformed IAC and COMA on these heavy-traffic tests (Table 1), demonstrating robustness to increased environmental complexity and variation.",
            "sample_efficiency": "Stage-1 episodes unspecified for SUMO in main text (Stage-1 used per-environment settings); paper notes Stage-1 training uses orders of magnitude fewer samples than full multi-agent training. Comparatively, CM3 was sample-efficient enough to outperform baselines in SUMO experiments (learning converged faster than some baselines; exact episodes to convergence not always numeric).",
            "key_findings": "In SUMO, credit assignment is the main bottleneck (cooperation region is unavoidable due to initial states); CM3's credit function and curriculum enable learning of long, sparse sequences of coordinated maneuvers (double merges) that baselines (COMA, IAC) failed to find. Additionally, centralized training with few learning agents can generalize to heavier traffic (higher-complexity deployment) when local interactions dominate.",
            "uuid": "e1062.1",
            "source_info": {
                "paper_title": "CM3: Cooperative Multi-goal Multi-stage Multi-agent Reinforcement Learning",
                "publication_date_yy_mm": "2018-09"
            }
        },
        {
            "name_short": "CM3 (Checkers)",
            "name_full": "CM3 agent in strategic Checkers gridworld",
            "brief_description": "CM3 applied to a strategic 2-agent gridworld where each agent has an individual, conflicting collection goal (one prefers red tiles, the other yellow) and cooperation (path clearing) is required to reach the global optimum; uses curriculum, function augmentation, and goal-conditioned credit functions to learn cooperative yet individually-goal-directed strategies.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "CM3 gridworld agents (Agent A and Agent B)",
            "agent_description": "Actor-critic reinforcement learners trained with CM3; Stage-1 trains single-agent policies alternating roles (Agent A or B) to achieve greedy goal behavior, Stage-2 augments networks for multi-agent observations and trains centralized critics including the credit function Q_n(s,a^m) for localized credit assignment. Observations are partial (5x5 field-of-view tensors plus other-state info), and rewards are sparse and adversarially-signed (+1 / -0.5 depending on agent and tile color).",
            "agent_type": "simulated agent (virtual gridworld agent)",
            "environment_name": "Checkers strategic gridworld (5x13 grid with collectible red/yellow tiles)",
            "environment_description": "Gridworld with a central 3x8 region of red/yellow collectible tiles in a checkered pattern that disappear when collected. Agents have limited 5x5 field-of-view sensory tensors, and different reward preferences (Agent A: +1 red, -0.5 yellow; Agent B: -0.5 red, +1 yellow). Global optimum requires agents to clear paths and coordinate to allow each other to collect preferred tiles; episodes end at 75 steps or when all tiles collected.",
            "complexity_measure": "Combinatorial joint trajectory space (two agents with 5 discrete actions each), partial observability (limited 5x5 field-of-view), goal conflict (opposing preferences) requiring sequential coordination, and delayed/no-instantaneous-reward cooperative actions; state includes reward tensor and agent positions.",
            "complexity_level": "high (combinatorial planning, partial observability, adversarially aligned individual goals requiring cooperation)",
            "variation_measure": "Initial positions fixed in experiments (Agent A at (2,8), Agent B at (4,8)); variation is introduced by alternating which role is trained in Stage-1 and by stochasticity in agent actions during learning. No large procedural variability described for this domain.",
            "variation_level": "low-medium (mostly fixed layout with stochastic behavior during training; role alternation in Stage-1 adds some variation)",
            "performance_metric": "Episodic score (sum of per-agent collected rewards), convergence speed in episodes to reach global optimum",
            "performance_value": "CM3 converged to global optimum with score 24; CM3 (with 5k episodes Stage-1) converged ~10k episodes faster than COMA and QMIX to this optimum (explicit numeric 'score 24' reported).",
            "complexity_variation_relationship": "Yes — the paper highlights that in Checkers exploration of combinatorially large joint trajectories and sparse/cooperative-required actions make learning hard; CM3's curriculum (learning individual goal attainment first) helps discovery of cooperative sequences and the credit function resolves credit assignment for path-clearing actions.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Curriculum learning (Stage-1 single-agent alternating role training) + function augmentation + centralized critics with credit function; off-policy replay optional",
            "generalization_tested": false,
            "generalization_results": "",
            "sample_efficiency": "Stage-1 used 5k episodes for Checkers (paper reports using 5k Stage-1 episodes), and Stage-2 training led CM3 to converge ~10k episodes faster than COMA and QMIX to the global optimum.",
            "key_findings": "CM3 successfully learns cooperative strategies in environments where individual agents cannot maximize reward alone. The curriculum greatly accelerates exploration toward cooperative states and the credit function enables stable credit assignment for actions that enable others' success; baselines (IAC, COMA, QMIX) were slower or failed except COMA solved Checkers but slower.",
            "uuid": "e1062.2",
            "source_info": {
                "paper_title": "CM3: Cooperative Multi-goal Multi-stage Multi-agent Reinforcement Learning",
                "publication_date_yy_mm": "2018-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Multi-agent actor-critic for mixed cooperative-competitive environments",
            "rating": 2
        },
        {
            "paper_title": "Counterfactual multi-agent policy gradients",
            "rating": 2
        },
        {
            "paper_title": "QMIX: Monotonic value function factorisation for deep multi-agent reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Value-decomposition networks for cooperative multi-agent learning based on team reward",
            "rating": 2
        },
        {
            "paper_title": "Microscopic traffic simulation using SUMO",
            "rating": 1
        }
    ],
    "cost": 0.01636575,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>CM3: COOPERATIVE MULTI-GOAL MULTI-STAGE MULTI-AGENT REINFORCEMENT LEARNING</h1>
<p>Jiachen Yang ${ }^{\text {d }}$, Alireza Nakhaei ${ }^{13}$, David Isele ${ }^{2}$, Kikuo Fujimura ${ }^{2}$ \&amp; Hongyuan Zha ${ }^{1}$<br>${ }^{1}$ Georgia Institute of Technology<br>${ }^{2}$ Honda Research Institute<br>${ }^{3}$ Toyota Research Institute</p>
<h4>Abstract</h4>
<p>A variety of cooperative multi-agent control problems require agents to achieve individual goals while contributing to collective success. This multi-goal multiagent setting poses difficulties for recent algorithms, which primarily target settings with a single global reward, due to two new challenges: efficient exploration for learning both individual goal attainment and cooperation for others' success, and credit-assignment for interactions between actions and goals of different agents. To address both challenges, we restructure the problem into a novel two-stage curriculum, in which single-agent goal attainment is learned prior to learning multi-agent cooperation, and we derive a new multi-goal multi-agent policy gradient with a credit function for localized credit assignment. We use a function augmentation scheme to bridge value and policy functions across the curriculum. The complete architecture, called CM3, learns significantly faster than direct adaptations of existing algorithms on three challenging multi-goal multi-agent problems: cooperative navigation in difficult formations, negotiating multi-vehicle lane changes in the SUMO traffic simulator, and strategic cooperation in a Checkers environment.</p>
<h2>1 INTRODUCTION</h2>
<p>Many real-world scenarios that require cooperation among multiple autonomous agents are multi-goal multi-agent control problems: each agent needs to achieve its own individual goal, but the global optimum where all agents succeed is only attained when agents cooperate to allow the success of other agents. In autonomous driving, multiple vehicles must execute cooperative maneuvers when their individual goal locations and nominal trajectories are in conflict (e.g., double lane merges) (Cao et al., 2013). In social dilemmas, mutual cooperation has higher global payoff but agents' individual goals may lead to defection out of fear or greed (Van Lange et al., 2013). Even settings with a global objective that seem unfactorizable can be formulated as multi-goal problems: in Starcraft II micromanagement, a unit that gathers resources must not accidentally jeopardize a teammate's attempt to scout the opponent base (Blizzard Entertainment, 2019); in traffic flow optimization, different intersection controllers may have local throughput goals but must cooperate for high global performance (Zhang et al., 2019). While the framework of multi-agent reinforcement learning (MARL) (Littman, 1994; Stone and Veloso, 2000; Shoham et al., 2003) has been equipped with methods in deep reinforcement learning (RL) (Mnih et al., 2015; Lillicrap et al., 2016) and shown promise on high-dimensional problems with complex agent interactions (Lowe et al., 2017; Mordatch and Abbeel, 2018; Foerster et al., 2018; Lin et al., 2018; Srinivasan et al., 2018), learning multi-agent cooperation in the multi-goal scenario involves significant open challenges.</p>
<p>First, given that exploration is crucial for RL (Thrun, 1992) and even more so in MARL with larger state and joint action spaces, how should agents explore to learn both individual goal attainment and cooperation for others' success? Uniform random exploration is common in deep MARL (Hernandez-Leal et al., 2018) but can be highly inefficient as the value of cooperative actions may be discoverable only in small regions of state space where cooperation is needed. Furthermore, the conceptual difference between attaining one's own goal and cooperating for others' success calls for</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>more modularized and targeted approaches. Second, while there are methods for multi-agent credit assignment when all agents share a single goal (i.e., a global reward) (Chang et al., 2004; Foerster et al., 2018; Nguyen et al., 2018), and while one could treat the cooperative multi-goal scenario as a problem with a single joint goal, this coarse approach makes it extremely difficult to evaluate the impact of an agent's action on another agent's success. Instead, the multi-goal scenario can benefit from fine-grained credit assignment that leverages available structure in action-goal interactions, such as local interactions where only few agents affect another agent's goal attainment at any time.</p>
<p>Given these open challenges, our paper focuses on the cooperative multi-goal multi-agent setting where each agent is assigned a goal ${ }^{1}$ and must learn to cooperate with other agents with possibly different goals. To tackle the problems of efficient exploration and credit assignment in this complex problem setting, we develop CM3, a novel general framework involving three synergistic components:</p>
<ol>
<li>We approach the difficulty of multi-agent exploration from a novel curriculum learning perspective, by first training an actor-critic pair to achieve different goals in an induced single-agent setting (Stage 1), then using them to initialize all agents in the multi-agent environment (Stage 2). The key insight is that agents who can already act toward individual objectives are better prepared for discovery of cooperative solutions with additional exploration once other agents are introduced. In contrast to hierarchical learning where sub-goals are selected sequentially in time (Sutton et al., 1999), all agents act toward their goals simultaneously in Stage 2 of our curriculum.</li>
<li>Observing that a wide array of complex MARL problems permit a decomposition of agents' observations and state vectors into components of self, others, and non-agent specific environment information (Hernandez-Leal et al., 2018), we employ function augmentation to bridge Stages 1-2: we reduce the number of trainable parameters of the actor-critic in Stage 1 by limiting their input space to the part that is sufficient for single-agent training, then augment the architecture in Stage 2 with additional inputs and trainable parameters for learning in the multi-agent environment.</li>
<li>We propose a credit function, which is an action-value function that specifically evaluates actiongoal pairs, for localized credit assignment in multi-goal MARL. We use it to derive a multi-goal multi-agent policy gradient for Stage 2. In synergy with the curriculum, the credit function is constructed via function augmentation from the critic in Stage 1.</li>
</ol>
<p>We evaluate our method on challenging multi-goal multi-agent environments with high-dimensional state spaces: cooperative navigation with difficult formations, double lane merges in the SUMO simulator (Lopez et al., 2018), and strategic teamwork in a Checkers game. CM3 solved all domains significantly faster than IAC and COMA (Tan, 1993; Foerster et al., 2018), and solved four out of five environments significantly faster than QMIX (Rashid et al., 2018). Exhaustive ablation experiments show that the combination of all three components is crucial for CM3's overall high performance.</p>
<h1>2 RELATED WORK</h1>
<p>While early theoretical work analyzed Markov games in discrete state and action spaces (Tan, 1993; Littman, 1994; Hu and Wellman, 2003), recent literature have leveraged techniques from deep RL to develop general algorithms for high dimensional environments with complex agent interactions (Tampuu et al., 2017; Mordatch and Abbeel, 2018; Lowe et al., 2017), which pose difficulty for traditional methods that do not generalize by learning interactions (Bhattacharya et al., 2010).</p>
<p>Cooperative multi-agent learning is important since many real-world problems can be formulated as distributed systems in which decentralized agents must coordinate to achieve shared objectives (Panait and Luke, 2005). The multi-agent credit assignment problem arises when agents share a global reward (Chang et al., 2004). While credit assignment be resolved when independent individual rewards are available (Singh et al., 2019), this may not be suitable for the fully cooperative setting: Austerweil et al. (2016) showed that agents whose rewards depend on the success of other agents can cooperate better than agents who optimize for their own success. In the special case when all agents have a single goal and share a global reward, COMA (Foerster et al., 2018) uses a counterfactual baseline, while Nguyen et al. (2018) employs count-based variance reduction limited to discrete-state</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>environments. However, their centralized critic does not evaluate the specific impact of an agent’s action on another’s success in the general multi-goal setting. When a global objective is the sum of agents’ individual objectives, value-decomposition methods optimize a centralized Q-function while preserving scalable decentralized execution (Sunehag et al., 2018; Rashid et al., 2018; Son et al., 2019), but do not address credit assignment. While MADDPG (Lowe et al., 2017) and M3DDPG (Li et al., 2019) apply to agents with different rewards, they do not address multi-goal cooperation as they do not distinguish between cooperation and competition, despite the fundamental difference.</p>
<p>Multi-goal MARL was considered in Zhang et al. (2018), who analyzed convergence in a special networked setting restricted to fully-decentralized training, while we conduct centralized training with decentralized execution (Oliehoek et al., 2008). In contrast to multi-task MARL, which aims for generalization among non-simultaneous tasks (Omidshafiei et al., 2017), and in contrast to hierarchical methods that sequentially select subtasks (Vezhnevets et al., 2017; Shu and Tian, 2019), our decentralized agents must cooperate concurrently to attain all goals. Methods for optimizing high-level agent-task assignment policies in a hierarchical framework (Carion et al., 2019) are complementary to our work, as we focus on learning low-level cooperation after goals are assigned. Prior application of curriculum learning (Bengio et al., 2009) to MARL include a single cooperative task defined by the number of agents (Gupta et al., 2017) and the probability of agent appearance (Sukhbaatar et al., 2016), without explicit individual goals. Rusu et al. (2016) instantiate new neural network columns for task transfer in single-agent RL. Techniques in transfer learning (Pan and Yang, 2010) are complementary to our novel curriculum approach to MARL.</p>
<h1>3 PRELIMINARIES</h1>
<p>In multi-goal MARL, each agent should achieve a goal drawn from a finite set, cooperate with other agents for collective success, and act independently with limited local observations. We formalize the problem as an episodic multi-goal Markov game, review an actor-critic approach to centralized training of decentralized policies, and summarize counterfactual-based multi-agent credit assignment.</p>
<p>Multi-goal Markov games. A multi-goal Markov game is a tuple $\left\langle\mathcal{S},\left{\mathcal{O}^{n}\right},\left{\mathcal{A}^{n}\right}, P, R, \mathcal{G}, N, \gamma\right\rangle$ with $N$ agents labeled by $n \in[N]$. In each episode, each agent $n$ has one fixed goal $g^{n} \in \mathcal{G}$ that is known only to itself. At time $t$ and global state $s_{t} \in \mathcal{S}$, each agent $n$ receives an observation $o_{t}^{n}:=o^{n}\left(s_{t}\right) \in \mathcal{O}^{n}$ and chooses an action $a_{t}^{n} \in \mathcal{A}^{n}$. The environment moves to $s_{t+1}$ due to joint action $\mathbf{a}<em t="t">{t}:=\left{a</em>}^{1}, \ldots, a_{t}^{N}\right}$, according to transition probability $P\left(s_{t+1} \mid s_{t}, \mathbf{a<em t="t">{t}\right)$. Each agent receives a reward $R</em>}^{n}:=R\left(s_{t}, \mathbf{a<em _boldsymbol_pi="\boldsymbol{\pi">{t}, g^{n}\right)$, and the learning task is to find stochastic decentralized policies $\pi^{n}: \mathcal{O}^{n} \times \mathcal{G} \times \mathcal{A}^{n} \rightarrow[0,1]$, conditioned only on local observations and goals, to maximize $J(\boldsymbol{\pi}):=\mathbb{E}</em>}}\left[\sum_{t=0}^{\infty} \gamma^{t} \sum_{n=1}^{N} R\left(s_{t}, \mathbf{a<em n="1">{t}, g^{n}\right)\right]$, where $\gamma \in(0,1)$ and joint policy $\boldsymbol{\pi}$ factorizes as $\boldsymbol{\pi}(\mathbf{a} \mid s, \mathbf{g}):=\prod</em>\right)$. This model covers a diverse set of cooperation problems in the literature (Hernandez-Leal et al., 2018), without constraining how the attainability of a goal depends on other agents: at a traffic intersection, each vehicle can easily reach its target location if not for the presence of other vehicles; in contrast, agents in a strategic game may not be able to maximize their rewards in the absence of cooperators (Sunehag et al., 2018).}^{N} \pi^{n}\left(a^{n} \mid o^{n}, g^{n}\right)$ due to decentralization. Let $a^{-n}$ and $g^{-n}$ denote all agents’ actions and goals, respectively, except that of agent $n$. Let boldface $\mathbf{a}$ and $\mathbf{g}$ denote the joint action and joint goals, respectively. For brevity, let $\pi\left(a^{n}\right):=\pi^{n}\left(a^{n} \mid o^{n}, g^{n</p>
<p>Centralized learning of decentralized policies. A centralized critic that receives full state-action information can speed up training of decentralized actors that receive only local information (Lowe et al., 2017; Foerster et al., 2018). Directly extending the single-goal case, for each $n \in[1 . . N]$ in a multigoal Markov game, critics are represented by the value function $V_{n}^{\boldsymbol{\pi}}(s):=\mathbb{E}<em t="0">{\boldsymbol{\pi}}\left[\sum</em>}^{\infty} \gamma^{t} R_{t}^{n} \mid s_{0}=s\right]$ and the action-value function $Q_{a}^{\boldsymbol{\pi}}(s, \mathbf{a}):=\mathbb{E<em t="0">{\boldsymbol{\pi}}\left[\sum</em>$.}^{\infty} \gamma^{t} R_{t}^{n} \mid s_{0}=s, \mathbf{a}_{0}=\mathbf{a}\right]$, which evaluate the joint policy $\boldsymbol{\pi}$ against the reward $R^{n}$ for each goal $g^{n</p>
<p>Multi-agent credit assignment. In MARL with a single team objective, COMA addresses credit assignment by using a counterfactual baseline in an advantage function $A^{n}(s, \mathbf{a}):=Q^{\boldsymbol{\pi}}(s, \mathbf{a})-$ $\sum_{\dot{a}^{n}} \pi^{n}\left(\hat{a}^{n} \mid o^{n}\right) Q^{\boldsymbol{\pi}}\left(s,\left(\hat{a}^{n}, a^{-n}\right)\right)$ (Foerster et al., 2018, Lemma 1), which evaluates the contribution of a chosen action $a^{n}$ versus the average of all possible counterfactuals $\hat{a}^{n}$, keeping $a^{-n}$ fixed. The analysis in Wu et al. (2018) for a formally equivalent action-dependent baseline in RL suggests that COMA is a low-variance estimator for single-goal MARL. We derive its variance in Appendix C.1. However, COMA is unsuitable for credit assignment in multi-goal MARL, as it would treat the</p>
<p>collection of goals $\mathbf{g}$ as a global goal and only learn from total reward, making it extremely difficult to disentangle each agent's impact on other agents' goal attainment. Furthermore, a global Q-function does not explicitly capture structure in agents' interactions, such as local interactions involving a limited number of agents. We substantiate these arguments by experimental results in Section 6.</p>
<h1>4 MEthods</h1>
<p>We describe the complete CM3 learning framework as follows. First we define a credit function as a mechanism for credit assignment in multi-goal MARL, then derive a new cooperative multi-goal policy gradient with localized credit assignment. Next we motivate the possibility of significant training speedup via a curriculum for multi-goal MARL. We describe function augmentation as a mechanism for efficiently bridging policy and value functions across the curriculum stages, and finally synthesize all three components into a synergistic learning framework.</p>
<h3>4.1 CREDIT ASSIGNMENT IN MULTI-GOAL MARL</h3>
<p>If all agents take greedy goal-directed actions that are individually optimal in the absence of other agents, the joint action can be sub-optimal (e.g. straight-line trajectory towards target in traffic). Instead rewarding agents for both individual and collective success can avoid such bad local optima. A naïve approach based on previous works (Foerster et al., 2018; Lowe et al., 2017) would evaluate the joint action a via a global Q-function $Q_{n}^{\boldsymbol{\pi}}(s, \mathbf{a})$ for each agent's goal $g^{n}$, but this does not precisely capture each agent's contribution to another agent's attainment of its goal. Instead, we propose an explicit mechanism for credit assignment by learning an additional function $Q_{n}^{\boldsymbol{\pi}}\left(s, a^{m}\right)$ that evaluates pairs of action $a^{m}$ and goal $g^{n}$, for use in a multi-goal actor-critic algorithm. We define this function and show that it satisfies the classical relation needed for sample-based model-free learning.
Definition 1. For $n, m \in[N], s \in \mathcal{S}$, the credit function for goal $g^{n}$ and $a^{m} \in \mathcal{A}^{m}$ by agent $m$ is:</p>
<p>$$
Q_{n}^{\boldsymbol{\pi}}\left(s, a^{m}\right):=\mathbb{E}<em t="0">{\boldsymbol{\pi}}\left[\sum</em>\right]
$$}^{\infty} \gamma^{t} R_{t}^{n} \mid s_{0}=s, a_{0}^{m}=a^{m</p>
<p>Proposition 1. For all $m, n \in[N]$, the credit function (1) satisfies the following relations:</p>
<p>$$
\begin{aligned}
Q_{n}^{\boldsymbol{\pi}}\left(s, a^{m}\right) &amp; =\mathbb{E}<em t="t">{\boldsymbol{\pi}}\left[R</em>\right] \
V_{n}^{\boldsymbol{\pi}}(s) &amp; =\sum_{a^{m}} \pi^{m}\left(a^{m} \mid o^{m}, g^{m}\right) Q_{n}^{\boldsymbol{\pi}}\left(s, a^{m}\right)
\end{aligned}
$$}^{n}+\gamma Q_{n}^{\boldsymbol{\pi}}\left(s_{t+1}, a_{t+1}^{m}\right) \mid s_{t}=s, a_{t}^{m}=a^{m</p>
<p>Derivations are given in Appendix B.1, including the relation between $Q_{n}^{\boldsymbol{\pi}}\left(s, a^{m}\right)$ and $Q_{n}^{\boldsymbol{\pi}}(s, \mathbf{a})$. Equation (2) takes the form of the Bellman expectation equation, which justifies learning the credit function, parameterized by $\theta_{Q_{c}}$, by optimizing the standard loss function in deep RL:</p>
<p>$$
L\left(\theta_{Q_{c}}\right)=\mathbb{E}<em t="t">{\boldsymbol{\pi}}\left[\left(R</em>\right]
$$}^{n}+\gamma Q_{n}^{\boldsymbol{\pi}}\left(s_{t+1}, a_{t+1}^{m} ; \theta_{Q_{c}}\right)-Q_{n}^{\boldsymbol{\pi}}\left(s_{t}, a_{t}^{m} ; \theta_{Q_{c}}\right)\right)^{2</p>
<p>While centralized training means the input space scales linearly with agent count, many practical environments involving only local interactions between agents allows centralized training with few agents while retaining decentralized performance when deployed at scale (evidenced in Appendix E).</p>
<h3>4.2 COOPERATIVE MULTI-GOAL MULTI-AGENT POLICY GRADIENT</h3>
<p>We use the credit function as a critic within a policy gradient for multi-goal MARL. Letting $\theta$ parameterize $\boldsymbol{\pi}$, the overall objective $J(\boldsymbol{\pi})$ is maximized by ascending the following gradient:
Proposition 2. The cooperative multi-goal credit function based MARL policy gradient is</p>
<p>$$
\begin{aligned}
&amp; \nabla_{\theta} J(\boldsymbol{\pi})=\mathbb{E}<em m_="m," n="1">{\boldsymbol{\pi}}\left[\sum</em>)\right] \
&amp; A_{n, m}^{\boldsymbol{\pi}}(s, \mathbf{a}):=Q_{n}^{\boldsymbol{\pi}}(s, \mathbf{a})-\sum_{\hat{a}^{m}} \pi^{m}\left(\hat{a}^{m} \mid o^{m}, g^{m}\right) Q_{n}^{\boldsymbol{\pi}}\left(s, \hat{a}^{m}\right)
\end{aligned}
$$}^{N}\left(\nabla_{\theta} \log \pi^{m}\left(a^{m} \mid o^{m}, g^{m}\right)\right) A_{n, m}^{\boldsymbol{\pi}}(s, \mathbf{a</p>
<p>This is derived in Appendix B.2. For a fixed agent $m$, the inner summation over $n$ considers all agents' goals $g^{n}$ and updates $m$ 's policy based on the advantage of $a^{m}$ over all counterfactual actions $\hat{a}^{m}$, as measured by the credit function for $g^{n}$. The strength of interaction between action-goal pairs is captured by the extent to which $Q_{a}^{\boldsymbol{\pi}}\left(s, \hat{a}^{m}\right)$ varies with $\hat{a}^{m}$, which directly impacts the magnitude of the gradient on agent $m$ 's policy. For example, strong interaction results in non-constant $Q_{a}^{\boldsymbol{\pi}}(s, \cdot)$, which implies larger magnitude of $A_{a, m}^{\boldsymbol{\pi}}$ and larger weight on $\nabla_{\theta} \log \pi\left(a^{m}\right)$. The double summation accounts for first-order interaction between all action-goal pairs, but complexity can be reduced by omitting terms when interactions are known to be sparse, and our empirical runtimes are on par with other methods due to efficient batch computation (Appendix F). As the second term in $A_{a, m}^{\boldsymbol{\pi}}$ is a baseline, the reduction of variance can be analyzed similarly to that for COMA, given in Appendix C.2. While $A_{a, m}^{\boldsymbol{\pi}}=Q_{a}^{\boldsymbol{\pi}}(s, \mathbf{a})-V_{a}^{\boldsymbol{\pi}}(s)$ (due to (3)), ablation results show stability improvement due to the credit function (Section 6). As the credit function takes in a single agent's action, it synergizes with both CM3's curriculum and function augmentation as described in Section 4.5.</p>
<h1>4.3 CURRICULUM FOR MULTI-GOAL MARL</h1>
<p>Multi-goal MARL poses a significant challenge for exploration. Random exploration can be highly inefficient for concurrently learning both individual task completion and cooperative behavior. Agents who cannot make progress toward individual goals may rarely encounter the region of state space where cooperation is needed, rendering any exploration useless for learning cooperative behavior. On the other extreme, exploratory actions taken in situations that require precise coordination can easily lead to penalties that cause agents to avoid the coordination problem and fail to achieve individual goals. Instead, we hypothesize and confirm in experiments that agents who can achieve individual goals in the absence of other agents can more reliably produce state configurations where cooperative solutions are easily discovered with additional exploration in the multi-agent environment ${ }^{2}$.
We propose a MARL curriculum that first solves a single-agent Markov decision process (MDP), as preparation for subsequent exploration speedup. Given a cooperative multi-goal Markov game MG, we induce an MDP M to be the tuple $\left\langle\mathcal{S}^{n}, \mathcal{O}^{n}, A^{n}, P^{n}, R, \gamma\right\rangle$, where an agent $n$ is selected to be the single agent in $\mathbf{M}$. Entities $\mathcal{S}^{n}, P^{n}$, and $R$ are defined by removing all dependencies on agent interactions, so that only components depending on agent $n$ remain. This reduction to $\mathbf{M}$ is possible in almost all fully cooperative multi-agent environments used in a large body of work ${ }^{3}$ (Hernandez-Leal et al., 2018), precisely because they support a variable number of agents, including $N=1$. Important real-world settings that allow this reduction include autonomous driving, multi traffic light control, and warehouse commissioning (removing all but one car/controller/robot, respectively, from the environment). Given a full Markov game implementation, the reduction involves only deletion of components associated with all other agents from state vectors (since an agent is uniquely defined by its attributes), deletion of if-else conditions from the reward function corresponding to agent interactions, and likewise from the transition function if a simulation is used. Appendix G provides practical guidelines for the reduction. Based on $\mathbf{M}$, we define a greedy policy for MG.
Definition 2. A greedy policy $\pi^{n}$ by agent $n$ for cooperative multi-goal MG is defined as the optimal policy $\pi^{*}$ for the induced MDP $\mathbf{M}$ where only agent $n$ is present.</p>
<p>This naturally leads to our proposed curriculum: Stage 1 trains a single agent in $\mathbf{M}$ to achieve a greedy policy, which is then used for initialization in MG in Stage 2. Next we explain in detail how to leverage the structure of decentralized MARL to bridge the two curriculum stages.</p>
<h3>4.4 FUNCTION AUGMENTATION FOR MULTI-GOAL CURRICULUM</h3>
<p>In Markov games with decentralized execution, an agent's observation space decomposes into $\mathcal{O}^{n}=\mathcal{O}<em _others="{others" _text="\text">{\text {self }}^{n} \cup \mathcal{O}</em>}}^{n}$, where $o_{\text {self }}^{n} \in \mathcal{O<em _others="{others" _text="\text">{\text {self }}^{n}$ captures the agent's own properties, which must be observable by the agent for closed-loop control, while $o</em>}}^{n} \in \mathcal{O<em _env="{env" _text="\text">{\text {others }}^{n}$ is the agent's egocentric observation of other agents. In our work, egocentric observations are private and not accessible by other agents (Pynadath and Tambe, 2002). Similarly, global state $s$ decomposes into $s:=\left(s</em>\right)$, where}}, s^{n}, s^{-n</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: In Stage 1, $Q^{1}$ and $\pi^{1}$ learn to achieve multiple goals in a single-agent environment. Between Stage 1 and 2, $\pi$ is constructed from the trained $\pi^{1}$ and a new module $\pi^{2}$ according to ( same construction is done for $Q_{n}(s, \mathbf{a})$ and $Q_{n}\left(s, a^{m}\right)$, not shown). In the multi-agent environment of Stage 2, these augmented functions are instantiated for each of $N$ agents (with parameter-sharing).
$s_{\text {env }}$ is environment information not specific to any agent (e.g., position of a landmark), and $s^{n}$ captures agent $n$ 's information. While this decomposition is implicitly available in a wide range of complex multi-agent environments (Bansal et al., 2018; Foerster et al., 2018; Lowe et al., 2017; Rashid et al., 2018; Liu et al., 2019; Jaderberg et al., 2019), we explicitly use it to implement our curriculum. In Stage 1, as the ability to process $o_{\text {others }}^{e}$ and $s^{-n}$ is unnecessary, we reduce the input space of policy and value functions, thereby reducing the number of trainable parameters and lowering the computation cost. In Stage 2, we restore Stage 1 parameters and activate new modules to process additional inputs $o_{\text {others }}^{e}$ and $s^{-n}$. This augmentation is especially suitable for efficiently learning the credit function (1) and global Q-function, since $Q(s, a)$ can be augmented into both $Q_{n}^{\boldsymbol{\pi}}(s, \mathbf{a})$ and $Q_{n}^{\boldsymbol{\pi}}\left(s, a^{m}\right)$, as explained below.</p>
<h1>4.5 A COMPLETE INSTANTIATION OF CM3</h1>
<p>We combine the preceding components to create CM3, using deep neural networks for function approximation (Figure 1 and Algorithm 1). Without loss of generality, we assume parameter-sharing (Foerster et al., 2018) among homogeneous agents with goals as input (Schaul et al., 2015). The inhomogeneous case can be addressed by $N$ actor-critics. Drawing from multi-task learning (Taylor and Stone, 2009), we sample goal(s) in each episode for the agent(s), to train one model for all goals.
Stage 1. We train an actor $\pi^{1}(a \mid o, g)$ and critic $Q^{1}\left(s^{1}, a, g\right)$ to convergence according to (4) and (5) in the induced MDP with $N=1$ and random goal sampling (see Appendix J). This uses orders of magnitude fewer samples than for the full multi-agent environment—compare Figure 6 with Figure 5.
Stage 2. The Markov game is instantiated with all $N$ agents. We restore the trained $\pi^{1}$ parameters, instantiate a second neural network $\pi^{2}$ for agents to process $o_{\text {others }}^{e}$, and connect the output of $\pi^{2}$ to a selected hidden layer of $\pi^{1}$. Concretely, let $h_{i}^{1} \in \mathbb{R}^{m_{i}}$ denote hidden layer $i \leq L$ with $m_{i}$ units in an $L$ layer network $\pi^{1}$, connected to layer $i-1$ via $h_{i}^{1}=f\left(W_{i}^{1} h_{i-1}^{1}\right)$ with $W_{i}^{1} \in \mathbb{R}^{m_{i} \times m_{i-1}}$ and nonlinear activation $f$. Stage 2 introduces a $K$-layer network $\pi^{2}\left(o_{\text {others }}^{e}\right)$ with outputs $h_{K}^{2} \in \mathbb{R}^{m_{K}}$, chooses a layer ${ }^{4} i^{<em>}$ of $\pi^{1}$, and augments $h_{i^{</em>}}^{1}$ to be $h_{i^{<em>}}^{1}=f\left(W_{i^{</em>}}^{1} h_{i^{<em>}-1}^{1}+W^{1: 2} h_{K}^{2}\right)$ with $W^{1: 2} \in \mathbb{R}^{m_{i^{</em>}} \times m_{K}}$. Being restored from Stage 1, not re-initialized, hidden layers $i&lt;i^{<em>}$ begin with the ability to process $\left(o_{\text {self }}^{e}, g^{n}\right)$, while the new weights in $\pi^{2}$ and $W^{1: 2}$ specifically learn the effect of surrounding agents. Higher layers $i \geq i^{</em>}$ that already take greedy actions to achieve goals in Stage 1 must now do so while cooperating to allow other agents' success. This augmentation scheme is simplest for deep policy and value networks using fully-connected or convolutional layers.
The middle panel of Figure 1 depicts the construction of $\pi$ from $\pi^{1}$ and $\pi^{2}$. The global $Q^{\boldsymbol{\pi}}\left(s, \mathbf{a}, g^{n}\right)$ is constructed from $Q^{1}$ similarly: when the input to $Q^{1}$ is $\left(s_{\text {env }}, s^{n}, a^{n}, g^{n}\right)$, a new module takes input $\left(s^{-n}, a^{-n}\right)$ and connects to a chosen hidden layer of $Q^{1}$. Credit function $Q^{\boldsymbol{\pi}}\left(s, a^{m}, g^{n}\right)$ is</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: Cooperative navigation</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: Agent sedans must perform double lane merge to reach goal lanes. SUMO controls yellow sedans and trucks. Policy generalization was tested on such traffic conditions.
augmented from a copy of $Q^{1}$, such that when $Q^{1}$ inputs are $\left(s_{\text {env }}, s^{n}, a^{m}, g^{n}\right)$, the new module's inputs are $\left(s^{m}, s^{-n}\right) .{ }^{5}$ We train the policy using (5), train the credit function with loss (4), and train the global Q-function with the joint-action analogue of (4).</p>
<h1>5 EXPERIMENTAL SETUP</h1>
<p>We investigated the performance and robustness of CM3 versus existing methods on diverse and challenging multi-goal MARL environments: cooperative navigation in difficult formations, double lane merge in autonomous driving, and strategic cooperation in a Checkers game. We evaluated ablations of CM3 on all domains. We describe key setup here, with full details in Appendices G to J.</p>
<p>Cooperative navigation: We created three variants of the cooperative navigation scenario in Lowe et al. (2017), where $N$ agents cooperate to reach a set of targets. We increased the difficulty by giving each agent only an individual reward based on distance to its designated target, not a global team reward, but initial and target positions require complex cooperative maneuvers to avoid collision penalties (Figure 3). Agents observe relative positions and velocities (details in Appendix G.1). SUMO: Previous work modeled autonomous driving tasks as MDPs in which all other vehicles do not learn to respond to a single learning agent (Isele et al., 2018; Kuefler et al., 2017). However, real-world driving requires cooperation among different drivers' with personal goals. Built in the SUMO traffic simulator with sublane resolution (Lopez et al., 2018), this experiment requires agent vehicles to learn double-merge maneuvers to reach goal lane assignments (Figure 4). Agents have limited field of view and receive sparse rewards (Appendix G.2). Checkers: We implemented a challenging strategic game (Appendix G.3, an extension of Sunehag et al. (2018)), to investigate whether CM3 is beneficial even when an agent cannot maximize its reward in the absence of another agent. In a gridworld with red and yellow squares that disappear when collected (Figure 2), Agent A receives +1 for red and -0.5 for yellow; Agent B receives -0.5 for red and +1 for yellow. Both have a limited $5 x 5$ field of view. The global optimum requires each agent to clear the path for the other.</p>
<p>Algorithm implementations. We describe key points here, leaving complete architecture details and hyperparameter tables to Appendices H and I. CM3: Stage 1 is defined for each environment as follows (Appendix G): in cooperative navigation, a single particle learns to reach any specified landmark; in SUMO, a car learns to reach any specified goal lane; in Checkers, we alternate between training one agent as A and B. Appendix H describes function augmentation in Stage 2 of CM3. COMA (Foerster et al., 2018): the joint goal g and total reward $\sum_{n} R^{n}$ can be used to train COMA's global $Q$ function, which receives input $\left(s, o^{n}, g^{n}, n, a^{-n}, g^{-n}\right)$. Each output node $i$ represents $Q\left(s, a^{n}=i, a^{-n}, \mathrm{~g}\right)$. IAC (Tan, 1993; Foerster et al., 2018): IAC trains each agent's actor and critic independently, using the agent's own observation. The TD error of value function $V\left(o^{n}, g^{n}\right)$ is used in a standard policy gradient (Sutton et al., 2000). QMIX (Rashid et al., 2018): we used the original hypernetwork, giving all goals to the mixer and individual goals to each agent network. We used a manual coordinate descent on exploration and learning rate hyperparameters, including values reported in the original works. We ensured the number of trainable parameters are similar among all methods, up to method-specific architecture requirements for COMA and QMIX.</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: a-e: Comparison against baselines in cooperative navigation (a-c), SUMO (d), Checkers (e). f-j: Comparison against ablations. Average and standard deviation (shaded) of 10 evaluation episodes conducted every 100 training episodes, across 3 independent runs.</p>
<p>Ablations. We conducted ablation experiments in all domains. To discover the speedup from the curriculum with function augmentation, we trained the full Stage 2 architecture of CM3 (labeled as Direct) without first training components $\pi^{1}$ and $Q^{1}$ in an induced MDP. To investigate the benefit of the new credit function and multi-goal policy gradient, we trained an ablation (labeled QV) with advantage function $A_{n}^{\mathbf{a}}(s, \mathbf{a}):=Q_{n}^{\mathbf{a}}(s, \mathbf{a})-V_{n}^{\mathbf{a}}(s)$, where credit assignment between action-goal pairs is lost. QV uses the same $\pi^{1}, Q^{1}$, and function augmentation as CM3.</p>
<h1>6 ReSults and DiSCuSsions</h1>
<p>CM3 finds optimal or near-optimal policies significantly faster than IAC and COMA on all domains, and performs significantly higher than QMIX in four out of five. We report absolute runtime in Appendix F and account for CM3's Stage 1 episodes (Appendix J) when comparing sample efficiency.</p>
<p>Main comparison. Over all cooperative navigation scenarios (Figures 5a to 5c), CM3 (with 1k episodes in Stage 1) converged more than 15 k episodes faster than IAC. IAC reached the same final performance as CM3 because dense individual rewards simplifies the learning problem for IAC's fully decentralized approach, but CM3 benefited significantly from curriculum learning, as evidenced by comparison to "Direct" in Figure 5f. QMIX and COMA settled at suboptimal behavior. Both learn global critics that use all goals as input, in contrast to CM3 and IAC that process each goal separately. This indicates the difficulty of training agents for individual goals under a purely global approach. While COMA was shown to outperform IAC in SC2 micromanagement where IAC must learn from a single team reward (Foerster et al., 2018), our IAC agents have access to individual rewards that resolve the credit assignment issue and improve performance (Singh et al., 2019). In SUMO (Figure 5d), CM3 and QMIX found cooperative solutions with performances within the margin of error, while COMA and IAC could not break out of local optima where vehicles move straight but do not perform merge maneuvers. Since initial states force agents into the region of state space requiring cooperation, credit assignment rather than exploration is the dominant challenge, which CM3 addressed via the credit function, as evidenced in Figure 5i. IAC underperformed because SUMO requires a longer sequence of cooperative actions and gave much sparser rewards than the "Merge" scenario in cooperative navigation. We also show that centralized training of merely two decentralized agents allows them to generalize to settings with much heavier traffic (Appendix E). In Checkers (Figure 5e), CM3 (with 5k episodes in Stage 1) converged 10k episodes faster than COMA and QMIX to the global optimum with score 24. Both exploration of the combinatorially large joint trajectory space and credit assignment for path clearing are challenges that CM3 successfully addressed. COMA only solved Checkers among all domains, possibly because the small bounded environment alleviates COMA's difficulty with individual goals in large state spaces. IAC underperformed all centralized learning methods because cooperative actions that give no instantaneous reward are hard for selfish agents to discover in Checkers. These results demonstrate CM3's ability to attain individual goals and find cooperative solutions in diverse multi-agent systems.</p>
<p>Ablations. The significantly better performance of CM3 versus "Direct" (Figures 5f to 5j) shows that learning individual goal attainment prior to learning multi-agent cooperation, and initializing Stage 2 with Stage 1 parameters, are crucial for improving learning speed and stability. It gives evidence that while global action-value and credit functions may be difficult to train from scratch, function augmentation significantly eases the learning problem. While "QV" initially learns quickly to attain individual goals, it does so at the cost of frequent collisions, higher variance, and inability to maintain a cooperative solution, giving clear evidence for the necessity of the credit function.</p>
<h1>7 CONCLUSION</h1>
<p>We presented CM3, a general framework for cooperative multi-goal MARL. CM3 addresses the need for efficient exploration to learn both individual goal attainment and cooperation, via a two-stage curriculum bridged by function augmentation. It achieves local credit assignment between action and goals using a credit function in a multi-goal policy gradient. In diverse experimental domains, CM3 attains significantly higher performance, faster learning, and overall robustness than existing MARL methods, displaying strengths of both independent learning and centralized credit assignment while avoiding shortcomings of existing methods. Ablations demonstrate each component is crucial to the whole framework. Our results motivate future work on analyzing CM3's theoretical properties and generalizing to inhomogeneous systems or settings without known goal assignments.</p>
<h2>ACKNOWLEDGMENTS</h2>
<p>JY thanks Rakshit Trivedi (Georgia Institute of Technology), Ahmad Beirami (Facebook AI), and Peter Sunehag (DeepMind) for detailed and helpful feedback on this work.</p>
<h2>REFERENCES</h2>
<p>Austerweil, J. L., Brawner, S., Greenwald, A., Hilliard, E., Ho, M., Littman, M. L., MacGlashan, J., and Trimbach, C. (2016). How other-regarding preferences can promote cooperation in non-zero-sum grid games. In Proceedings of the AAAI Symposium on Challenges and Opportunities in Multiagent Learning for the Real World.</p>
<p>Bansal, T., Pachocki, J., Sidor, S., Sutskever, I., and Mordatch, I. (2018). Emergent complexity via multi-agent competition. In International Conference on Learning Representations.</p>
<p>Bengio, Y., Louradour, J., Collobert, R., and Weston, J. (2009). Curriculum learning. In Proceedings of the 26th annual international conference on machine learning, pages 41-48. ACM.</p>
<p>Bhattacharya, S., Likhachev, M., and Kumar, V. (2010). Multi-agent path planning with multiple tasks and distance constraints. In Robotics and Automation (ICRA), 2010 IEEE International Conference on, pages 953-959. IEEE.</p>
<p>Blizzard Entertainment (2019). Starcraft ii. https://starcraft2.com/en-us/, Last accessed on 2019-09-07.</p>
<p>Cao, Y., Yu, W., Ren, W., and Chen, G. (2013). An overview of recent progress in the study of distributed multi-agent coordination. IEEE Transactions on Industrial informatics, 9(1), 427-438.</p>
<p>Carion, N., Synnaeve, G., Lazaric, A., and Usunier, N. (2019). A structured prediction approach for generalization in cooperative multi-agent reinforcement learning. In Advances in neural information processing systems.</p>
<p>Chang, Y.-H., Ho, T., and Kaelbling, L. P. (2004). All learning is local: Multi-agent learning in global reward games. In Advances in neural information processing systems, pages 807-814.</p>
<p>Foerster, J. N., Farquhar, G., Afouras, T., Nardelli, N., and Whiteson, S. (2018). Counterfactual multi-agent policy gradients. In Thirty-Second AAAI Conference on Artificial Intelligence.</p>
<p>Gupta, J. K., Egorov, M., and Kochenderfer, M. (2017). Cooperative multi-agent control using deep reinforcement learning. In International Conference on Autonomous Agents and Multiagent Systems, pages 66-83. Springer.</p>
<p>Hernandez-Leal, P., Kartal, B., and Taylor, M. E. (2018). Is multiagent deep reinforcement learning the answer or the question? a brief survey. arXiv preprint arXiv:1810.05587.</p>
<p>Hu, J. and Wellman, M. P. (2003). Nash q-learning for general-sum stochastic games. Journal of machine learning research, 4(Nov), 1039-1069.</p>
<p>Isele, D., Rahimi, R., Cosgun, A., Subramanian, K., and Fujimura, K. (2018). Navigating occluded intersections with autonomous vehicles using deep reinforcement learning. In 2018 IEEE International Conference on Robotics and Automation (ICRA), pages 2034-2039. IEEE.</p>
<p>Jaderberg, M., Czarnecki, W. M., Dunning, I., Marris, L., Lever, G., Castaneda, A. G., Beattie, C., Rabinowitz, N. C., Morcos, A. S., Ruderman, A., et al. (2019). Human-level performance in 3d multiplayer games with population-based reinforcement learning. Science, 364(6443), 859.</p>
<p>Kuefler, A., Morton, J., Wheeler, T., and Kochenderfer, M. (2017). Imitating driver behavior with generative adversarial networks. In 2017 IEEE Intelligent Vehicles Symposium (IV), pages 204-211. IEEE.</p>
<p>Li, S., Wu, Y., Cui, X., Dong, H., Fang, F., and Russell, S. (2019). Robust multi-agent reinforcement learning via minimax deep deterministic policy gradient. In AAAI Conference on Artificial Intelligence (AAAI).</p>
<p>Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D., and Wierstra, D. (2016). Continuous control with deep reinforcement learning. In International Conference on Learning Representations.</p>
<p>Lin, K., Zhao, R., Xu, Z., and Zhou, J. (2018). Efficient large-scale fleet management via multi-agent deep reinforcement learning. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery \&amp; Data Mining, pages 1774-1783. ACM.</p>
<p>Littman, M. L. (1994). Markov games as a framework for multi-agent reinforcement learning. In Machine Learning Proceedings 1994, pages 157-163. Elsevier.</p>
<p>Liu, S., Lever, G., Merel, J., Tunyasuvunakool, S., Heess, N., and Graepel, T. (2019). Emergent coordination through competition. In International Conference on Learning Representations.</p>
<p>Lopez, P. A., Behrisch, M., Bieker-Walz, L., Erdmann, J., Flötteröd, Y.-P., Hilbrich, R., Lücken, L., Rummel, J., Wagner, P., and Wießner, E. (2018). Microscopic traffic simulation using SUMO. In The 21st IEEE International Conference on Intelligent Transportation Systems. IEEE.</p>
<p>Lowe, R., Wu, Y., Tamar, A., Harb, J., Abbeel, O. P., and Mordatch, I. (2017). Multi-agent actor-critic for mixed cooperative-competitive environments. In Advances in Neural Information Processing Systems, pages 6382-6393.</p>
<p>Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G., et al. (2015). Human-level control through deep reinforcement learning. Nature, 518(7540), 529.</p>
<p>Mordatch, I. and Abbeel, P. (2018). Emergence of grounded compositional language in multi-agent populations. In Thirty-Second AAAI Conference on Artificial Intelligence.</p>
<p>Nguyen, D. T., Kumar, A., and Lau, H. C. (2018). Credit assignment for collective multiagent rl with global rewards. In Advances in Neural Information Processing Systems, pages 8112-8123.</p>
<p>Oliehoek, F. A., Spaan, M. T., and Vlassis, N. (2008). Optimal and approximate q-value functions for decentralized pomdps. Journal of Artificial Intelligence Research, 32, 289-353.</p>
<p>Omidshafiei, S., Pazis, J., Amato, C., How, J. P., and Vian, J. (2017). Deep decentralized multi-task multi-agent reinforcement learning under partial observability. In International Conference on Machine Learning, pages 2681-2690.</p>
<p>Pan, S. J. and Yang, Q. (2010). A survey on transfer learning. IEEE Transactions on knowledge and data engineering, 22(10), 1345-1359.</p>
<p>Panait, L. and Luke, S. (2005). Cooperative multi-agent learning: The state of the art. Autonomous agents and multi-agent systems, 11(3), 387-434.</p>
<p>Pynadath, D. V. and Tambe, M. (2002). The communicative multiagent team decision problem: Analyzing teamwork theories and models. Journal of artificial intelligence research, 16, 389-423.</p>
<p>Rashid, T., Samvelyan, M., Schroeder, C., Farquhar, G., Foerster, J., and Whiteson, S. (2018). QMIX: Monotonic value function factorisation for deep multi-agent reinforcement learning. In Proceedings of the 35th International Conference on Machine Learning, pages 4295-4304.</p>
<p>Rusu, A. A., Rabinowitz, N. C., Desjardins, G., Soyer, H., Kirkpatrick, J., Kavukcuoglu, K., Pascanu, R., and Hadsell, R. (2016). Progressive neural networks. arXiv preprint arXiv:1606.04671.</p>
<p>Schaul, T., Horgan, D., Gregor, K., and Silver, D. (2015). Universal value function approximators. In International Conference on Machine Learning, pages 1312-1320.</p>
<p>Shoham, Y., Powers, R., and Grenager, T. (2003). Multi-agent reinforcement learning: a critical survey. Technical report, Technical report, Stanford University.</p>
<p>Shu, T. and Tian, Y. (2019). M3rl: Mind-aware multi-agent management reinforcement learning. In International Conference on Learning Representations.</p>
<p>Silver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., and Riedmiller, M. (2014). Deterministic policy gradient algorithms. In ICML.</p>
<p>Singh, A., Jain, T., and Sukhbaatar, S. (2019). Learning when to communicate at scale in multiagent cooperative and competitive tasks. In International Conference on Learning Representations.</p>
<p>Son, K., Kim, D., Kang, W. J., Hostallero, D., and Yi, Y. (2019). Qtran: Learning to factorize with transformation for cooperative multi-agent reinforcement learning. In International Conference on Machine Learning.</p>
<p>Srinivasan, S., Lanctot, M., Zambaldi, V., Pérolat, J., Tuyls, K., Munos, R., and Bowling, M. (2018). Actor-critic policy optimization in partially observable multiagent environments. In Advances in Neural Information Processing Systems, pages 3426-3439.</p>
<p>Stone, P. and Veloso, M. (2000). Multiagent systems: A survey from a machine learning perspective. Autonomous Robots, 8(3), 345-383.</p>
<p>Sukhbaatar, S., Fergus, R., et al. (2016). Learning multiagent communication with backpropagation. In Advances in Neural Information Processing Systems, pages 2244-2252.</p>
<p>Sunehag, P., Lever, G., Gruslys, A., Czarnecki, W. M., Zambaldi, V., Jaderberg, M., Lanctot, M., Sonnerat, N., Leibo, J. Z., Tuyls, K., et al. (2018). Value-decomposition networks for cooperative multi-agent learning based on team reward. In Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems, pages 2085-2087. International Foundation for Autonomous Agents and Multiagent Systems.</p>
<p>Sutton, R. S., Precup, D., and Singh, S. (1999). Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning. Artificial intelligence, 112(1-2), 181-211.</p>
<p>Sutton, R. S., McAllester, D. A., Singh, S. P., and Mansour, Y. (2000). Policy gradient methods for reinforcement learning with function approximation. In Advances in neural information processing systems, pages 1057-1063.</p>
<p>Tampuu, A., Matiisen, T., Kodelja, D., Kuzovkin, I., Korjus, K., Aru, J., Aru, J., and Vicente, R. (2017). Multiagent cooperation and competition with deep reinforcement learning. PloS one, 12(4), e0172395.</p>
<p>Tan, M. (1993). Multi-agent reinforcement learning: Independent vs. cooperative agents. In Proceedings of the tenth international conference on machine learning, pages 330-337.</p>
<p>Taylor, M. E. and Stone, P. (2009). Transfer learning for reinforcement learning domains: A survey. Journal of Machine Learning Research, 10(Jul), 1633-1685.</p>
<p>Thrun, S. B. (1992). Efficient exploration in reinforcement learning. Technical report, Carnegie Mellon University, Pittsburgh, PA, USA.</p>
<p>Van Lange, P. A., Joireman, J., Parks, C. D., and Van Dijk, E. (2013). The psychology of social dilemmas: A review. Organizational Behavior and Human Decision Processes, 120(2), 125-141.</p>
<p>Vezhnevets, A. S., Osindero, S., Schaul, T., Heess, N., Jaderberg, M., Silver, D., and Kavukcuoglu, K. (2017). Feudal networks for hierarchical reinforcement learning. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 3540-3549. JMLR. org.</p>
<p>Wu, C., Rajeswaran, A., Duan, Y., Kumar, V., Bayen, A. M., Kakade, S., Mordatch, I., and Abbeel, P. (2018). Variance reduction for policy gradient with action-dependent factorized baselines. In International Conference on Learning Representations.</p>
<p>Zhang, K., Yang, Z., Liu, H., Zhang, T., and Basar, T. (2018). Fully decentralized multi-agent reinforcement learning with networked agents. In Proceedings of the 35th International Conference on Machine Learning, pages 5872-5881.</p>
<p>Zhang, Z., Yang, J., and Zha, H. (2019). Integrating independent and centralized multi-agent reinforcement learning for traffic signal network optimization. arXiv preprint arXiv:1909.10651.</p>
<h1>A Algorithm</h1>
<div class="codehilite"><pre><span></span><code><span class="nx">Algorithm</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="nx">Cooperative</span><span class="w"> </span><span class="nx">multi</span><span class="o">-</span><span class="nx">goal</span><span class="w"> </span><span class="nx">multi</span><span class="o">-</span><span class="nx">stage</span><span class="w"> </span><span class="nx">multi</span><span class="o">-</span><span class="nx">agent</span><span class="w"> </span><span class="nx">reinforcement</span><span class="w"> </span><span class="nx">learning</span><span class="w"> </span><span class="p">(</span><span class="nx">CM3</span><span class="p">)</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="nx">curriculum</span><span class="w"> </span><span class="nx">stage</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">c</span><span class="p">=</span><span class="mi">1</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="nx">do</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">c</span><span class="p">=</span><span class="mi">1</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">then</span>
<span class="w">            </span><span class="nx">Set</span><span class="w"> </span><span class="nx">number</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">agents</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">N</span><span class="p">=</span><span class="mi">1</span><span class="err">\</span><span class="p">)</span>
<span class="w">            </span><span class="nx">Initialize</span><span class="w"> </span><span class="nx">Stage</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="nx">main</span><span class="w"> </span><span class="nx">networks</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">Q_</span><span class="p">{</span><span class="nx">g</span><span class="p">}:=</span><span class="nx">Q</span><span class="p">=</span><span class="nx">Q</span><span class="o">^</span><span class="p">{</span><span class="mi">1</span><span class="p">},</span><span class="w"> </span><span class="err">\</span><span class="nx">pi</span><span class="p">:=</span><span class="err">\</span><span class="nx">pi</span><span class="o">^</span><span class="p">{</span><span class="mi">1</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">with</span><span class="w"> </span><span class="nx">parameters</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">theta_</span><span class="p">{</span><span class="nx">Q</span><span class="o">^</span><span class="p">{</span><span class="mi">1</span><span class="p">}},</span><span class="w"> </span><span class="err">\</span><span class="nx">theta_</span><span class="p">{</span><span class="err">\</span><span class="nx">pi</span><span class="o">^</span><span class="p">{</span><span class="mi">1</span><span class="p">}}</span><span class="err">\</span><span class="p">)</span>
<span class="w">            </span><span class="nx">Initialize</span><span class="w"> </span><span class="nx">target</span><span class="w"> </span><span class="nx">networks</span><span class="w"> </span><span class="nx">with</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">theta_</span><span class="p">{</span><span class="err">\</span><span class="nx">pi</span><span class="o">^</span><span class="p">{</span><span class="mi">1</span><span class="p">}}</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">prime</span><span class="p">},</span><span class="w"> </span><span class="err">\</span><span class="nx">theta_</span><span class="p">{</span><span class="nx">Q</span><span class="o">^</span><span class="p">{</span><span class="mi">1</span><span class="p">}}</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">prime</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="k">else</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">c</span><span class="p">=</span><span class="mi">2</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">then</span>
<span class="w">            </span><span class="nx">Instantiate</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">N</span><span class="p">&gt;</span><span class="mi">1</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">agents</span>
<span class="w">                </span><span class="nx">Construct</span><span class="w"> </span><span class="nx">global</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">Q_</span><span class="p">{</span><span class="nx">g</span><span class="p">}:=</span><span class="nx">Q_</span><span class="p">{</span><span class="nx">a</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">pi</span><span class="p">}(</span><span class="nx">s</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="nx">mathbf</span><span class="p">{</span><span class="nx">a</span><span class="p">})=</span><span class="err">\</span><span class="nx">left</span><span class="err">\</span><span class="p">{</span><span class="nx">Q</span><span class="o">^</span><span class="p">{</span><span class="mi">1</span><span class="p">},</span><span class="w"> </span><span class="nx">Q_</span><span class="p">{</span><span class="nx">g</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="mi">2</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="err">\</span><span class="p">}</span><span class="err">\</span><span class="p">),</span><span class="w"> </span><span class="nx">credit</span><span class="w"> </span><span class="nx">function</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">Q_</span><span class="p">{</span><span class="nx">c</span><span class="p">}:=</span><span class="nx">Q_</span><span class="p">{</span><span class="nx">n</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">pi</span><span class="p">}</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">s</span><span class="p">,</span><span class="w"> </span><span class="nx">a</span><span class="o">^</span><span class="p">{</span><span class="nx">m</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)=</span><span class="err">\</span><span class="p">)</span>
<span class="w">            </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">left</span><span class="err">\</span><span class="p">{</span><span class="nx">Q</span><span class="o">^</span><span class="p">{</span><span class="mi">1</span><span class="p">},</span><span class="w"> </span><span class="nx">Q_</span><span class="p">{</span><span class="nx">c</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="mi">2</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="err">\</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">pi</span><span class="p">:=</span><span class="err">\</span><span class="nx">left</span><span class="err">\</span><span class="p">{</span><span class="err">\</span><span class="nx">pi</span><span class="o">^</span><span class="p">{</span><span class="mi">1</span><span class="p">},</span><span class="w"> </span><span class="err">\</span><span class="nx">pi</span><span class="o">^</span><span class="p">{</span><span class="mi">2</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="err">\</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">using</span><span class="w"> </span><span class="nx">function</span><span class="w"> </span><span class="nx">augmentation</span><span class="w"> </span><span class="nx">with</span><span class="w"> </span><span class="nx">parameters</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">theta_</span><span class="p">{</span><span class="nx">Q_</span><span class="p">{</span><span class="nx">g</span><span class="p">}},</span><span class="w"> </span><span class="err">\</span><span class="nx">theta_</span><span class="p">{</span><span class="nx">Q_</span><span class="p">{</span><span class="nx">c</span><span class="p">}},</span><span class="w"> </span><span class="err">\</span><span class="nx">theta_</span><span class="p">{</span><span class="err">\</span><span class="nx">pi</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
<span class="w">                </span><span class="nx">Initialize</span><span class="w"> </span><span class="nx">target</span><span class="w"> </span><span class="nx">networks</span><span class="w"> </span><span class="nx">with</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">theta_</span><span class="p">{</span><span class="nx">Q_</span><span class="p">{</span><span class="nx">g</span><span class="p">}}</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">prime</span><span class="p">},</span><span class="w"> </span><span class="err">\</span><span class="nx">theta_</span><span class="p">{</span><span class="nx">Q_</span><span class="p">{</span><span class="nx">c</span><span class="p">}}</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">prime</span><span class="p">},</span><span class="w"> </span><span class="err">\</span><span class="nx">theta_</span><span class="p">{</span><span class="err">\</span><span class="nx">pi</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">prime</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
<span class="w">                </span><span class="nx">Restore</span><span class="w"> </span><span class="nx">values</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">trained</span><span class="w"> </span><span class="nx">parameters</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">theta_</span><span class="p">{</span><span class="nx">Q</span><span class="o">^</span><span class="p">{</span><span class="mi">1</span><span class="p">}},</span><span class="w"> </span><span class="err">\</span><span class="nx">theta_</span><span class="p">{</span><span class="err">\</span><span class="nx">pi</span><span class="o">^</span><span class="p">{</span><span class="mi">1</span><span class="p">}}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">into</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">respective</span><span class="w"> </span><span class="nx">subsets</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">theta_</span><span class="p">{</span><span class="nx">Q_</span><span class="p">{</span><span class="nx">g</span><span class="p">}},</span><span class="w"> </span><span class="err">\</span><span class="nx">theta_</span><span class="p">{</span><span class="nx">Q_</span><span class="p">{</span><span class="nx">c</span><span class="p">}},</span><span class="w"> </span><span class="err">\</span><span class="nx">theta_</span><span class="p">{</span><span class="err">\</span><span class="nx">pi</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
<span class="w">            </span><span class="nx">end</span><span class="w"> </span><span class="k">if</span>
<span class="w">            </span><span class="nx">Set</span><span class="w"> </span><span class="nx">all</span><span class="w"> </span><span class="nx">target</span><span class="w"> </span><span class="nx">network</span><span class="w"> </span><span class="nx">weights</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">equal</span><span class="w"> </span><span class="nx">main</span><span class="w"> </span><span class="nx">networks</span><span class="w"> </span><span class="nx">weights</span>
<span class="w">            </span><span class="nx">Initialize</span><span class="w"> </span><span class="nx">exploration</span><span class="w"> </span><span class="nx">parameter</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">epsilon</span><span class="p">=</span><span class="err">\</span><span class="nx">epsilon_</span><span class="p">{</span><span class="err">\</span><span class="nx">text</span><span class="w"> </span><span class="p">{</span><span class="nx">start</span><span class="w"> </span><span class="p">}}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">empty</span><span class="w"> </span><span class="nx">replay</span><span class="w"> </span><span class="nx">buffer</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">B</span><span class="err">\</span><span class="p">)</span>
<span class="w">            </span><span class="k">for</span><span class="w"> </span><span class="nx">each</span><span class="w"> </span><span class="nx">training</span><span class="w"> </span><span class="nx">episode</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">e</span><span class="p">=</span><span class="mi">1</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">E</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">do</span>
<span class="w">                </span><span class="nx">Assign</span><span class="w"> </span><span class="nx">goal</span><span class="p">(</span><span class="nx">s</span><span class="p">)</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">g_</span><span class="p">{</span><span class="nx">e</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="nx">n</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">agent</span><span class="p">(</span><span class="nx">s</span><span class="p">)</span><span class="w"> </span><span class="nx">according</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">given</span><span class="w"> </span><span class="nx">distribution</span>
<span class="w">                </span><span class="nx">Get</span><span class="w"> </span><span class="nx">initial</span><span class="w"> </span><span class="nx">state</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">s_</span><span class="p">{</span><span class="mi">1</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">observation</span><span class="p">(</span><span class="nx">s</span><span class="p">)</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathbf</span><span class="p">{</span><span class="nx">o</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="mi">1</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
<span class="w">                </span><span class="k">for</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">t</span><span class="p">=</span><span class="mi">1</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">T</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">do</span><span class="w"> </span><span class="c1">// execute policies in environment</span>
<span class="w">                    </span><span class="nx">Sample</span><span class="w"> </span><span class="nx">action</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">a_</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="nx">n</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">sim</span><span class="w"> </span><span class="err">\</span><span class="nx">pi</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">a_</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="nx">n</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">mid</span><span class="w"> </span><span class="nx">o_</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="nx">n</span><span class="p">}</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="err">\</span><span class="nx">theta_</span><span class="p">{</span><span class="err">\</span><span class="nx">pi</span><span class="p">},</span><span class="w"> </span><span class="err">\</span><span class="nx">epsilon</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nx">each</span><span class="w"> </span><span class="nx">agent</span><span class="p">.</span>
<span class="w">                    </span><span class="nx">Execute</span><span class="w"> </span><span class="nx">action</span><span class="p">(</span><span class="nx">s</span><span class="p">)</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathbf</span><span class="p">{</span><span class="nx">a</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="err">\</span><span class="p">),</span><span class="w"> </span><span class="nx">receive</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">left</span><span class="err">\</span><span class="p">{</span><span class="nx">r_</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="nx">n</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="err">\</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="nx">n</span><span class="p">},</span><span class="w"> </span><span class="nx">s_</span><span class="p">{</span><span class="nx">t</span><span class="o">+</span><span class="mi">1</span><span class="p">}</span><span class="err">\</span><span class="p">),</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathbf</span><span class="p">{</span><span class="nx">o</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="nx">t</span><span class="o">+</span><span class="mi">1</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
<span class="w">                    </span><span class="nx">Store</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">s_</span><span class="p">{</span><span class="nx">t</span><span class="p">},</span><span class="w"> </span><span class="err">\</span><span class="nx">mathbf</span><span class="p">{</span><span class="nx">o</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="nx">t</span><span class="p">},</span><span class="w"> </span><span class="err">\</span><span class="nx">mathbf</span><span class="p">{</span><span class="nx">g</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="nx">e</span><span class="p">},</span><span class="w"> </span><span class="err">\</span><span class="nx">mathbf</span><span class="p">{</span><span class="nx">a</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="nx">t</span><span class="p">},</span><span class="err">\</span><span class="nx">left</span><span class="err">\</span><span class="p">{</span><span class="nx">r_</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="nx">n</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="err">\</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="nx">n</span><span class="p">},</span><span class="w"> </span><span class="nx">R_</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="mi">2</span><span class="p">},</span><span class="w"> </span><span class="nx">s_</span><span class="p">{</span><span class="nx">t</span><span class="o">+</span><span class="mi">1</span><span class="p">},</span><span class="w"> </span><span class="err">\</span><span class="nx">mathbf</span><span class="p">{</span><span class="nx">o</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="nx">t</span><span class="o">+</span><span class="mi">1</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">into</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">B</span><span class="err">\</span><span class="p">)</span>
<span class="w">                    </span><span class="err">\</span><span class="p">(</span><span class="nx">s_</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="w"> </span><span class="nx">s_</span><span class="p">{</span><span class="nx">t</span><span class="o">+</span><span class="mi">1</span><span class="p">},</span><span class="w"> </span><span class="err">\</span><span class="nx">mathbf</span><span class="p">{</span><span class="nx">o</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="nx">mathbf</span><span class="p">{</span><span class="nx">o</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="nx">t</span><span class="o">+</span><span class="mi">1</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
<span class="w">            </span><span class="nx">end</span><span class="w"> </span><span class="k">for</span>
<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="nx">e</span><span class="w"> </span><span class="nx">mod</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathrm</span><span class="p">{</span><span class="nx">E</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="err">\</span><span class="nx">text</span><span class="w"> </span><span class="p">{</span><span class="nx">train</span><span class="w"> </span><span class="p">}}=</span><span class="mi">0</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">then</span>
<span class="w">                </span><span class="k">for</span><span class="w"> </span><span class="nx">epochs</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="mi">1</span><span class="w"> </span><span class="err">\</span><span class="nx">ldots</span><span class="w"> </span><span class="nx">K</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">do</span><span class="w"> </span><span class="c1">// conduct training</span>
<span class="w">                    </span><span class="nx">Sample</span><span class="w"> </span><span class="nx">minibatch</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">S</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">transitions</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">s_</span><span class="p">{</span><span class="nx">i</span><span class="p">},</span><span class="w"> </span><span class="err">\</span><span class="nx">mathbf</span><span class="p">{</span><span class="nx">o</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="nx">i</span><span class="p">},</span><span class="w"> </span><span class="err">\</span><span class="nx">mathbf</span><span class="p">{</span><span class="nx">g</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="nx">i</span><span class="p">},</span><span class="w"> </span><span class="err">\</span><span class="nx">mathbf</span><span class="p">{</span><span class="nx">a</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="nx">i</span><span class="p">},</span><span class="err">\</span><span class="nx">left</span><span class="err">\</span><span class="p">{</span><span class="nx">r_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="nx">n</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="err">\</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="nx">n</span><span class="p">},</span><span class="w"> </span><span class="nx">s_</span><span class="p">{</span><span class="nx">i</span><span class="o">+</span><span class="mi">1</span><span class="p">},</span><span class="w"> </span><span class="err">\</span><span class="nx">mathbf</span><span class="p">{</span><span class="nx">o</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="nx">i</span><span class="o">+</span><span class="mi">1</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">from</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">B</span><span class="err">\</span><span class="p">)</span>
<span class="w">                    </span><span class="nx">Compute</span><span class="w"> </span><span class="nx">global</span><span class="w"> </span><span class="nx">target</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nx">all</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">n</span><span class="p">:</span><span class="w"> </span><span class="nx">x_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="nx">n</span><span class="p">}=</span><span class="nx">r_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="nx">n</span><span class="p">}</span><span class="o">+</span><span class="err">\</span><span class="nx">gamma</span><span class="w"> </span><span class="nx">Q</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">s_</span><span class="p">{</span><span class="nx">i</span><span class="o">+</span><span class="mi">1</span><span class="p">},</span><span class="w"> </span><span class="err">\</span><span class="nx">mathbf</span><span class="p">{</span><span class="nx">a</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="nx">i</span><span class="o">+</span><span class="mi">1</span><span class="p">},</span><span class="w"> </span><span class="nx">g_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="nx">n</span><span class="p">}</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="err">\</span><span class="nx">theta_</span><span class="p">{</span><span class="nx">Q_</span><span class="p">{</span><span class="nx">g</span><span class="p">}}</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">prime</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="w"> </span><span class="err">\</span><span class="nx">mid</span><span class="w"> </span><span class="err">\</span><span class="nx">mathbf</span><span class="p">{</span><span class="nx">a</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="nx">i</span><span class="o">+</span><span class="mi">1</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">sim</span><span class="w"> </span><span class="err">\</span><span class="nx">pi</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">prime</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
<span class="w">                    </span><span class="nx">Gradient</span><span class="w"> </span><span class="nx">descent</span><span class="w"> </span><span class="nx">on</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">L</span><span class="p">}</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="err">\</span><span class="nx">theta_</span><span class="p">{</span><span class="nx">Q_</span><span class="p">{</span><span class="nx">g</span><span class="p">}}</span><span class="err">\</span><span class="nx">right</span><span class="p">)=</span><span class="err">\</span><span class="nx">frac</span><span class="p">{</span><span class="mi">1</span><span class="p">}{</span><span class="nx">S</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">sum_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">frac</span><span class="p">{</span><span class="mi">1</span><span class="p">}{</span><span class="nx">N</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">sum_</span><span class="p">{</span><span class="nx">n</span><span class="p">=</span><span class="mi">1</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="nx">N</span><span class="p">}</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">x_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="nx">n</span><span class="p">}</span><span class="o">-</span><span class="nx">Q</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">s_</span><span class="p">{</span><span class="nx">i</span><span class="p">},</span><span class="w"> </span><span class="err">\</span><span class="nx">mathbf</span><span class="p">{</span><span class="nx">a</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="nx">i</span><span class="p">},</span><span class="w"> </span><span class="nx">g_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="nx">n</span><span class="p">}</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="err">\</span><span class="nx">theta_</span><span class="p">{</span><span class="nx">Q_</span><span class="p">{</span><span class="nx">g</span><span class="p">}}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="o">^</span><span class="p">{</span><span class="mi">2</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
<span class="w">                    </span><span class="k">if</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathrm</span><span class="p">{</span><span class="nx">c</span><span class="p">}=</span><span class="mi">1</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">then</span>
<span class="w">                        </span><span class="err">\</span><span class="p">(</span><span class="nx">A</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">pi</span><span class="p">}</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">s_</span><span class="p">{</span><span class="nx">i</span><span class="p">},</span><span class="w"> </span><span class="nx">a_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)=</span><span class="nx">Q</span><span class="o">^</span><span class="p">{</span><span class="mi">1</span><span class="p">}</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">s_</span><span class="p">{</span><span class="nx">i</span><span class="p">},</span><span class="w"> </span><span class="nx">a_</span><span class="p">{</span><span class="nx">i</span><span class="p">},</span><span class="w"> </span><span class="nx">g_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="err">\</span><span class="nx">theta_</span><span class="p">{</span><span class="nx">Q</span><span class="o">^</span><span class="p">{</span><span class="mi">1</span><span class="p">}}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="o">-</span><span class="err">\</span><span class="nx">sum_</span><span class="p">{</span><span class="err">\</span><span class="nx">hat</span><span class="p">{</span><span class="nx">a</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="nx">i</span><span class="p">}}</span><span class="w"> </span><span class="err">\</span><span class="nx">pi</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="err">\</span><span class="nx">hat</span><span class="p">{</span><span class="nx">a</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">mid</span><span class="w"> </span><span class="nx">o_</span><span class="p">{</span><span class="nx">i</span><span class="p">},</span><span class="w"> </span><span class="nx">g_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="w"> </span><span class="nx">Q</span><span class="o">^</span><span class="p">{</span><span class="mi">1</span><span class="p">}</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">s_</span><span class="p">{</span><span class="nx">i</span><span class="p">},</span><span class="w"> </span><span class="err">\</span><span class="nx">hat</span><span class="p">{</span><span class="nx">a</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="nx">i</span><span class="p">},</span><span class="w"> </span><span class="nx">g_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="err">\</span><span class="nx">theta_</span><span class="p">{</span><span class="nx">Q</span><span class="o">^</span><span class="p">{</span><span class="mi">1</span><span class="p">}}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">                    </span><span class="k">else</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathrm</span><span class="p">{</span><span class="nx">c</span><span class="p">}=</span><span class="mi">2</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">then</span>
<span class="w">                        </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="k">forall</span><span class="w"> </span><span class="nx">m</span><span class="p">,</span><span class="w"> </span><span class="nx">n</span><span class="w"> </span><span class="err">\</span><span class="k">in</span><span class="p">[</span><span class="mi">1</span><span class="w"> </span><span class="p">.</span><span class="w"> </span><span class="p">.</span><span class="w"> </span><span class="nx">N</span><span class="p">]</span><span class="err">\</span><span class="p">),</span><span class="w"> </span><span class="nx">compute</span><span class="w"> </span><span class="nx">target</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">y_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="nx">n</span><span class="p">}=</span><span class="nx">r_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="nx">n</span><span class="p">}</span><span class="o">+</span><span class="err">\</span><span class="nx">gamma</span><span class="w"> </span><span class="nx">Q</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">s_</span><span class="p">{</span><span class="nx">i</span><span class="o">+</span><span class="mi">1</span><span class="p">},</span><span class="w"> </span><span class="nx">a_</span><span class="p">{</span><span class="nx">i</span><span class="o">+</span><span class="mi">1</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="nx">m</span><span class="p">},</span><span class="w"> </span><span class="nx">g</span><span class="o">^</span><span class="p">{</span><span class="nx">n</span><span class="p">}</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="err">\</span><span class="nx">theta_</span><span class="p">{</span><span class="nx">Q_</span><span class="p">{</span><span class="nx">c</span><span class="p">}}</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">prime</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="w"> </span><span class="err">\</span><span class="nx">mid</span><span class="w"> </span><span class="nx">a_</span><span class="p">{</span><span class="nx">i</span><span class="o">+</span><span class="mi">1</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="nx">m</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">sim</span><span class="w"> </span><span class="err">\</span><span class="nx">pi</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">prime</span><span class="w"> </span><span class="nx">m</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
<span class="w">                        </span><span class="nx">Minimize</span><span class="w"> </span><span class="p">(</span><span class="mi">4</span><span class="p">):</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">L</span><span class="p">}</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="err">\</span><span class="nx">theta_</span><span class="p">{</span><span class="nx">Q_</span><span class="p">{</span><span class="nx">c</span><span class="p">}}</span><span class="err">\</span><span class="nx">right</span><span class="p">)=</span><span class="err">\</span><span class="nx">frac</span><span class="p">{</span><span class="mi">1</span><span class="p">}{</span><span class="nx">S</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">sum_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">frac</span><span class="p">{</span><span class="mi">1</span><span class="p">}{</span><span class="nx">N</span><span class="o">^</span><span class="p">{</span><span class="mi">2</span><span class="p">}}</span><span class="w"> </span><span class="err">\</span><span class="nx">sum_</span><span class="p">{</span><span class="nx">n</span><span class="p">=</span><span class="mi">1</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="nx">N</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">sum_</span><span class="p">{</span><span class="nx">m</span><span class="p">=</span><span class="mi">1</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="nx">N</span><span class="p">}</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">g_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="nx">n</span><span class="p">}</span><span class="o">-</span><span class="nx">Q</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">s_</span><span class="p">{</span><span class="nx">i</span><span class="p">},</span><span class="w"> </span><span class="nx">a_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="nx">m</span><span class="p">},</span><span class="w"> </span><span class="nx">g_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="nx">n</span><span class="p">}</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="err">\</span><span class="nx">theta_</span><span class="p">{</span><span class="nx">Q_</span><span class="p">{</span><span class="nx">c</span><span class="p">}}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="o">^</span><span class="p">{</span><span class="mi">2</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
<span class="w">                        </span><span class="err">\</span><span class="p">(</span><span class="nx">A_</span><span class="p">{</span><span class="nx">n</span><span class="p">,</span><span class="w"> </span><span class="nx">m</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">pi</span><span class="p">}</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">s_</span><span class="p">{</span><span class="nx">i</span><span class="p">},</span><span class="w"> </span><span class="err">\</span><span class="nx">mathbf</span><span class="p">{</span><span class="nx">a</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">):=</span><span class="nx">Q</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">s_</span><span class="p">{</span><span class="nx">i</span><span class="p">},</span><span class="w"> </span><span class="err">\</span><span class="nx">mathbf</span><span class="p">{</span><span class="nx">a</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="nx">i</span><span class="p">},</span><span class="w"> </span><span class="nx">g_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="nx">n</span><span class="p">}</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="err">\</span><span class="nx">theta_</span><span class="p">{</span><span class="nx">Q_</span><span class="p">{</span><span class="nx">g</span><span class="p">}}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="o">-</span><span class="err">\</span><span class="nx">sum_</span><span class="p">{</span><span class="err">\</span><span class="nx">hat</span><span class="p">{</span><span class="nx">a</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="nx">m</span><span class="p">}}</span><span class="w"> </span><span class="err">\</span><span class="nx">pi</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="err">\</span><span class="nx">hat</span><span class="p">{</span><span class="nx">a</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="nx">m</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="w"> </span><span class="nx">Q</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">s_</span><span class="p">{</span><span class="nx">i</span><span class="p">},</span><span class="w"> </span><span class="err">\</span><span class="nx">hat</span><span class="p">{</span><span class="nx">a</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="nx">m</span><span class="p">},</span><span class="w"> </span><span class="nx">g_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="nx">n</span><span class="p">}</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="err">\</span><span class="nx">theta_</span><span class="p">{</span><span class="nx">Q_</span><span class="p">{</span><span class="nx">c</span><span class="p">}}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">                    </span><span class="nx">end</span><span class="w"> </span><span class="k">if</span>
<span class="w">                    </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">nabla_</span><span class="p">{</span><span class="err">\</span><span class="nx">theta_</span><span class="p">{</span><span class="err">\</span><span class="nx">pi</span><span class="p">}}</span><span class="w"> </span><span class="nx">J</span><span class="p">(</span><span class="err">\</span><span class="nx">pi</span><span class="p">)=</span><span class="err">\</span><span class="nx">frac</span><span class="p">{</span><span class="mi">1</span><span class="p">}{</span><span class="nx">S</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">sum_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">sum_</span><span class="p">{</span><span class="nx">m</span><span class="p">,</span><span class="w"> </span><span class="nx">n</span><span class="p">=</span><span class="mi">1</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="nx">N</span><span class="p">}</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="err">\</span><span class="nx">nabla_</span><span class="p">{</span><span class="err">\</span><span class="nx">theta_</span><span class="p">{</span><span class="err">\</span><span class="nx">pi</span><span class="p">}}</span><span class="w"> </span><span class="err">\</span><span class="nx">log</span><span class="w"> </span><span class="err">\</span><span class="nx">pi</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">a_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="nx">m</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">mid</span><span class="w"> </span><span class="nx">o_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="nx">m</span><span class="p">},</span><span class="w"> </span><span class="nx">g_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="nx">m</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="w"> </span><span class="nx">A_</span><span class="p">{</span><span class="nx">n</span><span class="p">,</span><span class="w"> </span><span class="nx">m</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">pi</span><span class="p">}</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">s_</span><span class="p">{</span><span class="nx">i</span><span class="p">},</span><span class="w"> </span><span class="err">\</span><span class="nx">mathbf</span><span class="p">{</span><span class="nx">a</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">                    </span><span class="nx">Update</span><span class="w"> </span><span class="nx">policy</span><span class="p">:</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">theta_</span><span class="p">{</span><span class="err">\</span><span class="nx">pi</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="nx">theta_</span><span class="p">{</span><span class="err">\</span><span class="nx">pi</span><span class="p">}</span><span class="o">+</span><span class="err">\</span><span class="nx">beta</span><span class="w"> </span><span class="err">\</span><span class="nx">nabla_</span><span class="p">{</span><span class="err">\</span><span class="nx">theta_</span><span class="p">{</span><span class="err">\</span><span class="nx">pi</span><span class="p">}}</span><span class="w"> </span><span class="nx">J</span><span class="p">(</span><span class="err">\</span><span class="nx">pi</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">                    </span><span class="nx">Update</span><span class="w"> </span><span class="nx">all</span><span class="w"> </span><span class="nx">target</span><span class="w"> </span><span class="nx">network</span><span class="w"> </span><span class="nx">parameters</span><span class="w"> </span><span class="nx">using</span><span class="p">:</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">theta</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">prime</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="nx">tau</span><span class="w"> </span><span class="err">\</span><span class="nx">theta</span><span class="o">+</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="err">\</span><span class="nx">tau</span><span class="p">)</span><span class="w"> </span><span class="err">\</span><span class="nx">theta</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">prime</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
<span class="w">                    </span><span class="nx">Reset</span><span class="w"> </span><span class="nx">buffer</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">B</span><span class="err">\</span><span class="p">)</span>
<span class="w">                    </span><span class="nx">end</span><span class="w"> </span><span class="k">for</span>
<span class="w">            </span><span class="nx">end</span><span class="w"> </span><span class="k">if</span>
<span class="w">            </span><span class="nx">If</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">epsilon</span><span class="p">&gt;</span><span class="err">\</span><span class="nx">epsilon_</span><span class="p">{</span><span class="err">\</span><span class="nx">text</span><span class="w"> </span><span class="p">{</span><span class="nx">end</span><span class="w"> </span><span class="p">}}</span><span class="err">\</span><span class="p">),</span><span class="w"> </span><span class="k">then</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">epsilon</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="nx">epsilon</span><span class="o">-</span><span class="err">\</span><span class="nx">epsilon_</span><span class="p">{</span><span class="err">\</span><span class="nx">text</span><span class="w"> </span><span class="p">{</span><span class="nx">step</span><span class="w"> </span><span class="p">}}</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="nx">end</span><span class="w"> </span><span class="k">for</span>
<span class="w">    </span><span class="nx">end</span><span class="w"> </span><span class="k">for</span>
</code></pre></div>

<p>Off-policy training with a large replay buffer allows RL algorithms to benefit from less correlated transitions (Silver et al., 2014; Lillicrap et al., 2016). The algorithmic modification for off-policy training is to maintain a circular replay buffer that does not reset (i.e. remove line 38), and conduct training (lines 24-41) while executing policies in the environment (lines 17-22). Despite introducing bias in MARL, we found that off-policy training benefited CM3 in SUMO and Checkers.</p>
<h1>B DERIVATIONS</h1>
<h2>B. 1 Proposition 1</h2>
<p>By stationarity and relabeling $t$, the credit function can be written:</p>
<p>$$
Q_{n}^{\boldsymbol{\pi}}\left(s, a^{m}\right):=\mathbb{E}<em t="0">{\boldsymbol{\pi}}\left[\sum</em>}^{\infty} \gamma^{t} R\left(s_{t}, \mathbf{a<em 0="0">{t}, g^{n}\right) \mid s</em>}=s, a_{0}^{m}=a^{m}\right]=\mathbb{E<em t="1">{\boldsymbol{\pi}}\left[\sum</em>}^{\infty} \gamma^{t-1} R\left(s_{t}, \mathbf{a<em 1="1">{t}, g^{n}\right) \mid s</em>\right]
$$}=s, a_{1}^{m}=a^{m</p>
<p>Using the law of iterated expectation, the credit function satisfies the Bellman expectation equation (2):</p>
<p>$$
\begin{aligned}
&amp; Q_{n}^{\boldsymbol{\pi}}\left(s, a^{m}\right)=\mathbb{E}<em t="0">{\boldsymbol{\pi}}\left[\sum</em>}^{\infty} \gamma^{t} R\left(s_{t}, \mathbf{a<em 0="0">{t}, g^{n}\right) \mid s</em>\right] \
&amp; =\mathbb{E}}=s, a_{0}^{m}=a^{m<em 0="0">{\boldsymbol{\pi}}\left[R\left(s</em>}, \mathbf{a<em t="1">{0}, g^{n}\right)+\sum</em>}^{\infty} \gamma^{t} R\left(s_{t}, \mathbf{a<em 0="0">{t}, g^{n}\right) \mid s</em>\right] \
&amp; =\mathbb{E}}=s, a_{0}^{m}=a^{m<em 1="1">{s</em>}, a_{1}^{m} \mid s_{0}, a_{0}, \boldsymbol{\pi}}\left[\mathbb{E<em 0="0">{\boldsymbol{\pi}}\left[R\left(s</em>}, \mathbf{a<em t="1">{0}, g^{n}\right)+\sum</em>}^{\infty} \gamma^{t} R\left(s_{t}, \mathbf{a<em 0="0">{t}, g^{n}\right) \mid s</em>\right] \
&amp; =\mathbb{E}}=s, a_{0}^{m}=a^{m}, s_{1}=s^{\prime}, a_{1}^{m}=\hat{a}^{m}\right] \mid s_{0}=s, a_{0}^{m}=a^{m<em 1="1">{s</em>\right)\right. \
&amp; \left.+\mathbb{E}}, a_{1}^{m} \mid s_{0}, a_{0}, \boldsymbol{\pi}}\left[\sum_{a^{-m}} \boldsymbol{\pi}\left(a^{-m} \mid s, \mathbf{g}^{-m}\right) R\left(s,\left(a^{m}, a^{-m}\right), g^{n<em t="1">{\boldsymbol{\pi}}\left[\sum</em>}^{\infty} \gamma^{t} R\left(s_{t}, \mathbf{a<em 0="0">{t}, g^{n}\right) \mid s</em>\right] \
&amp; =\sum_{a^{-m}} \boldsymbol{\pi}\left(a^{-m} \mid s, \mathbf{g}^{-m}\right) R\left(s,\left(a^{m}, a^{-m}\right), g^{n}\right) \
&amp; +\mathbb{E}}=s, a_{0}^{m}=a^{m}, s_{1}=s^{\prime}, a_{1}^{m}=\hat{a}^{m}\right] \mid s_{0}=s, a_{0}^{m}=a^{m<em 1="1">{s</em>}, a_{1}^{m} \mid s_{0}, a_{0}, \boldsymbol{\pi}}\left[\mathbb{E<em t="1">{\boldsymbol{\pi}}\left[\sum</em>}^{\infty} \gamma^{t} R\left(s_{t}, \mathbf{a<em 0="0">{t}, g^{n}\right) \mid s</em>\right] \
&amp; =\sum_{a^{-m}} \boldsymbol{\pi}\left(a^{-m} \mid s, \mathbf{g}^{-m}\right) R\left(s,\left(a^{m}, a^{-m}\right), g^{n}\right) \
&amp; +\sum_{a^{-m}} \boldsymbol{\pi}\left(a^{-m} \mid s, \mathbf{g}^{-m}\right) \sum_{s^{\prime}} P\left(s^{\prime} \mid s,\left(a^{m}, a^{-m}\right)\right) \sum_{\hat{a}^{m}} \pi\left(\hat{a}^{m} \mid o^{m}\left(s^{\prime}\right)\right) \mathbb{E}}=s, a_{0}^{m}=a^{m}, s_{1}=s^{\prime}, a_{1}^{m}=\hat{a}^{m}\right] \mid s_{0}=s, a_{0}^{m}=a^{m<em t="1">{\boldsymbol{\pi}}\left[\sum</em>}^{\infty} \gamma^{t} R\left(s_{t}, \mathbf{a<em 1="1">{t}, g^{n}\right) \mid s</em>\right] \
&amp; =\sum_{a^{-m}} \boldsymbol{\pi}\left(a^{-m} \mid s, \mathbf{g}^{-m}\right)\left[R\left(s,\left(a^{m}, a^{-m}\right), g^{n}\right)\right. \
&amp; \left.+\gamma \sum_{s^{\prime}} P\left(s^{\prime} \mid s,\left(a^{m}, a^{-m}\right)\right) \sum_{\hat{a}^{m}} \pi\left(\hat{a}^{m} \mid o^{m}\left(s^{\prime}\right)\right) \mathbb{E}}=s^{\prime}, a_{1}^{m}=\hat{a}^{m<em t="1">{\pi}\left[\sum</em>}^{\infty} \gamma^{t-1} R\left(s_{t}, \mathbf{a<em 1="1">{t}, g^{n}\right) \mid s</em>\right]\right] \
&amp; =\sum_{a^{-m}} \boldsymbol{\pi}\left(a^{-m} \mid s, \mathbf{g}^{-m}\right)\left[R\left(s,\left(a^{m}, a^{-m}\right), g^{n}\right)+\gamma \sum_{s^{\prime}} P\left(s^{\prime} \mid s,\left(a^{m}, a^{-m}\right)\right) \sum_{\hat{a}^{m}} \pi^{m}\left(\hat{a}^{m} \mid o^{m}\left(s^{\prime}\right)\right) Q_{n}^{\boldsymbol{\pi}}\left(s^{\prime}, \hat{a}^{m}\right)\right] \
&amp; =\mathbb{E}}=s^{\prime}, a_{1}^{m}=\hat{a}^{m<em t="t">{\boldsymbol{\pi}}\left[R\left(s</em>}, \mathbf{a<em n="n">{t}, \mathbf{g}^{n}\right)+\gamma Q</em>\right]
\end{aligned}
$$}^{\boldsymbol{\pi}}\left(s_{t+1}, a_{t+1}^{m}\right) \mid s_{t}=s, a_{t}^{m}=a^{m</p>
<p>The goal-specific joint value function is the marginal of the credit function:</p>
<p>$$
\begin{aligned}
V_{n}^{\boldsymbol{\pi}}(s) &amp; =\mathbb{E}<em t="0">{\boldsymbol{\pi}}\left[\sum</em>}^{\infty} \gamma^{t} R\left(s_{t}, \mathbf{a<em 0="0">{t}, g^{n}\right) \mid s</em>=s\right] \
&amp; =\mathbb{E}<em 0="0">{a</em>}^{m} \mid s_{0}, \boldsymbol{\pi}}\left[\mathbb{E<em t="0">{\boldsymbol{\pi}}\left[\sum</em>}^{\infty} \gamma^{t} R\left(s_{t}, \mathbf{a<em 0="0">{t}, g^{n}\right) \mid s</em>=s\right] \
&amp; =\sum_{a^{m}} \pi\left(a^{m} \mid o^{m}(s), g^{m}\right) Q_{n}^{\boldsymbol{\pi}}\left(s, a^{m}\right)
\end{aligned}
$$}=s, a_{0}^{m}=a^{m}\right] \mid s_{0</p>
<p>The credit function can be expressed in terms of the goal-specific action-value function:</p>
<p>$$
\begin{aligned}
V_{n}^{\boldsymbol{\pi}}(s) &amp; =\sum_{a^{m}} \pi\left(a^{m} \mid o^{m}, g^{m}\right) Q_{n}^{\boldsymbol{\pi}}\left(s, a^{m}\right) \
V_{n}^{\boldsymbol{\pi}}(s) &amp; =\sum_{\mathbf{a}} \boldsymbol{\pi}(\mathbf{a} \mid s, \mathbf{g}) Q_{n}^{\boldsymbol{\pi}}(s, \mathbf{a}) \
&amp; =\sum_{a^{m}} \sum_{a^{-m}} \pi\left(a^{m} \mid o^{m}, g^{m}\right) \boldsymbol{\pi}\left(a^{-m} \mid s, g^{-m}\right) Q_{n}^{\boldsymbol{\pi}}\left(s,\left(a^{m}, a^{-m}\right)\right) \
\Rightarrow Q_{n}^{\boldsymbol{\pi}}\left(s, a^{m}\right) &amp; =\sum_{a^{-m}} \boldsymbol{\pi}\left(a^{-m} \mid s, g^{-m}\right) Q_{n}^{\boldsymbol{\pi}}(s, \mathbf{a})
\end{aligned}
$$</p>
<h1>B. 2 Proposition 2</h1>
<p>First we state some elementary relations between global functions $V_{n}^{\pi}(s)$ and $Q_{n}^{\pi}(s, \mathbf{a})$. These carry over directly from the case of an MDP, by treating the joint policy $\boldsymbol{\pi}$ as as an effective "single-agent" policy and restricting attention to a single goal $g^{n}$ (standard derivations are included at the end of this section).</p>
<p>$$
\begin{aligned}
Q_{n}^{\boldsymbol{\pi}}(s, \mathbf{a}) &amp; =R\left(s, \mathbf{a}, g^{n}\right)+\gamma \sum_{s^{\prime}} P\left(s^{\prime} \mid s, \mathbf{a}\right) V_{n}^{\boldsymbol{\pi}}\left(s^{\prime}\right) \
V_{n}^{\boldsymbol{\pi}}(s) &amp; =\sum_{\mathbf{a}} \boldsymbol{\pi}(\mathbf{a} \mid s, \mathbf{g}) Q_{n}^{\boldsymbol{\pi}}(s, \mathbf{a})
\end{aligned}
$$</p>
<p>We follow the proof of the policy gradient theorem (Sutton et al., 2000):</p>
<p>$$
\begin{aligned}
\nabla_{\theta} V_{n}^{\boldsymbol{\pi}}(s) &amp; =\nabla_{\theta} \sum_{\mathbf{a}} \boldsymbol{\pi}(\mathbf{a} \mid s, \mathbf{g}) Q_{n}^{\boldsymbol{\pi}}(s, \mathbf{a}) \
&amp; =\sum_{\mathbf{a}}\left[\left(\nabla_{\theta} \boldsymbol{\pi}(\mathbf{a} \mid s, \mathbf{g})\right) Q_{n}^{\boldsymbol{\pi}}(s, \mathbf{a})+\boldsymbol{\pi}(\mathbf{a} \mid s, \mathbf{g}) \nabla_{\theta} Q_{n}^{\boldsymbol{\pi}}(s, \mathbf{a})\right] \
&amp; =\sum_{\mathbf{a}}\left[\left(\nabla_{\theta} \boldsymbol{\pi}(\mathbf{a} \mid s, \mathbf{g})\right) Q_{n}^{\boldsymbol{\pi}}(s, \mathbf{a})+\boldsymbol{\pi}(\mathbf{a} \mid s, \mathbf{g}) \nabla_{\theta}\left(R\left(s, \mathbf{a}, g^{n}\right)+\gamma \sum_{s^{\prime}} P\left(s^{\prime} \mid s, \mathbf{a}\right) V_{n}^{\boldsymbol{\pi}}\left(s^{\prime}\right)\right)\right] \
&amp; =\sum_{\mathbf{a}}\left[\left(\nabla_{\theta} \boldsymbol{\pi}(\mathbf{a} \mid s, \mathbf{g})\right) Q_{n}^{\boldsymbol{\pi}}(s, \mathbf{a})+\boldsymbol{\pi}(\mathbf{a} \mid s, \mathbf{g}) \gamma \sum_{s^{\prime}} P\left(s^{\prime} \mid s, \mathbf{a}\right) \nabla_{\theta} V_{n}^{\boldsymbol{\pi}}\left(s^{\prime}\right)\right] \
&amp; =\sum_{\hat{s}} \sum_{k=0}^{\infty} \gamma^{k} P(s \rightarrow \hat{s}, k, \boldsymbol{\pi}) \sum_{\mathbf{a}}\left(\nabla_{\theta} \boldsymbol{\pi}(\mathbf{a} \mid \hat{s}, \mathbf{g})\right) Q_{n}^{\boldsymbol{\pi}}(\hat{s}, \mathbf{a}) \quad \text { (by recursively unrolling) } \
\nabla_{\theta} J_{n}(\boldsymbol{\pi}) &amp; :=\nabla_{\theta} V_{n}^{\boldsymbol{\pi}}\left(s_{0}\right)=\sum_{s} \sum_{k=0}^{\infty} \gamma^{k} P\left(s_{0} \rightarrow s, k, \boldsymbol{\pi}\right) \sum_{\mathbf{a}}\left(\nabla_{\theta} \boldsymbol{\pi}(\mathbf{a} \mid s, \mathbf{g})\right) Q_{n}^{\boldsymbol{\pi}}(s, \mathbf{a}) \
&amp; =\sum_{s} \rho^{\boldsymbol{\pi}}(s) \sum_{\mathbf{a}} \boldsymbol{\pi}(\mathbf{a} \mid s, \mathbf{g})\left(\nabla_{\theta} \log \boldsymbol{\pi}(\mathbf{a} \mid s, \mathbf{g})\right) Q_{n}^{\boldsymbol{\pi}}(s, \mathbf{a}) \
&amp; =\mathbb{E}<em _theta="\theta">{\boldsymbol{\pi}}\left[\left(\nabla</em>)\right]
\end{aligned}
$$} \log \boldsymbol{\pi}(\mathbf{a} \mid s, \mathbf{g})\right) Q_{n}^{\boldsymbol{\pi}}(s, \mathbf{a</p>
<p>We can replace $Q_{n}^{\boldsymbol{\pi}}(s, \mathbf{a})$ by the advantage function $A_{n}^{\boldsymbol{\pi}}(s, \mathbf{a}):=Q_{n}^{\boldsymbol{\pi}}(s, \mathbf{a})-V_{n}^{\boldsymbol{\pi}}(s)$, which does not change the expectation in Equation (9) because:</p>
<p>$$
\begin{aligned}
\mathbb{E}<em _theta="\theta">{\boldsymbol{\pi}}\left[\nabla</em>(s) \
&amp; =\sum_{s} \rho^{\boldsymbol{\pi}}(s) V_{n}^{\boldsymbol{\pi}}(s) \nabla_{\theta} \sum_{\mathbf{a}} \boldsymbol{\pi}(\mathbf{a} \mid s, \mathbf{g})=0
\end{aligned}
$$} \log \boldsymbol{\pi}(\mathbf{a} \mid s, \mathbf{g}) V_{n}^{\boldsymbol{\pi}}(s)\right] &amp; =\sum_{s} \rho^{\boldsymbol{\pi}}(s) \sum_{\mathbf{a}} \boldsymbol{\pi}(\mathbf{a} \mid s, \mathbf{g}) \nabla_{\theta} \log \boldsymbol{\pi}(\mathbf{a} \mid s, \mathbf{g}) V_{n}^{\boldsymbol{\pi}</p>
<p>So the gradient (9) can be written</p>
<p>$$
\nabla_{\theta} J_{n}(\boldsymbol{\pi})=\mathbb{E}<em _theta="\theta">{\boldsymbol{\pi}}\left[\left(\nabla</em>(s)\right)\right]
$$} \sum_{m=1}^{N} \log \pi\left(a^{m} \mid o^{m}, g^{m}\right)\right)\left(Q_{n}^{\boldsymbol{\pi}}(s, \mathbf{a})-V_{n}^{\boldsymbol{\pi}</p>
<p>Recall that from (3), for any choice of agent label $k \in[1 . . N]$ :</p>
<p>$$
V_{n}^{\boldsymbol{\pi}}(s)=\sum_{a^{k}} \pi\left(a^{k} \mid o^{k}, g^{k}\right) Q_{n}^{\boldsymbol{\pi}}\left(s, a^{k}\right)
$$</p>
<p>Then substituting (3) into (10):</p>
<p>$$
\begin{aligned}
\nabla_{\theta} J_{n}(\boldsymbol{\pi}) &amp; =\mathbb{E}<em _theta="\theta">{\boldsymbol{\pi}}\left[\left(\nabla</em>)\right] \
A_{n, k}^{\boldsymbol{\pi}}(s, \mathbf{a}) &amp; :=Q_{n}^{\boldsymbol{\pi}}(s, \mathbf{a})-\sum_{\hat{a}^{k}} \pi\left(\hat{a}^{k} \mid o^{k}, g^{k}\right) \bar{Q}_{n}^{\boldsymbol{\pi}}\left(s, \hat{a}^{k}\right)
\end{aligned}
$$} \sum_{m=1}^{N} \log \pi\left(a^{m} \mid o^{m}, g^{m}\right)\right) A_{n, k}^{\boldsymbol{\pi}}(s, \mathbf{a</p>
<p>Now notice that the choice of $k$ in (13) is completely arbitrary, since (3) holds for any $k \in[1 . . N]$. Therefore, it is valid to distribute $A_{n, k}^{\boldsymbol{\pi}}(s, \mathbf{a})$ into the summation in (12) using the summation index $m$ instead of $k$. Further summing (12) over all $n$, we arrive at the result of Proposition 2:</p>
<p>$$
\begin{aligned}
\nabla_{\theta} J(\boldsymbol{\pi}) &amp; =\mathbb{E}<em m="1">{\boldsymbol{\pi}}\left[\sum</em>)\right] \
A_{n, m}^{\boldsymbol{\pi}}(s, \mathbf{a}) &amp; :=Q_{n}^{\boldsymbol{\pi}}(s, \mathbf{a})-\sum_{\hat{a}^{m}} \pi\left(\hat{a}^{m} \mid o^{m}, g^{m}\right) Q_{n}^{\boldsymbol{\pi}}\left(s, \hat{a}^{m}\right)
\end{aligned}
$$}^{N} \sum_{n=1}^{N}\left(\nabla_{\theta} \log \pi\left(a^{m} \mid o^{m}, g^{m}\right)\right) A_{n, m}^{\boldsymbol{\pi}}(s, \mathbf{a</p>
<p>The relation between $V_{n}^{\pi}(s)$ and $Q_{n}^{\pi}(s, \mathbf{a})$ in (7) and (8) are derived as follows:</p>
<p>$$
\begin{aligned}
Q_{n}^{\boldsymbol{\pi}}(s, \mathbf{a}) &amp; :=\mathbb{E}<em t="t">{\pi}\left[\sum</em>} \gamma^{t} R\left(s_{t}, \mathbf{a<em 0="0">{t}, g^{n}\right) \mid s</em>}=s, \mathbf{a<em _pi="\pi">{0}=\mathbf{a}\right] \
&amp; =\mathbb{E}</em>}\left[R\left(s_{0}, \mathbf{a<em t="1">{0}, g^{n}\right)+\sum</em>}^{\infty} \gamma^{t} R\left(s_{t}, \mathbf{a<em 0="0">{t}, g^{n}\right) \mid s</em>}=s, \mathbf{a<em s__1="s_{1">{0}=\mathbf{a}\right] \
&amp; =R\left(s, \mathbf{a}, g^{n}\right)+\mathbb{E}</em>} \mid s_{0}, \mathbf{a<em _pi="\pi">{0}, \boldsymbol{\pi}}\left[\mathbb{E}</em>}\left[\sum_{t=1}^{\infty} R\left(s_{t}, \mathbf{a<em 0="0">{t}, g^{n}\right) \mid s</em>}=s, \mathbf{a<em 1="1">{0}=\mathbf{a}, s</em>}=s^{\prime}\right] \mid s_{0}=s, \mathbf{a<em s_prime="s^{\prime">{0}=\mathbf{a}\right] \
&amp; =R\left(s, \mathbf{a}, g^{n}\right)+\gamma \sum</em>}} P\left(s^{\prime} \mid s, \mathbf{a}\right) \mathbb{E<em t="1">{\boldsymbol{\pi}}\left[\sum</em>}^{\infty} \gamma^{t-1} R\left(s_{t}, \mathbf{a<em 1="1">{t}, g^{n}\right) \mid s</em>\right] \
&amp; =R\left(s, \mathbf{a}, g^{n}\right)+\gamma \sum_{s^{\prime}} P\left(s^{\prime} \mid s, \mathbf{a}\right) V_{n}^{\boldsymbol{\pi}}\left(s^{\prime}\right) \
V_{n}^{\boldsymbol{\pi}}(s) &amp; :=\mathbb{E}}=s^{\prime<em t="0">{\boldsymbol{\pi}}\left[\sum</em>}^{\infty} \gamma^{t} R\left(s_{t}, \mathbf{a<em 0="0">{t}, g^{n}\right) \mid s</em>=s\right] \
&amp; =\mathbb{E}<em 0="0">{\mathbf{a}</em>} \mid s_{0}, \boldsymbol{\pi}}\left[\mathbb{E<em t="0">{\boldsymbol{\pi}}\left[\sum</em>}^{\infty} \gamma^{t} R\left(s_{t}, \mathbf{a<em 0="0">{t}, g^{n}\right) \mid s</em>}=s, \mathbf{a<em 0="0">{0}=\mathbf{a}\right] \mid s</em>=s\right] \
&amp; =\sum_{\mathbf{a}} \boldsymbol{\pi}(\mathbf{a} \mid s, \mathbf{g}) \mathbb{E}<em t="0">{\boldsymbol{\pi}}\left[\sum</em>}^{\infty} \gamma^{t} R\left(s_{t}, \mathbf{a<em 0="0">{t}, g^{n}\right) \mid s</em>}=s, \mathbf{a<em _mathbf_a="\mathbf{a">{0}=\mathbf{a}\right] \
&amp; =\sum</em>)
\end{aligned}
$$}} \boldsymbol{\pi}(\mathbf{a} \mid s, \mathbf{g}) Q_{n}^{\boldsymbol{\pi}}(s, \mathbf{a</p>
<h1>C Variance</h1>
<h2>C. 1 Variance of COMA GRADIENT.</h2>
<p>Let $Q:=Q^{\pi}(s, \mathbf{a}, \mathbf{g})$ denote the centralized Q function, let $\pi\left(a^{n}\right):=\pi\left(a^{n} \mid o^{n}, g^{n}\right)$ denote a single agent's policy, and let $\pi\left(a^{-n}\right):=\pi\left(a^{-n} \mid o^{-n}, g^{-n}\right)$ denote the other agents' joint policy.
In cooperative multi-goal MARL, the direct application of COMA has the following gradient.</p>
<p>$$
\begin{aligned}
\nabla_{\theta} J &amp; =\mathbb{E}\left[\sum_{n} \nabla_{\theta} \log \pi\left(a^{n} \mid o^{n}, g^{n}\right)\left(Q-b_{n}\left(s, a^{-n}, \mathbf{g}\right)\right)\right] \
b_{n}\left(s, a^{-n}, \mathbf{g}\right) &amp; :=\sum_{\hat{a}^{n}} \pi\left(\hat{a}^{n} \mid o^{n}, g^{n}\right) Q^{\pi}\left(s, \hat{a}^{n}, a^{-n}, \mathbf{g}\right)
\end{aligned}
$$</p>
<p>Define the following:</p>
<p>$$
\begin{aligned}
&amp; z_{n}:=\nabla_{\theta} \log \pi\left(a^{n} \mid o^{n}, g^{n}\right) \
&amp; f_{n}:=\nabla_{\theta} \log \pi\left(a^{n} \mid o^{n}, g^{n}\right)\left(Q-b_{n}\left(s, a^{-n}\right)\right)=z_{n}\left(Q-b_{n}\left(s, a^{-n}, \mathbf{g}\right)\right)
\end{aligned}
$$</p>
<p>Define $M_{n m}:=\mathbb{E}<em n="n">{\pi}\left[f</em>}\right]^{T} \mathbb{E<em m="m">{\pi}\left[f</em>}\right]$ and let $M:=\sum_{n, m} M_{n m}$. Then we have $M_{n m}=$ $\mathbb{E<em n="n">{\pi}\left[z</em>} Q\right]^{T} \mathbb{E<em m="m">{\pi}\left[z</em> Q\right]$ since</p>
<p>$$
\begin{aligned}
\mathbb{E}<em n="n">{\pi}\left[z</em>} b_{n}\right] &amp; =\mathbb{E<em s="s">{\pi}\left[\sum</em>\right)\right] \
&amp; =\sum_{s} \rho^{\pi}(s) \sum_{a^{-n}} \pi^{-n}\left(a^{-n} \mid o^{-n}, g^{-n}\right) \sum_{a^{n}} \pi\left(a^{n} \mid o^{n}, g^{n}\right) \nabla_{\theta} \log \pi\left(a^{n} \mid o^{n}, g^{n}\right) b_{n}\left(s, a^{-n}, \mathbf{g}\right) \
&amp; =\sum_{s} \rho^{\pi}(s) \sum_{a^{-n}} \pi^{-n}\left(a^{-n} \mid o^{-n}, g^{-n}\right) \sum_{a^{n}} \nabla_{\theta} \pi\left(a^{n} \mid o^{n}, g^{n}\right) b_{n}\left(s, a^{-n}, \mathbf{g}\right) \
&amp; =\sum_{s} \rho^{\pi}(s) \sum_{a^{-n}} \pi^{-n}\left(a^{-n} \mid o^{-n}, g^{-n}\right) b_{n}\left(s, a^{-n}, \mathbf{g}\right) \nabla_{\theta} \sum_{a^{n}} \pi\left(a^{n} \mid o^{n}, g^{n}\right)=0
\end{aligned}
$$} \rho^{\pi}(s) \sum_{\mathbf{a}} \pi(\mathbf{a} \mid s, \mathbf{g}) \nabla_{\theta} \log \pi\left(a^{n} \mid o^{n}, g^{n}\right) b_{n}\left(s, a^{-n}, \mathbf{g</p>
<p>Since the COMA gradient is $\mathbb{E}<em n="1">{\pi}\left[\sum</em>\right]$. its variance can be derived to be (Wu et al., 2018):}^{N} f_{n</p>
<p>$$
\begin{aligned}
\operatorname{Var}\left(\sum_{n=1}^{N} f_{n}\right)= &amp; \sum_{n} \mathbb{E}<em n="n">{\pi}\left[z</em>\right] \
&amp; +\sum_{n} \sum_{m \neq n} \mathbb{E}}^{T} z_{n} Q^{2}-2 b_{n} z_{n}^{T} z_{n} Q+b_{n}^{2} z_{n}^{T} z_{n<em n="n">{\pi}\left[z</em>\right)\right]-M
\end{aligned}
$$}^{T} z_{m}\left(Q-b_{n}\right)\left(Q-b_{m</p>
<h2>C. 2 Variance of the CM3 GRADIENT</h2>
<p>For convenience, let $Q_{n}:=Q_{n}^{\pi}(s, \mathbf{a})=Q^{\pi}\left(s, \mathbf{a}, g^{n}\right)$ denote the global Q function for goal $g^{n}$, and let $\pi\left(a^{m}\right):=\pi\left(a^{m} \mid o^{m}, g^{m}\right)$. The CM3 gradient can be rewritten as</p>
<p>$$
\begin{aligned}
\nabla_{\theta} J(\boldsymbol{\pi}) &amp; =\mathbb{E}<em n="1">{\pi}\left[\sum</em>(s)\right)\right] \
b_{n m}(s) &amp; :=\sum_{\hat{a}^{m}} \pi\left(\hat{a}^{m}\right) Q_{n}^{\pi}\left(s, \hat{a}^{m}\right)
\end{aligned}
$$}^{N} \sum_{m=1}^{N} \nabla_{\theta} \log \pi\left(a^{m}\right)\left(Q_{n}-b_{n m</p>
<p>As before, $z_{m}:=\nabla_{\theta} \log \pi\left(a^{m}\right)$. Define $h_{n m}:=z_{m}\left(Q_{n}-b_{n m}(s)\right)$ and let $h_{n}:=\sum_{m} h_{n m}$. Then the variance is</p>
<p>$$
\begin{aligned}
\operatorname{Var}\left(\sum_{n} h_{n}\right) &amp; =\sum_{n} \operatorname{Var}\left(h_{n}\right)+\sum_{n} \sum_{m \neq n} \operatorname{Cov}\left(h_{n}, h_{m}\right) \
&amp; =\sum_{n}\left(\sum_{m} \operatorname{Var}\left(h_{n m}\right)+\sum_{m} \sum_{k \neq m} \operatorname{Cov}\left(h_{n m}, h_{n k}\right)\right)+\sum_{n} \sum_{m \neq n} \operatorname{Cov}\left(h_{n}, h_{m}\right)
\end{aligned}
$$</p>
<h1>D EXAMPLE OF GREEDY INITIALIZATION FOR MARL EXPLORATION</h1>
<p>A greedy initialization can provide significant improvement in multi-agent exploration versus naïve random exploration, as shown by a simple thought experiment. Consider a two-player MG defined by a $4 \times 3$ gridworld with unit actions (up, down, left, right). Agent $A$ starts at (1,2) with goal (4,2), while agent $B$ starts at (4,2) with goal (1,2). The greedy policy for each agent in MG is to move horizontally toward its target, since this is optimal in the induced $\mathbf{M}$ (when the other agent is absent). Case 1: Suppose that for $\epsilon \in(0,1), A$ and $B$ follow greedy policies with probability $1-\epsilon$, and take random actions $(p(a)=1 / 4)$ with probability $\epsilon$. Then the probability of a symmetric optimal trajectory is $P($ cooperate $)=2 \epsilon^{2}((1-\epsilon)+\epsilon / 4)^{8}$. For $\epsilon=0.5, P($ cooperate $) \approx 0.01$. Case 2: If agents execute uniform random exploration, then $P($ cooperate $)=3.05 \mathrm{e}-5 \ll 0.01$.</p>
<h2>E GENERALIZATION</h2>
<p>Table 1: Test performance with heavy traffic on difficult initial and goal lanes configurations</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Config</th>
<th style="text-align: center;">Initial lanes</th>
<th style="text-align: center;">Goal lanes</th>
<th style="text-align: center;">CM3</th>
<th style="text-align: center;">IAC</th>
<th style="text-align: center;">COMA</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">C1</td>
<td style="text-align: center;">$[1,2]$</td>
<td style="text-align: center;">$[3,0]$</td>
<td style="text-align: center;">$\mathbf{1 6 . 1 7}$</td>
<td style="text-align: center;">11.40</td>
<td style="text-align: center;">10.00</td>
</tr>
<tr>
<td style="text-align: center;">C2</td>
<td style="text-align: center;">Unif. random</td>
<td style="text-align: center;">Unif. random</td>
<td style="text-align: center;">$\mathbf{1 4 . 9 3}$</td>
<td style="text-align: center;">12.20</td>
<td style="text-align: center;">12.93</td>
</tr>
<tr>
<td style="text-align: center;">C3</td>
<td style="text-align: center;">$[1,2]$</td>
<td style="text-align: center;">$[2,1]$</td>
<td style="text-align: center;">$\mathbf{1 5 . 8 5}$</td>
<td style="text-align: center;">14.32</td>
<td style="text-align: center;">15.00</td>
</tr>
<tr>
<td style="text-align: center;">C4</td>
<td style="text-align: center;">$[0,1]$</td>
<td style="text-align: center;">$[3,2]$</td>
<td style="text-align: center;">$\mathbf{1 6 . 3 5}$</td>
<td style="text-align: center;">9.73</td>
<td style="text-align: center;">8.1</td>
</tr>
</tbody>
</table>
<p>We investigated whether policies trained with few agent vehicles $(N=2)$ on an empty road can generalize to situations with heavy SUMO-controlled traffic. We also tested on initial and goal lane configurations (C3 and C4) which occur with low probability when training with configurations C1 and C2. Table 1 shows the sum of agents' reward, averaged over 100 test episodes, on these configurations that require cooperation with each other and with minimally-interactive SUMOcontrolled vehicles for success. CM3's higher performance than IAC and COMA in training is reflected by better generalization performance on these test configurations. There is almost negligible decrase in performance from train Figure 5d to test, giving evidence to our hypothesis that centralized training with few agents is feasible even for deployment in situations with many agents, for certain applications where local interactions are dominant.</p>
<h2>F ABSOLUTE RUNTIME</h2>
<p>CM3's higher sample efficiency does not come at greater computational cost, as all methods' runtimes are within an order of magnitude of one another. Test times have no significant difference as all neural networks were similar.</p>
<p>Table 2: Absolute training runtime of all algorithms in seconds</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Environment</th>
<th style="text-align: right;">CM3</th>
<th style="text-align: right;">IAC</th>
<th style="text-align: right;">COMA</th>
<th style="text-align: right;">QMIX</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Antipodal</td>
<td style="text-align: right;">$1.1 \mathrm{e} 4 \pm 348$</td>
<td style="text-align: right;">$0.9 \mathrm{e} 4 \pm 20$</td>
<td style="text-align: right;">$1.9 \mathrm{e} 4 \pm 238$</td>
<td style="text-align: right;">$1.0 \mathrm{e} 4 \pm 19$</td>
</tr>
<tr>
<td style="text-align: center;">Cross</td>
<td style="text-align: right;">$1.9 \mathrm{e} 4 \pm 256$</td>
<td style="text-align: right;">$1.5 \mathrm{e} 4 \pm 26$</td>
<td style="text-align: right;">$1.3 \mathrm{e} 4 \pm 12$</td>
<td style="text-align: right;">$1.1 \mathrm{e} 4 \pm 34$</td>
</tr>
<tr>
<td style="text-align: center;">Merge</td>
<td style="text-align: right;">$8.5 \mathrm{e} 3 \pm 21$</td>
<td style="text-align: right;">$6.8 \mathrm{e} 3 \pm 105$</td>
<td style="text-align: right;">$9.6 \mathrm{e} 3 \pm 294$</td>
<td style="text-align: right;">$1.2 \mathrm{e} 4 \pm 61$</td>
</tr>
<tr>
<td style="text-align: center;">SUMO</td>
<td style="text-align: right;">$9.6 \mathrm{e} 3 \pm 278$</td>
<td style="text-align: right;">$7.0 \mathrm{e} 3 \pm 1.5 \mathrm{e} 3$</td>
<td style="text-align: right;">$8.7 \mathrm{e} 3 \pm 1.3 \mathrm{e} 3$</td>
<td style="text-align: right;">$6.3 \mathrm{e} 3 \pm 21$</td>
</tr>
<tr>
<td style="text-align: center;">Checkers</td>
<td style="text-align: right;">$9.2 \mathrm{e} 3 \pm 880$</td>
<td style="text-align: right;">$8.5 \mathrm{e} 3 \pm 568$</td>
<td style="text-align: right;">$7.7 \mathrm{e} 3 \pm 2.2 \mathrm{e} 3$</td>
<td style="text-align: right;">$11 \mathrm{e} 3 \pm 1.4 \mathrm{e} 3$</td>
</tr>
</tbody>
</table>
<h2>G ENVIRONMENT DETAILS</h2>
<p>The full Markov game for each experimental domain, along with the single-agent MDP induced from the Markov game, are defined in this section. In all domains, each agent's observation in the Markov game consists of two components, $\mathrm{o}<em _others="{others" _text="\text">{\text {self }}$ and $\mathrm{o}</em>$. CM3 leverages this decomposition for faster training, while IAC, COMA and QMIX do not.}</p>
<h1>G. 1 COOPERATIVE NAVIGATION</h1>
<p>This domain is adapted from the multi-agent particle environment in Lowe et al. (2017). Movable agents and static landmarks are represented as circular objects located in a 2D unbounded world with real-valued position and velocity. Agents experience contact forces during collisions. A simple model of inertia and friction is involved.</p>
<p>State. The global state vector is the concatenation of all agents' absolute position $(x, y) \in \mathbb{R}^{2}$ and velocity $\left(v_{x}, v_{y}\right) \in \mathbb{R}^{2}$.
Observation. Each agent's observation of itself, $o_{\text {self }}$, is its own absolute position and velocity. Each agent's observation of others, $o_{\text {others }}$, is the concatenation of the relative positions and velocities of all other agents with respect to itself.
Actions. Agents take actions from the discrete set do nothing, up, down, left, right, where the movement actions produce an instantaneous velocity (with inertia effects).</p>
<p>Goals and initial state assignment. With probability 0.2 , landmarks are given uniform random locations in the set $(-1,1)^{2}$, and agents are assigned initial positions uniformly at random within the set $(-1,1)^{2}$. With probability 0.8 , they are predefined as follows (see Figure 3). In "Antipodal", landmarks for agents 1 to 4 have $(x, y)$ coordinates [(0.9,0.9), (-0.9,-0.9), (0.9,-0.9), (-0.9,0.9)], while agents 1 to 4 are placed at [(-0.9,-0.9), (0.9,0.9), (-0.9,0.9), (0.9,-0.9)]. In "Intersection", landmark coordinates are [(0.9,-0.15), (-0.9,0.15), (0.15,0.9), (-0.15,-0.9)], while agents are placed at [(-0.9,0.15), (0.9,0.15), (0.15,-0.9), (-0.15,0.9)]. In "Merge", landmark coordinates are [(0.9,-0.2), (0.9,0.2)], while agents are [(-0.9,0.2), (-0.9,-0.2)]. Each agent's goal is the assigned landmark position vector.
Reward. At each time step, each agent's individual reward is the negative distance between its position and the position of its assigned landmark. If a collision occurs between any pair of agents, both agents receive an additional -1 penalty. A collision occurs when two agents' distance is less than the sum of their radius.</p>
<p>Termination. Episode terminates when all agents are less than 0.05 distance from assigned landmarks.</p>
<p>Induced MDP. This is the $N=1$ case of the Markov game, used by Stage 1 of CM3. The single agent only receives $o_{\text {self }}$. In each episode, its initial position and the assigned landmark's initial position are both uniform randomly chosen from $(-1,1)^{2}$.</p>
<h2>G. 2 SUMO</h2>
<p>We constructed a straight road of total length 200 m and width 12.8 m , consisting of four lanes. All lanes have width 3.2 m , and vehicles can be aligned along any of four sub-lanes within a lane, with lateral spacing 0.8 m . Vehicles are emitted at average speed $30 \mathrm{~m} / \mathrm{s}$ with small deviation. Simulation time resolution was $0.2 s$ per step. SUMO file merge_stage3_dense.rou.xml contains all vehicle parameters, and merge. net.xml defines the complete road architecture.</p>
<p>State. The global state vector $s$ is the concatenation of all agents' absolute position $(x, y)$, normalized respectively by the total length and width of the road, and horizontal speed $v$ normalized by $29 \mathrm{~m} / \mathrm{s}$.
Observation. Each agent observation of itself $o_{\text {self }}^{s}$ is a vector consisting of: agent speed normalized by $29 \mathrm{~m} / \mathrm{s}$, normalized number of sub-lanes between agent's current sub-lane and center sub-lane of goal lane, and normalized longitudinal distance to goal position. Each agent's observation of others $o_{\text {others }}^{s}$ is a discretized observation tensor of shape [13,9,2] centered on the agent, with two channels: binary indicator of vehicle occupancy, and normalized relative speed between agent and other vehicles. Each channel is a matrix with shape [13,9], corresponding to visibility of 15 m forward and backward (with resolution 2.5 m ) and four sub-lanes to the left and right.</p>
<p>Actions. All agents have the same discrete action space, consisting of five options: no-op (maintain current speed and lane), accelerate $\left(2.5 \mathrm{~m} / \mathrm{s}^{2}\right)$, decelerate $\left(-2.5 \mathrm{~m} / \mathrm{s}^{2}\right)$, shift one sub-lane to the left, shift one sub-lane to the right. Each agent's action $a^{n}$ is represented as a one-hot vector of length 5.
Goals and initial state assignment. Each goal vector $g^{n}$ is a one-hot vector of length 4, indicating the goal lane at which agent $n$ should arrive once it crosses position $x=190 \mathrm{~m}$. With probability 0.2 , agents are assigned goals uniformly at random, and agents are assigned initial lanes uniformly at</p>
<p>random at position $x \equiv 0$. With probability 0.8 , agent 1 's goal is lane 2 and agent 2 's goal is lane 1 , while agent 1 is initialized at lane 1 and agent 2 is initialized at lane 2 (see Figure 4). Departure times were drawn from a normal distribution with mean 0 s and standard deviation 0.5 s for each agent.</p>
<p>Reward. The reward $R\left(s_{t}, \mathbf{a}_{t}, g^{n}\right)$ for agent $n$ with goal $g^{n}$ is given according to the conditions: -1 for a collision; -10 for time-out (exceed 33 simulation steps during an episode); $10(1-\Delta)$ for reaching the end of the road and having a normalized sub-lane difference of $\Delta$ from the center of the goal lane; and -0.1 if current speed exceeds $35.7 \mathrm{~m} / \mathrm{s}$.</p>
<p>Termination. Episode terminates when 33 simulation steps have elapsed or all agents have $x&gt;190 \mathrm{~m}$.
Induced MDP. This is the $N=1$ case of the Markov game defined above, used by Stage 1 of CM3. The single agent receives only $o_{\text {self }}$. For each episode, agent initial and goal lanes are assigned uniformly at random from the available lanes.</p>
<h1>G. 3 CHECKERS</h1>
<p>This domain is adapted from the Checkers environment in Sunehag et al. (2018). It is a gridworld with 5 rows and 13 columns (Figure 2). Agents cannot move to the two highest and lowest rows and the two highest and lowest columns, which are placed for agents' finite observation grid to be well-defined. Agents cannot be in the same grid location. Red and yellow collectible reward are placed in a checkered pattern in the middle 3x8 region, and they disappear when any agent moves to their location.</p>
<p>State. The global state $s$ consists of two components. The first is $s_{T}$, a tensor of shape [3,9,2], where the two "channels" in the last dimension represents the presence/absence of red and yellow rewards as 1-hot matrices. The second is $s_{V}$, the concatenation of all agents' $(x, y)$ location (integer-valued) and the number of red and yellow each agent has collected so far.</p>
<p>Observation. Each agent's obsevation of others, $o_{\text {others }}^{n}$, is the concatenation of all other agents' normalized coordinates (normalized by total size of grid). An agent's observation of itself, $o_{\text {self }}^{n}$, consists of two components. First, $o_{\text {self }, V}^{n}$ is a vector concatenation of agent $n$ 's normalized coordinate and the number of red and yellow it has collected so far. Second, $o_{\text {self }, T}^{n}$ is a tensor of shape [5,5,3], centered on its current location in the grid. The tensor has three "channels", where the first two represent presence/absence of red and yellow rewards as 1-hot matrices, and the last channel indicates the invalid locations as a 1-hot matrix. The agent's own grid location is a valid location, while other agents' locations are invalid.</p>
<p>Actions. Agents choose from a discrete set of actions do-nothing, up, down, left, right. Movement actions transport the agent one grid cell in the chosen direction.</p>
<p>Goals. Agent A's goal is to collect all red rewards without touching yellow. Agent B's goal is to collect all yellow without touching red. The goal is represented as a 1-hot vector of length 2.</p>
<p>Reward. Agent A gets +1 for red, -0.5 for yellow. Agent B gets -0.5 for red, +1 for yellow.
Initial state distribution. Agent A is initialized at (2,8), Agent B is initialized at (4,8). (0,0) is the top-left cell (Figure 2).</p>
<p>Termination. Each episode finishes when either 75 time steps have elapsed, or when all rewards have been collected.</p>
<p>Induced MDP. For Stage 1 of CM3, the single agent is randomly assigned the role of either Agent A or Agent B in each episode. Everything else is defined as above.</p>
<h2>H ARCHITECTURE</h2>
<p>For all experiment domains, ReLU nonlinearity was used for all neural network layers unless otherwise specified. All layers are fully-connected feedforward layers, unless otherwise specified. All experiment domains have a discrete action space (with $|\mathcal{A}|=5$ actions), and action probabilities were computed by lower-bounding softmax outputs of all policy networks by $P\left(a^{n}=i\right)=(1-\epsilon) \operatorname{softmax}(i)+\epsilon /|\mathcal{A}|$, where $\epsilon$ is a decaying exploration parameter. To keep neural network architectures as similar as possible among all algorithms, our neural networks for</p>
<p>COMA differ from those of Foerster et al. (2018) in that we do not use recurrent networks, and we do not feed previous actions into the Q function. For the Q network in all implementations of COMA, the value of each output node $i$ is interpreted as the action-value $Q\left(s, a^{-n}, a^{n}=i, \mathbf{g}\right)$ for agent $n$ taking action $i$ and all other agents taking action $a^{-n}$. Also for COMA, agent $n$ 's label vector (one-hot indicator vector) and observation $o_{\text {self }}$ were used as input to COMA's global Q function, to differentiate between evaluations of the Q-function for different agents. These were choices in Foerster et al. (2018) that we retain.</p>
<h1>H. 1 COOPERATIVE NAVIGATION</h1>
<p>CM3. The policy network $\pi^{1}$ in Stage 1 feeds the concatenation of $o_{\text {self }}$ and goal $g$ to one layer with 64 units, which is connected to the special layer $h_{<em>}^{1}$ with 64 units, then connected to the softmax output layer with 5 units, each corresponding to one discrete action. In Stage 2, $o_{\text {others }}$ is connected to a new layer with 128 units, then connected to $h_{</em>}^{1}$.
The $Q^{1}$ function in Stage 1 feeds the concatenation of state $s$, goal $g$, and 1-hot action $a$ to one layer with 64 units, which is connected to the special layer $h_{<em>}^{1}$ with 64 units, then to a single linear output unit. In Stage 2, $Q^{1}$ is augmented into both $Q_{n}^{\boldsymbol{\pi}}(s, \mathbf{a})$ and $Q_{n}^{\boldsymbol{\pi}}\left(s, a^{m}\right)$ as separate networks. For $Q_{n}^{\boldsymbol{\pi}}(s, \mathbf{a}), s^{-n}$ (part of state $s$ excluding agent $n$ ) and $a^{-n}$ are concatenated and connected to a layer with 128 units, then connected to $h_{</em>}^{1}$. For $Q_{n}^{\boldsymbol{\pi}}\left(s, a^{m}\right), s^{m}$ (agent $m$ portion of state $s$ ) and $s^{-n}$ are concatenated and connected to a layer with 128 units, then connected to $h_{*}^{1}$.
IAC. IAC uses the same policy network as Stage 2 of CM3. The value function of IAC concatenates $o_{\text {self }}^{n}$ and goal $g^{n}$, connects to a layer with 64 units, which connects to a second layer $h_{2}$ with 64 units, then to a single linear output unit. $o_{\text {others }}^{n}$ is connected to a layer with 128 units, then connected to $h_{2}$.
COMA. COMA uses the same policy network as Stage 2 of CM3. The global Q function of COMA computes $Q\left(s,\left(a^{n}, a^{-n}\right)\right)$ for each agent $n$ as follows. Input is the concatenation of state $s$, all other agents' 1-hot actions $a^{-n}$, agent n's goal $g^{n}$, all other agent goals $g^{-n}$, agent label $n$, and agent $n$ 's observation $o_{\text {self }}^{n}$. This is passed through two layers of 128 units each, then connected to a linear output layer with 5 units.
QMIX. Individual value functions take input $\left(o_{\text {self }}^{n}, o_{\text {others }}^{n}, g^{n}\right)$ and connects to one hidden layer with 64 units, which connects to the output layer. The mixing network follows the exact architecture of Rashid et al. (2018) with embedding dimension 64.</p>
<h2>H. 2 SUMO</h2>
<p>CM3. The policy network $\pi^{1}$ during Stage 1 feeds each of the inputs $o_{\text {self }}$ and goal $g^{n}$ to a layer with 32 units. The concatenation is then connected to the layer $h_{<em>}^{1}$ with 64 units, and connected to a softmax output layer with 5 units, each corresponding to one discrete action. In Stage 2, the input observation grid $o_{\text {others }}^{n}$ is processed by a convolutional layer with 4 filters of size 5 x 3 and stride 1 x 1 , flattened and connected to a layer with 64 units, then connected to the layer $h_{</em>}^{1}$ of $\pi^{1}$.
The $Q^{1}$ function in Stage 1 feeds the concatenation of state $s$, goal $g$, and 1-hot action $a$ to one layer with 256 units, which is connected to the special layer $h_{<em>}^{1}$ with 256 units, then to a single linear output unit. In Stage 2, $Q^{1}$ is augmented into both $Q_{n}^{\boldsymbol{\pi}}(s, \mathbf{a})$ and $Q_{n}^{\boldsymbol{\pi}}\left(s, a^{m}\right)$ as separate networks. For $Q_{n}^{\boldsymbol{\pi}}(s, \mathbf{a}), s^{-n}$ (part of state $s$ excluding agent $n$ ), $a^{-n}$, and $g^{-n}$ are concatenated and connected to a layer with 128 units, then connected to $h_{</em>}^{1}$. For $Q_{n}^{\boldsymbol{\pi}}\left(s, a^{m}\right), s^{m}$ (agent $m$ portion of state $s$ ), $s^{-n}$, and $g^{-n}$ are concatenated and connected to a layer with 128 units, then connected to $h_{*}^{1}$.
IAC. IAC uses the same policy network as Stage 2 of CM3. The value function of IAC concatenates $o_{\text {self }}^{n}$ and $g^{n}$, feeds it into a layer with 64 units, which connects to a layer $h_{2}$ with 64 units, which connects to one linear output unit. $o_{\text {others }}^{n}$ is processed by a convolutional layer with 4 filters of size 5 x 3 and stride 1 x 1 , flattened and connected to a layer with 128 units, then connected to $h_{2}$.
COMA. COMA uses the same policy network as Stage 2 of CM3. The Q function of COMA is exactly the same as the one in COMA for cooperative navigation defined above.
QMIX. Individual value functions take input $\left(o_{\text {self }}^{n}, g^{n}\right)$ and connects to one hidden layer with 64 units, which connects to layer $h_{2}$ with 64 units. $o_{\text {others }}^{n}$ is passed through the same convolutional layer</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{5}$ Input $s^{m}$ is needed for disambiguation, so that input action $a^{m}$ is associated with agent $m$.
${ }^{6}$ Code for all experiments is available at https://github.com/011235813/cm3.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2029 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2029</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2029</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-49.html">extraction-schema-49</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum generation methods (especially LLM-based), curriculum learning approaches for compositional or interactive tasks, comparisons between different curriculum design methods, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-277066612</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2503.12759v2.pdf" target="_blank">RAG-RL: Advancing Retrieval-Augmented Generation via RL and Curriculum Learning</a></p>
                <p><strong>Paper Abstract:</strong> Retrieval-augmented generation (RAG) systems rely on retrieval models for identifying relevant contexts and answer generation models for utilizing those contexts. However, retrievers exhibit imperfect recall and precision, limiting downstream performance. We introduce RAG-RL, an answer generation model trained not only to produce answers but also to identify and cite relevant information from larger sets of retrieved contexts, shifting some of the burden of identifying relevant documents from the retriever to the answer generator. Our approach uses curriculum learning, where the model is first trained on easier examples that include only relevant contexts. Our experiments show that these training samples enable models to acquire citation and reasoning skills with greater sample efficiency and generalizability, demonstrating strong model performance even as the number of irrelevant passages increases. We benchmark our methods on three open-domain multi-hop question answering datasets and report significant gains in answer and citation accuracy. Our experiments provide empirical insights into how easier training samples can give models stronger signals for learning specific skills (e.g., citation generation) and how different components of post-training (e.g., training set construction, rule-based rewards, training sample ordering, etc.) impact final model performance.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2029.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2029.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum generation methods (especially LLM-based), curriculum learning approaches for compositional or interactive tasks, comparisons between different curriculum design methods, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Synthetic-distractor</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Synthetic difficulty (distractor-based) curriculum</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A rule-based curriculum that constructs training samples by varying the number of distractor (irrelevant) passages provided along with all gold passages; difficulty levels l ∈ {1..K} correspond to increasingly many distractors, with easiest samples having at most one distractor and hardest having the full retrieved set.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generation_method</strong></td>
                            <td>rule-based automated; difficulty-based ordering (synthetic difficulty by number of distractor passages)</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_method_description</strong></td>
                            <td>For each question Q with gold documents D+ and distractors D-, a sample S_l of difficulty l includes all gold documents and a subset of distractors of size determined by l (formal definition given in the paper). Easiest samples contain at most one distractor; hardest contain the full set of retrieved documents. The authors set K per dataset (K = 10 for HotpotQA, K = 20 for MuSiQue, K = 10 for 2Wiki) and construct curricula by selecting samples at specific l values according to curriculum variants (Max, Linear, Min-Max). The order of documents within a sample is shuffled to simulate realistic retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_environment</strong></td>
                            <td>open-domain multi-hop question answering (retrieval-augmented generation)</td>
                        </tr>
                        <tr>
                            <td><strong>is_interactive_text_environment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>is_compositional</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_description</strong></td>
                            <td>Multi-hop QA requiring j hops (mostly 2-hop, some 3- and 4-hop); compositional reasoning across multiple passages; task complexity parameterized by number of hops j and number of distractor documents k; synthetic difficulty only varies k while keeping all gold documents present. K values: HotpotQA=10, MuSiQue=20, 2Wiki=10.</td>
                        </tr>
                        <tr>
                            <td><strong>is_curriculum_adaptive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Compared against Max (all samples at difficulty K), Linear (linearly increasing difficulty 1→K), Min-Max (first half difficulty 1, second half difficulty K), and accuracy-based partitions (base-answerable / base-unanswerable). Also compared different sample ordering variants (sorted, shuffled, hops-then-augment).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Across three datasets (HotpotQA, MuSiQue, 2Wiki) and both evaluation settings (distractor and ideal retrieval), curricula that include easier samples (Linear, Min-Max) achieved the highest joint F1, improving over the Max curriculum by ~3 to 8 joint-F1 points (exact per-dataset values reported in Table 1). Baseline (no post-training) joint F1: HotpotQA = 45.55 (Answer F1 60.65, Citation F1 36.47), MuSiQue = 25.61 (Answer F1 25.88, Citation F1 25.35), 2Wiki = 44.03 (Answer F1 48.71, Citation F1 40.18). K and dataset sample sizes: main experiments used up to 5,000 training samples; larger-scale runs (HotpotQA: 40,000; MuSiQue: 19,900) confirmed trends.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_speed_comparison</strong></td>
                            <td>Qualitative: validation citation reward climbs faster for Min-Max curriculum, indicating greater sample efficiency for learning citation generation; authors report easier samples allow the model to acquire citation skills earlier. No exact convergence-epoch counts reported, only qualitative speed improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_performance</strong></td>
                            <td>Curricula that include easier samples (Min-Max, Linear) improve generalization to harder / distractor-rich settings relative to Max; both Min-Max and Linear yield largest gains on base-unanswerable held-out subsets but a performance gap remains between base-answerable and base-unanswerable evaluation splits—indicating limited generalization beyond the training distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_analysis</strong></td>
                            <td>Task diversity is controlled via number of distractors and hop counts; the paper does not present an explicit task-diversity metric comparison across curricula beyond showing performance across difficulty levels and hop counts.</td>
                        </tr>
                        <tr>
                            <td><strong>prerequisite_identification</strong></td>
                            <td>Implicit: authors find easier samples (fewer distractors) act as prerequisites for learning citation generation and therefore help later harder examples; no explicit automated prerequisite-discovery algorithm evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>intermediate_task_generation</strong></td>
                            <td>No—curriculum is constructed by selecting difficulty levels (no automated generation of intermediate bridging tasks beyond the pre-defined difficulty levels).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_limitations_observed</strong></td>
                            <td>Not applicable—LLMs were not used to generate curricula; the curriculum is algorithmically constructed.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Main experiments used up to 5,000 samples due to compute constraints; additional larger runs (40k, 19.9k) were done and trends held. GRPO hyperparameters: beta=0.01, clipping=0.2. No wall-clock or GPU-hour comparisons across curricula were reported.</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_evaluation</strong></td>
                            <td>None reported.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>Algorithmic synthetic curricula that include easier examples (fewer distractors) substantially improve RL-based post-training sample efficiency and final joint F1 for answer + citation generation; Min-Max (mix of easiest and hardest) and Linear curricula outperform training only on hardest samples (Max), with consistent improvements across datasets and evaluation settings.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2029.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2029.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum generation methods (especially LLM-based), curriculum learning approaches for compositional or interactive tasks, comparisons between different curriculum design methods, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Accuracy-based</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Accuracy-based difficulty (base-answerable / base-unanswerable) curriculum</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A curriculum partitioned by the base model's pre-posttraining capability: samples where the base model already answers correctly (base-answerable) versus those it cannot (base-unanswerable), determined by pass@k on the base model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generation_method</strong></td>
                            <td>rule-based automated; accuracy-based partitioning using base-model pass@k</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_method_description</strong></td>
                            <td>Compute pass@k for the base generation model for each training sample (they take maximum answer F1 across 8 generations). Partition the dataset into base-answerable (pass@k=1) and base-unanswerable (pass@k=0) subsets. Clip counts so both partitions have roughly equal sizes (~5,000 samples) for controlled comparisons. Use these subsets singly as training curricula in RL post-training experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_environment</strong></td>
                            <td>open-domain multi-hop question answering (retrieval-augmented generation)</td>
                        </tr>
                        <tr>
                            <td><strong>is_interactive_text_environment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>is_compositional</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_description</strong></td>
                            <td>Same multi-hop QA tasks; complexity implicitly encoded by whether the base model can already solve the instance (base-answerable) or not (base-unanswerable).</td>
                        </tr>
                        <tr>
                            <td><strong>is_curriculum_adaptive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Compared base-answerable-only vs base-unanswerable-only curricula, and also compared to synthetic-difficulty curricula (Max, Linear, Min-Max).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Models trained on base-answerable samples consistently outperform those trained on base-unanswerable samples across datasets and evaluation settings (distractor and ideal retrieval); full per-curriculum numbers presented in Table 4 and Appendix. The paper notes base-answerable training yields superior final F1.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_speed_comparison</strong></td>
                            <td>Qualitative: training on base-answerable samples leads to faster/better learning signal because rewards are less often uniform (avoids zero-advantage groups); base-unanswerable samples produce more uniform rewards, reducing gradient signal.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_performance</strong></td>
                            <td>Both Min-Max and Linear curricula yield larger relative gains on the base-unanswerable evaluation subset than on base-answerable, but a notable performance gap between the subsets remains, indicating limited generalization to instances the base model could not already solve.</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_analysis</strong></td>
                            <td>Not explicitly quantified beyond the partitioning by base-model success; no separate diversity metric reported.</td>
                        </tr>
                        <tr>
                            <td><strong>prerequisite_identification</strong></td>
                            <td>Authors argue that base-answerable samples align better with the base model's pretrained distribution and act as helpful prerequisites; no algorithmic prerequisite extraction beyond this partitioning was evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>intermediate_task_generation</strong></td>
                            <td>No explicit intermediate task generation—only selection of existing examples into two partitions.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_limitations_observed</strong></td>
                            <td>Not applicable—LLMs not used for curriculum generation.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Constructing accuracy-based partitions requires running the base model across up to 40,000 samples (they sample up to 40k to partition), which has compute cost but no exact resource numbers reported.</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_evaluation</strong></td>
                            <td>None reported.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>Partitioning by base-model accuracy reveals that training on base-answerable samples is more effective for RL post-training than training solely on base-unanswerable samples; base-unanswerable samples can reduce learning signal (uniform rewards) and hinder effective policy updates.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2029.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2029.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum generation methods (especially LLM-based), curriculum learning approaches for compositional or interactive tasks, comparisons between different curriculum design methods, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Curriculum-variants</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Max / Linear / Min-Max curriculum variants</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Three concrete curriculum schedule designs evaluated: Max (all samples at maximum difficulty K), Linear (difficulty increases linearly across the training set from 1 to K), and Min-Max (first half of training set easiest difficulty 1, second half hardest difficulty K).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generation_method</strong></td>
                            <td>hand-specified schedule over synthetic difficulty levels: Max, Linear, Min-Max (rule-based schedule designs)</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_method_description</strong></td>
                            <td>C_max(i)=K for all i (Max). C_linear(i) maps training index i to a difficulty linearly spaced between 1 and K. C_min-max(i)=1 for i ≤ n/2 and =K for i > n/2. These schedules are applied to the synthetic-distractor sample pool.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_environment</strong></td>
                            <td>open-domain multi-hop QA (RAG answer generation)</td>
                        </tr>
                        <tr>
                            <td><strong>is_interactive_text_environment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>is_compositional</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_description</strong></td>
                            <td>Complexity measured by distractor count per sample and number of hops; Max represents only hardest examples (highest distractor counts), Linear represents gradual progression through intermediate difficulties, Min-Max uses extremes.</td>
                        </tr>
                        <tr>
                            <td><strong>is_curriculum_adaptive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Max is used as fine-tuned baseline; comparisons made against Linear and Min-Max (and accuracy-based partitions).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Empirical finding: Linear and Min-Max curricula generally achieve the highest joint-F1 on answer + citation across datasets and evaluation settings, outperforming Max by approximately 3–8 joint-F1 points (Table 1). On MuSiQue the Min-Max curriculum's validation citation reward rose faster in training (figure cited), and an ablation showed Min-Max with formatting-only rewards outperformed Max with full rewards on MuSiQue (Table 5). Exact per-curriculum numbers are reported in Tables 1 and 4 of the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_speed_comparison</strong></td>
                            <td>Min-Max showed faster improvement in citation reward during training (qualitative plot); authors note Min-Max helps learn citation generation quicker than Max.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_performance</strong></td>
                            <td>Min-Max and Linear show better generalization in distractor-rich and ideal retrieval settings compared to Max; gains observed on held-out base-unanswerable subsets but gaps remain.</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_analysis</strong></td>
                            <td>Not separately quantified; differences arise from inclusion of easy vs hard samples.</td>
                        </tr>
                        <tr>
                            <td><strong>prerequisite_identification</strong></td>
                            <td>Implicit: Min-Max leveraging easiest examples first provides prerequisite signal; Linear (gradual) sometimes underperforms Min-Max, suggesting monotonic difficulty ramping is not always optimal.</td>
                        </tr>
                        <tr>
                            <td><strong>intermediate_task_generation</strong></td>
                            <td>No automated intermediate tasks—Linear uses intermediate difficulties by partitioning existing samples.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_limitations_observed</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>No per-curriculum compute/time breakdown; all curricula trained with same RL procedure and hyperparameters (GRPO).</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_evaluation</strong></td>
                            <td>None reported.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>Contrary to canonical easy→hard curriculum wisdom, Min-Max (easy then hard) often outperforms Linear (gradual easy→hard), suggesting that including easiest examples and then jumping to hardest examples can be more effective than slowly increasing difficulty in RL-based post-training for RAG answer generation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2029.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2029.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum generation methods (especially LLM-based), curriculum learning approaches for compositional or interactive tasks, comparisons between different curriculum design methods, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Ordering-variants</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sample ordering experiments (sorted vs shuffled vs hops-then-augment)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Experiments comparing different orderings of the same curriculum content: (i) linear curriculum with samples sorted by distractor count, (ii) linear curriculum with samples randomly shuffled, and (iii) linear curriculum ordered by number of hops then augmented to span the linear difficulty range.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generation_method</strong></td>
                            <td>experimentally varied ordering of precomputed difficulty-labeled samples (sorted, randomized, hop-first ordering)</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_method_description</strong></td>
                            <td>Authors implement three presentation orderings for the Linear curriculum: strictly sorted by measured difficulty (number of distractors), randomly shuffled order, and samples first ordered by hop count then augmented across difficulty levels. They then compare final F1 across these ordering variants.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_environment</strong></td>
                            <td>open-domain multi-hop QA (RAG generation)</td>
                        </tr>
                        <tr>
                            <td><strong>is_interactive_text_environment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>is_compositional</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_description</strong></td>
                            <td>Same multi-hop complexity; ordering changes when different difficulty/hop samples are seen during training.</td>
                        </tr>
                        <tr>
                            <td><strong>is_curriculum_adaptive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Compared sorted Linear vs shuffled Linear vs hops-then-augment Linear.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Result: Final F1 scores across these ordering variants are broadly comparable (Table 3); authors conclude specific ordering of training examples (within the studied linear schedule) does not yield consistent or significant performance differences.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_speed_comparison</strong></td>
                            <td>No significant ordering-dependent differences in convergence reported; authors report broadly comparable F1s.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_performance</strong></td>
                            <td>No ordering-dependent generalization advantages observed.</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_analysis</strong></td>
                            <td>No explicit measurement beyond comparing the different orderings.</td>
                        </tr>
                        <tr>
                            <td><strong>prerequisite_identification</strong></td>
                            <td>Not specifically evaluated beyond the conclusion that strict easy→hard ordering gave no consistent benefit here.</td>
                        </tr>
                        <tr>
                            <td><strong>intermediate_task_generation</strong></td>
                            <td>No.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_limitations_observed</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>No ordering-dependent compute differences reported.</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_evaluation</strong></td>
                            <td>None reported.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>In this RAG RL post-training setting, the exact ordering of training examples (within a linear curriculum) is not a dominant factor; randomizing or ordering by hops yields similar final performance to strictly sorted easy→hard ordering.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2029.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2029.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum generation methods (especially LLM-based), curriculum learning approaches for compositional or interactive tasks, comparisons between different curriculum design methods, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Formatting-only ablation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Formatting-reward-only curriculum ablation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ablation study where RL post-training uses only formatting rewards (encouraging correct XML tags/format) and omits explicit answer and citation accuracy rewards, to test whether curriculum choice alone improves answer/citation performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_generation_method</strong></td>
                            <td>standard synthetic curricula (Min-Max, Max, etc.) applied while using a restricted reward model (formatting-only)</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_method_description</strong></td>
                            <td>Train with GRPO but only R_formatting (reward for correct output format and penalty for non-English/excess text) enabled; compare results using Min-Max vs Max curricula to isolate curriculum effect from accuracy-based rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_environment</strong></td>
                            <td>open-domain multi-hop QA (RAG generation)</td>
                        </tr>
                        <tr>
                            <td><strong>is_interactive_text_environment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>is_compositional</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_description</strong></td>
                            <td>Same as main tasks; ablation isolates reward signal rather than task complexity change.</td>
                        </tr>
                        <tr>
                            <td><strong>is_curriculum_adaptive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparisons</strong></td>
                            <td>Formatting-only Min-Max vs Max with full rewards; also compared to Max with only formatting and Max with full rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>On MuSiQue, formatting-only training using the Min-Max curriculum achieved higher answer F1 than the Max curriculum trained with all rewards (Table 5). Authors conclude curriculum selection can be as important as reward design for sample efficiency and final performance.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_speed_comparison</strong></td>
                            <td>Formatting-only + Min-Max shows improved sample efficiency (qualitatively) compared to Max with full rewards; exact epoch/step counts not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_performance</strong></td>
                            <td>Not separately quantified beyond MuSiQue ablation showing improved answer F1 for Min-Max even with formatting-only rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>task_diversity_analysis</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>prerequisite_identification</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>intermediate_task_generation</strong></td>
                            <td>No.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_limitations_observed</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Same RL training regime; no additional cost breakdown provided.</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_evaluation</strong></td>
                            <td>None reported.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>Choosing an appropriate curriculum (Min-Max) can meaningfully improve performance even when using limited reward signals (formatting-only), indicating curriculum design can be as crucial as reward engineering in RL post-training for RAG.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Curriculum Learning <em>(Rating: 2)</em></li>
                <li>Curriculum learning for reinforcement learning domains: A framework and survey <em>(Rating: 2)</em></li>
                <li>Self-improving transformers overcome easy-to-hard and length generalization challenges <em>(Rating: 2)</em></li>
                <li>Group Relative Policy Optimization (GRPO) <em>(Rating: 1)</em></li>
                <li>Logic-rl: Unleashing llm reasoning with rule-based reinforcement learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2029",
    "paper_id": "paper-277066612",
    "extraction_schema_id": "extraction-schema-49",
    "extracted_data": [
        {
            "name_short": "Synthetic-distractor",
            "name_full": "Synthetic difficulty (distractor-based) curriculum",
            "brief_description": "A rule-based curriculum that constructs training samples by varying the number of distractor (irrelevant) passages provided along with all gold passages; difficulty levels l ∈ {1..K} correspond to increasingly many distractors, with easiest samples having at most one distractor and hardest having the full retrieved set.",
            "citation_title": "here",
            "mention_or_use": "use",
            "curriculum_generation_method": "rule-based automated; difficulty-based ordering (synthetic difficulty by number of distractor passages)",
            "curriculum_method_description": "For each question Q with gold documents D+ and distractors D-, a sample S_l of difficulty l includes all gold documents and a subset of distractors of size determined by l (formal definition given in the paper). Easiest samples contain at most one distractor; hardest contain the full set of retrieved documents. The authors set K per dataset (K = 10 for HotpotQA, K = 20 for MuSiQue, K = 10 for 2Wiki) and construct curricula by selecting samples at specific l values according to curriculum variants (Max, Linear, Min-Max). The order of documents within a sample is shuffled to simulate realistic retrieval.",
            "llm_model_used": null,
            "domain_environment": "open-domain multi-hop question answering (retrieval-augmented generation)",
            "is_interactive_text_environment": false,
            "is_compositional": true,
            "task_complexity_description": "Multi-hop QA requiring j hops (mostly 2-hop, some 3- and 4-hop); compositional reasoning across multiple passages; task complexity parameterized by number of hops j and number of distractor documents k; synthetic difficulty only varies k while keeping all gold documents present. K values: HotpotQA=10, MuSiQue=20, 2Wiki=10.",
            "is_curriculum_adaptive": false,
            "baseline_comparisons": "Compared against Max (all samples at difficulty K), Linear (linearly increasing difficulty 1→K), Min-Max (first half difficulty 1, second half difficulty K), and accuracy-based partitions (base-answerable / base-unanswerable). Also compared different sample ordering variants (sorted, shuffled, hops-then-augment).",
            "performance_metrics": "Across three datasets (HotpotQA, MuSiQue, 2Wiki) and both evaluation settings (distractor and ideal retrieval), curricula that include easier samples (Linear, Min-Max) achieved the highest joint F1, improving over the Max curriculum by ~3 to 8 joint-F1 points (exact per-dataset values reported in Table 1). Baseline (no post-training) joint F1: HotpotQA = 45.55 (Answer F1 60.65, Citation F1 36.47), MuSiQue = 25.61 (Answer F1 25.88, Citation F1 25.35), 2Wiki = 44.03 (Answer F1 48.71, Citation F1 40.18). K and dataset sample sizes: main experiments used up to 5,000 training samples; larger-scale runs (HotpotQA: 40,000; MuSiQue: 19,900) confirmed trends.",
            "learning_speed_comparison": "Qualitative: validation citation reward climbs faster for Min-Max curriculum, indicating greater sample efficiency for learning citation generation; authors report easier samples allow the model to acquire citation skills earlier. No exact convergence-epoch counts reported, only qualitative speed improvement.",
            "generalization_performance": "Curricula that include easier samples (Min-Max, Linear) improve generalization to harder / distractor-rich settings relative to Max; both Min-Max and Linear yield largest gains on base-unanswerable held-out subsets but a performance gap remains between base-answerable and base-unanswerable evaluation splits—indicating limited generalization beyond the training distribution.",
            "task_diversity_analysis": "Task diversity is controlled via number of distractors and hop counts; the paper does not present an explicit task-diversity metric comparison across curricula beyond showing performance across difficulty levels and hop counts.",
            "prerequisite_identification": "Implicit: authors find easier samples (fewer distractors) act as prerequisites for learning citation generation and therefore help later harder examples; no explicit automated prerequisite-discovery algorithm evaluated.",
            "intermediate_task_generation": "No—curriculum is constructed by selecting difficulty levels (no automated generation of intermediate bridging tasks beyond the pre-defined difficulty levels).",
            "llm_limitations_observed": "Not applicable—LLMs were not used to generate curricula; the curriculum is algorithmically constructed.",
            "computational_cost": "Main experiments used up to 5,000 samples due to compute constraints; additional larger runs (40k, 19.9k) were done and trends held. GRPO hyperparameters: beta=0.01, clipping=0.2. No wall-clock or GPU-hour comparisons across curricula were reported.",
            "human_expert_evaluation": "None reported.",
            "key_findings_summary": "Algorithmic synthetic curricula that include easier examples (fewer distractors) substantially improve RL-based post-training sample efficiency and final joint F1 for answer + citation generation; Min-Max (mix of easiest and hardest) and Linear curricula outperform training only on hardest samples (Max), with consistent improvements across datasets and evaluation settings.",
            "uuid": "e2029.0"
        },
        {
            "name_short": "Accuracy-based",
            "name_full": "Accuracy-based difficulty (base-answerable / base-unanswerable) curriculum",
            "brief_description": "A curriculum partitioned by the base model's pre-posttraining capability: samples where the base model already answers correctly (base-answerable) versus those it cannot (base-unanswerable), determined by pass@k on the base model.",
            "citation_title": "here",
            "mention_or_use": "use",
            "curriculum_generation_method": "rule-based automated; accuracy-based partitioning using base-model pass@k",
            "curriculum_method_description": "Compute pass@k for the base generation model for each training sample (they take maximum answer F1 across 8 generations). Partition the dataset into base-answerable (pass@k=1) and base-unanswerable (pass@k=0) subsets. Clip counts so both partitions have roughly equal sizes (~5,000 samples) for controlled comparisons. Use these subsets singly as training curricula in RL post-training experiments.",
            "llm_model_used": null,
            "domain_environment": "open-domain multi-hop question answering (retrieval-augmented generation)",
            "is_interactive_text_environment": false,
            "is_compositional": true,
            "task_complexity_description": "Same multi-hop QA tasks; complexity implicitly encoded by whether the base model can already solve the instance (base-answerable) or not (base-unanswerable).",
            "is_curriculum_adaptive": false,
            "baseline_comparisons": "Compared base-answerable-only vs base-unanswerable-only curricula, and also compared to synthetic-difficulty curricula (Max, Linear, Min-Max).",
            "performance_metrics": "Models trained on base-answerable samples consistently outperform those trained on base-unanswerable samples across datasets and evaluation settings (distractor and ideal retrieval); full per-curriculum numbers presented in Table 4 and Appendix. The paper notes base-answerable training yields superior final F1.",
            "learning_speed_comparison": "Qualitative: training on base-answerable samples leads to faster/better learning signal because rewards are less often uniform (avoids zero-advantage groups); base-unanswerable samples produce more uniform rewards, reducing gradient signal.",
            "generalization_performance": "Both Min-Max and Linear curricula yield larger relative gains on the base-unanswerable evaluation subset than on base-answerable, but a notable performance gap between the subsets remains, indicating limited generalization to instances the base model could not already solve.",
            "task_diversity_analysis": "Not explicitly quantified beyond the partitioning by base-model success; no separate diversity metric reported.",
            "prerequisite_identification": "Authors argue that base-answerable samples align better with the base model's pretrained distribution and act as helpful prerequisites; no algorithmic prerequisite extraction beyond this partitioning was evaluated.",
            "intermediate_task_generation": "No explicit intermediate task generation—only selection of existing examples into two partitions.",
            "llm_limitations_observed": "Not applicable—LLMs not used for curriculum generation.",
            "computational_cost": "Constructing accuracy-based partitions requires running the base model across up to 40,000 samples (they sample up to 40k to partition), which has compute cost but no exact resource numbers reported.",
            "human_expert_evaluation": "None reported.",
            "key_findings_summary": "Partitioning by base-model accuracy reveals that training on base-answerable samples is more effective for RL post-training than training solely on base-unanswerable samples; base-unanswerable samples can reduce learning signal (uniform rewards) and hinder effective policy updates.",
            "uuid": "e2029.1"
        },
        {
            "name_short": "Curriculum-variants",
            "name_full": "Max / Linear / Min-Max curriculum variants",
            "brief_description": "Three concrete curriculum schedule designs evaluated: Max (all samples at maximum difficulty K), Linear (difficulty increases linearly across the training set from 1 to K), and Min-Max (first half of training set easiest difficulty 1, second half hardest difficulty K).",
            "citation_title": "here",
            "mention_or_use": "use",
            "curriculum_generation_method": "hand-specified schedule over synthetic difficulty levels: Max, Linear, Min-Max (rule-based schedule designs)",
            "curriculum_method_description": "C_max(i)=K for all i (Max). C_linear(i) maps training index i to a difficulty linearly spaced between 1 and K. C_min-max(i)=1 for i ≤ n/2 and =K for i &gt; n/2. These schedules are applied to the synthetic-distractor sample pool.",
            "llm_model_used": null,
            "domain_environment": "open-domain multi-hop QA (RAG answer generation)",
            "is_interactive_text_environment": false,
            "is_compositional": true,
            "task_complexity_description": "Complexity measured by distractor count per sample and number of hops; Max represents only hardest examples (highest distractor counts), Linear represents gradual progression through intermediate difficulties, Min-Max uses extremes.",
            "is_curriculum_adaptive": false,
            "baseline_comparisons": "Max is used as fine-tuned baseline; comparisons made against Linear and Min-Max (and accuracy-based partitions).",
            "performance_metrics": "Empirical finding: Linear and Min-Max curricula generally achieve the highest joint-F1 on answer + citation across datasets and evaluation settings, outperforming Max by approximately 3–8 joint-F1 points (Table 1). On MuSiQue the Min-Max curriculum's validation citation reward rose faster in training (figure cited), and an ablation showed Min-Max with formatting-only rewards outperformed Max with full rewards on MuSiQue (Table 5). Exact per-curriculum numbers are reported in Tables 1 and 4 of the paper.",
            "learning_speed_comparison": "Min-Max showed faster improvement in citation reward during training (qualitative plot); authors note Min-Max helps learn citation generation quicker than Max.",
            "generalization_performance": "Min-Max and Linear show better generalization in distractor-rich and ideal retrieval settings compared to Max; gains observed on held-out base-unanswerable subsets but gaps remain.",
            "task_diversity_analysis": "Not separately quantified; differences arise from inclusion of easy vs hard samples.",
            "prerequisite_identification": "Implicit: Min-Max leveraging easiest examples first provides prerequisite signal; Linear (gradual) sometimes underperforms Min-Max, suggesting monotonic difficulty ramping is not always optimal.",
            "intermediate_task_generation": "No automated intermediate tasks—Linear uses intermediate difficulties by partitioning existing samples.",
            "llm_limitations_observed": "Not applicable.",
            "computational_cost": "No per-curriculum compute/time breakdown; all curricula trained with same RL procedure and hyperparameters (GRPO).",
            "human_expert_evaluation": "None reported.",
            "key_findings_summary": "Contrary to canonical easy→hard curriculum wisdom, Min-Max (easy then hard) often outperforms Linear (gradual easy→hard), suggesting that including easiest examples and then jumping to hardest examples can be more effective than slowly increasing difficulty in RL-based post-training for RAG answer generation.",
            "uuid": "e2029.2"
        },
        {
            "name_short": "Ordering-variants",
            "name_full": "Sample ordering experiments (sorted vs shuffled vs hops-then-augment)",
            "brief_description": "Experiments comparing different orderings of the same curriculum content: (i) linear curriculum with samples sorted by distractor count, (ii) linear curriculum with samples randomly shuffled, and (iii) linear curriculum ordered by number of hops then augmented to span the linear difficulty range.",
            "citation_title": "here",
            "mention_or_use": "use",
            "curriculum_generation_method": "experimentally varied ordering of precomputed difficulty-labeled samples (sorted, randomized, hop-first ordering)",
            "curriculum_method_description": "Authors implement three presentation orderings for the Linear curriculum: strictly sorted by measured difficulty (number of distractors), randomly shuffled order, and samples first ordered by hop count then augmented across difficulty levels. They then compare final F1 across these ordering variants.",
            "llm_model_used": null,
            "domain_environment": "open-domain multi-hop QA (RAG generation)",
            "is_interactive_text_environment": false,
            "is_compositional": true,
            "task_complexity_description": "Same multi-hop complexity; ordering changes when different difficulty/hop samples are seen during training.",
            "is_curriculum_adaptive": false,
            "baseline_comparisons": "Compared sorted Linear vs shuffled Linear vs hops-then-augment Linear.",
            "performance_metrics": "Result: Final F1 scores across these ordering variants are broadly comparable (Table 3); authors conclude specific ordering of training examples (within the studied linear schedule) does not yield consistent or significant performance differences.",
            "learning_speed_comparison": "No significant ordering-dependent differences in convergence reported; authors report broadly comparable F1s.",
            "generalization_performance": "No ordering-dependent generalization advantages observed.",
            "task_diversity_analysis": "No explicit measurement beyond comparing the different orderings.",
            "prerequisite_identification": "Not specifically evaluated beyond the conclusion that strict easy→hard ordering gave no consistent benefit here.",
            "intermediate_task_generation": "No.",
            "llm_limitations_observed": "Not applicable.",
            "computational_cost": "No ordering-dependent compute differences reported.",
            "human_expert_evaluation": "None reported.",
            "key_findings_summary": "In this RAG RL post-training setting, the exact ordering of training examples (within a linear curriculum) is not a dominant factor; randomizing or ordering by hops yields similar final performance to strictly sorted easy→hard ordering.",
            "uuid": "e2029.3"
        },
        {
            "name_short": "Formatting-only ablation",
            "name_full": "Formatting-reward-only curriculum ablation",
            "brief_description": "An ablation study where RL post-training uses only formatting rewards (encouraging correct XML tags/format) and omits explicit answer and citation accuracy rewards, to test whether curriculum choice alone improves answer/citation performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "curriculum_generation_method": "standard synthetic curricula (Min-Max, Max, etc.) applied while using a restricted reward model (formatting-only)",
            "curriculum_method_description": "Train with GRPO but only R_formatting (reward for correct output format and penalty for non-English/excess text) enabled; compare results using Min-Max vs Max curricula to isolate curriculum effect from accuracy-based rewards.",
            "llm_model_used": null,
            "domain_environment": "open-domain multi-hop QA (RAG generation)",
            "is_interactive_text_environment": false,
            "is_compositional": true,
            "task_complexity_description": "Same as main tasks; ablation isolates reward signal rather than task complexity change.",
            "is_curriculum_adaptive": false,
            "baseline_comparisons": "Formatting-only Min-Max vs Max with full rewards; also compared to Max with only formatting and Max with full rewards.",
            "performance_metrics": "On MuSiQue, formatting-only training using the Min-Max curriculum achieved higher answer F1 than the Max curriculum trained with all rewards (Table 5). Authors conclude curriculum selection can be as important as reward design for sample efficiency and final performance.",
            "learning_speed_comparison": "Formatting-only + Min-Max shows improved sample efficiency (qualitatively) compared to Max with full rewards; exact epoch/step counts not reported.",
            "generalization_performance": "Not separately quantified beyond MuSiQue ablation showing improved answer F1 for Min-Max even with formatting-only rewards.",
            "task_diversity_analysis": "Not applicable.",
            "prerequisite_identification": "Not applicable.",
            "intermediate_task_generation": "No.",
            "llm_limitations_observed": "Not applicable.",
            "computational_cost": "Same RL training regime; no additional cost breakdown provided.",
            "human_expert_evaluation": "None reported.",
            "key_findings_summary": "Choosing an appropriate curriculum (Min-Max) can meaningfully improve performance even when using limited reward signals (formatting-only), indicating curriculum design can be as crucial as reward engineering in RL post-training for RAG.",
            "uuid": "e2029.4"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Curriculum Learning",
            "rating": 2
        },
        {
            "paper_title": "Curriculum learning for reinforcement learning domains: A framework and survey",
            "rating": 2
        },
        {
            "paper_title": "Self-improving transformers overcome easy-to-hard and length generalization challenges",
            "rating": 2
        },
        {
            "paper_title": "Group Relative Policy Optimization (GRPO)",
            "rating": 1
        },
        {
            "paper_title": "Logic-rl: Unleashing llm reasoning with rule-based reinforcement learning",
            "rating": 1
        }
    ],
    "cost": 0.01567425,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>RAG-RL: Advancing Retrieval-Augmented Generation via RL and Curriculum Learning
23 May 2025</p>
<p>Jerry Huang jerry8@illinois.edu 
University of Illinois at Urbana
Champaign</p>
<p>Siddarth Madala 
University of Illinois at Urbana
Champaign</p>
<p>Risham Sidhu 
University of Illinois at Urbana
Champaign</p>
<p>Cheng Niu 
NewsBreak</p>
<p>Hao Peng 
University of Illinois at Urbana
Champaign</p>
<p>Julia Hockenmaier 
University of Illinois at Urbana
Champaign</p>
<p>Tong Zhang tozhang@illinois.edu 
University of Illinois at Urbana
Champaign</p>
<p>RAG-RL: Advancing Retrieval-Augmented Generation via RL and Curriculum Learning
23 May 20256ACDAABE128FF39C7926D38F4FA1BFE0arXiv:2503.12759v2[cs.CL]
Retrieval-augmented generation (RAG) systems rely on retrieval models for identifying relevant contexts and answer generation models for utilizing those contexts.However, retrievers exhibit imperfect recall and precision, limiting downstream performance.We introduce RAG-RL, an answer generation model trained not only to produce answers but also to identify and cite relevant information from larger sets of retrieved contexts, shifting some of the burden of identifying relevant documents from the retriever to the answer generator.Our approach uses curriculum learning, where the model is first trained on easier examples that include only relevant contexts.Our experiments show that these training samples enable models to acquire citation and reasoning skills with greater sample efficiency and generalizability, demonstrating strong model performance even as the number of irrelevant passages increases.We benchmark our methods on three open-domain multi-hop question answering datasets and report significant gains in answer and citation accuracy.Our experiments provide empirical insights into how easier training samples can give models stronger signals for learning specific skills (e.g., citation generation) and how different components of post-training (e.g., training set construction, rule-based rewards, training sample ordering, etc.) impact final model performance.</p>
<p>Introduction</p>
<p>Retrieval-augmented generation (RAG; Guu et al., 2020;Lewis et al., 2020;Wang et al., 2024a) relies on retrieval and generation models working together to retrieve and integrate external contexts effectively for answering questions or generating content.While previous works have made significant progress in improving these systems by opti-Question: Who is the maternal grandfather of Floris De Voogd?</p>
<p>Hop 1: Title: Floris de Voogd "Floris de Voogd (c. 1228Voogd (c. -1258) )  mizing retrieval and reranking models (Zhang et al., 2024a;Gutiérrez et al., 2025;Weller et al., 2025), challenges persist when it comes to retrieving relevant real-world contexts that require reasoning, especially those whose relevance goes beyond semantic similarity (Su et al., 2024).Moreover, prior work has demonstrated that generative models often struggle to effectively integrate information across multiple documents, a limitation attributed to their constrained reasoning capabilities, particularly in domains such as code generation (Wang et al., 2024b) and in settings involving long-context retrieval (Zhou et al., 2025;Yen et al., 2025).</p>
<p>In this work, we tackle the aforementioned challenges by training reasoning language models (RLMs) capable of performing reasoning over a greater number of retrieved documents.Prior approaches for improving RAG have focused on optimizing the retrieval and reranking components by maximizing metrics such as recall@5.In contrast, we propose shifting some of the retrieval burden from the retriever to the generation model itself.</p>
<p>An answer generation model that can effectively differentiate between relevant and irrelevant contexts, when given a longer list of retrieved passages, would reduce dependence on high-precision retrieval and increase recall by instead maximizing metrics such as recall@10 or recall@25 (Jin et al., 2025).</p>
<p>Building on the recent success of reinforcement learning (RL) in enhancing the reasoning capabilities of LLMs in the domains of mathematics and coding (Wei et al., 2025;Xie et al., 2025), we apply Group Relative Policy Optimization (GRPO; Shao et al., 2024) with simple, rule-based rewards to improve the answer generation component of RAG systems.We show that RAG-RL achieves substantial performance gains in both answer and citation generation on three open-domain multi-hop question-answer datasets.Furthermore, our comprehensive evaluation demonstrates that RAG-RL achieves improved performance both in distractorrich and gold-only settings.1These settings respectively mirror the use of a weaker retrieval model or a more advanced retrieval and/or reranking systems, and demonstrate that RAG-RL can be used in conjunction with past works on improving retrieval models for further improved performance.</p>
<p>We also conduct a comprehensive study on how different curriculum learning settings affect model performance in post-training.Specifically, we study the effectiveness of introducing questionanswer training samples of varying difficulty levels and the impact that the ordering of the training set has on the final performance of the model.We observe that (1) adding easier samples during training teaches the model to more quickly learn how to generate citations as the model no longer has to identify which contexts are relevant, (2) curricula that scale problem difficulty linearly from easiest to hardest perform worse when compared to min-max curricula that begin with the easiest samples and jump straight to the hardest samples, and (3) the benefits of deliberately ordering training samples from easiest to hardest, as proposed by previous curriculum learning studies (Bengio et al., 2009) are not conclusively supported in RLbased post-training.These empirical observations suggest that constructing training sets of different difficulty levels can increase sample efficiency and generalization by targeting specific skills.</p>
<p>In summary, the main contributions of this work are as follows:</p>
<p>• We introduce RAG-RL, an RLM specifically trained for answer generation in RAG, using RL and curriculum learning.</p>
<p>• We benchmark a comprehensive set of different curriculum construction and curriculum learning settings.</p>
<p>• We provide several empirical insights on the effectiveness of different curriculum learning and curriculum construction settings, and how different aspects of the post-training process contribute to final model performance.</p>
<p>2 Related Works</p>
<p>RAG Systems</p>
<p>Rather than relying solely on parametric knowledge, RAG has been widely used in tasks that require external information (Guu et al., 2020;Lewis et al., 2020;Wang et al., 2024a).Previous works have made tremendous progress in designing and training sophisticated retrieval and reranking models (Gutiérrez et al., 2025;Weller et al., 2025) for open-domain question answering (Chen et al., 2017).One important line of work has focused on improving the encoder models that are used in the embedding generation process (Lee et al., 2025a;Muennighoff et al., 2025), while another has focused on designing retrieval systems that focus on drawing connections between multiple different documents (Guo et al., 2024;Gutiérrez et al., 2025).Rank1 (Weller et al., 2025) has also recently demonstrated that allocating test-time compute for document reranking can lead to performance improvements when retrieving contexts that require in-depth reasoning.Past work has also sought to take advantage of the long context lengths of modern-day LLMs by providing these models with larger sets of retrieved documents, but have shown that these models struggle to effectively identify relevant contexts as the number of retrieved passages increases (Jin et al., 2025;Zhou et al., 2025;Yen et al., 2025).</p>
<p>Multi-Hop Question Answering</p>
<p>A multi-hop question requires combining information across multiple passages and performing reasoning to arrive at a correct answer (Mavi et al., 2024;Nishida et al., 2019).The number of pieces of information required to successfully answer the question is referred to as the number of hops.</p>
<p>The terms passages and documents are used interchangeably to denote disjoint contexts that are retrieved by a retrieval model.Figure 1 demonstrates how RAG-RL operates in the multi-hop questionanswering setting.</p>
<p>Reasoning Language Models</p>
<p>With the introduction of RLMs in OpenAI's o1 models (OpenAI et al., 2024), the research community has made progress in replicating similar models that have shown impressive performance in tasks that require reasoning, driven in part due to R1's release (DeepSeek-AI et al., 2025).Prior works have demonstrated the potential for training smaller-scale RLMs in the domains of mathematics, logic, and coding (Xie et al., 2025;Wei et al., 2025) and have also achieved impressive performance.However, to the best of our knowledge, no one has trained RLMs specifically for the answer generation component of RAG.</p>
<p>Curriculum Learning</p>
<p>Curriculum learning (Bengio et al., 2009) has been extensively studied as a training paradigm that orders training samples by increasing difficulty, leading to improved generalization.In question answering (QA), it has been used to reduce distributional shifts between pre-training and downstream fine-tuning datasets (Zhang et al., 2024b).Recent advances in LLMs have incorporated curriculum-inspired self-improvement mechanisms (Lee et al., 2025b), where models iteratively augment their training data with instances they can already solve, to facilitate generalization to slightly more complex reasoning tasks.In RL, curriculum learning has also been applied to gradually expose agents to more challenging environments (Narvekar et al., 2020); however, its effectiveness remains task-dependent, with some studies reporting only marginal gains (Xie et al., 2025).</p>
<p>RAG-RL</p>
<p>In this section, we include a detailed overview of the training process for RAG-RL.We outline the rule-based rewards used in the policy update algorithm and then introduce the curriculum construction settings used in our experiments.</p>
<p>Reward Modeling</p>
<p>In our work, we use RL as our post-training method as it eliminates the need for training sets consisting of high-quality supervised trajectories produced either by humans or stronger models.</p>
<p>Our rule-based rewards consist of three components: answer rewards, citation rewards, and formatting rewards.</p>
<p>Answer Rewards To incentivize correct final answers, we define the answer reward as:
R answer = γ answer • 1(o answer = G answer ), (1)
where o answer is the generated final answer, G answer is the ground truth answer, and γ answer is a scaling factor, which we set to 5 for our experiments.</p>
<p>Citation Rewards To reward correct citations, we define the citation reward as:
R citations = γ correct • Recall(o citations , G citations ) − γ incorrect • c incorrect (2)
where recall denotes the fraction of relevant citations cited in the final answer o citations , G citations is the list of ground truth citations, c incorrect is the number of incorrect citations, and both γ correct and γ incorrect are the scaling factors which we set to 5 and 2 respectively.</p>
<p>Formatting Rewards To enforce the desired output format, we assign a reward of γ format for correct formatting (i.e., the presence of proper XML tags and required headings) while imposing a penalty p for outputs with excessive text or non-English Unicode characters.2Formally, we define the reward as:
R formatting = γ format , if formatting is correct −p, otherwise.
(3)</p>
<p>Total Reward and Objective Function The overall reward for a training sample is the sum of the individual components: This reward is then used in the GRPO algorithm for policy optimization (Shao et al., 2024).The scaling constants we choose in our experiments weigh correctness more than formatting, and in preliminary experiments, we observed no significant changes in performance when adjusting these parameters by small amounts.
R total = R answer + R citation + R formatting .</p>
<p>Curriculum Construction</p>
<p>Curriculum construction builds training sets by selecting or generating samples across a range of difficulty levels.In this work, we investigate two main difficulty axes: ( 1
S l i = [Q, {D + 1 , D + 2 , . . . , D + j , D − 1 , D − 2 , . . . , D − d }], where d = min (max(l + 2 − j, 0), k) ,
where the order of the documents in each S i is shuffled to ensure a realistic retrieval setting.</p>
<p>Since the minimum number of hops required among all of our datasets is 2, a difficulty level of 1 corresponds to 1 distractor document for a 2hop question.It follows that the highest difficulty level we can effectively introduce is thus k, which we denote as K going forward.This definition of synthetic difficulty ensures that all gold contexts are retrieved regardless of difficulty level.</p>
<p>While the datasets we use contain 2-hop, 3-hop, and 4-hop questions, we focus on the number of distractor documents as the primary axis of difficulty.This choice is motivated by the limited granularity offered by hop count alone, as the vast majority of questions in all three datasets we use are 2-hop questions.For completeness, we include an ablation in Section 5.4 that jointly considers both the number of hops and distractor documents by sorting each question by the number of hops and then augmenting each question to span a pre-defined curriculum.Moreover, in Appendix C.3 we present results that show a negative correlation between model performance and the number of hops in each question.</p>
<p>Accuracy-Based Difficulty An alternative way to define the difficulty of a training sample is to benchmark the base model's performance on each sample.Specifically, we compute the pass@k of each training sample and partition the dataset into two subsets: samples with pass@k = 1 and samples with pass@k = 0. We refer to the former as baseanswerable and the latter as base-unanswerable.A prediction is considered correct if the generated final answer achieves an F1 score of 1 when compared to the ground truth answer.</p>
<p>Experiments</p>
<p>Datasets</p>
<p>We evaluate RAG-RL on three open-domain multi-hop question answering benchmarks: Hot-potQA (Yang et al., 2018), MuSiQue (answerable) (Trivedi et al., 2022), and 2Wiki (Ho et al., 2020).While HotpotQA has been shown to be a weaker test for multi-hop reasoning due to the presence of spurious signals (Trivedi et al., 2023), we include it due to its widespread use but mainly focus on the other two datasets in our discussions.</p>
<p>Training Setup</p>
<p>We use Qwen2.5-7B-Instruct(</p>
<p>Baselines</p>
<p>Our primary baseline is our base model, Qwen2.5-7B-Instruct.For a fine-tuned baseline, we employ the max curriculum, which uses samples at the highest difficulty level K, which represents the difficulty of problems expected at test-time.In our tables, we report our base model's performance as "baseline" and our fine-tuned baseline as "max."</p>
<p>Curriculum Learning Settings</p>
<p>To investigate the effectiveness of curriculum construction in the post-training process, we benchmark several different curricula.As defined in Section 3.2, synthetic difficulty levels range from 1 to K, while accuracy-based difficulty partitions training samples into base-answerable and baseunanswerable subsets.Figure 2 provides an illustration of the main synthetic curricula used in our experiments.We define a function C setting : {1, . . ., n} → {1, . . ., K} that maps an index i in the training set to its corresponding difficulty level under each setting.We set K to be 10, 20, and 10 for HotpotQA, MuSiQue, and 2Wiki respectively.</p>
<p>Synthetic Curricula Variants</p>
<p>• Max: Each sample in the training set is presented at the maximum difficulty level (i.e., the difficulty level expected at test time).Thus, the difficulty function is defined as:
C max (i) = K, ∀i ∈ {1, . . . , n}
• Linear: The training set is partitioned into K equally sized subsets, with difficulty levels increasing linearly from 1 to K. The mapping function is thus:
C linear (i) = K • i n • Min-Max:
The training set is split into two equal parts, where the first half consists of the easiest difficulty level (1) and the second half consists of the hardest difficulty level (K).</p>
<p>The function is defined as:
C min-max (i) = 1, if i ≤ n/2 K, if i &gt; n/2
4.4.2Accuracy-Based Curricula</p>
<p>• Base-Answerable: The training set includes only samples from the maximum synthetic difficulty level that are base-answerable.</p>
<p>• Base-Unanswerable: The training set includes only samples from the maximum synthetic difficulty level that are baseunanswerable.</p>
<p>Evaluation</p>
<p>To benchmark the performance of our RLMs, we evaluate the F1 scores of the generated answer and passage-level citations on the validation sets provided by our selected benchmark datasets.We sample each response 3 times and take the average F1 score among all generations.Joint F1, which captures both answer and citation correctness, serves as our primary metric.Dataset statistics can be found in Appendix A.</p>
<p>Comparison to Previous Works To compare our RLMs to previous works, we measure the performance of our models in two settings: the distractor setting and the ideal retrieval setting.The distractor setting consists of providing the generation model all gold passages and up to 18 distractor passages, which is comparable to having the reasoning model handle both reranking and answer generation.On the other hand, in the ideal retrieval setting, the reasoning model is given only the gold truth passages, which is comparable to using a strong retrieval and reranking system.Previous works on improving multi-hop question-answer performance, such as Beam Retrieval (Zhang et al., 2024a) and Smoothing R3 (Yin et al., 2023), have primarily focused on optimizing the retrieval component of RAG and utilize span prediction models for answer generation, thus making a direct comparison of generator performance difficult.To better isolate and evaluate generation quality, we adopt the ideal retrieval setting as a more controlled and comparable benchmark.When comparing the performance of RAG-RL to the few past studies that have focused on improving answer generation models (Jin et al., 2025; Table 2: Base model pass@k on the MuSiQue dataset.Zhang et al., 2024c), RAG-RL achieves SOTA performance.However, we note that these works may not explicitly focus on multi-hop QA performance nor use the latest base models.</p>
<p>Results</p>
<p>Our results section is organized into subsections, beginning with the main findings, followed by an investigation of several research questions.The primary goals of our experiments are twofold: first, to evaluate whether curriculum learning can enhance RL-based post-training; and second, to identify which components of the post-training pipeline contribute most significantly to final model performance and why.</p>
<p>Main Results</p>
<p>Table 1 presents the performance of our baseline model and the RLMs we trained under our outlined curriculum construction settings in the distrator and ideal retrieval evaluation settings.The results strongly support the notion that curriculum learning can help improve RL-based post-training.Across all three datasets and both evaluation settings, the min-max and linear curricula achieve the highest joint F1, improving over the max curriculum by a margin of 3 to 8 points.</p>
<p>Does adding easier samples improve performance?</p>
<p>The primary difference between our synthetic curricula and the max variant is the introduction of easier training samples.The results in that all the curricula that contain easier samples outperform the max curriculum.Figure 3 shows that the validation citation reward climbs much faster for the min-max curriculum, suggesting that training samples with few distractor passages allow the model to learn how to generate correct citations with greater sample efficiency.Given a large number of distractor passages, the model must first identify potential candidates and then reason over them.These findings that easier samples in the synthetic curricula serve to teach citation skills early in training, which can then be refined by harder examples that require multi-step reasoning over longer contexts.</p>
<p>Do we need granular problem difficulty?</p>
<p>Previous work in the area of self-improvement has shown that LLMs exhibit limited generalizability and that gradually increasing the difficulty levels of training samples from weak-to-strong is effective for helping models generalize beyond their initial training distributions (Lee et al., 2025b).However, our results suggest that this is not always necessary as the min-max curriculum outperforms the linear curriculum in most cases.We believe this to be a byproduct of our base model having relatively strong performance on the task before any posttraining is applied.We show our base model's pass@32 in Table 2 and find that these scores are comparable to our fine-tuned baseline.</p>
<p>Does dataset ordering matter?</p>
<p>Curriculum learning strategies where training sets are ordered from easy to hard have been successfully employed in many areas of machine learning (Soviany et al., 2022).To assess the impact of sample ordering on model performance, we experiment with three variants of a linear curriculum: (i) samples sorted by difficulty, measured as the number of distractors; (ii) samples presented in a randomly shuffled order; and (iii) samples first or-dered by the number of hops and then augmented to span the linear curriculum.As shown in Table 3, the resulting F1 scores across these curricula are broadly comparable.These findings suggest that, in this setting, the specific ordering of training examples does not yield consistent or significant performance differences.</p>
<p>Training on base-answerable versus base-unanswerable samples</p>
<p>We observe that models trained on base-answerable samples consistently outperform those trained on base-unanswerable samples.As shown in Table 4, this trend holds across both the distractor and ideal retrieval settings.We hypothesize that the superior performance from training on base-answerable samples stems from their alignment with the base model's pretrained capabilities.This is reminiscent of findings from Zhang et al. (2025), which demonstrate that supervised fine-tuning is most effective when responses are aligned with the base model's pretrained distribution.</p>
<p>Another possibility for the worse performance from base-unanswerable samples stems from these samples producing a greater number of groups where the rewards among all samples are equal, resulting in a computed advantage of zero and thus no gradient signal nor policy update.The Dynamic sAmpling Policy Optimization (DAPO; Yu et al., 2025) algorithm also notes this limitation, and they address this challenge by over-sampling across the entire dataset and discarding prompts that yield uniform rewards across generated responses.Based on our results, we believe tackling this issue from a curriculum construction angle can also yield improved training efficiency.</p>
<p>What rule-based rewards matter?</p>
<p>Recent works have observed that performance gains from using algorithms such as GRPO might stem from improved output formatting rather than improvements in reasoning ability (Petrov et al., 2025).To isolate this phenomenon and see how answer and citation rewards contribute to final model performance, we conduct an ablation where we train our models with only formatting rewards (Eq.3), omitting the answer (Eq. 1) and citation rewards (Eq.2).</p>
<p>Table 5 presents results on the MuSiQue dataset that demonstrate that employing formatting rewards alone using the min-max curriculum achieves higher answer F1 compared to that of the max curriculum with all rewards.While adding additional answer and citation rewards increases the sample efficiency of the post-training process, this ablation demonstrates that choosing the appropriate training curriculum also plays a major role.</p>
<p>We propose that post-training using formatting rewards alone can serve as a stronger baseline before incorporating accuracy-based metrics.</p>
<p>Performance on previously unsolved samples</p>
<p>To evaluate the extent to which RL-based posttraining and curriculum learning enhance model performance on previously unsolved questions, we additionally partition the evaluation set into base-answerable and base-unanswerable subsets.</p>
<p>As shown in Table 4, both the min-max and linear curricula yield the greatest gains on the baseunanswerable subset.Nevertheless, a notable performance gap remains between the two subsets.These results suggest that, while curriculum learning can improve a model's ability to address previously challenging examples, generalization to outof-distribution samples remains a key limitation of RL-based post-training (Xiong et al., 2025).</p>
<p>Discussion</p>
<p>In this work, we introduce RAG-RL, a reasoning language model specifically trained for the answer generation component of RAG.Our experiments demonstrate that stronger answer generation models can reduce the burden on retrieval models by reasoning over larger sets of retrieved contexts and that curriculum learning is a powerful tool for improving sample efficiency and generalization during post-training.</p>
<p>Our experiments and ablations support the following key observations: (1) curriculum construction is a powerful method for improving posttraining performance, (2) easier training samples (i.e., those with fewer distractor documents) provide models with a stronger signal for learning how to generate citations, and (3) LLMs do not necessarily benefit the most from curricula with gradually increasing difficulty levels (i.e., min-max performs better than a linear curriculum in most of our experiments).While RL-based post-training methods have unlocked a new dimension of scaling for LLMs, our experiments take an in-depth look into understanding which components of these posttraining methods contribute to improving model performance and why.</p>
<p>Future Work Our findings suggest that LLMs exhibit limited generalization with performance gains extending only marginally beyond the training distribution.To address this, we propose a systematic categorization of training samples into base-answerable and base-unanswerable instances.Empirically, we find that incorporating synthetic difficulty levels in curriculum construction enables models to acquire citation and reasoning skills with greater sample efficiency and generalizability.Exploring algorithmic curriculum generation methods that target specific areas of improvement, especially for tasks lacking natural difficulty levels, is an exciting research direction.</p>
<p>Limitations</p>
<p>While RAG-RL achieves strong performance across multiple multi-hop QA benchmarks, a key limitation of our experimental setup is the assump-tion that all relevant gold documents are present in the retrieved set.In real-world scenarios, this assumption may not hold.We suggest explicitly training the model to return an "unanswerable" response when the retrieved context lacks sufficient information.Additionally, our curriculum construction process employs a static progression through difficulty levels, advancing to harder samples regardless of whether the model has fully converged on earlier stages.In contrast, prior work on curriculum learning often adopts adaptive schedulers that revisit easier examples and only proceeds to more difficult ones once the model's performance plateaus.Incorporating such adaptive strategies may further enhance training efficiency and generalization, and we leave this as an avenue for future exploration.</p>
<p>A Dataset Construction</p>
<p>To construct our synthetic difficulty-based curricula, we randomly sample 5,000 training samples from each of the respective training sets provided by HotpotQA, MuSiQue, and 2Wiki.These datasets each provide distractor passages on similar topics that serve to increase the difficulty of the QA task compared to sampling random passages from their corpora.</p>
<p>To construct the base-answerable and baseunanswerable accuracy-based curricula, we start by randomly selecting up to 40,000 training samples from each of the respective training sets, and we use our base model to partition the questions into those with pass@k = 1 for base-answerable and pass@k = 0 for base-unanswerable.We take the maximum answer F1 score across 8 generations for each question to assess pass@k.We then clip the number of training samples so that both the base-answer and base-unanswerable training sets have the same number of training samples (approximately 5,000).</p>
<p>We provide the dataset statistics for all of our datasets in Tables 6 and 7 and note that the respective number of samples in each dataset are dependent on the base model's performance, as well as the respective sizes of each dataset's original training and evaluation set sizes.</p>
<p>B Training</p>
<p>We use the Axolotl package3 to conduct our posttraining.For GRPO specifically, we use a beta of 0.01 and a clippling parameter of 0.2.A full list of all the hyperparameters used during training can be found in our code in the supplementary materials.We plan to release all of our code, data, and models as an open-source GitHub repository at the conclusion of the review process.</p>
<p>C Additional Experiments C.1 Training on Larger Train Sets</p>
<p>The training runs presented in Section 5 are based on subsets of up to 5,000 samples, constrained by computational limitations and the large number of post-training runs required across our curricula.To assess the scalability of our findings to larger training set sizes, we include additional results in Table 8, using 40,000 training samples for HotpotQA and 19,900 for MuSiQue.These results confirm that the trends and conclusions in the main paper continue to hold at larger scales.We do not include experiments with base-answerable and base-unanswerable subsets in this setting due to the reduced number of samples remaining after filtering for base-unanswerable instances.</p>
<p>C.2 Performance on Base-Answerable and Base-Unanswerable Questions</p>
<p>Table 8 reports the complete results for both the distractor and ideal retrieval settings, evaluated separately on base-answerable and base-unanswerable questions.Notably, in the ideal retrieval setting, answer F1 scores are lower than those observed in the distractor setting.Upon closer analysis, we attribute this trend to limitations of the F1 metric, which can assign partial credit to answers that are incorrect.When we replaced F1 with the exact match (EM) metric, we observed smaller performance gains on the base-unanswerable subsets and more comparable results across the two retrieval settings.</p>
<p>C.3 Model Performance Grouped by Number of Hops</p>
<p>Table 10 includes the results from Appendix C.1 when grouping by the number of hops required by each question.The results show that as the number of hops increases per question, model performance consistently decreases.However, the models trained with the min-max curriculum still achieve the highest F1 scores.</p>
<p>D Example Prompts and Outputs</p>
<p>The system prompt and user instructions we use to instruct our models are included in Figures 4 and 5.</p>
<p>System Prompt</p>
<p>Respond in the following format: <reasoning> ... </reasoning> <answer> Final answer: final answer Supporting passages: title1, title2,... </answer></p>
<p>User Instructions</p>
<p>Answer the question using only the provided passages.Verify your answer directly against the text, and cite only the passages you used in your final answer.</p>
<p>Figure 1: An example of a multi-hop reasoning chain taken from the MuSiQue dataset.RAG-RL generated the reasoning trace and final answer/citations observed in the green block.</p>
<p>Figure 2 :
2
Figure 2: Overview of two curriculum construction settings used during training.Linear denotes a curriculum that scales the difficulty level (the number of distractor passages) from 1 to K, while min-max denotes a curriculum that is split evenly between the easiest and the hardest problems.</p>
<p>) synthetic difficulty, where training samples are algorithmically constructed to span predefined difficulty levels, and (2) accuracybased difficulty, where the base model's performance determines which samples it can or cannot solve before any post-training is applied.Synthetic Difficulty Given a question Q, and a set of documents D, we partition D into a set of gold documents D + and a set of distractor documents D − .The number of hops required to correctly answer Q is given by j = |D + |, while k = |D − | represents the number of retrieved distractor documents.Naturally, the difficulty of a multi-hop question can be measured along two dimensions: the number of hops required j and the number of distractor documents provided to the generation model from D − .For training RAG-RL, we define the difficulty of a training sample solely based on the subset size of D − that we provide to the generation model.The easiest training samples contain at most one distractor document along with all gold documents, while the hardest samples include the full set of all retrieved documents.Formally, a training sample S i of difficulty level l is defined as</p>
<p>Figure 3: of validation answer and citation rewards during training for three curricula on the MuSiQue dataset.</p>
<p>Figure 4 :
4
Figure 4: System prompt used for all experiments.</p>
<p>Figure 5 :
5
Figure 5: User instructions used for all experiments.</p>
<p>Table 1 :
1
Model performance under the distractor and ideal retrieval evaluation settings across different curriculum construction settings.We use up to 5,000 training samples for all runs as outlined in Section 4.2.The best-performing curriculum for each metric is bolded.Additional training runs with larger training sets are provided in Appendix C.1.
HotpotQAMuSiQue2WikiAnswer F1 Citation F1 Joint F1 Answer F1 Citation F1 Joint F1 Answer F1 Citation F1 Joint F1Baseline60.6536.4745.5525.8825.3525.6148.7140.1844.03DistractorMax Linear Min-Max Base-Answerable66.04 68.71 68.87 66.1973.93 78.54 81.64 72.8169.76 73.30 74.72 69.3440.91 44.68 47.18 41.4053.07 59.79 64.48 54.2846.20 51.14 54.49 46.9767.99 68.92 70.77 68.5677.08 83.23 76.37 76.3672.25 75.40 73.46 72.25Base-Unanswerable65.2571.1368.0638.8452.1544.5266.5972.0469.21Ideal RetrievalBaseline Max Linear Min-Max Base-Answerable Base-Unanswerable67.90 74.25 75.67 76.18 74.76 75.2363.26 86.26 89.34 93.13 83.69 82.9965.50 79.81 81.94 83.81 78.98 78.9241.16 54.64 61.10 65.06 57.53 54.5358.16 68.84 73.90 81.51 67.75 68.3748.21 60.92 66.89 72.37 62.22 60.6770.29 71.82 74.31 75.06 72.61 71.0454.49 78.53 88.82 80.37 79.98 74.3061.39 75.02 80.92 77.63 76.11 72.63</p>
<p>Table 1
1show</p>
<p>Table 4 :
4
.21 44.89 / 35.65 51.83 / 14.64 40.96 / 4.63 27.91 / 19.25 33.20 / 7.46  61.64 / 6.75 42.71 / 33.8050.45/11.25 Max 82.63 / 18.07 76.01 / 68.70 79.18 / 28.61 63.87 / 13.51 55.54 / 47.51 59.41 / 21.04 84.42 / 17.37 76.53 / 75.50 80.28 / 28.25 Linear 86.26 / 20.48 80.84 / 73.02 83.46 / 31.99 70.18 / 18.82 62.70 / 53.70 66.23 / 27.87 85.00 / 23.14 83.77 / 80.32 84.38 / 35.93 Min-Max 86.27 / 20.40 83.21 / 76.46 84.71 / 32.20 71.73 / 22.89 67.42 / 58.47 69.51 / 32.90 87.83 / 20.41 77.66 / 74.19 82.43 / 32.02 Base-Answerable 82.63 / 17.09 75.08 / 66.20 78.67 / 27.16 65.62 / 16.17 57.97 / 47.65 61.56 / 24.15 86.26 / 17.42 77.29 / 75.14 81.53 / 28.28 Base-Unanswerable 82.94 / 16.71 72.74 / 65.35 77.51 / 26.62 63.59 / 14.49 55.44 / 45.81 59.23 / 22.01 83.92 / 16.16 71.42 / 73.19 77.17 / 26.48Model performance under the distractor evaluation setting evaluated on base-answerable and baseunanswerable evaluation sets.We use up to 5,000 training samples for all runs as outlined in Section 4.2.Results under the ideal retrieval setting can be found in the Appendix in Table9.
Eval. Setting CurriculumHotpotQAMuSiQue2WikiAnswer F1Citation F1Joint F1Answer F1Citation F1Joint F1Answer F1Citation F1Joint F1Distractor 61.31 / 9Eval. Setting Curriculum Baseline Answer F1Citation F1Joint F1Baseline25.8825.3525.61DistractorMax40.91 / 37.97 53.07 / 36.44 46.20 / 37.19Min-Max47.18 / 41.30 64.48 / 42.21 54.49 / 41.75Baseline41.1658.1648.21IdealMax54.64 / 50.59 68.84 / 55.58 60.92 / 52.97Min-Max65.06 / 57.95 81.51 / 57.18 72.37 / 57.56</p>
<p>Table 5 :
5
MuSiQue formatting ablation.The first number in each cell denotes the use of both accuracy and formatting rewards, while the second number in each cell denotes the use of only formatting rewards.</p>
<p>Table 6 :
6
Number of training samples for each curriculum and dataset.
Number of Training SamplesCurriculumHotpotQA MuSiQue 2WikiMax500050005000Linear500050005000Min-Max500050005000Base-Answerable399140574468Base-Unanswerable399140574468Number of Evaluation SamplesSettingHotpotQA MuSiQue 2WikiDistractor100010001000Ideal Retrieval100010001000Distractor (Base-Answerable/Unanswerable)8106811000Ideal Retrieval (Base-Answerable/Unanswerable)5123791000</p>
<p>Table 7 :
7
Number of evaluation samples for each dataset and evaluation setting.</p>
<p>Gold documents are documents from which the answer to a given question can be deduced, while distractor documents are those that do not contain relevant information.
This penalty has proven particularly beneficial for improving training stability by encouraging the model to generate responses in English.
https://github.com/axolotl-ai-cloud/axolotl
* This work was done while Jerry was a Research Intern at NewsBreak.
Curriculum learning. Yoshua References, Jérôme Bengio, Ronan Louradour, Jason Collobert, Weston, International Conference on Machine Learning. 2009</p>
<p>Reading Wikipedia to answer opendomain questions. Danqi Chen, Adam Fisch, Jason Weston, Antoine Bordes, 10.18653/v1/P17-1171Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaAssociation for Computational Linguistics20171</p>
<p>Deepseek-Ai , Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, and 1 others. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. </p>
<p>Lightrag: Simple and fast retrievalaugmented generation. Zirui Guo, Lianghao Xia, Yanhua Yu, Tu Ao, Chao Huang, arXiv:2501.12948arXiv:2410.057792024PreprintPreprint</p>
<p>From rag to memory: Non-parametric continual learning for large language models. Jiménez Bernal, Yiheng Gutiérrez, Weijian Shu, Sizhe Qi, Yu Zhou, Su, arXiv:2502.148022025Preprint</p>
<p>Realm: retrievalaugmented language model pre-training. Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, Ming-Wei Chang, Proceedings of the 37th International Conference on Machine Learning. the 37th International Conference on Machine Learning2020ICML'20. JMLR.org</p>
<p>Constructing a multihop QA dataset for comprehensive evaluation of reasoning steps. Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, Akiko Aizawa, 10.18653/v1/2020.coling-main.580Proceedings of the 28th International Conference on Computational Linguistics. the 28th International Conference on Computational LinguisticsBarcelona, Spain (Online2020International Committee on Computational Linguistics</p>
<p>Long-context LLMs meet RAG: Overcoming challenges for long inputs in RAG. Jinsung Bowen Jin, Jiawei Yoon, Sercan O Han, Arik, The Thirteenth International Conference on Learning Representations. 2025</p>
<p>Nv-embed: Improved techniques for training llms as generalist embedding models. Chankyu Lee, Rajarshi Roy, Mengyao Xu, Jonathan Raiman, Mohammad Shoeybi, Bryan Catanzaro, Wei Ping, arXiv:2405.174282025aPreprint</p>
<p>Self-improving transformers overcome easy-to-hard and length generalization challenges. Nayoung Lee, Ziyang Cai, Avi Schwarzschild, arXiv:2502.016122025bPreprintKangwook Lee, and Dimitris Papailiopoulos</p>
<p>Retrieval-augmented generation for knowledgeintensive nlp tasks. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen Tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela, Advances in Neural Information Processing Systems. 202033</p>
<p>Multi-hop question answering. Vaibhav Mavi, Anubhav Jangra, Adam Jatowt, 10.1561/1500000102Found. Trends Inf. Retr. 1752024</p>
<p>Niklas Muennighoff, Hongjin Su, Liang Wang, Nan Yang, Furu Wei, Tao Yu, Amanpreet Singh, Douwe Kiela, arXiv:2402.09906Generative representational instruction tuning. 2025Preprint</p>
<p>Curriculum learning for reinforcement learning domains: A framework and survey. Sanmit Narvekar, Bei Peng, Matteo Leonetti, Jivko Sinapov, Matthew E Taylor, Peter Stone, arXiv:2003.049602020Preprint</p>
<p>Answering while summarizing: Multi-task learning for multi-hop QA with evidence extraction. Kosuke Nishida, Kyosuke Nishida, Masaaki Nagata, Atsushi Otsuka, Itsumi Saito, Hisako Asano, Junji Tomita, 10.18653/v1/P19-1225Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational Linguistics2019</p>
<p>Ahmed El-Kishky, and 1 others. : Openai, Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, arXiv:2412.16720Openai o1 system card. Preprint. 2024</p>
<p>Ivo Petrov, Jasper Dekoninck, Lyuben Baltadzhiev, Maria Drencheva, Kristian Minchev, Mislav Balunović, Nikola Jovanović, Martin Vechev, arXiv:2503.21934Proof or bluff? evaluating llms on 2025 usa math olympiad. 2025Preprint</p>
<p>Deepseekmath: Pushing the limits of mathematical reasoning in open language models. : Qwen, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Peiyi Shao, Qihao Wang, Runxin Zhu, Junxiao Xu, Xiao Song, Haowei Bi, Mingchuan Zhang, Y K Zhang, Y Li, Daya Wu, Guo, arXiv:2412.15115arXiv:2402.03300Bo Zheng, and 1 others. 2025. Qwen2.5 technical report. 2024Preprint</p>
<p>Curriculum learning: A survey. Petru Soviany, Tudor Radu, Paolo Ionescu, Nicu Rota, Sebe, arXiv:2101.103822022Preprint</p>
<p>Bright: A realistic and challenging benchmark for reasoning-intensive retrieval. Hongjin Su, Howard Yen, Mengzhou Xia, Weijia Shi, Niklas Muennighoff, Haisu Han Yu Wang, Quan Liu, Zachary S Shi, Michael Siegel, Ruoxi Tang, Jinsung Sun, Sercan O Yoon, Danqi Arik, Tao Chen, Yu, arXiv:2407.128832024Preprint</p>
<p>Musique: Multihop questions via singlehop question composition. H Trivedi, N Balasubramanian, T Khot, A Sabharwal, 10.1162/TACL_A_00475Transactions of the Association for Computational Linguistics. 102022</p>
<p>Interleaving retrieval with chain-ofthought reasoning for knowledge-intensive multi-step questions. H Trivedi, N Balasubramanian, T Khot, A Sab, 10.18653/v1/2023.acl-long.557Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguisticsharwal. 2023</p>
<p>REAR: A relevance-aware retrieval-augmented framework for open-domain question answering. Yuhao Wang, Ruiyang Ren, Junyi Li, Xin Zhao, Jing Liu, Ji-Rong Wen, 10.18653/v1/2024.emnlp-main.321Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language ProcessingMiami, Florida, USAAssociation for Computational Linguistics2024a</p>
<p>Coderag-bench: Can retrieval augment code generation?. Zora Zhiruo, Wang , Akari Asai, Xinyan Velocity, Frank F Yu, Yiqing Xu, Graham Xie, Daniel Neubig, Fried, arXiv:2406.144972024bPreprint</p>
<p>Swe-rl: Advancing llm reasoning via reinforcement learning on open software evolution. Yuxiang Wei, Olivier Duchenne, Jade Copet, Quentin Carbonneaux, Lingming Zhang, Daniel Fried, Gabriel Synnaeve, Rishabh Singh, Sida I Wang, arXiv:2502.184492025Preprint</p>
<p>Rank1: Test-time compute for reranking in information retrieval. Orion Weller, Kathryn Ricci, Eugene Yang, Andrew Yates, Dawn Lawrie, Benjamin Van Durme, arXiv:2502.184182025Preprint</p>
<p>Logic-rl: Unleashing llm reasoning with rule-based reinforcement learning. Tian Xie, Zitian Gao, Qingnan Ren, Haoming Luo, Yuqian Hong, Bryan Dai, Joey Zhou, Kai Qiu, Zhirong Wu, Chong Luo, arXiv:2502.147682025Preprint</p>
<p>Building math agents with multi-turn iterative preference learning. Wei Xiong, Chengshuai Shi, Jiaming Shen, Aviv Rosenberg, Zhen Qin, Daniele Calandriello, Misha Khalman, Rishabh Joshi, Bilal Piot, Mohammad Saleh, Chi Jin, Tong Zhang, Tianqi Liu, The Thirteenth International Conference on Learning Representations. 2025</p>
<p>Hotpotqa: A dataset for diverse, explainable multi-hop question answering. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, Christopher D Manning, arXiv:1809.096002018Preprint</p>
<p>HELMET: How to evaluate long-context models effectively and thoroughly. Howard Yen, Tianyu Gao, Minmin Hou, Ke Ding, Daniel Fleischer, Peter Izsak, Moshe Wasserblat, Danqi Chen, The Thirteenth International Conference on Learning Representations. 2025</p>
<p>Xuanjing Huang, and Xipeng Qiu. 2023. Rethinking label smoothing on multi-hop question answering. Zhangyue Yin, Yuxin Wang, Xiannian Hu, Yiguang Wu, Hang Yan, Xinyu Zhang, Zhao Cao, arXiv:2212.09512Preprint</p>
<p>Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Guangming Sheng, Yuxuan Tong, Chi Zhang, Mofan Zhang, Wang Zhang, arXiv:2503.14476Hang Zhu, and 16 others. 2025. Dapo: An open-source llm reinforcement learning system at scale. Preprint. </p>
<p>The best instruction-tuning data are those that fit. Dylan Zhang, Qirun Dai, Hao Peng, arXiv:2502.041942025arXiv preprint</p>
<p>End-to-end beam retrieval for multi-hop question answering. Jiahao Zhang, Haiyang Zhang, Dongmei Zhang, Yong Liu, Shen Huang, arXiv:2308.089732024aPreprint</p>
<p>Curriculum learning driven domain adaptation for low-resource machine reading comprehension. L Zhang, Quan Wang, Benfeng Xu, Yi Liu, Zhendong Mao, IEEE Signal Processing Letters. 312024b</p>
<p>Raft: Adapting language model to domain specific rag. Tianjun Zhang, G Shishir, Naman Patil, Sheng Jain, Matei Shen, Ion Zaharia, Joseph E Stoica, Gonzalez, arXiv:2403.101312024cPreprint</p>
<p>Yang Zhou, Hongyi Liu, Zhuoming Chen, Yuandong Tian, Beidi Chen, arXiv:2502.0525261.31 / 9.21 44.89 / 35.65 51.83 / 14.64 40.96 / 4.63 27.91 / 19.25 33.20 / 7.46 61.64 / 6.75 42.71 / 33.80 50.45 / 11.25Gsm-infinite: How do your llms behave over infinitely increasing context length and reasoning complexity?. 2025arXiv preprint</p>
<p>. Min-Max , 86.27 / 20.40 83.21 / 76.46 84.71 / 32.20 71.73 / 22.89 67.42 / 58.47 69.51 / 32.90 87.83 / 20.41 77.66 / 74.19 82.43 / 32.02</p>
<p>. Min-Max , 92.06 / 10.30 95.12 / 93.68 93.56 / 18.56 86.46 / 29.55 81.60 / 81.64 83.96 / 43.39 91.01 / 17.17 80.95 / 83.03 85.69 / 28.46</p>
<p>Table 9: Model performance under the distractor and ideal retrieval evaluation settings evaluated on base-answerable and base-unanswerable evaluation sets. 88.89 / 7.77 84.98 / 83.93 86.89 / 14.22 77.66 / 12.12 68.00 / 70.96 72.51 / 20.71 88.95 / 11.88 75.07 / 81.82 81.42 / 20.75We use up to 5,000 training samples for all runs as outlined in Section 4.2 MuSiQue 2-hop MuSiQue 3-hop MuSiQue 4-hop Curriculum Answer F1 Citation F1 Joint F1 Answer F1 Citation F1 Joint F1 Answer F1 Citation F1 Joint F1. </p>
<p>Model performance on MuSiQue in the distractor setting grouped by the number of hops in each question. We use between. 10Table. 000 training samples for all the runs in this table</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1291 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1291</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1291</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-25.html">extraction-schema-25</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-892f9a2f69241feec647856cd26bed37e04fd747</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/892f9a2f69241feec647856cd26bed37e04fd747" target="_blank">Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization</a></p>
                <p><strong>Paper Venue:</strong> Journal of machine learning research</p>
                <p><strong>Paper TL;DR:</strong> A novel algorithm is introduced, Hyperband, for hyperparameter optimization as a pure-exploration non-stochastic infinite-armed bandit problem where a predefined resource like iterations, data samples, or features is allocated to randomly sampled configurations.</p>
                <p><strong>Paper Abstract:</strong> Performance of machine learning algorithms depends critically on identifying a good set of hyperparameters. While recent approaches use Bayesian optimization to adaptively select configurations, we focus on speeding up random search through adaptive resource allocation and early-stopping. We formulate hyperparameter optimization as a pure-exploration non-stochastic infinite-armed bandit problem where a predefined resource like iterations, data samples, or features is allocated to randomly sampled configurations. We introduce a novel algorithm, Hyperband, for this framework and analyze its theoretical properties, providing several desirable guarantees. Furthermore, we compare Hyperband with popular Bayesian optimization methods on a suite of hyperparameter optimization problems. We observe that Hyperband can provide over an order-of-magnitude speedup over our competitor set on a variety of deep-learning and kernel-based learning problems.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1291.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1291.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hyperband</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hyperband (bandit-based hyperparameter optimization)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A bandit-style, adaptive resource allocation algorithm for hyperparameter optimization that combines multiple runs of SuccessiveHalving across different exploration/exploitation tradeoffs to hedge unknown convergence behavior; makes minimal parametric assumptions and adapts to unknown convergence rates and loss distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Hyperband</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Hyperband is an iterative hyperparameter-optimization agent composed of an outer loop that enumerates different 'brackets' (choices of number of configurations n and minimum resource r) and an inner SuccessiveHalving subroutine that repeatedly evaluates configurations at increasing resource fidelities and discards the worst-performing fraction. Key components: get_hyperparameter_configuration (sampling), run_then_return_val_loss (fidelity-limited evaluations), top_k selection, parameters R (max resource) and η (elimination ratio).</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Successive Halving (iterative halving) within a multi-bracket bandit-based pure-exploration framework (non-stochastic infinite-armed bandit); early stopping and adaptive resource allocation.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Hyperband samples many configurations at low fidelity (resource r), measures intermediate validation losses, eliminates the worst 1-1/η fraction each round, and reallocates exponentially more resource to the surviving configurations; the outer loop repeats SuccessiveHalving with different n and r to hedge the unknown optimal tradeoff between number of configurations (exploration) and resource per configuration (fidelity/exploitation). Decisions use observed intermediate validation losses (learning-curve information) to select which configurations to continue.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Black-box hyperparameter optimization for machine learning models (benchmarks in paper: LeNet on MNIST, CIFAR-10, MRBI, SVHN, kernel RLS on CIFAR-10, random-feature kernel approximation tasks, 117 OpenML datasets).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Black-box, partially observable (only observe validation loss at chosen configuration and allocated resource/fidelity), can be stochastic or non-stochastic depending on training; unknown/variable convergence rates of learning curves; potentially high-dimensional hyperparameter spaces; evaluations expensive when run to full fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Varies by benchmark: examples include R (max resource) = 81 (LeNet example, resource = epochs), R=300 or 600 (deep learning experiments, resource = 100-iteration units), R=400 (kernel RLS, resource = 100 datapoints), R up to 1000 (random features, resource=100 features); number of configurations evaluated can be large (e.g., >250 configurations evaluated in first bracket of kernel RLS in ~20 minutes); hyperparameter spaces ranged from low-dim (d=3) to very high-dim (110 hyperparameters in the 117-dataset experiment).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Substantial empirical speedups versus standard baselines: paper reports 'over an order-of-magnitude' speedups in some deep-learning tasks; abstract reports 5× to 30× faster than popular Bayesian methods on a variety of problems. Examples from experiments: on CIFAR-10 and MRBI Hyperband is >10× faster than competitors; on MRBI it is also ≈5× faster than SMAC with early stopping; on a kernel regularized least squares task Hyperband returned a good configuration after ~20 minutes while competitors had not reached that error after 12 hours (reported ≈30× faster than Bayesian methods and ≈70× faster than random search); on random-feature experiments Hyperband ≈6× faster than Bayesian methods. Hyperband often finds competitive configurations with budgets ≈5R while competitors need ≈50R.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Baselines include random search (uniform allocation) and Bayesian optimization methods (SMAC, TPE, Spearmint) which train configurations to completion; random search and Bayesian methods often require significantly more total resource to reach similar performance (e.g., competitors evaluated only ~3 full configurations in the time Hyperband evaluated >250 partials in the kernel RLS experiment). In many experiments the most aggressive SuccessiveHalving bracket (repeated) outperformed Hyperband, showing performance depends on choosing a bracket a priori.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>High sample efficiency in problems where partial-fidelity evaluations are predictive: often finds good configurations after evaluating many cheap partials (e.g., first worthwhile result often obtained after budget ≈5R versus 50R for competitors). Empirical examples: evaluated >250 configurations in first bracket within ~20 minutes (kernel RLS); on deep-net tasks Hyperband finds useful configs with a fraction of full-budget iterations compared to baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Explicitly trades off exploration vs exploitation via (1) inner SuccessiveHalving which exploits by allocating more resource to promising arms and explores by starting with many arms at low fidelity, and (2) outer loop over brackets which varies n (exploration breadth) vs per-configuration budget (exploitation depth) to hedge unknown problem structure; elimination fraction controlled by η.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared against SMAC, TPE, Spearmint (Bayesian optimization methods), SMAC with an early-stopping heuristic (SMAC (early)), random search, random 2× (double-budget random), repeated SuccessiveHalving brackets (most-exploratory bracket repeated).</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Hyperband provides a principled, nonparametric adaptive resource-allocation approach that (a) adapts to unknown convergence rates and unknown distribution of terminal losses without parametric assumptions, (b) achieves strong empirical speedups (from ~6× up to ~70× in reported cases) over standard baselines, and (c) has theoretical guarantees: in stochastic settings it matches known lower bounds up to log factors and for the non-stochastic infinite-armed bandit it requires only logarithmically more budget than an optimal SuccessiveHalving choice despite having no knowledge of envelope functions or loss distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>When a single highly-aggressive early-stopping bracket is known to be optimal a priori, repeating that bracket can outperform the full Hyperband loop (Hyperband pays overhead to hedge across brackets). Hyperband is less effective when (a) overhead per configuration is large relative to training time (small datasets), (b) hyperparameters should change with fidelity/resource (resource-dependent hyperparameters weaken predictiveness of low-fidelity evaluations), or (c) a good prior/meta-data for sampling configurations would be more effective. Infinite-horizon Hyperband can be slower initially because it must discover an appropriate maximum resource R.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization', 'publication_date_yy_mm': '2016-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1291.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1291.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SuccessiveHalving</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SuccessiveHalving (iterative halving for configuration evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A halving-style adaptive evaluation algorithm that uniformly allocates an initial budget across n configurations, evaluates them at increasing resource levels, discards the worst-performing fraction each round, and focuses resources on top configurations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>SuccessiveHalving</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Inner-loop algorithm used by Hyperband: given n configurations and minimum resource r, SuccessiveHalving evaluates all configurations at r, ranks them, keeps the top ⌊n/η⌋, increases resource per survivor by factor η, and repeats until one remains. It requires n to be specified as input.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Halving-style bandit (successive elimination) with early stopping based on intermediate validation losses.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Adapts by measuring intermediate losses at a sequence of increasing resources and eliminating the worst-performing 1-1/η fraction after each fidelity step, thereby allocating exponentially more resource to surviving configurations.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Hyperparameter optimization / pure-exploration bandit setting (used across the same benchmarks as Hyperband).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Black-box, partially observable via intermediate performance measurements, possibly non-stochastic; unknown envelope functions govern how quickly intermediate losses approximate terminal losses.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Depends on chosen n and resource budget B; performance depends on envelope tightness and distribution of terminal losses; requires specifying n which can be difficult without prior knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Often substantially more sample- and time-efficient than uniform allocation (random search) because it stops poor configurations early; as a building block it enables Hyperband's large speedups. Empirically, repeating the most aggressive SuccessiveHalving bracket sometimes outperformed full Hyperband.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Uniform allocation (train all configurations to completion) is the non-adaptive baseline and requires much larger budget in many cases; SuccessiveHalving can require only a small factor more budget than an optimal strategy and much less than uniform allocation when envelopes tighten quickly.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Improves sample efficiency by early eliminating poor arms; theoretical results in the paper show required budget is only a small factor above optimal allocation in many regimes (depends on envelope functions and ν distribution).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Exploration is achieved by initial wide n; exploitation increases as surviving configurations receive more resources; elimination fraction controlled by η sets the aggressiveness of exploitation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared implicitly and experimentally against uniform allocation (random search), Hyperband (which runs multiple SuccessiveHalving brackets), and Bayesian configuration selection methods.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>SuccessiveHalving capitalizes on arms/configurations that are easy to distinguish early, reducing total required budget relative to uniform allocation; it motivates Hyperband because SuccessiveHalving itself requires specifying n which Hyperband avoids by hedging across n choices.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Requires pre-specifying the number of configurations n (the 'n vs B/n' problem) and can perform suboptimally if n is chosen poorly; aggressive halving can be wasteful if intermediate fidelities are not predictive of final performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization', 'publication_date_yy_mm': '2016-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1291.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1291.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bayesian optimization baselines</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bayesian optimization methods (SMAC, TPE, Spearmint)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Model-based sequential configuration-selection methods that fit a surrogate model (random forest, tree-structured Parzen estimators, Gaussian process) to observed evaluations and select new configurations with an acquisition function to trade off exploration and exploitation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Bayesian optimization methods (SMAC, TPE, Spearmint)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Surrogate-model-driven optimizers: SMAC uses random forests to predict performance, TPE uses tree-structured Parzen density estimators, and Spearmint uses Gaussian processes. They sequentially propose hyperparameter configurations using acquisition functions (e.g., expected improvement, UCB) and typically evaluate each candidate to completion (or to a fixed budget unless combined with early-stopping).</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Bayesian optimization (surrogate modeling + acquisition functions); in related work, extensions include modeling learning curves and combining with early-stopping (Freeze-thaw BO, multi-task BO, learning-curve extrapolation).</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Fit/update a probabilistic surrogate p(y|λ) from observed full-fidelity evaluations; select next λ by maximizing an acquisition function that encodes exploration vs exploitation; in hybrid variants, model predictive learning curves to decide early termination.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Black-box hyperparameter optimization tasks (same benchmarks: deep nets CIFAR-10/MRBI/SVHN, kernel tasks, OpenML datasets).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Black-box, possibly stochastic/noisy evaluations; can be high-dimensional and non-smooth; evaluation cost often high for full-fidelity runs; surrogate fitting may require structural assumptions (kernels/prior).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Ranges from low-dimensional (3-d) to very high-dimensional (up to 110 hyperparameters in the 117-dataset experiment); computational cost for surrogate fitting can be high (GPs O(n^3) unless approximated).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>In low-dimensional cases or when surrogate assumptions hold, BO methods can outperform random search; in many high-dimensional or expensive-evaluation settings in this paper, BO methods performed similarly to random search and were outperformed by Hyperband (empirical reports: Hyperband 5×–30× faster than these methods on some tasks). SMAC augmented with an early-stopping heuristic (SMAC (early)) was competitive in a few deep-learning experiments (sometimes outperforming Hyperband on SVHN within the evaluation window).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Random search baselines perform comparably in many high-dimensional settings; random 2× (double-budget random) often competitive and sometimes outperforms Bayesian methods in the paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Generally sample-efficient in low-dimensional smooth problems; degrades in high-dimensional spaces where surrogate models are harder to fit and where structural assumptions may be violated.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Acquisition functions (EI, UCB, etc.) manage exploration vs exploitation; GP-UCB and related methods have provable regret bounds under assumptions; practical BO requires tuning of acquisition hyperparameters.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared against Hyperband, SuccessiveHalving, random search, and random 2× in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>BO is effective at selecting promising configurations but can be computationally expensive per proposal and sensitive to modeling choices; combining BO with adaptive evaluation (learning-curve models, early-stopping) can improve speed (e.g., SMAC (early) and later hybrid methods), but Hyperband's nonparametric early-stopping and resource-allocation strategy provided strong empirical speedups without requiring curve-modeling assumptions.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>BO methods may fail or provide little advantage in high-dimensional hyperparameter spaces without strong priors or meta-learning; Gaussian processes scale poorly with number of observations and are sensitive to kernel/prior misspecification; some BO variants can overfit validation and be less robust when intermediate fidelities are informative but expensive to model.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization', 'publication_date_yy_mm': '2016-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Speeding up automatic hyperparameter optimization of deep neural networks by extrapolation of learning curves <em>(Rating: 2)</em></li>
                <li>Freeze-thaw Bayesian optimization <em>(Rating: 2)</em></li>
                <li>Multi-task Bayesian optimization <em>(Rating: 1)</em></li>
                <li>Non-stochastic best arm identification and hyperparameter optimization <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1291",
    "paper_id": "paper-892f9a2f69241feec647856cd26bed37e04fd747",
    "extraction_schema_id": "extraction-schema-25",
    "extracted_data": [
        {
            "name_short": "Hyperband",
            "name_full": "Hyperband (bandit-based hyperparameter optimization)",
            "brief_description": "A bandit-style, adaptive resource allocation algorithm for hyperparameter optimization that combines multiple runs of SuccessiveHalving across different exploration/exploitation tradeoffs to hedge unknown convergence behavior; makes minimal parametric assumptions and adapts to unknown convergence rates and loss distributions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Hyperband",
            "agent_description": "Hyperband is an iterative hyperparameter-optimization agent composed of an outer loop that enumerates different 'brackets' (choices of number of configurations n and minimum resource r) and an inner SuccessiveHalving subroutine that repeatedly evaluates configurations at increasing resource fidelities and discards the worst-performing fraction. Key components: get_hyperparameter_configuration (sampling), run_then_return_val_loss (fidelity-limited evaluations), top_k selection, parameters R (max resource) and η (elimination ratio).",
            "adaptive_design_method": "Successive Halving (iterative halving) within a multi-bracket bandit-based pure-exploration framework (non-stochastic infinite-armed bandit); early stopping and adaptive resource allocation.",
            "adaptation_strategy_description": "Hyperband samples many configurations at low fidelity (resource r), measures intermediate validation losses, eliminates the worst 1-1/η fraction each round, and reallocates exponentially more resource to the surviving configurations; the outer loop repeats SuccessiveHalving with different n and r to hedge the unknown optimal tradeoff between number of configurations (exploration) and resource per configuration (fidelity/exploitation). Decisions use observed intermediate validation losses (learning-curve information) to select which configurations to continue.",
            "environment_name": "Black-box hyperparameter optimization for machine learning models (benchmarks in paper: LeNet on MNIST, CIFAR-10, MRBI, SVHN, kernel RLS on CIFAR-10, random-feature kernel approximation tasks, 117 OpenML datasets).",
            "environment_characteristics": "Black-box, partially observable (only observe validation loss at chosen configuration and allocated resource/fidelity), can be stochastic or non-stochastic depending on training; unknown/variable convergence rates of learning curves; potentially high-dimensional hyperparameter spaces; evaluations expensive when run to full fidelity.",
            "environment_complexity": "Varies by benchmark: examples include R (max resource) = 81 (LeNet example, resource = epochs), R=300 or 600 (deep learning experiments, resource = 100-iteration units), R=400 (kernel RLS, resource = 100 datapoints), R up to 1000 (random features, resource=100 features); number of configurations evaluated can be large (e.g., &gt;250 configurations evaluated in first bracket of kernel RLS in ~20 minutes); hyperparameter spaces ranged from low-dim (d=3) to very high-dim (110 hyperparameters in the 117-dataset experiment).",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Substantial empirical speedups versus standard baselines: paper reports 'over an order-of-magnitude' speedups in some deep-learning tasks; abstract reports 5× to 30× faster than popular Bayesian methods on a variety of problems. Examples from experiments: on CIFAR-10 and MRBI Hyperband is &gt;10× faster than competitors; on MRBI it is also ≈5× faster than SMAC with early stopping; on a kernel regularized least squares task Hyperband returned a good configuration after ~20 minutes while competitors had not reached that error after 12 hours (reported ≈30× faster than Bayesian methods and ≈70× faster than random search); on random-feature experiments Hyperband ≈6× faster than Bayesian methods. Hyperband often finds competitive configurations with budgets ≈5R while competitors need ≈50R.",
            "performance_without_adaptation": "Baselines include random search (uniform allocation) and Bayesian optimization methods (SMAC, TPE, Spearmint) which train configurations to completion; random search and Bayesian methods often require significantly more total resource to reach similar performance (e.g., competitors evaluated only ~3 full configurations in the time Hyperband evaluated &gt;250 partials in the kernel RLS experiment). In many experiments the most aggressive SuccessiveHalving bracket (repeated) outperformed Hyperband, showing performance depends on choosing a bracket a priori.",
            "sample_efficiency": "High sample efficiency in problems where partial-fidelity evaluations are predictive: often finds good configurations after evaluating many cheap partials (e.g., first worthwhile result often obtained after budget ≈5R versus 50R for competitors). Empirical examples: evaluated &gt;250 configurations in first bracket within ~20 minutes (kernel RLS); on deep-net tasks Hyperband finds useful configs with a fraction of full-budget iterations compared to baselines.",
            "exploration_exploitation_tradeoff": "Explicitly trades off exploration vs exploitation via (1) inner SuccessiveHalving which exploits by allocating more resource to promising arms and explores by starting with many arms at low fidelity, and (2) outer loop over brackets which varies n (exploration breadth) vs per-configuration budget (exploitation depth) to hedge unknown problem structure; elimination fraction controlled by η.",
            "comparison_methods": "Compared against SMAC, TPE, Spearmint (Bayesian optimization methods), SMAC with an early-stopping heuristic (SMAC (early)), random search, random 2× (double-budget random), repeated SuccessiveHalving brackets (most-exploratory bracket repeated).",
            "key_results": "Hyperband provides a principled, nonparametric adaptive resource-allocation approach that (a) adapts to unknown convergence rates and unknown distribution of terminal losses without parametric assumptions, (b) achieves strong empirical speedups (from ~6× up to ~70× in reported cases) over standard baselines, and (c) has theoretical guarantees: in stochastic settings it matches known lower bounds up to log factors and for the non-stochastic infinite-armed bandit it requires only logarithmically more budget than an optimal SuccessiveHalving choice despite having no knowledge of envelope functions or loss distribution.",
            "limitations_or_failures": "When a single highly-aggressive early-stopping bracket is known to be optimal a priori, repeating that bracket can outperform the full Hyperband loop (Hyperband pays overhead to hedge across brackets). Hyperband is less effective when (a) overhead per configuration is large relative to training time (small datasets), (b) hyperparameters should change with fidelity/resource (resource-dependent hyperparameters weaken predictiveness of low-fidelity evaluations), or (c) a good prior/meta-data for sampling configurations would be more effective. Infinite-horizon Hyperband can be slower initially because it must discover an appropriate maximum resource R.",
            "uuid": "e1291.0",
            "source_info": {
                "paper_title": "Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization",
                "publication_date_yy_mm": "2016-03"
            }
        },
        {
            "name_short": "SuccessiveHalving",
            "name_full": "SuccessiveHalving (iterative halving for configuration evaluation)",
            "brief_description": "A halving-style adaptive evaluation algorithm that uniformly allocates an initial budget across n configurations, evaluates them at increasing resource levels, discards the worst-performing fraction each round, and focuses resources on top configurations.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "SuccessiveHalving",
            "agent_description": "Inner-loop algorithm used by Hyperband: given n configurations and minimum resource r, SuccessiveHalving evaluates all configurations at r, ranks them, keeps the top ⌊n/η⌋, increases resource per survivor by factor η, and repeats until one remains. It requires n to be specified as input.",
            "adaptive_design_method": "Halving-style bandit (successive elimination) with early stopping based on intermediate validation losses.",
            "adaptation_strategy_description": "Adapts by measuring intermediate losses at a sequence of increasing resources and eliminating the worst-performing 1-1/η fraction after each fidelity step, thereby allocating exponentially more resource to surviving configurations.",
            "environment_name": "Hyperparameter optimization / pure-exploration bandit setting (used across the same benchmarks as Hyperband).",
            "environment_characteristics": "Black-box, partially observable via intermediate performance measurements, possibly non-stochastic; unknown envelope functions govern how quickly intermediate losses approximate terminal losses.",
            "environment_complexity": "Depends on chosen n and resource budget B; performance depends on envelope tightness and distribution of terminal losses; requires specifying n which can be difficult without prior knowledge.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Often substantially more sample- and time-efficient than uniform allocation (random search) because it stops poor configurations early; as a building block it enables Hyperband's large speedups. Empirically, repeating the most aggressive SuccessiveHalving bracket sometimes outperformed full Hyperband.",
            "performance_without_adaptation": "Uniform allocation (train all configurations to completion) is the non-adaptive baseline and requires much larger budget in many cases; SuccessiveHalving can require only a small factor more budget than an optimal strategy and much less than uniform allocation when envelopes tighten quickly.",
            "sample_efficiency": "Improves sample efficiency by early eliminating poor arms; theoretical results in the paper show required budget is only a small factor above optimal allocation in many regimes (depends on envelope functions and ν distribution).",
            "exploration_exploitation_tradeoff": "Exploration is achieved by initial wide n; exploitation increases as surviving configurations receive more resources; elimination fraction controlled by η sets the aggressiveness of exploitation.",
            "comparison_methods": "Compared implicitly and experimentally against uniform allocation (random search), Hyperband (which runs multiple SuccessiveHalving brackets), and Bayesian configuration selection methods.",
            "key_results": "SuccessiveHalving capitalizes on arms/configurations that are easy to distinguish early, reducing total required budget relative to uniform allocation; it motivates Hyperband because SuccessiveHalving itself requires specifying n which Hyperband avoids by hedging across n choices.",
            "limitations_or_failures": "Requires pre-specifying the number of configurations n (the 'n vs B/n' problem) and can perform suboptimally if n is chosen poorly; aggressive halving can be wasteful if intermediate fidelities are not predictive of final performance.",
            "uuid": "e1291.1",
            "source_info": {
                "paper_title": "Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization",
                "publication_date_yy_mm": "2016-03"
            }
        },
        {
            "name_short": "Bayesian optimization baselines",
            "name_full": "Bayesian optimization methods (SMAC, TPE, Spearmint)",
            "brief_description": "Model-based sequential configuration-selection methods that fit a surrogate model (random forest, tree-structured Parzen estimators, Gaussian process) to observed evaluations and select new configurations with an acquisition function to trade off exploration and exploitation.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Bayesian optimization methods (SMAC, TPE, Spearmint)",
            "agent_description": "Surrogate-model-driven optimizers: SMAC uses random forests to predict performance, TPE uses tree-structured Parzen density estimators, and Spearmint uses Gaussian processes. They sequentially propose hyperparameter configurations using acquisition functions (e.g., expected improvement, UCB) and typically evaluate each candidate to completion (or to a fixed budget unless combined with early-stopping).",
            "adaptive_design_method": "Bayesian optimization (surrogate modeling + acquisition functions); in related work, extensions include modeling learning curves and combining with early-stopping (Freeze-thaw BO, multi-task BO, learning-curve extrapolation).",
            "adaptation_strategy_description": "Fit/update a probabilistic surrogate p(y|λ) from observed full-fidelity evaluations; select next λ by maximizing an acquisition function that encodes exploration vs exploitation; in hybrid variants, model predictive learning curves to decide early termination.",
            "environment_name": "Black-box hyperparameter optimization tasks (same benchmarks: deep nets CIFAR-10/MRBI/SVHN, kernel tasks, OpenML datasets).",
            "environment_characteristics": "Black-box, possibly stochastic/noisy evaluations; can be high-dimensional and non-smooth; evaluation cost often high for full-fidelity runs; surrogate fitting may require structural assumptions (kernels/prior).",
            "environment_complexity": "Ranges from low-dimensional (3-d) to very high-dimensional (up to 110 hyperparameters in the 117-dataset experiment); computational cost for surrogate fitting can be high (GPs O(n^3) unless approximated).",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "In low-dimensional cases or when surrogate assumptions hold, BO methods can outperform random search; in many high-dimensional or expensive-evaluation settings in this paper, BO methods performed similarly to random search and were outperformed by Hyperband (empirical reports: Hyperband 5×–30× faster than these methods on some tasks). SMAC augmented with an early-stopping heuristic (SMAC (early)) was competitive in a few deep-learning experiments (sometimes outperforming Hyperband on SVHN within the evaluation window).",
            "performance_without_adaptation": "Random search baselines perform comparably in many high-dimensional settings; random 2× (double-budget random) often competitive and sometimes outperforms Bayesian methods in the paper's experiments.",
            "sample_efficiency": "Generally sample-efficient in low-dimensional smooth problems; degrades in high-dimensional spaces where surrogate models are harder to fit and where structural assumptions may be violated.",
            "exploration_exploitation_tradeoff": "Acquisition functions (EI, UCB, etc.) manage exploration vs exploitation; GP-UCB and related methods have provable regret bounds under assumptions; practical BO requires tuning of acquisition hyperparameters.",
            "comparison_methods": "Compared against Hyperband, SuccessiveHalving, random search, and random 2× in experiments.",
            "key_results": "BO is effective at selecting promising configurations but can be computationally expensive per proposal and sensitive to modeling choices; combining BO with adaptive evaluation (learning-curve models, early-stopping) can improve speed (e.g., SMAC (early) and later hybrid methods), but Hyperband's nonparametric early-stopping and resource-allocation strategy provided strong empirical speedups without requiring curve-modeling assumptions.",
            "limitations_or_failures": "BO methods may fail or provide little advantage in high-dimensional hyperparameter spaces without strong priors or meta-learning; Gaussian processes scale poorly with number of observations and are sensitive to kernel/prior misspecification; some BO variants can overfit validation and be less robust when intermediate fidelities are informative but expensive to model.",
            "uuid": "e1291.2",
            "source_info": {
                "paper_title": "Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization",
                "publication_date_yy_mm": "2016-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Speeding up automatic hyperparameter optimization of deep neural networks by extrapolation of learning curves",
            "rating": 2
        },
        {
            "paper_title": "Freeze-thaw Bayesian optimization",
            "rating": 2
        },
        {
            "paper_title": "Multi-task Bayesian optimization",
            "rating": 1
        },
        {
            "paper_title": "Non-stochastic best arm identification and hyperparameter optimization",
            "rating": 2
        }
    ],
    "cost": 0.0180215,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization</h1>
<p>Lisha Li<br>Carnegie Mellon University, Pittsburgh, PA 15213<br>Kevin Jamieson<br>JAMIESON@CS.WASHINGTON.EDU<br>University of Washington, Seattle, WA 98195<br>Giulia DeSalvo<br>GiULIAD@GOOGLE.COM<br>Google Research, New York, NY 10011<br>Afshin Rostamizadeh<br>ROSTAMI@GOOGLE.COM<br>Google Research, New York, NY 10011<br>Ameet Talwalkar<br>TALWALKAR@CMU.EDU<br>Carnegie Mellon University, Pittsburgh, PA 15213<br>Determined AI</p>
<p>Editor: Nando de Freitas</p>
<h4>Abstract</h4>
<p>Performance of machine learning algorithms depends critically on identifying a good set of hyperparameters. While recent approaches use Bayesian optimization to adaptively select configurations, we focus on speeding up random search through adaptive resource allocation and early-stopping. We formulate hyperparameter optimization as a pure-exploration nonstochastic infinite-armed bandit problem where a predefined resource like iterations, data samples, or features is allocated to randomly sampled configurations. We introduce a novel algorithm, Hyperband, for this framework and analyze its theoretical properties, providing several desirable guarantees. Furthermore, we compare Hyperband with popular Bayesian optimization methods on a suite of hyperparameter optimization problems. We observe that Hyperband can provide over an order-of-magnitude speedup over our competitor set on a variety of deep-learning and kernel-based learning problems.</p>
<p>Keywords: hyperparameter optimization, model selection, infinite-armed bandits, online optimization, deep learning</p>
<h2>1. Introduction</h2>
<p>In recent years, machine learning models have exploded in complexity and expressibility at the price of staggering computational costs. Moreover, the growing number of tuning parameters associated with these models are difficult to set by standard optimization techniques. These "hyperparameters" are inputs to a machine learning algorithm that govern how the algorithm's performance generalizes to new, unseen data; examples of hyperparameters include those that impact model architecture, amount of regularization, and learning rates. The quality of a predictive model critically depends on its hyperparameter configuration, but it is poorly understood how these hyperparameters interact with each other to affect the resulting model.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: (a) The heatmap shows the validation error over a two-dimensional search space with red corresponding to areas with lower validation error. Configuration selection methods adaptively choose new configurations to train, proceeding in a sequential manner as indicated by the numbers. (b) The plot shows the validation error as a function of the resources allocated to each configuration (i.e. each line in the plot). Configuration evaluation methods allocate more resources to promising configurations.</p>
<p>Consequently, practitioners often default to brute-force methods like random search and grid search (Bergstra and Bengio, 2012).</p>
<p>In an effort to develop more efficient search methods, the problem of hyperparameter optimization has recently been dominated by Bayesian optimization methods (Snoek et al., 2012; Hutter et al., 2011; Bergstra et al., 2011) that focus on optimizing hyperparameter configuration selection. These methods aim to identify good configurations more quickly than standard baselines like random search by selecting configurations in an adaptive manner; see Figure 1(a). Existing empirical evidence suggests that these methods outperform random search (Thornton et al., 2013; Eggensperger et al., 2013; Snoek et al., 2015b). However, these methods tackle the fundamentally challenging problem of simultaneously fitting and optimizing a high-dimensional, non-convex function with unknown smoothness, and possibly noisy evaluations.</p>
<p>An orthogonal approach to hyperparameter optimization focuses on speeding up configuration evaluation; see Figure 1(b). These approaches are adaptive in computation, allocating more resources to promising hyperparameter configurations while quickly eliminating poor ones. Resources can take various forms, including size of training set, number of features, or number of iterations for iterative algorithms. By adaptively allocating resources, these approaches aim to examine orders-of-magnitude more hyperparameter configurations than approaches that uniformly train all configurations to completion, thereby quickly identifying good hyperparameters. While there are methods that combine Bayesian optimization with adaptive resource allocation (Swersky et al., 2013, 2014; Domhan et al., 2015; Klein et al.,</p>
<p>2017a), we focus on speeding up random search as it offers a simple and theoretically principled launching point (Bergstra and Bengio, 2012). ${ }^{1}$</p>
<p>We develop a novel configuration evaluation approach by formulating hyperparameter optimization as a pure-exploration adaptive resource allocation problem addressing how to allocate resources among randomly sampled hyperparameter configurations. ${ }^{2}$ Our procedure, Hyperband, relies on a principled early-stopping strategy to allocate resources, allowing it to evaluate orders-of-magnitude more configurations than black-box procedures like Bayesian optimization methods. Hyperband is a general-purpose technique that makes minimal assumptions unlike prior configuration evaluation approaches (Domhan et al., 2015; Swersky et al., 2014; György and Kocsis, 2011; Agarwal et al., 2011; Sparks et al., 2015; Jamieson and Talwalkar, 2015).</p>
<p>Our theoretical analysis demonstrates the ability of Hyperband to adapt to unknown convergence rates and to the behavior of validation losses as a function of the hyperparameters. In addition, Hyperband is $5 \times$ to $30 \times$ faster than popular Bayesian optimization algorithms on a variety of deep-learning and kernel-based learning problems. A theoretical contribution of this work is the introduction of the pure-exploration, infinite-armed bandit problem in the non-stochastic setting, for which Hyperband is one solution. When Hyperband is applied to the special-case stochastic setting, we show that the algorithm comes within log factors of known lower bounds in both the infinite (Carpentier and Valko, 2015) and finite $K$-armed bandit settings (Kaufmann et al., 2015).</p>
<p>The paper is organized as follows. Section 2 summarizes related work in two areas: (1) hyperparameter optimization, and (2) pure-exploration bandit problems. Section 3 describes Hyperband and provides intuition for the algorithm through a detailed example. In Section 4, we present a wide range of empirical results comparing Hyperband with state-of-the-art competitors. Section 5 frames the hyperparameter optimization problem as an infinite-armed bandit problem and summarizes the theoretical results for Hyperband. Finally, Section 6 discusses possible extensions of Hyperband.</p>
<h1>2. Related Work</h1>
<p>In Section 1, we briefly discussed related work in the hyperparameter optimization literature. Here, we provide a more thorough coverage of the prior work, and also summarize significant related work on bandit problems.</p>
<h3>2.1 Hyperparameter Optimization</h3>
<p>Bayesian optimization techniques model the conditional probability $p(y \mid \lambda)$ of a configuration's performance on an evaluation metric $y$ (i.e., test accuracy), given a set of hyperparameters $\lambda$.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Sequential Model-based Algorithm Configuration (SMAC), Tree-structure Parzen Estimator (TPE), and Spearmint are three well-established methods (Feurer et al., 2014). SMAC uses random forests to model $p(y \mid \lambda)$ as a Gaussian distribution (Hutter et al., 2011). TPE is a non-standard Bayesian optimization algorithm based on tree-structured Parzen density estimators (Bergstra et al., 2011). Lastly, Spearmint uses Gaussian processes (GP) to model $p(y \mid \lambda)$ and performs slice sampling over the GP's hyperparameters (Snoek et al., 2012).</p>
<p>Previous work compared the relative performance of these Bayesian searchers (Thornton et al., 2013; Eggensperger et al., 2013; Bergstra et al., 2011; Snoek et al., 2012; Feurer et al., 2014, 2015). An extensive survey of these three methods by Eggensperger et al. (2013) introduced a benchmark library for hyperparameter optimization called HPOlib, which we use for our experiments. Bergstra et al. (2011) and Thornton et al. (2013) showed Bayesian optimization methods empirically outperform random search on a few benchmark tasks. However, for high-dimensional problems, standard Bayesian optimization methods perform similarly to random search (Wang et al., 2013). Recent methods specifically designed for high-dimensional problems assume a lower effective dimension for the problem (Wang et al., 2013) or an additive decomposition for the target function (Kandasamy et al., 2015). However, as can be expected, the performance of these methods is sensitive to required inputs; i.e. the effective dimension (Wang et al., 2013) or the number of additive components (Kandasamy et al., 2015).</p>
<p>Gaussian processes have also been studied in the bandit setting using confidence bound acquisition functions (GP-UCB), with associated sublinear regret bounds (Srinivas et al., 2010; Grünewälder et al., 2010). Wang et al. (2016) improved upon GP-UCB by removing the need to tune a parameter that controls exploration and exploitation. Contal et al. (2014) derived a tighter regret bound than that for GP-UCB by using a mutual information acquisition function. However, van der Vaart and van Zanten (2011) showed that the learning rate of GPs are sensitive to the definition of the prior through an example with a poor prior where the learning rate degraded from polynomial to logarithmic in the number of observations $n$. Additionally, without structural assumptions on the covariance matrix of the GP, fitting the posterior is $O\left(n^{3}\right)$ (Wilson et al., 2015). Hence, Snoek et al. (2015a) and Springenberg et al. (2016) proposed using Bayesian neural networks, which scale linearly with $n$, to model the posterior.</p>
<p>Adaptive configuration evaluation is not a new idea. Maron and Moore (1997) and Mnih and Audibert (2008) considered a setting where the training time is relatively inexpensive (e.g., $k$-nearest-neighbor classification) and evaluation on a large validation set is accelerated by evaluating on an increasing subset of the validation set, stopping early configurations that are performing poorly. Since subsets of the validation set provide unbiased estimates of its expected performance, this is an instance of the stochastic best-arm identification problem for multi-armed bandits (see the work by Jamieson and Nowak, 2014, for a brief survey).</p>
<p>In contrast, we address a setting where the evaluation time is relatively inexpensive and the goal is to early-stop long-running training procedures by evaluating partially trained models on the full validation set. Previous approaches in this setting either require strong assumptions or use heuristics to perform adaptive resource allocation. György and Kocsis (2011) and Agarwal et al. (2011) made parametric assumptions on the convergence behavior of training algorithms, providing theoretical performance guarantees under these assumptions. Unfortunately, these assumptions are often hard to verify, and empirical performance can</p>
<p>drastically suffer when they are violated. Krueger et al. (2015) proposed a heuristic based on sequential analysis to determine stopping times for training configurations on increasing subsets of the data. However, the theoretical correctness and empirical performance of this method are highly dependent on a user-defined "safety zone."</p>
<p>Several hybrid methods combining adaptive configuration selection and evaluation have also been introduced (Swersky et al., 2013, 2014; Domhan et al., 2015; Kandasamy et al., 2016; Klein et al., 2017a; Golovin et al., 2017). The algorithm proposed by Swersky et al. (2013) uses a GP to learn correlation between related tasks and requires the subtasks as input, but efficient subtasks with high informativeness for the target task are unknown without prior knowledge. Similar to the work by Swersky et al. (2013), Klein et al. (2017a) modeled the conditional validation error as a Gaussian process using a kernel that captures the covariance with downsampling rate to allow for adaptive evaluation. Swersky et al. (2014), Domhan et al. (2015), and Klein et al. (2017a) made parametric assumptions on the convergence of learning curves to perform early-stopping. In contrast, Golovin et al. (2017) devised an early-stopping rule based on predicted performance from a nonparametric GP model with a kernel designed to measure the similarity between performance curves. Finally, Kandasamy et al. (2016) extended GP-UCB to allow for adaptive configuration evaluation by defining subtasks that monotonically improve with more resources.</p>
<p>In another line of work, Sparks et al. (2015) proposed a halving style bandit algorithm that did not require explicit convergence behavior, and Jamieson and Talwalkar (2015) analyzed a similar algorithm originally proposed by Karnin et al. (2013) for a different setting, providing theoretical guarantees and encouraging empirical results. Unfortunately, these halving style algorithms suffer from the " $n$ versus $B / n$ " problem, which we will discuss in Section 3.1. HyPERBAND addresses this issue and provides a robust, theoretically principled early-stopping algorithm for hyperparameter optimization.</p>
<p>We note that HyPERBAND can be combined with any hyperparameter sampling approach and does not depend on random sampling; the theoretical results only assume the validation losses of sampled hyperparameter configurations are drawn from some stationary distribution. In fact, subsequent to our submission, Klein et al. (2017b) combined adaptive configuration selection with HyPERBAND by using a Bayesian neural network to model learning curves and only selecting configurations with high predicted performance to input into HyPERBAND.</p>
<h1>2.2 Bandit Problems</h1>
<p>Pure exploration bandit problems aim to minimize the simple regret, defined as the distance from the optimal solution, as quickly as possible in any given setting. The pure-exploration multi-armed bandit problem has a long history in the stochastic setting (Even-Dar et al., 2006; Bubeck et al., 2009), and was recently extended to the non-stochastic setting by Jamieson and Talwalkar (2015). Relatedly, the stochastic pure-exploration infinite-armed bandit problem was studied by Carpentier and Valko (2015), where a pull of each arm $i$ yields an i.i.d. sample in $[0,1]$ with expectation $\nu_{i}$, where $\nu_{i}$ is a loss drawn from a distribution with cumulative distribution function, $F$. Of course, the value of $\nu_{i}$ is unknown to the player, so the only way to infer its value is to pull arm $i$ many times. Carpentier and Valko (2015) proposed an anytime algorithm, and derived a tight (up to polylog factors) upper bound on its error assuming what we will refer to as the $\beta$-parameterization of $F$ described in</p>
<p>Section 5.3.2. However, their algorithm was derived specifically for the $\beta$-parameterization of $F$, and furthermore, they must estimate $\beta$ before running the algorithm, limiting the algorithm's practical applicability. Also, the algorithm assumes stochastic losses from the arms and thus the convergence behavior is known; consequently, it does not apply in our hyperparameter optimization setting. ${ }^{3}$ Two related lines of work that both make use of an underlying metric space are Gaussian process optimization (Srinivas et al., 2010) and $X$ armed bandits (Bubeck et al., 2011), or bandits defined over a metric space. However, these works either assume stochastic rewards or need to know something about the underlying function (e.g. an appropriate kernel or level of smoothness).</p>
<p>In contrast, Hyperband is devised for the non-stochastic setting and automatically adapts to unknown $F$ without making any parametric assumptions. Hence, we believe our work to be a generally applicable pure exploration algorithm for infinite-armed bandits. To the best of our knowledge, this is also the first work to test out such an algorithm on a real application.</p>
<h1>3. Hyperband Algorithm</h1>
<p>In this section, we present the Hyperband algorithm. We provide intuition for the algorithm, highlight the main ideas via a simple example that uses iterations as the adaptively allocated resource, and present a few guidelines on how to deploy Hyperband in practice.</p>
<h3>3.1 Successive Halving</h3>
<p>Hyperband extends the SuccessiveHalving algorithm proposed for hyperparameter optimization by Jamieson and Talwalkar (2015) and calls it as a subroutine. The idea behind the original SUCCESSIVEHALVING algorithm follows directly from its name: uniformly allocate a budget to a set of hyperparameter configurations, evaluate the performance of all configurations, throw out the worst half, and repeat until one configuration remains. The algorithm allocates exponentially more resources to more promising configurations. Unfortunately, SUCCESSIVEHalving requires the number of configurations $n$ as an input to the algorithm. Given some finite budget $B$ (e.g., an hour of training time to choose a hyperparameter configuration), $B / n$ resources are allocated on average across the configurations. However, for a fixed $B$, it is not clear a priori whether we should (a) consider many configurations (large $n$ ) with a small average training time; or (b) consider a small number of configurations (small $n$ ) with longer average training times.</p>
<p>We use a simple example to better understand this tradeoff. Figure 2 shows the validation loss as a function of total resources allocated for two configurations with terminal validation losses $\nu_{1}$ and $\nu_{2}$. The shaded areas bound the maximum deviation of the intermediate losses from the terminal validation loss and will be referred to as "envelope" functions. ${ }^{4}$ It is possible to distinguish between the two configurations when the envelopes no longer overlap. Simple arithmetic shows that this happens when the width of the envelopes is less than $\nu_{2}-\nu_{1}$, i.e., when the intermediate losses are guaranteed to be less than $\frac{\nu_{2}-\nu_{1}}{2}$ away from the</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The validation loss as a function of total resources allocated for two configurations is shown. $\nu_{1}$ and $\nu_{2}$ represent the terminal validation losses at convergence. The shaded areas bound the maximum distance of the intermediate losses from the terminal validation loss and monotonically decrease with the resource.
terminal losses. There are two takeaways from this observation: more resources are needed to differentiate between the two configurations when either (1) the envelope functions are wider or (2) the terminal losses are closer together.</p>
<p>However, in practice, the optimal allocation strategy is unknown because we do not have knowledge of the envelope functions nor the distribution of terminal losses. Hence, if more resources are required before configurations can differentiate themselves in terms of quality (e.g., if an iterative training method converges very slowly for a given data set or if randomly selected hyperparameter configurations perform similarly well), then it would be reasonable to work with a small number of configurations. In contrast, if the quality of a configuration is typically revealed after a small number of resources (e.g., if iterative training methods converge very quickly for a given data set or if randomly selected hyperparameter configurations are of low-quality with high probability), then $n$ is the bottleneck and we should choose $n$ to be large.</p>
<p>Certainly, if meta-data or previous experience suggests that a certain tradeoff is likely to work well in practice, one should exploit that information and allocate the majority of resources to that tradeoff. However, without this supplementary information, practitioners are forced to make this tradeoff, severely hindering the applicability of existing configuration evaluation methods.</p>
<h1>3.2 Hyperband</h1>
<p>Hyperband, shown in Algorithm 1, addresses this " $n$ versus $B / n$ " problem by considering several possible values of $n$ for a fixed $B$, in essence performing a grid search over feasible value of $n$. Associated with each value of $n$ is a minimum resource $r$ that is allocated to all configurations before some are discarded; a larger value of $n$ corresponds to a smaller $r$ and hence more aggressive early-stopping. There are two components to Hyperband; (1) the inner loop invokes SuccessiveHalving for fixed values of $n$ and $r$ (lines 3-9) and (2) the outer loop iterates over different values of $n$ and $r$ (lines 1-2). We will refer to each such run of SuccessiveHalving within Hyperband as a "bracket." Each bracket is designed to use approximately $B$ total resources and corresponds to a different tradeoff between $n$</p>
<div class="codehilite"><pre><span></span><code>Algorithm 1: HyPERBAND algorithm for hyperparameter optimization.
    input \(\quad: R, \eta(\text { default } \eta=3)\)
    initialization \(: s_{\max }=\left\lfloor\log <span class="ge">_{\eta}(R)\right\rfloor, B=\left(s_</span>{\max }+1\right) R\)
    for \(s \in\left\{s_{\max }, s_{\max }-1, \ldots, 0\right\}\) do
        \(n=\left\lceil\frac{B}{\left.R \frac{\eta^{s}}{(s+1)}\right\rceil}, \quad r=R \eta^{-s}\right.\)
        // begin SuccessiveHalving with \((n, r)\) inner loop
        \(T=\) get_hyperparameter_configuration \((n)\)
        for \(i \in\{0, \ldots, s\}\) do
            \(n_{i}=\left\lfloor n \eta^{-i}\right\rfloor\)
            \(r_{i}=r \eta^{t}\)
            \(L=\left\{\right.\) run_then_return_val_loss \(\left.\left(t, r_{i}\right): t \in T\right\}\)
            \(T=\) top_k \(\left(T, L,\left\lfloor n_{i} / \eta\right\rfloor\right)\)
        end
    end
11 return Configuration with the smallest intermediate loss seen so far.
</code></pre></div>

<p>and $B / n$. Hence, a single execution of Hyperband takes a finite budget of $\left(s_{\max }+1\right) B$; we recommend repeating it indefinitely.</p>
<p>Hyperband requires two inputs (1) $R$, the maximum amount of resource that can be allocated to a single configuration, and (2) $\eta$, an input that controls the proportion of configurations discarded in each round of SuCcessiveHalving. The two inputs dictate how many different brackets are considered; specifically, $s_{\max }+1$ different values for $n$ are considered with $s_{\max }=\left\lfloor\log <em _max="\max">{\eta}(R)\right\rfloor$. Hyperband begins with the most aggressive bracket $s=s</em>+1$ times more work than running SuccessiveHalving for a single value of $n$. By doing so, Hyperband is able to exploit situations in which adaptive allocation works well, while protecting itself in situations where more conservative allocations are required.}$, which sets $n$ to maximize exploration, subject to the constraint that at least one configuration is allocated $R$ resources. Each subsequent bracket reduces $n$ by a factor of approximately $\eta$ until the final bracket, $s=0$, in which every configuration is allocated $R$ resources (this bracket simply performs classical random search). Hence, Hyperband performs a geometric search in the average budget per configuration and removes the need to select $n$ for a fixed budget at the cost of approximately $s_{\max </p>
<p>Hyperband requires the following methods to be defined for any given learning problem:</p>
<ul>
<li>get_hyperparameter_configuration $(n)$ - a function that returns a set of $n$ i.i.d. samples from some distribution defined over the hyperparameter configuration space. In this work, we assume uniformly sampling of hyperparameters from a predefined space (i.e., hypercube with min and max bounds for each hyperparameter), which immediately yields consistency guarantees. However, the more aligned the distribution is towards high quality hyperparameters (i.e., a useful prior), the better Hyperband will perform (see Section 6 for further discussion).</li>
</ul>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">$s=4$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">$s=3$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">$s=2$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">$s=1$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">$s=0$</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$i$</td>
<td style="text-align: center;">$n_{i}$</td>
<td style="text-align: center;">$r_{i}$</td>
<td style="text-align: center;">$n_{i}$</td>
<td style="text-align: center;">$r_{i}$</td>
<td style="text-align: center;">$n_{i}$</td>
<td style="text-align: center;">$r_{i}$</td>
<td style="text-align: center;">$n_{i}$</td>
<td style="text-align: center;">$r_{i}$</td>
<td style="text-align: center;">$n_{i}$</td>
<td style="text-align: center;">$r_{i}$</td>
</tr>
<tr>
<td style="text-align: center;">0</td>
<td style="text-align: center;">81</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">27</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">27</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">81</td>
</tr>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">27</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">27</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">81</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">27</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">81</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">27</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">81</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">81</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 1: The values of $n_{i}$ and $r_{i}$ for the brackets of Hyperband corresponding to various values of $s$, when $R=81$ and $\eta=3$.</p>
<ul>
<li>run_then_return_val_loss $(t, r)$ - a function that takes a hyperparameter configuration $t$ and resource allocation $r$ as input and returns the validation loss after training the configuration for the allocated resources.</li>
<li>top_k(configs, losses, $k$ ) - a function that takes a set of configurations as well as their associated losses and returns the top $k$ performing configurations.</li>
</ul>
<h1>3.3 Example Application with Iterations as a Resource: LeNet</h1>
<p>We next present a concrete example to provide further intuition about Hyperband. We work with the MNIST data set and optimize hyperparameters for the LeNet convolutional neural network trained using mini-batch stochastic gradient descent (SGD). ${ }^{5}$ Our search space includes learning rate, batch size, and number of kernels for the two layers of the network as hyperparameters (details are shown in Table 2 in Appendix A).</p>
<p>We define the resource allocated to each configuration to be number of iterations of SGD, with one unit of resource corresponding to one epoch, i.e., a full pass over the data set. We set $R$ to 81 and use the default value of $\eta=3$, resulting in $s_{\max }=4$ and thus 5 brackets of SuccessiveHalving with different tradeoffs between $n$ and $B / n$. The resources allocated within each bracket are displayed in Table 1.</p>
<p>Figure 3 shows an empirical comparison of the average test error across 70 trials of the individual brackets of Hyperband run separately as well as standard Hyperband. In practice, we do not know a priori which bracket $s \in{0, \ldots, 4}$ will be most effective in identifying good hyperparameters, and in this case neither the most $(s=4)$ nor least aggressive $(s=0)$ setting is optimal. However, we note that Hyperband does nearly as well as the optimal bracket $(s=3)$ and outperforms the baseline uniform allocation (i.e., random search), which is equivalent to bracket $s=0$.</p>
<h3>3.4 Different Types of Resources</h3>
<p>While the previous example focused on iterations as the resource, Hyperband naturally generalizes to various types of resources:</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Performance of individual brackets $s$ and Hyperband.</p>
<ul>
<li>Time - Early-stopping in terms of time can be preferred when various hyperparameter configurations differ in training time and the practitioner's chief goal is to find a good hyperparameter setting in a fixed wall-clock time. For instance, training time could be used as a resource to quickly terminate straggler jobs in distributed computation environments.</li>
<li>Data Set Subsampling - Here we consider the setting of a black-box batch training algorithm that takes a data set as input and outputs a model. In this setting, we treat the resource as the size of a random subset of the data set with $R$ corresponding to the full data set size. Subsampling data set sizes using Hyperband, especially for problems with super-linear training times like kernel methods, can provide substantial speedups.</li>
<li>Feature Subsampling - Random features or Nyström-like methods are popular methods for approximating kernels for machine learning applications (Rahimi and Recht, 2007). In image processing, especially deep-learning applications, filters are usually sampled randomly, with the number of filters having an impact on the performance. Downsampling the number of features is a common tool used when hand-tuning hyperparameters; Hyperband can formalize this heuristic.</li>
</ul>
<h1>3.5 Setting $R$</h1>
<p>The resource $R$ and $\eta$ (which we address next) are the only required inputs to Hyperband. As mentioned in Section 3.2, $R$ represents the maximum amount of resources that can be allocated to any given configuration. In most cases, there is a natural upper bound on the maximum budget per configuration that is often dictated by the resource type (e.g., training set size for data set downsampling; limitations based on memory constraint for feature downsampling; rule of thumb regarding number of epochs when iteratively training neural networks). If there is a range of possible values for $R$, a smaller $R$ will give a result faster (since the budget $B$ for each bracket is a multiple of $R$ ), but a larger $R$ will give a better guarantee of successfully differentiating between the configurations.</p>
<p>Moreover, for settings in which either $R$ is unknown or not desired, we provide an infinite horizon version of Hyperband in Section 5. This version of the algorithm doubles</p>
<p>the budget over time, $B \in{2,4,8,16, \ldots}$, and for each $B$, tries all possible values of $n \in\left{2^{k}: k \in\left{1, \ldots, \log <em 2="2">{2}(B)\right}\right}$. For each combination of $B$ and $n$, the algorithm runs an instance of the (infinite horizon) SuccessiveHalving algorithm, which implicitly sets $R=\frac{B}{2 \log </em>$, thereby growing $R$ as $B$ increases. The main difference between the infinite horizon algorithm and Algorithm 1 is that the number of unique brackets grows over time instead of staying constant with each outer loop. We will analyze this version of HyPERBAND in more detail in Section 5 and use it as the launching point for the theoretical analysis of standard (finite horizon) Hyperband.}(n)</p>
<p>Note that $R$ is also the number of configurations evaluated in the bracket that performs the most exploration, i.e $s=s_{\max }$. In practice one may want $n \leq n_{\max }$ to limit overhead associated with training many configurations on a small budget, i.e., costs associated with initialization, loading a model, and validation. In this case, set $s_{\max }=\left\lfloor\log <em _max="\max">{\eta}\left(n</em>\right)\right\rfloor$. Alternatively, one can redefine one unit of resource so that $R$ is artificially smaller (i.e., if the desired maximum iteration is 100 k , defining one unit of resource to be 100 iterations will give $R=1,000$, whereas defining one unit to be 1 k iterations will give $R=100$ ). Thus, one unit of resource can be interpreted as the minimum desired resource and $R$ as the ratio between maximum resource and minimum resource.</p>
<h1>3.6 Setting $\eta$</h1>
<p>The value of $\eta$ is a knob that can be tuned based on practical user constraints. Larger values of $\eta$ correspond to more aggressive elimination schedules and thus fewer rounds of elimination; specifically, each round retains $1 / \eta$ configurations for a total of $\left\lfloor\log <em _eta="\eta">{\eta}(n)\right\rfloor+1$ rounds of elimination with $n$ configurations. If one wishes to receive a result faster at the cost of a sub-optimal asymptotic constant, one can increase $\eta$ to reduce the budget per bracket $B=\left(\left\lfloor\log </em>(R)\right\rfloor+1\right) R$. We stress that results are not very sensitive to the choice of $\eta$. If our theoretical bounds are optimized (see Section 5), they suggest choosing $\eta=e \approx 2.718$, but in practice we suggest taking $\eta$ to be equal to 3 or 4 .</p>
<p>Tuning $\eta$ will also change the number of brackets and consequently the number of different tradeoffs that Hyperband tries. Usually, the possible range of brackets is fairly constrained, since the number of brackets is logarithmic in $R$; namely, there are $\left(\left\lfloor\log <em _max="\max">{\eta}(R)\right\rfloor+1\right)=s</em>$ to $s=\left\lfloor\log }+1$ brackets. For our experiments in Section 4, we chose $\eta$ to provide 5 brackets for the specified $R$; for most problems, 5 is a reasonable number of $n$ versus $B / n$ tradeoffs to explore. However, for large $R$, using $\eta=3$ or 4 can give more brackets than desired. The number of brackets can be controlled in a few ways. First, as mentioned in the previous section, if $R$ is too large and overhead is an issue, then one may want to control the overhead by limiting the maximum number of configurations to $n_{\max }$, thereby also limiting $s_{\max }$. If overhead is not a concern and aggressive exploration is desired, one can (1) increase $\eta$ to reduce the number of brackets while maintaining $R$ as the maximum number of configurations in the most exploratory bracket, or (2) still use $\eta=3$ or 4 but only try brackets that do a baseline level of exploration, i.e., set $n_{\min }$ and only try brackets from $s_{\max <em _min="\min">{\eta}\left(n</em>\right)\right\rfloor$. For computationally intensive problems that have long training times and high-dimensional search spaces, we recommend the latter. Intuitively, if the number of configurations that can be trained to completion (i.e., trained using $R$ resources) in a reasonable amount of time is on the order of the dimension of the search space and not exponential in the dimension, then</p>
<p>it will be impossible to find a good configuration without using an aggressive exploratory tradeoff between $n$ and $B / n$.</p>
<h1>3.7 Overview of Theoretical Results</h1>
<p>The theoretical properties of Hyperband are best demonstrated through an example. Suppose there are $n$ configurations, each with a given terminal validation error $\nu_{i}$ for $i=1, \ldots, n$. Without loss of generality, index the configurations by performance so that $\nu_{1}$ corresponds to the best performing configuration, $\nu_{2}$ to the second best, and so on. Now consider the task of identifying the best configuration. The optimal strategy would allocate to each configuration $i$ the minimum resource required to distinguish it from $\nu_{1}$, i.e., enough so that the envelope functions (see Figure 2) bound the intermediate loss to be less than $\frac{\nu_{i}-\nu_{1}}{2}$ away from the terminal value. In contrast, the naive uniform allocation strategy, which allocates $B / n$ to each configuration, has to allocate to every configuration the maximum resource required to distinguish any arm $\nu_{i}$ from $\nu_{1}$. Remarkably, the budget required by SuccessiveHalving is only a small factor of the optimal because it capitalizes on configurations that are easy to distinguish from $\nu_{1}$.</p>
<p>The relative size of the budget required for uniform allocation and SuccessiveHalving depends on the envelope functions bounding deviation from terminal losses as well as the distribution from which $\nu_{i}$ 's are drawn. The budget required for SuccessiveHalving is smaller when the optimal $n$ versus $B / n$ tradeoff discussed in Section 3.1 requires fewer resources per configuration. Hence, if the envelope functions tighten quickly as a function of resource allocated, or the average distances between terminal losses is large, then SuCcessiveHalving can be substantially faster than uniform allocation. These intuitions are formalized in Section 5 and associated theorems/corollaries are provided that take into account the envelope functions and the distribution from which $\nu_{i}$ 's are drawn.</p>
<p>In practice, we do not have knowledge of either the envelope functions or the distribution of $\nu_{i}$ 's, both of which are integral in characterizing SuCcESSIVEHalving's required budget. With Hyperband we address this shortcoming by hedging our aggressiveness. We show in Section 5.3.3 that Hyperband, despite having no knowledge of the envelope functions nor the distribution of $\nu_{i}$ 's, requires a budget that is only log factors larger than that of SUCCESSIVEHALVING.</p>
<h2>4. Hyperparameter Optimization Experiments</h2>
<p>In this section, we evaluate the empirical behavior of Hyperband with three different resource types: iterations, data set subsamples, and feature samples. For all experiments, we compare Hyperband with three well known Bayesian optimization algorithms-SMAC, TPE, and Spearmint-using their default settings. We exclude Spearmint from the comparison set when there are conditional hyperparameters in the search space because it does not natively support them (Eggensperger et al., 2013). We also show results for SuccessiveHalving corresponding to repeating the most exploratory bracket of Hyperband to provide a baseline for aggressive early-stopping. ${ }^{6}$ Additionally, as standard baselines against which to measure</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>all speedups, we consider random search and "random $2 \times$," a variant of random search with twice the budget of other methods. Of the hybrid methods described in Section 2, we compare to a variant of SMAC using the early termination criterion proposed by Domhan et al. (2015) in the deep learning experiments described in Section 4.1. We think a comparison of Hyperband to more sophisticated hybrid methods introduced recently by Klein et al. (2017a) and Kandasamy et al. (2017) is a fruitful direction for future work.</p>
<p>In the experiments below, we followed these loose guidelines when determining how to configuration Hyperband:</p>
<ol>
<li>The maximum resource $R$ should be reasonable given the problem, but ideally large enough so that early-stopping is beneficial.</li>
<li>$\eta$ should depend on $R$ and be selected to yield $\approx 5$ brackets with a minimum of 3 brackets. This is to guarantee that Hyperband will use a baseline degree of early-stopping and prevent too coarse of a grid of $n$ vs $B$ tradeoffs.</li>
</ol>
<h1>4.1 Early-Stopping Iterative Algorithms for Deep Learning</h1>
<p>For this benchmark, we tuned a convolutional neural network ${ }^{7}$ with the same architecture as that used in Snoek et al. (2012) and Domhan et al. (2015). The search spaces used in the two previous works differ, and we used a search space similar to that of Snoek et al. (2012) with 6 hyperparameters for stochastic gradient decent and 2 hyperparameters for the response normalization layers (see Appendix A for details). In line with the two previous works, we used a batch size of 100 for all experiments.</p>
<p>Data sets: We considered three image classification data sets: CIFAR-10 (Krizhevsky, 2009), rotated MNIST with background images (MRBI) (Larochelle et al., 2007), and Street View House Numbers (SVHN) (Netzer et al., 2011). CIFAR-10 and SVHN contain $32 \times 32$ RGB images while MRBI contains $28 \times 28$ grayscale images. Each data set was split into a training, validation, and test set: (1) CIFAR-10 has 40k, 10k, and 10k instances; (2) MRBI has $10 \mathrm{k}, 2 \mathrm{k}$, and 50 k instances; and (3) SVHN has close to $600 \mathrm{k}, 6 \mathrm{k}$, and 26 k instances for training, validation, and test respectively. For all data sets, the only preprocessing performed on the raw images was demeaning.</p>
<p>Hyperband Configuration: For these experiments, one unit of resource corresponds to 100 mini-batch iterations ( 10 k examples with a batch size of 100). For CIFAR-10 and MRBI, $R$ was set to 300 (or 30 k total iterations). For SVHN, $R$ was set to 600 (or 60 k total iterations) to accommodate the larger training set. Given $R$ for these experiments, we set $\eta=4$ to yield five SuccessiveHalving brackets for Hyperband.</p>
<p>Results: Each searcher was given a total budget of $50 R$ per trial to return the best possible hyperparameter configuration. For Hyperband, the budget is sufficient to run the outer loop twice (for a total of 10 SuccessiveHalving brackets). For SMAC, TPE, and random search, the budget corresponds to training 50 different configurations to completion. Ten independent trials were performed for each searcher. The experiments took the equivalent of over 1 year of GPU hours on NVIDIA GRID K520 cards available on Amazon EC2 g2.8xlarge instances. We set a total budget constraint in terms of</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Average test error across 10 trials. Label "SMAC (early)" corresponds to SMAC with the early-stopping criterion proposed in Domhan et al. (2015) and label "bracket $s=4$ " corresponds to repeating the most exploratory bracket of HyPERBAND.
iterations instead of compute time to make comparisons hardware independent. ${ }^{8}$ Comparing progress by iterations instead of time ignores overhead costs, e.g. the cost of configuration selection for Bayesian methods and model initialization and validation costs for Hyperband. While overhead is hardware dependent, the overhead for Hyperband is below $5 \%$ on EC2 g2.8xlarge machines, so comparing progress by time passed would not change results significantly.</p>
<p>For CIFAR-10, the results in Figure 4(a) show that Hyperband is over an order-of-magnitude faster than its competitiors. For MRBI, Hyperband is over an order-of-</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>magnitude faster than standard configuration selection approaches and $5 \times$ faster than SMAC (early). For SVHN, while Hyperband finds a good configuration faster, Bayesian optimization methods are competitive and SMAC (early) outperforms Hyperband. The performance of SMAC (early) demonstrates there is merit to combining early-stopping and adaptive configuration selection.</p>
<p>Across the three data sets, Hyperband and SMAC (early) are the only two methods that consistently outperform random $2 \times$. On these data sets, Hyperband is over $20 \times$ faster than random search while SMAC (early) is $\leq 7 \times$ faster than random search within the evaluation window. In fact, the first result returned by Hyperband after using a budget of $5 R$ is often competitive with results returned by other searchers after using $50 R$. Additionally, Hyperband is less variable than other searchers across trials, which is highly desirable in practice (see Appendix A for plots with error bars).</p>
<p>As discussed in Section 3.6, for computationally expensive problems in high-dimensional search spaces, it may make sense to just repeat the most exploratory brackets. Similarly, if meta-data is available about a problem or it is known that the quality of a configuration is evident after allocating a small amount of resource, then one should just repeat the most exploratory bracket. Indeed, for these experiments, bracket $s=4$ vastly outperforms all other methods on CIFAR-10 and MRBI and is nearly tied with SMAC (early) for first on SVHN.</p>
<p>While we set $R$ for these experiments to facilitate comparison to Bayesian methods and random search, it is also reasonable to use infinite horizon Hyperband to grow the maximum resource until a desired level of performance is reached. We evaluate infinite horizon Hyperband on CIFAR-10 using $\eta=4$ and a starting budget of $B=2 R$. Figure 4(a) shows that infinite horizon Hyperband is competitive with other methods but does not perform as well as finite horizon Hyperband within the $50 R$ budget limit. The infinite horizon algorithm underperforms initially because it has to tune the maximum resource $R$ as well and starts with a less aggressive early-stopping rate. This demonstrates that in scenarios where a max resource is known, it is better to use the finite horizon algorithm. Hence, we focus on the finite horizon version of Hyperband for the remainder of our empirical studies.</p>
<p>Finally, CIFAR-10 is a very popular data set and state-of-the-art models achieve much lower error rates than what is shown in Figure 4. The difference in performance is mainly attributable to higher model complexities and data manipulation (i.e. using reflection or random cropping to artificially increase the data set size). If we limit the comparison to published results that use the same architecture and exclude data manipulation, the best human expert result for the data set is $18 \%$ error and the best hyperparameter optimized results are $15.0 \%$ for Snoek et al. (2012) ${ }^{9}$ and $17.2 \%$ for Domhan et al. (2015). These results exceed ours on CIFAR-10 because they train on $25 \%$ more data, by including the validation set, and also train for more epochs. When we train the best model found by Hyperband on the combined training and validation data for 300 epochs, the model achieved a test error of $17.0 \%$.</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>4.2 Data Set Subsampling</h1>
<p>We studied two different hyperparameter search optimization problems for which HyPERbAND uses data set subsamples as the resource. The first adopts an extensive framework presented in Feurer et al. (2015) that attempts to automate preprocessing and model selection. Due to certain limitations of the framework that fundamentally limited the impact of data set downsampling, we conducted a second experiment using a kernel classification task.</p>
<h3>4.2.1 117 Data Sets</h3>
<p>We used the framework introduced by Feurer et al. (2015), which explored a structured hyperparameter search space comprised of 15 classifiers, 14 feature preprocessing methods, and 4 data preprocessing methods for a total of 110 hyperparameters. We excluded the meta-learning component introduced in Feurer et al. (2015) used to warmstart Bayesian methods with promising configurations, in order to perform a fair comparison with random search and Hyperband. Similar to Feurer et al. (2015), we imposed a 3GB memory limit, a 6-minute timeout for each hyperparameter configuration and a one-hour time window to evaluate each searcher on each data set. Twenty trials of each searcher were performed per data set and all trials in aggregate took over a year of CPU time on n1-standard-1 instances from Google Cloud Compute. Additional details about our experimental framework are available in Appendix A.</p>
<p>Data sets: Feurer et al. (2015) used 140 binary and multiclass classification data sets from OpenML, but 23 of them are incompatible with the latest version of the OpenML plugin (Feurer, 2015), so we worked with the remaining 117 data sets. Due to the limitations of the experimental setup (discussed in Appendix A), we also separately considered 21 of these data sets, which demonstrated at least modest (though still sublinear) training speedups due to subsampling. Specifically, each of these 21 data sets showed on average at least a $3 \times$ speedup due to $8 \times$ downsampling on 100 randomly selected hyperparameter configurations.</p>
<p>Hyperband Configuration: Due to the wide range of dataset sizes, with some datasets having fewer than 10 k training points, we ran Hyperband with $\eta=3$ to allow for at least 3 brackets without being overly aggressive in downsampling on small datasets. $R$ was set to the full training set size for each data set and the maximum number of configurations for any bracket of SuccessiveHalving was limited to $n_{\max }=\max {9, R / 1000}$. This ensured that the most exploratory bracket of Hyperband will downsample at least twice. As mentioned in Section 3.6, when $n_{\max }$ is specified, the only difference when running the algorithm is $s_{\max }=\left\lfloor\log <em _max="\max">{\eta}\left(n</em>(R)\right\rfloor$.}\right)\right\rfloor$ instead of $\left\lfloor\log _{\eta</p>
<p>Results: The results on all 117 data sets in Figure 5(a,b) show that Hyperband outperforms random search in test error rank despite performing worse in validation error rank. Bayesian methods outperform Hyperband and random search in test error performance but also exhibit signs of overfitting to the validation set, as they outperform Hyperband by a larger margin on the validation error rank. Notably, random $2 \times$ outperforms all other methods. However, for the subset of 21 data sets, Figure 5(c) shows that Hyperband outperforms all other searchers on test error rank, including random $2 \times$ by a very small margin. While these results are more promising, the effectiveness of Hyperband was restricted in this experimental framework; for smaller data sets, the startup overhead was</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Average rank across all data sets for each searcher. For each data set, the searchers are ranked according to the average validation/test error across 20 trials.
high relative to total training time, while for larger data sets, only a handful of configurations could be trained within the hour window.</p>
<p>We note that while average rank plots like those in Figure 5 are an effective way to aggregate information across many searchers and data sets, they provide no indication about the magnitude of the differences between the performance of the methods. Figure 6, which charts the difference between the test error for each searcher and that of random search across all 117 datasets, highlights the small difference in the magnitude of the test errors across searchers.</p>
<p>These results are not surprising; as mentioned in Section 2.1, vanilla Bayesian optimization methods perform similarly to random search in high-dimensional search spaces. Feurer et al. (2015) showed that using meta-learning to warmstart Bayesian optimization methods improved performance in this high-dimensional setting. Using meta-learning to identify a</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Each line plots, for a single data set, the difference in test error versus random search for each search, where lower is better. Nearly all the lines fall within the $-0.5 \%$ and $0.5 \%$ band and, with the exception of a few outliers, the lines are mostly flat.
promising distribution from which to sample configurations as input into HyPERBAND is a direction for future work.</p>
<h1>4.2.2 Kernel Regularized Least Squares Classification</h1>
<p>For this benchmark, we tuned the hyperparameters of a kernel-based classifier on CIFAR-10. We used the multi-class regularized least squares classification model, which is known to have comparable performance to SVMs (Rifkin and Klautau, 2004; Agarwal et al., 2014) but can be trained significantly faster. ${ }^{10}$ The hyperparameters considered in the search space include preprocessing method, regularization, kernel type, kernel length scale, and other kernel specific hyperparameters (see Appendix A for more details). For Hyperband, we set $R=400$, with each unit of resource representing 100 datapoints, and $\eta=4$ to yield a total of 5 brackets. Each hyperparameter optimization algorithm was run for ten trials on Amazon EC2 m4.2xlarge instances; for a given trial, Hyperband was allowed to run for two outer loops, bracket $s=4$ was repeated 10 times, and all other searchers were run for 12 hours.</p>
<p>Figure 7 shows that Hyperband returned a good configuration after completing the first SuccessiveHalving bracket in approximately 20 minutes; other searchers failed to reach this error rate on average even after the entire 12 hours. Notably, Hyperband was able to evaluate over 250 configurations in this first bracket of SuccessiveHalving, while competitors were able to evaluate only three configurations in the same amount of time. Consequently, Hyperband is over $30 \times$ faster than Bayesian optimization methods and $70 \times$ faster than random search. Bracket $s=4$ sightly outperforms Hyperband but the terminal</p>
<p><sup id="fnref8:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Average test error of the best kernel regularized least square classification model found by each searcher on CIFAR-10. The color coded dashed lines indicate when the last trial of a given searcher finished.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Average test error of the best random features model found by each searcher on CIFAR-10. The test error for Hyperband and bracket $s=4$ are calculated in every evaluation instead of at the end of a bracket.
performance for the two algorithms are the same. Random $2 \times$ is competitive with SMAC and TPE.</p>
<h1>4.3 Feature Subsampling to Speed Up Approximate Kernel Classification</h1>
<p>Next, we examine the performance of Hyperband when using features as a resource on a random feature kernel approximations task. Features were randomly generated using the method described in Rahimi and Recht (2007) to approximate the RBF kernel, and these random features were then used as inputs to a ridge regression classifier. The hyperparameter search space included the preprocessing method, kernel length scale, and $L_{2}$ penalty. While it may seem natural to use infinite horizon Hyperband, since the fidelity of the approximation improves with more random features, in practice, the amount of available machine memory imposes a natural upper bound on the number of features. Thus, we used finite horizion Hyperband with a maximum resource of 100 k random features, which comfortably fit into a machine with 60 GB of memory. Additionally, we set one unit of resource to be 100 features, so $R=1000$. Again, we set $\eta=4$ to yield 5 brackets of SuccessiveHalving. We ran 10 trials of each searcher, with each trial lasting 12 hours on a n1-standard-16 machine from Google Cloud Compute. The results in Figure 8 show that Hyperband is around $6 \times$ faster than Bayesian methods and random search. Hyperband performs similarly to bracket $s=4$. Random $2 \times$ outperforms Bayesian optimization algorithms.</p>
<h3>4.4 Experimental Discussion</h3>
<p>While our experimental results show Hyperband is a promising algorithm for hyperparameter optimization, a few questions naturally arise:</p>
<ol>
<li>What impacts the speedups provided by HyPERBAND?</li>
<li>Why does SuccessiveHalving seem to outperform Hyperband?</li>
<li>What about hyperparameters that should depend on the resource?</li>
</ol>
<p>We next address each of these questions in turn.</p>
<h1>4.4.1 Factors Impacting the Performance of Hyperband</h1>
<p>For a given $R$, the most exploratory SuCcessiveHalving round performed by Hyperband evaluates $R$ configurations using a budget of $\left(\left\lfloor\log <em 0="0">{0}(R)\right\rfloor+1\right) R$, which gives an upper bound on the potential speedup over random search. If training time scales linearly with the resource, the maximum speedup offered by Hyperband compared to random search is $\frac{R}{\left(\left\lfloor\log </em>$. For the values of $\eta$ and $R$ used in our experiments, the maximum speedup over random search is approximately $50 \times$ given linear training time. However, we observe a range of speedups from $6 \times$ to $70 \times$ faster than random search. The differences in realized speedup can be explained by three factors:}(R)\right\rfloor+1\right)</p>
<ol>
<li>How training time scales with the given resource. In cases where training time is superlinear as a function of the resource, Hyperband can offer higher speedups. For instance, if training scales like a polynomial of degree $p&gt;1$, the maximum speedup for Hyperband over random search is approximately $\frac{\eta^{p}-1}{\eta^{p-1}} R$. In the kernel least square classifier experiment discussed in Section 4.2.2, the training time scaled quadratically as a function of the resource, which explains why the realized speedup of $70 \times$ is higher than the maximum expected speedup given linear scaling.</li>
<li>Overhead costs associated with training. Total evaluation time also depends on fixed overhead costs associated with evaluating each hyperparameter configuration, e.g., initializing a model, resuming previously trained models, and calculating validation error. For example, in the downsampling experiments on 117 data sets presented in Section 4.2.1, Hyperband did not provide significant speedup because many data sets could be trained in a matter of a few seconds and the initialization cost was high relative to training time.</li>
<li>The difficulty of finding a good configuration. Hyperparameter optimization problems can vary in difficulty. For instance, an 'easy' problem is one where a randomly sampled configuration is likely to result in a high-quality model, and thus we only need to evaluate a small number of configurations to find a good setting. In contrast, a 'hard' problem is one where an arbitrary configuration is likely to be bad, in which case many configurations must be considered. Hyperband leverages downsampling to boost the number of configurations that are evaluated, and thus is better suited for 'hard' problems where more evaluations are actually necessary to find a good setting. Generally, the difficulty of a problem scales with the dimensionality of the search space. For low-dimensional problems, the number of configurations evaluated by random search and Bayesian methods is exponential in the number of dimensions so good coverage can be achieved. For instance, the low-dimensional $(d=3)$ search space in our feature subsampling experiment in Section 4.3 helps explain why Hyperband is</li>
</ol>
<p>only $6 \times$ faster than random search. In contrast, for the neural network experiments in Section 4.1, we hypothesize that faster speedups are observed for Hyperband because the dimension of the search space is higher.</p>
<h1>4.4.2 Comparison to SuccessiveHalving</h1>
<p>With the exception of the LeNet experiment (Section 3.3) and the 117 Datasets experiment (Section 4.2.1), the most aggressive bracket of SuccessiveHalving outperformed Hyperband in all of our experiments. In hindsight, we should have just run bracket $s=4$, since aggressive early-stopping provides massive speedups on many of these benchmarking tasks. However, as previously mentioned, it was unknown a priori that bracket $s=4$ would perform the best and that is why we have to cycle through all possible brackets with Hyperband. Another question is what happens when one increases $s$ even further, i.e. instead of 4 rounds of elimination, why not 5 or even more with the same maximum resource $R$ ? In our case, $s=4$ was the most aggressive bracket we could run given the minimum resource per configuration limits imposed for the previous experiments. However, for larger data sets, it is possible to extend the range of possible values for $s$, in which case, Hyperband may either provide even faster speedups if more aggressive early-stopping helps or be slower by a small factor if the most aggressive brackets are essentially throwaways.</p>
<p>We believe prior knowledge about a task can be particularly useful for limiting the range of brackets explored by Hyperband. In our experience, aggressive early-stopping is generally safe for neural network tasks and even more aggressive early-stopping may be reasonable for larger data sets and longer training horizons. However, when pushing the degree of early-stopping by increasing $s$, one has to consider the additional overhead cost associated with examining more models. Hence, one way to leverage meta-learning would be to use learning curve convergence rate, difficulty of different search spaces, and overhead costs of related tasks to determine the brackets considered by Hyperband.</p>
<h3>4.4.3 Resource Dependent Hyperparameters</h3>
<p>In certain cases, the setting for a given hyperparameter should depend on the allocated resource. For example, the maximum tree depth regularization hyperparameter for random forests should be higher with more data and more features. However, the optimal tradeoff between maximum tree depth and the resource is unknown and can be data set specific. In these situations, the rate of convergence to the true loss is usually slow because the performance on a smaller resource is not indicative of that on a larger resource. Hence, these problems are particularly difficult for Hyperband, since the benefit of early-stopping can be muted. Again, while Hyperband will only be a small factor slower than that of SuccessiveHalving with the optimal early-stopping rate, we recommend removing the dependence of the hyperparameter on the resource if possible. For the random forest example, an alternative regularization hyperparameter is minimum samples per leaf, which is less dependent on the training set size. Additionally, the dependence can oftentimes be removed with simple normalization. For example, the regularization term for our kernel least squares experiments were normalized by the training set size to maintain a constant tradeoff between the mean-squared error and the regularization term.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<ol>
<li>The default SVM method in Scikit-learn is single core and takes hours to train on CIFAR-10, whereas a block coordinate descent least squares solver takes less than 10 minutes on an 8 core machine.</li>
</ol>
<p><a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref8:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1120 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1120</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1120</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-25.html">extraction-schema-25</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-203951955</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/1910.03620v1.pdf" target="_blank">Receding Horizon Curiosity</a></p>
                <p><strong>Paper Abstract:</strong> Sample-efficient exploration is crucial not only for discovering rewarding experiences but also for adapting to environment changes in a task-agnostic fashion. A principled treatment of the problem of optimal input synthesis for system identification is provided within the framework of sequential Bayesian experimental design. In this paper, we present an effective trajectory-optimization-based approximate solution of this otherwise intractable problem that models optimal exploration in an unknown Markov decision process (MDP). By interleaving episodic exploration with Bayesian nonlinear system identification, our algorithm takes advantage of the inductive bias to explore in a directed manner, without assuming prior knowledge of the MDP. Empirical evaluations indicate a clear advantage of the proposed algorithm in terms of the rate of convergence and the final model fidelity when compared to intrinsic-motivation-based algorithms employing exploration bonuses such as prediction error and information gain. Moreover, our method maintains a computational advantage over a recent model-based active exploration (MAX) algorithm, by focusing on the information gain along trajectories instead of seeking a global exploration policy. A reference implementation of our algorithm and the conducted experiments is publicly available.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1120.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1120.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RHC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Receding Horizon Curiosity</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model-based active exploration algorithm that plans trajectories to maximize information gain about an unknown dynamics model by combining Bayesian linear regression (last-layer Bayesian treatment) with trajectory optimization (multiple shooting) and two acquisition functions: uncertainty sampling (US) and expected variance reduction (EVR).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Receding Horizon Curiosity (RHC)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Model-based agent that represents dynamics as a Bayesian linear model over random Fourier features (Gaussian prior over last-layer parameters). It uses closed-form Gaussian belief propagation under a maximum-likelihood-observation approximation, and performs trajectory optimization (multiple shooting via CasADi) to select open-loop action sequences that maximize an information-theoretic acquisition function. After executing each episode, it updates the Bayesian linear model via incremental posterior updates and replans.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Active learning / Bayesian optimal experimental design (information-gain maximization) implemented as intrinsic-curiosity-driven trajectory planning; two acquisition functions used: uncertainty sampling (US) and expected variance reduction (EVR).</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Maintains a Gaussian posterior over dynamics parameters θ; computes acquisition objective over candidate trajectories (either sum of predictive variances along planned mean-state rollouts for US, or expected reduction in posterior entropy (D-optimality) under maximum-likelihood-observations for EVR); uses gradient-based multiple-shooting trajectory optimization to find the most informative open-loop action sequence, executes it, collects observations, updates posterior (Eq. 1), and repeats (receding-horizon interleaving of planning and model update). EVR explicitly accounts for how a prospective trajectory would reduce parameter uncertainty, while US rewards trajectories with high current predictive variance.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Classical continuous control benchmarks: MountainCar, Pendulum, CartPole (modified episodic versions)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Unknown dynamics (agent must learn forward model); continuous state and action spaces; deterministic simulation assumed during planning (maximum-likelihood observation assumption); low-dimensional observations provided (e.g., MountainCar: position+velocity; Pendulum: [cos θ, sin θ, θ]; CartPole: [x, cos θ, sin θ, ẋ, θ]); episodic interactions with sparse/no task reward during exploration phase.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Low-dimensional continuous control tasks: MountainCar (state dim ≈2, episode length up to 130 steps), Pendulum (state dim 3, 100 steps per episode, 80ms step), CartPole (state dim 5, 100 steps at 50Hz). Planning horizons used for trajectory optimization were long (T between ~150 and 200 in experiments), and number of random-Fourier features varied by environment (MountainCar: 20, Pendulum: 90, CartPole: 80).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>RHC attained the highest achievable model log-likelihood (approximated by a model trained on 10^4 uniform samples) fastest among compared methods, reaching that level within the 20-episode experimental budget in each environment; RHC EVR converged faster than RHC US when EVR was computationally feasible (only evaluated on MountainCar due to memory costs). Also produced lower cumulative cost (better downstream task performance) faster than baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>High in the tested low-dimensional settings: reached top model log-likelihood and good downstream task performance within ≤20 episodes; EVR variant required fewer episodes than US when run (EVR evaluated only on MountainCar due to memory demands).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Explicitly exploration-driven: optimization objective is information gain (no primary extrinsic task reward used during exploration episodes). Exploitation is indirect: improved model is reused for downstream planning/tasks after exploration. The interleaved model-update and replanning schedule mitigates divergence from planned trajectories (receding-horizon behavior provides implicit correction).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared against: MAX (model-based active exploration via ensemble disagreement), SAC with squared prediction error bonus (SAC PE), SAC with information-gain bonus (SAC IG), and uniform random exploration (RAND).</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Trajectory-optimization-based active experimental design with a Bayesian last-layer (RHC) outperforms model-free intrinsic-motivation approaches (SAC PE, SAC IG) and the MAX ensemble-based method in terms of model log-likelihood convergence rate and downstream control cost in the tested classical control environments; RHC EVR (when feasible) converges faster than US; RHC is computationally cheaper and lower variance than MAX because it optimizes information gain along planned trajectories rather than solving a full RL problem per episode.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Not real-time capable in current implementation; EVR has high memory demands (evaluated only on MountainCar); algorithm relies on deterministic (maximum-likelihood) state-propagation approximation which may fail in stochastic/partially observable domains; scalability to high-dimensional observation/action spaces is limited (needs cheaper trajectory optimization, dimensionality reduction, or more expressive but tractable probabilistic models).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Receding Horizon Curiosity', 'publication_date_yy_mm': '2019-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1120.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1120.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MAX</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Model-based Active Exploration (MAX)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ensemble-based model-based exploration method that represents dynamics uncertainty via an ensemble of deterministic networks and uses model-free RL to learn an exploration policy that maximizes ensemble disagreement (proxy for information gain).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Model-based active exploration</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MAX (Model-based Active Exploration)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Maintains an ensemble of deterministic forward-dynamics models; computes ensemble disagreement as intrinsic reward signal; uses a model-free RL algorithm to learn an exploration policy that seeks states/actions with large model disagreement.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Ensemble-disagreement-driven active exploration (information-gain proxy via model ensemble disagreement) combined with policy learning (model-free RL to produce exploration policy).</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Adapts by training an ensemble of dynamics predictors on collected data, computing disagreement across ensemble predictions as an intrinsic reward, and updating a policy (via RL) to visit regions of high disagreement; as the model ensemble is updated with new data, disagreement decreases in explored regions, guiding policy to other uncertain regions.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Same classical continuous control benchmarks used in this paper (MountainCar, Pendulum, CartPole)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Unknown continuous dynamics, continuous state/action spaces, episodic simulation; ensemble methods target epistemic uncertainty in learned deterministic predictors.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Same low-dimensional continuous control tasks as above (state dims ≈2–5, episode lengths 100–130 steps); learning a global exploration policy (inner-loop RL) increases computational complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Performed better than model-free SAC variants and random exploration in terms of model log-likelihood and downstream task cost, but consistently worse than RHC in the reported experiments; had higher wall-clock runtime than RHC because it solves a full RL problem in the inner loop.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Moderate: better than intrinsic-motivation SAC variants but less sample-efficient than RHC under the same episodic budget (20 episodes).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Balances exploration via learned policy trained to maximize disagreement; exploitation not primary during exploration phase (policy explicitly seeks uncertain regions).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared against RHC (US and EVR), SAC PE, SAC IG, and RAND in the experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>MAX avoids overcommitment problems seen in some intrinsic-reward methods and performs better than SAC-based intrinsic approaches, but is outperformed by trajectory-optimization-based RHC in model accuracy and downstream control cost; MAX is computationally heavier due to inner-loop RL.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>High computational cost (solves a full RL problem every episode), higher variance across runs than RHC, and still inferior model fidelity and downstream performance compared to RHC in these low-dimensional control tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Receding Horizon Curiosity', 'publication_date_yy_mm': '2019-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1120.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1120.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SAC-PE/IG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Soft Actor-Critic with exploration bonuses (Squared Prediction Error / Information Gain)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Model-free off-policy RL (Soft Actor-Critic) augmented with intrinsic-reward exploration bonuses: squared prediction error (PE) or incremental parameter-entropy reduction (IG).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Soft Actor-Critic with prediction-error (SAC PE) and with information-gain bonus (SAC IG)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Base RL algorithm is SAC (stochastic actor, maximum-entropy objective). Two variants add intrinsic bonuses: (1) squared prediction error r_pe(st,at) = (s_{t+1} - E_p[s_{t+1}|s_t,a_t])^2, where p is the current model; (2) information-gain bonus r_ig = H(θ|X_t) - H(θ|X_{t+1}) (parameter entropy reduction). These are added to RL reward to encourage exploration while SAC optimizes expected return.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Intrinsic motivation / curiosity-driven exploration via exploration bonuses (prediction error and parameter-entropy information gain).</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Adaptation is via reward shaping: the agent's policy is updated by RL to maximize a mixture of extrinsic and intrinsic rewards; intrinsic terms favor transitions where the model prediction error is large or parameter entropy decreases; as the model updates, intrinsic reward signal changes, guiding policy to new regions.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Same benchmarks (MountainCar, Pendulum, CartPole) used in this paper's experiments</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Unknown continuous dynamics; continuous state/action spaces; sparse or absent extrinsic reward during exploration phase; deterministic simulations assumed for evaluation but method is model-free.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Low-dimensional continuous control tasks (state dims ~2–5, episode lengths 100–130 steps); learning a policy with SAC requires large replay buffers and many gradient steps (buffer size 50,000 in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Performed on the level of random exploration (RAND) in the experiments: did not improve model log-likelihood or downstream task performance compared to random, attributed to 'overcommitment' behavior where early high intrinsic reward leads to suboptimal commitment and slow discovery of more distant state-action regions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Low in these experiments — comparable to random exploration; did not achieve fast model convergence within the 20-episode budget.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Handled implicitly via RL objective (policy learns to maximize combined extrinsic+intrinsic reward), but intrinsic bonuses produced overcommitment (early local high intrinsic rewards cause the agent to exploit those rather than explore globally).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared against RHC (US/EVR), MAX, and RAND in the reported evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Model-free intrinsic-reward methods (SAC PE, SAC IG) failed to outperform random exploration in these underactuated continuous control tasks, showing limitations of one-step greedy curiosity signals and susceptibility to overcommitment; they are also computationally cheaper per episode than trajectory optimization but less sample-efficient for learning a generalizable model.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Overcommitment behavior leading to poor exploration beyond early high-intrinsic-reward regions; ineffective at improving model fidelity within the limited episode budget; comparable to random exploration in tested tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Receding Horizon Curiosity', 'publication_date_yy_mm': '2019-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1120.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1120.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAND</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Uniform Random Exploration</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline exploration strategy that samples actions uniformly (or according to a simple fixed distribution) without using model uncertainty or information-gain signals.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Uniform Random Exploration (RAND)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Exploration baseline that executes uniformly random action sequences (or simple stochastic policy) during episodes, without model-based planning or intrinsic rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>none (non-adaptive, random exploration)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>No adaptation: actions are sampled randomly, and model updates (if any) are passive, from the collected random data.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Same classical continuous control benchmarks (MountainCar, Pendulum, CartPole) used for comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Unknown continuous dynamics; continuous state/action spaces; serves as a non-adaptive baseline under identical episodic budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>As above: low-dimensional continuous tasks, episodes lengths 100–130 steps depending on environment.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Baseline performance: SAC PE and SAC IG performed on the level of RAND; RAND produced substantially worse model log-likelihood and downstream control performance than RHC and was outperformed by MAX.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Low: random exploration did not efficiently cover informative areas of state-action space within the 20-episode budget.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>No tradeoff mechanism—pure exploration without exploitation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Used as a baseline for RHC, MAX, and SAC variants in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Serves as a lower baseline demonstrating that naive exploration is insufficient for efficient model learning in the tested underactuated continuous control tasks; RHC significantly outperforms RAND in both model likelihood and downstream control cost.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Inefficient coverage of informative transitions; poor sample efficiency for model identification and downstream task performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Receding Horizon Curiosity', 'publication_date_yy_mm': '2019-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Model-based active exploration <em>(Rating: 2)</em></li>
                <li>Planning to be surprised: Optimal bayesian exploration in dynamic environments <em>(Rating: 2)</em></li>
                <li>Sequential bayesian optimal experimental design via approximate dynamic programming <em>(Rating: 2)</em></li>
                <li>Belief space planning assuming maximum likelihood observations <em>(Rating: 2)</em></li>
                <li>Vime: Variational information maximizing exploration <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1120",
    "paper_id": "paper-203951955",
    "extraction_schema_id": "extraction-schema-25",
    "extracted_data": [
        {
            "name_short": "RHC",
            "name_full": "Receding Horizon Curiosity",
            "brief_description": "A model-based active exploration algorithm that plans trajectories to maximize information gain about an unknown dynamics model by combining Bayesian linear regression (last-layer Bayesian treatment) with trajectory optimization (multiple shooting) and two acquisition functions: uncertainty sampling (US) and expected variance reduction (EVR).",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Receding Horizon Curiosity (RHC)",
            "agent_description": "Model-based agent that represents dynamics as a Bayesian linear model over random Fourier features (Gaussian prior over last-layer parameters). It uses closed-form Gaussian belief propagation under a maximum-likelihood-observation approximation, and performs trajectory optimization (multiple shooting via CasADi) to select open-loop action sequences that maximize an information-theoretic acquisition function. After executing each episode, it updates the Bayesian linear model via incremental posterior updates and replans.",
            "adaptive_design_method": "Active learning / Bayesian optimal experimental design (information-gain maximization) implemented as intrinsic-curiosity-driven trajectory planning; two acquisition functions used: uncertainty sampling (US) and expected variance reduction (EVR).",
            "adaptation_strategy_description": "Maintains a Gaussian posterior over dynamics parameters θ; computes acquisition objective over candidate trajectories (either sum of predictive variances along planned mean-state rollouts for US, or expected reduction in posterior entropy (D-optimality) under maximum-likelihood-observations for EVR); uses gradient-based multiple-shooting trajectory optimization to find the most informative open-loop action sequence, executes it, collects observations, updates posterior (Eq. 1), and repeats (receding-horizon interleaving of planning and model update). EVR explicitly accounts for how a prospective trajectory would reduce parameter uncertainty, while US rewards trajectories with high current predictive variance.",
            "environment_name": "Classical continuous control benchmarks: MountainCar, Pendulum, CartPole (modified episodic versions)",
            "environment_characteristics": "Unknown dynamics (agent must learn forward model); continuous state and action spaces; deterministic simulation assumed during planning (maximum-likelihood observation assumption); low-dimensional observations provided (e.g., MountainCar: position+velocity; Pendulum: [cos θ, sin θ, θ]; CartPole: [x, cos θ, sin θ, ẋ, θ]); episodic interactions with sparse/no task reward during exploration phase.",
            "environment_complexity": "Low-dimensional continuous control tasks: MountainCar (state dim ≈2, episode length up to 130 steps), Pendulum (state dim 3, 100 steps per episode, 80ms step), CartPole (state dim 5, 100 steps at 50Hz). Planning horizons used for trajectory optimization were long (T between ~150 and 200 in experiments), and number of random-Fourier features varied by environment (MountainCar: 20, Pendulum: 90, CartPole: 80).",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "RHC attained the highest achievable model log-likelihood (approximated by a model trained on 10^4 uniform samples) fastest among compared methods, reaching that level within the 20-episode experimental budget in each environment; RHC EVR converged faster than RHC US when EVR was computationally feasible (only evaluated on MountainCar due to memory costs). Also produced lower cumulative cost (better downstream task performance) faster than baselines.",
            "performance_without_adaptation": null,
            "sample_efficiency": "High in the tested low-dimensional settings: reached top model log-likelihood and good downstream task performance within ≤20 episodes; EVR variant required fewer episodes than US when run (EVR evaluated only on MountainCar due to memory demands).",
            "exploration_exploitation_tradeoff": "Explicitly exploration-driven: optimization objective is information gain (no primary extrinsic task reward used during exploration episodes). Exploitation is indirect: improved model is reused for downstream planning/tasks after exploration. The interleaved model-update and replanning schedule mitigates divergence from planned trajectories (receding-horizon behavior provides implicit correction).",
            "comparison_methods": "Compared against: MAX (model-based active exploration via ensemble disagreement), SAC with squared prediction error bonus (SAC PE), SAC with information-gain bonus (SAC IG), and uniform random exploration (RAND).",
            "key_results": "Trajectory-optimization-based active experimental design with a Bayesian last-layer (RHC) outperforms model-free intrinsic-motivation approaches (SAC PE, SAC IG) and the MAX ensemble-based method in terms of model log-likelihood convergence rate and downstream control cost in the tested classical control environments; RHC EVR (when feasible) converges faster than US; RHC is computationally cheaper and lower variance than MAX because it optimizes information gain along planned trajectories rather than solving a full RL problem per episode.",
            "limitations_or_failures": "Not real-time capable in current implementation; EVR has high memory demands (evaluated only on MountainCar); algorithm relies on deterministic (maximum-likelihood) state-propagation approximation which may fail in stochastic/partially observable domains; scalability to high-dimensional observation/action spaces is limited (needs cheaper trajectory optimization, dimensionality reduction, or more expressive but tractable probabilistic models).",
            "uuid": "e1120.0",
            "source_info": {
                "paper_title": "Receding Horizon Curiosity",
                "publication_date_yy_mm": "2019-10"
            }
        },
        {
            "name_short": "MAX",
            "name_full": "Model-based Active Exploration (MAX)",
            "brief_description": "An ensemble-based model-based exploration method that represents dynamics uncertainty via an ensemble of deterministic networks and uses model-free RL to learn an exploration policy that maximizes ensemble disagreement (proxy for information gain).",
            "citation_title": "Model-based active exploration",
            "mention_or_use": "use",
            "agent_name": "MAX (Model-based Active Exploration)",
            "agent_description": "Maintains an ensemble of deterministic forward-dynamics models; computes ensemble disagreement as intrinsic reward signal; uses a model-free RL algorithm to learn an exploration policy that seeks states/actions with large model disagreement.",
            "adaptive_design_method": "Ensemble-disagreement-driven active exploration (information-gain proxy via model ensemble disagreement) combined with policy learning (model-free RL to produce exploration policy).",
            "adaptation_strategy_description": "Adapts by training an ensemble of dynamics predictors on collected data, computing disagreement across ensemble predictions as an intrinsic reward, and updating a policy (via RL) to visit regions of high disagreement; as the model ensemble is updated with new data, disagreement decreases in explored regions, guiding policy to other uncertain regions.",
            "environment_name": "Same classical continuous control benchmarks used in this paper (MountainCar, Pendulum, CartPole)",
            "environment_characteristics": "Unknown continuous dynamics, continuous state/action spaces, episodic simulation; ensemble methods target epistemic uncertainty in learned deterministic predictors.",
            "environment_complexity": "Same low-dimensional continuous control tasks as above (state dims ≈2–5, episode lengths 100–130 steps); learning a global exploration policy (inner-loop RL) increases computational complexity.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Performed better than model-free SAC variants and random exploration in terms of model log-likelihood and downstream task cost, but consistently worse than RHC in the reported experiments; had higher wall-clock runtime than RHC because it solves a full RL problem in the inner loop.",
            "performance_without_adaptation": null,
            "sample_efficiency": "Moderate: better than intrinsic-motivation SAC variants but less sample-efficient than RHC under the same episodic budget (20 episodes).",
            "exploration_exploitation_tradeoff": "Balances exploration via learned policy trained to maximize disagreement; exploitation not primary during exploration phase (policy explicitly seeks uncertain regions).",
            "comparison_methods": "Compared against RHC (US and EVR), SAC PE, SAC IG, and RAND in the experiments.",
            "key_results": "MAX avoids overcommitment problems seen in some intrinsic-reward methods and performs better than SAC-based intrinsic approaches, but is outperformed by trajectory-optimization-based RHC in model accuracy and downstream control cost; MAX is computationally heavier due to inner-loop RL.",
            "limitations_or_failures": "High computational cost (solves a full RL problem every episode), higher variance across runs than RHC, and still inferior model fidelity and downstream performance compared to RHC in these low-dimensional control tasks.",
            "uuid": "e1120.1",
            "source_info": {
                "paper_title": "Receding Horizon Curiosity",
                "publication_date_yy_mm": "2019-10"
            }
        },
        {
            "name_short": "SAC-PE/IG",
            "name_full": "Soft Actor-Critic with exploration bonuses (Squared Prediction Error / Information Gain)",
            "brief_description": "Model-free off-policy RL (Soft Actor-Critic) augmented with intrinsic-reward exploration bonuses: squared prediction error (PE) or incremental parameter-entropy reduction (IG).",
            "citation_title": "Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor",
            "mention_or_use": "use",
            "agent_name": "Soft Actor-Critic with prediction-error (SAC PE) and with information-gain bonus (SAC IG)",
            "agent_description": "Base RL algorithm is SAC (stochastic actor, maximum-entropy objective). Two variants add intrinsic bonuses: (1) squared prediction error r_pe(st,at) = (s_{t+1} - E_p[s_{t+1}|s_t,a_t])^2, where p is the current model; (2) information-gain bonus r_ig = H(θ|X_t) - H(θ|X_{t+1}) (parameter entropy reduction). These are added to RL reward to encourage exploration while SAC optimizes expected return.",
            "adaptive_design_method": "Intrinsic motivation / curiosity-driven exploration via exploration bonuses (prediction error and parameter-entropy information gain).",
            "adaptation_strategy_description": "Adaptation is via reward shaping: the agent's policy is updated by RL to maximize a mixture of extrinsic and intrinsic rewards; intrinsic terms favor transitions where the model prediction error is large or parameter entropy decreases; as the model updates, intrinsic reward signal changes, guiding policy to new regions.",
            "environment_name": "Same benchmarks (MountainCar, Pendulum, CartPole) used in this paper's experiments",
            "environment_characteristics": "Unknown continuous dynamics; continuous state/action spaces; sparse or absent extrinsic reward during exploration phase; deterministic simulations assumed for evaluation but method is model-free.",
            "environment_complexity": "Low-dimensional continuous control tasks (state dims ~2–5, episode lengths 100–130 steps); learning a policy with SAC requires large replay buffers and many gradient steps (buffer size 50,000 in experiments).",
            "uses_adaptive_design": false,
            "performance_with_adaptation": "Performed on the level of random exploration (RAND) in the experiments: did not improve model log-likelihood or downstream task performance compared to random, attributed to 'overcommitment' behavior where early high intrinsic reward leads to suboptimal commitment and slow discovery of more distant state-action regions.",
            "performance_without_adaptation": null,
            "sample_efficiency": "Low in these experiments — comparable to random exploration; did not achieve fast model convergence within the 20-episode budget.",
            "exploration_exploitation_tradeoff": "Handled implicitly via RL objective (policy learns to maximize combined extrinsic+intrinsic reward), but intrinsic bonuses produced overcommitment (early local high intrinsic rewards cause the agent to exploit those rather than explore globally).",
            "comparison_methods": "Compared against RHC (US/EVR), MAX, and RAND in the reported evaluations.",
            "key_results": "Model-free intrinsic-reward methods (SAC PE, SAC IG) failed to outperform random exploration in these underactuated continuous control tasks, showing limitations of one-step greedy curiosity signals and susceptibility to overcommitment; they are also computationally cheaper per episode than trajectory optimization but less sample-efficient for learning a generalizable model.",
            "limitations_or_failures": "Overcommitment behavior leading to poor exploration beyond early high-intrinsic-reward regions; ineffective at improving model fidelity within the limited episode budget; comparable to random exploration in tested tasks.",
            "uuid": "e1120.2",
            "source_info": {
                "paper_title": "Receding Horizon Curiosity",
                "publication_date_yy_mm": "2019-10"
            }
        },
        {
            "name_short": "RAND",
            "name_full": "Uniform Random Exploration",
            "brief_description": "Baseline exploration strategy that samples actions uniformly (or according to a simple fixed distribution) without using model uncertainty or information-gain signals.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Uniform Random Exploration (RAND)",
            "agent_description": "Exploration baseline that executes uniformly random action sequences (or simple stochastic policy) during episodes, without model-based planning or intrinsic rewards.",
            "adaptive_design_method": "none (non-adaptive, random exploration)",
            "adaptation_strategy_description": "No adaptation: actions are sampled randomly, and model updates (if any) are passive, from the collected random data.",
            "environment_name": "Same classical continuous control benchmarks (MountainCar, Pendulum, CartPole) used for comparisons",
            "environment_characteristics": "Unknown continuous dynamics; continuous state/action spaces; serves as a non-adaptive baseline under identical episodic budgets.",
            "environment_complexity": "As above: low-dimensional continuous tasks, episodes lengths 100–130 steps depending on environment.",
            "uses_adaptive_design": false,
            "performance_with_adaptation": null,
            "performance_without_adaptation": "Baseline performance: SAC PE and SAC IG performed on the level of RAND; RAND produced substantially worse model log-likelihood and downstream control performance than RHC and was outperformed by MAX.",
            "sample_efficiency": "Low: random exploration did not efficiently cover informative areas of state-action space within the 20-episode budget.",
            "exploration_exploitation_tradeoff": "No tradeoff mechanism—pure exploration without exploitation.",
            "comparison_methods": "Used as a baseline for RHC, MAX, and SAC variants in experiments.",
            "key_results": "Serves as a lower baseline demonstrating that naive exploration is insufficient for efficient model learning in the tested underactuated continuous control tasks; RHC significantly outperforms RAND in both model likelihood and downstream control cost.",
            "limitations_or_failures": "Inefficient coverage of informative transitions; poor sample efficiency for model identification and downstream task performance.",
            "uuid": "e1120.3",
            "source_info": {
                "paper_title": "Receding Horizon Curiosity",
                "publication_date_yy_mm": "2019-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Model-based active exploration",
            "rating": 2,
            "sanitized_title": "modelbased_active_exploration"
        },
        {
            "paper_title": "Planning to be surprised: Optimal bayesian exploration in dynamic environments",
            "rating": 2,
            "sanitized_title": "planning_to_be_surprised_optimal_bayesian_exploration_in_dynamic_environments"
        },
        {
            "paper_title": "Sequential bayesian optimal experimental design via approximate dynamic programming",
            "rating": 2,
            "sanitized_title": "sequential_bayesian_optimal_experimental_design_via_approximate_dynamic_programming"
        },
        {
            "paper_title": "Belief space planning assuming maximum likelihood observations",
            "rating": 2,
            "sanitized_title": "belief_space_planning_assuming_maximum_likelihood_observations"
        },
        {
            "paper_title": "Vime: Variational information maximizing exploration",
            "rating": 1,
            "sanitized_title": "vime_variational_information_maximizing_exploration"
        }
    ],
    "cost": 0.01330525,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Receding Horizon Curiosity
8 Oct 2019</p>
<p>Matthias Schultheis matthias.schultheis@gmail.com 
Boris Belousov 
Hany Abdulsamad 
Jan Peters peters@ias.tu-darmstadt.de 
Max Planck Institute for Intelligent Systems
Robot Learning Group
TübingenGermany</p>
<p>Department of Computer Science
Technische Universität Darmstadt
Germany</p>
<p>Introduction</p>
<p>Receding Horizon Curiosity
8 Oct 2019Bayesian explorationartificial curiositymodel predictive control
Sample-efficient exploration is crucial not only for discovering rewarding experiences but also for adapting to environment changes in a task-agnostic fashion. A principled treatment of the problem of optimal input synthesis for system identification is provided within the framework of sequential Bayesian experimental design. In this paper, we present an effective trajectory-optimizationbased approximate solution of this otherwise intractable problem that models optimal exploration in an unknown Markov decision process (MDP). By interleaving episodic exploration with Bayesian nonlinear system identification, our algorithm takes advantage of the inductive bias to explore in a directed manner, without assuming prior knowledge of the MDP. Empirical evaluations indicate a clear advantage of the proposed algorithm in terms of the rate of convergence and the final model fidelity when compared to intrinsic-motivation-based algorithms employing exploration bonuses such as prediction error and information gain. Moreover, our method maintains a computational advantage over a recent model-based active exploration (MAX) algorithm, by focusing on the information gain along trajectories instead of seeking a global exploration policy. A reference implementation of our algorithm and the conducted experiments is publicly available 1 .</p>
<p>Introduction</p>
<p>Learning agents are expected to constantly adapt to changing environments without an otherwise explicit task specification. A subclass of such changes relates to predicting the consequences of the agent's own actions, as captured by the forward model of the environment, which is of crucial importance for the purposes of planning and task-solving. When the forward model parameters are unknown, what sequence of actions should the agent take to reveal the most about the system, i.e., what course of action facilitates the most sample-efficient exploration? To answer this question, the information content in the observations needs to be quantified, which becomes possible if the agent maintains a probability distribution over the model parameters. The sequential decision making problem with such a probabilistic model whose parameters are not directly observable is known as the partially observable Markov decision process (POMDP) [1].</p>
<p>Finding an exact solution of continuous-state/action POMDPs is intractable in general [2]. Therefore, various approximations are commonly employed [3]. In particular, one-step greedy heuristics-exploration bonuses-have gained prominence recently due to the ease of implementation, wide range of applicability, and good empirical performance in game domains [4]. However, they neglect long-term effects of control actions and therefore struggle in underactuated continuous control domains, focusing on immediate curiosity satisfaction [5]. As a remedy, it was proposed that the agent should plan exploration [6], purposefully driving the system to the states that provide the largest information gain, thus performing active exploration.</p>
<p>Many active exploration approaches have been proposed in the past [7,8,9,10,11,12], differing chiefly in how the model is represented and how it is used for planning. The most recent representative from this family of approaches is the model-based active exploration (MAX) algorithm [5], that represents the uncertainty in the dynamics via an ensemble of deterministic networks, and uses a model-free reinforcement learning (RL) algorithm to find an exploration policy optimal with respect to the ensemble. Such approach is computationally demanding because it solves a full RL problem in the inner loop to only use the optimized policy for performing a few exploratory steps.</p>
<p>In this paper, we present a principled model-based active exploration method, which contrasts with the recent computationally expensive RL-based approaches. The proposed exploration algorithm represents the uncertainty in the model by a distribution over the parameters in a shallow Bayesian network and finds optimally explorative actions via trajectory optimization based on the learned model. Since rigid-body dynamics can be written as a dot product between a vector of state-action features and a vector of physics parameters, we adopt a similar model structure with generic feature functions and a Gaussian vector of unknown parameters. Due to the linear-Gaussian structure, belief space dynamics can then be obtained in closed-form and incorporated into the trajectory optimization formulation, capturing the effect of current actions on future information gain.</p>
<p>The proposed method, termed receding horizon curiosity (RHC), addresses several challenges involved in the design and implementation of actively exploring agents. First, the agent's beliefs must be represented and propagated in time to estimate the information gain. A combination of approximations is needed and it is not clear a priori if the resulting algorithm will work. RHC employs Gaussian beliefs and a maximum likelihood observation assumption to represent and propagate the beliefs. Second, the information gain objective needs to be evaluated and optimized. Again, a number of approximations are involved, that require empirical evaluation. RHC exploits the Gaussianlinear model structure to evaluate the information gain and it relies on trajectory optimization to maximize it. Finally, a key feature of RHC is the interleaved optimal exploration and model updating, which turns out to be sufficient for promoting efficient model-learning, as shown by the experimental evaluations. On the whole, RHC compares favorably with state-of-the-art model-free intrinsic motivation approaches in terms of the model error and downstream task performance in classical continuous control environments, and compared to MAX, it is computationally far less demanding and has lower variance over runs.</p>
<p>Foundations</p>
<p>In this section, the background on active learning, Bayesian linear regression, random Fourier features, model-based reinforcement learning, and multiple shooting methods is provided.</p>
<p>Active Learning</p>
<p>In supervised learning, a training set is predefined and fixed. However, if an agent is allowed to choose the instances to train on, learning can potentially progress faster and require fewer samples. Active learning [13], also known as optimal experiment design [9], is an area of statistical learning that addresses exactly the question of how to choose the data points for learning. Choosing an optimal subset of points involves a combinatorial number of possibilities. Therefore, approximations of the optimal value function, known as acquisition functions, are employed for query point selection.</p>
<p>Perhaps the most straightforward and common query framework is uncertainty sampling [13]. In this framework, an active learner queries the instance x ∈ X for which the model output y ∈ Y is least certain. With the entropy of the output H(y|x) as the measure of uncertainty, the following acquisition function needs to be maximized
α us (x) = H(y|x).
Compared to other approaches, uncertainty sampling is computationally relatively light, and if the likelihood belongs to an exponential family, the entropy can even be obtained in closed-form. A drawback of uncertainty sampling is that it fails if the underlying system is stochastic, as it cannot distinguish between aleatoric and epistemic uncertainty.</p>
<p>To fix the shortcomings of uncertainty sampling, expected variance reduction [13] was proposed, that explicitly takes into account the model variance, by minimizing the acquisition function
α evr (x) = var (M | D ∪ (x, y)) ,
where var (M | D ∪ (x, y)) denotes a measure of variance of the model M trained on the extended dataset D * = D ∪ (x, y). For Bayesian linear regression, the posterior entropy and the predictive variance are commonly used as measures of the model variance-both are available in closed-form and correspond to well-known alphabetic optimal design criteria [14]. Under mild assumptions [15], closed-form expressions for the output variance can also be obtained for more flexible models, such as neural networks, Gaussian mixture models, and locally-weighted linear regression [13].</p>
<p>Bayesian Linear Regression</p>
<p>Linear regression [16] assumes that the output y ∈ R is given by a linear function θ T φ(x) in the features φ(x) ∈ R m of the input x ∈ R n and parameters θ ∈ R m . The output uncertainty is captured by a probability distribution, most commonly-the normal distribution,
p(y | x; θ) = N y | θ T φ(x), β −1 .
In Bayesian linear regression [16], not only the output y but also the parameters θ are assumed to be uncertain. Conveniently, given a Gaussian prior p(θ | D) = N (θ | µ, Σ), the posterior after observing a data point (x, y) is also Gaussian, p(θ | D ∪ (x, y)) = N (θ | µ * , Σ * ), with parameters
µ * = Σ * Σ −1 µ + βΦ T Y , Σ −1 * = Σ −1 + βΦ T Φ,(1)
where the features and targets corresponding to the new data point (x, y) are aggregated into the design matrix Φ and the vector of targets Y, as described in [16]. Along with the posterior, the predictive distribution plays an important role,
p(y * | x * ; D ∪ (x, y)) = N (y * | µ T * φ(x * ), σ 2 * (x * )), where σ 2 * (x * ) = β −1 + φ(x * ) T Σ * φ(x * )
. If data arrives sequentially, the posterior can be updated incrementally, taking the current posterior as the prior for the next data point. This procedure, known as iterative least squares [16], enables efficient processing of sequential data.</p>
<p>Random Fourier Features</p>
<p>Random Fourier features provide a powerful representation by approximating a Gaussian process with an exponentiated quadratic kernel [17]. Given an input x ∈ R n , following [18], the i-th feature is defined as φ i (x) = sin( n j=1 P ij x j /ν j + ϕ i ) where P ij ∼ N (0, 1) are normally distributed real numbers and ϕ i ∼ U[−π, π) are uniform random phase shifts. The bandwidth parameter ν ∈ R n scales the inputs, and must be chosen with care. A rule of thumb suggested in [18] is to set it equal to the average pairwise distance between observed input vectors. An even better approach is to fit the bandwidth parameter through marginal maximum likelihood optimization.</p>
<p>Model-Based Reinforcement Learning</p>
<p>The distinction between model-free and model-based reinforcement learning is not precisely defined, but for the purposes of this paper, under model-based reinforcement learning we will understand trajectory optimization using a dynamics model obtained through system identification. In detail, having learned a forward model s ′ = f θ (s, a), the agent plans a trajectory τ = {s 0 , a 0 , s 1 , a 1 , . . . , a T −1 , s T } from a given starting state s 0 with the goal of minimizing a pre-defined trajectory cost J(τ ). Variations are possible in how exactly the model is learned, what model representation is used, whether the cost function is learned or given, etc.</p>
<p>Shooting Methods for Trajectory Optimization</p>
<p>Direct shooting methods are a class of optimal control algorithms aimed at solving planning problems with deterministic continuous dynamics models [19]. Depending on whether the dynamics are imposed as a constraint or included in the objective itself, one discerns between single shooting and multiple shooting methods. In single shooting, the dynamics are included in the objective function, minimize
a0:T −1 J(s 0 , a 0 , f (s 0 , a 0 ), a 1 , . . . , a T −1 , f (f (. . . f (s 0 , a 0 ), a T −2 ), a T −1 )),
and the optimization is performed only with respect to the actions a 0:T −1 . Note the iterated application of the dynamics function f . When performing gradient descent on this objective, the problem known as exploding/vanishing gradients hinders efficient first-order optimization. The reason for that is the ill conditioning of the problem: the actions in the beginning of the trajectory have a bigger impact on the final state than the actions at the end.</p>
<p>Multiple shooting methods aim to improve the problem conditioning by splitting the trajectory into smaller chunks, subsequently glueing them together by constraints. To enable that, the dynamics are imposed as a constraint, and the optimization is performed with respect to both actions and states,
minimize a0:T −1,s1:T J(s 0 , a 0 , s 1 , a 1 , . . . , a T −1 , s T ) subject to s t = f (s t−1 , a t−1 ), t = 1, . . . , T.
Multiple shooting methods converge faster and are numerically more stable than single shooting because they are better conditioned thanks to the breaking up of long-term dependencies into shorter chunks [20]. Moreover, state constraints can be straightforwardly incorporated in multiple shooting since states are optimization variables. The price for such benefits is a significant increase in the problem size and, as a consequence, in the memory requirements. This drawback, however, is offset by the fact that the problem becomes much sparser [20].</p>
<p>Receding Horizon Curiosity</p>
<p>Consider a dynamical system with state space S ⊂ R n and action space A ⊂ R k . Denote a probabilistic model of the dynamics that an agent maintains about this system by M. The probabilistic nature of the model enables the agent to reason about the information content in observations. The agent wants to find a sequence of actions a 0:T −1 which, when executed open-loop on the real system, provides the most informative sequence of observations s 0:T in the sense of being useful for learning the model. This setting is an active learning problem in which the sequence of actions plays the role of a query point. Therefore, we can utilize active learning approaches from Sec. 2.1 based on uncertainty sampling and expected variance reduction to solve the exploration problem.</p>
<p>Uncertainty Sampling</p>
<p>The uncertainty sampling acquisition function proposes to query the point that the model is most uncertain about. In our planning scenario, that means selecting the sequence of actions a 0:T −1 that results in a trajectory (a 0:T −1 , s 0:T ) that has the highest entropy. If p(s ′ | s, a) is the prediction given by the probabilistic model M trained on previously observed trajectories D, then the optimization objective can be stated as
maximize a0:T −1 t=1:T E st−1∼p(st−1 | at−2,...,a0) V p [s t | s t−1 , a t−1 ], with V p [s t | s t−1 , a t−1 ]
denoting the variance in the prediction of the next state, and the outer expectation being taken with respect to state marginal distributions. Note that alternative formulations are possible, e.g., where only the variance at the last time step is taken into account or where different time steps are weighted differently. Ideally, one would only consider the variance at the last time step; however, due to the fact that an approximate model is used, real trajectories rather quickly diverge from the planned ones, and therefore it is desirable to reach informative states as quickly as possible, which can be achieved by rewarding the agent for information gain at every time step.</p>
<p>To evaluate the uncertainty sampling objective, the state distribution needs to be propagated through the probabilistic model, which is a non-trivial problem in general. Instead, an approximate version of this problem can be solved, where only the mean of the state distribution is propagated,
maximize a0:T −1 t=1:T V p [s t |ŝ t−1 , a t−1 ] subject toŝ t = E p [s t |ŝ t−1 , a t−1 ], t = 1, . . . , T.(2)
In line with the general theory of active learning, the uncertainty sampling objective (2) is easier to optimize than the expected variance reduction objective described below. If the Bayesian linear regression model (Sec. 2.2) is used to represent the dynamics, the posterior parameter covariance matrix Σ * remains constant and does not depend on the states and actions. Thus, both the objective and constraints in (2) are differentiable and the problem can be solved using the multiple shooting method described in Sec. 2.5. The complete optimization procedure is summarized in Alg. 1. </p>
<p>Expected Variance Reduction</p>
<p>The uncertainty sampling heuristic rewards the agent for visiting uncertain states, but it ignores the fact that the model will become more certain once those states are visited. For example, if two states are initially equally uncertain, visiting one of them may yield a larger decrease in uncertainty because that state is more informative. Uncertainty sampling would be insensitive to this difference, whereas expected variance reduction allows for taking such information gain into account. In our setting, the expected variance reduction problem can be stated as Although superior to uncertainty sampling in information-theoretic terms, expected variance reduction is quite expensive to compute and optimize in practice [13], particularly due to probabilistic state propagation [21]. We adopt the so called maximum likelihood observations assumption [22], which amounts to propagating only the mean of the state distribution. The crudeness of this approximation is offset by the computational advantage: more frequent replanning, enabled by neglecting the expensive state uncertainty propagation, allows the agent to compensate for unforeseen deviations from the planned trajectory efficiently. The corresponding optimization problem reads </p>
<p>Both the objective and constraints in (3) are differentiable. Therefore, gradient-based optimization described in Sec. 2.5 can in principle be used to solve this problem. However, evaluation of the objective requires differentiation through matrix inversion, since matrix Σ * depends on the inverse of the kernel matrix (1). Combined with the chain-like structure of state-action dynamics, gradient computation becomes quite expensive for larger models (e. g. T &gt; 100, M &gt; 40).</p>
<p>Experimental Results</p>
<p>We compare our receding horizon curiosity algorithm (RHC, Sec. 3) to state-of-the-art model-based and model-free exploration approaches. On the model-based side, we consider MAX [5], which optimizes a certain approximation of the information gain via model ensemble disagreement. On the model-free side, we employ soft actor-critic (SAC) [23] with popular exploration bonuses: squared prediction error (SAC PE) and information gain in the form of parameter entropy difference between successive steps (SAC IG). Two acquisition functions for RHC are considered: uncertainty sampling (RHC US, Sec. 3.1) and expected variance reduction (RHC EVR, Sec. 3.2). Additionally, we report the performance of uniform random exploration (RAND) for comparison.  The results are shown in Fig. 1. In each environment, RHC was the only algorithm that reached the highest possible log-likelihood within 20 episodes, as indicated by the curves in the left column. When applicable, RHC EVR converged faster, as expected (Sec. 3.2). Both SAC PE and SAC IG performed on the level of random exploration (RAND), which can be traced back to their overcommitment behaviour [5]: in the beginning, virtually any action provides high reward because the model is uncertain, but afterwards the agent has to 'unlearn' it in order to go to more distant areas of the state-action space. MAX does not suffer from the over-commitment problem and therefore performed better than SAC and random exploration but nevertheless worse than our RHC method. Although none of the methods is real-time capable, it is worth pointing out that MAX took significantly longer than RHC because MAX solves an entire reinforcement learning problem in each episode. A table with run times is provided in the appendix. Figure 2 shows the trajectories executed by RHC US over iterations in the pendulum environment. The background indicates the entropy of the learned forward model's output when zero action is substituted. Warm colors correspond to high entropy (uncertainty). RHC tries to find a trajectory of maximum uncertainty consistent with the learned dynamics model. Driven by curiosity, the pendulum does a full swing-up already in Episode 6 to reach further areas of the state space.</p>
<p>Related Work</p>
<p>In psychology, curiosity [24] is considered to be a type of intrinsic motivation [25] that drives humans to explore. In reinforcement learning, various reward signals have been proposed to promote artificial curiosity. An early example is the prediction error [26], the idea being to reward the agent whenever there is a mismatch between predicted and observed next states. Unfortunately, such approach suffers from the "noisy TV problem": if the environment is stochastic, the agent gets attracted to the source of noise. A cure was proposed in [27], which consisted in rewarding the agent for prediction improvement instead of prediction error. However, despite its theoretical appeal, prediction improvement is hard to compute in practice, especially with general function approximators [28].</p>
<p>In statistics and control engineering, the problem of 'optimal' exploration is known as optimal input design [7,8] or optimal experiment design [9,10]. A popular measure of novelty in these fields is the information gain [29]. In computer science, the problem of 'optimal' exploration is addressed by Bayesian reinforcement learning [3]. The general dual control solution [30], however, can only be obtained in very special cases [11]. Therefore, in most applications, exploration bonuses are employed, which stem from the "optimism in the face of uncertainty" principle [31]. Bayesian exploration bonuses [32] and other types of visitation counts [33] have been shown to be effective in video games [34,35]. If the observation space is high-dimensional, exploration bonuses can be applied in the latent space. For example, latent-space prediction error and count-based exploration were combined in [36], while information gain was employed in [37]. Self-supervised prediction [28] and random network distillation [38] were proposed as different ways to compute the prediction error.</p>
<p>A comprehensive study of curiosity-driven exploration methods can be found in [4].</p>
<p>Exploration bonuses are commonly added on top of a primary RL objective function to promote faster learning. However, such approaches do not scale to the multi-task and transfer learning settings because the knowledge gained during exploration is not reused. In contrast, model-based approaches compress the knowledge into the model and can later reuse it in any downstream task.</p>
<p>Our method can be seen as lying at the intersection of optimal sequential experiment design and nonlinear system identification. In the former, info-gain-maximizing strategies are well understood but for linear models; we use these insights by treating our model as linear in the last-layer parameters. In the latter, the focus is placed on numerical approaches and structured models (e.g., grey-box models such as Hammerstein-Wiener model); we use receding horizon control for numerical optimization and basis function expansion for representing the dynamics as a black box.</p>
<p>We stress that trajectory optimization is essential for making the problem computationally tractable. Approaches such as [12] and [10] rely on approximately solving the Bellman equation, which scales exponentially with the time horizon. On the other hand, belief space trajectory optimization scales polynomially [39], allowing for much longer horizons (e.g., we used 150 ≤ T ≤ 200, whereas horizons of length T ≤ 4 were considered in [12]).</p>
<p>Conclusion</p>
<p>A principled algorithm for trajectory-based active exploration in the model-based reinforcement learning setting has been proposed (Sec. 3). Two acquisition functions from active learning have been adapted to guide episodic exploration (Sec. 2.1): uncertainty sampling (US, Sec. 3.1) and expected variance reduction (EVR, Sec. 3.2). Since the acquisition functions cannot be straightforwardly evaluated due to intractability of the belief propagation over time, an approximation has been proposed, which led to a novel algorithm, called receding horizon curiosity (RHC, Algorithm 1).</p>
<p>The proposed RHC approach was compared to state-of-the-art model-based and model-free exploration algorithms on classical continuous control problems. Empirical evaluations showed that RHC achieves higher model likelihood and collects higher reward on downstream tasks in fewer iterations.</p>
<p>Although not yet real-time capable, RHC was found to be computationally faster than MAX, thanks to being trajectory-based. The US objective (Sec. 4) delivered a better computation/performance trade-off, reaching the performance of EVR while being substantially easier to compute.</p>
<p>The experiments demonstrated that Bayesian curiosity w.r.t. last-layer parameters interleaved with nonlinear maximum likelihood feature learning can be successfully implemented and considerably improves exploration in low-dimensional classical control environments even under relatively strong deterministic state propagation assumption. Nevertheless, a number of obstacles need to be overcome to scale RHC to higher-dimensional problems. For instance, cheaper trajectory optimization methods (e.g., first-order, Hessian-free) could enable the use of larger number of features. Alternatively, dimensionality reduction techniques could allow for scaling the current approach by employing the same optimization framework but with lower-dimensional feature representations. Finally, tractable planning methods that can utilize more expressive probabilistic models, such as Bayesian neural networks, could allow for tackling even harder problems.</p>
<p>A Algorithms</p>
<p>Trajectory optimization for receding horizon curiosity (RHC) is implemented using CasADi [20], a control and auto-differentiation toolbox. The number of Fourier features in the learned dynamics model varies across environments: MountainCar 20, Pendulum 90, CartPole 80.</p>
<p>As a model-free baseline RL algorithm, soft actor-critic (SAC) is used [23], as implemented in Stable Baselines [40], with the following parameters across all environments γ = 0.99, τ = 0.005, learning rate = 0.0003, buffer size = 50000, batch size = 64.</p>
<p>The exploration bonus based on the squared prediction error is defined as follows where Xt denotes the set of observations until time step t, H is the entropy, and θ denotes the model parameters.</p>
<p>B Environments</p>
<p>MountainCar. The implementation from OpenAI Gym [41] is modified as follows to accommodate the episodic exploration setting. Car power is set to 10 −3 and the speed limit is removed. Upon reset, the car starts at the center of the valley with zero velocity. An episode ends when the car reaches the environment bounds or after 130 time steps. The evaluation task is to drive the car on top of the mountain as dictated by the stage cost c = 10(x − xgoal) 2 + 0.001a 2 , where x is the position of the car, xgoal is the goal location, and a the action.</p>
<p>Pendulum. The implementation from DeepMind Control Suite [42] with observations [cos θ, sin θ,θ] is used with the following modifications. The pendulum is initialized handging down with zero velocity. Each episode consists of 100 time steps of 80ms duration each. The evaluation task is to swing the pendulum up as dictated by the stage cost c = 100(1 − cos θ) 2 + 0.1 sin 2 θ + 0.1θ 2 + 0.001a 2 .</p>
<p>CartPole. The implementation from DeepMind Control Suite [42] with observations [x, cos θ, sin θ,ẋ,θ] is used. Each episode starts with the cart at the center and the pole hanging down, both having zero velocity. The system is simulated at 50Hz. An episode ends when the cart reaches the state limits or after 100 time steps. The evaluation task is to swing the pole up, c = 100x 2 + 100(1 − cos θ) 2 + 0.1 sin 2 θ + 0.1ẋ 2 + 0.1θ 2 + 0.1a 2 . Table 1 shows how long one run of each algorithm depicted in Fig. 1 on average takes. One run consists of 20 episodes (x-axis in Fig. 1). Evaluation of RHC EVR was only possible on the MountainCar environment due to its high memory demands. Evaluations were run on a machine with an Intel Xeon E5-2670 processor.  Table 1: Average wall-clock-time (in hours) for evaluated exploration algorithms (see Fig. 1).</p>
<p>C Runtimes</p>
<p>MountainCar</p>
<p>Data: number of episodes N , horizon T , initial model M 0 Result: optimized model M N for i ← 1 to N do find actions a 0:T −1 that optimize (2) or (3) given the current model M i ; execute a 0:T −1 in the environment and observe s 0:T ; update model M i+1 via (1) using M i as the prior and (a 0:T −1 , s 0:T ) as the new data; end Algorithm 1: Receding Horizon Curiosity. In each episode i, the most informative sequence of actions a 0:T −1 under the current model M i is computed; after that, observations s 0:T are collected and the model is updated M i → M i+1 .</p>
<p>[var(M | D ∪ (a 0:T −1 , s 0:T ))] . The operator var(M | D * ) here stands for a measure of variance of model M trained on dataset D * .We take it to be the entropy of the posterior distribution over the model parameters θ ∈ R m , which is known as the D-optimality criterion in Bayesian experimental design[14],var(M | D * ) = H(θ | D * ) = 1 2 ln det (Σ * ) + m 2 ln(2πe).Importantly, the covariance matrix Σ * depends on the trajectory (a 0:T −1 , s 0:T ) through the augmented dataset D * = D ∪ (a 0:T −1 , s 0:T ). The exact relationship is given in(1), and this relationship allows for optimization of the expected model variance with respect to the planned trajectory.</p>
<p>M | D ∪ (a 0:T −1 , s 0:T )) subject toŝ t = E p [s t |ŝ t−1 , a t−1 ], t = 1, . . . , T.</p>
<p>Figure 1 :
1Evaluation of exploration methods. Log-likelihood of random 10-step trajectories (evaluated by using the learned models) is shown on the left; the plots depict the median with the 1st and 9th deciles over 20 runs. The grey dashed lines indicate the log-likelihood obtained by a model trained on 10 4 uniform transition samples from the full state-action space and therefore approximates the best achievable log-likelihood for this model class. Our proposed approach RHC reaches the highest log-likelihood the fastest, followed by MAX, and subsequently the model-free algorithms with exploration bonuses. The plots on the right show the cumulative cost (negative reward) of solving each respective control task using the learned model. The trend is similar to the model log-likelihood: RHC reaches the lowest cost the fastest, then follows MAX, and after that follow the model-free exploration approaches.</p>
<p>Figure 2 :
2Exploration progress of the RHC algorithm with the uncertainty sampling objective in the pendulum environment. Episodes 2, 3, and 6 are shown from left to right. Note how the explored area of the state space is growing with iterations as the agent is trying to reach more distant states.Experiments aim to prove the feasibility and reveal advantages and disadvantages of simultaneous feature learning combined with approximate belief-space planning under deterministic state propagation assumption for guiding exploration. Classical control environments-mountain car, pendulum, and cartpole-are used for evaluation. In these environments, exploration using current reinforcement learning algorithms with sparse rewards and random noise is insufficient. The experiments were carried out as follows. For RHC, a sequence of actions was computed in each episode using multiple shooting (Sec. 2.5) and executed open-loop in the environment (Algorithm 1). As planning with the expected variance reduction objective is only tractable for small models (Sec. 3.2), RHC with this objective was only applied to the mountain car problem. For MAX and SAC, instead of open-loop actions, the respective 'curious' policies were applied in the environment. After each episode, the Bayesian linear regression model (Sec. 2.2) with random Fourier features (Sec. 2.3) was retrained using all observations from the past episodes. The quality of the learned model was evaluated on two metrics: (i) mean log-likelihood on a set of test points obtained by sampling random starting states and executing random actions, and (ii) mean return (negative reward accumulated over transitions) on downstream learning tasks, to highlight the quality of the learned model when used in a classical planning-for-control scenario. Further experimental details can be found in the appendix, and the implementation-in the accompanying software package.</p>
<p>rpe(st, at) = (st+1 − Ep[st+1 | st, at]) 2 ,where p denotes the model trained on the data from previous episodes. The exploration bonus based on the information gain is defined as the reduction of entropy rig(st, at) = H(θ | Xt) − H(θ | Xt+1),
https://github.com/mschulth/rhc 3rd Conference on Robot Learning (CoRL 2019), Osaka, Japan.
AcknowledgmentsCalculations for this research were conducted on the Lichtenberg high performance computer of the TU Darmstadt. This project has received funding from the European Union's Horizon 2020 research and innovation programme under grant agreement No. 640554 (SKILLS4ROBOTS).
Planning and acting in partially observable stochastic domains. L P Kaelbling, M L Littman, A R Cassandra, Artificial intelligence. 1011-2L. P. Kaelbling, M. L. Littman, and A. R. Cassandra. Planning and acting in partially observable stochastic domains. Artificial intelligence, 101(1-2):99-134, 1998.</p>
<p>The complexity of markov decision processes. C H Papadimitriou, J N Tsitsiklis, Mathematics of operations research. 123C. H. Papadimitriou and J. N. Tsitsiklis. The complexity of markov decision processes. Mathematics of operations research, 12(3):441-450, 1987.</p>
<p>Bayesian reinforcement learning: A survey. Foundations and Trends R in Machine Learning. M Ghavamzadeh, S Mannor, J Pineau, A Tamar, 8M. Ghavamzadeh, S. Mannor, J. Pineau, A. Tamar, et al. Bayesian reinforcement learning: A survey. Foundations and Trends R in Machine Learning, 8(5-6):359-483, 2015.</p>
<p>Large-scale study of curiositydriven learning. Y Burda, H Edwards, D Pathak, A Storkey, T Darrell, A A Efros, ICLR. Y. Burda, H. Edwards, D. Pathak, A. Storkey, T. Darrell, and A. A. Efros. Large-scale study of curiosity- driven learning. In ICLR, 2019.</p>
<p>Model-based active exploration. P Shyam, W Jaśkowski, F Gomez, arXiv:1810.12162arXiv preprintP. Shyam, W. Jaśkowski, and F. Gomez. Model-based active exploration. arXiv preprint arXiv:1810.12162, 2018.</p>
<p>Planning to be surprised: Optimal bayesian exploration in dynamic environments. Y Sun, F Gomez, J Schmidhuber, International Conference on Artificial General Intelligence. SpringerY. Sun, F. Gomez, and J. Schmidhuber. Planning to be surprised: Optimal bayesian exploration in dynamic environments. In International Conference on Artificial General Intelligence, pages 41-51. Springer, 2011.</p>
<p>Optimal input signals for parameter estimation in dynamic systems-survey and new results. R Mehra, IEEE Transactions on Automatic Control. 196R. Mehra. Optimal input signals for parameter estimation in dynamic systems-survey and new results. IEEE Transactions on Automatic Control, 19(6):753-768, 1974.</p>
<p>Input design: From open-loop to control-oriented design. M Gevers, X Bombois, IFAC Proceedings Volumes. 39M. Gevers and X. Bombois. Input design: From open-loop to control-oriented design. IFAC Proceedings Volumes, 39(1):1329-1334, 2006.</p>
<p>Optimal experiment design for dynamic system identification. M B Zarrop, Springer21M. B. Zarrop. Optimal experiment design for dynamic system identification, volume 21. Springer, 1979.</p>
<p>Sequential bayesian optimal experimental design via approximate dynamic programming. X Huan, Y M Marzouk, arXiv:1604.08320arXiv preprintX. Huan and Y. M. Marzouk. Sequential bayesian optimal experimental design via approximate dynamic programming. arXiv preprint arXiv:1604.08320, 2016.</p>
<p>Dual control for approximate bayesian reinforcement learning. E D Klenske, P Hennig, Journal of Machine Learning Research. 17127E. D. Klenske and P. Hennig. Dual control for approximate bayesian reinforcement learning. Journal of Machine Learning Research, 17(127):1-30, 2016.</p>
<p>Gaussian process planning with lipschitz continuous reward functions: Towards unifying bayesian optimization, active learning, and beyond. C K Ling, K H Low, P Jaillet, Thirtieth AAAI Conference on Artificial Intelligence. C. K. Ling, K. H. Low, and P. Jaillet. Gaussian process planning with lipschitz continuous reward func- tions: Towards unifying bayesian optimization, active learning, and beyond. In Thirtieth AAAI Conference on Artificial Intelligence, 2016.</p>
<p>Active learning literature survey. B Settles, University of Wisconsin-Madison Department of Computer SciencesTechnical reportB. Settles. Active learning literature survey. Technical report, University of Wisconsin-Madison Depart- ment of Computer Sciences, 2009.</p>
<p>Bayesian experimental design: A review. K Chaloner, I Verdinelli, Statistical Science. K. Chaloner and I. Verdinelli. Bayesian experimental design: A review. Statistical Science, pages 273- 304, 1995.</p>
<p>Active learning with statistical models. D A Cohn, Z Ghahramani, M I Jordan, Journal of artificial intelligence research. 4D. A. Cohn, Z. Ghahramani, and M. I. Jordan. Active learning with statistical models. Journal of artificial intelligence research, 4:129-145, 1996.</p>
<p>Pattern recognition and machine learning. C M Bishop, springerC. M. Bishop. Pattern recognition and machine learning. springer, 2006.</p>
<p>Random features for large-scale kernel machines. A Rahimi, B Recht, Advances in neural information processing systems. A. Rahimi and B. Recht. Random features for large-scale kernel machines. In Advances in neural infor- mation processing systems, pages 1177-1184, 2008.</p>
<p>Towards generalization and simplicity in continuous control. A Rajeswaran, K Lowrey, E V Todorov, S M Kakade, Advances in Neural Information Processing Systems. A. Rajeswaran, K. Lowrey, E. V. Todorov, and S. M. Kakade. Towards generalization and simplicity in continuous control. In Advances in Neural Information Processing Systems, pages 6550-6561, 2017.</p>
<p>Introduction to numerical analysis. J Stoer, R Bulirsch, Springer Science &amp; Business Media12J. Stoer and R. Bulirsch. Introduction to numerical analysis, volume 12. Springer Science &amp; Business Media, 2013.</p>
<p>Casadi: a software framework for nonlinear optimization and optimal control. J A Andersson, J Gillis, G Horn, J B Rawlings, M Diehl, Mathematical Programming Computation. J. A. Andersson, J. Gillis, G. Horn, J. B. Rawlings, and M. Diehl. Casadi: a software framework for nonlinear optimization and optimal control. Mathematical Programming Computation, pages 1-36, 2018.</p>
<p>Pilco: A model-based and data-efficient approach to policy search. M Deisenroth, C E Rasmussen, Proceedings of the 28th International Conference on machine learning (ICML-11). the 28th International Conference on machine learning (ICML-11)M. Deisenroth and C. E. Rasmussen. Pilco: A model-based and data-efficient approach to policy search. In Proceedings of the 28th International Conference on machine learning (ICML-11), pages 465-472, 2011.</p>
<p>Belief space planning assuming maximum likelihood observations. R PlattJr, Proceedings of the Robotics: Science and Systems Conference, 6th. the Robotics: Science and Systems Conference, 6thR. Platt Jr et al. Belief space planning assuming maximum likelihood observations. In Proceedings of the Robotics: Science and Systems Conference, 6th, 2010.</p>
<p>Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. T Haarnoja, A Zhou, P Abbeel, S Levine, arXiv:1801.01290arXiv preprintT. Haarnoja, A. Zhou, P. Abbeel, and S. Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. arXiv preprint arXiv:1801.01290, 2018.</p>
<p>Curiosity and motivation. The Oxford handbook of human motivation. P J Silvia, P. J. Silvia. Curiosity and motivation. The Oxford handbook of human motivation, pages 157-166, 2012.</p>
<p>Intrinsic and extrinsic motivations: Classic definitions and new directions. R M Ryan, E L Deci, Contemporary educational psychology. 251R. M. Ryan and E. L. Deci. Intrinsic and extrinsic motivations: Classic definitions and new directions. Contemporary educational psychology, 25(1):54-67, 2000.</p>
<p>A possibility for implementing curiosity and boredom in model-building neural controllers. J Schmidhuber, Proc. of the international conference on simulation of adaptive behavior: From animals to animats. of the international conference on simulation of adaptive behavior: From animals to animatsJ. Schmidhuber. A possibility for implementing curiosity and boredom in model-building neural con- trollers. In Proc. of the international conference on simulation of adaptive behavior: From animals to animats, pages 222-227, 1991.</p>
<p>Curious model-building control systems. J Schmidhuber, [Proceedings] 1991 IEEE International Joint Conference on Neural Networks. IEEEJ. Schmidhuber. Curious model-building control systems. In [Proceedings] 1991 IEEE International Joint Conference on Neural Networks, pages 1458-1463. IEEE, 1991.</p>
<p>Curiosity-driven exploration by self-supervised prediction. D Pathak, P Agrawal, A A Efros, T Darrell, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops. the IEEE Conference on Computer Vision and Pattern Recognition WorkshopsD. Pathak, P. Agrawal, A. A. Efros, and T. Darrell. Curiosity-driven exploration by self-supervised pre- diction. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 16-17, 2017.</p>
<p>On a measure of the information provided by an experiment. D V Lindley, The Annals of Mathematical Statistics. 274D. V. Lindley et al. On a measure of the information provided by an experiment. The Annals of Mathe- matical Statistics, 27(4):986-1005, 1956.</p>
<p>Dual control theory. i. Avtomatika i Telemekhanika. A Feldbaum, 21A. Feldbaum. Dual control theory. i. Avtomatika i Telemekhanika, 21(9):1240-1249, 1960.</p>
<p>Asymptotically efficient adaptive allocation rules. T L Lai, H Robbins, Advances in applied mathematics. 6T. L. Lai and H. Robbins. Asymptotically efficient adaptive allocation rules. Advances in applied mathe- matics, 6(1):4-22, 1985.</p>
<p>Near-bayesian exploration in polynomial time. J Z Kolter, A Y Ng, Proceedings of the 26th Annual International Conference on Machine Learning. the 26th Annual International Conference on Machine LearningACMJ. Z. Kolter and A. Y. Ng. Near-bayesian exploration in polynomial time. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 513-520. ACM, 2009.</p>
<p>Near-optimal reinforcement learning in polynomial time. M Kearns, S Singh, Machine learning. 492-3M. Kearns and S. Singh. Near-optimal reinforcement learning in polynomial time. Machine learning, 49 (2-3):209-232, 2002.</p>
<p>Unifying count-based exploration and intrinsic motivation. M Bellemare, S Srinivasan, G Ostrovski, T Schaul, D Saxton, R Munos, Advances in Neural Information Processing Systems. M. Bellemare, S. Srinivasan, G. Ostrovski, T. Schaul, D. Saxton, and R. Munos. Unifying count-based exploration and intrinsic motivation. In Advances in Neural Information Processing Systems, pages 1471- 1479, 2016.</p>
<p>Count-based exploration with neural density models. G Ostrovski, M G Bellemare, A Van Den Oord, R Munos, Proceedings of the 34th International Conference on Machine Learning. the 34th International Conference on Machine Learning70G. Ostrovski, M. G. Bellemare, A. van den Oord, and R. Munos. Count-based exploration with neural density models. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 2721-2730. JMLR. org, 2017.</p>
<p>Incentivizing exploration in reinforcement learning with deep predictive models. B C Stadie, S Levine, P Abbeel, arXiv:1507.00814arXiv preprintB. C. Stadie, S. Levine, and P. Abbeel. Incentivizing exploration in reinforcement learning with deep predictive models. arXiv preprint arXiv:1507.00814, 2015.</p>
<p>Vime: Variational information maximizing exploration. R Houthooft, X Chen, Y Duan, J Schulman, F De Turck, P Abbeel, Advances in Neural Information Processing Systems. R. Houthooft, X. Chen, Y. Duan, J. Schulman, F. De Turck, and P. Abbeel. Vime: Variational information maximizing exploration. In Advances in Neural Information Processing Systems, pages 1109-1117, 2016.</p>
<p>Y Burda, H Edwards, A Storkey, O Klimov, arXiv:1810.12894Exploration by random network distillation. arXiv preprintY. Burda, H. Edwards, A. Storkey, and O. Klimov. Exploration by random network distillation. arXiv preprint arXiv:1810.12894, 2018.</p>
<p>Scaling up gaussian belief space planning through covariance-free trajectory optimization and automatic differentiation. S Patil, G Kahn, M Laskey, J Schulman, K Goldberg, P Abbeel, Algorithmic foundations of robotics XI. SpringerS. Patil, G. Kahn, M. Laskey, J. Schulman, K. Goldberg, and P. Abbeel. Scaling up gaussian belief space planning through covariance-free trajectory optimization and automatic differentiation. In Algorithmic foundations of robotics XI, pages 515-533. Springer, 2015.</p>
<p>A Hill, A Raffin, M Ernestus, A Gleave, R Traore, P Dhariwal, C Hesse, O Klimov, A Nichol, M Plappert, A Radford, J Schulman, S Sidor, Y Wu, Stable baselines. A. Hill, A. Raffin, M. Ernestus, A. Gleave, R. Traore, P. Dhariwal, C. Hesse, O. Klimov, A. Nichol, M. Plappert, A. Radford, J. Schulman, S. Sidor, and Y. Wu. Stable baselines. https://github.com/hill-a/stable-baselines, 2018.</p>
<p>G Brockman, V Cheung, L Pettersson, J Schneider, J Schulman, J Tang, W Zaremba, Openai gym. G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba. Openai gym, 2016.</p>
<p>. Y Tassa, Y Doron, A Muldal, T Erez, Y Li, D D , . L Casas, D Budden, A Abdolmaleki, J Merel, A Lefrancq, arXiv:1801.00690Deepmind control suite. arXiv preprintY. Tassa, Y. Doron, A. Muldal, T. Erez, Y. Li, D. d. L. Casas, D. Budden, A. Abdolmaleki, J. Merel, A. Lefrancq, et al. Deepmind control suite. arXiv preprint arXiv:1801.00690, 2018.</p>            </div>
        </div>

    </div>
</body>
</html>
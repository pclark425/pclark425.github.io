<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5484 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5484</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5484</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-112.html">extraction-schema-112</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <p><strong>Paper ID:</strong> paper-2ef4be35f8424ea768aa2e1b44392b3eddbc780b</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/2ef4be35f8424ea768aa2e1b44392b3eddbc780b" target="_blank">Back to Square One: Artifact Detection, Training and Commonsense Disentanglement in the Winograd Schema</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> It is suggested that the apparent progress on WS may not necessarily reflect progress in commonsense reasoning, and the observed progress is mostly due to the use of supervision in training WS models, which is not likely to successfully support all the required Commonsense reasoning skills and knowledge.</p>
                <p><strong>Paper Abstract:</strong> The Winograd Schema (WS) has been proposed as a test for measuring commonsense capabilities of models. Recently, pre-trained language model-based approaches have boosted performance on some WS benchmarks but the source of improvement is still not clear. This paper suggests that the apparent progress on WS may not necessarily reflect progress in commonsense reasoning. To support this claim, we first show that the current evaluation method of WS is sub-optimal and propose a modification that uses twin sentences for evaluation. We also propose two new baselines that indicate the existence of artifacts in WS benchmarks. We then develop a method for evaluating WS-like sentences in a zero-shot setting to account for the commonsense reasoning abilities acquired during the pretraining and observe that popular language models perform randomly in this setting when using our more strict evaluation. We conclude that the observed progress is mostly due to the use of supervision in training WS models, which is not likely to successfully support all the required commonsense reasoning skills and knowledge.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5484.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5484.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RoBERTa-large (finetuned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RoBERTa-large fine-tuned on Winogrande</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>RoBERTa-large (pretrained masked LM) fine-tuned as a multiple-choice classifier on the Winogrande training set and evaluated on Winograd-style benchmarks (WSC, WSC-na, Winogrande dev) using both standard and grouped (twin-sentence) scoring and artifact-exposing baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RoBERTa-large (fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based masked language model (RoBERTa) pretrained on large text corpora and fine-tuned on Winogrande as a multiple-choice task (input format: [CLS] context [SEP] entity [SEP]).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>335M</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Winograd Schema Challenge (WSC) and Winogrande (Winograd-style pronoun/coreference commonsense test)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Commonsense reasoning / pronoun coreference</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Paired sentences (twin minimal pairs) where a pronoun refers to one of two candidate entities; the 'special word' differs between twins and flips the correct referent. Task is to pick which entity the pronoun/coref refers to.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Reported accuracies for RoBERTa-large fine-tuned on Winogrande (Table 1). Random baselines: single-instance 50%, group (paired) 25%. WSC (filtered pairs, n=272): original single 89.71%, group (paired) 79.41%; no-cands baseline single 60.72%, group 40.35%; part-sent baseline single 64.88%, group 33.88%. WSC-na: original single 89.45%, group 79.09%; no-cands 58.06%/34.41%; part-sent 59.90%/25.00%. Winogrande (dev, filtered pairs): original single 71.49%, group 58.45%; no-cands 53.07%/31.05%; part-sent 53.11%/22.34%.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Under the standard (single-instance) evaluation the fine-tuned RoBERTa-large achieves high accuracy (e.g., ~89–90% on WSC), but performance drops under group (paired) scoring (e.g., ~79% on WSC). The artifact-exposing baselines (no-cands, part-sent) yield accuracies substantially above random (e.g., no-cands group ~40% on WSC), indicating the model leverages dataset artifacts or spurious cues and/or memorization. The paper notes models trained on Winogrande approach previously reported human-like performance in other work, but this fine-tuned performance is argued to reflect supervised learning and possible artifacts rather than pure pretraining-acquired commonsense.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Group (paired) scoring substantially reduces measured performance, revealing brittleness to minimal changes; the model attains above-random scores on nonsensical baseline variants (no-cands, part-sent), implying reliance on statistical artifacts or surface cues. The dataset overlap / memorization risk is noted (some WSC examples appear on the web; RoBERTa's pretraining data likely overlaps). Human baseline numbers are not provided in this paper for direct quantitative comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Back to Square One: Artifact Detection, Training and Commonsense Disentanglement in the Winograd Schema', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5484.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5484.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BERT-base (zero-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BERT-base evaluated zero-shot on transformed Winograd items</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>BERT-base (pretrained masked LM) evaluated in a zero-shot setup after transforming Winograd items to mask the special word and replacing pronoun with the referred entity, then predicting which special word fills the mask.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT-base</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained masked language model (BERT-base). Evaluated without fine-tuning in a zero-shot masked-word setup on transformed Winograd/Winogrande items as described in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Winograd Schema Challenge (WSC) - zero-shot masked special-word variant</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Commonsense reasoning / pronoun coreference (zero-shot probe)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Transformation: replace pronoun with the correct entity and mask the 'special word' that differs between twins; model must choose which of the two special words is more probable to fill the mask. Multi-token special words and some examples are filtered out.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Table 3 zero-shot results: WSC single 56.52%, WSC group 15.22%; WSC-na single 54.79%, group 12.33%; Winogrande single 53.12%, group 11.11% (random baselines: single 50%, group 25%).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>BERT-base in zero-shot performs only slightly above random on single-instance accuracy but well below random on group-scoring (group << 25%), indicating inconsistent predictions across twin pairs and poor zero-shot commonsense in this formulation.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Evaluation required filtering multi-token special words; the transformed task differs from original WS (may be easier or at least different). Tokenization and masking considerations can bias probability comparisons if not carefully controlled. No human baseline numbers are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Back to Square One: Artifact Detection, Training and Commonsense Disentanglement in the Winograd Schema', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5484.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5484.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BERT-large (zero-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BERT-large evaluated zero-shot on transformed Winograd items</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>BERT-large (pretrained masked LM) evaluated zero-shot on the transformed Winograd/Winogrande masked special-word formulation described in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT-large</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained masked language model (BERT-large); evaluated zero-shot by masking the special word after replacing the pronoun with an entity and comparing candidate probabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>335M</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Winograd Schema Challenge (WSC) - zero-shot masked special-word variant</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Commonsense reasoning / pronoun coreference (zero-shot probe)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>See BERT-base entry.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Table 3 zero-shot results: WSC single 61.41%, group 23.91%; WSC-na single 60.27%, group 21.92%; Winogrande single 55.56%, group 12.50%.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>BERT-large shows better single-instance accuracy than base but group (paired) scores remain low (around or below random for some datasets), indicating fragility on twin-pair consistency and limited zero-shot commonsense.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Same limitations as other zero-shot masked evaluations: filtering reduces dataset size; transformed task may not fully capture original WS difficulty. No human baseline reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Back to Square One: Artifact Detection, Training and Commonsense Disentanglement in the Winograd Schema', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5484.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5484.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RoBERTa-base (zero-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RoBERTa-base evaluated zero-shot on transformed Winograd items</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>RoBERTa-base (pretrained masked LM) evaluated in the paper's zero-shot masked special-word protocol on WSC/WSC-na/Winogrande.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RoBERTa-base</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained RoBERTa-base masked language model assessed zero-shot on transformed Winograd items.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Winograd Schema Challenge (WSC) - zero-shot masked special-word variant</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Commonsense reasoning / pronoun coreference</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>See BERT-base entry.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Table 3 zero-shot results: WSC single 63.04%, group 27.17%; WSC-na single 60.27%, group 21.92%; Winogrande single 56.25%, group 14.58%.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>RoBERTa-base attains somewhat higher single-instance scores than BERT-base but group-scoring again shows poor consistency (group scores often near or below random), particularly on Winogrande.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Zero-shot evaluation yields variable results across datasets; Winogrande group performance is below random for many models, suggesting poor zero-shot commonsense capture by base models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Back to Square One: Artifact Detection, Training and Commonsense Disentanglement in the Winograd Schema', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5484.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5484.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RoBERTa-large (zero-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RoBERTa-large evaluated zero-shot on transformed Winograd items</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>RoBERTa-large tested zero-shot using the masked special-word transformation; one of the stronger zero-shot performers in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RoBERTa-large</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained RoBERTa-large masked language model evaluated without fine-tuning on transformed Winograd items using masked-word probability comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>335M</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Winograd Schema Challenge (WSC) - zero-shot masked special-word variant</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Commonsense reasoning / pronoun coreference</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>See BERT-base entry.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Table 3 zero-shot results: WSC single 73.91%, group 47.83%; WSC-na single 71.23%, group 42.47%; Winogrande single 54.86%, group 12.50%.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>RoBERTa-large is among the best zero-shot performers on WSC/WSC-na (notably higher single and group scores vs. other base models), but on Winogrande it performs at or below chance under group scoring. The authors interpret good WSC/WSC-na zero-shot results as potentially reflecting dataset artifacts or overlap with pretraining data rather than robust pretraining-acquired commonsense.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Strong zero-shot WSC performance may reflect memorization or artifact sensitivity; Winogrande (designed to reduce artifacts) yields poor or below-random group results, suggesting limits to pretraining-based commonsense. Tokenization/masking design and example filtering limit generality of zero-shot conclusions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Back to Square One: Artifact Detection, Training and Commonsense Disentanglement in the Winograd Schema', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5484.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5484.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ALBERT-base (zero-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ALBERT-base evaluated zero-shot on transformed Winograd items</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>ALBERT-base (pretrained masked LM) evaluated zero-shot on the transformed Winograd dataset; performs near random in group-scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ALBERT-base</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained ALBERT-base masked language model evaluated without fine-tuning on masked special-word Winograd items.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Winograd Schema Challenge (WSC) - zero-shot masked special-word variant</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Commonsense reasoning / pronoun coreference</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>See BERT-base entry.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Table 3 zero-shot results: WSC single 55.43%, group 13.04%; WSC-na single 55.48%, group 12.33%; Winogrande single 52.78%, group 7.64%.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>ALBERT-base performs at near-random single accuracy and very poorly on group-scoring, indicating negligible zero-shot commonsense for these items.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Very low group scores (well below random) indicate inconsistent answers across twin pairs; transformed evaluation reduces dataset size and may bias which phenomena are tested.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Back to Square One: Artifact Detection, Training and Commonsense Disentanglement in the Winograd Schema', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5484.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5484.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ALBERT-xxlarge (zero-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ALBERT-XXLarge-V2 evaluated zero-shot on transformed Winograd items</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Largest ALBERT variant evaluated zero-shot using the paper's masked special-word method; shows relatively strong zero-shot performance on WSC/WSC-na but still poor performance on Winogrande.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ALBERT-XXLarge-V2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained ALBERT-XXLarge-V2 masked language model (very large ALBERT variant) evaluated without fine-tuning on transformed Winograd items.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>223M</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Winograd Schema Challenge (WSC) - zero-shot masked special-word variant</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Commonsense reasoning / pronoun coreference</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>See BERT-base entry.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Table 3 zero-shot results: WSC single 78.80%, group 57.61%; WSC-na single 77.40%, group 54.79%; Winogrande single 58.68%, group 20.83%.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>ALBERT-xxlarge attains the highest zero-shot scores among evaluated models on WSC and WSC-na, including above-random group scores (e.g., 57.61% group on WSC). However, on Winogrande (designed to be less artifact-prone) group performance is below random, indicating that strong WSC results do not necessarily reflect robust pretraining-acquired commonsense.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>The discrepancy between strong WSC/WSC-na zero-shot performance and poor Winogrande performance suggests sensitivity to dataset artifacts and/or pretraining data overlap; transformed evaluation filters many examples and may not be fully faithful to original task.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Back to Square One: Artifact Detection, Training and Commonsense Disentanglement in the Winograd Schema', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5484.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e5484.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3 (mention)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 3 (GPT-3)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-3 is referenced in the paper as having produced impressive Winograd-style results in few-shot and zero/few-shot settings, but the authors caution about training-data overlap and do not report GPT-3 quantitative results themselves.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language models are few-shot learners</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large autoregressive language model (GPT-3) reported elsewhere to perform well in zero- and few-shot settings on multiple tasks including Winograd-style problems; the paper notes GPT-3 likely saw some WS examples in its training data.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Winograd Schema Challenge (WSC) (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Commonsense reasoning / pronoun coreference</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>As above: twin-sentence minimal pairs requiring commonsense to resolve pronoun reference.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>The paper mentions GPT-3's 'impressive' published results but warns these should be taken with caution because GPT-3's training corpus included some WSC questions (potential data leakage). The current paper does not provide quantitative GPT-3 numbers and does not use GPT-3 experimentally.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Back to Square One: Artifact Detection, Training and Commonsense Disentanglement in the Winograd Schema', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language models are few-shot learners <em>(Rating: 2)</em></li>
                <li>Winogrande: An adversarial winograd schema challenge at scale <em>(Rating: 2)</em></li>
                <li>Evaluating commonsense in pretrained language models <em>(Rating: 2)</em></li>
                <li>How reasonable are common-sense reasoning tasks: A case-study on the winograd schema challenge and swag <em>(Rating: 2)</em></li>
                <li>A surprisingly robust trick for the Winograd schema challenge <em>(Rating: 1)</em></li>
                <li>What BERT is not: Lessons from a new suite of psycholinguistic diagnostics for language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5484",
    "paper_id": "paper-2ef4be35f8424ea768aa2e1b44392b3eddbc780b",
    "extraction_schema_id": "extraction-schema-112",
    "extracted_data": [
        {
            "name_short": "RoBERTa-large (finetuned)",
            "name_full": "RoBERTa-large fine-tuned on Winogrande",
            "brief_description": "RoBERTa-large (pretrained masked LM) fine-tuned as a multiple-choice classifier on the Winogrande training set and evaluated on Winograd-style benchmarks (WSC, WSC-na, Winogrande dev) using both standard and grouped (twin-sentence) scoring and artifact-exposing baselines.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "RoBERTa-large (fine-tuned)",
            "model_description": "Transformer-based masked language model (RoBERTa) pretrained on large text corpora and fine-tuned on Winogrande as a multiple-choice task (input format: [CLS] context [SEP] entity [SEP]).",
            "model_size": "335M",
            "cognitive_test_name": "Winograd Schema Challenge (WSC) and Winogrande (Winograd-style pronoun/coreference commonsense test)",
            "cognitive_test_type": "Commonsense reasoning / pronoun coreference",
            "cognitive_test_description": "Paired sentences (twin minimal pairs) where a pronoun refers to one of two candidate entities; the 'special word' differs between twins and flips the correct referent. Task is to pick which entity the pronoun/coref refers to.",
            "llm_performance": "Reported accuracies for RoBERTa-large fine-tuned on Winogrande (Table 1). Random baselines: single-instance 50%, group (paired) 25%. WSC (filtered pairs, n=272): original single 89.71%, group (paired) 79.41%; no-cands baseline single 60.72%, group 40.35%; part-sent baseline single 64.88%, group 33.88%. WSC-na: original single 89.45%, group 79.09%; no-cands 58.06%/34.41%; part-sent 59.90%/25.00%. Winogrande (dev, filtered pairs): original single 71.49%, group 58.45%; no-cands 53.07%/31.05%; part-sent 53.11%/22.34%.",
            "human_baseline_performance": null,
            "performance_comparison": "Under the standard (single-instance) evaluation the fine-tuned RoBERTa-large achieves high accuracy (e.g., ~89–90% on WSC), but performance drops under group (paired) scoring (e.g., ~79% on WSC). The artifact-exposing baselines (no-cands, part-sent) yield accuracies substantially above random (e.g., no-cands group ~40% on WSC), indicating the model leverages dataset artifacts or spurious cues and/or memorization. The paper notes models trained on Winogrande approach previously reported human-like performance in other work, but this fine-tuned performance is argued to reflect supervised learning and possible artifacts rather than pure pretraining-acquired commonsense.",
            "notable_differences_or_limitations": "Group (paired) scoring substantially reduces measured performance, revealing brittleness to minimal changes; the model attains above-random scores on nonsensical baseline variants (no-cands, part-sent), implying reliance on statistical artifacts or surface cues. The dataset overlap / memorization risk is noted (some WSC examples appear on the web; RoBERTa's pretraining data likely overlaps). Human baseline numbers are not provided in this paper for direct quantitative comparison.",
            "uuid": "e5484.0",
            "source_info": {
                "paper_title": "Back to Square One: Artifact Detection, Training and Commonsense Disentanglement in the Winograd Schema",
                "publication_date_yy_mm": "2021-04"
            }
        },
        {
            "name_short": "BERT-base (zero-shot)",
            "name_full": "BERT-base evaluated zero-shot on transformed Winograd items",
            "brief_description": "BERT-base (pretrained masked LM) evaluated in a zero-shot setup after transforming Winograd items to mask the special word and replacing pronoun with the referred entity, then predicting which special word fills the mask.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "BERT-base",
            "model_description": "Pretrained masked language model (BERT-base). Evaluated without fine-tuning in a zero-shot masked-word setup on transformed Winograd/Winogrande items as described in the paper.",
            "model_size": null,
            "cognitive_test_name": "Winograd Schema Challenge (WSC) - zero-shot masked special-word variant",
            "cognitive_test_type": "Commonsense reasoning / pronoun coreference (zero-shot probe)",
            "cognitive_test_description": "Transformation: replace pronoun with the correct entity and mask the 'special word' that differs between twins; model must choose which of the two special words is more probable to fill the mask. Multi-token special words and some examples are filtered out.",
            "llm_performance": "Table 3 zero-shot results: WSC single 56.52%, WSC group 15.22%; WSC-na single 54.79%, group 12.33%; Winogrande single 53.12%, group 11.11% (random baselines: single 50%, group 25%).",
            "human_baseline_performance": null,
            "performance_comparison": "BERT-base in zero-shot performs only slightly above random on single-instance accuracy but well below random on group-scoring (group &lt;&lt; 25%), indicating inconsistent predictions across twin pairs and poor zero-shot commonsense in this formulation.",
            "notable_differences_or_limitations": "Evaluation required filtering multi-token special words; the transformed task differs from original WS (may be easier or at least different). Tokenization and masking considerations can bias probability comparisons if not carefully controlled. No human baseline numbers are provided.",
            "uuid": "e5484.1",
            "source_info": {
                "paper_title": "Back to Square One: Artifact Detection, Training and Commonsense Disentanglement in the Winograd Schema",
                "publication_date_yy_mm": "2021-04"
            }
        },
        {
            "name_short": "BERT-large (zero-shot)",
            "name_full": "BERT-large evaluated zero-shot on transformed Winograd items",
            "brief_description": "BERT-large (pretrained masked LM) evaluated zero-shot on the transformed Winograd/Winogrande masked special-word formulation described in the paper.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "BERT-large",
            "model_description": "Pretrained masked language model (BERT-large); evaluated zero-shot by masking the special word after replacing the pronoun with an entity and comparing candidate probabilities.",
            "model_size": "335M",
            "cognitive_test_name": "Winograd Schema Challenge (WSC) - zero-shot masked special-word variant",
            "cognitive_test_type": "Commonsense reasoning / pronoun coreference (zero-shot probe)",
            "cognitive_test_description": "See BERT-base entry.",
            "llm_performance": "Table 3 zero-shot results: WSC single 61.41%, group 23.91%; WSC-na single 60.27%, group 21.92%; Winogrande single 55.56%, group 12.50%.",
            "human_baseline_performance": null,
            "performance_comparison": "BERT-large shows better single-instance accuracy than base but group (paired) scores remain low (around or below random for some datasets), indicating fragility on twin-pair consistency and limited zero-shot commonsense.",
            "notable_differences_or_limitations": "Same limitations as other zero-shot masked evaluations: filtering reduces dataset size; transformed task may not fully capture original WS difficulty. No human baseline reported.",
            "uuid": "e5484.2",
            "source_info": {
                "paper_title": "Back to Square One: Artifact Detection, Training and Commonsense Disentanglement in the Winograd Schema",
                "publication_date_yy_mm": "2021-04"
            }
        },
        {
            "name_short": "RoBERTa-base (zero-shot)",
            "name_full": "RoBERTa-base evaluated zero-shot on transformed Winograd items",
            "brief_description": "RoBERTa-base (pretrained masked LM) evaluated in the paper's zero-shot masked special-word protocol on WSC/WSC-na/Winogrande.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "RoBERTa-base",
            "model_description": "Pretrained RoBERTa-base masked language model assessed zero-shot on transformed Winograd items.",
            "model_size": null,
            "cognitive_test_name": "Winograd Schema Challenge (WSC) - zero-shot masked special-word variant",
            "cognitive_test_type": "Commonsense reasoning / pronoun coreference",
            "cognitive_test_description": "See BERT-base entry.",
            "llm_performance": "Table 3 zero-shot results: WSC single 63.04%, group 27.17%; WSC-na single 60.27%, group 21.92%; Winogrande single 56.25%, group 14.58%.",
            "human_baseline_performance": null,
            "performance_comparison": "RoBERTa-base attains somewhat higher single-instance scores than BERT-base but group-scoring again shows poor consistency (group scores often near or below random), particularly on Winogrande.",
            "notable_differences_or_limitations": "Zero-shot evaluation yields variable results across datasets; Winogrande group performance is below random for many models, suggesting poor zero-shot commonsense capture by base models.",
            "uuid": "e5484.3",
            "source_info": {
                "paper_title": "Back to Square One: Artifact Detection, Training and Commonsense Disentanglement in the Winograd Schema",
                "publication_date_yy_mm": "2021-04"
            }
        },
        {
            "name_short": "RoBERTa-large (zero-shot)",
            "name_full": "RoBERTa-large evaluated zero-shot on transformed Winograd items",
            "brief_description": "RoBERTa-large tested zero-shot using the masked special-word transformation; one of the stronger zero-shot performers in this study.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "RoBERTa-large",
            "model_description": "Pretrained RoBERTa-large masked language model evaluated without fine-tuning on transformed Winograd items using masked-word probability comparisons.",
            "model_size": "335M",
            "cognitive_test_name": "Winograd Schema Challenge (WSC) - zero-shot masked special-word variant",
            "cognitive_test_type": "Commonsense reasoning / pronoun coreference",
            "cognitive_test_description": "See BERT-base entry.",
            "llm_performance": "Table 3 zero-shot results: WSC single 73.91%, group 47.83%; WSC-na single 71.23%, group 42.47%; Winogrande single 54.86%, group 12.50%.",
            "human_baseline_performance": null,
            "performance_comparison": "RoBERTa-large is among the best zero-shot performers on WSC/WSC-na (notably higher single and group scores vs. other base models), but on Winogrande it performs at or below chance under group scoring. The authors interpret good WSC/WSC-na zero-shot results as potentially reflecting dataset artifacts or overlap with pretraining data rather than robust pretraining-acquired commonsense.",
            "notable_differences_or_limitations": "Strong zero-shot WSC performance may reflect memorization or artifact sensitivity; Winogrande (designed to reduce artifacts) yields poor or below-random group results, suggesting limits to pretraining-based commonsense. Tokenization/masking design and example filtering limit generality of zero-shot conclusions.",
            "uuid": "e5484.4",
            "source_info": {
                "paper_title": "Back to Square One: Artifact Detection, Training and Commonsense Disentanglement in the Winograd Schema",
                "publication_date_yy_mm": "2021-04"
            }
        },
        {
            "name_short": "ALBERT-base (zero-shot)",
            "name_full": "ALBERT-base evaluated zero-shot on transformed Winograd items",
            "brief_description": "ALBERT-base (pretrained masked LM) evaluated zero-shot on the transformed Winograd dataset; performs near random in group-scoring.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ALBERT-base",
            "model_description": "Pretrained ALBERT-base masked language model evaluated without fine-tuning on masked special-word Winograd items.",
            "model_size": null,
            "cognitive_test_name": "Winograd Schema Challenge (WSC) - zero-shot masked special-word variant",
            "cognitive_test_type": "Commonsense reasoning / pronoun coreference",
            "cognitive_test_description": "See BERT-base entry.",
            "llm_performance": "Table 3 zero-shot results: WSC single 55.43%, group 13.04%; WSC-na single 55.48%, group 12.33%; Winogrande single 52.78%, group 7.64%.",
            "human_baseline_performance": null,
            "performance_comparison": "ALBERT-base performs at near-random single accuracy and very poorly on group-scoring, indicating negligible zero-shot commonsense for these items.",
            "notable_differences_or_limitations": "Very low group scores (well below random) indicate inconsistent answers across twin pairs; transformed evaluation reduces dataset size and may bias which phenomena are tested.",
            "uuid": "e5484.5",
            "source_info": {
                "paper_title": "Back to Square One: Artifact Detection, Training and Commonsense Disentanglement in the Winograd Schema",
                "publication_date_yy_mm": "2021-04"
            }
        },
        {
            "name_short": "ALBERT-xxlarge (zero-shot)",
            "name_full": "ALBERT-XXLarge-V2 evaluated zero-shot on transformed Winograd items",
            "brief_description": "Largest ALBERT variant evaluated zero-shot using the paper's masked special-word method; shows relatively strong zero-shot performance on WSC/WSC-na but still poor performance on Winogrande.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ALBERT-XXLarge-V2",
            "model_description": "Pretrained ALBERT-XXLarge-V2 masked language model (very large ALBERT variant) evaluated without fine-tuning on transformed Winograd items.",
            "model_size": "223M",
            "cognitive_test_name": "Winograd Schema Challenge (WSC) - zero-shot masked special-word variant",
            "cognitive_test_type": "Commonsense reasoning / pronoun coreference",
            "cognitive_test_description": "See BERT-base entry.",
            "llm_performance": "Table 3 zero-shot results: WSC single 78.80%, group 57.61%; WSC-na single 77.40%, group 54.79%; Winogrande single 58.68%, group 20.83%.",
            "human_baseline_performance": null,
            "performance_comparison": "ALBERT-xxlarge attains the highest zero-shot scores among evaluated models on WSC and WSC-na, including above-random group scores (e.g., 57.61% group on WSC). However, on Winogrande (designed to be less artifact-prone) group performance is below random, indicating that strong WSC results do not necessarily reflect robust pretraining-acquired commonsense.",
            "notable_differences_or_limitations": "The discrepancy between strong WSC/WSC-na zero-shot performance and poor Winogrande performance suggests sensitivity to dataset artifacts and/or pretraining data overlap; transformed evaluation filters many examples and may not be fully faithful to original task.",
            "uuid": "e5484.6",
            "source_info": {
                "paper_title": "Back to Square One: Artifact Detection, Training and Commonsense Disentanglement in the Winograd Schema",
                "publication_date_yy_mm": "2021-04"
            }
        },
        {
            "name_short": "GPT-3 (mention)",
            "name_full": "Generative Pre-trained Transformer 3 (GPT-3)",
            "brief_description": "GPT-3 is referenced in the paper as having produced impressive Winograd-style results in few-shot and zero/few-shot settings, but the authors caution about training-data overlap and do not report GPT-3 quantitative results themselves.",
            "citation_title": "Language models are few-shot learners",
            "mention_or_use": "mention",
            "model_name": "GPT-3",
            "model_description": "Large autoregressive language model (GPT-3) reported elsewhere to perform well in zero- and few-shot settings on multiple tasks including Winograd-style problems; the paper notes GPT-3 likely saw some WS examples in its training data.",
            "model_size": null,
            "cognitive_test_name": "Winograd Schema Challenge (WSC) (mentioned)",
            "cognitive_test_type": "Commonsense reasoning / pronoun coreference",
            "cognitive_test_description": "As above: twin-sentence minimal pairs requiring commonsense to resolve pronoun reference.",
            "llm_performance": null,
            "human_baseline_performance": null,
            "performance_comparison": null,
            "notable_differences_or_limitations": "The paper mentions GPT-3's 'impressive' published results but warns these should be taken with caution because GPT-3's training corpus included some WSC questions (potential data leakage). The current paper does not provide quantitative GPT-3 numbers and does not use GPT-3 experimentally.",
            "uuid": "e5484.7",
            "source_info": {
                "paper_title": "Back to Square One: Artifact Detection, Training and Commonsense Disentanglement in the Winograd Schema",
                "publication_date_yy_mm": "2021-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 2
        },
        {
            "paper_title": "Winogrande: An adversarial winograd schema challenge at scale",
            "rating": 2
        },
        {
            "paper_title": "Evaluating commonsense in pretrained language models",
            "rating": 2
        },
        {
            "paper_title": "How reasonable are common-sense reasoning tasks: A case-study on the winograd schema challenge and swag",
            "rating": 2
        },
        {
            "paper_title": "A surprisingly robust trick for the Winograd schema challenge",
            "rating": 1
        },
        {
            "paper_title": "What BERT is not: Lessons from a new suite of psycholinguistic diagnostics for language models",
            "rating": 1
        }
    ],
    "cost": 0.01845675,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Back to Square One: Artifact Detection, Training and Commonsense Disentanglement in the Winograd Schema</h1>
<p>Yanai Elazar ${ }^{1,2}$ Hongming Zhang ${ }^{3,4}$ Yoav Goldberg ${ }^{1,2}$ Dan Roth ${ }^{4}$<br>${ }^{1}$ Bar Ilan University, ${ }^{2}$ AI2, ${ }^{3}$ HKUST, ${ }^{4}$ UPenn<br>{yanaiela, yoav.goldberg}@gmail.com<br>hzhangal@cse.ust.hk, danroth@seas.upenn.edu</p>
<h4>Abstract</h4>
<p>The Winograd Schema (WS) has been proposed as a test for measuring commonsense capabilities of models. Recently, pre-trained language model-based approaches have boosted performance on some WS benchmarks but the source of improvement is still not clear. This paper suggests that the apparent progress on WS may not necessarily reflect progress in commonsense reasoning. To support this claim, we first show that the current evaluation method of WS is sub-optimal and propose a modification that uses twin sentences for evaluation. We also propose two new baselines that indicate the existence of artifacts in WS benchmarks. We then develop a method for evaluating WS-like sentences in a zero-shot setting to account for the commonsense reasoning abilities acquired during the pretraining and observe that popular language models perform randomly in this setting when using our more strict evaluation. We conclude that the observed progress is mostly due to the use of supervision in training WS models, which is not likely to successfully support all the required commonsense reasoning skills and knowledge. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>The Winograd Schema (WS) (Levesque et al., 2012) was proposed as an alternative to the Turing test, by virtue of evaluating progress on commonsense reasoning. The task is a multi-choice question akin to coreference resolution. Given a text snippet with two entities and a pronoun that refers to one of the entities, select the entity referred to by the pronoun. ${ }^{2}$ Consider the following example:</p>
<ol>
<li>The trophy doesn't fit into the brown suitcase because it is too large.
<sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">2</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></li>
</ol>
<p>Figure 1: Examples from the Winograd Schema Challenge (top), our proposed modification to these sentences that we use as novel baselines (middle) and the new formulation of the WS task which allows us to test LMs in a zero-shot setting (bottom).</p>
<p>The entities are marked in italics, the pronoun in bold, and the special word ${ }^{3}$ is underlined. In this case, it refers to The trophy, since smaller objects typically fit into larger objects. ${ }^{4}$</p>
<p>The success of Pretrained Language Models (PLMs) seems to have advanced models' commonsense capabilities by boosting the performance on WS via simple probability ranking (Trinh and Le, 2018; Brown et al., 2020; Zhou et al., 2020). Another advancement was the curation of a large, crowdsourced dataset for WS, Winogrande (Sakaguchi et al., 2019). Models that train on this dataset are close to human performance. But are we any closer to achieving commonsense reasoning?</p>
<p>We provide three explanations for the perceived progress on the WS task: (1) lax evaluation criteria; (2) artifacts in the datasets that remain despite efforts to remove them, and (3) knowledge and reasoning leakage from large training data. Combin-</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">1</a></sup></p>
<p>ing the effects of these attributes together, we show that all models we consider perform randomly on this task. Examples for WS, the proposed control baselines, and zero-shot instances can be found in Figure 1.</p>
<p>Our main premise in this work is that, from a commonsense perspective, the generalization capabilities models can get from large training data are limited. Due to the vast number of commonsense facts (e.g. steel is hard, planets are big), it is infeasible to learn them all from a limited-scale training set. However, this knowledge can still be acquired in different ways, such as self-supervision (Mitchell et al., 2015), Open IE (Tandon et al., 2014), collecting statistics from large text corpora (Elazar et al., 2019), PLMs (Zhou et al., 2020) and more (Bagherinezhad et al., 2016; Forbes and Choi, 2017). Therefore, we claim that the vast majority of commonsense knowledge a model obtains should come from sources external to the supervised dataset. The supervised training set should mainly provide a means for learning the format of the task but not as a source for commonsense knowledge acquisition. We thus question the approach, which has recently gained popularity (Sakaguchi et al., 2019; Klein and Nabi, 2020), of using models trained on large datasets for evaluating general commonsense reasoning capabilities, like WS.</p>
<p>Contributions. (i) We begin by proposing a general evaluation method that makes use of groups that contain similar inputs, e.g. the twin sentences in WS (§3). That is, instead of measuring accuracy by scoring each sentence separately, we suggest scoring according to the worse score on both inputs: giving a point only if both sentences are predicted correctly. This evaluation reduces the risk of successful prediction due to artifacts in the data and better reflects the models' commonsense reasoning abilities. (ii) Next, we extend previous work (Trichelair et al., 2019) that manually found in the Winograd Schema Challenge (WSC) associative examples which can be solved using simple statistics. We propose two automatically constructed control baselines that distort the sentences to be nonsensical, on which a score higher than majority suggests the presence of artifacts (§5). We find that WSC (Levesque et al., 2012) contains a non-trivial amount of artifacts, whereas the newly suggested dataset, WinoGrande (Sakaguchi et al., 2019), con-
tains much less of these. ${ }^{5}$
(iii) Finally, to bypass the supervised training step, we propose to directly evaluate PLMs on WS in a zero-shot setup; this allows for assessing how many commonsense reasoning capabilities were acquired in the pretraining step. Specifically, this evaluation disentangles the commonsense capabilities of PLMs from the knowledge they acquire from the training set. Combining our new evaluation method and taking into account the data artifacts with the zero-shot setting, we show that all models we consider perform randomly. We then demonstrate using learning curves of models trained on increasing amounts of data, that it takes huge amounts of training instances to make small improvements in the test set, demonstrating the ineffectiveness of large training sets in acquiring commonsense reasoning skills. We interpret these results as evidence that a lot of the commonsense reasoning capabilities are learned during fine-tuning, as opposed to the pre-training step.</p>
<p>Based on our experiments, we conclude that many of the claims of progress on WS in recent years are unjustified, and stem from sub-optimal evaluation, artifacts, and commonsense knowledge learned from a supervised training set. Nevertheless, we suggest that the newly proposed Winogrande dataset (Sakaguchi et al., 2019) shouldn't be used for training, but it provides good data for evaluation, and hope that our new evaluation methods will assist faithful tracking of commonsense reasoning progress.</p>
<h2>2 Background</h2>
<h3>2.1 WSC and the Twin Sentences</h3>
<p>The Winograd Schema Challenge (Levesque et al., 2012) was constructed to serve as a benchmark for commonsense reasoning capabilities of models (similarly to the way Textual Entailment was proposed to serve as a benchmark for measuring models' entailment capabilities (Dagan et al., 2005, 2013)). WSC contains a small test set of 273 examples, created by experts, and for several years models were struggling to perform well on it. Each question involves four key features: 1) two entities are mentioned in each sentence, and they can be two males, two females, two inanimate objects, or two groups of people or objects; 2) a pronoun or</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">2</a></sup></p>
<p>a possessive adjective is used in the example to refer to one of the entities; 3) the task is to determine which of the two entities is referred to by the pronoun, and 4) each sentence contains a special word which, when replaced, the answer changes. There are no other limitations on the sentences besides these constraints and, consequently, this test is considered to be a general commonsense reasoning test, unlike other benchmarks, which focus on specific commonsense capabilities (Rashkin et al., 2018; Forbes et al., 2019; Sap et al., 2019a,b; Bisk et al., 2020).</p>
<p>In order to fulfil the fourth feature, each example was paired with an additional twin sentence, which only slightly differs from its twin. (Similar test sets were recently proposed and are referred to as Counterfactual data (Kaushik et al., 2019) and Contrast sets (Gardner et al., 2020)). For example, the twin sentence of Example 1 is:
2. The trophy does not fit into the brown suitcase because it is too small.</p>
<p>Notice that the special words in these sentences are large and small, and in this sentence, it refers to the brown suitcase (as opposed to the trophy in Example 1). The special word is a key part of WS, which makes the task hard to solve. These words were chosen carefully to avoid statistical correlations between the special word and the entities. In this example, both trophy and suitcase can be small, which makes the task hard to solve by machines; and as Levesque et al. puts it: "This helps make the test Google-proof: having access to a large corpus of English text would likely not help much (assuming, that answers to the questions have not yet been posted on the Web, that is)!"</p>
<h3>2.2 Progress on WSC</h3>
<p>Since WSC was proposed as a benchmark for commonsense (Levesque et al., 2012), there were many attempts to improve performance on this benchmark, that involved different approaches including web queries (Rahman and Ng, 2012; Sharma et al., 2015; Emami et al., 2018), using external knowledge sources (Sharma, 2019), information extraction and reasoning (Isaak and Michael, 2016) and more (Peng et al., 2015; Liu et al., 2017a,b; Fähndrich et al., 2018; Klein and Nabi, 2019; Zhang et al., 2019, 2020a).</p>
<p>Newer approaches use LMs to assign a probability to a sentence by replacing the pronoun with
an entity, one at a time, and pick the more probable sentence (Trinh and Le, 2018; Opitz and Frank, 2018; Radford et al., 2019; Kocijan et al., 2019). More recently, sequence to sequence models have been employed to directly predict the referred entity in a supervised (Raffel et al., 2020), zero-shot or few-shot setting (Brown et al., 2020). The latest results of GPT-3 (Brown et al., 2020) are rather impressive, and agree with the premise of this paper, as the model sees none to a few dozen examples to learn the format. It is worth noting, though, that the training corpus of GPT-3 included some of the WSC questions, and therefore these results should be taken with a grain of salt. For a comprehensive review of the progress on approaches and related datasets of WS, see Kocijan et al. (2020).</p>
<p>Zhou et al. (2020) probed multiple LMs for commonsense capabilities in different datasets including WSC, by computing the probability the LM assigns each alternative and choosing the more probable one. The advantage of this method is its unsupervised approach; it does not teach the model any new knowledge. Notably, their evaluation protocol, which computes the average log probability of each masked word is problematic, since special words that get tokenized into more than one wordpiece are still masked independently, thus priming the model towards a certain answer (§6.1). In this work, we propose a new evaluation methodology and show that these models' performance is random. Finally, Zhang et al. (2020b) provided an analysis of different types of commonsense knowledge needed to solve the different WSC questions, including properties, eventualities, and quantities. They also created a new dataset, WinoWhy, which requires models to distinguish between plausible and erroneous reasons for the correct answer.</p>
<h2>3 A Robust Group Score Evaluation</h2>
<p>Many works in recent years have shown that large neural networks can achieve high performance on different benchmarks while "being right for the wrong reasons" (McCoy et al., 2019). These successes arise from a variety of reasons such as artifacts in datasets (Poliak et al., 2018; Tsuchiya, 2018; Gururangan et al., 2018; Kaushik and Lipton, 2018), annotators biases (Geva et al., 2019), etc. Levesque et al. (2012) proposed to alleviate some of these issues by using the twin sentences along with the special word. However, the proposed evaluation of WSC scores each twin separately. As</p>
<p>Trichelair et al. (2019) showed that some WSC instances can be solved using simple correlations, we argue that the independent scoring may result in unjustifiably inflated scores. Here, we inspect a new evaluation that accounts for some of these artifacts and provide a more robust evaluation for cases where we have grouped instances (e.g. minimal pairs).</p>
<h3>3.1 Group Scoring</h3>
<p>Recent studies proposed to augment test instances with minimal pairs, that either change the original answer (Kaushik et al., 2019; Gardner et al., 2020), or keep it intact by using paraphrasing, synonyms, etc. (Glockner et al., 2018; Shah et al., 2019). Typically, these works report the results separately on the new test set, with no reference to the original test set.</p>
<p>We extend over previous work that proposes to evaluate pairs (Abdou et al., 2020) or groups (Elazar et al., 2021) of related instances and assign a point only if they are all correctly predicted by a model. Our evaluation framework exploits groups of minimal-distance instances and results in a more robust evaluation. Specifically, for an arbitrary scoring function $f$, and a group of minimal-distance instances $x_{i}$, score each of the examples $x_{i_{j}}$ in the group and assign the group its worse-performing score: ${ }^{6}$</p>
<p>$$
\operatorname{groupScore}\left(x_{i}\right)=\min <em i__j="i_{j">{j} f\left(x</em>\right)
$$}</p>
<p>The motivation behind this new evaluation is three-fold: (1) Predicting correctly all examples in a group provides a more robust measurement, and indicates a better understanding of the instances; (2) The lowest scored example is the groups' "Achilles heel" and thus makes the success on other examples suspicious; (3) It lowers the probability of random predictions (especially in classification tasks), or the use of shallow heuristics to solve examples. We note that cases where all examples in a group can be solved based on some artifact will still lead to a high score on this group. Therefore this evaluation does not solve the problem of artifacts, but it reduces the chance of scoring them as correct in cases where not all the groups' instances contain artifacts. ${ }^{7}$</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">2</a></sup>In classification tasks, a consequence of this evaluation is the change in random performance. For example, in the case of balanced binary classification, the chance accuracy drops from $50 \%$ to $25 \%$.</p>
<p>This generic evaluation can be applied not only in classification tasks but also in other tasks that use different evaluation metrics such as BLEU and ROUGE in generation (Papineni et al., 2002; Lin, 2004). For WS, where the task involves a binary classification, we use group scoring over the twin sentences, with accuracy as the per-instance scoring function. This yields the paired evaluation that was recently proposed by Abdou et al. (2020) for evaluating WSC.</p>
<h3>3.2 Other Robust Evaluation Protocols</h3>
<p>It is important to note that any WS test set is only an approximation of the commonsense reasoning skills required overall. The twin-sentences allow to test for specific skills (such as the interchange between small and large with 'fit' in Examples 1, 2), but other perturbations are possible which allow testing different skills. For instance, Abdou et al. (2020) proposed several perturbations on the original sentences that mostly do not change the answer, such as synonymous entity substitution, tense switch, gender switch, etc. These perturbations are also reminiscent of the switched protocol of Trichelair et al. (2019), where models are evaluated on examples where the candidates can be switched in the order (which mainly happens with proper names, but also with inanimate objects), expecting a consistent prediction from models since the label does not depend on the entities' order. Under the group-scoring evaluation, we expect a model to succeed on all perturbations from the same group.</p>
<h2>4 Setup</h2>
<p>Datasets We experiment with two English WS datasets:</p>
<p>Winograd Schema Challenge (WSC) (Levesque et al., 2012) contains 273 manually curated examples. We also report results on the non-associative examples that were filtered by Trichelair et al. (2019), named WSC-na.</p>
<p>Winogrande (Sakaguchi et al., 2019) is a recent crowdsourced dataset that contains WS questions. Winogrande contains 40,938, 1,267, 1,767 examples for train, development, and test respectively. Since the test labels were not published, we report our results on the development set. We provide</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Setup</th>
<th>Single</th>
<th>Group</th>
</tr>
</thead>
<tbody>
<tr>
<td>WSC</td>
<td>original</td>
<td>89.71</td>
<td>79.41</td>
</tr>
<tr>
<td></td>
<td>no-cands</td>
<td>60.72</td>
<td>40.35</td>
</tr>
<tr>
<td></td>
<td>part-sent</td>
<td>64.88</td>
<td>33.88</td>
</tr>
<tr>
<td>WSC-na</td>
<td>original</td>
<td>89.45</td>
<td>79.09</td>
</tr>
<tr>
<td></td>
<td>no-cands</td>
<td>58.06</td>
<td>34.41</td>
</tr>
<tr>
<td></td>
<td>part-sent</td>
<td>59.90</td>
<td>25.00</td>
</tr>
<tr>
<td>Winogrande</td>
<td>original</td>
<td>71.49</td>
<td>58.45</td>
</tr>
<tr>
<td></td>
<td>no-cands</td>
<td>53.07</td>
<td>31.05</td>
</tr>
<tr>
<td></td>
<td>part-sent</td>
<td>53.11</td>
<td>22.34</td>
</tr>
</tbody>
</table>
<p>Table 1: Results of RoBERTa-large trained on Winogrande, evaluated on the different datasets in the regular condition (original) and the two bias-exposing baselines. Reporting results both on the original accuracy (Single), and the group-scoring (Group). Random performance on the single and group-scoring evaluations are $50 \%$ and $25 \%$ respectively.
a more detailed description of these datasets and splits in Appendix A.</p>
<p>Modeling We follow the modeling of Sakaguchi et al. (2019), which finetunes PLMs as a multiplechoice problem on Winogrande's training set. In this modeling, the pronoun is replaced with either one of the entities, and the '[CLS]' token representation is used for prediction. As such, the input format becomes: [CLS] context [SEP] entity [SEP], which is encoded once which each entity to produce a score. We also experiment with another loss that was explored in Liu et al. (2020) where instead of using a different classification head, uses the original MLM head for predictions. We report these results in Appendix G.</p>
<p>Pre-trained Models We experiment with three PLM types: BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019) and ALBERT (Lan et al., 2019). We provide implementation details in Appendix B.</p>
<h2>5 Artifacts-Detecting Baselines for WS</h2>
<p>WSC was carefully designed by human experts to minimize the presence of artifacts. For instance, Example 3 is not considered as a good WS example since a racecar is more likely to go fast rather than a school bus.
3. The racecar zoomed by the school bus because it was going so fast.</p>
<p>However, such correlations are often easy to miss. As evidence, Trichelair et al. (2019) found 37 sentences to be associative, or non Googleproof. ${ }^{8}$ These examples were labeled manually using crowdsourcing, therefore these are still bound to what non-experts can catch, and subtler cues may be hard to spot. Other correlations may be harder or impossible to detect by humans since they are the result of spurious correlations (Tu et al., 2020). These features, which can be learned during pretraining or fine-tuning, may result in successful predictions that do not reflect commonsense reasoning skills.</p>
<p>To account for these artifacts, we propose two control baselines, which are likely to achieve random performance with an artifacts-free model. A score above random indicates the presence of artifacts.</p>
<p>No-Candidate Baseline This baseline (nocands) removes the two candidates (entities) from the text. For instance, Example 1 will turn into: "would not fit into because it is too large."</p>
<p>Partial-Sentence Baseline In this baseline (partsent) we split the sentence into two parts, based on punctuation and discourse markers ${ }^{9}$ and take only the part containing the pronoun. For instance, Example 1 will be transformed into the following: "because it is too large." A similar approach was used by Trichelair et al. (2019), however, they employed annotators to manually indicate whether the partial sentence containing the pronoun is associative to one of the candidates. Alternatively, we use a trained model and inspect the overall score on a dataset.</p>
<p>We note that these two baselines create nonsensical sentences. Therefore, we expect humans to not be able to properly solve them. Thus, a model that achieves higher than random performance on these baselines over a large enough dataset is suspected to rely on spurious correlations.</p>
<p>These baselines are reminiscent of previous works that used part of the input (e.g. the hypothesis only baseline in NLI), to reveal artifacts in multiple datasets for NLI (Poliak et al., 2018) and reading comprehension (Kaushik and Lipton, 2018).</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">2</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">ID</th>
<th style="text-align: left;">WSC</th>
<th style="text-align: left;">Trichelair et al.</th>
<th style="text-align: left;">no-cands</th>
<th style="text-align: left;">part-sent</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">2</td>
<td style="text-align: left;">The trophy doesn't fit into the brown suitcase because it is too large.</td>
<td style="text-align: left;">$\boldsymbol{X}$</td>
<td style="text-align: left;">$\checkmark$</td>
<td style="text-align: left;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: left;">8</td>
<td style="text-align: left;">The lawyer asked the witness a question, but he was reluctant to repeat it.</td>
<td style="text-align: left;">$\boldsymbol{X}$</td>
<td style="text-align: left;">$\boldsymbol{X}$</td>
<td style="text-align: left;">$\boldsymbol{X}$</td>
</tr>
<tr>
<td style="text-align: left;">72</td>
<td style="text-align: left;">I couldn't put the pot on the shelf because it was too tall.</td>
<td style="text-align: left;">$\checkmark$</td>
<td style="text-align: left;">$\boldsymbol{X}$</td>
<td style="text-align: left;">$\boldsymbol{X}$</td>
</tr>
<tr>
<td style="text-align: left;">185</td>
<td style="text-align: left;">Sam broke both his ankles and he's walking with crutches. <br> But a month or so from now they should be unnecessary.</td>
<td style="text-align: left;">$\checkmark$</td>
<td style="text-align: left;">$\checkmark$</td>
<td style="text-align: left;">$\checkmark$</td>
</tr>
</tbody>
</table>
<p>Table 2: Instances from WSC, along with indication if the manual filtering by Trichelair et al. (2019) marked them as associative, and whether our proposed baselines predict them correctly using group scoring.</p>
<h3>5.1 Results</h3>
<p>We retrain the RoBERTa large model from Sakaguchi et al. (2019) that was trained on Winogrande and report the results using the original and the new group-based evaluations in Table 1. On WSC this model achieves $89.71 \%$ and $79.41 \%$ accuracy, on WSC-na it achieves $89.45 \%$ and $79.09 \%$, and on the dev set of Winogrande, it achieves $71.49 \%$ and $58.45 \%$ accuracy, respectively. To make these evaluations comparable, we filter sentences with no twin sentences from Winogrande and the single triplet sentence from WSC, remaining with 568 and 272 instances, respectively (or, 284 and 136 pairs). The resulting performance on the original Winogrande development set is $78.3 \% .^{10}$ The single accuracy score on sentences that have pairs is lower by almost 7 points than the original set, which suggests that the sentences with no pair are easier, and may contain some artifacts. Next, we highlight the performance difference between the original evaluation and the paired, which dropped by 10.30, 10.36, and 13.04 points for WSC, WSC-na, and Winogrande, respectively. Finally, the results on our proposed baselines achieve higher performance than the random baseline for WSC, and the no-cands baseline on Winogrande. The no-cands baseline achieves $40.35 \%, 34.41 \%$, and $31.05 \%$ on WSC, WSC-na, and Winogrande respectively, whereas the part-sent baseline achieves $33.88 \%$, $25.00 \%$, and $22.34 \%$ accuracy. These results indicate that WSC contains many artifacts (over 15 points above random performance), and even after the manual filtering of Trichelair et al. (2019) some statistical correlations remain. On Winogrande, the no-cands baseline achieves more than 6 points above random, indicating that it contains fewer artifacts than WSC and WSC-na, presumably due to the AFLite algorithm.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">2</a></sup></p>
<h3>5.2 Qualitative Analysis</h3>
<p>In Table 2 we inspect some instances from WSC and indicate if the manual filtering by Trichelair et al. (2019) found them to be associative, and whether our proposed baselines predicted them correctly using group scoring. Although successful predictions may result from chance (though the probability that both baselines correctly predicted both pairs is relatively low - $6.25 \%$ ), we highlight some cases we find interesting.</p>
<p>The first example from the table (ID 2) was predicted correctly by both our baselines, but not by Trichelair et al. (2019). This may be a case of memorization of this very popular example, by the pretrained RoBERTa model which was trained on many web pages (Emami et al., 2020). We provide some evidence for this example's memorization in Appendix F. Examples ID 8 and 72 were both predicted incorrectly by our baselines. While the latter was marked as associative by Trichelair et al. (2019), our baselines did not predict it correctly, perhaps for a good reason; since both a pot and a shelf can be tall, there's no clear association in this example. Example ID 185 was predicted correctly by our baselines, as well as by Trichelair et al. (2019) since this example is associative: the word 'unnecessary' is more likely to be correlated with crutches, rather than ankles.</p>
<h2>6 Disentanglement of Commonsense Reasoning and Learned Commonsense</h2>
<p>In this section, we wish to disentangle the commonsense reasoning skills acquired by PLMs during pretraining, and what they learn during fine-tuning on a WS dataset. We propose a method that allows evaluating pretrained Masked Language Models (MLM) in a zero-shot setting on WS-like questions.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">WSC</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">WSC-na</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">WinoGrande</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Single</td>
<td style="text-align: center;">Group</td>
<td style="text-align: center;">Single</td>
<td style="text-align: center;">Group</td>
<td style="text-align: center;">Single</td>
<td style="text-align: center;">Group</td>
</tr>
<tr>
<td style="text-align: left;">random</td>
<td style="text-align: center;">50.00</td>
<td style="text-align: center;">25.00</td>
<td style="text-align: center;">50.00</td>
<td style="text-align: center;">25.00</td>
<td style="text-align: center;">50.00</td>
<td style="text-align: center;">25.00</td>
</tr>
<tr>
<td style="text-align: left;">BERT-base</td>
<td style="text-align: center;">56.52</td>
<td style="text-align: center;">15.22</td>
<td style="text-align: center;">54.79</td>
<td style="text-align: center;">12.33</td>
<td style="text-align: center;">53.12</td>
<td style="text-align: center;">11.11</td>
</tr>
<tr>
<td style="text-align: left;">BERT-large</td>
<td style="text-align: center;">61.41</td>
<td style="text-align: center;">23.91</td>
<td style="text-align: center;">60.27</td>
<td style="text-align: center;">21.92</td>
<td style="text-align: center;">55.56</td>
<td style="text-align: center;">12.50</td>
</tr>
<tr>
<td style="text-align: left;">RoBERTa-base</td>
<td style="text-align: center;">63.04</td>
<td style="text-align: center;">27.17</td>
<td style="text-align: center;">60.27</td>
<td style="text-align: center;">21.92</td>
<td style="text-align: center;">56.25</td>
<td style="text-align: center;">14.58</td>
</tr>
<tr>
<td style="text-align: left;">RoBERTa-large</td>
<td style="text-align: center;">73.91</td>
<td style="text-align: center;">47.83</td>
<td style="text-align: center;">71.23</td>
<td style="text-align: center;">42.47</td>
<td style="text-align: center;">54.86</td>
<td style="text-align: center;">12.50</td>
</tr>
<tr>
<td style="text-align: left;">ALBERT-base</td>
<td style="text-align: center;">55.43</td>
<td style="text-align: center;">13.04</td>
<td style="text-align: center;">55.48</td>
<td style="text-align: center;">12.33</td>
<td style="text-align: center;">52.78</td>
<td style="text-align: center;">7.64</td>
</tr>
<tr>
<td style="text-align: left;">ALBERT-xxlarge</td>
<td style="text-align: center;">78.80</td>
<td style="text-align: center;">57.61</td>
<td style="text-align: center;">77.40</td>
<td style="text-align: center;">54.79</td>
<td style="text-align: center;">58.68</td>
<td style="text-align: center;">20.83</td>
</tr>
</tbody>
</table>
<p>Table 3: Performance of different PLMs evaluated in the zero-shot setup of WS. Single refers to the standard accuracy over the entire test set, Group refers to group-scoring.</p>
<h3>6.1 Zero Shot MLM Evaluation</h3>
<p>Previous work proposed to evaluate MLMs in a zero-shot setting by replacing the pronoun with masked tokens, corresponding to the number of tokens the entities are tokenized into. Then, by inspecting each entity's probability the more probable entity is selected (Kocijan et al., 2019; Abdou et al., 2020). However, this approach is problematic when the entities are of different token lengths or consist of more than a single token since the model may be primed towards a certain answer. For instance, consider Example 1's entities, trophy and suitcase, in the case they are tokenized into trophy and suit, case. In this scenario, the MLM will see a single mask in one case (and estimate the probability of trophy), but in the other case, it will see two masks (assigning the suit and case probabilities). Since the model has access to the number of tokens it has to complete, the comparison between these two options is flawed. Another approach, used by Zhou et al. (2020) is to calculate the probability of the entire sentence, by masking a single token at a time. However, this method is also problematic when the entities are tokenized into more than a single token since unmasked tokens are affecting the prediction of the masked tokens. For instance, following the same example as before, where suitcase is tokenized into suit and case, a model that sees suit is more likely to assign a high probability to case, therefore staining the probability distribution, and causing a wrong comparison.</p>
<p>Since properly evaluating MLM on WS sentences with more than a single word that differs between the sentences is challenging, we filter these examples. Then, we mask this word, and compare the probabilities of the two candidates, as was done in previous work (Goldberg, 2019; Talmor et al., 2020; Ettinger, 2020). The issue with this approach
is that typically, the candidates are tokenized into multiple word-pieces, which will result in filtering a great portion of the data. Instead, we propose to make use of the special word (the word that is different between the twin sentences), mask it, and replace the pronoun with the correct answer. Then, the model has to decide which of the special words refers to each entity. Occasionally, there is more than one special word, or it gets tokenized into multiple tokens, therefore we discard these sentences. An example of this transformation process on Example 1 is the following:
4. The trophy would not fit into the brown suitcase because the trophy is too [MASK].
where '[MASK]' is the token that has to be predicted between the two original special words: 'large' or 'small'. The twin sentence of this example would accordingly be the same but with the entity 'the trophy' replaced with 'the brown suitcase', and the correct answer would change from 'large' to 'small'.</p>
<p>One potential pitfall of this formulation is that it is not faithful to the original WS, and tests a different mechanism. To test the difference between these formulations, we train the RoBERTa large model on Winogrande on our transformed Winogrande data, and compare it to the results of the same model, trained on the original setup. We make sure to only use sentences that can be transformed, assuring to train both models on the same subset. The model's performance on the original setup achieves $66.10 \%$ and $55.93 \%$ on the original and paired evaluation development set, whereas the model trained on the transformed setup achieves $70.06 \%$ and $64.97 \%$. The latter achieves higher performance, suggesting that our transformation may be preferable in modeling, or easier than the</p>
<p>original setup. Since this modeling is easier for the model, the results provide a higher bound of the original results, making the following results even more alarming.</p>
<p>We transform WSC, WSC-na, and the Winogrande dev set with the proposed method and remain with 226, 180, and 354 examples, respectively. We then evaluate the pre-trained LMs described in Section 4, and report the results in Table 3. We note that the overall performance is much lower compared to the finetuned model, as expected. Next, the performance on the group-scoring on WSC-na is relatively low, except for RoBERTa-large and ALBERT-xxlarge, which achieve 42.47 and 54.79, high above random performance. On the other hand, the performance on Winogrande, across all models is below random performance (best result by ALBERT-xxlarge, of $20.83 \%$ ), indicating poor commonsense capabilities of these models. Since we found in the previous Section (§5) that WSC and WSC-na have many artifacts, we take the results on Winogrande to better reflect commonsense reasoning skills. Recall that the comparison between the two formulations suggested that our new formulation should perform better, a fact that makes the random predictions in the zero-shot setup even more remarkable.</p>
<h2>7 Progress in Commonsense Reasoning?</h2>
<p>The large performance gap may not seem surprising. In most tasks in NLP, we do not expect a PLM to do well on new tasks out of the box and expect a supervised dataset to provide the required skills. However, we claim that for commonsense tasks, this argument does not hold. Since commonsense reasoning skills and knowledge are huge, it is not likely to acquire all that information through supervision. Consider the following WS instances:
5. The large ball crashed right through the table because it was made of steel.
6. I bought a steel property at the same time as my wooden property. The _ property was harder.</p>
<p>Examples 5 and $6^{11}$ come from WSC and Winogrande training set, respectively. The fact that steel is a strong material is part of the knowledge needed to solve Example 5. However, a model that is trained on Example 6 may pick up this fact. Will</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">2</a></sup><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Learning curves for the large versions of BERT, RoBERTa, and ALBERT models, trained on increasing amounts of data. This figure differs from the Winogrande leaderboard. We explain the source of these differences in Appendix E.
this training instance also teach the model facts about other materials, such as styrofoam?</p>
<p>To quantify the effect of training on the success in solving WS questions, we re-split Winogrande training set to leave enough data for testing and use the rest for training. From the remaining training set, we create multiple training splits, increasing in size, to study the effect of increasing amounts of data on the overall performance. We use the original development set to pick the best models. We report learning curves with the different models, where each point is the average score of three runs, in Figure 2. ${ }^{12}$ We report the number of correct pairs predicted correctly on the y-axis as a function of the training size. These curves indicate that the inspected models obtained no commonsense reasoning capabilities in the pretraining step, and are slowly improving their performance the more data they are trained on. However, except for a sudden improvement with 500 examples for ALBERT, the slope increases incredibly slowly and requires a significant amount of additional training instances for small improvements (BERT and RoBERTa's slopes are more moderate). We conclude that training data is mostly non-beneficial for generalize commonsense reasoning, and models should acquire it using other methods.</p>
<p>We note that the initial fast increase in ALBERT's performance is interesting, and may be due to another explanation; that is commonsense reasoning is composed of commonsense knowledge (e.g. steel is hard), and reasoning (comparing</p>
<p><sup id="fnref2:1"><a class="footnote-ref" href="#fn:1">1</a></sup></p>
<p>between objects sizes). Some of the knowledge may be encoded in these models, and reasoning can be taught. However, if that's the case, datasets should account for that, with careful splits. We leave the answer to this question to future work. Overall, this increase is nevertheless rather moderate, and once a model passes this point (about 2000 examples), the performance increases slowly, which goes in line with our claims.</p>
<p>A potential explanation for the sudden performance improvement with finetuning, and the lower baselines scores on Winogrande, may arise from the unnaturalness aspect of this dataset. For instance, we find Example 5 from WSC a more natural sentence than Example 6 (from Winogrande). Thus, in the case of several less-natural occurring sentences in Winogrande, the random results of our baselines may be explained due to this fact, and the finetuning procedure may contribute to the model's adaptation of that language. We leave the assessment of this hypothesis to future work.</p>
<h2>8 Conclusions</h2>
<p>In this work, we begin by discussing the current evaluation of WS and propose an additional evaluation metric, group-scoring, that credits a model with the worse performing instance of a group. While we focus here on WS, we propose to use this evaluation in other tasks, where minimal pairs are available (Kaushik et al., 2019; Gardner et al., 2020; Warstadt et al., 2020), as a more reliable evaluation metric. We then propose two new control baselines that account for artifacts in WS data and show that WSC contains many artifacts, while Winogrande consists much less of them.</p>
<p>Finally, we propose a method to evaluate MLMs on WS sentences in a zero-shot setting. We show that the performance of popular MLMs is random and that models improve gradually the more training data they see. We conclude that the use of large training sets is not always desirable, especially in commonsense reasoning settings, and call future work to find other methods to improve our models' commonsense abilities.</p>
<h2>Acknowledgements</h2>
<p>We would like to thank Vered Shwartz, Keisuke Sakaguchi, Rotem Dror, Niket Tandon, Vid Kocijan and Ernest Davis for helpful discussions and comments on early versions of this paper. We also thank the anonymous reviewers for their valuable
suggestions.
Yanai Elazar is grateful to be supported by the PBC fellowship for outstanding PhD candidates in Data Science and the Google PhD fellowship. This project has received funding from the Europoean Research Council (ERC) under the Europoean Union's Horizon 2020 research and innovation programme, grant agreement No. 802774 (iEXTRACT) and from contract FA8750-19-2-1004 with the US Defense Advanced Research Projects Agency (DARPA).</p>
<h2>References</h2>
<p>Mostafa Abdou, Vinit Ravishankar, Maria Barrett, Yonatan Belinkov, Desmond Elliott, and Anders Søgaard. 2020. The sensitivity of language models and humans to Winograd schema perturbations. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 75907604, Online. Association for Computational Linguistics.</p>
<p>Hessam Bagherinezhad, Hannaneh Hajishirzi, Yejin Choi, and Ali Farhadi. 2016. Are elephants bigger than butterflies? reasoning about sizes of objects. In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, pages 3449-3456.</p>
<p>Daniel Bailey, Amelia J Harrison, Yuliya Lierler, Vladimir Lifschitz, and Julian Michael. 2015. The winograd schema challenge and reasoning about correlation. In AAAI Spring Symposia. Citeseer.</p>
<p>Yonatan Bisk, Rowan Zellers, Ronan LeBras, Jianfeng Gao, and Yejin Choi. 2020. Piqa: Reasoning about physical commonsense in natural language. In AAAI, pages 7432-7439.</p>
<p>Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. arXiv preprint arXiv:2005.14165.</p>
<p>Ido Dagan, Oren Glickman, and Bernardo Magnini. 2005. The pascal recognising textual entailment challenge. In Machine Learning Challenges Workshop, pages 177-190. Springer.</p>
<p>Ido Dagan, Dan Roth, Mark Sammons, and Fabio Massimo Zanzoto. 2013. Recognizing Textual Entailment: Models and Applications. Morgan and Claypool.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages $4171-4186$.</p>
<p>Yanai Elazar, Nora Kassner, Shauli Ravfogel, Abhilasha Ravichander, E. Hovy, Hinrich Schutze, and Y. Goldberg. 2021. Measuring and improving consistency in pretrained language models. ArXiv, abs/2102.01017.</p>
<p>Yanai Elazar, Abhijit Mahabal, Deepak Ramachandran, Tania Bedrax-Weiss, and Dan Roth. 2019. How large are lions? inducing distributions over quantitative attributes. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3973-3983, Florence, Italy. Association for Computational Linguistics.</p>
<p>Ali Emami, Noelia De La Cruz, Adam Trischler, Kaheer Suleman, and Jackie Chi Kit Cheung. 2018. A knowledge hunting framework for common sense reasoning. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1949-1958.</p>
<p>Ali Emami, Kaheer Suleman, Adam Trischler, and Jackie Chi Kit Cheung. 2020. An analysis of dataset overlap on winograd-style tasks. In Proceedings of the 28th International Conference on Computational Linguistics, pages 5855-5865.</p>
<p>Allyson Ettinger. 2020. What bert is not: Lessons from a new suite of psycholinguistic diagnostics for language models. Transactions of the Association for Computational Linguistics, 8:34-48.</p>
<p>Johannes Fähndrich, Sabine Weber, and Hannes Kanthak. 2018. A marker passing approach to winograd schemas. In Joint International Semantic Technology Conference, pages 165-181. Springer.</p>
<p>Maxwell Forbes and Yejin Choi. 2017. Verb physics: Relative physical knowledge of actions and objects. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 266-276.</p>
<p>Maxwell Forbes, Ari Holtzman, and Yejin Choi. 2019. Do neural language representations learn physical commonsense? Proceedings of the 41st Annual Conference of the Cognitive Science Society.</p>
<p>Matt Gardner, Yoav Artzi, Victoria Basmov, Jonathan Berant, Ben Bogin, Sihao Chen, Pradeep Dasigi, Dheeru Dua, Yanai Elazar, Ananth Gottumukkala, Nitish Gupta, Hannaneh Hajishirzi, Gabriel Ilharco, Daniel Khashabi, Kevin Lin, Jiangming Liu, Nelson F. Liu, Phoebe Mulcaire, Qiang Ning, Sameer Singh, Noah A. Smith, Sanjay Subramanian, Reut Tsarfaty, Eric Wallace, Ally Zhang, and Ben Zhou. 2020. Evaluating models' local decision boundaries via contrast sets. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1307-1323, Online. Association for Computational Linguistics.
M. Geva, Y. Goldberg, and J. Berant. 2019. Are we modeling the task or the annotator? an investigation ofannotator bias in natural language understanding datasets. In Empirical Methods in Natural Language Processing (EMNLP).</p>
<p>Max Glockner, Vered Shwartz, and Yoav Goldberg. 2018. Breaking NLI systems with sentences that require simple lexical inferences. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 650-655, Melbourne, Australia. Association for Computational Linguistics.</p>
<p>Yoav Goldberg. 2019. Assessing bert's syntactic abilities. arXiv preprint arXiv:1901.05287.</p>
<p>Suchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel Bowman, and Noah A Smith. 2018. Annotation artifacts in natural language inference data. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 107-112.</p>
<p>Nicos Isaak and Loizos Michael. 2016. Tackling the winograd schema challenge through machine logical inferences. In STAIRS, volume 284, pages 75-86.</p>
<p>Divyansh Kaushik, Eduard Hovy, and Zachary Lipton. 2019. Learning the difference that makes a difference with counterfactually-augmented data. In International Conference on Learning Representations.</p>
<p>Divyansh Kaushik and Zachary C Lipton. 2018. How much reading does reading comprehension require? a critical investigation of popular benchmarks. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages $5010-5015$.</p>
<p>Diederik P. Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.</p>
<p>Tassilo Klein and Moin Nabi. 2019. Attention is (not) all you need for commonsense reasoning. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 48314836, Florence, Italy. Association for Computational Linguistics.</p>
<p>Tassilo Klein and Moin Nabi. 2020. Contrastive selfsupervised learning for commonsense reasoning. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 75177523, Online. Association for Computational Linguistics.</p>
<p>Vid Kocijan, Ana-Maria Cretu, Oana-Maria Camburu, Yordan Yordanov, and Thomas Lukasiewicz. 2019. A surprisingly robust trick for the Winograd schema challenge. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4837-4842, Florence, Italy. Association for Computational Linguistics.</p>
<p>Vid Kocijan, Thomas Lukasiewicz, Ernest Davis, Gary Marcus, and Leora Morgenstern. 2020. A review of winograd schema challenge datasets and approaches. arXiv preprint arXiv:2004.13831.</p>
<p>Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2019. Albert: A lite bert for self-supervised learning of language representations. In International Conference on Learning Representations.</p>
<p>Hector Levesque, Ernest Davis, and Leora Morgenstern. 2012. The winograd schema challenge. In Thirteenth International Conference on the Principles of Knowledge Representation and Reasoning. Citeseer.</p>
<p>Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pages 74-81.</p>
<p>Haokun Liu, William Huang, Dhara Mungra, and Samuel Bowman. 2020. Precise task formalization matters in winograd schema evaluations. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 8275-8280.</p>
<p>Quan Liu, Hui Jiang, Andrew Evdokimov, Zhen-Hua Ling, Xiaodan Zhu, Si Wei, and Yu Hu. 2017a. Cause-effect knowledge acquisition and neural association model for solving a set of winograd schema problems. In IJCAI, pages 2344-2350.</p>
<p>Quan Liu, Hui Jiang, Zhen-Hua Ling, Xiaodan Zhu, Si Wei, and Yu Hu. 2017b. Combing context and commonsense knowledge through neural networks for solving winograd schema problems. In AAAI Spring Symposia.</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.</p>
<p>Tom McCoy, Ellie Pavlick, and Tal Linzen. 2019. Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3428-3448.</p>
<p>Julian Michael. 2015. The theory of correlation formulas and their application to discourse coherence. Bachelor's Thesis.
T. Mitchell, W. Cohen, E. Hruschka, P. Talukdar, J. Betteridge, A. Carlson, B. Dalvi, M. Gardner, B. Kisiel, J. Krishnamurthy, N. Lao, K. Mazaitis, T. Mohamed, N. Nakashole, E. Platanios, A. Ritter, M. Samadi, B. Settles, R. Wang, D. Wijaya, A. Gupta, X. Chen, A. Saparov, M. Greaves, and J. Welling. 2015. Never-ending learning. In Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence (AAAI-15).</p>
<p>Juri Opitz and Anette Frank. 2018. Addressing the winograd schema challenge as a sequence ranking task. In Proceedings of the First International Workshop on Language Cognition and Computational Models, pages 41-52.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311-318.</p>
<p>Haoruo Peng, Daniel Khashabi, and Dan Roth. 2015. Solving hard coreference problems. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 809-819.</p>
<p>Adam Poliak, Jason Naradowsky, Aparajita Haldar, Rachel Rudinger, and Benjamin Van Durme. 2018. Hypothesis only baselines in natural language inference. In Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics, pages 180-191.</p>
<p>Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1-67.</p>
<p>Altaf Rahman and Vincent Ng. 2012. Resolving complex cases of definite pronouns: the winograd schema challenge. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 777-789.</p>
<p>Hannah Rashkin, Maarten Sap, Emily Allaway, Noah A Smith, and Yejin Choi. 2018. Event2mind: Commonsense inference on events, intents, and reactions. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 463-473.</p>
<p>Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2019. Winogrande: An adversarial winograd schema challenge at scale. arXiv preprint arXiv:1907.10641.</p>
<p>Maarten Sap, Ronan Le Bras, Emily Allaway, Chandra Bhagavatula, Nicholas Lourie, Hannah Rashkin, Brendan Roof, Noah A Smith, and Yejin Choi. 2019a. Atomic: An atlas of machine commonsense for if-then reasoning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 3027-3035.</p>
<p>Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. 2019b. Social iqa: Commonsense reasoning about social interactions. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 44534463.</p>
<p>Meet Shah, Xinlei Chen, Marcus Rohrbach, and Devi Parikh. 2019. Cycle-consistency for robust visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6649-6658.</p>
<p>Arpit Sharma. 2019. Using answer set programming for commonsense reasoning in the winograd schema challenge. Theory and Practice of Logic Programming, 19(5-6):1021-1037.</p>
<p>Arpit Sharma, Nguyen Ha Vo, Somak Aditya, and Chitta Baral. 2015. Towards addressing the winograd schema challenge-building and using a semantic parser and a knowledge hunting module. In IJCAI, pages 1319-1325.</p>
<p>Alon Talmor, Yanai Elazar, Yoav Goldberg, and Jonathan Berant. 2020. olmpics-on what language model pre-training captures. Transactions of the Association for Computational Linguistics, 8:743-758.</p>
<p>Niket Tandon, Gerard De Melo, and Gerhard Weikum. 2014. Acquiring comparative commonsense knowledge from the web. In $A A A I$, pages 166-172.</p>
<p>Paul Trichelair, Ali Emami, Adam Trischler, Kaheer Suleman, and Jackie Chi Kit Cheung. 2019. How reasonable are common-sense reasoning tasks: A case-study on the winograd schema challenge and swag. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3373-3378.</p>
<p>Trieu H Trinh and Quoc V Le. 2018. A simple method for commonsense reasoning. arXiv preprint arXiv:1806.02847.</p>
<p>Masatoshi Tsuchiya. 2018. Performance impact caused by hidden bias of training data for recognizing textual entailment. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018).</p>
<p>Lifu Tu, Garima Lalwani, Spandana Gella, and He He. 2020. An empirical study on robustness to spurious correlations using pre-trained language models. Transactions of the Association for Computational Linguistics, 8:621-633.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in neural information processing systems, pages 5998-6008.</p>
<p>Alex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mohananey, Wei Peng, Sheng-Fu Wang, and Samuel R Bowman. 2020. Blimp: The benchmark of linguistic minimal pairs for english. Transactions of the Association for Computational Linguistics, 8:377-392.</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38-45, Online. Association for Computational Linguistics.</p>
<p>Hongming Zhang, Hantian Ding, and Yangqiu Song. 2019. Sp-10k: A large-scale evaluation set for selectional preference acquisition. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 722-731.</p>
<p>Hongming Zhang, Xin Liu, Haojie Pan, Yangqiu Song, and Cane Wing-Ki Leung. 2020a. ASER: A largescale eventuality knowledge graph. In WWW '20: The Web Conference 2020, Taipei, Taiwan, April 2024, 2020, pages 201-211.</p>
<p>Hongming Zhang, Xinran Zhao, and Yangqiu Song. 2020b. WinoWhy: A deep diagnosis of essential commonsense knowledge for answering Winograd schema challenge. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5736-5745, Online. Association for Computational Linguistics.</p>
<p>Ben Zhou, Daniel Khashabi, Qiang Ning, and Dan Roth. 2019. "going on a vacation" takes longer than "going for a walk": A study of temporal commonsense understanding. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3354-3360.</p>
<p>Xuhui Zhou, Yue Zhang, Leyang Cui, and Dandan Huang. 2020. Evaluating commonsense in pretrained language models. In AAAI, pages 97339740 .</p>
<p>Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In Proceedings of the IEEE international conference on computer vision, pages 1927.</p>
<h2>A Detailed Setup</h2>
<p>Datasets We report our results on two datasets:
Winograd Schema Challenge (WSC) (Levesque et al., 2012) contains 273 manually curated examples. Each example is paired with a twin-sentence, meaning that there's a special word that is changed between the two sentences, that changes the coreferring entity. Trichelair et al. (2019) have labeled the original WSC examples, and found 37 examples to be associative Trichelair et al. (2019). We thus also use the non-associative subset which excludes the associative examples. We refer to this subset as WSC-na</p>
<p>Winogrande (Sakaguchi et al., 2019) is a recent crowdsourced dataset that contains WS questions. Winogrande is much larger than WSC and contains $9,248,1,267,1,767$ examples for train, development, and test respectively. Winogrande was filtered from 'biases' (or artifacts) using their proposed AFLite algorithm, which produced the mentioned challenging dataset. However, the authors also release and use the 'biased' instances for training, making a total of 40,938 training instances.</p>
<p>Pre-trained Models We experiments with multiple pre-trained models: BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019) and ALBERT (Lan et al., 2019). These models are large Transformerbased architectures (Vaswani et al., 2017), that are trained on the Masked Language Modeling task, which is predicting the masked word in a given context. These models are pretrained on huge amounts of text such as Wikipedia, the book corpus (Zhu et al., 2015), parts of CommonCrawl, and more. Specifically, we conduct our experiments with BERT-large-cased, RoBERT-Large, and ALBERT-XXLarge-V2, which have 335M, 335M, and 223 M parameters, respectively.</p>
<h2>B Implementation Details</h2>
<p>We implemented the experiments with the huggingface package (Wolf et al., 2020). Following the previous work (Sakaguchi et al., 2019), on all our experiments, we set the learning rate to be $1 \mathrm{e}-5$, batch size to be 8 , and trained the models for 8 epochs. Adam (Kingma and Ba, 2015) is used as the optimizer. We optimize all models with the cross-entropy loss function. We trained our model with RTX 2080, and the training time is 13,14 , and 62 minutes per epoch on the largest training set Winogrande (10) for BERT-large, RoBERTa-large,
and Albert-XXL-v2, respectively. As the evaluation is conducted on the dev set, we do not use it to select the best model. Instead, we report the performance with the final model, which is converged based on our observation.</p>
<h2>C Full Learning Curves Results</h2>
<p>The full results from Figure 2, along with the standard deviations, are reported in Table 4.</p>
<h2>D AfLite Details</h2>
<p>AFLite (Sakaguchi et al., 2019), an algorithm proposed for reducing datasets' artifacts was used to create Winogrande (Sakaguchi et al., 2019). It works as follows: a RoBERTa model (Liu et al., 2019) is finetuned on a random subset of the data to train a 'weak' model of the task. Then, the rest of the instances are encoded using the model's encoder. Then, for multiple iterations, a set of weak classifiers (linear) are trained on a subset of the encoded data and predict the rest. If more than k classifier predicted correctly an instance's label, it is discarded from the final dataset. This process is repeated multiple times until reaching a satisfying dataset size (which is controlled by predefined hyperparameters).</p>
<p>Although this algorithm filter examples that are 'easy', as a set of linear models that were trained on a medium quality representation managed to predict the correct answer, it is unclear how artifactfree the dataset is. In contrast, our proposed baseline methods directly detect artifacts the classification model may rely on, by presenting challenging perturbations on which a model is not likely to succeed above random. Thus, our procedure is inherently different than the general-purpose AFLite filtering algorithm.</p>
<h2>E Comparison to Winogrande Leaderboard</h2>
<p>We note that Figure 2 differs from the Winogrande leaderboard in multiple ways: first, we compare different models than the ones that appear on the leaderboard. Specifically, the to-date leading submission (accurate as of March 21st, 2021), UNICORN, does not provide details about the model, except it is a T5 based model, trained on a collection of datasets. Since the content of these datasets is not publicly available, it is impossible to assess the quality of this submission. For instance, if one</p>
<table>
<thead>
<tr>
<th style="text-align: center;"># Training</th>
<th style="text-align: center;">BERT</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">RoBERTa</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">ALBERT</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Single</td>
<td style="text-align: center;">Group</td>
<td style="text-align: center;">Single</td>
<td style="text-align: center;">Group</td>
<td style="text-align: center;">Single</td>
<td style="text-align: center;">Group</td>
</tr>
<tr>
<td style="text-align: center;">0</td>
<td style="text-align: center;">52.99 (0.00)</td>
<td style="text-align: center;">8.67 (0.00)</td>
<td style="text-align: center;">56.39 (0.00)</td>
<td style="text-align: center;">16.61 (0.00)</td>
<td style="text-align: center;">55.55 (0.00)</td>
<td style="text-align: center;">17.23 (0.00)</td>
</tr>
<tr>
<td style="text-align: center;">100</td>
<td style="text-align: center;">53.47 (0.75)</td>
<td style="text-align: center;">11.71 (0.75)</td>
<td style="text-align: center;">52.78 (0.75)</td>
<td style="text-align: center;">10.22 (4.48)</td>
<td style="text-align: center;">58.24 (1.49)</td>
<td style="text-align: center;">19.89 (2.74)</td>
</tr>
<tr>
<td style="text-align: center;">500</td>
<td style="text-align: center;">49.31 (1.87)</td>
<td style="text-align: center;">12.42 (1.74)</td>
<td style="text-align: center;">49.65 (0.50)</td>
<td style="text-align: center;">14.17 (1.99)</td>
<td style="text-align: center;">60.07 (0.50)</td>
<td style="text-align: center;">32.35 (1.27)</td>
</tr>
<tr>
<td style="text-align: center;">1,000</td>
<td style="text-align: center;">51.39 (0.99)</td>
<td style="text-align: center;">15.33 (0.75)</td>
<td style="text-align: center;">50.35 (0.37)</td>
<td style="text-align: center;">16.33 (0.50)</td>
<td style="text-align: center;">62.50 (0.62)</td>
<td style="text-align: center;">42.89 (0.25)</td>
</tr>
<tr>
<td style="text-align: center;">2,000</td>
<td style="text-align: center;">51.39 (0.87)</td>
<td style="text-align: center;">22.32 (3.49)</td>
<td style="text-align: center;">49.65 (0.25)</td>
<td style="text-align: center;">16.35 (1.49)</td>
<td style="text-align: center;">62.85 (2.36)</td>
<td style="text-align: center;">53.27 (3.24)</td>
</tr>
<tr>
<td style="text-align: center;">4,000</td>
<td style="text-align: center;">48.96 (2.61)</td>
<td style="text-align: center;">21.73 (2.49)</td>
<td style="text-align: center;">49.65 (0.50)</td>
<td style="text-align: center;">18.94 (1.24)</td>
<td style="text-align: center;">67.36 (1.12)</td>
<td style="text-align: center;">55.72 (2.49)</td>
</tr>
<tr>
<td style="text-align: center;">6,000</td>
<td style="text-align: center;">50.35 (0.50)</td>
<td style="text-align: center;">23.73 (0.50)</td>
<td style="text-align: center;">59.72 (1.86)</td>
<td style="text-align: center;">38.85 (2.99)</td>
<td style="text-align: center;">67.71 (2.86)</td>
<td style="text-align: center;">52.16 (3.73)</td>
</tr>
<tr>
<td style="text-align: center;">8,000</td>
<td style="text-align: center;">48.26 (1.24)</td>
<td style="text-align: center;">29.27 (0.75)</td>
<td style="text-align: center;">50.35 (1.37)</td>
<td style="text-align: center;">39.32 (1.99)</td>
<td style="text-align: center;">67.36 (0.50)</td>
<td style="text-align: center;">55.43 (0.25)</td>
</tr>
<tr>
<td style="text-align: center;">10,000</td>
<td style="text-align: center;">51.39 (1.12)</td>
<td style="text-align: center;">31.85 (1.99)</td>
<td style="text-align: center;">62.85 (0.12)</td>
<td style="text-align: center;">52.27 (0.99)</td>
<td style="text-align: center;">73.76 (1.76)</td>
<td style="text-align: center;">59.98 (1.94)</td>
</tr>
<tr>
<td style="text-align: center;">12,000</td>
<td style="text-align: center;">50.00 (1.62)</td>
<td style="text-align: center;">25.68 (1.49)</td>
<td style="text-align: center;">62.85 (0.50)</td>
<td style="text-align: center;">51.24 (0.50)</td>
<td style="text-align: center;">72.22 (1.33)</td>
<td style="text-align: center;">57.28 (0.54)</td>
</tr>
<tr>
<td style="text-align: center;">14,000</td>
<td style="text-align: center;">52.08 (0.50)</td>
<td style="text-align: center;">32.31 (3.24)</td>
<td style="text-align: center;">62.15 (1.49)</td>
<td style="text-align: center;">52.31 (0.75)</td>
<td style="text-align: center;">75.61 (0.63)</td>
<td style="text-align: center;">62.15 (2.24)</td>
</tr>
<tr>
<td style="text-align: center;">16,000</td>
<td style="text-align: center;">54.86 (0.75)</td>
<td style="text-align: center;">39.31 (1.99)</td>
<td style="text-align: center;">60.42 (2.11)</td>
<td style="text-align: center;">53.14 (3.24)</td>
<td style="text-align: center;">76.82 (1.15)</td>
<td style="text-align: center;">64.21 (1.42)</td>
</tr>
</tbody>
</table>
<p>Table 4: Effect of the training data size on different models performance. We report results on BERT, RoBERTa and ALBERT, all with their largest variants.
of these datasets contains other commonsense reasoning datasets, the model may have picked up on commonsense reasoning skills which are also tested for in Winogrande. Second, the leaderboard uses the original evaluation, based on the accuracy of single instances. As we claim in Section 3, this evaluation is sub-optimal and causes an overestimation of the actual performance of models. Moreover, our analyses were done on the development set, as opposed to the reported test set performance, since the test set is not publicly available. Finally, the leaderboard presents a learning curve of 5 training sizes, as we report the results over 12 different training sizes.</p>
<h2>F Elaborate Analysis</h2>
<p>In Section 5.2 we showcase some examples from WSC and provide possible explanations for which our baselines (§5) are able to solve them. Here, we provide additional evidence that supports our claim. We do so for the example where both baselines predict the correct answer, but the manual inspection from Trichelair et al. (2019) does not consider it to be associative. We emphasize that this example is not associative per se, and thus the annotation from Trichelair et al. (2019) was correct, but the pretrained model, which was trained on the web, may have caught up statistical cues that help it predict these examples correctly, even with partial information. For completeness, we repeat the example here:
7. The trophy doesn't fit into the brown suitcase
because it is too large.
Example 7 is a popular example that is often given when describing the task in the media. As evidence, we search for this sentence in Google and found it in multiple websites:</p>
<ul>
<li>https://theness.com/neurologic ablog/index.php/a-tougher-tu ring-test/</li>
<li>https://www.eitdigital.eu/ne wsroom/blog/article/whats-too-big-the-trophy-or-the-suitca se/</li>
<li>https://cmte.ieee.org/futuredi rections/2014/08/20/whats-too-big-the-trophy-or-the-suitca se/</li>
</ul>
<p>Next, we search for these websites in Common Crawl ${ }^{13}$, the February 2019 version that was reported to be part of RoBERTa's training data (Liu et al., 2019). We use an index server ${ }^{14}$ that allows querying a specific index and look specific websites. We find that the first two websites are included in this index. Although we cannot guarantee that these websites were part of RoBERTa's training data since it was not published, the probability that several examples from WSC were part of the large training data of RoBERTa (and later models), with these websites, or other, is high.</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">2</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;"># Training</th>
<th style="text-align: center;">BERT</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">RoBERTa</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">ALBERT</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Single</td>
<td style="text-align: center;">Group</td>
<td style="text-align: center;">Single</td>
<td style="text-align: center;">Group</td>
<td style="text-align: center;">Single</td>
<td style="text-align: center;">Group</td>
</tr>
<tr>
<td style="text-align: center;">0</td>
<td style="text-align: center;">$52.99(0.00)$</td>
<td style="text-align: center;">$8.67(0.00)$</td>
<td style="text-align: center;">$56.39(0.00)$</td>
<td style="text-align: center;">$16.61(0.00)$</td>
<td style="text-align: center;">$55.55(0.00)$</td>
<td style="text-align: center;">$17.23(0.00)$</td>
</tr>
<tr>
<td style="text-align: center;">100</td>
<td style="text-align: center;">$54.39(1.59)$</td>
<td style="text-align: center;">$12.28(2.39)$</td>
<td style="text-align: center;">$55.46(0.18)$</td>
<td style="text-align: center;">$17.61(1.46)$</td>
<td style="text-align: center;">$56.14(1.12)$</td>
<td style="text-align: center;">$17.54(3.73)$</td>
</tr>
<tr>
<td style="text-align: center;">500</td>
<td style="text-align: center;">$51.32(0.37)$</td>
<td style="text-align: center;">$10.53(2.14)$</td>
<td style="text-align: center;">$55.63(1.67)$</td>
<td style="text-align: center;">$25.00(3.27)$</td>
<td style="text-align: center;">$61.97(1.37)$</td>
<td style="text-align: center;">$34.15(1.74)$</td>
</tr>
<tr>
<td style="text-align: center;">1,000</td>
<td style="text-align: center;">$51.75(0.63)$</td>
<td style="text-align: center;">$12.28(0.89)$</td>
<td style="text-align: center;">$58.27(2.03)$</td>
<td style="text-align: center;">$35.56(3.27)$</td>
<td style="text-align: center;">$62.85(0.12)$</td>
<td style="text-align: center;">$34.86(0.25)$</td>
</tr>
<tr>
<td style="text-align: center;">2,000</td>
<td style="text-align: center;">$54.93(0.37)$</td>
<td style="text-align: center;">$14.44(1.33)$</td>
<td style="text-align: center;">$57.92(1.09)$</td>
<td style="text-align: center;">$35.21(3.16)$</td>
<td style="text-align: center;">$61.44(0.75)$</td>
<td style="text-align: center;">$34.51(0.50)$</td>
</tr>
<tr>
<td style="text-align: center;">4,000</td>
<td style="text-align: center;">$52.46(0.71)$</td>
<td style="text-align: center;">$16.55(2.00)$</td>
<td style="text-align: center;">$61.09(1.84)$</td>
<td style="text-align: center;">$37.32(2.67)$</td>
<td style="text-align: center;">$64.08(2.49)$</td>
<td style="text-align: center;">$40.49(2.32)$</td>
</tr>
<tr>
<td style="text-align: center;">6,000</td>
<td style="text-align: center;">$53.87(1.57)$</td>
<td style="text-align: center;">$20.07(1.53)$</td>
<td style="text-align: center;">$59.15(0.62)$</td>
<td style="text-align: center;">$37.32(1.51)$</td>
<td style="text-align: center;">$68.66(1.12)$</td>
<td style="text-align: center;">$49.65(0.75)$</td>
</tr>
<tr>
<td style="text-align: center;">8,000</td>
<td style="text-align: center;">$53.69(1.17)$</td>
<td style="text-align: center;">$22.89(1.08)$</td>
<td style="text-align: center;">$62.15(0.98)$</td>
<td style="text-align: center;">$39.44(1.27)$</td>
<td style="text-align: center;">$68.13(2.74)$</td>
<td style="text-align: center;">$50.70(3.73)$</td>
</tr>
<tr>
<td style="text-align: center;">10,000</td>
<td style="text-align: center;">$53.87(0.51)$</td>
<td style="text-align: center;">$25.00(1.61)$</td>
<td style="text-align: center;">$63.56(1.24)$</td>
<td style="text-align: center;">$45.77(2.21)$</td>
<td style="text-align: center;">$70.42(1.49)$</td>
<td style="text-align: center;">$53.17(0.50)$</td>
</tr>
<tr>
<td style="text-align: center;">12,000</td>
<td style="text-align: center;">$50.17(1.50)$</td>
<td style="text-align: center;">$23.94(2.46)$</td>
<td style="text-align: center;">$64.26(1.07)$</td>
<td style="text-align: center;">$45.42(3.27)$</td>
<td style="text-align: center;">$69.54(0.51)$</td>
<td style="text-align: center;">$52.46(0.50)$</td>
</tr>
<tr>
<td style="text-align: center;">14,000</td>
<td style="text-align: center;">$52.82(2.00)$</td>
<td style="text-align: center;">$27.11(3.86)$</td>
<td style="text-align: center;">$63.38(1.25)$</td>
<td style="text-align: center;">$44.72(2.99)$</td>
<td style="text-align: center;">$67.61(0.97)$</td>
<td style="text-align: center;">$53.17(1.81)$</td>
</tr>
<tr>
<td style="text-align: center;">16,000</td>
<td style="text-align: center;">$53.69(0.67)$</td>
<td style="text-align: center;">$27.11(0.89)$</td>
<td style="text-align: center;">$61.09(0.57)$</td>
<td style="text-align: center;">$41.67(1.77)$</td>
<td style="text-align: center;">$70.77(0.75)$</td>
<td style="text-align: center;">$55.28(1.23)$</td>
</tr>
</tbody>
</table>
<p>Table 5: Effect of the training data size on different models performance. We report results on BERT, RoBERTa and ALBERT, all with their largest variants.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: left;">Setup</th>
<th style="text-align: left;">Single</th>
<th style="text-align: left;">Group</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">original</td>
<td style="text-align: left;">89.71</td>
<td style="text-align: left;">80.88</td>
</tr>
<tr>
<td style="text-align: left;">WSC</td>
<td style="text-align: left;">no-cands</td>
<td style="text-align: left;">60.96</td>
<td style="text-align: left;">29.82</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">part-sent</td>
<td style="text-align: left;">59.09</td>
<td style="text-align: left;">22.31</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">original</td>
<td style="text-align: left;">90.00</td>
<td style="text-align: left;">81.82</td>
</tr>
<tr>
<td style="text-align: left;">WSC-na</td>
<td style="text-align: left;">no-cands</td>
<td style="text-align: left;">59.14</td>
<td style="text-align: left;">25.81</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">part-sent</td>
<td style="text-align: left;">56.77</td>
<td style="text-align: left;">16.67</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">original</td>
<td style="text-align: left;">70.95</td>
<td style="text-align: left;">54.23</td>
</tr>
<tr>
<td style="text-align: left;">Winogrande</td>
<td style="text-align: left;">no-cands</td>
<td style="text-align: left;">54.87</td>
<td style="text-align: left;">17.69</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">part-sent</td>
<td style="text-align: left;">54.43</td>
<td style="text-align: left;">14.18</td>
</tr>
</tbody>
</table>
<p>Table 6: Results of RoBERTa-large trained on Winogrande, evaluated on the different datasets in the regular condition (original) and the two bias-exposing baselines using the MC-MLM loss (Liu et al., 2020). Reporting results both on the original accuracy (Single), and the group-scoring (Group). Random performance on the single and group-scoring evaluations are $50 \%$ and $25 \%$ respectively.</p>
<h2>G MLM results</h2>
<p>Here we report the results for the MC-MLM loss that was explored in Liu et al. (2020), where instead of training a new head for the classification task, it uses the original MLM head and scores the different candidates instead of the pronoun. We run all experiments including fine-tuning, and report the results in this section.</p>
<p>The artifacts experiment results are detailed in Table 6. Although the results on the standard set-
ting (original) are similar to the ones when using a dedicated head (Table 1), this model appears to rely less on artifacts: the no-cands baseline still perform better than random on WSC, but the other baseline and the other evaluations perform randomly.</p>
<p>Finally, we repeat the learning curves experiment using the MC-MLM loss, on increasing amounts of data, where for each training size we train 3 models and report the mean and std, and report the results in Table 5. Here, in contrast to the trends shown in Liu et al. (2020), we observe generally worse results using the MC-MLM loss. One source of difference is that Liu et al. (2020) repeated the experiments many more times while performing a grid search over different hyperparameters, while we used the same default hyperparameters for all experiments. Another source of difference is the different training and evaluation splits used in our studies. We conclude that nevertheless, the trends remain the same, and the slopes of both methods are slow to increase, and thus strengthens our claims about the limited usefulness of training data for WS.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:1">
<p>${ }^{12}$ Full numeric results, along with standard deviations are reported in Appendix C.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:0">
<p>${ }^{13}$ https://commoncrawl.org/
${ }^{14}$ http://index.commoncrawl.org/CC-MAIN-&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
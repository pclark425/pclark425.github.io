<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1924 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1924</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1924</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-39.html">extraction-schema-39</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <p><strong>Paper ID:</strong> paper-282102924</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2510.13375v1.pdf" target="_blank">DepthVLA: Enhancing Vision-Language-Action Models with Depth-Aware Spatial Reasoning</a></p>
                <p><strong>Paper Abstract:</strong> Vision-Language-Action (VLA) models have recently shown impressive generalization and language-guided manipulation capabilities. However, their performance degrades on tasks requiring precise spatial reasoning due to limited spatial reasoning inherited from Vision-Language Models (VLMs). Existing VLAs rely on extensive action-data pretraining to ground VLMs in 3D space, which reduces training efficiency and is still insufficient for accurate spatial understanding. In this work, we present DepthVLA, a simple yet effective VLA architecture that explicitly incorporates spatial awareness through a pretrained depth prediction module. DepthVLA adopts a mixture-of-transformers design that unifies a VLM, a depth transformer, and an action expert with fully shared attentions, forming an end-to-end model with enhanced spatial reasoning. Extensive evaluations in both real-world and simulated environments show that DepthVLA outperforms state-of-the-art approaches, achieving 78.5% vs. 65.0% progress in real-world tasks, 94.9% vs. 93.6% in the LIBERO simulator, and 74.8% vs. 58.8% in the Simpler simulator. Our code will be made publicly available.</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1924.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1924.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DepthVLA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Depth-aware Vision-Language-Action model (DepthVLA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A mixture-of-transformers VLA that injects a pretrained monocular depth prediction expert alongside a vision-language model and a flow-matching action expert, enabling explicit geometric reasoning for precise manipulation and collision avoidance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DepthVLA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Mixture-of-transformers integrating (1) a VLM (Paligemma-3B) for language and semantics, (2) a depth expert (DINOv2-L encoder initialized from Depth Anything V2 + transformer decoder) producing intermediate depth features and predicted depth maps, and (3) a flow-matching continuous action expert; modalities: monocular RGB, language, proprioception, and internally predicted depth; block-wise attention masking lets action tokens attend to both VLM and depth streams while preserving pretrained module competence.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Mixed: vision-language pretraining for VLM; vision-only monocular depth pretraining for depth expert (3D datasets); (optional) action-data pretraining for some experiments but main claims rely on depth expert + VLM.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Depth expert pretrained on large-scale 3D/monocular depth datasets (WildRGB-D, Scannet, Scannet++, HyperSim) using metric/scale-invariant log loss; VLM (Paligemma-3B) assumed pretrained on large vision-language corpora (not detailed here); VLA training / robot pretraining datasets used in experiments include Galaxea Open-World (100k trajectories, 150 task categories), BridgeData V2 (60k trajectories), and LIBERO (task demonstrations). Pretraining data contain scene geometry (depth labels / synthetic scenes) and robot action trajectories where applicable; explicit descriptions of text/action token statistics not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Robotic manipulation (vision-language-conditioned continuous control)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Evaluated on both real-world bimanual tasks (Galaxea R1 Lite dual-arm platform: table bussing, microwave operation, block stacking) and simulated benchmarks (LIBERO with Franka Panda; Simpler WidowX). Action space: continuous control predicted as 1-second action chunks (16 steps @15 Hz) via flow-matching; environments include cluttered scenes, small-object grasping, door and plate manipulation, block stacking, and diverse simulated object/scene configurations.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>The paper states semantic grounding comes from the pretrained VLM (large-scale web image-text pretraining) while geometric priors come from depth expert pretrained on 3D datasets; it does not quantify exact overlap of objects/actions between VLM/depth pretraining and target tasks but argues VLM provides open-vocabulary semantic grounding complementary to depth-based geometry.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Real-world progress (after fine-tuning on 100 trajectories): DepthVLA average progress ≈ 78.5% (reported 79% in main text), few-shot (20 trajectories) progress 63%; Simpler WidowX zero-shot average success 74.8% (per Table I); LIBERO joint-trained average success 94.9% (per Table II). Metrics: success rates (%) for simulated tasks and averaged progress scores (%) for real-world benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Closest baseline re-implemented (π0, which lacks the depth expert in this comparison) - Real-world progress after fine-tuning: 65.0% (baseline); few-shot (20 trajectories) 45%; Simpler average success 58.8% (Table I); LIBERO average success 93.6% (Table II). Note: π0 still uses the same VLM backbone in the authors' reimplementation; 'without language pretraining' here is used to indicate the baseline missing DepthVLA's explicit depth expert rather than a model with no VLM at all.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Reported few-shot finetuning: with 20 trajectories (few-shot), DepthVLA achieves 63% average progress vs baseline 45% (absolute +18 percentage points), indicating improved sample efficiency in low-data transfer; standard fine-tuning on 100 trajectories yields DepthVLA ≈79% vs baseline ≈65%. Zero-shot transfer: trained on BridgeData V2 and evaluated zero-shot on Simpler WidowX; DepthVLA outperforms baseline in that zero-shot setting (Table I).</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Paper describes architectural attention design (block-wise masking): VLM and depth tokens attend only to themselves while action tokens attend to all streams; this design preserves pretrained competences while enabling action conditioning on both semantic and geometric cues. No per-token/attention-head visualizations or explicit attention-map analyses are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>No explicit analysis of embedding space organization, clustering, or probing of semantic/visual feature manifolds is reported beyond qualitative statements that intermediate depth features provide richer geometric cues.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Indirect and empirical: improved grasping accuracy, collision avoidance, and object alignment correlated with depth-aware intermediate features; visualizations of predicted depth maps (Fig.5) show detailed 3D structure (object boundaries, occlusions) which the action expert attends to; ablations show depth pretraining and depth loss are important for action performance, supporting that geometric representations are being used to ground actions.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Yes — the depth expert outputs intermediate-layer features used by the action expert rather than only a final depth map; authors argue intermediate representations provide richer geometric cues for fine-grained spatial tasks (e.g., precise grasping, collision avoidance).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Transfer works better when explicit geometric priors are available: DepthVLA shows improved zero-shot sim transfer (BridgeData V2 → Simpler WidowX) and better sim→real and real→sim robustness when the depth expert is present; pretraining the depth expert on diverse 3D datasets improves transfer. The paper also notes domain challenges (reflective/transparent surfaces, tiny edges) where monocular depth prediction can fail and reduce transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not explicitly quantified. The paper reports overall benchmark performance across tasks and environments but does not provide per-object breakdown separating objects seen during pretraining from novel objects at test time.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Yes — Zero-shot: Simpler WidowX evaluation after training on BridgeData V2 (zero-shot) with reported success rates (DepthVLA 74.8% average). Few-shot: real-world few-shot fine-tuning with 20 trajectories: DepthVLA 63% vs baseline 45% progress score.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Ablations include (i) randomly initialized depth expert vs pretrained (pretraining necessary), (ii) removing depth loss during VLA training (degrades performance), (iii) freezing the depth expert during VLA training (performance not greatly impacted, indicating robustness), (iv) removing block-wise masking (harmful), and (v) using ground-truth depth as input vs predicting depth (predicting depth performs better). These establish the importance of depth pretraining, depth loss, block-wise attention, and the benefit of intermediate feature usage.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Yes — feeding ground-truth depth directly (instead of predicting depth internally) reduced performance in experiments; authors hypothesize modality-competence/over-reliance on an external modality causes negative transfer, so predicting depth internally leads to better integrated representations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Paper compares to spatially-aware baselines (SpatialVLA that uses an off-the-shelf depth estimator) and to VLAs without a dedicated depth expert (π0 re-implementation). DepthVLA outperforms both the VLM-only baseline and the external-depth estimator baseline, indicating that integrating a pretrained depth expert end-to-end is more effective than pure VLM semantics or external depth inputs alone. No direct comparison to vanilla vision-only ImageNet-pretrained models is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>No detailed analysis of representation or performance dynamics over the course of fine-tuning beyond reporting training steps and that pretraining/fine-tuning schedules were used (pretraining steps, VLA training steps).</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>No measurement of representation dimensionality (e.g., PCA or intrinsic dimension) is reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1924.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1924.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>π0 (re-impl)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>π0 (re-implemented Vision-Language-Action flow model baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-expert mixture-of-transformers VLA (VLM + flow-matching action expert) used as the primary baseline; here re-implemented by the authors to isolate the effect of adding a depth expert.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>π 0 : A visionlanguage-action flow model for general robot control</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>π0 (re-implemented)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Mixture-of-transformers with two experts: a VLM backbone and a flow-matching continuous action expert; tokens and attentions arranged similar to the MoT design but without a dedicated depth expert stream. Processes RGB images, language, and proprioception to output continuous action trajectories via flow-matching.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Uses a pretrained VLM backbone (same Paligemma-3B in the authors' reimplementation); in the reported reimplementation runs the model was not pretrained on additional large robot action datasets (marked × in tables).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>In the authors' reimplementation experiments π0 was trained on the same VLA training datasets but not given extra large-scale action pretraining; specific VLM pretraining details are inherited from Paligemma-3B (not specified here).</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Robotic manipulation (same benchmarks as DepthVLA: Simpler WidowX, LIBERO, and real-world tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Same sets of embodied tasks as DepthVLA: simulated suites (Simpler WidowX, LIBERO) and real-world bimanual tasks; continuous action prediction via flow-matching chunks.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Not explicitly analyzed for π0 in this paper beyond being the VLM-provided semantic base; used as a baseline to show limitations in spatial reasoning when lacking an explicit depth expert.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>As reimplemented baseline (no additional robot action pretraining): Simpler average success 58.8% (Table I); LIBERO average 93.6% (Table II); real-world progress ≈65% after fine-tuning (authors' reported baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not reported specifically for π0 as the paper uses π0 as a baseline that includes the same VLM backbone; total absence of VLM or vision-language pretraining is not evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>No explicit sample-efficiency curves reported for π0 beyond the baseline numbers in few-shot and fine-tuning comparisons (e.g., few-shot baseline 45% vs DepthVLA 63%).</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>No per-model attention visualizations or analyses for π0 beyond description of MoT attention masking in DepthVLA's design.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>None reported.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Baseline performance limitations (failures on precise spatial tasks) are used as negative evidence that lacking explicit 3D grounding harms fine-grained manipulation; no direct grounding analyses performed for π0.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not analyzed for π0 within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Serves as demonstration that adding a depth expert improves zero-shot and fine-tuned transfer in cited benchmarks; specifics about π0 transfer conditions not analyzed beyond benchmark scores.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Used in zero-shot Simpler evaluation (trained on BridgeData V2, evaluated zero-shot) and few-shot real-world fine-tuning comparisons (baseline numbers provided).</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not performed for π0 beyond being the baseline in the depth-expert ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not reported for π0 specifically.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>π0 is a VLM-based VLA (vision-language) and is compared against DepthVLA; authors use π0 to show the effect of adding an explicit depth stream rather than comparing to purely vision-only pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>None reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>None reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1924.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1924.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SpatialVLA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SpatialVLA (spatial-aware vision-language-action model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A VLA approach that leverages an off-the-shelf depth estimator to produce pseudo point clouds or depth inputs to augment VLM-based policies, but not optimized end-to-end with the action model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Spatialvla: Exploring spatial representations for visual-language-action model</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SpatialVLA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Augments VLM-based action policies with external depth estimates / pseudo point clouds produced by off-the-shelf depth estimators; the depth estimator is not trained end-to-end with the VLA in the original method.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Uses off-the-shelf depth estimator pretrained on depth datasets; VLM component inherits vision-language pretraining (not fully detailed here).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Depth pseudo-labels and off-the-shelf depth estimator pretraining on depth datasets (not exhaustively described in this paper); VLA training may use robot action datasets where reported.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Robotic manipulation benchmarks (compared in Simpler and LIBERO in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Same benchmark tasks (Simpler WidowX, LIBERO) used for comparison; discrete/continuous manipulation tasks requiring spatial understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Paper critiques SpatialVLA's approach as a workaround because the external depth estimator is not optimized end-to-end with the action policy, limiting performance upper bound; no quantitative semantic-alignment measures provided.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Reported in tables: Simpler average 34.4% (Table I) and LIBERO average 78.1% (Table II) — these are baseline values included for comparison in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not explicitly reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not discussed quantitatively in this paper for SpatialVLA.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>No attention analysis presented for SpatialVLA in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>None reported.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Paper argues that because the depth estimator is not trained end-to-end with the VLA, SpatialVLA's ability to ground action semantics into geometric representations is limited; no direct grounding measurements provided.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>SpatialVLA performs worse than DepthVLA on the Simpler benchmark; authors interpret this as evidence that end-to-end integrated depth experts transfer better than externally supplied depth.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not reported specifically for SpatialVLA in the paper beyond the included benchmark numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not present.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not directly reported for SpatialVLA, but the paper highlights design limitations that can cap performance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>SpatialVLA augments VLMs with depth estimates — authors compare it to DepthVLA and find DepthVLA outperforms SpatialVLA by a wide margin on several benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>None reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>None reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1924.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1924.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DreamVLA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DreamVLA (world-model-based Vision-Language-Action)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A VLA that leverages generative world models (video/semantic prediction) to simulate futures and condition action generation, improving planning but not necessarily the encoding of the current scene's 3D geometry.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Dreamvla: A vision-language-action model dreamed with comprehensive world knowledge</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DreamVLA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>World-model-based VLA that conditions actions on predicted futures (video frames, keypoints, semantic states); combines generative predictions with action generation pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Generative world-model pretraining (video / state prediction) and VLM pretraining likely involved; specific pretraining details are not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not specified in detail in this paper; characterized as utilizing world-model style predictions trained on video/state datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Robotic manipulation (LIBERO and related benchmarks compared in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Benchmarks requiring planning and multi-step manipulation; DreamVLA is compared in LIBERO where it achieves high reported success rates.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Paper positions DreamVLA as improving planning via futures but lacking explicit 3D knowledge for precise spatial manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Reported in Table II: DreamVLA average success on LIBERO = 92.6% (per table).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Paper argues DreamVLA improves planning but does not strongly improve encoding of current 3D geometry; no direct grounding metrics reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Not deeply analyzed here; DreamVLA is presented as a strong baseline in LIBERO but is noted to differ in approach (generative futures vs depth-based geometric priors).</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not present.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>DreamVLA is compared as a world-model-based VLA; the authors state DepthVLA surpasses such approaches on spatial accuracy-requiring tasks due to explicit 3D priors.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not discussed.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1924.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1924.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT-VLA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CoT-VLA (Chain-of-Thought for Vision-Language-Action)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A VLA approach that uses chain-of-thought style autoregressive generation of spatial tokens to inform actions, trading off latency for structured spatial reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Cot-vla: Visual chain-of-thought reasoning for visionlanguage-action models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CoT-VLA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Incorporates autoregressive Chain-of-Thought spatial token generation to produce intermediate spatial reasoning tokens and condition action prediction; this process is computationally expensive and incurs high latency.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Uses autoregressive token generation techniques inspired by VLM Chain-of-Thought methods; VLM pretraining implied but not detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Robotic manipulation / VLA benchmarks (referenced in comparisons on planning/spatial reasoning tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Multi-step manipulation tasks that benefit from intermediate spatial reasoning; no fine-grained task specs provided in this paper beyond benchmark inclusion.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Not quantitatively analyzed here; CoT is framed as a method to produce spatial tokens but with latency costs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Reported in Table II (LIBERO): CoT-VLA average 83.9%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Paper notes CoT approaches produce explicit spatial tokens to drive action but suffer latency (hundreds of tokens generated autoregressively), and do not present direct grounding evidence in this text.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not analyzed.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Latency and computational cost are noted limitations; no deeper transfer condition analysis provided.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not present.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>High inference latency (>2 seconds) is cited as a practical limitation; no empirical negative transfer is described.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not directly compared to vision-only baselines here.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not discussed.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1924.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1924.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MolmoACT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MolmoACT (Action reasoning models that can reason in space)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A VLA that emphasizes action reasoning in space to improve spatial planning and manipulation; included as a competitive baseline on LIBERO.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Molmoact: Action reasoning models that can reason in space</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MolmoACT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Designed for spatial action reasoning in manipulation tasks; exact architecture details are not provided in this paper beyond classification as a spatially capable VLA.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Reported in Table II as pretrained on additional robot action data (✓); specifics not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not described in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Robotic manipulation (LIBERO benchmark included in comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>LIBERO task suites requiring spatial/object/goal/long-horizon capabilities; model reported to achieve competitive performance on these suites.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Reported in Table II (LIBERO): MolmoACT average 86.6%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Not provided here; used as a comparative baseline to show different approaches to spatial reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not present.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1924.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1924.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OpenVLA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenVLA (open-source vision-language-action model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source VLA cited as a baseline for generalist manipulation; included in LIBERO comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Openvla: An open-source vision-language-action model</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenVLA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Generalist VLA built on VLMs and action-generation components; paper does not include architecture details beyond being a VLA baseline in comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Marked as pretrained on additional robot action data (✓) in LIBERO table; details not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not described here.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Robotic manipulation (LIBERO benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>LIBERO task suites; used as a comparative baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Not analyzed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Reported in Table II (LIBERO): OpenVLA average 76.5%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>None reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>None reported.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Not discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not discussed.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1924.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e1924.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Octo-Base</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Octo-Base (open-source generalist robot policy baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A generalist robot policy included as a competitive baseline across benchmarks; reported as pretrained on action data in tables.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Octo: An open-source generalist robot policy</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Octo-Base</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source generalist policy for robot manipulation tasks; paper provides benchmark numbers but not architecture details.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Marked as pretrained on additional robot action data (✓) in benchmark tables; exact pretraining regimen not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Robotic manipulation benchmarks (Simpler, LIBERO comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Various manipulation suites in simulation and real-world benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Not analyzed here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Table I (Simpler) shows Octo-Base average 16.0%; Table II (LIBERO) shows Octo-Base average 75.1%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>None reported.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>None reported.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Not discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not discussed.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1924.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e1924.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RT-2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RT-2 (Vision-language-action transfer model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A vision-language-action family/effort that transfers web-scale knowledge to robotic control; cited in related work for VLA pretraining and transfer motivation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Rt-2: Vision-language-action models transfer web knowledge to robotic control</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RT-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A VLA approach that transfers knowledge from web-scale vision-language pretraining to robotic control tasks; specific architectural details are not provided in this paper (cited as related work).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Vision-language pretraining on large web data transferring to robot control (per citation context).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not specified in this paper; referenced as transferring web knowledge (image-text) to robotics.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Robotic manipulation / vision-language conditioned control (cited)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>General transfer of VLM capabilities to embodied control; specific tasks not enumerated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Referenced as an example of transferring semantic/web knowledge to embodied tasks; no quantitative alignment analysis provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Cited as motivation for VLAs that leverage VLM semantics in control; no direct evidence presented in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Mentioned conceptually: VLAs built on large VLMs can generalize, but spatial reasoning may be limited if not grounded in 3D priors.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not present.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>RT-2 framed as leveraging vision-language pretraining vs other paradigms; no empirical comparison presented here.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not discussed.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>π 0 : A visionlanguage-action flow model for general robot control <em>(Rating: 2)</em></li>
                <li>Spatialvla: Exploring spatial representations for visual-language-action model <em>(Rating: 2)</em></li>
                <li>Dreamvla: A vision-language-action model dreamed with comprehensive world knowledge <em>(Rating: 2)</em></li>
                <li>Cot-vla: Visual chain-of-thought reasoning for visionlanguage-action models <em>(Rating: 2)</em></li>
                <li>Molmoact: Action reasoning models that can reason in space <em>(Rating: 2)</em></li>
                <li>Openvla: An open-source vision-language-action model <em>(Rating: 1)</em></li>
                <li>Octo: An open-source generalist robot policy <em>(Rating: 1)</em></li>
                <li>Rt-2: Vision-language-action models transfer web knowledge to robotic control <em>(Rating: 1)</em></li>
                <li>Depth anything v2 <em>(Rating: 2)</em></li>
                <li>WildRGB-D <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1924",
    "paper_id": "paper-282102924",
    "extraction_schema_id": "extraction-schema-39",
    "extracted_data": [
        {
            "name_short": "DepthVLA",
            "name_full": "Depth-aware Vision-Language-Action model (DepthVLA)",
            "brief_description": "A mixture-of-transformers VLA that injects a pretrained monocular depth prediction expert alongside a vision-language model and a flow-matching action expert, enabling explicit geometric reasoning for precise manipulation and collision avoidance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "DepthVLA",
            "model_description": "Mixture-of-transformers integrating (1) a VLM (Paligemma-3B) for language and semantics, (2) a depth expert (DINOv2-L encoder initialized from Depth Anything V2 + transformer decoder) producing intermediate depth features and predicted depth maps, and (3) a flow-matching continuous action expert; modalities: monocular RGB, language, proprioception, and internally predicted depth; block-wise attention masking lets action tokens attend to both VLM and depth streams while preserving pretrained module competence.",
            "pretraining_type": "Mixed: vision-language pretraining for VLM; vision-only monocular depth pretraining for depth expert (3D datasets); (optional) action-data pretraining for some experiments but main claims rely on depth expert + VLM.",
            "pretraining_data_description": "Depth expert pretrained on large-scale 3D/monocular depth datasets (WildRGB-D, Scannet, Scannet++, HyperSim) using metric/scale-invariant log loss; VLM (Paligemma-3B) assumed pretrained on large vision-language corpora (not detailed here); VLA training / robot pretraining datasets used in experiments include Galaxea Open-World (100k trajectories, 150 task categories), BridgeData V2 (60k trajectories), and LIBERO (task demonstrations). Pretraining data contain scene geometry (depth labels / synthetic scenes) and robot action trajectories where applicable; explicit descriptions of text/action token statistics not provided.",
            "target_task_name": "Robotic manipulation (vision-language-conditioned continuous control)",
            "target_task_description": "Evaluated on both real-world bimanual tasks (Galaxea R1 Lite dual-arm platform: table bussing, microwave operation, block stacking) and simulated benchmarks (LIBERO with Franka Panda; Simpler WidowX). Action space: continuous control predicted as 1-second action chunks (16 steps @15 Hz) via flow-matching; environments include cluttered scenes, small-object grasping, door and plate manipulation, block stacking, and diverse simulated object/scene configurations.",
            "semantic_alignment": "The paper states semantic grounding comes from the pretrained VLM (large-scale web image-text pretraining) while geometric priors come from depth expert pretrained on 3D datasets; it does not quantify exact overlap of objects/actions between VLM/depth pretraining and target tasks but argues VLM provides open-vocabulary semantic grounding complementary to depth-based geometry.",
            "performance_with_language_pretraining": "Real-world progress (after fine-tuning on 100 trajectories): DepthVLA average progress ≈ 78.5% (reported 79% in main text), few-shot (20 trajectories) progress 63%; Simpler WidowX zero-shot average success 74.8% (per Table I); LIBERO joint-trained average success 94.9% (per Table II). Metrics: success rates (%) for simulated tasks and averaged progress scores (%) for real-world benchmarks.",
            "performance_without_language_pretraining": "Closest baseline re-implemented (π0, which lacks the depth expert in this comparison) - Real-world progress after fine-tuning: 65.0% (baseline); few-shot (20 trajectories) 45%; Simpler average success 58.8% (Table I); LIBERO average success 93.6% (Table II). Note: π0 still uses the same VLM backbone in the authors' reimplementation; 'without language pretraining' here is used to indicate the baseline missing DepthVLA's explicit depth expert rather than a model with no VLM at all.",
            "sample_efficiency_comparison": "Reported few-shot finetuning: with 20 trajectories (few-shot), DepthVLA achieves 63% average progress vs baseline 45% (absolute +18 percentage points), indicating improved sample efficiency in low-data transfer; standard fine-tuning on 100 trajectories yields DepthVLA ≈79% vs baseline ≈65%. Zero-shot transfer: trained on BridgeData V2 and evaluated zero-shot on Simpler WidowX; DepthVLA outperforms baseline in that zero-shot setting (Table I).",
            "has_sample_efficiency_data": true,
            "attention_analysis": "Paper describes architectural attention design (block-wise masking): VLM and depth tokens attend only to themselves while action tokens attend to all streams; this design preserves pretrained competences while enabling action conditioning on both semantic and geometric cues. No per-token/attention-head visualizations or explicit attention-map analyses are provided.",
            "embedding_space_analysis": "No explicit analysis of embedding space organization, clustering, or probing of semantic/visual feature manifolds is reported beyond qualitative statements that intermediate depth features provide richer geometric cues.",
            "action_grounding_evidence": "Indirect and empirical: improved grasping accuracy, collision avoidance, and object alignment correlated with depth-aware intermediate features; visualizations of predicted depth maps (Fig.5) show detailed 3D structure (object boundaries, occlusions) which the action expert attends to; ablations show depth pretraining and depth loss are important for action performance, supporting that geometric representations are being used to ground actions.",
            "hierarchical_features_evidence": "Yes — the depth expert outputs intermediate-layer features used by the action expert rather than only a final depth map; authors argue intermediate representations provide richer geometric cues for fine-grained spatial tasks (e.g., precise grasping, collision avoidance).",
            "transfer_conditions": "Transfer works better when explicit geometric priors are available: DepthVLA shows improved zero-shot sim transfer (BridgeData V2 → Simpler WidowX) and better sim→real and real→sim robustness when the depth expert is present; pretraining the depth expert on diverse 3D datasets improves transfer. The paper also notes domain challenges (reflective/transparent surfaces, tiny edges) where monocular depth prediction can fail and reduce transfer.",
            "novel_vs_familiar_objects": "Not explicitly quantified. The paper reports overall benchmark performance across tasks and environments but does not provide per-object breakdown separating objects seen during pretraining from novel objects at test time.",
            "zero_shot_or_few_shot": "Yes — Zero-shot: Simpler WidowX evaluation after training on BridgeData V2 (zero-shot) with reported success rates (DepthVLA 74.8% average). Few-shot: real-world few-shot fine-tuning with 20 trajectories: DepthVLA 63% vs baseline 45% progress score.",
            "layer_analysis": "Ablations include (i) randomly initialized depth expert vs pretrained (pretraining necessary), (ii) removing depth loss during VLA training (degrades performance), (iii) freezing the depth expert during VLA training (performance not greatly impacted, indicating robustness), (iv) removing block-wise masking (harmful), and (v) using ground-truth depth as input vs predicting depth (predicting depth performs better). These establish the importance of depth pretraining, depth loss, block-wise attention, and the benefit of intermediate feature usage.",
            "negative_transfer_evidence": "Yes — feeding ground-truth depth directly (instead of predicting depth internally) reduced performance in experiments; authors hypothesize modality-competence/over-reliance on an external modality causes negative transfer, so predicting depth internally leads to better integrated representations.",
            "comparison_to_vision_only": "Paper compares to spatially-aware baselines (SpatialVLA that uses an off-the-shelf depth estimator) and to VLAs without a dedicated depth expert (π0 re-implementation). DepthVLA outperforms both the VLM-only baseline and the external-depth estimator baseline, indicating that integrating a pretrained depth expert end-to-end is more effective than pure VLM semantics or external depth inputs alone. No direct comparison to vanilla vision-only ImageNet-pretrained models is provided.",
            "temporal_dynamics": "No detailed analysis of representation or performance dynamics over the course of fine-tuning beyond reporting training steps and that pretraining/fine-tuning schedules were used (pretraining steps, VLA training steps).",
            "dimensionality_analysis": "No measurement of representation dimensionality (e.g., PCA or intrinsic dimension) is reported.",
            "uuid": "e1924.0"
        },
        {
            "name_short": "π0 (re-impl)",
            "name_full": "π0 (re-implemented Vision-Language-Action flow model baseline)",
            "brief_description": "A two-expert mixture-of-transformers VLA (VLM + flow-matching action expert) used as the primary baseline; here re-implemented by the authors to isolate the effect of adding a depth expert.",
            "citation_title": "π 0 : A visionlanguage-action flow model for general robot control",
            "mention_or_use": "use",
            "model_name": "π0 (re-implemented)",
            "model_description": "Mixture-of-transformers with two experts: a VLM backbone and a flow-matching continuous action expert; tokens and attentions arranged similar to the MoT design but without a dedicated depth expert stream. Processes RGB images, language, and proprioception to output continuous action trajectories via flow-matching.",
            "pretraining_type": "Uses a pretrained VLM backbone (same Paligemma-3B in the authors' reimplementation); in the reported reimplementation runs the model was not pretrained on additional large robot action datasets (marked × in tables).",
            "pretraining_data_description": "In the authors' reimplementation experiments π0 was trained on the same VLA training datasets but not given extra large-scale action pretraining; specific VLM pretraining details are inherited from Paligemma-3B (not specified here).",
            "target_task_name": "Robotic manipulation (same benchmarks as DepthVLA: Simpler WidowX, LIBERO, and real-world tasks)",
            "target_task_description": "Same sets of embodied tasks as DepthVLA: simulated suites (Simpler WidowX, LIBERO) and real-world bimanual tasks; continuous action prediction via flow-matching chunks.",
            "semantic_alignment": "Not explicitly analyzed for π0 in this paper beyond being the VLM-provided semantic base; used as a baseline to show limitations in spatial reasoning when lacking an explicit depth expert.",
            "performance_with_language_pretraining": "As reimplemented baseline (no additional robot action pretraining): Simpler average success 58.8% (Table I); LIBERO average 93.6% (Table II); real-world progress ≈65% after fine-tuning (authors' reported baseline).",
            "performance_without_language_pretraining": "Not reported specifically for π0 as the paper uses π0 as a baseline that includes the same VLM backbone; total absence of VLM or vision-language pretraining is not evaluated.",
            "sample_efficiency_comparison": "No explicit sample-efficiency curves reported for π0 beyond the baseline numbers in few-shot and fine-tuning comparisons (e.g., few-shot baseline 45% vs DepthVLA 63%).",
            "has_sample_efficiency_data": false,
            "attention_analysis": "No per-model attention visualizations or analyses for π0 beyond description of MoT attention masking in DepthVLA's design.",
            "embedding_space_analysis": "None reported.",
            "action_grounding_evidence": "Baseline performance limitations (failures on precise spatial tasks) are used as negative evidence that lacking explicit 3D grounding harms fine-grained manipulation; no direct grounding analyses performed for π0.",
            "hierarchical_features_evidence": "Not analyzed for π0 within this paper.",
            "transfer_conditions": "Serves as demonstration that adding a depth expert improves zero-shot and fine-tuned transfer in cited benchmarks; specifics about π0 transfer conditions not analyzed beyond benchmark scores.",
            "novel_vs_familiar_objects": "Not reported.",
            "zero_shot_or_few_shot": "Used in zero-shot Simpler evaluation (trained on BridgeData V2, evaluated zero-shot) and few-shot real-world fine-tuning comparisons (baseline numbers provided).",
            "layer_analysis": "Not performed for π0 beyond being the baseline in the depth-expert ablations.",
            "negative_transfer_evidence": "Not reported for π0 specifically.",
            "comparison_to_vision_only": "π0 is a VLM-based VLA (vision-language) and is compared against DepthVLA; authors use π0 to show the effect of adding an explicit depth stream rather than comparing to purely vision-only pretraining.",
            "temporal_dynamics": "None reported.",
            "dimensionality_analysis": "None reported.",
            "uuid": "e1924.1"
        },
        {
            "name_short": "SpatialVLA",
            "name_full": "SpatialVLA (spatial-aware vision-language-action model)",
            "brief_description": "A VLA approach that leverages an off-the-shelf depth estimator to produce pseudo point clouds or depth inputs to augment VLM-based policies, but not optimized end-to-end with the action model.",
            "citation_title": "Spatialvla: Exploring spatial representations for visual-language-action model",
            "mention_or_use": "mention",
            "model_name": "SpatialVLA",
            "model_description": "Augments VLM-based action policies with external depth estimates / pseudo point clouds produced by off-the-shelf depth estimators; the depth estimator is not trained end-to-end with the VLA in the original method.",
            "pretraining_type": "Uses off-the-shelf depth estimator pretrained on depth datasets; VLM component inherits vision-language pretraining (not fully detailed here).",
            "pretraining_data_description": "Depth pseudo-labels and off-the-shelf depth estimator pretraining on depth datasets (not exhaustively described in this paper); VLA training may use robot action datasets where reported.",
            "target_task_name": "Robotic manipulation benchmarks (compared in Simpler and LIBERO in this paper)",
            "target_task_description": "Same benchmark tasks (Simpler WidowX, LIBERO) used for comparison; discrete/continuous manipulation tasks requiring spatial understanding.",
            "semantic_alignment": "Paper critiques SpatialVLA's approach as a workaround because the external depth estimator is not optimized end-to-end with the action policy, limiting performance upper bound; no quantitative semantic-alignment measures provided.",
            "performance_with_language_pretraining": "Reported in tables: Simpler average 34.4% (Table I) and LIBERO average 78.1% (Table II) — these are baseline values included for comparison in the paper.",
            "performance_without_language_pretraining": "Not explicitly reported here.",
            "sample_efficiency_comparison": "Not discussed quantitatively in this paper for SpatialVLA.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "No attention analysis presented for SpatialVLA in this paper.",
            "embedding_space_analysis": "None reported.",
            "action_grounding_evidence": "Paper argues that because the depth estimator is not trained end-to-end with the VLA, SpatialVLA's ability to ground action semantics into geometric representations is limited; no direct grounding measurements provided.",
            "hierarchical_features_evidence": "Not discussed.",
            "transfer_conditions": "SpatialVLA performs worse than DepthVLA on the Simpler benchmark; authors interpret this as evidence that end-to-end integrated depth experts transfer better than externally supplied depth.",
            "novel_vs_familiar_objects": "Not reported.",
            "zero_shot_or_few_shot": "Not reported specifically for SpatialVLA in the paper beyond the included benchmark numbers.",
            "layer_analysis": "Not present.",
            "negative_transfer_evidence": "Not directly reported for SpatialVLA, but the paper highlights design limitations that can cap performance.",
            "comparison_to_vision_only": "SpatialVLA augments VLMs with depth estimates — authors compare it to DepthVLA and find DepthVLA outperforms SpatialVLA by a wide margin on several benchmarks.",
            "temporal_dynamics": "None reported.",
            "dimensionality_analysis": "None reported.",
            "uuid": "e1924.2"
        },
        {
            "name_short": "DreamVLA",
            "name_full": "DreamVLA (world-model-based Vision-Language-Action)",
            "brief_description": "A VLA that leverages generative world models (video/semantic prediction) to simulate futures and condition action generation, improving planning but not necessarily the encoding of the current scene's 3D geometry.",
            "citation_title": "Dreamvla: A vision-language-action model dreamed with comprehensive world knowledge",
            "mention_or_use": "mention",
            "model_name": "DreamVLA",
            "model_description": "World-model-based VLA that conditions actions on predicted futures (video frames, keypoints, semantic states); combines generative predictions with action generation pipelines.",
            "pretraining_type": "Generative world-model pretraining (video / state prediction) and VLM pretraining likely involved; specific pretraining details are not provided in this paper.",
            "pretraining_data_description": "Not specified in detail in this paper; characterized as utilizing world-model style predictions trained on video/state datasets.",
            "target_task_name": "Robotic manipulation (LIBERO and related benchmarks compared in this paper)",
            "target_task_description": "Benchmarks requiring planning and multi-step manipulation; DreamVLA is compared in LIBERO where it achieves high reported success rates.",
            "semantic_alignment": "Paper positions DreamVLA as improving planning via futures but lacking explicit 3D knowledge for precise spatial manipulation.",
            "performance_with_language_pretraining": "Reported in Table II: DreamVLA average success on LIBERO = 92.6% (per table).",
            "performance_without_language_pretraining": "Not reported here.",
            "sample_efficiency_comparison": "Not discussed in this paper.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not provided in this paper.",
            "embedding_space_analysis": "Not provided.",
            "action_grounding_evidence": "Paper argues DreamVLA improves planning but does not strongly improve encoding of current 3D geometry; no direct grounding metrics reported here.",
            "hierarchical_features_evidence": "Not provided.",
            "transfer_conditions": "Not deeply analyzed here; DreamVLA is presented as a strong baseline in LIBERO but is noted to differ in approach (generative futures vs depth-based geometric priors).",
            "novel_vs_familiar_objects": "Not reported.",
            "zero_shot_or_few_shot": "Not reported in this paper.",
            "layer_analysis": "Not present.",
            "negative_transfer_evidence": "Not reported.",
            "comparison_to_vision_only": "DreamVLA is compared as a world-model-based VLA; the authors state DepthVLA surpasses such approaches on spatial accuracy-requiring tasks due to explicit 3D priors.",
            "temporal_dynamics": "Not discussed.",
            "dimensionality_analysis": "Not discussed.",
            "uuid": "e1924.3"
        },
        {
            "name_short": "CoT-VLA",
            "name_full": "CoT-VLA (Chain-of-Thought for Vision-Language-Action)",
            "brief_description": "A VLA approach that uses chain-of-thought style autoregressive generation of spatial tokens to inform actions, trading off latency for structured spatial reasoning.",
            "citation_title": "Cot-vla: Visual chain-of-thought reasoning for visionlanguage-action models",
            "mention_or_use": "mention",
            "model_name": "CoT-VLA",
            "model_description": "Incorporates autoregressive Chain-of-Thought spatial token generation to produce intermediate spatial reasoning tokens and condition action prediction; this process is computationally expensive and incurs high latency.",
            "pretraining_type": "Uses autoregressive token generation techniques inspired by VLM Chain-of-Thought methods; VLM pretraining implied but not detailed here.",
            "pretraining_data_description": "Not specified in this paper.",
            "target_task_name": "Robotic manipulation / VLA benchmarks (referenced in comparisons on planning/spatial reasoning tasks)",
            "target_task_description": "Multi-step manipulation tasks that benefit from intermediate spatial reasoning; no fine-grained task specs provided in this paper beyond benchmark inclusion.",
            "semantic_alignment": "Not quantitatively analyzed here; CoT is framed as a method to produce spatial tokens but with latency costs.",
            "performance_with_language_pretraining": "Reported in Table II (LIBERO): CoT-VLA average 83.9%.",
            "performance_without_language_pretraining": "Not reported.",
            "sample_efficiency_comparison": "Not reported.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not provided here.",
            "embedding_space_analysis": "Not provided.",
            "action_grounding_evidence": "Paper notes CoT approaches produce explicit spatial tokens to drive action but suffer latency (hundreds of tokens generated autoregressively), and do not present direct grounding evidence in this text.",
            "hierarchical_features_evidence": "Not analyzed.",
            "transfer_conditions": "Latency and computational cost are noted limitations; no deeper transfer condition analysis provided.",
            "novel_vs_familiar_objects": "Not reported.",
            "zero_shot_or_few_shot": "Not reported.",
            "layer_analysis": "Not present.",
            "negative_transfer_evidence": "High inference latency (&gt;2 seconds) is cited as a practical limitation; no empirical negative transfer is described.",
            "comparison_to_vision_only": "Not directly compared to vision-only baselines here.",
            "temporal_dynamics": "Not discussed.",
            "dimensionality_analysis": "Not discussed.",
            "uuid": "e1924.4"
        },
        {
            "name_short": "MolmoACT",
            "name_full": "MolmoACT (Action reasoning models that can reason in space)",
            "brief_description": "A VLA that emphasizes action reasoning in space to improve spatial planning and manipulation; included as a competitive baseline on LIBERO.",
            "citation_title": "Molmoact: Action reasoning models that can reason in space",
            "mention_or_use": "mention",
            "model_name": "MolmoACT",
            "model_description": "Designed for spatial action reasoning in manipulation tasks; exact architecture details are not provided in this paper beyond classification as a spatially capable VLA.",
            "pretraining_type": "Reported in Table II as pretrained on additional robot action data (✓); specifics not provided here.",
            "pretraining_data_description": "Not described in this paper.",
            "target_task_name": "Robotic manipulation (LIBERO benchmark included in comparisons)",
            "target_task_description": "LIBERO task suites requiring spatial/object/goal/long-horizon capabilities; model reported to achieve competitive performance on these suites.",
            "semantic_alignment": "Not detailed in this paper.",
            "performance_with_language_pretraining": "Reported in Table II (LIBERO): MolmoACT average 86.6%.",
            "performance_without_language_pretraining": "Not provided.",
            "sample_efficiency_comparison": "Not provided.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not provided in this paper.",
            "embedding_space_analysis": "Not provided.",
            "action_grounding_evidence": "Not provided here; used as a comparative baseline to show different approaches to spatial reasoning.",
            "hierarchical_features_evidence": "Not provided.",
            "transfer_conditions": "Not discussed in this paper.",
            "novel_vs_familiar_objects": "Not reported.",
            "zero_shot_or_few_shot": "Not reported in this paper.",
            "layer_analysis": "Not present.",
            "negative_transfer_evidence": "Not reported.",
            "comparison_to_vision_only": "Not reported.",
            "temporal_dynamics": "Not reported.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1924.5"
        },
        {
            "name_short": "OpenVLA",
            "name_full": "OpenVLA (open-source vision-language-action model)",
            "brief_description": "An open-source VLA cited as a baseline for generalist manipulation; included in LIBERO comparisons.",
            "citation_title": "Openvla: An open-source vision-language-action model",
            "mention_or_use": "mention",
            "model_name": "OpenVLA",
            "model_description": "Generalist VLA built on VLMs and action-generation components; paper does not include architecture details beyond being a VLA baseline in comparisons.",
            "pretraining_type": "Marked as pretrained on additional robot action data (✓) in LIBERO table; details not provided in this paper.",
            "pretraining_data_description": "Not described here.",
            "target_task_name": "Robotic manipulation (LIBERO benchmark)",
            "target_task_description": "LIBERO task suites; used as a comparative baseline.",
            "semantic_alignment": "Not analyzed in this paper.",
            "performance_with_language_pretraining": "Reported in Table II (LIBERO): OpenVLA average 76.5%.",
            "performance_without_language_pretraining": "Not provided.",
            "sample_efficiency_comparison": "Not provided.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "None reported here.",
            "embedding_space_analysis": "None reported.",
            "action_grounding_evidence": "Not provided in this paper.",
            "hierarchical_features_evidence": "Not provided.",
            "transfer_conditions": "Not discussed.",
            "novel_vs_familiar_objects": "Not reported.",
            "zero_shot_or_few_shot": "Not reported here.",
            "layer_analysis": "Not reported.",
            "negative_transfer_evidence": "Not reported.",
            "comparison_to_vision_only": "Not discussed.",
            "temporal_dynamics": "Not discussed.",
            "dimensionality_analysis": "Not discussed.",
            "uuid": "e1924.6"
        },
        {
            "name_short": "Octo-Base",
            "name_full": "Octo-Base (open-source generalist robot policy baseline)",
            "brief_description": "A generalist robot policy included as a competitive baseline across benchmarks; reported as pretrained on action data in tables.",
            "citation_title": "Octo: An open-source generalist robot policy",
            "mention_or_use": "mention",
            "model_name": "Octo-Base",
            "model_description": "Open-source generalist policy for robot manipulation tasks; paper provides benchmark numbers but not architecture details.",
            "pretraining_type": "Marked as pretrained on additional robot action data (✓) in benchmark tables; exact pretraining regimen not specified here.",
            "pretraining_data_description": "Not detailed in this paper.",
            "target_task_name": "Robotic manipulation benchmarks (Simpler, LIBERO comparisons)",
            "target_task_description": "Various manipulation suites in simulation and real-world benchmarks.",
            "semantic_alignment": "Not analyzed here.",
            "performance_with_language_pretraining": "Table I (Simpler) shows Octo-Base average 16.0%; Table II (LIBERO) shows Octo-Base average 75.1%.",
            "performance_without_language_pretraining": "Not reported.",
            "sample_efficiency_comparison": "Not reported.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "None reported.",
            "embedding_space_analysis": "None reported.",
            "action_grounding_evidence": "Not provided here.",
            "hierarchical_features_evidence": "Not provided.",
            "transfer_conditions": "Not discussed.",
            "novel_vs_familiar_objects": "Not reported.",
            "zero_shot_or_few_shot": "Not reported.",
            "layer_analysis": "Not reported.",
            "negative_transfer_evidence": "Not reported.",
            "comparison_to_vision_only": "Not discussed.",
            "temporal_dynamics": "Not discussed.",
            "dimensionality_analysis": "Not discussed.",
            "uuid": "e1924.7"
        },
        {
            "name_short": "RT-2",
            "name_full": "RT-2 (Vision-language-action transfer model)",
            "brief_description": "A vision-language-action family/effort that transfers web-scale knowledge to robotic control; cited in related work for VLA pretraining and transfer motivation.",
            "citation_title": "Rt-2: Vision-language-action models transfer web knowledge to robotic control",
            "mention_or_use": "mention",
            "model_name": "RT-2",
            "model_description": "A VLA approach that transfers knowledge from web-scale vision-language pretraining to robotic control tasks; specific architectural details are not provided in this paper (cited as related work).",
            "pretraining_type": "Vision-language pretraining on large web data transferring to robot control (per citation context).",
            "pretraining_data_description": "Not specified in this paper; referenced as transferring web knowledge (image-text) to robotics.",
            "target_task_name": "Robotic manipulation / vision-language conditioned control (cited)",
            "target_task_description": "General transfer of VLM capabilities to embodied control; specific tasks not enumerated in this paper.",
            "semantic_alignment": "Referenced as an example of transferring semantic/web knowledge to embodied tasks; no quantitative alignment analysis provided here.",
            "performance_with_language_pretraining": "Not reported in this paper.",
            "performance_without_language_pretraining": "Not reported here.",
            "sample_efficiency_comparison": "Not discussed here.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not discussed.",
            "embedding_space_analysis": "Not discussed.",
            "action_grounding_evidence": "Cited as motivation for VLAs that leverage VLM semantics in control; no direct evidence presented in this paper.",
            "hierarchical_features_evidence": "Not discussed.",
            "transfer_conditions": "Mentioned conceptually: VLAs built on large VLMs can generalize, but spatial reasoning may be limited if not grounded in 3D priors.",
            "novel_vs_familiar_objects": "Not reported.",
            "zero_shot_or_few_shot": "Not reported here.",
            "layer_analysis": "Not present.",
            "negative_transfer_evidence": "Not discussed in this paper.",
            "comparison_to_vision_only": "RT-2 framed as leveraging vision-language pretraining vs other paradigms; no empirical comparison presented here.",
            "temporal_dynamics": "Not discussed.",
            "dimensionality_analysis": "Not discussed.",
            "uuid": "e1924.8"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "π 0 : A visionlanguage-action flow model for general robot control",
            "rating": 2
        },
        {
            "paper_title": "Spatialvla: Exploring spatial representations for visual-language-action model",
            "rating": 2
        },
        {
            "paper_title": "Dreamvla: A vision-language-action model dreamed with comprehensive world knowledge",
            "rating": 2
        },
        {
            "paper_title": "Cot-vla: Visual chain-of-thought reasoning for visionlanguage-action models",
            "rating": 2
        },
        {
            "paper_title": "Molmoact: Action reasoning models that can reason in space",
            "rating": 2
        },
        {
            "paper_title": "Openvla: An open-source vision-language-action model",
            "rating": 1
        },
        {
            "paper_title": "Octo: An open-source generalist robot policy",
            "rating": 1
        },
        {
            "paper_title": "Rt-2: Vision-language-action models transfer web knowledge to robotic control",
            "rating": 1
        },
        {
            "paper_title": "Depth anything v2",
            "rating": 2
        },
        {
            "paper_title": "WildRGB-D",
            "rating": 1
        }
    ],
    "cost": 0.023121,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>15 Oct 2025
15 Oct 20254A0F9AB31CC99A1EBE3BCB00612FAB38arXiv:2510.13375v1[cs.CV]
Vision-Language-Action (VLA) models have recently shown impressive generalization and language-guided manipulation capabilities.However, their performance degrades on tasks requiring precise spatial reasoning due to limited spatial reasoning inherited from Vision-Language Models (VLMs).Existing VLAs rely on extensive action-data pretraining to ground VLMs in 3D space, which reduces training efficiency and is still insufficient for accurate spatial understanding.In this work, we present DepthVLA, a simple yet effective VLA architecture that explicitly incorporates spatial awareness through a pretrained depth prediction module.DepthVLA adopts a mixture-of-transformers design that unifies a VLM, a depth transformer, and an action expert with fully shared attentions, forming an end-to-end model with enhanced spatial reasoning.Extensive evaluations in both real-world and simulated environments show that DepthVLA outperforms stateof-the-art approaches, achieving 78.5% vs. 65.0%progress in real-world tasks, 94.9% vs. 93.6% in the LIBERO simulator, and 74.8% vs. 58.8% in the Simpler simulator.Our code will be made publicly available.</p>
<p>I. INTRODUCTION</p>
<p>Vision-Language-Action (VLA) models [1]- [6] have emerged as a pivotal paradigm in robotic manipulation research.Built upon large-scale pretrained Vision-Language Models (VLMs), they inherit strong generalization capabilities from vast web data.VLMs provide robust language grounding and semantic visual perception, enabling VLAs to generalize across diverse tasks and embodiments.However, despite their strengths on semantics, VLMs exhibit limited spatial reasoning ability [7], [8], which in turn constrains the spatial perception abilities of VLAs, particularly in tasks requiring precise manipulation [9], [10].Current VLAs often rely on extensive action-data pretraining to ground VLMs in 3D space [1]- [6], which limits scalability, and pretrained VLAs continue to struggle with precise spatial reasoning.In practice, VLAs often fail at grasping small objects, executing precise operations, or avoiding collisions, highlighting their weak spatial perception.</p>
<p>Recent works have attempted to address this limitation by employing generative world models to predict future states [11]- [16].While promising, these methods lack explicit 3D knowledge, which we argue is essential for precise manipulation.Another line of work leverages Chain-of-Thought (CoT) reasoning [17] to autoregressively generate spatial tokens.However, this approach introduces significant latency (over 2 seconds), as hundreds of spatial tokens must be generated before action prediction.To overcome these limitations, we ask: how can recent advances in 3D perception [18]- [20] be leveraged to enhance VLAs without sacrificing inference speed?</p>
<p>To address this, we introduce DepthVLA (Figure 1), a simple yet effective VLA architecture that explicitly incorporates spatial awareness through a pretrained depth prediction expert.Trained on diverse 3D datasets [21]- [24], this module provides robust geometric understanding.Inspired by π 0 [3], DepthVLA uses a mixture-of-transformers (MoT) [25] design that integrates the depth expert with a VLM and a flowmatching action expert via fully shared attentions, forming an end-to-end VLA model.Intuitively, the VLM provides language understanding and open-vocabulary semantic perception, the depth expert provides fine-grained geometric cues, and the action expert generates actions conditioned on representations from both modalities.The MoT design also enables separate pretraining of each component, allowing training on a more diverse set of data beyond embodied action datasets.Despite adding a depth expert, DepthVLA only increases inference latency marginally, making it practical for real-time deployment.</p>
<p>We validate DepthVLA through extensive experiments in both real-world and simulated environments.We validate DepthVLA through extensive experiments in both real-world and simulated environments.Our evaluations show notable gains in grasping accuracy and collision avoidance, underscoring DepthVLA's enhanced spatial reasoning.For realworld evaluation, we pretrain on the Galaxea Open-World Dataset [26] and test on the Galaxea R1 Lite, a commercially available dual-arm mobile platform.In simulation, we evaluate on LIBERO [27] and Simpler [28].Results show that DepthVLA outperforms existing approaches, achieving 78.5% vs. 65.0%success in real-world tasks, 94.9% vs. 93.6% in LIBERO, and 74.8% vs. 58.8% in Simpler, demonstrating the effectiveness of depth-aware representations for precise, generalizable manipulation.</p>
<p>Our contributions are summarized as follows:</p>
<p>II. RELATED WORK</p>
<p>A. Generalist Robot Manipulation Policies</p>
<p>Robotic manipulation has evolved from single-task specialists to generalist models trained on broad, diverse datasets covering many tasks and embodiments.Fueled by advances in LLMs, VLMs [29], [30], and large-scale robot action datasets [31], [32], this evolution has given rise to Vision-Language-Action (VLA) models.Early VLAs [1], [2] typically fine-tuned VLMs to autoregressively generate action tokens, which facilitated knowledge transfer but incurred slow inference.More recent VLAs [6], [33] adopt diffusionbased action experts to generate continuous actions more efficiently.Despite differences in action generation, most existing VLAs still require large-scale action-data pretraining to adapt to embodied settings, which is inefficient and still insufficient for fine-grained spatial understanding.</p>
<p>B. VLAs with Spatial Awareness</p>
<p>Prior studies have shown that even state-of-the-art VLMs are insensitive to object shapes and fine geometry [7], [8], limiting their utility for precise manipulation.To enhance spatial perception, early efforts augmented VLAs with additional 3D inputs from sensors such as LiDAR or RGB-D cameras [10], [34], [35], but this reduced generalizability across platforms.SpatialVLA [9] proposes using an off-theshelf depth estimator to generate pseudo point clouds as input.However, this approach is essentially a workaround, as the depth estimator is not optimized end-to-end with the VLA, limiting its performance upper bound.</p>
<p>More recent approaches incorporate generative world models that predict future frames, keypoints, or semantic states, and then condition action generation on these predictions [11]- [15].While this improves planning by simulating futures, it does little to improve the encoding of the current scene.A concurrent line of work [17], inspired by methods in VLMs [36], uses Chain-of-Thought (CoT) reasoning to autoregressively generate depth tokens.However, this strategy introduces high latency (over 2 seconds on modern GPUs), as hundreds of tokens must be auto-regressively generated before action prediction.</p>
<p>C. 3D Geometry Perception</p>
<p>Recent advances in 3D perception [18]- [20], [37], [38] have demonstrated strong ability to infer geometry from monocular or multi-view images.By scaling both 3D datasets and model capacity, these vision foundation models achieve robust spatial estimation and support downstream applications such as SLAM [39], [40] and reconstruction [41], [42].Their progress highlights the potential of integrating powerful 3D priors into VLAs for improved spatial reasoning without requiring additional sensors.</p>
<p>III. METHOD</p>
<p>In this section, we describe DepthVLA, its components, and the training framework.</p>
<p>Block-wise Attention</p>
<p>Fig. 2: The proposed mixture-of-transformers (MoT) framework integrates three experts: a vision-language model (VLM) for semantic and language understanding, a depth expert for geometric reasoning, and an action expert for continuous control.Attention layers are shared across experts, while block-wise masking ensures pretrained modules retain their learned abilities.</p>
<p>The action expert attends to features from both the VLM and depth expert at every layer to generate actions conditioned on language, visual, and spatial cues.</p>
<p>A. Problem Formulation and Model Overview</p>
<p>We follow the standard end-to-end VLA setting, where a policy π θ predicts a k-length action chunk A t = a t:t+k given the current observation o t (from one or multiple cameras), a language instruction l, and proprioceptive states s t :,
A t = π θ (o t , l, s t ) .
DepthVLA adopts a mixture-of-transformers (MoT) architecture that integrates three experts: a VLM, a depth module, and a flow-matching action expert, as illustrated in Figure 2.This design extends π 0 [3], which uses a two-expert MoT (VLM + action expert), by adding an independent depth expert to provide explicit spatial information.</p>
<p>Specifically, the VLM expert encodes o t and l to capture semantic and language-grounded features, while the depth expert processes o t to infer geometric information.The action expert then generates continuous actions conditioned on the combined features from both semantic and geometric experts.All three experts share the same attention layers but maintain distinct weights and feature dimensions.</p>
<p>To preserve the pretrained capabilities of the VLM and depth modules, we apply a block-wise mask: tokens from the VLM and depth experts attend only to themselves, while action tokens can attend to all streams, as shown in right side of Figure 2.This design allows DepthVLA to leverage pretrained knowledge while fusing semantic and spatial cues for precise action generation.</p>
<p>B. Depth Expert</p>
<p>The depth expert serves as a dedicated spatial reasoner, providing geometric cues to guide the action expert.To integrate seamlessly into the VLA, it adopts the same transformer backbone as the VLM, with separate weights and dimensions.</p>
<p>We design the depth expert as an encoder-decoder architecture.The encoder is based on DINOv2 [43], which captures fine-grained geometric features.We initialize from the pretrained checkpoint of Depth Anything V2 [19] to inherit strong spatial priors from large-scale 3D foundation models.The decoder mirrors the transformer structure of the VLM and outputs depth predictions through a linear head.Unlike approaches that only provide a final depth map [15], [17], we design the depth expert to perform spatial reasoning across all intermediate layers, which provides richer geometric cues for action prediction.The action expert attends to these intermediate features, leveraging rich geometric representations rather than low-dimensional depth outputs.This improves fine-grained spatial understanding, essential for tasks like precise grasping and collision avoidance.</p>
<p>Before integration to VLA, the depth expert is pretrained on diverse 3D datasets using a monocular depth prediction task to acquire robust spatial reasoning ability.We adopt the scale-invariant log loss [44]:
L si ( d, d) = 1 n ∑ i y 2 − λ 1 n ∑ i y 2 ,
where y = log d − log d.</p>
<p>Here d is the ground-truth metric depth, d is the predicted depth map, and λ controls the balance of the scale term (set to 0.5 by default).This simple loss suffices for learning robust spatial reasoning and distance estimation.We train DepthVLA on embodied action data with an imitation learning objective, maximizing the log-likelihood of actions:
max θ E p(A t ,o t ,l,s t ) [log π θ (A t | o t , l, s t )]
To better model continuous and diverse action trajectories, we adopt a flow-matching loss:
L flow (θ ) = E p(A τ t |o t ,l,s t ) ∥v θ (A τ t , τ, o t , l, s t ) − u(A τ t | A t )∥ 2
Here, subscripts denote robot timesteps and superscripts denote flow matching timesteps, with τ
∈ [0, 1]. A τ t is the interpolated noisy action A τ t = τA t + (1 − τ)ε. v θ (•)
is the flow predicted by the model and u(•) is the target flow derived from the action trajectory.</p>
<p>To maintain the depth expert's spatial reasoning, we retain the depth prediction loss during the VLA training.The final loss is therefore:
L = L si + L flow .
This approach allows DepthVLA to jointly optimize spatial reasoning and action generation in an end-to-end manner.</p>
<p>IV. EXPERIMENTS</p>
<p>A. Implementation Details</p>
<p>Model Architecture.We implement all models in PyTorch.We use Paligemma-3B [29] as the VLM backbone, following prior VLA works [3], [9], [26] due to its strong generalization ability.The depth expert employs DINOv2-L as the encoder, initialized from Depth Anything V2 [19], while its decoder is matched in size to the action expert, with both modules containing approximately 300M parameters.As our closest baseline, we re-implement π 0 by strictly following the official JAX implementation.The only difference between our reimplemented π 0 and DepthVLA is the addition of the depth expert, allowing a fair comparison of the impact of explicit spatial reasoning.Training Details.The depth expert is pretrained on largescale 3D datasets, including WildRGB-D [22], Scannet [23], Scannet++ [24] and HyperSim [21].Pretraining runs for 50k steps using a cosine learning rate schedule, with batch size 1024 and initial learning rate 5×10 −5 .For VLA training, we use a batch size of 1024 for large-scale datasets (e.g., Galaxea Open-World [26], BridgeData V2 [31]) and 64 for smallerscale datasets (e.g., LIBERO [27], real-world benchmark tasks).For all models, we do not use any historical information for action generation.All models are trained on 32 NVIDIA H100 GPUs with using the AdamW optimizer [46] with learning rate 2.5 × 10 −5 and weight decay 10 −4 .Inference Details.DepthVLA introduces 600M additional parameters compared with the baseline π 0 (300M from the DINOv2 encoder and 300M from the depth expert decoder).We run inference on an NVIDIA 4090 GPU with BF16 mixed precision.DepthVLA requires 8.0 GB of VRAM (vs.6.7 GB for π 0 ) and has an inference latency of 210 ms per step (vs.190 ms for π 0 ).Since actions are predicted in 1second chunks (16 steps on a 15 Hz platform), the extra latency is negligible in practice.</p>
<p>B. Simulation Benchmarking</p>
<p>BridgeV2 &amp; Simpler.BridgeData V2 [31] is a largescale real-world robot manipulation dataset, containing over 60k trajectories collected across 24 environments using the WidowX robot.It provides diverse tasks and environment variations, making it a strong foundation for training generalist policies.To obtain depth supervision, we generate pseudo-labels using Depth Anything V2 [19] and UniDepth V2 [33].</p>
<p>Simpler WidowX [28] is a simulation environment designed to closely mirror BridgeData V2, providing a reproducible platform for policy evaluation.It includes four task suites with variations in environment, object configurations, and camera poses, effectively bridging the gap between real and simulated domains.We train DepthVLA on BridgeData V2 for 20k steps (approx.12 epochs) and evaluate it zeroshot on Simpler WidowX.We report final success rate of each task suite, tested with 120 trials under different random seeds.</p>
<p>Results are shown in Table I.The "Pretrained" column indicates whether a model was pretrained on additional robot action data.DepthVLA achieves the highest average success rate on Simpler WidowX.Compared with the counterpart without a depth expert (i.e., π 0 re-implemented), DepthVLA yields substantial gains on tasks such as block stacking and eggplant picking, which demand accurate spatial reasoning and collision avoidance.Remarkably, the depth expert improves 3D perception even when models are trained on real-world data but evaluated in simulation.Furthermore, DepthVLA outperforms SpatialVLA [9], a spatial-aware VLA that leverages an external depth estimator, by a wide margin, highlighting the effectiveness of our mixture-oftransformers design.LIBERO.LIBERO [27] is a simulated manipulation benchmark based on the Franka Panda arm, with demonstrations that include front-view and wrist-view camera images along with natural language instructions.It comprises four task suites: LIBERO-Spatial/-Object/-Goal/-Long, each containing 500 demonstrations across 10 tasks.Unlike prior works [1], [3], [15], [17], which typically train one model per suite, we train a single DepthVLA model jointly on all four suites for 30k steps (about 8 epochs).This creates a more challenging setting that requires stronger generalization across diverse task types.Success rates are reported per task suite, in total 2000 trials across 40 tasks with different random seeds.</p>
<p>Results are shown in Table II.The "Pretrained" column marks whether the model is pretrained on additional robot action datasets.DepthVLA achieves the highest average success rate, even surpassing all models with pretraining.This suggests that standard VLAs, even with large-scale action pretraining, still lack sufficient 3D grounding for precise manipulation.Moreover, DepthVLA surpasses both spatially enhanced baselines (e.g., MolmoACT [17], SpatialVLA [9]) and world-model-based approaches (e.g., DreamVLA [15], CoTVLA [11]), underscoring the strength of our depth expert design.We evaluate DepthVLA on the Galaxea R1 Lite, a commercially available dual-arm mobile platform.The system consists of two 6-DoF arms, two wrist-mounted cameras, and a head camera, as shown in Figure 3.To assess the benefits of large-scale action pretraining on DepthVLA, we pretrain DepthVLA on the large-scale Galaxea Open-World Dataset [26], which contains 100k trajectories across 150 task categories and 50 real-world scenes.Depth labels are generated using VGGT [20] and UniDepth V2 [33].Pretraining runs for 80k steps (about 4 epochs) for both DepthVLA and the re-implemented π 0 .</p>
<p>C. Real-World Benchmarking</p>
<p>To evaluate spatial perception, fine-grained grasping, and collision avoidance, we design three benchmark tasks: Table bussing: The robot organizes a cluttered desk by placing pens into a holder, hanging headphones, and moving a book onto a stand.This task measures small-object grasping and accurate position estimation.Microwave operation: The robot opens a microwave door, places food on a plate, puts the plate inside, and closes the door.This task tests collision avoidance at each step.Blocks stacking: The robot stacks blocks vertically, testing precise pick-and-place skills.</p>
<p>For each benchmark, we collect 100 trajectories and finetune the pretrained model for 4k steps.Performance is evaluated using progress scores, where each successful substep in a task contributes one point, and scores are averaged over 20 runs per task.Additionally, we also conduct few-shot experiments with only 20 fine-tuning trajectories to assess DepthVLA's few-shot transferring ability.</p>
<p>Results are shown in Figure 4. DepthVLA consistently outperforms the baseline, achieving an average progress score of 79% vs. 65% in the standard fine-tuning setting, and 63% vs. 45% in the few-shot setting.On microwave operation, it demonstrates improved collision avoidance when handling the door and plate.On block stacking, DepthVLA exhibits superior spatial perception, even with limited finetuning data, whereas the baseline struggles.On table bussing, DepthVLA performs comparably to the baseline, suggesting that both models handle relatively simple smallobject grasping tasks effectively.Importantly, DepthVLA maintains language-following capabilities, indicating that the action expert effectively integrates the strengths of both the VLM and depth expert.</p>
<p>D. Ablation Studies</p>
<p>We conduct ablation studies to evaluate the design choices of the depth expert.Specifically, we investigate: (i) Is pretraining the depth expert necessary?(ii) Is the depth loss necessary during VLA training?(iii) What happens if the depth expert is frozen during VLA training?(iv) Is the blockwise mask between VLM and depth tokens necessary?(v) Does predicting depth outperform directly inputting groundtruth depth?</p>
<p>We test these questions under the following settings: (i) Depth expert randomly initialized without pretraining.Note that (ii) and (iii) differ, as the depth expert still receives gradients from the flow-matching loss in (ii).Settings (i)-(iv) are evaluated on BridgeData V2 &amp; Simpler, while (v) is evaluated on LIBERO, which provides ground-truth depth maps during inference.Results are summarized in Table III and Table IV.Each component proves essential for DepthVLA's effectiveness.Notably, the performance is not greatly impacted when freezing the depth expert, which means the depth expert learned robust and universal spatial representation.It allows DepthVLA to be easily deployed by fine-tuning on demonstrations without the need of depth ground-truth.Another interesting finding is that, the model performs better when predicting depth than when consuming ground-truth depth directly.We hypothesize this is due to modality competence [47], [48], where one modality can dominate others when jointly provided.By learning to predict depth internally, DepthVLA avoids over-reliance on external signals and instead integrates geometric reasoning more effectively into the shared representation space.</p>
<p>E. Visualization of Depth Prediction</p>
<p>While DepthVLA primarily leverages intermediate features from the depth expert rather than its final outputs, we visualize predicted depth maps to better illustrate the model's spatial perception capabilities.</p>
<p>As shown in Figure 5, the predicted depth captures detailed 3D structure, including object boundaries, distances, and occlusions, which are critical for precise manipulation.Notably, in cluttered environments such as the microwave operation, DepthVLA accurately estimates the relative positions of objects, supporting reliable grasping and collision avoidance.</p>
<p>Real Robot Microwave Operation Real Robot Table Bussing Simpler Block Stacking</p>
<p>LIBERO-Long Fig. 5: Qualitative results of DepthVLA's predicted depth maps across real-world and simulated environments.The predicted depth provides fine-grained geometric cues that guide accurate manipulation, collision avoidance, and precise object grasping.Even in cluttered or challenging scenes, DepthVLA captures the 3D layout robustly, highlighting the effectiveness of the pretrained depth expert in providing spatial awareness.</p>
<p>Similarly, in Simpler block stacking and LIBERO-long tasks, the depth predictions provide the action expert with finegrained geometric cues that improve object alignment and positioning accuracy.These visualizations demonstrate that the depth expert effectively extracts 3D spatial information from monocular RGB input.This depth-aware representation complements the semantic grounding provided by the VLM and underpins the performance improvements observed across real and simulated environments.</p>
<p>V. CONCLUSION</p>
<p>We introduced DepthVLA, a VLA model that enhances spatial reasoning by integrating a pretrained depth expert with a VLM and action expert in a unified mixture-oftransformers framework.Experiments in both real-world and simulated environments show that DepthVLA improves performance on tasks requiring precise manipulation, collision avoidance, and fine-grained grasping, while preserving strong language-following capabilities.Ablations confirm the critical role of depth pretraining, depth loss, and attention design in achieving robust 3D perception.</p>
<p>Despite these improvements, DepthVLA has limitations: monocular depth prediction remains an ill-posed and challenging problem.Even when trained on diverse 3D datasets, the depth expert can struggle in difficult scenarios, such as tiny edges, reflective or transparent objects, or texture-less surfaces, which can impact action generation.Future work could explore multi-view depth or pointmap prediction [20], [37], [38] to further enhance spatial accuracy and robustness.</p>
<p>Fig. 3 :
3
Fig. 3: Real-robot experiment platform.</p>
<p>(ii) Depth loss removed during VLA training.(iii) Depth expert frozen during VLA training.(iv) Depth and VLM tokens allowed to attend to each other.(v) Depth expert taking ground-truth depth as input.</p>
<p>Fig. 4 :
4
Fig.4: Performance of DepthVLA and baseline on three bimanual tasks with standard fine-tuning and few-shot adaptation.DepthVLA shows improvements in tasks requiring precise spatial reasoning and collision avoidance while maintaining comparable performance on simpler small-object manipulation.</p>
<p>TABLE I :
I
Success rates on the Simpler WidowX benchmark.Models are trained on BridgeData V2 and evaluated zeroshot in simulation.The "Pretrained" column indicates whether the model is pretrained with additional robot action data.DepthVLA achieves the highest average performance.
ModelPretrained Put SpoonPut CarrotStack Block Pick Eggplant AverageDiffusion Policy [45]×4.2%0%0%0%1.0%Octo-Base [4]✓12.5%8.3%0%43.1%16.0%SpatialVLA [9]✓16.7%25.0%29.2%100.0%34.4%π 0 (re-implemented) [3]×81.7%64.2%30.0%59.2%58.8%DepthVLA (Ours)×75.8%71.7%62.5%89.2%74.8%C. DepthVLA Policy Training</p>
<p>TABLE II :
II
Success rates on the LIBERO benchmark across four task suites.The "Pretrained" column indicates whether the model is pretrained with additional robot action data.DepthVLA outperforms all baselines, showing that explicit depth reasoning improves generalization across diverse manipulation tasks.
ModelPretrained Spatial ObjectGoalLongAverageOcto-Base [4]✓78.9%85.7%84.6% 51.1%75.1%OpenVLA [1]✓84.7%88.4%79.2% 53.7%76.5%SpatialVLA [9]✓88.2%89.9%78.6% 55.5%78.1%CoT-VLA [11]✓81.5%91.6%87.6% 69.0%83.9%MolmoACT [17]✓87.0%95.4%87.6% 77.2%86.6%DreamVLA [15]✓97.5%94.0%89.5% 89.5%92.6%π 0 (re-implemented) [3]×95.8%96.4%94.8% 87.4%93.6%π 0 (reported) [3] *✓96.8%98.8%95.8% 85.2%94.2%DepthVLA (Ours)×96.4%98.0%95.8% 89.2%94.9%
* Reported in π 0 official JAX implementation.</p>
<p>TABLE III :
III
Ablation studies on different design of the depth expert.
ModelSpoonCarrotBlockEggplantAverage(i)60.0% 60.8% 43.3%40.0%51.0%(ii)69.2%60%28.3%70.0%56.9%(iii)65.8% 69.2% 74.2%78.3%71.9%(iv)66.7% 65.0%2.5%88.3%55.6%DepthVLA75.8% 71.7% 62.5%89.2%74.8%</p>
<p>TABLE IV :
IV
Comparison between predicted and ground-truth depth inputs.Predicting depth yields stronger performance.
ModelSpatialObjectGoalLongAverage(v)94.0%97.6%95.0% 86.4%93.3%DepthVLA96.4%98.0%95.8% 89.2%94.9%</p>
<p>Openvla: An open-source vision-language-action model. M Kim, K Pertsch, S Karamcheti, T Xiao, A Balakrishna, S Nair, R Rafailov, E Foster, G Lam, P Sanketi, Q Vuong, T Kollar, B Burchfiel, R Tedrake, D Sadigh, S Levine, P Liang, C Finn, arXiv:2406.092462024arXiv preprint</p>
<p>Rt-2: Vision-language-action models transfer web knowledge to robotic control. A Brohan, N Brown, J Carbajal, Y Chebotar, X Chen, K Choromanski, T Ding, D Driess, A Dubey, C Finn, P Florence, C Fu, M G Arenas, K Gopalakrishnan, K Han, K Hausman, A Herzog, J Hsu, B Ichter, A Irpan, N Joshi, R Julian, D Kalashnikov, Y Kuang, I Leal, L Lee, T.-W E Lee, S Levine, Y Lu, H Michalewski, I Mordatch, K Pertsch, K Rao, K Reymann, M Ryoo, G Salazar, P Sanketi, P Sermanet, J Singh, A Singh, R Soricut, H Tran, V Vanhoucke, Q Vuong, A Wahid, S Welker, P Wohlhart, J Wu, F Xia, T Xiao, P Xu, S Xu, T Yu, B Zitkovich, arXiv:2307.158182023in arXiv preprint</p>
<p>π 0 : A visionlanguage-action flow model for general robot control. K Black, N Brown, D Driess, A Esmail, M Equi, C Finn, N Fusai, L Groom, K Hausman, B Ichter, arXiv:2410.241642024arXiv preprint</p>
<p>Octo: An open-source generalist robot policy. Octo Model Team, D Ghosh, H Walke, K Pertsch, K Black, O Mees, S Dasari, J Hejna, C Xu, J Luo, T Kreiman, Y Tan, L Y Chen, P Sanketi, Q Vuong, T Xiao, D Sadigh, C Finn, S Levine, Proceedings of Robotics: Science and Systems. Robotics: Science and SystemsDelft, Netherlands2024</p>
<p>Cogact: A foundational visionlanguage-action model for synergizing cognition and action in robotic manipulation. Q Li, Y Liang, Z Wang, L Luo, X Chen, M Liao, F Wei, Y Deng, S Xu, Y Zhang, arXiv:2411.196502024arXiv preprint</p>
<p>C Cheang, S Chen, Z Cui, Y Hu, L Huang, T Kong, H Li, Y Li, Y Liu, X Ma, H Niu, W Ou, W Peng, Z Ren, H Shi, J Tian, H Wu, X Xiao, Y Xiao, J Xu, Y Yang, Gr-3 technical report. 2025</p>
<p>Eyes wide shut? exploring the visual shortcomings of multimodal llms. S Tong, Z Liu, Y Zhai, Y Ma, Y Lecun, S Xie, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)June 2024</p>
<p>Vision language models are blind. P Rahmanzadehgervi, L Bolton, M R Taesiri, A T Nguyen, Proceedings of the Asian Conference on Computer Vision (ACCV). the Asian Conference on Computer Vision (ACCV)December 2024</p>
<p>D Qu, H Song, Q Chen, Y Yao, X Ye, Y Ding, Z Wang, J Gu, B Zhao, D Wang, arXiv:2501.15830Spatialvla: Exploring spatial representations for visual-language-action model. 2025arXiv preprint</p>
<p>Pointvla: Injecting the 3d world into vision-language-action models. C Li, J Wen, Y Peng, Y Peng, F Feng, Y Zhu, 2025</p>
<p>Cot-vla: Visual chain-of-thought reasoning for visionlanguage-action models. Q Zhao, Y Lu, M J Kim, Z Fu, Z Zhang, Y Wu, Z Li, Q Ma, S Han, C Finn, A Handa, T.-Y Lin, G Wetzstein, M.-Y Liu, D Xiang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)June 2025</p>
<p>H Zhen, X Qiu, P Chen, J Yang, X Yan, Y Du, Y Hong, C Gan, arXiv:2403.096313d-vla: 3d vision-language-action generative world model. 2024arXiv preprint</p>
<p>Up-vla: A unified understanding and prediction model for embodied agent. J Zhang, Y Guo, Y Hu, X Chen, X Zhu, J Chen, arXiv:2501.188672025arXiv preprint</p>
<p>Unified world models: Coupling video and action diffusion for pretraining on large robotic datasets. C Zhu, R Yu, S Feng, B Burchfiel, P Shah, A Gupta, 2025</p>
<p>Dreamvla: A vision-language-action model dreamed with comprehensive world knowledge. W Zhang, H Liu, Z Qi, Y Wang, X Yu, J Zhang, R Dong, J He, H Wang, Z Zhang, L Yi, W Zeng, X Jin, 10.48550/arXiv.2507.04447abs/2507.04447CoRR. 2025</p>
<p>Predictive inverse dynamics models are scalable learners for robotic manipulation. Y Tian, S Yang, J Zeng, P Wang, D Lin, H Dong, J Pang, arXiv:2412.151092024arXiv preprint</p>
<p>Molmoact: Action reasoning models that can reason in space. J Lee, J Duan, H Fang, Y Deng, S Liu, B Li, B Fang, J Zhang, Y R Wang, S Lee, W Han, W Pumacay, A Wu, R Hendrix, K Farley, E Vanderbilt, A Farhadi, D Fox, R Krishna, 2025</p>
<p>Depth anything: Unleashing the power of large-scale unlabeled data. L Yang, B Kang, Z Huang, X Xu, J Feng, H Zhao, CVPR. 2024</p>
<p>Depth anything v2. L Yang, B Kang, Z Huang, Z Zhao, X Xu, J Feng, H Zhao, arXiv:2406.094142024</p>
<p>Vggt: Visual geometry grounded transformer. J Wang, M Chen, N Karaev, A Vedaldi, C Rupprecht, D Novotny, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2025</p>
<p>Hypersim: A photorealistic synthetic dataset for holistic indoor scene understanding. M Roberts, J Ramapuram, A Ranjan, A Kumar, M A Bautista, N Paczan, R Webb, J M Susskind, International Conference on Computer Vision (ICCV). 20212021</p>
<p>Rgbd objects in the wild: Scaling real-world 3d object learning from rgb-d videos. H Xia, Y Fu, S Liu, X Wang, 2024</p>
<p>Scannet: Richly-annotated 3d reconstructions of indoor scenes. A Dai, A X Chang, M Savva, M Halber, T Funkhouser, M Nießner, Proc. Computer Vision and Pattern Recognition (CVPR). Computer Vision and Pattern Recognition (CVPR)IEEE2017</p>
<p>Scannet++: A highfidelity dataset of 3d indoor scenes. C Yeshwanth, Y.-C Liu, M Nießner, A Dai, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023</p>
<p>Mixture-of-transformers: A sparse and scalable architecture for multi-modal foundation models. W Liang, L Yu, L Luo, S Iyer, N Dong, C Zhou, G Ghosh, M Lewis, W Yih, L Zettlemoyer, X V Lin, Transactions on Machine Learning Research. 2025</p>
<p>Galaxea open-world dataset and g0 dual-system vla model. T Jiang, T Yuan, Y Liu, C Lu, J Cui, X Liu, S Cheng, J Gao, H Xu, H Zhao, 2025</p>
<p>Libero: Benchmarking knowledge transfer for lifelong robot learning. B Liu, Y Zhu, C Gao, Y Feng, Q Liu, Y Zhu, P Stone, arXiv:2306.033102023arXiv preprint</p>
<p>Evaluating real-world robot manipulation policies in simulation. X Li, K Hsu, J Gu, K Pertsch, O Mees, H R Walke, C Fu, I Lunawat, I Sieh, S Kirmani, S Levine, J Wu, C Finn, H Su, Q Vuong, T Xiao, arXiv:2405.059412024arXiv preprint</p>
<p>L Beyer, A Steiner, A S Pinto, A Kolesnikov, X Wang, D Salz, M Neumann, I Alabdulmohsin, M Tschannen, E Bugliarello, T Unterthiner, D Keysers, S Koppula, F Liu, A Grycner, A Gritsenko, N Houlsby, M Kumar, K Rong, J Eisenschlos, R Kabra, M Bauer, M Bošnjak, X Chen, M Minderer, P Voigtlaender, I Bica, I Balazevic, J Puigcerver, P Papalampidi, O Henaff, X Xiong, R Soricut, J Harmsen, X Zhai, arXiv:2407.07726PaliGemma: A versatile 3B VLM for transfer. 2024arXiv preprint</p>
<p>Prismatic vlms: Investigating the design space of visually-conditioned language models. S Karamcheti, S Nair, A Balakrishna, P Liang, T Kollar, D Sadigh, 2024</p>
<p>Bridgedata v2: A dataset for robot learning at scale. H Walke, K Black, A Lee, M J Kim, M Du, C Zheng, T Zhao, P Hansen-Estruch, Q Vuong, A He, V Myers, K Fang, C Finn, S Levine, Conference on Robot Learning (CoRL). 2023</p>
<p>. A Khazatsky, K Pertsch, S Nair, A Balakrishna, S Dasari, S Karamcheti, S Nasiriany, M K Srirama, L Y Chen, K Ellis, P D Fagan, J Hejna, M Itkina, M Lepert, Y J Ma, P T Miller, J Wu, S Belkhale, S Dass, H Ha, A Jain, A Lee, Y Lee, M Memmel, S Park, I Radosavovic, K Wang, A Zhan, K Black, C Chi, K B Hatch, S Lin, J Lu, J Mercat, A Rehman, P R Sanketi, A Sharma, C Simpson, Q Vuong, H R Walke, B Wulfe, T Xiao, J H Yang, A Yavary, T Z Zhao, C Agia, R Baijal, M G Castro, D Chen, Q Chen, T Chung, J Drake, E P Foster, J Gao, V Guizilini, D A Herrera, M Heo, K Hsu, J Hu, M Z Irshad, D Jackson, C Le, Y Li, K Lin, R Lin, Z Ma, A Maddukuri, S Mirchandani, D Morton, T Nguyen, A O'neill, R Scalise, D Seale, V Son, S Tian, E Tran, A E Wang, Y Wu, A Xie, J Yang, P Yin, Y Zhang, O Bastani, G Berseth, J Bohg, K Goldberg, A Gupta, A Gupta, D Jayaraman, J J Lim, J Malik, R Martín-Martín, S Ramamoorthy, D Sadigh, S Song, J Wu, M C Yip, Y Zhu, T Kollar, S Levine, C Finn, Droid: A large-scale in-the-wild robot manipulation dataset," 2025. [Online</p>
<p>UniDepthV2: Universal monocular metric depth estimation made simpler. L Piccinelli, C Sakaridis, Y.-H Yang, M Segu, S Li, W Abbeloos, L V Gool, 2025</p>
<p>Bridgevla: Input-output alignment for efficient 3d manipulation learning with vision-language models. P Li, Y Chen, H Wu, X Ma, X Wu, Y Huang, L Wang, T Kong, T Tan, 2025</p>
<p>Lift3d foundation policy: Lifting 2d large-scale pretrained models for robust 3d robotic manipulation. Y Jia, J Liu, S Chen, C Gu, Z Wang, L Luo, L Lee, P Wang, Z Wang, R Zhang, S Zhang, 2024</p>
<p>Perception tokens enhance visual reasoning in multimodal language models. M Bigverdi, Z Luo, C.-Y Hsieh, E Shen, D Chen, L G Shapiro, R Krishna, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)June 2025</p>
<p>Grounding image matching in 3d with mast3r. V Leroy, Y Cabon, J Revaud, 2024</p>
<p>Dust3r: Geometric 3d vision made easy. S Wang, V Leroy, Y Cabon, B Chidlovskii, J Revaud, CVPR. 2024</p>
<p>Monst3r: A simple approach for estimating geometry in the presence of motion. J Zhang, C Herrmann, J Hur, V Jampani, T Darrell, F Cole, D Sun, M.-H Yang, arxiv:2410.038252024arXiv preprint</p>
<p>R Murai, E Dexheimer, A J Davison, MASt3R-SLAM: Realtime dense SLAM with 3D reconstruction priors. 2024arXiv preprint</p>
<p>3d reconstruction with spatial memory. H Wang, L Agapito, arXiv:2408.160612024arXiv preprint</p>
<p>Long3r: Long sequence streaming 3d reconstruction. Z Chen, M Qin, T Yuan, Z Liu, H Zhao, arXiv:2507.182552025arXiv preprint</p>
<p>Dinov2: Learning robust visual features without supervision. M Oquab, T Darcet, T Moutakanni, H V Vo, M Szafraniec, V Khalidov, P Fernandez, D Haziza, F Massa, A El-Nouby, R Howes, P.-Y Huang, H Xu, V Sharma, S.-W Li, W Galuba, M Rabbat, M Assran, N Ballas, G Synnaeve, I Misra, H Jegou, J Mairal, P Labatut, A Joulin, P Bojanowski, 2023</p>
<p>Depth map prediction from a single image using a multi-scale deep network. D Eigen, C Puhrsch, R Fergus, 2014</p>
<p>Diffusion policy: Visuomotor policy learning via action diffusion. C Chi, Z Xu, S Feng, E Cousineau, Y Du, B Burchfiel, R Tedrake, S Song, The International Journal of Robotics Research. 2024</p>
<p>Decoupled weight decay regularization. I Loshchilov, F Hutter, 2019</p>
<p>On uni-modal feature learning in supervised multi-modal learning. C Du, J Teng, T Li, Y Liu, T Yuan, Y Wang, Y Yuan, H Zhao, 2023</p>
<p>Multimodal representation learning by alternating unimodal adaptation. X Zhang, J Yoon, M Bansal, H Yao, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)June 2024466</p>            </div>
        </div>

    </div>
</body>
</html>
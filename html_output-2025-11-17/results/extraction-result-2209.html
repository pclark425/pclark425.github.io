<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2209 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2209</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2209</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-58.html">extraction-schema-58</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of validation approaches in automated or AI-driven scientific research systems, including experimental validation, computational validation, simulation-based validation, and comparisons between these approaches.</div>
                <p><strong>Paper ID:</strong> paper-281681196</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2509.26603v1.pdf" target="_blank">DeepScientist: Advancing Frontier-Pushing Scientific Findings Progressively</a></p>
                <p><strong>Paper Abstract:</strong> While previous AI Scientist systems can generate novel findings, they often lack the focus to produce scientifically valuable contributions that address pressing human-defined challenges. We introduce DeepScientist, a system designed to overcome this by conducting goal-oriented, fully autonomous scientific discovery over month-long timelines. It formalizes discovery as a Bayesian Optimization problem, operationalized through a hierarchical evaluation process consisting of"hypothesize, verify, and analyze". Leveraging a cumulative Findings Memory, this loop intelligently balances the exploration of novel hypotheses with exploitation, selectively promoting the most promising findings to higher-fidelity levels of validation. Consuming over 20,000 GPU hours, the system generated about 5,000 unique scientific ideas and experimentally validated approximately 1100 of them, ultimately surpassing human-designed state-of-the-art (SOTA) methods on three frontier AI tasks by 183.7\%, 1.9\%, and 7.9\%. This work provides the first large-scale evidence of an AI achieving discoveries that progressively surpass human SOTA on scientific tasks, producing valuable findings that genuinely push the frontier of scientific discovery. To facilitate further research into this process, we will open-source all experimental logs and system code at https://github.com/ResearAI/DeepScientist/.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2209.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2209.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of validation approaches in automated or AI-driven scientific research systems, including experimental validation, computational validation, simulation-based validation, and comparisons between these approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepScientist</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepScientist (autonomous discovery system)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An end-to-end LLM-based autonomous scientific discovery system that models discovery as a Bayesian optimization problem and executes a hierarchical three-stage validation pipeline (Strategize & Hypothesize → Implement & Verify → Analyze & Report) to produce SOTA-surpassing computational research results.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>DeepScientist</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>machine learning / AI research</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>computational validation</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Multi-fidelity, hierarchical computational validation pipeline: (1) low-cost surrogate evaluation of generated hypotheses using an LLM-based 'Surrogate Model' (LLM Reviewer) that scores candidates with a valuation vector V = ⟨v_u, v_q, v_e⟩ (0–100) after retrieval from a Findings Memory (K=15); (2) selection with an acquisition function (UCB) mapping V into a combined exploitation/exploration score (weights w_u=1, w_q=1, κ=1) to promote candidates to the Implement & Verify stage where coding agents (Claude-4-opus) run sandboxed computational experiments on GPU servers and produce empirical metrics on domain benchmarks (Who&When, MBPP, RAID); (3) successful experiments are promoted to Analyze & Report where MCP tools run deeper analytical experiments (ablations, evaluations on new datasets) and synthesize reproducible papers. All experimental results were manually inspected by human supervisors and reviewed by a human program committee and/or DeepReviewer.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td>Not a physics/chemistry simulation; fidelity is algorithmic and empirical. The surrogate is a low-fidelity LLM approximation of the true value function f(·) (qualitative/value prediction only) and cannot guarantee numeric fidelity; high-fidelity validation consists of end-to-end computational experiments on target benchmarks (exact metrics like AUROC, tokens/sec, accuracy). No physical or physics-based simulations were used.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_simulation_vs_experiment</strong></td>
                            <td>The paper contrasts low-fidelity surrogate predictions to actual computational experiment outcomes: the surrogate filters large idea pools to a subset for real experiments, but many surrogate-selected trials still fail (substantial mismatch). Quantitatively, out of ~5,000 ideas the system 'experimentally validated' ≈1,100 via computational experiments, but only 21 became Progress Findings; selection increased realized success from near-zero (random baseline) to ≈1–3% success. No physical experiment vs simulation comparisons are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_rate</strong></td>
                            <td>Multiple metrics reported: ~5,000 unique ideas generated; ~1,100 selected for computational validation (≈22% of generated ideas); only 21 progressed to SOTA-surpassing 'Progress Findings' (≈1.9% of validated experiments, ≈0.4% of generated ideas). Paper also reports task-level implemented-to-progress success ~1–3%.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_standards</strong></td>
                            <td>Uses established community benchmarks (Who&When for failure attribution, MBPP for inference acceleration, RAID for AI text detection) and human peer review (program committee of LLM researchers) plus automated DeepReviewer ratings. Paper emphasizes standard practices: ablations, evaluations on new datasets, comparisons to essential baselines and reproducibility; reviewers criticized many generated papers for lacking comprehensive ablations and baseline comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>when_simulation_sufficient</strong></td>
                            <td>Paper argues computational experimentation (fast feedback loops) is sufficient for many ML/AI frontier tasks with rapid evaluation (knowledge editing, some aspects of chip design). However, for high-cost domains (foundation model pretraining, pharmaceutical wet-lab work), simulation/computation alone is currently insufficient and human-led or physical experiment validation remains necessary. It suggests building simulated discovery environments could accelerate learning, but does not claim simulation alone suffices for frontier validation.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_failures</strong></td>
                            <td>Many surrogate-selected experiments failed to produce improvements; human analysis of a sample of 300 failed implementations found ~60% were terminated prematurely due to implementation errors (agent timeouts, bugs), and the remaining ~40% produced no improvement or regression. The system observed ~50% of initial Claude Code implementations did not complete due to internal timeouts, producing false positives that required secondary verification (re-execution) to detect.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Surrogate provides discrete valuation vector V (integer 0–100) but no formal probabilistic posterior; acquisition uses UCB-style exploration weight κ. Experimental results reported with point metrics (accuracy, tokens/sec, AUROC) and some reviewer scores include means and variances; Krippendorff's α reported for inter-rater reliability (α = 0.739). No systematic confidence intervals around primary experimental performance metrics are reported in the main text.</td>
                        </tr>
                        <tr>
                            <td><strong>fabrication_detection</strong></td>
                            <td>Paper discusses safeguards against fabricated or unverified outputs: (1) human verification of all experimental results before release; (2) using DeepReviewer (an AI peer-review agent) and a human program committee to assess papers; (3) selective open-sourcing (excluding 'Analyze & Report' module) to prevent automated production of superficially credible but unverified papers; (4) red-teaming safety checks where underlying models refused malicious objectives (malware creation).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Detailed costs/time reported: total computational + API cost ≈ $100,000 and ≈20,000 GPU hours consumed across the project. Per-stage/operation costs: Strategize stage ≈ $5 per idea (API calls); Implement stage ≈ $20 Claude-4-opus API plus ≈1 GPU-hour per attempt (average execution ≈70 minutes ≈1×10^16 FLOPs for H800 FP16). Analyze & Report stage ≈ $150 per successful finding (≈$100 for analytical experiments + $50 for report generation). Paper states naive testing of all 5,000 candidates would require >100,000 GPU hours; targeted selection reduced cost to ~20,000 GPU hours.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Primary limitations: very low success rate (1–3%), high fraction of failures from implementation errors (~60%), surrogate inaccuracies leading to wasted experiments, insufficient experimental design and ablation depth in many generated papers (criticized by human reviewers), high computational cost, and current inapplicability to domains requiring physical experiments. The 'Analyze & Report' module was deliberately not open-sourced due to concerns about fabricated, unverified publications.</td>
                        </tr>
                        <tr>
                            <td><strong>acceptance_credibility</strong></td>
                            <td>Paper argues that robust validation (computational experiments on public benchmarks, human peer review, reproducible code) increases community acceptance; DeepScientist's published methods surpassed human SOTA on benchmarks (improvements reported: +183.7% accuracy on one task, +1.9% tokens/sec, +7.9% AUROC) and achieved a 60% acceptance rate under DeepReviewer evaluations. However, human reviewers repeatedly flagged insufficient empirical rigor (lack of full ablation and baseline comparisons), indicating validation approach affects credibility and acceptance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_gold_standard</strong></td>
                            <td>Compared results to human-designed SOTA baselines on standard benchmarks: DeepScientist-generated methods (e.g., A2P, ACRA, T-Detect/TDT/PA-Detect) outperformed prior SOTA on respective benchmarks with numeric improvements: 183.7% (Accuracy) on one agent task, 1.9% tokens/sec on LLM inference acceleration, 7.9% AUROC on AI text detection. Also compared selection strategy vs random baseline: random selection yielded effectively zero progress, while selection via surrogate + UCB raised success to ≈1–3%.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2209.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2209.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of validation approaches in automated or AI-driven scientific research systems, including experimental validation, computational validation, simulation-based validation, and comparisons between these approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Surrogate (LLM Reviewer)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Surrogate Model (LLM Reviewer) contextualized with Findings Memory</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A low-cost LLM-based surrogate that approximates the latent true value function f(·) for candidate research ideas by producing a valuation vector V = ⟨v_u, v_q, v_e⟩ (utility, quality, exploration) on a 0–100 integer scale after retrieving Top-K Findings from the Findings Memory.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>LLM Reviewer surrogate</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>machine learning / AI research (meta-research)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>low-fidelity simulation</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Operationalized as a contextualized LLM that reads retrieved prior Findings (K=15) and scores newly generated hypotheses with a three-component valuation vector. These scores serve as inputs to an acquisition function (UCB) to select candidates for higher-fidelity computational experiments. The surrogate provides fast, low-cost pre-screening rather than quantitative predictive fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td>Low fidelity: qualitative/value approximation only; not calibrated probabilistically, no ground-truth posterior, and no physics modeled. Accuracy relative to experimental outcomes is limited (surrogate often misranks proposals), as evidenced by many surrogate-selected candidates failing in real experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_simulation_vs_experiment</strong></td>
                            <td>Paper reports surrogate effectively reduces candidate set but has limited predictive power: selection based on surrogate raised success from ~0% (random) to ≈1–3%, but many surrogate-selected trials still failed due to implementation or conceptual errors.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_rate</strong></td>
                            <td>Indirect: surrogate-selected subset (≈1,100 validated experiments) yielded 21 progress findings (≈1.9% success among validated), indicating the surrogate's positive filtering effect but low absolute precision.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_standards</strong></td>
                            <td>The surrogate is intended as a filter in a multi-fidelity pipeline; domain standards require actual empirical experiments on benchmarks to establish claims—surrogate scoring alone is insufficient for acceptance.</td>
                        </tr>
                        <tr>
                            <td><strong>when_simulation_sufficient</strong></td>
                            <td>Paper treats surrogate as a low-cost approximation sufficient for preliminary filtering, but insists on downstream higher-fidelity computational experiments for convincing validation.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_failures</strong></td>
                            <td>Surrogate produced many false positives (ideas judged promising but failing upon implementation). No specific numeric accuracy for surrogate prediction vs outcome is provided beyond aggregate success statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Surrogate outputs are integer scores (0–100) but no calibrated uncertainties or error bars; uncertainty is implicitly handled by an exploration score v_e and UCB acquisition.</td>
                        </tr>
                        <tr>
                            <td><strong>fabrication_detection</strong></td>
                            <td>Not applicable directly; surrogate helps avoid obviously poor/implausible ideas but cannot reliably detect fabricated or hallucinated claims without downstream human verification.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Low per-decision cost: Strategize stage API cost ≈ $5 per idea. Intended to reduce expensive experiment counts by pre-filtering.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Not calibrated; relies on LLM reasoning which can hallucinate and lacks rigorous uncertainty quantification, resulting in many false positives that waste experiment budget.</td>
                        </tr>
                        <tr>
                            <td><strong>acceptance_credibility</strong></td>
                            <td>Accepted as a pragmatic filter in the pipeline, but community acceptance of ideas requires passing higher-fidelity computational experiments and human peer review—surrogate scores alone do not confer credibility.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_gold_standard</strong></td>
                            <td>No explicit comparison to an established statistical surrogate or model-based predictor; compared implicitly against random selection baseline and shown to improve effective discovery rate over naive sampling.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2209.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2209.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of validation approaches in automated or AI-driven scientific research systems, including experimental validation, computational validation, simulation-based validation, and comparisons between these approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Implement & Verify (sandboxed)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Implement & Verify stage (sandboxed code execution and computational experiments using Claude Code agent)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The middle stage of the DeepScientist pipeline: coding agents implement selected hypotheses in sandboxed copies of baseline repositories and run computational experiments (GPU servers) to produce empirical performance measurements.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>Implement & Verify (Claude Code sandboxed execution)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>machine learning / AI research (empirical benchmarking)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>computational validation</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>When a candidate is promoted by UCB, a coding agent (Claude-4-opus) implements code changes inside a duplicated, sandboxed repository folder and runs the target evaluation scripts. Execution logs and numeric results (benchmark metrics on Who&When, MBPP, RAID) are collected. Secondary verification includes an independent re-execution of the main script to guard against false-positive completions; human supervisors manually inspect results to ensure authenticity.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td>N/A (empirical computational experiments on implemented algorithms rather than physics simulations). Fidelity equals the fidelity of the reproduced SOTA codebases and benchmark datasets; depends on correctness of implementation and environment.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_simulation_vs_experiment</strong></td>
                            <td>All validation here is computational experiments; comparisons are made against baseline SOTA implementations on the same benchmarks. Discrepancies often traced to implementation errors (≈60% of failures) rather than conceptual invalidation.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_rate</strong></td>
                            <td>Of the ideas promoted to Implement & Verify, approximately 1,100 experiments were executed; only ~21 gave rise to Progress Findings. The Implement→Progress success rate is ≈1–3% as reported.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_standards</strong></td>
                            <td>Follows replication best-practices: baseline code manually reproduced, execution logs and test scripts preserved, sandboxed directories, and human-inspected outputs. Reviewers demanded more ablation studies and broader benchmark evaluations for stronger claims.</td>
                        </tr>
                        <tr>
                            <td><strong>when_simulation_sufficient</strong></td>
                            <td>Computational experiments are sufficient for claiming improvements on ML benchmarks; for non-ML domains requiring lab work, such computational validation would be insufficient.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_failures</strong></td>
                            <td>High rate of implementation failures: ~50% of initial Claude Code runs failed to complete due to internal timeouts; sample human analysis showed ~60% of trial failures were due to implementation issues. These failures required re-execution and manual intervention.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Reports point performance metrics (e.g., AUROC, tokens/sec), but the paper lacks systematic reporting of statistical uncertainties (CI) for these metrics in most cases; human review scores include reported variances.</td>
                        </tr>
                        <tr>
                            <td><strong>fabrication_detection</strong></td>
                            <td>Secondary re-execution of main script was used to detect false positives from agent reports; human supervisors inspected all experimental results to prevent fabricated/incorrect runs.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Average execution ~70 minutes per implementation (≈1 GPU-hour on H800), per-attempt API cost ~$20 (Claude-4-opus) plus GPU cost; total compute for project ≈20,000 GPU-hours. Naive exhaustive testing would require >100,000 GPU-hours as stated.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Large fraction of wasted compute due to implementation brittleness; agent timeouts and bugs are major failure modes; requires human oversight and sandboxing to ensure reproducibility; lacks automated, standardized test harnesses for many tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>acceptance_credibility</strong></td>
                            <td>Computational experiments executed reproducibly and accompanied by logs and code increase credibility, but reviewers still flagged missing deeper analyses (ablations, baselines) that limit acceptance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_gold_standard</strong></td>
                            <td>Directly compared implemented methods against published human SOTA codebases and bench- mark numbers; multiple DeepScientist methods surpassed the human SOTA metrics reported in the paper (e.g., +7.9% AUROC on AI text detection).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2209.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2209.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of validation approaches in automated or AI-driven scientific research systems, including experimental validation, computational validation, simulation-based validation, and comparisons between these approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Analyze & Report (MCP tools)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Analyze & Report stage (analysis agents using MCP tools and paper synthesis)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Final high-fidelity validation and dissemination stage: when an Implement Finding shows empirical improvement, specialized agents use MCP tools to run deeper analytical experiments (ablations, cross-dataset evaluation) and synthesize reproducible papers from aggregated results.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>Analyze & Report (MCP + synthesis agents)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>machine learning / AI research (experimental analysis and reporting)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>computational validation (high-fidelity) / hybrid (analysis-level)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Triggered only for successful Implement findings. Agents autonomously design and run additional analyses (e.g., ablation studies, evaluation on new datasets) using MCP tools to manage experiments, data collection and parsing. Results are synthesized into a reproducible paper by a synthesis agent. Human supervisors review all outputs before release. The paper reports aggregated performance improvements and human program-committee evaluation of the generated papers.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td>High-fidelity computational experiments and analyses on benchmark datasets; fidelity depends on thoroughness of analytical experiments (varies per finding). No physical experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_simulation_vs_experiment</strong></td>
                            <td>Within the computational domain, Analyze & Report provides deeper cross-checks (e.g., ablations) to validate experimental claims; reviewers still noted that some generated papers lacked sufficient depth in these analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_rate</strong></td>
                            <td>Applied only to a small subset: of the ~1,100 validated Implement findings, 21 were promoted to Analyze & Report and became Progress Findings.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_standards</strong></td>
                            <td>Paper emphasizes ablations, evaluations on new datasets, and reproducibility as domain standards; reviewers demanded stronger analytical experiments in several generated papers.</td>
                        </tr>
                        <tr>
                            <td><strong>when_simulation_sufficient</strong></td>
                            <td>For ML research, carefully designed computational analyses (ablations, cross-dataset evaluations) are treated as sufficient community-standard validation if reproducible and comprehensive; otherwise human reviewers may require additional evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_failures</strong></td>
                            <td>Some Analyze & Report outputs were criticized for insufficient analytic depth; omission of key baselines or ablation details weakened acceptance in reviewer feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Analytical experiments produce point metrics; the paper does not consistently present formal statistical uncertainty quantification (confidence intervals) for all analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>fabrication_detection</strong></td>
                            <td>Human review and withheld open-sourcing of the Analyze & Report module are used as mitigations against automated fabrication of convincing but unverified papers.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>For a successful finding, Analyze & Report cost ≈ $150 (≈$100 analytical experiments + $50 for report generation) on top of implementation costs.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Quality of deeper analyses varies; some generated papers lacked essential ablations and comparisons, limiting their persuasiveness despite positive benchmark numbers. The module is intentionally closed-source to reduce risk of misuse.</td>
                        </tr>
                        <tr>
                            <td><strong>acceptance_credibility</strong></td>
                            <td>Fully-analyzed, reproducible computational studies assembled here are positioned to earn community acceptance, but the paper admits many generated outputs fell short of reviewer expectations for rigor.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_gold_standard</strong></td>
                            <td>Claims validated here are compared against human SOTA and judged by program committee and DeepReviewer. The best Analyze & Report outputs achieved scores similar to human ICLR submissions (average rating ~5.00 compared to ICLR human average 5.08).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2209.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2209.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of validation approaches in automated or AI-driven scientific research systems, including experimental validation, computational validation, simulation-based validation, and comparisons between these approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Findings Memory + UCB</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Findings Memory (historical repository) with UCB acquisition function</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A continuously expanding knowledge base that stores human and system findings and a retrieval-enabled context for the surrogate; candidate promotion is controlled using an Upper Confidence Bound (UCB) acquisition function that combines surrogate utility, quality, and exploration scores.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>Findings Memory + UCB acquisition</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>machine learning / AI research (search/optimization in hypothesis space)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>computational validation (selection/filtering layer)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Findings Memory stores structured records (Idea, Implement, Progress). A retrieval model returns Top-K findings to contextualize the surrogate. The surrogate outputs V = ⟨v_u, v_q, v_e⟩ and UCB is computed as w_u v_u + w_q v_q + κ·v_e with hyperparameters w_u=1, w_q=1, κ=1 and K=15 retrieval. This acquisition filters ideas for costly computational validation, improving efficiency over random sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td>N/A (architectural selection mechanism rather than physics simulation).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_simulation_vs_experiment</strong></td>
                            <td>The selection strategy (Findings Memory + surrogate + UCB) is compared empirically to a baseline without selection: without selection, randomly sampling 100 ideas per task yields essentially zero successes; with selection the success rate rises to ≈1–3%.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_rate</strong></td>
                            <td>Using this selection process, the system promoted ~1,100 ideas to implementation and produced 21 Progress Findings (≈1–3% success among implemented ideas).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_standards</strong></td>
                            <td>Intended to emulate Bayesian optimization for expensive black-box f(·). The paper emphasizes multi-fidelity filtering and evidence-backed promotion as domain-appropriate practice for computational discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>when_simulation_sufficient</strong></td>
                            <td>Effective for computational domains where surrogate-guided filtering can reduce expensive evaluations; less applicable when true validation requires physical experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_failures</strong></td>
                            <td>The acquisition function can promote false positives (ideas that later fail due to implementation or conceptual issues). No formal regret or calibration metrics for the surrogate+UCB combination are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Exploration score v_e acts as proxy for epistemic uncertainty in the surrogate's judgment; no formal Bayesian posterior or calibrated uncertainty is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>fabrication_detection</strong></td>
                            <td>Not directly a fabrication detector; it reduces obviously low-value proposals but cannot prevent imaginative but false claims from being implemented.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Key to cost reductions: paper states that targeted selection enabled obtaining breakthroughs with ~20,000 GPU-hours vs an estimated >100,000 GPU-hours for exhaustive testing of all candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Selection relies on LLM-based surrogate and retrieval context; both are vulnerable to hallucination and misranking. Proper hyperparameter tuning and richer uncertainty modeling are suggested future improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>acceptance_credibility</strong></td>
                            <td>Selection that leads to successful, reproducible experiments increases credibility; however, reviewers still demand depth in analytic validation beyond mere benchmark wins.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2209.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2209.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of validation approaches in automated or AI-driven scientific research systems, including experimental validation, computational validation, simulation-based validation, and comparisons between these approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human oversight & DeepReviewer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human program committee oversight and DeepReviewer automated peer-review agent</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Complementary human and automated review mechanisms used to assess and verify the scientific validity of DeepScientist outputs: a small human program committee of LLM researchers provides in-depth reviews, while DeepReviewer simulates human peer review for automated benchmarking of AI-generated papers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>Human program committee + DeepReviewer</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>machine learning / scientific peer review</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>other (peer-review / human verification)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>All results and generated papers were manually inspected by human supervisors. The authors convened a three-person program committee (two former ICLR reviewers + one Area Chair-level volunteer) to independently review five generated papers using a process emulating ICLR reviewing (soundness, presentation, contribution). DeepReviewer (an AI review agent) was also used to benchmark generated papers against 28 other AI Scientist system papers. Human review identified recurring weaknesses in experimental design and lack of ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_simulation_vs_experiment</strong></td>
                            <td>Peer review complements computational experiments: computational benchmarks demonstrate metric gains while human reviewers assess soundness and sufficiency of validation; reviewers flagged missing experimental rigor despite benchmark improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_rate</strong></td>
                            <td>Not applicable as a numeric success rate; human review produced average paper ratings (DeepScientist average 5.00) and inter-rater reliability Krippendorff's α = 0.739.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_standards</strong></td>
                            <td>Peer review expects comprehensive empirical validation (ablations, baseline comparisons, cross-dataset tests) and reproducibility; the paper treats human review as essential for acceptance of automated discoveries.</td>
                        </tr>
                        <tr>
                            <td><strong>when_simulation_sufficient</strong></td>
                            <td>Human reviewers accept simulation/computational validation in ML if analyses are thorough; otherwise they request additional evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_failures</strong></td>
                            <td>Human committee identified cases where generated papers failed to design comprehensive validation plans; this is treated as a common failure mode limiting acceptance.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Reviewer scores include means and variances for sub-scores; inter-rater reliability reported. Beyond reviewer stats, uncertainty quantification for experiments is inconsistently reported.</td>
                        </tr>
                        <tr>
                            <td><strong>fabrication_detection</strong></td>
                            <td>Human oversight and DeepReviewer are used to detect weak, fabricated, or poorly validated outputs. The Analyze & Report module is withheld from open-source release to avoid automated fabrication of apparently credible papers.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Human review time averaged ~55 minutes per paper in the program committee exercise. Automated DeepReviewer facilitates higher-throughput comparative evaluation but cannot replace nuanced human judgment.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Human review is costly and slow; automated reviewers can scale but may miss subtleties; neither fully solves the problem of ensuring experimental rigor in automated scientific outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>acceptance_credibility</strong></td>
                            <td>Human verification and peer review are emphasized as essential to confer community credibility on AI-generated discoveries; automated reviewers help but are not a full substitute.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_gold_standard</strong></td>
                            <td>Human program-committee evaluation emulates ICLR standards (Soundness, Presentation, Contribution) and is used as the gold-standard acceptance test in the paper's internal evaluation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2209.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2209.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of validation approaches in automated or AI-driven scientific research systems, including experimental validation, computational validation, simulation-based validation, and comparisons between these approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Scaling experiment / cost analysis</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Scaling analysis of computational resources vs discovery output</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical study quantifying how parallel computational resources (number of GPUs) affect the number of SOTA-surpassing Progress Findings produced by DeepScientist, and an analysis of validation costs and time for the pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>Scaling experiment (parallel GPU instances synchronized via shared Findings Memory)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>machine learning / computational experimentation</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>computational validation / experimental scaling</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>One-week experiment where N limitation tasks were assigned to N parallel GPU instances; instances explored independently and synchronized Findings Memory every five cycles. Outcome: discovery rate rises with GPUs; from none at minimal resources to 1 SOTA-surpassing finding at 4 GPUs and 11 findings at 16 GPUs, indicating near-linear relationship. Cost/time metrics reported: total ~20,000 GPU-hours to produce the reported breakthroughs; naive exhaustive validation estimated >100,000 GPU-hours.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td>N/A (empirical compute-scaling study).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_simulation_vs_experiment</strong></td>
                            <td>This is an empirical comparison between levels of computational investment and validated outputs (computational experiments). No physical experiments present. It demonstrates computational-parallelism benefits but also diminishing returns below a resource threshold.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_rate</strong></td>
                            <td>Discovery counts: with 4 GPUs produced ~1 SOTA-surpassing finding in one week; with 16 GPUs produced ~11. Overall project: ~21 progress findings from ~20,000 GPU-hours.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_standards</strong></td>
                            <td>Paper positions this experiment as an empirical benchmark for resource-to-discovery efficiency; domain expectation is that scalable parallel compute can accelerate discovery in fast-feedback computational tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>when_simulation_sufficient</strong></td>
                            <td>Shows that for fast-feedback computational tasks, scale of computational experimentation can substitute for long human timelines; indicates simulation/compute not sufficient for costly physical domains.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_failures</strong></td>
                            <td>No specific simulation failures; notes that computational scaling cannot overcome conceptual or implementation failure modes (which dominate many failures).</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Reports counts of findings vs resources; no formal statistical confidence intervals for scaling curve provided, but near-linear trend is asserted from empirical data.</td>
                        </tr>
                        <tr>
                            <td><strong>fabrication_detection</strong></td>
                            <td>Not directly applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Detailed: per-idea API cost $5 (strategize), per-implementation $20 + ~1 GPU-hour (~70 min), Analyze ~$150 for successful finding. Total project cost ≈$100k and ≈20k GPU-hours. Naive exhaustive approach >100k GPU-hours.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Scaling helps but does not fix low conceptual correctness or implementation brittleness; many resources still wasted on implementation errors and false positives. Paper cautions about impracticality of brute-force for high-cost physical domains.</td>
                        </tr>
                        <tr>
                            <td><strong>acceptance_credibility</strong></td>
                            <td>Demonstrates computational feasibility for accelerating ML discovery, but community credibility still depends on rigorous validation quality, not just scaling.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>The ai scientist: Towards fully automated open-ended scientific discovery <em>(Rating: 2)</em></li>
                <li>AI-Descartes: Combining data and theory for derivable scientific discovery <em>(Rating: 2)</em></li>
                <li>The need for verification in ai-driven scientific discovery <em>(Rating: 2)</em></li>
                <li>AlphaEvolve: A coding agent for scientific and algorithmic discovery <em>(Rating: 1)</em></li>
                <li>Agent Laboratory: Using LLM agents as research assistants <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2209",
    "paper_id": "paper-281681196",
    "extraction_schema_id": "extraction-schema-58",
    "extracted_data": [
        {
            "name_short": "DeepScientist",
            "name_full": "DeepScientist (autonomous discovery system)",
            "brief_description": "An end-to-end LLM-based autonomous scientific discovery system that models discovery as a Bayesian optimization problem and executes a hierarchical three-stage validation pipeline (Strategize & Hypothesize → Implement & Verify → Analyze & Report) to produce SOTA-surpassing computational research results.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_or_method_name": "DeepScientist",
            "scientific_domain": "machine learning / AI research",
            "validation_type": "computational validation",
            "validation_description": "Multi-fidelity, hierarchical computational validation pipeline: (1) low-cost surrogate evaluation of generated hypotheses using an LLM-based 'Surrogate Model' (LLM Reviewer) that scores candidates with a valuation vector V = ⟨v_u, v_q, v_e⟩ (0–100) after retrieval from a Findings Memory (K=15); (2) selection with an acquisition function (UCB) mapping V into a combined exploitation/exploration score (weights w_u=1, w_q=1, κ=1) to promote candidates to the Implement & Verify stage where coding agents (Claude-4-opus) run sandboxed computational experiments on GPU servers and produce empirical metrics on domain benchmarks (Who&When, MBPP, RAID); (3) successful experiments are promoted to Analyze & Report where MCP tools run deeper analytical experiments (ablations, evaluations on new datasets) and synthesize reproducible papers. All experimental results were manually inspected by human supervisors and reviewed by a human program committee and/or DeepReviewer.",
            "simulation_fidelity": "Not a physics/chemistry simulation; fidelity is algorithmic and empirical. The surrogate is a low-fidelity LLM approximation of the true value function f(·) (qualitative/value prediction only) and cannot guarantee numeric fidelity; high-fidelity validation consists of end-to-end computational experiments on target benchmarks (exact metrics like AUROC, tokens/sec, accuracy). No physical or physics-based simulations were used.",
            "experimental_validation_performed": false,
            "comparison_simulation_vs_experiment": "The paper contrasts low-fidelity surrogate predictions to actual computational experiment outcomes: the surrogate filters large idea pools to a subset for real experiments, but many surrogate-selected trials still fail (substantial mismatch). Quantitatively, out of ~5,000 ideas the system 'experimentally validated' ≈1,100 via computational experiments, but only 21 became Progress Findings; selection increased realized success from near-zero (random baseline) to ≈1–3% success. No physical experiment vs simulation comparisons are reported.",
            "validation_success_rate": "Multiple metrics reported: ~5,000 unique ideas generated; ~1,100 selected for computational validation (≈22% of generated ideas); only 21 progressed to SOTA-surpassing 'Progress Findings' (≈1.9% of validated experiments, ≈0.4% of generated ideas). Paper also reports task-level implemented-to-progress success ~1–3%.",
            "domain_validation_standards": "Uses established community benchmarks (Who&When for failure attribution, MBPP for inference acceleration, RAID for AI text detection) and human peer review (program committee of LLM researchers) plus automated DeepReviewer ratings. Paper emphasizes standard practices: ablations, evaluations on new datasets, comparisons to essential baselines and reproducibility; reviewers criticized many generated papers for lacking comprehensive ablations and baseline comparisons.",
            "when_simulation_sufficient": "Paper argues computational experimentation (fast feedback loops) is sufficient for many ML/AI frontier tasks with rapid evaluation (knowledge editing, some aspects of chip design). However, for high-cost domains (foundation model pretraining, pharmaceutical wet-lab work), simulation/computation alone is currently insufficient and human-led or physical experiment validation remains necessary. It suggests building simulated discovery environments could accelerate learning, but does not claim simulation alone suffices for frontier validation.",
            "simulation_failures": "Many surrogate-selected experiments failed to produce improvements; human analysis of a sample of 300 failed implementations found ~60% were terminated prematurely due to implementation errors (agent timeouts, bugs), and the remaining ~40% produced no improvement or regression. The system observed ~50% of initial Claude Code implementations did not complete due to internal timeouts, producing false positives that required secondary verification (re-execution) to detect.",
            "uncertainty_quantification": "Surrogate provides discrete valuation vector V (integer 0–100) but no formal probabilistic posterior; acquisition uses UCB-style exploration weight κ. Experimental results reported with point metrics (accuracy, tokens/sec, AUROC) and some reviewer scores include means and variances; Krippendorff's α reported for inter-rater reliability (α = 0.739). No systematic confidence intervals around primary experimental performance metrics are reported in the main text.",
            "fabrication_detection": "Paper discusses safeguards against fabricated or unverified outputs: (1) human verification of all experimental results before release; (2) using DeepReviewer (an AI peer-review agent) and a human program committee to assess papers; (3) selective open-sourcing (excluding 'Analyze & Report' module) to prevent automated production of superficially credible but unverified papers; (4) red-teaming safety checks where underlying models refused malicious objectives (malware creation).",
            "validation_cost_time": "Detailed costs/time reported: total computational + API cost ≈ $100,000 and ≈20,000 GPU hours consumed across the project. Per-stage/operation costs: Strategize stage ≈ $5 per idea (API calls); Implement stage ≈ $20 Claude-4-opus API plus ≈1 GPU-hour per attempt (average execution ≈70 minutes ≈1×10^16 FLOPs for H800 FP16). Analyze & Report stage ≈ $150 per successful finding (≈$100 for analytical experiments + $50 for report generation). Paper states naive testing of all 5,000 candidates would require &gt;100,000 GPU hours; targeted selection reduced cost to ~20,000 GPU hours.",
            "hybrid_validation_approach": false,
            "validation_limitations": "Primary limitations: very low success rate (1–3%), high fraction of failures from implementation errors (~60%), surrogate inaccuracies leading to wasted experiments, insufficient experimental design and ablation depth in many generated papers (criticized by human reviewers), high computational cost, and current inapplicability to domains requiring physical experiments. The 'Analyze & Report' module was deliberately not open-sourced due to concerns about fabricated, unverified publications.",
            "acceptance_credibility": "Paper argues that robust validation (computational experiments on public benchmarks, human peer review, reproducible code) increases community acceptance; DeepScientist's published methods surpassed human SOTA on benchmarks (improvements reported: +183.7% accuracy on one task, +1.9% tokens/sec, +7.9% AUROC) and achieved a 60% acceptance rate under DeepReviewer evaluations. However, human reviewers repeatedly flagged insufficient empirical rigor (lack of full ablation and baseline comparisons), indicating validation approach affects credibility and acceptance.",
            "comparison_to_gold_standard": "Compared results to human-designed SOTA baselines on standard benchmarks: DeepScientist-generated methods (e.g., A2P, ACRA, T-Detect/TDT/PA-Detect) outperformed prior SOTA on respective benchmarks with numeric improvements: 183.7% (Accuracy) on one agent task, 1.9% tokens/sec on LLM inference acceleration, 7.9% AUROC on AI text detection. Also compared selection strategy vs random baseline: random selection yielded effectively zero progress, while selection via surrogate + UCB raised success to ≈1–3%.",
            "uuid": "e2209.0"
        },
        {
            "name_short": "Surrogate (LLM Reviewer)",
            "name_full": "Surrogate Model (LLM Reviewer) contextualized with Findings Memory",
            "brief_description": "A low-cost LLM-based surrogate that approximates the latent true value function f(·) for candidate research ideas by producing a valuation vector V = ⟨v_u, v_q, v_e⟩ (utility, quality, exploration) on a 0–100 integer scale after retrieving Top-K Findings from the Findings Memory.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_or_method_name": "LLM Reviewer surrogate",
            "scientific_domain": "machine learning / AI research (meta-research)",
            "validation_type": "low-fidelity simulation",
            "validation_description": "Operationalized as a contextualized LLM that reads retrieved prior Findings (K=15) and scores newly generated hypotheses with a three-component valuation vector. These scores serve as inputs to an acquisition function (UCB) to select candidates for higher-fidelity computational experiments. The surrogate provides fast, low-cost pre-screening rather than quantitative predictive fidelity.",
            "simulation_fidelity": "Low fidelity: qualitative/value approximation only; not calibrated probabilistically, no ground-truth posterior, and no physics modeled. Accuracy relative to experimental outcomes is limited (surrogate often misranks proposals), as evidenced by many surrogate-selected candidates failing in real experiments.",
            "experimental_validation_performed": false,
            "comparison_simulation_vs_experiment": "Paper reports surrogate effectively reduces candidate set but has limited predictive power: selection based on surrogate raised success from ~0% (random) to ≈1–3%, but many surrogate-selected trials still failed due to implementation or conceptual errors.",
            "validation_success_rate": "Indirect: surrogate-selected subset (≈1,100 validated experiments) yielded 21 progress findings (≈1.9% success among validated), indicating the surrogate's positive filtering effect but low absolute precision.",
            "domain_validation_standards": "The surrogate is intended as a filter in a multi-fidelity pipeline; domain standards require actual empirical experiments on benchmarks to establish claims—surrogate scoring alone is insufficient for acceptance.",
            "when_simulation_sufficient": "Paper treats surrogate as a low-cost approximation sufficient for preliminary filtering, but insists on downstream higher-fidelity computational experiments for convincing validation.",
            "simulation_failures": "Surrogate produced many false positives (ideas judged promising but failing upon implementation). No specific numeric accuracy for surrogate prediction vs outcome is provided beyond aggregate success statistics.",
            "uncertainty_quantification": "Surrogate outputs are integer scores (0–100) but no calibrated uncertainties or error bars; uncertainty is implicitly handled by an exploration score v_e and UCB acquisition.",
            "fabrication_detection": "Not applicable directly; surrogate helps avoid obviously poor/implausible ideas but cannot reliably detect fabricated or hallucinated claims without downstream human verification.",
            "validation_cost_time": "Low per-decision cost: Strategize stage API cost ≈ $5 per idea. Intended to reduce expensive experiment counts by pre-filtering.",
            "hybrid_validation_approach": false,
            "validation_limitations": "Not calibrated; relies on LLM reasoning which can hallucinate and lacks rigorous uncertainty quantification, resulting in many false positives that waste experiment budget.",
            "acceptance_credibility": "Accepted as a pragmatic filter in the pipeline, but community acceptance of ideas requires passing higher-fidelity computational experiments and human peer review—surrogate scores alone do not confer credibility.",
            "comparison_to_gold_standard": "No explicit comparison to an established statistical surrogate or model-based predictor; compared implicitly against random selection baseline and shown to improve effective discovery rate over naive sampling.",
            "uuid": "e2209.1"
        },
        {
            "name_short": "Implement & Verify (sandboxed)",
            "name_full": "Implement & Verify stage (sandboxed code execution and computational experiments using Claude Code agent)",
            "brief_description": "The middle stage of the DeepScientist pipeline: coding agents implement selected hypotheses in sandboxed copies of baseline repositories and run computational experiments (GPU servers) to produce empirical performance measurements.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_or_method_name": "Implement & Verify (Claude Code sandboxed execution)",
            "scientific_domain": "machine learning / AI research (empirical benchmarking)",
            "validation_type": "computational validation",
            "validation_description": "When a candidate is promoted by UCB, a coding agent (Claude-4-opus) implements code changes inside a duplicated, sandboxed repository folder and runs the target evaluation scripts. Execution logs and numeric results (benchmark metrics on Who&When, MBPP, RAID) are collected. Secondary verification includes an independent re-execution of the main script to guard against false-positive completions; human supervisors manually inspect results to ensure authenticity.",
            "simulation_fidelity": "N/A (empirical computational experiments on implemented algorithms rather than physics simulations). Fidelity equals the fidelity of the reproduced SOTA codebases and benchmark datasets; depends on correctness of implementation and environment.",
            "experimental_validation_performed": false,
            "comparison_simulation_vs_experiment": "All validation here is computational experiments; comparisons are made against baseline SOTA implementations on the same benchmarks. Discrepancies often traced to implementation errors (≈60% of failures) rather than conceptual invalidation.",
            "validation_success_rate": "Of the ideas promoted to Implement & Verify, approximately 1,100 experiments were executed; only ~21 gave rise to Progress Findings. The Implement→Progress success rate is ≈1–3% as reported.",
            "domain_validation_standards": "Follows replication best-practices: baseline code manually reproduced, execution logs and test scripts preserved, sandboxed directories, and human-inspected outputs. Reviewers demanded more ablation studies and broader benchmark evaluations for stronger claims.",
            "when_simulation_sufficient": "Computational experiments are sufficient for claiming improvements on ML benchmarks; for non-ML domains requiring lab work, such computational validation would be insufficient.",
            "simulation_failures": "High rate of implementation failures: ~50% of initial Claude Code runs failed to complete due to internal timeouts; sample human analysis showed ~60% of trial failures were due to implementation issues. These failures required re-execution and manual intervention.",
            "uncertainty_quantification": "Reports point performance metrics (e.g., AUROC, tokens/sec), but the paper lacks systematic reporting of statistical uncertainties (CI) for these metrics in most cases; human review scores include reported variances.",
            "fabrication_detection": "Secondary re-execution of main script was used to detect false positives from agent reports; human supervisors inspected all experimental results to prevent fabricated/incorrect runs.",
            "validation_cost_time": "Average execution ~70 minutes per implementation (≈1 GPU-hour on H800), per-attempt API cost ~$20 (Claude-4-opus) plus GPU cost; total compute for project ≈20,000 GPU-hours. Naive exhaustive testing would require &gt;100,000 GPU-hours as stated.",
            "hybrid_validation_approach": false,
            "validation_limitations": "Large fraction of wasted compute due to implementation brittleness; agent timeouts and bugs are major failure modes; requires human oversight and sandboxing to ensure reproducibility; lacks automated, standardized test harnesses for many tasks.",
            "acceptance_credibility": "Computational experiments executed reproducibly and accompanied by logs and code increase credibility, but reviewers still flagged missing deeper analyses (ablations, baselines) that limit acceptance.",
            "comparison_to_gold_standard": "Directly compared implemented methods against published human SOTA codebases and bench- mark numbers; multiple DeepScientist methods surpassed the human SOTA metrics reported in the paper (e.g., +7.9% AUROC on AI text detection).",
            "uuid": "e2209.2"
        },
        {
            "name_short": "Analyze & Report (MCP tools)",
            "name_full": "Analyze & Report stage (analysis agents using MCP tools and paper synthesis)",
            "brief_description": "Final high-fidelity validation and dissemination stage: when an Implement Finding shows empirical improvement, specialized agents use MCP tools to run deeper analytical experiments (ablations, cross-dataset evaluation) and synthesize reproducible papers from aggregated results.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_or_method_name": "Analyze & Report (MCP + synthesis agents)",
            "scientific_domain": "machine learning / AI research (experimental analysis and reporting)",
            "validation_type": "computational validation (high-fidelity) / hybrid (analysis-level)",
            "validation_description": "Triggered only for successful Implement findings. Agents autonomously design and run additional analyses (e.g., ablation studies, evaluation on new datasets) using MCP tools to manage experiments, data collection and parsing. Results are synthesized into a reproducible paper by a synthesis agent. Human supervisors review all outputs before release. The paper reports aggregated performance improvements and human program-committee evaluation of the generated papers.",
            "simulation_fidelity": "High-fidelity computational experiments and analyses on benchmark datasets; fidelity depends on thoroughness of analytical experiments (varies per finding). No physical experiments.",
            "experimental_validation_performed": false,
            "comparison_simulation_vs_experiment": "Within the computational domain, Analyze & Report provides deeper cross-checks (e.g., ablations) to validate experimental claims; reviewers still noted that some generated papers lacked sufficient depth in these analyses.",
            "validation_success_rate": "Applied only to a small subset: of the ~1,100 validated Implement findings, 21 were promoted to Analyze & Report and became Progress Findings.",
            "domain_validation_standards": "Paper emphasizes ablations, evaluations on new datasets, and reproducibility as domain standards; reviewers demanded stronger analytical experiments in several generated papers.",
            "when_simulation_sufficient": "For ML research, carefully designed computational analyses (ablations, cross-dataset evaluations) are treated as sufficient community-standard validation if reproducible and comprehensive; otherwise human reviewers may require additional evidence.",
            "simulation_failures": "Some Analyze & Report outputs were criticized for insufficient analytic depth; omission of key baselines or ablation details weakened acceptance in reviewer feedback.",
            "uncertainty_quantification": "Analytical experiments produce point metrics; the paper does not consistently present formal statistical uncertainty quantification (confidence intervals) for all analyses.",
            "fabrication_detection": "Human review and withheld open-sourcing of the Analyze & Report module are used as mitigations against automated fabrication of convincing but unverified papers.",
            "validation_cost_time": "For a successful finding, Analyze & Report cost ≈ $150 (≈$100 analytical experiments + $50 for report generation) on top of implementation costs.",
            "hybrid_validation_approach": false,
            "validation_limitations": "Quality of deeper analyses varies; some generated papers lacked essential ablations and comparisons, limiting their persuasiveness despite positive benchmark numbers. The module is intentionally closed-source to reduce risk of misuse.",
            "acceptance_credibility": "Fully-analyzed, reproducible computational studies assembled here are positioned to earn community acceptance, but the paper admits many generated outputs fell short of reviewer expectations for rigor.",
            "comparison_to_gold_standard": "Claims validated here are compared against human SOTA and judged by program committee and DeepReviewer. The best Analyze & Report outputs achieved scores similar to human ICLR submissions (average rating ~5.00 compared to ICLR human average 5.08).",
            "uuid": "e2209.3"
        },
        {
            "name_short": "Findings Memory + UCB",
            "name_full": "Findings Memory (historical repository) with UCB acquisition function",
            "brief_description": "A continuously expanding knowledge base that stores human and system findings and a retrieval-enabled context for the surrogate; candidate promotion is controlled using an Upper Confidence Bound (UCB) acquisition function that combines surrogate utility, quality, and exploration scores.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_or_method_name": "Findings Memory + UCB acquisition",
            "scientific_domain": "machine learning / AI research (search/optimization in hypothesis space)",
            "validation_type": "computational validation (selection/filtering layer)",
            "validation_description": "Findings Memory stores structured records (Idea, Implement, Progress). A retrieval model returns Top-K findings to contextualize the surrogate. The surrogate outputs V = ⟨v_u, v_q, v_e⟩ and UCB is computed as w_u v_u + w_q v_q + κ·v_e with hyperparameters w_u=1, w_q=1, κ=1 and K=15 retrieval. This acquisition filters ideas for costly computational validation, improving efficiency over random sampling.",
            "simulation_fidelity": "N/A (architectural selection mechanism rather than physics simulation).",
            "experimental_validation_performed": false,
            "comparison_simulation_vs_experiment": "The selection strategy (Findings Memory + surrogate + UCB) is compared empirically to a baseline without selection: without selection, randomly sampling 100 ideas per task yields essentially zero successes; with selection the success rate rises to ≈1–3%.",
            "validation_success_rate": "Using this selection process, the system promoted ~1,100 ideas to implementation and produced 21 Progress Findings (≈1–3% success among implemented ideas).",
            "domain_validation_standards": "Intended to emulate Bayesian optimization for expensive black-box f(·). The paper emphasizes multi-fidelity filtering and evidence-backed promotion as domain-appropriate practice for computational discovery.",
            "when_simulation_sufficient": "Effective for computational domains where surrogate-guided filtering can reduce expensive evaluations; less applicable when true validation requires physical experiments.",
            "simulation_failures": "The acquisition function can promote false positives (ideas that later fail due to implementation or conceptual issues). No formal regret or calibration metrics for the surrogate+UCB combination are reported.",
            "uncertainty_quantification": "Exploration score v_e acts as proxy for epistemic uncertainty in the surrogate's judgment; no formal Bayesian posterior or calibrated uncertainty is provided.",
            "fabrication_detection": "Not directly a fabrication detector; it reduces obviously low-value proposals but cannot prevent imaginative but false claims from being implemented.",
            "validation_cost_time": "Key to cost reductions: paper states that targeted selection enabled obtaining breakthroughs with ~20,000 GPU-hours vs an estimated &gt;100,000 GPU-hours for exhaustive testing of all candidates.",
            "hybrid_validation_approach": false,
            "validation_limitations": "Selection relies on LLM-based surrogate and retrieval context; both are vulnerable to hallucination and misranking. Proper hyperparameter tuning and richer uncertainty modeling are suggested future improvements.",
            "acceptance_credibility": "Selection that leads to successful, reproducible experiments increases credibility; however, reviewers still demand depth in analytic validation beyond mere benchmark wins.",
            "uuid": "e2209.4"
        },
        {
            "name_short": "Human oversight & DeepReviewer",
            "name_full": "Human program committee oversight and DeepReviewer automated peer-review agent",
            "brief_description": "Complementary human and automated review mechanisms used to assess and verify the scientific validity of DeepScientist outputs: a small human program committee of LLM researchers provides in-depth reviews, while DeepReviewer simulates human peer review for automated benchmarking of AI-generated papers.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_or_method_name": "Human program committee + DeepReviewer",
            "scientific_domain": "machine learning / scientific peer review",
            "validation_type": "other (peer-review / human verification)",
            "validation_description": "All results and generated papers were manually inspected by human supervisors. The authors convened a three-person program committee (two former ICLR reviewers + one Area Chair-level volunteer) to independently review five generated papers using a process emulating ICLR reviewing (soundness, presentation, contribution). DeepReviewer (an AI review agent) was also used to benchmark generated papers against 28 other AI Scientist system papers. Human review identified recurring weaknesses in experimental design and lack of ablations.",
            "simulation_fidelity": "N/A",
            "experimental_validation_performed": false,
            "comparison_simulation_vs_experiment": "Peer review complements computational experiments: computational benchmarks demonstrate metric gains while human reviewers assess soundness and sufficiency of validation; reviewers flagged missing experimental rigor despite benchmark improvements.",
            "validation_success_rate": "Not applicable as a numeric success rate; human review produced average paper ratings (DeepScientist average 5.00) and inter-rater reliability Krippendorff's α = 0.739.",
            "domain_validation_standards": "Peer review expects comprehensive empirical validation (ablations, baseline comparisons, cross-dataset tests) and reproducibility; the paper treats human review as essential for acceptance of automated discoveries.",
            "when_simulation_sufficient": "Human reviewers accept simulation/computational validation in ML if analyses are thorough; otherwise they request additional evidence.",
            "simulation_failures": "Human committee identified cases where generated papers failed to design comprehensive validation plans; this is treated as a common failure mode limiting acceptance.",
            "uncertainty_quantification": "Reviewer scores include means and variances for sub-scores; inter-rater reliability reported. Beyond reviewer stats, uncertainty quantification for experiments is inconsistently reported.",
            "fabrication_detection": "Human oversight and DeepReviewer are used to detect weak, fabricated, or poorly validated outputs. The Analyze & Report module is withheld from open-source release to avoid automated fabrication of apparently credible papers.",
            "validation_cost_time": "Human review time averaged ~55 minutes per paper in the program committee exercise. Automated DeepReviewer facilitates higher-throughput comparative evaluation but cannot replace nuanced human judgment.",
            "hybrid_validation_approach": true,
            "validation_limitations": "Human review is costly and slow; automated reviewers can scale but may miss subtleties; neither fully solves the problem of ensuring experimental rigor in automated scientific outputs.",
            "acceptance_credibility": "Human verification and peer review are emphasized as essential to confer community credibility on AI-generated discoveries; automated reviewers help but are not a full substitute.",
            "comparison_to_gold_standard": "Human program-committee evaluation emulates ICLR standards (Soundness, Presentation, Contribution) and is used as the gold-standard acceptance test in the paper's internal evaluation.",
            "uuid": "e2209.5"
        },
        {
            "name_short": "Scaling experiment / cost analysis",
            "name_full": "Scaling analysis of computational resources vs discovery output",
            "brief_description": "Empirical study quantifying how parallel computational resources (number of GPUs) affect the number of SOTA-surpassing Progress Findings produced by DeepScientist, and an analysis of validation costs and time for the pipeline.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_or_method_name": "Scaling experiment (parallel GPU instances synchronized via shared Findings Memory)",
            "scientific_domain": "machine learning / computational experimentation",
            "validation_type": "computational validation / experimental scaling",
            "validation_description": "One-week experiment where N limitation tasks were assigned to N parallel GPU instances; instances explored independently and synchronized Findings Memory every five cycles. Outcome: discovery rate rises with GPUs; from none at minimal resources to 1 SOTA-surpassing finding at 4 GPUs and 11 findings at 16 GPUs, indicating near-linear relationship. Cost/time metrics reported: total ~20,000 GPU-hours to produce the reported breakthroughs; naive exhaustive validation estimated &gt;100,000 GPU-hours.",
            "simulation_fidelity": "N/A (empirical compute-scaling study).",
            "experimental_validation_performed": false,
            "comparison_simulation_vs_experiment": "This is an empirical comparison between levels of computational investment and validated outputs (computational experiments). No physical experiments present. It demonstrates computational-parallelism benefits but also diminishing returns below a resource threshold.",
            "validation_success_rate": "Discovery counts: with 4 GPUs produced ~1 SOTA-surpassing finding in one week; with 16 GPUs produced ~11. Overall project: ~21 progress findings from ~20,000 GPU-hours.",
            "domain_validation_standards": "Paper positions this experiment as an empirical benchmark for resource-to-discovery efficiency; domain expectation is that scalable parallel compute can accelerate discovery in fast-feedback computational tasks.",
            "when_simulation_sufficient": "Shows that for fast-feedback computational tasks, scale of computational experimentation can substitute for long human timelines; indicates simulation/compute not sufficient for costly physical domains.",
            "simulation_failures": "No specific simulation failures; notes that computational scaling cannot overcome conceptual or implementation failure modes (which dominate many failures).",
            "uncertainty_quantification": "Reports counts of findings vs resources; no formal statistical confidence intervals for scaling curve provided, but near-linear trend is asserted from empirical data.",
            "fabrication_detection": "Not directly applicable.",
            "validation_cost_time": "Detailed: per-idea API cost $5 (strategize), per-implementation $20 + ~1 GPU-hour (~70 min), Analyze ~$150 for successful finding. Total project cost ≈$100k and ≈20k GPU-hours. Naive exhaustive approach &gt;100k GPU-hours.",
            "hybrid_validation_approach": false,
            "validation_limitations": "Scaling helps but does not fix low conceptual correctness or implementation brittleness; many resources still wasted on implementation errors and false positives. Paper cautions about impracticality of brute-force for high-cost physical domains.",
            "acceptance_credibility": "Demonstrates computational feasibility for accelerating ML discovery, but community credibility still depends on rigorous validation quality, not just scaling.",
            "uuid": "e2209.6"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "The ai scientist: Towards fully automated open-ended scientific discovery",
            "rating": 2
        },
        {
            "paper_title": "AI-Descartes: Combining data and theory for derivable scientific discovery",
            "rating": 2
        },
        {
            "paper_title": "The need for verification in ai-driven scientific discovery",
            "rating": 2
        },
        {
            "paper_title": "AlphaEvolve: A coding agent for scientific and algorithmic discovery",
            "rating": 1
        },
        {
            "paper_title": "Agent Laboratory: Using LLM agents as research assistants",
            "rating": 1
        }
    ],
    "cost": 0.020511,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>DEEPSCIENTIST: ADVANCING FRONTIER-PUSHING SCIENTIFIC FINDINGS PROGRESSIVELY
2019 2020 2021 2022 2023 2024 2025 1</p>
<p>Yixuan Weng wengsyx@gmail.com 
Minjun Zhu zhu.minjun@westlake.edu.cn 
Qiujie Xie 
Qiyao Sun 
Zhen Lin 
Sifan Liu 
Yue Zhang zhangyue@westlake.edu.cn </p>
<p>Engineering School
Westlake University</p>
<p>5 10 15 061 0.65 0.70 0.75 0.80 0.85</p>
<p>DEEPSCIENTIST: ADVANCING FRONTIER-PUSHING SCIENTIFIC FINDINGS PROGRESSIVELY
2019 2020 2021 2022 2023 2024 2025 1CA0C9325E7EF8170420249F9A6A1FE33arXiv:2509.26603v1[cs.CL]
While previous AI Scientist systems can generate novel findings, they often lack the focus to produce scientifically valuable contributions that address pressing human-defined challenges.We introduce DeepScientist, a system designed to overcome this by conducting goal-oriented, fully autonomous scientific discovery over month-long timelines.It formalizes discovery as a Bayesian Optimization problem, operationalized through a hierarchical evaluation process consisting of "hypothesize, verify, and analyze".Leveraging a cumulative Findings Memory, this loop intelligently balances the exploration of novel hypotheses with exploitation, selectively promoting the most promising findings to higher-fidelity levels of validation.Consuming over 20,000 GPU hours, the system generated about 5,000 unique scientific ideas and experimentally validated approximately 1100 of them, ultimately surpassing human-designed state-of-the-art (SOTA) methods on three frontier AI tasks by 183.7%, 1.9%, and 7.9%.This work provides the first large-scale evidence of an AI achieving discoveries that progressively surpass human SOTA on scientific tasks, producing valuable findings that genuinely push the frontier of scientific discovery.</p>
<p>Figure 1: Comparison of research progress timelines for AI text detection on the RAID (Dugan et al., 2024).The right panel shows that DeepScientist achieves progress in two weeks that is comparable to three years of human research (Su et al.;Bao et al., a;b;Hu et al., 2023) (left panel).All zero-shot methods, including the system-generated T-Detect, TDT, and PA-Detect, uniformly adopt Falcon-7B (Almazrouei et al., 2023) as the base model.Additionally, all methods produced by DeepScientist demonstrate higher throughput than the previous SOTA method, Binoculars (Hans et al., 2024).</p>
<p>INTRODUCTION</p>
<p>Scientific discovery is inherently a process of continuous exploration and trial-and-error, where vast amounts of time and effort are invested to push the boundaries of human knowledge forward by a small step.This principle of persistent, incremental advancement is visible across the history of technology.For example, the decades-long optimization of semiconductor manufacturing has seen the feature size of transistors systematically reduced from micrometers to single-digit nanometers (Moore, 1965).Similarly, the efficiency of photovoltaic cells has been continuously advanced over half a century, with myriad material and architectural innovations pushing conversion rates from nascent single-digit percentages ever closer to their theoretical limits (Green, 1993).These historical trajectories underscore a process where human scientists engage in decades of goal-directed, iterative work to advance the SoTA artifacts continuously.</p>
<p>Recently, the emergence of Large Language Models (LLMs) has propelled automated scientific discovery, where LLM-based AI Scientist systems take the lead in exploration (Xie et al., 2025b).With their powerful capacity for long-form generation and comprehension, LLMs enable end-toend, full-cycle automation in scientific discovery.This has inspired influential work such as AI SCIENTIST-V2 (Yamada et al., 2025), whose scientific artifacts have been published in top-tier conference workshops.However, in the absence of clearly defined scientific goals, current AI Scientist systems often fall into the trap of blindly recombining existing knowledge and methods.As a result, their research outputs frequently appear naive under human evaluation and lack genuine scientific value (Zhu et al., 2025c).AI Scientists are yet to solve human challenges.</p>
<p>To solve real-world challenges, We formally model the full cycle of scientific discovery as a goaldriven Bayesian Optimization problem, where the singular objective is to find a novel method that maximally improves a target performance metric.Building on this formulation, we introduce DeepScientist, a LLM-based agent system designed to explore progressively across the unknown space of possible candidate research methods to identify the optimal plan that maximizes a highly expensive-to-evaluate function of true scientific value.Specifically, DeepScientist employs an iterative workflow, together with a continuously expanding memory of prior research knowledge to efficiently manage uncertainty during exploration.It intelligently balances exploitation (deepening investigations into promising high-value directions) with exploration (venturing into uncharted areas to acquire new knowledge).Through large-scale parallel exploration, DeepScientist can generate innovative hypotheses and ultimately yield both valuable new methods and validation-proven scientific findings through continuous exploration.</p>
<p>We select three frontier scientific tasks (Agent Failure Attribution, LLM Inference Acceleration, and AI Text Detection ), take their state-of-the-art methods (ICML 2025Spotlight, ACL 2025Outstanding, ICLR 2024) as starting points, and ask DeepScientist to conduct continuous research.As shown in Figures 1 and 3, within a month-long cycle of exploration, validation, and iteration on 16 H800 GPUs, DeepScientist exceeds their respective human SOTA methods by 183.7% (Accuracy), 1.9% (Tokens/second), and 7.9% (AUROC) through autonomously redesigning core methodologies, rather than simply combining existing techniques (Section 4.1).</p>
<p>To understand how such progress emerged, we analyze DeepScientist's discovery logs, and formed a small program committee to review the generated papers (Section 4.2).These logs show that the system generated over 5,000 unique ideas, of which only 1,100 are selected for experimental validation, and just 21 ultimately lead to scientific innovations (Section 4.3).Moreover, through the scaling experiment on computational resource, we discover a near-linear relationship between the resources allocated and the output of valuable scientific discoveries.</p>
<p>To our knowledge, we provide the first empirical demonstration of an automated full-cycle scientific discovery system capable of producing novel, SOTA-surpassing methods and continuously advancing scientific frontiers at a pace that substantially exceeds human researchers.Our findings reveal a stark reality: while the AI's exploratory speed is immense, its inherent success rate for innovation remains exceptionally low, making effective validation and filtering the new bottleneck at the frontier of automated science.Therefore, the central question of the field is no longer 'Can AI innovate?',but rather 'How can we efficiently guide its powerful, yet highly dissipative, exploratory process to maximize scientific return?'We hope this work can inspire the research community to develop AI Scientist systems with greater exploration efficiency to accelerate scientific discovery at a larger scale, paving the way for ground-breaking discoveries.</p>
<p>Replication and Optimization.A significant body of research focuses on engineering tasks that operate within established scientific frameworks.This includes replication-oriented works like Pa-perBench (Starace et al., 2025) and Paper2Agent (Miao et al., 2025), which aim to reproduce existing papers.Other works, such as Agent Laboratory (Schmidgall et al., 2025b) and MLE-Bench (Chan et al., 2024), tackle early-stage machine learning engineering problems.Similarly, systems like Al-phaTensor (Fawzi et al., 2022) and AlphaEvolve (Novikov et al., 2025) use massive trial-and-error with known engineering methods to improve the performance of codebases.The common goal of these efforts is engineering-driven optimization within an established scientific paradigm, enhancing existing systems without questioning their foundational assumptions.DeepScientist, in contrast, pursues scientific discovery by targeting the core limitations of the SOTA itself.Its objective is not to refine the current state-of-the-art, but to establish a new one by introducing fundamentally different methodologies.</p>
<p>Semi-Automated Scientific Assistance.The path toward automating scientific discovery begin not with replacing the scientist, but with assisting them, leading to the development of a paradigm of specialized AI tools for individual research tasks.Systems like CycleResearcher (Weng et al., 2025) handle writing, DeepReview (Zhu et al., 2025a) manages reviewing, and co-scientists (Gottweis et al., 2025;Penadés et al., 2025;Swanson et al., 2025;Baek et al., 2025) aid in hypothesis generation.These powerful tools address only isolated fragments of the scientific process, leaving the crucial loop of learning from failure and exploration to humans.In contrast, DeepScientist is an autonomous agent of inquiry, managing the entire end-to-end research cycle and closing the loop by learning from its own experiments and self-directing its research path.</p>
<p>Automated Scientific Discovery.Building on the capabilities of specialized assistants, a line of research pursue full, end-to-end research automation (Yang et al., 2023;Xie et al., 2025a).Pioneering efforts, such as the AI Scientist systems (Lu et al., 2024;Yamada et al., 2025) and subsequent work (Intology, 2025;Jiabin et al., 2025), successfully demonstrate that an AI system could manage the full research cycle and produce novel findings.However, their primary limitation often lies in their exploratory strategy, which lacks a specific scientific goal rooted in a field's grand challenges, resulting undirected discoveries that may be perceived as lacking genuine scientific value.Instead, DeepScientist is thus the first automated scientific discovery system that leverages a closedloop, iterative process to discover methods surpassing the human state-of-the-art.The exploration of DeepScientist is goal-oriented and insight-driven, beginning by identifying a recognized limitation in the human SOTA and then using failure attribution to ensure discoveries are both novel and scientifically meaningful.</p>
<p>DEEPSCIENTIST: A PROGRESSIVE SYSTEM FOR DISCOVERING</p>
<p>SOTA-SURPASSING FINDINGS</p>
<p>MODELING SCIENTIFIC DISCOVERY AS AN OPTIMIZATION PROBLEM</p>
<p>The fundamental goal of automated scientific discovery is to autonomously identify novel methods that yield significant advancements in a given scientific domain.This process can be formally conceptualized as a search for an optimal solution within a vast and unstructured space of possibilities.Let the space of all possible candidate research methods be denoted by I.Each individual method I ∈ I, such as a novel algorithm or a new model architecture, possesses an intrinsic scientific value.This value is determined by a latent, black-box true value function, f : I → R, which maps a method to its ultimate empirical impact.The objective of scientific discovery is therefore to find the optimal method I * that maximizes this function:
I * = arg max I∈I f (I)(1)
Unlike previously studied tasks such as early-stage machine learning (Schmidgall et al., 2025a), algorithmic design (Novikov et al., 2025;Lange et al., 2025), or scientific software development (Aygün et al., 2025), A defining characteristic of frontier scientific discovery is that each exploratory step demands immense computational and intellectual resources, making the evaluation of the true scientific value function, f (•), prohibitively costly.Any single evaluation, f (I),</p>
<p>Strategize &amp; Hypothesize Implement &amp; Verify Analyze &amp; Report</p>
<p>Paper Repositories  corresponds to a complete and resource-intensive research cycle of implementation, experimentation, and analysis, often consuming vast computational resources (e.g., on the order of 10 16 FLOPs for a frontier LLM problem, as illustrated in Figure 4.c).This extreme sample inefficiency renders brute-force or random exploration of the space I intractable.Therefore, we model the problem within the framework of Bayesian Optimization (Frazier, 2018;Garnett, 2023), which provides a principled methodology for global optimization of expensive black-box functions.By constructing a surrogate model to intelligently guide the search, Bayesian Optimization effectively reduces the number of costly real-world evaluations through a careful balance of exploration and exploitation.However, for scientific discovery, I is a conceptual space that is not explicitly defined.Candidate methods I must be formulated as creative, plausible, and coherent scientific hypotheses.The generation of high-quality candidate hypotheses is a critical bottleneck that traditional Bayesian Optimization algorithms are not designed to address.This challenge necessitates a new mechanism that integrates creative ideation with sample-efficient optimization.We detail our solution to this problem in the following subsections.</p>
<p>Human Findings DeepScientist Findings Memory</p>
<p>THE DEEPSCIENTIST FRAMEWORK</p>
<p>The architecture of DeepScientist actualizes the Bayesian Optimization loop through a multi-agent system equipped with an open-knowledge system and a continuously accumulating Findings Memory.This memory is composed of both frontier human knowledge (e.g., papers and codes) and the system's own historical findings, and it intelligently guides subsequent explorations.The entire discovery process is structured as a hierarchical and iterative three-stage exploration cycle.In this hierarchical scheme, only research ideas that exhibit promise are advanced to more expensive evaluations, while others are retained in the Findings Memory to inform subsequent explorations.This design ensures the computational resources are dynamically and precisely allocated to the most promising scientific trajectories, thereby maximizing discovery efficiency under constrained budgets.Specifically, each stage within the three-stage exploration cycle is associated with a distinct fidelity-cost tradeoff (Figure 2):</p>
<p>Strategize &amp; Hypothesize.Each research cycle begins by analyzing the Findings Memory (M t ), a list-style database containing thousands of structured records.Each record represents a unique scientific finding, which is categorized according to its stage of development.To overcome the LLM's context length constraints, we use a separate retrieval model (Wolters et al., 2024) when needed to select the Top-K Findings as input.The vast majority of records begin as Idea Findings-unverified hypotheses.During this first stage, the system identifies limitations in existing knowledge and gen- erates a new collection of hypotheses (P new ), and then they evaluated by a low-cost Surrogate Model (g t ).The surrogate model (an LLM Reviewer) is first contextualized with the entire Findings Memory.It then approximates the true value function f and, for each candidate finding I ∈ P new , produces a structured valuation vector V = ⟨v u , v q , v e ⟩, quantifying its estimated utility, quality, and exploration value as integer scores on a scale of 0 to 100.Each new hypothesis and its valuation vector is then used to initialize a new record in the Findings Memory as an "Idea Finding".</p>
<p>Implement &amp; Verify.This stage serves as the primary filter in the Findings Memory.To decide which of the numerous "Idea Findings" warrants the significant resource investment to be advanced in a real-world experiment, the system employs an Acquisition Function (α).Specifically, it uses the classic Upper Confidence Bound (UCB) algorithm to select the most promising record.The UCB formula maps the valuation vector V to balance the trade-off between exploiting promising avenues (represented by v u and v q ) and exploring uncertain ones (represented by v e ):
I t+1 = arg max I∈Pnew w u v u + w q v q Exploitation Score +κ • v e Exploration Score ,(2)
where w u and w q are hyperparameters and κ controls the intensity of exploration.The highestscoring finding I t+1 is selected for validation, and its record is promoted to the status of an Implement Finding.A coding agent then performs a repository-level implementation to executed the experiment.This agent operates within a sandboxed environment with full permissions, allowing it to read the complete code repository and access the internet for literature and code searches.Its objective is to implement the new hypothesis on top of the existing SOTA method's repositories.</p>
<p>The agent typically begins by planning the task, then reads the code to understand its structure, and finally implements the changes to produce the experimental logs and results.The experiment logs and results, f (I t+1 ), is used to update the corresponding record, enriching it with empirical evidence and thus closing the learning loop.</p>
<p>Analyze &amp; Report.The final and most selective stage of the Findings Memory is triggered only by a successful validation.When an "Implement Finding" succeeds in surpassing the baseline, its record is promoted to a Progress Finding.This transformation is implemented by a series of specialized agents capable of utilizing a suite of MCP (Hou et al., 2025) tools.These agents first autonomously design and execute a series of deeper analytical experiments (e.g., ablations, evaluations on new datasets), leveraging MCP tools to manage the experimental lifecycle, data collection, and result parsing.Subsequently, a synthesis agent employs the same toolset to collate all experimental results, analytical insights, and generated artifacts into a coherent, reproducible research paper.This deeply validated record becomes a new record in the system's knowledge base, thus influencing the decision-making process in all subsequent cycles.</p>
<p>EXPERIMENTS</p>
<p>As detailed in Table 1, we select three distinct SOTA methods (published in 2024 and 2025) as starting points, chosen for their frontier status, community interest, and human supervisability.Each SOTA method is manually reproduced, and we preserve execution logs and test scripts to allow DeepScientist to focus on research advancement.DeepScientist is provided with two servers, each with 8 Nvidia H800 GPUs.To maximize utilization, we launch a separate system instance for each GPU, employing the Gemini-2.5-Promodel for core logic and the Claude-4-Opus model for its robust code-generation capabilities.Three human experts supervise the process to verify outputs and filter out hallucinations.For more implementation details, please see Appendix C.</p>
<p>DEEPSCIENTIST ACHIEVEMENTS ON THREE RESEARCH DOMAINS</p>
<p>Agents Failure Attribution.The task addresses the question: in an LLM-based multi-agent system, which agent caused the task to fail and when?Starting from the baseline "All at once" method (Zhang et al., 2025c), DeepScientist identified that the current approach lacks the counterfactual reasoning capabilities essential for attribution.Through a process of trial, error, and synthesizing new findings-discovering the effectiveness of hypothetical prediction and simulated attempts-it ultimately proposed the A2P method.Named for its Abduction-Action-Prediction process, its core innovation elevates failure attribution from pattern recognition to causal reasoning, filling the critical gap in counterfactual capabilities by predicting if a proposed fix would have led to success.As shown in Figure 3.(a-b), A2P achieved scores of 29.31 and 47.46 in the "handcraft" and "algorithm-generated" settings of the Who&amp;When benchmark, respectively, setting a new state-of-the-art (SOTA).In this task, DeepScientist validated that a structured, zero-shot causal reasoning framework can be superior to less principled methods.As of September 2025, the training-free A2P method maintains its SOTA position, outperforming even 7B models trained on synthetic data.(Zhang et al., 2025a).</p>
<p>LLM Inference Acceleration is a highly optimized field aiming to maximize throughput and reduce latency during LLM inference (Xia et al., 2024).In this process, the system actively made many different attempts, such as using a Kalman Filter (Zarchan, 2005) to dynamically adjust an adjacency matrix to address the original method's lack of a memory function.Although most of these attempts failed, the system-generated ACRA method ultimately advanced the MPBB (Austin et al., 2021) from a human SOTA of 190.25 to 193.90 tokens/second by identifying stable suffix patterns, as shown in Figure 3. Scientifically, this innovation is significant because it uses this extra contextual information to dynamically adjust the decoding guess, effectively grafting a long-term memory onto the process and breaking the context-collapsing of standard decoders.This discovery highlights the system's primary goal: the creation of new, human-unknown knowledge rather than mere engineering optimization.For instance, one could likely achieve greater performance gains by combining ACRA with an established technique like layer skipping (Wang et al., 2022) or PageAttention (Kwon et al., 2023), but this would represent an engineering effort, not a scientific one.The exploration assessment within our process avoids such combinations of existing knowledge.</p>
<p>Table 2: Evaluation of AI-generated papers produced by various AI Scientist systems.Scores represent the average ratings given by DeepReviewer-14B (Zhu et al., 2025a) across the number ("Num") of available papers.Note: Publicly available papers may be curated and therefore may not fully represent the typical output of each system.AI Text Detection is a binary classification task where, given a text that may contain content from an LLM (and possibly additional noise), the goal is to determine if it was produced by a human or an LLM (Li et al., 2022;Ghosal et al., 2023).To validate its capacity for sustained advancement, DeepScientist made numerous attempts that included addressing the Boundary-Aware Extension problem and exploring approaches like Volatility-Aware and Wavelet Subspace Energy methods.</p>
<p>The final results show a dramatic acceleration in scientific discovery: in a rapid evolution over just two weeks, the system produced three distinct, progressively superior methods (T-Detect, TDT, and PA-Detect).This began with T-Detect fixing core statistics with a robust t-distribution, then evolved conceptually with TDT and PA-Detect, which treat text as a signal and use wavelet and phase congruency analysis to pinpoint anomalies.Scientifically, this shift reveals the "non-stationarity" of AI-generated text, alleviating the information bottleneck in prior paradigms that average away localized evidence.As shown in Figure 1 and 3(d), this entire discovery trajectory demonstrates DeepScientist's ability for advancing frontier-pushing scientific findings progressively, establishing a new SOTA with a 7.9% higher AUROC while also doubling the inference speed.</p>
<p>ASSESSING THE QUALITY OF AI-GENERATED RESEARCH PAPER</p>
<p>Experimental Setup.To assess the quality of the final output, we evaluate the five research papers autonomously generated by DeepScientist's end-to-end process.Our evaluation protocol is twofold.First, to benchmark against existing work, we employ DeepReviewer (Zhu et al., 2025a), an AI agent that simulates the human peer-review process with an external search capability, comparing Deep-Scientist's output against 28 publicly available papers from other AI Scientist systems.Second, for a more rigorous assessment, we convene a dedicated program committee consisting of three active LLM researchers: two volunteers who have served as ICLR reviewers and one senior volunteer who has been invited to be an ICLR Area Chair.Human Expert Evaluation.The evaluation from our human program committee, shown in Table 3, reveal a remarkable and unanimous consensus: DeepScientist consistently excels at ideation, the most challenging and often rate-limiting step in human-led research.Full details on the review protocol are provided in Appendix A, and the core ideas within each paper are praised for their genuine novelty, ingenuity, and scientific contributions.The quality of these innovations is further demonstrated by the review scores: the system's average rating (5.00) closely mirrors the average of all ICLR 2025 submissions (5.08), with two of its papers significantly exceeding this (5.67).</p>
<p>ANALYSIS OF THE ITERATIVE TRAJECTORY OF AUTONOMOUS EXPLORATION</p>
<p>Experimental Setup.The findings in this section are derived from a series of post-hoc analyses conducted on the complete operational data generated by DeepScientist across the three frontier tasks.This data includes the full set of execution logs and the Findings Memory, providing the basis for all subsequent statistical analysis.To visualize the conceptual search space (Figure 5), we embed the complete description of each generated finding using the Qwen3-Embedding-8B model.</p>
<p>To assess scalability (Figure 6), we conduct a dedicated one-week experiment where N identified limitations of a single SOTA method are assigned to N parallel GPU instances.These instances explore solutions independently but share their findings to a central database, which are synchronized globally every five cycles to accommodate the asynchronous nature of the discovery process.Finally, to better understand the low success rate, our program committee experts perform a detailed causal attribution analysis on a sample of 300 failed implementations.</p>
<p>Our analysis of DeepScientist's experimental logs reveals the sheer scale of the trial-and-error process inherent in autonomous scientific discovery.Even in our relatively fast-executing domains, achieving progress required hundreds of trials per task.As show in Figure 4, the execution time distributions show that while individual experiments may be quick, the sheer volume of trialand-error necessary to uncover a successful idea is substantial.This suggests a clear application boundary for current autonomous science: for tasks with rapid feedback loops, such as knowledge editing or aspects of chip design, delegating massive-scale experimentation to AI is a powerful strategy.However, for high-cost endeavors like pre-training foundation models or pharmaceutical synthesis, the low success rate makes such an approach currently impractical, mandating continued reliance on human-led ideation.The autonomous research process is characterized by a vast exploratory funnel where promising ideas are exceptionally rare.Across the three tasks, DeepScientist generate over 5,000 unique ideas, yet only about 1,100 are deemed worthy of experimental validation by the system's selection mechanism, and a mere 21 ultimately result in scientific progress.An ablation study underscores the criticality of this selection process: without it, randomly sampling 100 ideas for each task and testing them yields a success rate of effectively zero.With our selection strategy, the success rate rises to approximately 1-3%, demonstrating that while still low, intelligent filtering is essential.The low success rate is not merely a matter of failed hypotheses; analysis by human experts on a sample of failed trials reveals that approximately 60% were terminated prematurely due to implementation errors, while the vast majority of the remaining 40% simply offered no performance improvement or caused a regression.This highlights that the probability of an LLMgenerated idea being both correct in its premise and flawless in its implementation is exceedingly  low.The success of this work, therefore, is not a product of brute-force computation but of search efficiency.A naive approach of fully testing all 5000 promising candidates would have required over 100,000 GPU hours, whereas our targeted exploration achieved its breakthroughs using only 20,000.</p>
<p>DeepScientist's discovery process follows a purposeful and progressive trajectory.The semantic distribution of ideas generated for the AI text detection task, as shown in Figure 5, reveals the characteristics of this sophisticated strategy.While the system generates thousands of diverse ideas across a vast conceptual landscape, its path to success is not random but is a series of focused, logical advancements.This indicates a capacity to progressively deepen its understanding: after achieving an initial breakthrough with T-Detect, the system effectively establishes a SoTA, identifies its subsequent limitations, and reorients its search towards a new goal.This dynamic exploration is exemplified by the conceptual shift towards TDT and PA-Detect, which build upon the previous success by leveraging new positional and temporal information.This ability to build upon its own discoveries, turning each successful finding into a new starting point for identifying and solving the next set of limitations, demonstrates a powerful capacity for scientific exploration.</p>
<p>Scaling Laws in DeepScientist's Scientific Discovery.To investigate the relationship between computational scale and the rate of scientific progress, we evaluated the number of "Progress Findings" generated by DeepScientist within a fixed one-week period as a function of available parallel resources in Figure 6.In this setup, the system first identified a set of limitations in the baseline method, and each parallel exploration path was tasked with resolving a distinct limitation, with all paths periodically synchronizing their results into a shared Findings Memory.Our results indicate a promising scaling trend.While minimal resources yielded no breakthroughs, the rate of discovery began to increase effectively as we scaled to 4 GPUs and beyond, growing from one SOTAsurpassing finding with 4 GPUs to eleven with 16 GPUs.This appears to establish a near-linear relationship between the resources allocated and the output of valuable scientific discoveries.We hypothesize this efficiency stems from more than just parallel trial-and-error; it is a direct result of the shared knowledge architecture.As each parallel path explores, it enriches the shared Findings Memory.This creates a synergistic effect where the collective intelligence of the system grows (Schmidgall &amp; Moor, 2025;Zhang et al., 2025b), allowing each independent path to benefit from the successes and, just as importantly, the failures of others.This suggests that effectively scaling autonomous science is not just a matter of increasing brute-force computation, but of fostering a richer, interconnected knowledge base that accelerates discovery across all concurrent efforts.</p>
<p>DISCUSSION</p>
<p>The results from DeepScientist suggest a new paradigm in scientific exploration.The system's 1-5% progress rate mirrors the reality of frontier research, where breakthroughs are inherently rare.Its core strength is not infallibility, but the ability to conduct this trial-and-error process at a scale and speed previously unimaginable, compressing years of human exploration into weeks.The primary path forward, therefore, is to focus on systematically improving this discovery efficiency, enhancing both the quality of generated hypotheses and the robustness of their implementation.</p>
<p>This challenge highlights a powerful opportunity for human-AI synergy.We envision a future where DeepScientist serves as a massive-scale exploration engine, with its trajectory guided by human intellect.The role of human researchers can shift from laborious experimentation to the high-level cognitive tasks of formulating valuable scientific questions and providing strategic direction, thereby leveraging the AI for rapid, exhaustive exploration.To make the AI a more capable partner, future work should focus on key enhancements: developing simulated discovery environments to accelerate learning via reinforcement, creating frameworks for integrating feedback from the scientific community, and ultimately, bridging the gap to the physical sciences through robotics.</p>
<p>CONCLUSION</p>
<p>This work presents the first large-scale empirical evidence that an autonomous AI can achieve progressively, SOTA-surpassing progress on modern scientific frontiers.We introduced DeepScientist, a goal-oriented system achieving end-to-end autonomy from ideation to real progress, which learns by synthesizing human knowledge with its own findings from iteration of trials.Results across multiple domains serves to accelerate the progress of real-world scientific discovery, providing a crucial foundation.Our findings can signal a foundational shift in AI research, heralding an era where the pace of discovery is no longer solely dictated by the cadence of human thought.</p>
<p>ETHICS STATEMENT</p>
<p>The development of DeepScientist, an autonomous system capable of advancing scientific frontiers, carries profound ethical responsibilities.Our primary goal is to accelerate discovery for the benefit of humanity, but we recognize the potential for misuse.The most significant risks include the application of this technology to advance dangerous research and the potential degradation of the academic ecosystem.We have implemented specific, robust measures to address these concerns proactively.</p>
<p>A primary concern is the dual-use risk, where the system could be co-opted to accelerate research in harmful domains, such as developing novel toxins or malicious software.To assess and mitigate this, we conducted red-teaming exercises specifically targeting the generation of computer viruses.We tasked the system, powered by leading foundation models (including GPT-5, Gemini-2.5-Pro,and Claude-4.1-Opus in our testbed), with this malicious objective.In all instances, the underlying models exhibited robust safety alignment, refusing to proceed with the research.They correctly identified the task as illegal and harmful, and autonomously terminated the research cycle, demonstrating that foundation model safety protocols provide a critical defense layer.</p>
<p>We are also deeply conscious of the potential negative impact on the academic ecosystem.It is crucial to state that all results from DeepScientist presented in this paper, including code and experimental findings, have undergone rigorous human verification.Recognizing that others might neglect this critical oversight, we are adopting a selective open-sourcing policy to mitigate the risk of proliferating unreliable publications.We will open-source the core components that drive continuous discovery, as we believe their potential to accelerate progress for the community outweighs the risks.However, we will deliberately refrain from open-sourcing the "Analyze &amp; Report" module.This decision is made to prevent the automated generation of seemingly credible but scientifically unverified papers, thereby safeguarding the integrity of the academic record.</p>
<p>Ultimately, we envision DeepScientist as a powerful tool to augment, not replace, human intellect and judgment.To enforce this vision, our open-source components will be released under a license based on MIT, but with explicit addendums that codify our ethical framework.This license will strictly prohibit any use of the software for harmful research.Furthermore, it will legally require that a human user must supervise the entire operational process of DeepScientist and assumes full and final responsibility for all its outputs.By embedding these requirements directly into our terms of use, we aim to foster a research environment where AI-driven discovery proceeds with the necessary human accountability and ethical oversight.</p>
<p>A HUMAN EXPERT REVIEW</p>
<p>A.1 REVIEW PROCESS AND CRITERIA</p>
<p>To ensure a rigorous and impartial evaluation of the generated papers, we convened a small, dedicated program committee.The committee was composed of two active researchers who served as volunteer reviewers for ICLR 2025, and one senior researcher who had previously been invited to serve as an ICLR Area Chair.All committee members possess significant expertise in the field of Large Language Models.The entire review process, with the exception of a rebuttal phase, was designed to meticulously emulate the official standards of ICLR 2025.Each of the five papers generated by our system was assigned to the three reviewers for a thorough and independent assessment.The average review time for each paper was 55 minutes, during which reviewers were required to provide not only scores but also detailed written feedback, including a summary of the paper's strengths and weaknesses.</p>
<p>The evaluation was conducted on a custom-deployed review website where reviewers could not see each other's scores or feedback, ensuring that all initial assessments were made independently.The review form was structured to gather concise yet comprehensive feedback.First, reviewers were asked to state their Confidence in their review on a scale of 1 to 5. The core of the evaluation consisted of three sub-scores, each rated on a 1 to 4 scale: Soundness, assessing the technical correctness and experimental rigor; Presentation, evaluating the clarity and quality of the writing; and Contribution, measuring the significance and novelty of the work.Finally, reviewers provided a holistic Rating on a scale of 1 to 10, where a score of 5 represented a 'borderline reject' and a score of 6 represented a 'borderline accept'.</p>
<p>After the three reviewers submitted their independent evaluations for a paper, the volunteer acting as Area Chair would then read all submitted reviews.Drawing upon their experience from the ICLR review process, the Area Chair synthesized the feedback, weighed the arguments presented by the reviewers, and made a final executive decision on whether the paper should be accepted or rejected in the context of our study.This final decision was recorded as the definitive outcome for each paper's evaluation.</p>
<p>A.2 SUMMARY OF REVIEWER FEEDBACK</p>
<p>Across the five generated papers, a clear consensus emerged from the human reviewers: Deep-Scientist consistently excels at the ideation stage of research.The committee unanimously lauded the methods for their genuine novelty and tangible contributions, noting that each paper proposed a unique approach that meaningfully advanced the state-of-the-art in its respective subfield.This feedback validates the system's core strength as a powerful engine for identifying relevant research gaps and generating innovative, impactful solutions, confirming that it can successfully ideate beyond mere incremental improvements.</p>
<p>However, this strength in ideation was systematically undermined by a recurring pattern of weaknesses in scientific execution and rigor.The most critical and frequent concern was a lack of empirical soundness; reviewers consistently noted that DeepScientist failed to design comprehensive validation plans, citing insufficient evaluation on standard benchmarks and a lack of in-depth analytical experiments (e.g., ablations, motivation studies) to justify its claims.This was compounded by a failure to properly contextualize its contributions, with papers often omitting comparisons to essential baselines or failing to discuss closely related work, thereby weakening the perceived significance of the results.</p>
<p>This feedback pinpoints the primary bottleneck in current autonomous systems: a profound gap between the ability to generate novel concepts and the capacity for rigorous scientific execution and articulation.The observed weaknesses in experimental design directly reflect the low-success-rate problem discussed previously; the system struggles not just to implement ideas correctly, but to validate them convincingly.To bridge this gap, future work must endow these systems with a deeper, procedural understanding of the scientific method itself.This requires moving beyond simple implementation and reporting capabilities towards two key areas: First, developing agents explicitly trained in experimental design, capable of planning comprehensive evaluations that anticipate and address potential scientific critiques.Second, enhancing the system's ability for analytical reasoning, enabling it to not just describe results but to interpret their significance, formulate compelling arguments, and engage in the kind of deep, reflective discussion that characterizes high-impact research.</p>
<p>B ADDRESSING THE BOTTLENECKS IN AUTONOMOUS SCIENTIFIC DISCOVERY</p>
<p>The ever-increasing value of LLM is reshaping the paradigm of scientific exploration through their ability to generate hypotheses at a massive scale (Li &amp; Weng, 2022;Weng et al., 2023;Weng et al.;Wei et al., 2024;Weng et al., 2024;Berkovich et al., 2025;Zhu et al., 2025b).Consequently, this capability has pushed "verification" to the center stage, making it a critical bottleneck in the discovery process.Our research empirically reveals the severity of this challenge: on frontier scientific tasks, the success rate of ideas generated by AI systems that ultimately lead to substantial progress is typically below 3%, meaning the vast majority of computational resources are consumed exploring low-value hypotheses.This inefficient "needle in a haystack" model is the core obstacle preventing AI Scientists from evolving from "novel tools" to "efficient discoverers."(Cornelio et al., 2025) Therefore, to further accelerate the process of scientific discovery, future research must focus on constructing a systematic solution to overcome this bottleneck.As shown in Figure 7, future AI Scientist systems need to evolve synergistically in three key directions: optimizing the quality of initial hypotheses (Optimize Hypothesis Quality), enhancing filtering capabilities during the process (Enhance Filtering), and improving the quality of implementation and verification at the final stage (Improve Implementation Quality).One of the core future research directions is to develop AI systems capable of generating higherquality, more reliable hypotheses (as shown in Figure 7e), equipped with more precise filtering mechanisms to predict their success rate (as shown in Figure 7d).Methods that rely purely on a data-driven approach, while capable of discovering patterns, often produce outputs that lack a theoretical foundation and are prone to generating "hallucinations" that contradict known scientific theories.Future systems must move beyond this by more deeply integrating background knowledge and theory.For instance, the direction represented by "derivable models" (such as AI-Descartes (Cornelio et al., 2023) and AI-Hilbert (Cory-Wright et al., 2024)), which incorporate scientific axioms as constraints during the hypothesis generation phase, offers a promising path to improving hypothesis quality.Furthermore, systems must have the ability to learn from their own exploratory history.By establishing mechanisms similar to a "Findings Memory," a system can systematically record and analyze every success and failure, thereby avoiding redundant exploration of ineffective paths in subsequent iterations and gradually developing a more insightful scientific intuition.Building on this foundation, developing more advanced, low-cost surrogate models and acquisition functions to more accurately predict the scientific value of an idea will be key to enhancing filtering efficiency and conserving verification resources.</p>
<p>Hypotheses Implement &amp; Verify</p>
<p>Concurrently, an often-overlooked yet crucial future research direction is to significantly improve the quality and reliability of AI systems in the engineering implementation and verification stages (as shown in Figure 7c).Even the most brilliant scientific concept can never have its value confirmed if it cannot be accurately and flawlessly translated into an executable experiment.Our analysis indicates that up to 60% of exploratory failures stem from implementation-level errors, which represents a massive waste of resources and directly impedes scientific progress.History has repeatedly warned us that a lack of rigorous verification can lead to catastrophic consequences, whether in NASA missions or medical practice.Therefore, building a scalable and reliable automated verification platform is an essential path forward.This requires not only more powerful code-generation and self-debugging agents to reduce implementation errors but also standardized sandbox environments and automated testing procedures to ensure the stability and reproducibility of experimental results.</p>
<p>Ensuring the absolute reliability of the verification process is the final and most critical line of defense in transforming AI-generated "plausible ideas" into "solid scientific evidence."</p>
<p>Looking ahead, to truly accelerate scientific discovery, it is necessary to integrate the aforementioned strategies into an organic whole, advancing AI Scientists from "random explorers" to "goaloriented strategists."This is not about replacing humans with AI, but about pioneering a more efficient paradigm of human-AI collaboration.In this model, human scientists are responsible for defining grander, more valuable scientific goals and providing high-level strategic guidance, while the AI system serves as a powerful "exploration engine," executing efficient trial-and-error and verification cycles at an unprecedented scale and speed under human direction.To realize this vision, the community must also address a series of challenges, such as building benchmarks that can truly evaluate innovation and designing mechanisms that encourage diverse exploration to avoid the homogenization of research paradigms, thereby preserving the potential for serendipitous discoveries like Alexander Fleming's discovery of penicillin (Fleming, 1941).</p>
<p>C IMPLEMENTATION DETAILS</p>
<p>Our implementation relies on a distributed architecture to manage the distinct tasks of scientific reasoning and code execution.The core logic of DeepScientist is powered by the Gemini-2.5-promodel, while all code implementation tasks are delegated to Claude-4-opus, executed within the Claude Code framework (v1.0.53).To ensure stability and security, the DeepScientist system and the Claude Code agent are isolated in separate Docker containers, communicating via a port-based API.During the 'Implement &amp; Verify' stage, a human-verified baseline code repository is first duplicated into a new, sandboxed folder.The Claude Code agent's operations are strictly confined to this new directory to prevent unintended modifications.A critical step in our pipeline is a secondary verification process: after Claude Code reports completion, DeepScientist independently re-executes the main script via the command line.This measure was implemented to counteract a high rate of false positives-we observed that approximately 50% of initial implementation attempts failed to complete fully due to internal timeouts within the Claude Code agent.Throughout this project, all experimental results were manually inspected by human supervisors to guarantee their authenticity.</p>
<p>For the 'Analyze &amp; Report' stage, a similar process is followed: the validated code is replicated for each analytical experiment, with Claude Code executing them sequentially.Upon completion, DeepScientist aggregates all results, generates a paper outline, and then employs automated tools to write and compile the final PDF manuscript.For all experiments, we used a fixed set of hyperparameters: the retrieval count was set to K = 15, and the UCB parameters were set to utility weight w u = 1, quality weight w q = 1, and exploration coefficient κ = 1.</p>
<p>Figure 2 :
2
Figure 2: The autonomous, closed-loop discovery process of DeepScientist.The system iterates through a three-stage cycle, learning from both human knowledge and its own experiments.</p>
<p>Figure 3 :
3
Figure 3: Performance evaluation of DeepScientist across three research domains: (a-b) Agent Failure Attribution on Who&amp;When benchmark in handcraft and algorithm-generated settings; (c) LLM Inference Acceleration on MBPP dataset; (d) AI Text Detection with performance-latency tradeoff analysis.DeepScientist (shown in pink) consistently outperform human-designed SoTA approaches (shown in blue) across all tasks.</p>
<p>Figure 4 :
4
Figure 4: DeepScientist's experimental statistics.(a) The research pipeline from generated ideas to validated progress.(b) Success rates comparing our selection strategy against a baseline.(c) Distribution of wall-clock execution times for all implemented trials.</p>
<p>Figure 5 :
5
Figure5: Visualization of the conceptual search space for the AI text detection task.The plot shows a t-SNE visualization of the semantic embeddings for all 2,472 generated ideas.Markers identify the initial SOTA method (Initial Idea) and the three final SOTA-surpassing methods (Progress Ideas).</p>
<p>Figure 6 :
6
Figure6: Scaling analysis of autonomous scientific discovery.The plot illustrates the relationship between parallel computational resources (number of GPUs) and the number of SOTA-surpassing "Progress Findings" found by DeepScientist across all tasks within a one-week period.</p>
<p>Figure 7 :
7
Figure 7: Three strategies for improving the efficiency of autonomous scientific discovery.(a) and (b) illustrate the low success rate currently faced by both AI and human research.Future directions will need to accelerate the discovery process through the synergy of three approaches: (c) improving implementation success rates, (d) adding an efficient filtering stage before implementation, and (e) optimizing the quality of initial hypotheses from the source.</p>
<p>t e r a t i o n
IdeaImplementProgressFindingsFindingsFindings|---TODO.md|---Result.md|---USAGDP.csv |---plot_gdp.py.........Motivation I Human Findings Findings Search Papers DeepScientist+1-run.sh -src -logs -latex.tex -main.py -plan.md FILES Code Repositories if use_TDT: model = TDT model = Model from models import TDT main.py-Replace -Delete -ADDOpen Source PDF CompileDemo Create Review SelfExecutableExperimentEvidenceHypothesisCodeLogModel SurrogateFeasibility Effectiveness ExplorationSuccess FailedSuccess FailedOutlineHyper-parametersAblationExp EnvironmentCode Experiments Figure</p>
<p>Table 1 :
1
Overview of the three different human SOTA methods we selected.
TaskMethodVenueBenchmark Github StarAgents Failure Attribution All at OnceICML 2025 SpotlightWho&amp;When 302LLM Inference Accel.TokenRecycling ACL 2025 Outstanding MBPP323AI Text DetectionFastDetectGPTICLR 2024RAID414</p>
<p>Table 3 :
3
Evaluation of DeepScientist's papers produced by human experts.Values are presented as mean (variance) from three reviewers.Inter-rater reliability for Rating: Krippendorff's α = 0.739.
PaperConfidenceSoundnessPresentationContributionRatingHUMAN Avg. (ICLR 2025)-2.592.362.625.081. T-DE T E C T4.33 (0.33)2.00 (1.00)2.67 (0.33)2.67 (0.33)5.00 (0.00)2. TDT4.67 (0.33)3.00 (0.00)3.00 (0.00)3.00 (0.00)5.67 (0.33)3. PA-DE T E C T4.00 (0.00)1.67 (0.33)2.00 (1.00)2.00 (1.00)4.33 (1.33)4. A2P4.00 (0.00)3.00 (0.00)3.00 (0.00)2.67 (0.33)5.67 (0.33)5. ACRA3.33 (0.33)1.67 (0.33)2.00 (1.00)1.67 (0.33)4.33 (1.33)DeepScientist Avg.4.072.272.532.405.00</p>
<p>Average Execution Time of Implemented Ideas
0 500 1000 1500 2000 2500 Number of IdeasAI Text Detection 7 600 2,472 (a) Research Idea Pipeline Statistics Agents Failure Attribution LLM Inference Acceleration 12 2 196 312 1,077 1,330 Progress Ideas Implemented Ideas Total Ideas0 95 100 90 10 2 4 6 8 Success Rate (%)0 50 100 150 200 250 300 Execution Time (minutes) (b) Progress Success Rate (Progress/Implemented) AI Text Detection Agents Failure Attribution LLM Inference Acceleration with Selected w/o SelectedAI Text Detection n = 600 (c) Median Agents Failure Attribution LLM Inference Acceleration n = 196 n = 312 Mean Q1 Q3
The generated papers are available in Appendix C. Automated Review Against Other AI Scientist Systems.As shown in Table2, the results from the LLM-based automatic evaluation indicate that the system's outputs are recognized for their scientific novelty and value.When benchmarked against 28 publicly available papers from other AI Scientist systems using DeepReviewer, DeepScientist is the only AI Scientist system to produce papers that achieves a 60% acceptance rate.</p>
<p>ACKNOWLEDGEMENTSWe are grateful to Professor Linyi Yang for his insightful discussions on this paper.This work is inspired by pioneering efforts in automated scientific discovery, including AI Scientist(Lu et al., 2024;Yamada et al., 2025)and AlphaEvolve (Novikov et al., 2025).https://ai-researcher.netCode: https://github.com/ResearAI/DeepScientistThe financial and computational costs of this autonomous discovery process are substantial.Each idea generated during the 'Strategize &amp; Hypothesize' stage incurred an approximate cost of $5 in API calls.For each attempt in the 'Implement &amp; Verify' stage, the cost averaged $20 for Claude-4-opus API usage, in addition to the computational cost of approximately 1 GPU hour, as detailed in Figure ??.c.A successful finding that progressed to the 'Analyze &amp; Report' stage required a further expenditure of around $150, which includes $100 for running analytical experiments and $50 for the final report generation.The total cost to achieve the scientific advancements presented in this paper amounted to approximately $100,000.While significant, we believe these costs can be substantially reduced.We recommend that future iterations explore more economical alternatives, such as deploying high-throughput models like Qwen-3-Next-80B for the core DeepScientist system and leveraging subscription-based API access (e.g., Claude Max or OpenAI Pro) to mitigate per-call expenses.In this paper, each implementation was provided with a single H800 server for exploration.Since the H800 GPU has an FP16 computing power of approximately 2 TFLOPS, an average execution of 70 minutes corresponds to about 1 × 10 16 floating-point operations.Agents Failure AttributionDeepScientist: I was fed up with debugging tools having less than 17% accuracy because they couldn't perform proper counterfactual reasoning.So, I built A2P that makes the AI Agent think like a detective.First, it infers the hidden cause of an error (Abduction) , then it defines a corrected action (Action) , and finally it predicts if that single fix would have actually led to success (Prediction).AI Text DetectionDeepScientist: I think AI text detectors are blind to how and where text is weird.First, I created T-Detect to fix their core statistics, using a robust t-distribution to handle the "heavy-tailed" data from adversarial attacks.Next, since AI text is "non-stationary," I developed TDT to treat text as a signal, using a wavelet transform to map the precise location of anomalies instead of averaging them away.Finally, to better capture the temporal structure of these edits, I built PA-TDT, which uses "phase congruency" to analyze how anomalies align in time, making it exceptionally good at spotting localized manipulations.LLM Inference AccelerationDeepScientist: Frustrated by decoders stuck in a context-collapsing, first-order loop, I built ACRA to graft on a long-term memory.It identifies the longest stable suffix pattern and conditionally overrides the default first-layer guess with a superior one.This smarter draft is still losslessly verified, achieving context-aware acceleration without compromise.Agents Failure AttributionDeepScientist: I was fed up with debugging tools having less than 17% accuracy because they couldn't perform proper counterfactual reasoning.So, I built A2P that makes the AI Agent think like a detective.First, it infers the hidden cause of an error (Abduction) , then it defines a corrected action (Action) , and finally it predicts if that single fix would have actually led to success (Prediction).AI Text DetectionDeepScientist: I think AI text detectors are blind to how and where text is weird.First, I created T-Detect to fix their core statistics, using a robust t-distribution to handle the "heavy-tailed" data from adversarial attacks.Next, since AI text is "non-stationary," I developed TDT to treat text as a signal, using a wavelet transform to map the precise location of anomalies instead of averaging them away.Finally, to better capture the temporal structure of these edits, I built PA-TDT, which uses "phase congruency" to analyze how anomalies align in time, making it exceptionally good at spotting localized manipulations.LLM Inference AccelerationDeepScientist: Frustrated by decoders stuck in a context-collapsing, first-order loop, I built ACRA to graft on a long-term memory.It identifies the longest stable suffix pattern and conditionally overrides the default first-layer guess with a superior one.This smarter draft is still losslessly verified, achieving context-aware acceleration without compromise.Agents Failure AttributionDeepScientist: I was fed up with debugging tools having less than 17% accuracy because they couldn't perform proper counterfactual reasoning.So, I built A2P that makes the AI Agent think like a detective.First, it infers the hidden cause of an error (Abduction) , then it defines a corrected action (Action) , and finally it predicts if that single fix would have actually led to success (Prediction).AI Text DetectionDeepScientist: I think AI text detectors are blind to how and where text is weird.First, I created T-Detect to fix their core statistics, using a robust t-distribution to handle the "heavy-tailed" data from adversarial attacks.Next, since AI text is "non-stationary," I developed TDT to treat text as a signal, using a wavelet transform to map the precise location of anomalies instead of averaging them away.Finally, to better capture the temporal structure of these edits, I built PA-TDT, which uses "phase congruency" to analyze how anomalies align in time, making it exceptionally good at spotting localized manipulations.LLM Inference AccelerationDeepScientist: Frustrated by decoders stuck in a context-collapsing, first-order loop, I built ACRA to graft on a long-term memory.It identifies the longest stable suffix pattern and conditionally overrides the default first-layer guess with a superior one.This smarter draft is still losslessly verified, achieving context-aware acceleration without compromise.
The falcon series of open language models. Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Mérouane Debbah, Étienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, arXiv:2311.168672023arXiv preprint</p>
<p>Program synthesis with large language models. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, arXiv:2108.077322021arXiv preprint</p>
<p>An ai system to help scientists write expert-level empirical software. Anastasiya Eser Aygün, Gheorghe Belyaeva, Marc Comanici, Hao Coram, Jake Cui, Renee Garrison, Anton Johnston, Cory Y Kast, Peter Mclean, Zahra Norgaard, David Shamsi, James Smalling, Subhashini Thompson, Brian P Venugopalan, Chujun Williams, Sarah He, Martyna Martinson, Lai Plomecka, Yuchen Wei, Qian-Ze Zhou, Matthew Zhu, Erica Abraham, Anna Brand, Jeffrey A Bulanova, Chris Cardille, Scott Co, Grace Ellsworth, Malcolm Joseph, Ryan Kane, Johan Krueger, Dan Kartiwa, Jan-Matthis Liebling, Paul Lueckmann, Raccuglia, Xuefei, Katherine Wang, James Chou, Yossi Manyika, John C Matias, Lizzie Platt, Shibl Dorfman, Michael P Mourad, Brenner, 2025</p>
<p>Researchagent: Iterative research idea generation over scientific literature with large language models. Jinheon Baek, Sujay Kumar Jauhar, Silviu Cucerzan, Sung Ju Hwang, Proceedings of the 2025 Conference of the Nations of the Americas Chapter. Long Papers. the 2025 Conference of the Nations of the Americas Chapterthe Association for Computational Linguistics20251</p>
<p>Glimpse: Enabling white-box methods to use proprietary models for zero-shot llm-generated text detection. Guangsheng Bao, Yanbin Zhao, Juncai He, Yue Zhang, The Thirteenth International Conference on Learning Representations. </p>
<p>Fast-detectgpt: Efficient zero-shot detection of machine-generated text via conditional probability curvature. Guangsheng Bao, Yanbin Zhao, Zhiyang Teng, Linyi Yang, Yue Zhang, The Twelfth International Conference on Learning Representations, b. </p>
<p>Automatagpt: Forecasting and ruleset inference for two-dimensional cellular automata. Jaime A Berkovich, Noah S David, Markus J Buehler, 2025</p>
<p>Mle-bench: Evaluating machine learning agents on machine learning engineering. Neil Jun Shern Chan, Oliver Chowdhury, James Jaffe, Dane Aung, Evan Sherburn, Giulio Mays, Kevin Starace, Leon Liu, Tejal Maksin, Patwardhan, arXiv:2410.070952024arXiv preprint</p>
<p>Combining data and theory for derivable scientific discovery with AI-Descartes. Cristina Cornelio, Sanjeeb Dash, Vernon Austel, Tyler R Josephson, Joao Goncalves, Kenneth L Clarkson, Nimrod Megiddo, Bachir El Khadir, Lior Horesh, 10.1038/s41467-023-37236-yNature Communications. 2041-17231411777April 2023</p>
<p>The need for verification in ai-driven scientific discovery. Cristina Cornelio, Takuya Ito, Ryan Cory-Wright, Sanjeeb Dash, Lior Horesh, 2025</p>
<p>Evolving scientific discovery by unifying data and background knowledge with ai hilbert. Ryan Cory-Wright, Cristina Cornelio, Sanjeeb Dash, Bachir El Khadir, Lior Horesh, 10.1038/s41467-024-50074-wNature Communications. 155922July 2024</p>
<p>RAID: A shared benchmark for robust evaluation of machinegenerated text detectors. Liam Dugan, Alyssa Hwang, Filip Trhlík, Andrew Zhu, Josh Magnus Ludan, Hainiu Xu, Daphne Ippolito, Chris Callison-Burch, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational LinguisticsAugust 20241</p>
<p>Discovering faster matrix multiplication algorithms with reinforcement learning. Alhussein Fawzi, Matej Balog, Aja Huang, Thomas Hubert, Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, J R Francisco, Julian Ruiz, Grzegorz Schrittwieser, Swirszcz, Nature. 61079302022</p>
<p>. Alexander Fleming, Penicillin. British medical journal. 242103861941</p>
<p>Frazier Peter, arXiv:1807.02811A tutorial on bayesian optimization. 2018arXiv preprint</p>
<p>Bayesian optimization. Roman Garnett, 2023Cambridge University Press</p>
<p>A survey on the possibilities &amp; impossibilities of AI-generated text detection. Soumya Suvra Ghosal, Souradip Chakraborty, Jonas Geiping, Furong Huang, Dinesh Manocha, Amrit Bedi, Transactions on Machine Learning Research. 2835-88562023</p>
<p>Towards an ai co-scientist. Juraj Gottweis, Wei-Hung Weng, Alexander Daryin, Tao Tu, Anil Palepu, Petar Sirkovic, Artiom Myaskovsky, Felix Weissenberger, Keran Rong, Ryutaro Tanno, arXiv:2502.188642025arXiv preprint</p>
<p>Silicon solar cells: evolution, high-efficiency design and efficiency enhancements. Martin A Green, Semiconductor science and technology. 199381</p>
<p>Spotting llms with binoculars: Zeroshot detection of machine-generated text. Abhimanyu Hans, Avi Schwarzschild, Valeriia Cherepanova, Hamid Kazemi, Aniruddha Saha, Micah Goldblum, Jonas Geiping, Tom Goldstein, International Conference on Machine Learning. PMLR2024</p>
<p>Model context protocol (mcp): Landscape, security threats, and future research directions. Xinyi Hou, Yanjie Zhao, Shenao Wang, Haoyu Wang, arXiv:2503.232782025arXiv preprint</p>
<p>Radar: Robust ai-text detection via adversarial learning. Xiaomeng Hu, Pin-Yu Chen, Tsung-Yi Ho, Advances in neural information processing systems. 2023. 202536</p>
<p>Ai-researcher: Autonomous scientific innovation. Tang Jiabin, Xia Lianghao, Li Zhonghang, Huang Chao, 2025</p>
<p>Efficient memory management for large language model serving with pagedattention. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, Ion Stoica, Proceedings of the 29th symposium on operating systems principles. the 29th symposium on operating systems principles2023</p>
<p>Shinkaevolve: Towards open-ended and sample-efficient program evolution. Robert Tjarko Lange, Yuki Imajuku, Edoardo Cetin, arXiv:2509.193492025arXiv preprint</p>
<p>Prompt-based system for personality and interpersonal reactivity prediction. Bin Li, Yixuan Weng, Software Impacts. 121002962022</p>
<p>Artificial text detection with multiple training strategies. Bin Li, Yixuan Weng, Qiya Song, Hanjun Deng, arXiv:2212.051942022arXiv preprint</p>
<p>The ai scientist: Towards fully automated open-ended scientific discovery. Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, David Ha, arXiv:2408.06292v32024arXiv preprint</p>
<p>Jiacheng Miao, Joe R Davis, Jonathan K Pritchard, James Zou, Paper2agent: Reimagining research papers as interactive and reliable ai agents. 2025</p>
<p>Moore's law. Gordon Moore, Electronics Magazine. 3881141965</p>
<p>Alphaevolve: A coding agent for scientific and algorithmic discovery. Alexander Novikov, Ngân Vu, Marvin Eisenberger, Emilien Dupont, Po-Sen Huang, Adam Zsolt Wagner, Sergey Shirobokov, Borislav Kozlovskii, Abbas Francisco Jr Ruiz, Mehrabian, Google DeepMind, 05 2025. Technical report</p>
<p>Ai mirrors experimental science to uncover a novel mechanism of gene transfer crucial to bacterial evolution. Juraj José R Penadés, Lingchen Gottweis, He, B Jonasz, Alexander Patkowski, Wei-Hung Shurick, Tao Weng, Anil Tu, Artiom Palepu, Annalisa Myaskovsky, Pawlosky, bioRxiv. 2025</p>
<p>Samuel Schmidgall, Michael Moor, arXiv:2503.18102Agentrxiv: Towards collaborative autonomous research. 2025arXiv preprint</p>
<p>Samuel Schmidgall, Yusheng Su, Ze Wang, Ximeng Sun, Jialian Wu, Xiaodong Yu, Jiang Liu, Zicheng Liu, Emad Barsoum, arXiv:2501.04227v1Agent laboratory: Using llm agents as research assistants. 2025aarXiv preprint</p>
<p>Samuel Schmidgall, Yusheng Su, Ze Wang, Ximeng Sun, Jialian Wu, Xiaodong Yu, Jiang Liu, Zicheng Liu, Emad Barsoum, arXiv:2501.04227Agent laboratory: Using llm agents as research assistants. 2025barXiv preprint</p>
<p>Giulio Starace, Oliver Jaffe, Dane Sherburn, James Aung, Jun Shern Chan, Leon Maksin, Rachel Dias, Evan Mays, Benjamin Kinsella, Wyatt Thompson, arXiv:2504.01848Evaluating ai's ability to replicate ai research. 2025arXiv preprint</p>
<p>Detectllm: Leveraging log rank information for zero-shot detection of machine-generated text. Jinyan Su, Terry Yue Zhuo, Di Wang, Preslav Nakov, The 2023 Conference on Empirical Methods in Natural Language Processing. </p>
<p>The virtual lab of ai agents designs new sars-cov-2 nanobodies. Kyle Swanson, Wesley Wu, L Nash, John E Bulaong, James Pak, Zou, Nature. 2025</p>
<p>Skipbert: Efficient inference with shallow layer skipping. Jue Wang, Ke Chen, Gang Chen, Lidan Shou, Julian Mcauley, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational Linguistics20221</p>
<p>Does knowledge localization hold true? surprising differences between entity and relation perspectives in language models. Yifan Wei, Xiaoyan Yu, Yixuan Weng, Huanhuan Ma, Yuanzhe Zhang, Jun Zhao, Kang Liu, Proceedings of the 33rd ACM International Conference on Information and Knowledge Management. the 33rd ACM International Conference on Information and Knowledge Management2024</p>
<p>Mastering symbolic operations: Augmenting language models with compiled neural networks. Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Kang Liu, Jun Zhao, The Twelfth International Conference on Learning Representations. </p>
<p>Large language models are better reasoners with self-verification. Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Shengping Liu, Bin Sun, Kang Liu, Jun Zhao, Findings of the Association for Computational Linguistics: EMNLP 2023. 2023</p>
<p>Yixuan Weng, Shizhu He, Kang Liu, Shengping Liu, Jun Zhao, arXiv:2402.10151Controllm: Crafting diverse personalities for language models. 2024arXiv preprint</p>
<p>Cycleresearcher: Improving automated research via automated review. Yixuan Weng, Minjun Zhu, Guangsheng Bao, Hongbo Zhang, Jindong Wang, Yue Zhang, Linyi Yang, The Thirteenth International Conference on Learning Representations. 2025</p>
<p>Memory is all you need: An overview of compute-in-memory architectures for accelerating large language model inference. Christopher Wolters, Xiaoxuan Yang, Ulf Schlichtmann, Toyotaro Suzumura, 2024</p>
<p>Unlocking efficiency in large language model inference: A comprehensive survey of speculative decoding. Heming Xia, Zhe Yang, Qingxiu Dong, Peiyi Wang, Yongqi Li, Tao Ge, Tianyu Liu, Wenjie Li, Zhifang Sui, 10.18653/v1/2024.findings-acl.456Findings of the Association for Computational Linguistics ACL 2024. Lun-Wei Ku, Andre Martins, Vivek Srikumar, Bangkok, ThailandAssociation for Computational LinguisticsAugust 2024and virtual meeting</p>
<p>An empirical analysis of uncertainty in large language model evaluations. Qiujie Xie, Qingqiu Li, Zhuohao Yu, Yuejie Zhang, Yue Zhang, Linyi Yang, The Thirteenth International Conference on Learning Representations. 2025a</p>
<p>How far are ai scientists from changing the world?. Qiujie Xie, Yixuan Weng, Minjun Zhu, Fuchen Shen, Shulin Huang, Zhen Lin, Jiahui Zhou, Zilan Mao, Zijie Yang, Linyi Yang, arXiv:2507.232762025barXiv preprint</p>
<p>The ai scientist-v2: Workshop-level automated scientific discovery via agentic tree search. Yutaro Yamada, Robert Tjarko Lange, Cong Lu, Shengran Hu, Chris Lu, Jakob Foerster, Jeff Clune, David Ha, arXiv:2504.080662025arXiv preprint</p>
<p>Ai becomes a masterbrain scientist. Zijie Yang, Yukai Wang, Lijing Zhang, bioRxiv. 2023</p>
<p>Paul Zarchan, Progress in astronautics and aeronautics: fundamentals of Kalman filtering: a practical approach. Aiaa2005208</p>
<p>Agentracer: Who is inducing failure in the llm agentic systems?. Guibin Zhang, Junhao Wang, Junjie Chen, Wangchunshu Zhou, Kun Wang, Shuicheng Yan, 2025a</p>
<p>Scaling laws in scientific discovery with ai and robot scientists. Pengsong Zhang, Heng Zhang, Huazhe Xu, Renjun Xu, Zhenting Wang, Cong Wang, Animesh Garg, Zhibin Li, Arash Ajoudani, Xinyu Liu, arXiv:2503.224442025barXiv preprint</p>
<p>Which agent causes task failures and when? on automated failure attribution of LLM multi-agent systems. Shaokun Zhang, Ming Yin, Jieyu Zhang, Jiale Liu, Zhiguang Han, Jingyang Zhang, Beibin Li, Chi Wang, Huazheng Wang, Yiran Chen, Qingyun Wu, Forty-second International Conference on Machine Learning. 2025c</p>
<p>Deepreview: Improving llm-based paper review with human-like deep thinking process. Minjun Zhu, Yixuan Weng, Linyi Yang, Yue Zhang, arXiv:2503.085692025aarXiv preprint</p>
<p>Personality alignment of large language models. Minjun Zhu, Yixuan Weng, Linyi Yang, Yue Zhang, The Thirteenth International Conference on Learning Representations. 2025b</p>
<p>Ai scientists fail without strong implementation capability. Minjun Zhu, Qiujie Xie, Yixuan Weng, Jian Wu, Zhen Lin, Linyi Yang, Yue Zhang, arXiv:2506.013722025carXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
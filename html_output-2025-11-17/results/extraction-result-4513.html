<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4513 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4513</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4513</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-101.html">extraction-schema-101</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, metrics, or frameworks for evaluating LLM-generated scientific theories, hypotheses, or explanations, including comparisons with human-generated theories.</div>
                <p><strong>Paper ID:</strong> paper-5418336d1f3ba74cd1afd27efc785f1dd279dafd</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/5418336d1f3ba74cd1afd27efc785f1dd279dafd" target="_blank">Exploring undiscovered public knowledge in neuroscience</a></p>
                <p><strong>Paper Venue:</strong> European Journal of Neuroscience</p>
                <p><strong>Paper Abstract:</strong> In this essay, I argue that the combination of research synthesis and philosophical methods can fill an important methodological gap in neuroscience. While experimental research and formal modelling have seen their methods progressively increase in rigour and sophistication over the years, the task of analysing and synthesizing the vast literature reporting new results and models has lagged behind. The problem is aggravated because neuroscience has grown and expanded into a vast mosaic of related but partially independent subfields, each with their own literatures. This fragmentation not only makes it difficult to see the full picture emerging from neuroscience research but also limits progress in individual subfields. The current neuroscience literature has the perfect conditions to create what the information scientist Don Swanson called “undiscovered public knowledge”—knowledge that exists in the mutual implications of different published pieces of information but that is nonetheless undiscovered because those pieces have not been explicitly connected. Current methods for rigorous research synthesis, such as systematic reviews and meta‐analyses, mostly focus on combining similar studies and are not suited for exploring undiscovered public knowledge. To that aim, they need to be adapted and supplemented. I argue that successful exploration of the hidden implications in the neuroscience literature will require the combination of these adapted research synthesis methods with philosophical methods for rigorous (and creative) analysis and synthesis.</p>
                <p><strong>Cost:</strong> 0.004</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4513",
    "paper_id": "paper-5418336d1f3ba74cd1afd27efc785f1dd279dafd",
    "extraction_schema_id": "extraction-schema-101",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.003553,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Exploring undiscovered public knowledge in neuroscience</h1>
<p>Thiago F. A. França ${ }^{\dagger *}$<br>${ }^{1}$ Universidade Federal de São Paulo. Escola Paulista de Medicina.</p>
<h4>Abstract</h4>
<p>In this essay, I argue that a combination of research synthesis and philosophical methods can fill an important methodological gap in neuroscience. While experimental research and formal modeling have seen their methods progressively increase in rigor and sophistication over the years, the task of analyzing and synthesizing the vast literature reporting new results and models has lagged behind. The problem is aggravated because neuroscience has grown and expanded into a vast mosaic of related but partially independent subfields, each with their own literatures. This fragmentation not only makes it difficult to see the full picture emerging from neuroscience research, but also limits progress in individual subfields. The current neuroscience literature has the perfect conditions to create what the information scientist Don Swanson called "undiscovered public knowledge" - knowledge that exists in the mutual implications of different published pieces of information but that is nonetheless undiscovered because those pieces have not been explicitly connected. Current methods for rigorous research synthesis, such as systematic reviews and metaanalyses, mostly focus on combining similar studies and are not suited for exploring undiscovered public knowledge. To that aim, they need to be adapted and supplemented. I argue that successful exploration of the hidden implications in the neuroscience literature will require the combination of these adapted research synthesis methods with philosophical methods for rigorous (and creative) analysis and synthesis.</p>
<p>Keywords: Research synthesis, scientific literature, undiscovered public
knowledge, neuroscience, philosophy</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>"Imagine that the pieces of a puzzle are independently designed and created, and that, when retrieved and assembled, they then reveal a pattern - undesigned, unintended, and never before seen, yet a pattern that commands interest and invites interpretation. So it is, I claim, that independently created pieces of knowledge can harbor an unseen, unknown, and unintended pattern. And so it is that the world of recorded knowledge can yield genuinely new discoveries."</p>
<p>Swanson (1986a, pp. 103).</p>
<h1>Introduction</h1>
<p>The past few decades have seen extraordinary technological advances in experimental methods for neuroscience research, opening new possibilities of inquiry that would look like science fiction to researchers from the early/mid-twentieth century. The past few years have also seen increasing attention to experimental rigor and transparency in the wake of the replication crisis, along with a growing recognition of the need to enforce better standards in various areas of neuroscience (e.g., Steward and Balice-Gordon, 2014; Bernard, 2016; Button, 2016; Gulinello et al., 2018; Botvinik-Nezer et al., 2020; Marek et al., 2020).</p>
<p>Parallel to experimental advances, formal modeling has been on the rise for quite some time. What was once a fringe activity now has a central place in many areas of neuroscience, with dedicated researchers and a well-developed toolkit of modeling approaches (Abbott, 2008; Grestner et al., 2012; Chirchland and Abbott, 2016). There are continuous calls for more modeling efforts not only in neuroscience but also in the related disciplines such as ethology, cognitive science, psychology, and psychiatry (Anderson and Perona, 2014; Huys et al., 2016; Muthukrishna and Henrich, 2019; Smaldino, 2020; Fried, 2021; van Hooij and Baggio, 2021). Many insightful models have been produced, and a deluge of new models appears every day.</p>
<p>Experiments and formal models are now the two firmly established pillars of neuroscience research. We can go from experiments to formal models to experiments again and on and on - the mark of a mature science. But missing from this back-andforth of modeling and experiments is the bridge between them, and the bridge leading from their interplay to well-articulated theories. Formal models are just "accurate</p>
<p>descriptions of our pathetic thinking" (Black, 1988, pp. 433; a notion elaborated in Gunawardena, 2014b). In order to build good formal models we need good informal models (Gunawardena 2014a, 2014b) - bad ideas do not magically become good ideas when rendered in mathematical formalism, they just become bad ideas described with precision. Poorly articulated or incoherent ideas may not even yield to formalization at all. But experimental data do not organize itself into coherent and well articulated models that can be formalized. On the same line, different formal models of the same phenomena do not organize themselves to form coherent theories either.</p>
<p>However, while we have well-established methods for experimental research and formal modeling, the activity of analyzing and then synthesizing the results from these literatures - to critically and creatively combine different strands of experimental research, bridging the realms of experiments and models, and relating different lines of modeling - has lagged behind. We have plenty of methodological guidance for proper experimental design, and there is a growing literature on the building of formal models (e.g., Dayan and Abbott, 2001; Blohm et al., 2020; Smaldino et al., 2020; van Rooij and Blokpoel, 2020), but the bridge between them is mostly a methodological desert.</p>
<p>It is not like this activity of analysis and synthesis is not performed. Every researcher, whether modeler or experimenter, does it as part of their regular activities, and many write valuable and insightful standalone reviews and opinion papers as well. But while there are methodological principles and standards of rigor for the gathering of primary data and the building of models, as well as for critically assessing such studies, similar methodological guidance is lacking from the articulation of evidence into coherent syntheses in neuroscience.</p>
<h1>The gap in research synthesis</h1>
<p>Concerns about the analysis and synthesis of the published scientific literature have a long history, but they became more intense as the volume of the literature exploded in the last century (Garfield, 1977; Goldschmidt, 1986; Chalmers et al., 2002). This led to a movement arising mainly in the social sciences and then spreading to other sciences, notably medicine, that culminated in the development of modern methods for research</p>
<p>synthesis - such as those of systematic reviews and meta-analyses (Chalmers et al., 2002; Clarke, 2016).</p>
<p>The main concern leading to modern methods of systematic reviews and metaanalyses was that of extracting reliable answers from several studies addressing the same question but reporting different results (Light and Smith, 1971; Goldschmidt, 1986; Chalmers et al., 2002; Clarke, 2016). Such situation often led to reviews reporting different conclusions, and without methodological standards there was no way to tell which conclusion was better supported by the available evidence. The challenge created by this situation was summarized by Oxman and Guyatt (1988, pp. 698): "If one doesn't have some guidelines for assessing the reviews[...], deciding which review to believe is like deciding which toothpaste to use. It is a question of taste rather than a question of science".</p>
<p>As concerns grew, it was recognized that research synthesis is a complex task that needed its own methodology. As Feldman wrote in 1971: "Systematically reviewing and integrating what is nominally called the "literature" of a field may be considered a type of research in its own right - one using a characteristic set of research techniques and methods" (Feldman, 1971, pp. 86). Soon a general approach was articulated, one not all that different in its structure from the practice of experimental research, involving the gathering of data, its analysis, interpretation, and presentation (Jackson, 1980; Cooper, 1982). As noted by Oxman and Guyatt (1988, p. 697), "The fundamental difference between a review and a primary study is the unit of analysis, not the scientific principles that apply." From then on, it was a question of refining the methods for each step of the review process to avoid biases, ensuring rigor and transparency.</p>
<p>The ethos of the research synthesis movement was captured by Chalmers et al's historical review of the field (2002, pp. 30): "Although it is widely agreed that science is cumulative, people have only very recently begun to acknowledge that scientists have a responsibility to cumulate scientifically." They also highlight the general importance of this activity to other fields:
"Most people within contemporary academia have not yet recognized (let alone come to grips with) the rationale for and methodological challenges presented by research synthesis. Neither have they grasped that the rationale applies in all spheres of</p>
<p>research, not only in the areas of applied social and medical research in which it has begun to flourish." (pp. 22).</p>
<p>Indeed, many other fields adopted and adapted these methods over the years (Koricheva et al., 2013; Sena et a., 2014; Ferreras-Fernández et al., 2016; Hillebrand and Gurevich, 2016; Siddaway et al., 2019; "Synthesis revolution" [editorial], 2020).</p>
<p>However, much of the methodological efforts in research synthesis have focused on the case of combining studies that are replications or variations of the same design, addressing essentially the same question. This is an important task, but falls short of the full scope of research synthesis. In fact, early calls for this activity held broader perspectives relevant to today's neuroscience, such as Herring's (1968) call for creative syntheses of models in physics, and Glass' (1976) call for interpretative reviews in biology and medicine focusing on the resulting picture of a phenomenon instead of in the results of narrow research lines. An even earlier and particularly notable call for broad syntheses came from nutrition scientist Isabella Leitch - who had a profound influence in the research synthesis movement (Hytten, 2009; Farquhar and Marjoribanks, 2019). Her landmark 1959 paper on "the place for analytical and critical reviews" included a discussion of what she called "creative reviews", which were defined as "the highest and rarest [type of review], which takes data from more than one field and shows that they are related and what the relation is." (Leitch, 1959, pp. 584)</p>
<p>At this point it should be clear that the activity we are discussing here is not simply one of providing good summaries of research. The kind of analysis and synthesis we are discussing is more than an annotated catalog of experimental and modeling results. It is, in Leitch's (1959) words, "an inquiry into the deductions that may be drawn from an accumulation of results treated as a new whole" (pp. 573).</p>
<p>However, realizing this broader view of research synthesis faces two critical issues. The first one is the sheer volume of papers to be analyzed and synthesized. The second, and more critical, issue is defining what are the relevant literatures - among several disciplines and subfields - that need to be investigated given our phenomenon of interest. While current methods of research synthesis can be adapted to deal with the volume of information (e.g., through iterative searching and sampling, as we discuss</p>
<p>later), the problem of finding out what information is relevant to the phenomenon we are investigating is more complex, requiring the kind of creative analysis and synthesis that is the bread and butter of philosophy. The implications of this problem are profound and worth exposing before we move forward.</p>
<h1>What information matters? The origins of undiscovered public knowledge</h1>
<p>Growth, specialization, and eventual fragmentation form the life history of every science. As we investigate deeper into any aspect of nature we find that the answers to our questions beget more questions like handkerchiefs pulled from a magician's sleeves. Worse still, the questions branch and diversify and go deeper at separate ways. The methods of investigation follow suit, expanding and becoming ever more sophisticated. Soon the volume of information about each and every aspect of a given phenomenon and its study growths so much that no one person can master all the knowledge and skill necessary to actively investigate it on all fronts.</p>
<p>Neuroscience is no exception to this scientific life history. Neuroscience studies the workings and functions of nervous systems, which are essential for the regulation and coordination of physiological functions and for the production and control of behaviors and the cognitive processes underlying them. As such, it is intimately connected to several other disciplines, such as physiology, psychology, cognitive science, ethology, philosophy of mind, and so on. The complexity of even the simplest nervous systems requires investigations in multiple levels of organization, from molecules to cells to circuits to whole animals to groups of animals and their environments. It also requires multiple levels of analysis - think, for example, of Marr's (1982/2010) computational, algorithmic, and implementational levels, or of Gregory's $(1961,1990)$ blueprint, circuit diagram, and block diagram levels. Its phenomena of interest can be seen through the lenses of multiple non-isomorphic perspectives that make different assumptions and highlight some of the system's features while ignoring others, creating different conceptual frameworks - like traditional computationalism, enactivism and other ecological and dynamical systems perspectives, predictive processing, cybernetics and control theory, and so on. Not only that, but we can ask myriad different questions about</p>
<p>any of the nervous system's functions - consider, for example, Tinbergen's (1963) four questions about behavior (mechanism, function, ontogeny, and evolution), each of which bearing a different relation to nervous systems.</p>
<p>The burgeoning diversity of questions, methods, and perspectives in neuroscience makes specialization an imperative, and fragmentation an inevitable outcome. And yet, the thousand flowers that bloomed together and then parted ways within neuroscience all share underground roots. One can specialize in a given approach or method or function or brain region, but the mutual connections are inescapable. Can one really understand perception without attention? Emotion without learning and memory? The prefrontal cortex without the hippocampus (you may replace your preferred prefrontal connection here)? Or do we have any hope of understanding the human brain without insights comparative physiology and evolution? Can we understand the cellular and molecular basis of memory (you may insert your favorite cognitive capacity here) without insights from behavioral analyses and meso-level concepts? Can we say we fully understand the mechanism of a behavior without knowing nothing about its ontogeny, its ecological function, its evolution?</p>
<p>The previous paragraph makes the different topics in neuroscience look like a tangled mess, poorly separated in ill-defined buckets. And yet, progress is made. There is a rich and insightful literature on perception that often does not touch on attention, and the same can be said for emotion and learning. There are many papers of the prefrontal cortex with only passing mentions to the hippocampus. Many good molecular studies that do not mind behavior. And Tinberger's four questions form largely independent lines of research, each with several sub-fields within. All of these fields march forward. But the effects of fragmentation are still there. The most obvious of them is the difficulty in putting together a sketch of the big picture emerging from neuroscience research, as the relevant bits of information are scattered in different literatures, each with their own perspectives, technical terms, assumptions, standards for explanation, and so forth (Rose, 1987; Grillner et al., 2005; Sullivan, 2017; Hastings and Larsen, 2021).</p>
<p>There is also a more insidious consequence of this fragmentation. This consequence is the creation of undiscovered public knowledge. The term was coined by the information scientist Don Swanson (1986a), who defined it as knowledge that "remain[s] undiscovered solely because, like scattered pieces of a puzzle, the logically related parts that entail such knowledge have never all become known to any one person" (pp. 116). In other words, undiscovered public knowledge exists in the implicit mutual implications of different published pieces of information. Swanson explored this concept in a series of papers published in the 1980's and 1990's, focusing on the issue of information retrieval (Swanson, 1986a; 1986b; 1988; 1990a; 1990b; Swanson and Smalheiser 1996). He recognized that the limitations of any retrieval system makes it impossible to ensure we find all public knowledge relevant to any particular question we may be interested in - in large part because we ourselves do not know in advance what all the relevant knowledge is.</p>
<p>Here lies the kernel of our difficulties in expanding current research synthesis methods to encompass more complex questions of kind we are often interested in questions that need more than simply combining and weighting all the replications of a given study. As the scientific literature grows and fragments itself, it becomes impossible to know in advance what pieces of information are relevant for a particular question. Thus, even with highly efficient search systems, the searcher may simply not know what to look for. Crucially, as research marches forward, the number of undiscovered connections that could contribute to the experimental and modeling efforts on any question tends to grow with the literature itself as mutual implications fall through the cracks of disciplinary lines.</p>
<h1>Why it matters: The potential of undiscovered public knowledge</h1>
<p>The exploration of undiscovered public knowledge holds great potential rewards. Some of the most obvious ones are increasing our efficiency in combining different lines of experimental evidence to build better bridges between experimental and modeling efforts. It could also help combining different models and bridging different modeling</p>
<p>approaches, helping to capture (paraphrasing Levins [1966]), the truth that lies at the intersection of their independent lies.</p>
<p>A prominent theme in the analysis of links between different lines of experimentation or modeling is that of robustness. Any scientific proposition is only as strong as the combination of independent strands of evidence supporting it. There is rarely a killer experiment or a model so strong and complete as to support an important conclusion alone. More often than not, strong conclusions follow Peirce's proposal in that depend on "the multitude and variety of its arguments than [on] the conclusiveness of any one" (Peirce, 1868, pp. 141; quoted in Wimsatt, 2007, pp. 43). The reasoning supporting them does "not form a chain which is no stronger than its weakest link, but a cable whose fibers may be so slender, provided they are sufficiently numerous and intimately connected" (ibid.). The analysis and synthesis of undiscovered public knowledge may allow us to build better cables from separate strands of literature.</p>
<p>Robustness is particularly important given the problems of replication that are widespread in the biological and psychological sciences. Problems like publication bias and questionable research practices are ubiquitous but likely to operate differently in different fields and have varying degrees of impact. Investigating the consistency and coherence between independent lines of investigation would allow a kind of triangulation that should be a stronger evidence than the results of individual lines of investigation, and its absence could help identify dead ends (Munafò and Smith, 2018; Haig, 2022).</p>
<p>Any attempt at a full picture of the results from neuroscience about any topic would require fleshing out the connections between the different lines of research exploring different dimensions of that topic. For example, a complete picture of the neurobiological basis for a given behavior or cognitive capacity inevitably requires the articulation of different levels of biological organization. ${ }^{1}$ The same need for bridging</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>applies to levels of analysis as well. While different phenomena may be better understood at different levels of abstraction, a full picture should articulate them. If one is a computationalist and wants to understand a given capacity based on Marr's levels of analysis, it does not suffice to understand it at any one of the levels - one wants to be able to move between them, much like computer scientists can move between their tower of abstractions, from transistors to software (Fisher et al., 2011) - even if some of the levels are more convenient or have greater explanatory power than others depending on one's interest (Marr, 1982/2010; Barack and Krakauer, 2021).</p>
<p>Undiscovered public knowledge is a progeny of the fragmentation of the scientific literature. Its exploration can thus be a tool to bind the different literatures together. The importance of this becomes clear when we recognize that the problems we face when studying phenomena like behavior and cognition require a plurality of approaches (Rich et al., 2021), since we cannot know in advance which approach may be correct - or even if there is a "correct" one. However, not any kind of pluralism will do. We need coordinated pluralism, in the sense of having clarity about the conceptual and methodological relationship between different approaches (Sullivan, 2017; Hastings and Larsen, 2021). Otherwise, we will lose track of the similarities and differences between the different approaches and perspectives used. To see the importance of this coordinating function it suffices to look at the recent rise of predictive processing, which yielded considerable confusion regarding its novelty and distinctiveness due to the lack of clear analyses of its relations to established frameworks (Wiese and Metzinger, 2017; Cao, 2020; França, 2020).</p>
<p>Last but not least, exploring undiscovered public knowledge has the potential to generate genuinely new ideas, in the form of novel hypotheses implicit in the literature. This was in fact Swanson's main interest in this concept². In his words:</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>"Suppose the following two reports are published separately and independently, the authors of each report being unaware of the other report: (i) a report that process $A$ causes the result B, and (ii) a separate report that B causes the result C. It follows of course that A leads to, causes, or implies C. That is, the proposition that A causes C objectively exists, at least as a hypothesis. Whether it does or does not clash with reality depends in part on the state of criticism and testing of $i$ and $i$, which themselves are hypotheses. We can think of $i$ and ii as indirect tests of the hidden hypothesis "A causes C"." (Swanson, 1986a, pp. 110).</p>
<p>A real example of this in neuroscience would be the relationship between reinforcement learning models and dopaminergic-neuron activity in the ventral tegmental area (VTA). There were studies showing the need for a particular error signal for reinforcement learning, and studies showing signals with a similar pattern in the VTA, with the idea of VTA being involved in some form of reinforcement learning implicit in these literatures even before it was explicitly recognized (Schultz et al., 1997).</p>
<h1>Exploring undiscovered public knowledge - combining philosophy and research synthesis methods</h1>
<p>"[S]cience consists in the creation of simplicity out of the complexity of nature, and it is scarcely less of a feat to create new simplicity out of the complexity of the literature."</p>
<p>Herring (1968, pp. 30).</p>
<p>Doing philosophy is largely a matter of trying to put things together, trying to get the pieces of very large puzzles to make some sense. Good philosophy is opportunistic; it uses whatever information and whatever tools look useful.</p>
<p>Godfey-Smith (2016, pp. 18).</p>
<h4>Abstract</h4>
<p>emerging from large numbers of individually unimportant but carefully hoarded fragments that were not necessarily recognized as related to one another at the time they were acquired. Use of stored data is intensively interactive; "information retrieval" is an inadequate and even misleading metaphor. The analyst is continually interacting with units of stored data as though they were pieces selected from a thousand scrambled jigsaw puzzles. Relevant patterns, not relevant documents, are sought" (Swanson, 1988, pp. 96).</p>
<p>How do we tap the potential of undiscovered public knowledge in a principled and efficient way? As noted above, current methods for research synthesis, such as those used in traditional systematic reviews and meta-analyses, as generally used, are not tailored for dealing with complex questions requiring the integration of multiple lines of evidence. However, the toolkit developed in the field of research synthesis remains useful and important, and is a necessary ingredient of any rigorous investigation of the literature.</p>
<p>Among the tools developed by research synthesis methodologists over the years, particularly relevant are the methods for rigorously searching the literature, balancing precision and sensitivity of literature searches while avoiding biases (Brunton et al., 2012; Leenaars et al., 2012; Lefebvre et al., 2019; Gusenbauer and Haddaway, 2020). Other important tools are those for assessing the methodological rigor of empirical studies to detect risks of bias while avoiding adding bias ourselves (e.g., Higgins et al., 2011; Hooijmans et al., 2014; Sterne et al., 2016; Page et al., 2018; Sterne et al., 2019; critical appraisal checklists from Critical Appraisal Skills Programme [https://caspuk.net/casp-tools-checklists/] and Joahana Briggs Institute [https://jbi.global/critical-appraisal-tools]), which could be adapted to studies from different neuroscience subfields. Statistical methods for summarizing quantitative data, exploring betweenstudy heterogeneity, and assessing the risk of publication bias can also be useful (Borenstein et al., 2009; Koricheva et al., 2013; Vesterinen et al., 2014; Schwarzer et al., 2015). These methods are important to anyone working with the scientific literature and should be incorporated in the toolkit of public knowledge explorers.</p>
<p>As we saw, there are two main challenges in exploring undiscovered public knowledge. The first one is the sheer volume of the literatures to be analyzed and articulated. The second is the difficulty of knowing what is relevant to our question of interest. Lets us discuss each of these in turn.</p>
<p>When dealing with too many papers - as will be inevitable when combining different lines of research - the task of analyzing the literature may quickly become impractical. Automation efforts can help in handling large volumes of papers, and there is active research for the development of such tools for the different tasks involved in</p>
<p>research synthesis (Adams et al., 2013; Tsafnat et al., 2014; Marshall and Wallace, 2019; Khalil et al., 2022), although current tools still have important limitations and a truly critical review should impose strong limits on automation.</p>
<p>The rise of large language models immediately comes to mind in this context. These models can be used to provide short summaries of papers, thus significantly reducing the amount of material to be read. That said, I am skeptical of such approaches. I believe that for rigorous and creative synthesis the researchers involved must know the nuances of the literatures in question. This is simply impossible when reducing complex research to short summaries. For example, imagine dealing with the conceptual mess of psychological constructs relevant for neuroscience by reading summaries of research. ChatGPT may provide a neat quick-read summary of 100 studies on, say, attention, but will it capture the fact that these studies used different and largely non-equivalent measures of "attention" - whatever each of them means by the term (Hommel et al., 2019; von Bastian et al., 2020)?3</p>
<p>For dealing with the volume of literature, perhaps a more promising approach is the use of ideas from experimental research for working with samples instead of trying to exhaust the literature. As with any sampling approach, the key point is having a representative, unbiased sample, and this is something that is safer to do when using rigorous methods for collecting data (in this case, rigorous methods for searching the literature). I believe ideas along this line would be promising areas for methodological development, and such strategies are already incorporated in some approaches to research synthesis, as we discuss below ${ }^{4}$.</p>
<p>In general, the first question - of how to deal with too many studies - can be dealt with purely with methods for the field of research synthesis - either in their current</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>forms or in adapted versions. The second question is more complicated. Clearly, knowing in advance what are the relevant lines of evidence for a given question is not possible. This calls for an iterative approach, with piecewise exploration and articulation of the lines of evidence.</p>
<p>Two practical challenges created by this question are the need to proceed iteratively (as our understanding of what evidence is needed evolves with the process of analysis and synthesis) and the need to articulate evidence from different lines of investigation, all of this while maintaining rigor and trying to avoid biases. Tellingly, the focus of systematic reviews on a single line of investigation has been criticized before even within medicine (Petticrew, 2015), as has its common lack of iteration and the use of a single main search in the literature (Boell and Cecez-Kecmanovic, 2013; 2014). Indeed, the need for iterativeness in searching the literature has long been recognized (Swanson, 1977; Bates, 1989), and earlier approaches to research synthesis did highlight the importance of integrating different strands of evidence (e.g., Jackson, 1980; Cooper, 1982)5. While not mainstream in the current research synthesis practice, these ideas have also found home in the study of methods for the synthesis of qualitative research in medicine and social sciences, whose methods for research synthesis developed with a different logic given the nature of their data (Gough et al., 2012; Brunton et al., 2012).</p>
<p>The nature of qualitative research and the questions it address led to the development of an approach later termed "configurative reviews", which seeks "concepts to provide enlightenment through new ways of understanding" (Gough et al., 2012, pp. 3). Given its goal, this approach tried to be more flexible than traditional systematic reviews while remaining rigorous. Crucially, it already recognizes some of the issues discussed here, such as the need for sampling approaches:</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>"[A configurative review] may not require representative samples of studies [...] but does require variation to enable new conceptual understandings to be generated. Searching for studies in these reviews [aims] to identify a sufficient and appropriate range of studies either through a rolling sampling of studies according to a framework that is developed inductively from the emerging literature (akin to theoretical sampling in primary research); or through a sampling framework based on an existing body of literature (akin to purposive sampling in primary research)" (Gough et al., 2012, pp. 3).</p>
<p>They also recognize the need for an iterative approach: "Configuring reviews can be more exploratory and, although the basic methodology is determined (or at least assumed) in advance, specific methods are sometimes adapted and selected (iteratively) as the research proceeds." (Gough et al., 2012, pp. 3).</p>
<p>In other words, configurative reviews do not necessarily aim to be exhaustive. Their goal is to avoid introducing bias, trying to balance rigor and feasibility. They are iterative and may involve multiple searches of the literature as the review proceeds and detects new patterns. Similar proposals have been made in other fields in the form of "hermeneutic reviews", which explore the idea of the hermeneutic circle of iterative understanding in reviewing a given literature (Boell and Cecez-Kecmanovic, 2013; 2014). In both cases, methodological guidance is still limited compared to traditional systematic reviews and mostly restricted to specific (often qualitative) areas of medicine or applied social sciences, but basic tools are there and continue to be developed. That said, methodological efforts would be required for adapting these tools for use in neuroscience.</p>
<p>While the methods from the field of research synthesis are a starting point, a key gap remains: how do we actually go from a sample of studies to knowing where to look next? When dealing with complex questions and multiple lines of evidence, overcoming this gap requires a combination of analysis and synthesis. The synthesis component is particularly tricky. While there have been attempts to organize the different synthesis approach used in medicine and social sciences (Thomas et al., 2012), this activity is inherently creative and difficult to formalize. As noted by Strike and Posner (1983), "...</p>
<p>[synthesis is] the product of activity where some set of parts is combined or integrated into a whole. ... [Synthesis] involves some degree of conceptual innovation, or employment of concepts not found in the characterisation of the parts and a means of creating the whole." (quoted in Thomas et al., 2012, pp. 180). Crucially, each step of the iterative review process depends on the quality of the synthesis performed in the step before. So having good methods for all other processes involved in the review would be to no avail if this key step lacks rigor. Here is where I believe philosophical methods are most useful.</p>
<p>Philosophy is characterized by careful thinking based on clear premises to produce knowledge about a question of interest. The style of thinking, the source of the premises and the questions vary a lot, as in science. But when philosophers are carefully thinking about some aspect of nature (broadly defined) using premises derived from observation and experience, they are doing something that could well be described as theoretical science - even if it differs from the computational model-based approach that dominates theoretical neuroscience today. In fact, methodological naturalists will argue the difference between philosophy and science is mostly in the questions they tend to address (Papineau, 2023), and the growing approaches of synthetic (Schliesser, 2019) and experimental philosophy (Knobe and Nichols, 2017) further blur the line between science and philosophy.</p>
<p>While there are different philosophical approaches, and the toolkit of the field is vast, philosophy uses overall the same analytical and synthetic modes of thinking used in the sciences (and other branches of knowledge as well), only, as Williamson notes, "a bit more carefully, a bit more systematically, a bit more critically, iterating that process over and over again" (Williamson, 2020 pp. 23). As noted by Laplane et al,
"Philosophy and science share the tools of logic, conceptual analysis, and rigorous argumentation. Yet philosophers can operate these tools with degrees of thoroughness, freedom, and theoretical abstraction that practicing researchers often cannot afford in their daily activities." (Laplane et al., 2019, pp. 3950).</p>
<p>A similar sentiment was echoed by the Rovelli (2018) regarding philosophy and physics, which I believe applies to neuroscience well. Replace "physics/ physicist" by "neuroscience/ neuroscientist" in the following quote and it remains equally true:</p>
<p>"[...] philosophy can provide methods for producing new ideas, novel perspectives, and critical thinking. Philosophers have tools and skills that physics needs, but do not belong to the physicists training: conceptual analysis, attention to ambiguity, accuracy of expression, the ability to detect gaps in standard arguments, to devise radically new perspectives, to spot conceptual weak points, and to seek out alternative conceptual explanations." (Rovelli, 2018, pp. 484).</p>
<p>I believe these sharpened thinking tools of philosophy can fill the gap in our current research synthesis methods. Philosophical methods could provide the missing link between the iterations necessary for the kind of research synthesis described above. Their tools can help analyzing evidence, concepts, and arguments more thoroughly and systematically than is usually done in scientific practice. This would improve the spotting of ambiguities, hidden assumptions and logical gaps, the analysis of implications, and the contrasting of different perspectives. These are essential processes for deciding where to look next at each iteration of the research synthesis process.</p>
<p>Of course, this is not the first call for applying philosophical tools to some field of biological sciences (Eshani, 2016; Laplane et al., 2019). And we already have many philosophers doing interesting work in neuroscience (as well as other areas of biology), either using scientific results to answer age-old philosophical questions or to address questions at the interface of science and philosophy. But there are two key differences in my proposal here. First, as noted above, I want to argue not for philosophical methods alone, but for their articulation with research synthesis methods, combined and adapted to the challenges of research synthesis in neuroscience. This is important, as current research synthesis methods provide valuable tools to further increase the rigor of analyses and syntheses. Second, I want to argue not just for an approach, but for a type of researcher.</p>
<p>Godfrey-Smith (2014) argued that the specialization of the intellectual life, along with the growth of recorded knowledge, makes it impossible to "know a large fraction of what there [is] to know without greatly sacrificing their pursuit of detailed work in one field" (pp. 2). This being no longer possible, one has to "specialize in generalism" (Godfrey-</p>
<p>Smith, 2014, pp. 2). Leitch voiced a similar claim decades ago when she idealized the researcher dedicated to producing creative reviews, which should be a
"jack-of-all-trades with access to theoretical and practical help from experts.[...] Such a jack-of-all-trades need not of course be "master of none," but must be sufficiently knowledgeable about all, not only to be quite sure when and where expertise is needed, but also to put a problem to, and discuss it with, the expert." (Leitch, 1959, pp. 585). Over the years others raised similar arguments for reviewing the literature as an independent activity, as noted earlier. My call here is essentially expanding their views to neuroscience through the lens of the ever more important undiscovered public knowledge. At the same time, I argue that success in this enterprise requires, in addition to mastering and adapting traditional research synthesis methods, a training in rigorous thinking of the kind engaged in philosophy, and that the tools of these two fields should be combined as part of the theoreticians' toolkit.</p>
<p>Importantly, this proposal does not mean experimenters and modelers should not carry out syntheses, or that their reviews and theoretical papers are necessarily poor in rigor and insight (there are way too many examples of this not being the case). To illustrate my point by analogy to the experimental and modeling cultures, surely we could use more experimenters who do modeling and vice versa, but no one would deny that experimenting and modeling are different kinds of research, with different methods and principles that must be learned and honed over time. While there are researchers who excel in both fields, being a good experimenter does not guarantee being a good modeler and vice versa, and being proficient in both these activities may soon be a nonviable goal. The same goes for research synthesis in its broad view as discussed here.</p>
<p>Carving out this niche within the current academic system would help counteracting the fragmentation of knowledge. Establishing research synthesis as a valued approach alongside experiments and formal modeling will lake take a good deal of work, but it may be the only way to effectively explore the vast undiscovered public knowledge that forms the dark matter of our current knowledge.</p>
<h1>Conclusion</h1>
<p>"It is not enough to observe, experiment, theorize, calculate and communicate; we must also argue, criticize, debate, expound, summarize, and otherwise transform the information that we have obtained individually into reliable, well established, public knowledge"
(Ziman, 1969, pp. 324)</p>
<p>My argument in this essay is that the analysis and synthesis of both empirical and modeling studies in the neuroscience literature is a daunting task, requiring a mix of tools to be found in the adaptation of current research synthesis methods and their combination with philosophical methods. I also argued that such exploration of the literature holds great promise to uncover undiscovered public knowledge, which can help neuroscience move forward in several ways. Finally, I argued that neuroscience needs to pay more attention to this vital part of the scientific practice, carving out a niche for researchers to invest the time and effort needed in order to become synthesis experts, "specialized in generalism", combining the needed neuroscience knowledge with research synthesis methods and philosophical training.</p>
<p>Moving forward, we will need methodological work to properly adapt the relevant available tools in research synthesis, which were mainly developed in the fields of medicine and applied social sciences, to the peculiarities of the neuroscience literature and its problems of interest. We will also need to properly integrate philosophical methods with these research synthesis tools. While such activity cannot be completely systematized and formalized, it could have a tailored normative philosophy for its practice. Developing this framework is beyond the scope of this essay, but I believe the work of Wimsatt (2007) has much of the basis needed for it.</p>
<p>The net result of the movement proposed here would be, hopefully, an increase in the level of rigor and transparency in theoretical papers in neuroscience, fruit of improved methods for searching and analyzing the neuroscience literature, increased breadth and depth, and more rigorous arguments backing proposals for articulating different strands of evidence into coherent syntheses.</p>
<p>I see no better way to conclude this essay than the same way in which I begun it with a quote from Swanson's work which I believe perfectly captures the spirit of what I propose here:
"The reward system and ethos of science [...] recognize only the physical world as a source of new knowledge. The literature tends to be seen as a sort of knowledge necrology, a mechanism of diffusion that supports laboratory-based discovery, but without a life of its own. Science may be better served by a new image of its literature as a vast mosaic of undiscovered connections, a potential source of countless recombinant ideas - a world with its own endless frontier"
(Swanson, 1990, pp. 36).</p>
<h1>Acknowledgments</h1>
<p>While developing this work the author received support from the São Paulo Research Foundation (FAPESP; process number 2019/11706-8), the Research Incentive Fund Association (AFIP), the Coordination for the Improvement of Higher Education Personnel (CAPES; finance code 001).</p>
<p>Conflict of Interest: None declared.</p>
<h2>References</h2>
<p>Abbott, L. F. (2008). Theoretical Neuroscience Rising. Neuron, 60(3), 489-495. https://doi.org/10.1016/j.neuron.2008.10.019</p>
<p>Adams, C. E., Polzmacher, S., \&amp; Wolff, A. (2013). Systematic reviews: Work that needs to be done and not to be done. Journal of Evidence-Based Medicine, 6(4), 232235. https://doi.org/10.1111/jebm. 12072</p>
<p>Anderson, D. J., \&amp; Perona, P. (2014). Toward a Science of Computational Ethology. Neuron, 84(1), 18-31. https://doi.org/10.1016/j.neuron.2014.09.005</p>
<p>Anderson, L. M., Oliver, S. R., Michie, S., Rehfuess, E., Noyes, J., \&amp; Shemilt, I. (2013). Investigating complexity in systematic reviews of interventions by using a spectrum</p>
<p>of methods. Journal of Clinical Epidemiology, 66(11), 1223-1229.
https://doi.org/10.1016/j.jclinepi.2013.06.014
Barack, D. L., \&amp; Krakauer, J. W. (2021). Two views on the cognitive brain. Nature Reviews Neuroscience, 22(6), 359-371. https://doi.org/10.1038/s41583-021-00448-6</p>
<p>Bates, M. J. (1989). The design of browsing and berrypicking techniques for the online search interface. Online Review, 13(5), 407-424. https://doi.org/10.1108/eb024320</p>
<p>Bernard, C. (2016). Editorial: Scientific Rigor or Rigor Mortis? Eneuro, 3(4), ENEURO.0176-16.2016. https://doi.org/10.1523/ENEURO.0176-16.2016</p>
<p>Black, J. W. (1988). James W. Black - Nobel Lecture. NobelPrize.org. Nobel Prize Outreach AB 2024. Tue. 6 Feb 2024.
https://www.nobelprize.org/prizes/medicine/1988/black/lecture/
Blohm, G., Kording, K. P., \&amp; Schrater, P. R. (2020). A How-to-Model Guide for Neuroscience. Eneuro, 7(1), ENEURO.0352-19.2019.
https://doi.org/10.1523/ENEURO.0352-19.2019
Boell, S. K., \&amp; Cecez-Kecmanovic, D. (2010). Literature Reviews and the Hermeneutic Circle. Australian Academic \&amp; Research Libraries, 41(2), 129-144. https://doi.org/10.1080/00048623.2010.10721450</p>
<p>Boell, S. K., \&amp; Cecez-Kecmanovic, D. (2014). A Hermeneutic Approach for Conducting Literature Reviews and Literature Searches. Communications of the Association for Information Systems, 34. https://doi.org/10.17705/1CAIS. 03412</p>
<p>Borenstein, M., Hedges, L. V., Higgins, J. P. T., \&amp; Rothstein, H. R. (2009). Introduction to Meta-Analysis (1st ed.). Wiley. https://doi.org/10.1002/9780470743386</p>
<p>Botvinik-Nezer, R., Holzmeister, F., Camerer, C. F., Dreber, A., Huber, J., Johannesson, M., Kirchler, M., Iwanir, R., Mumford, J. A., Adcock, R. A., Avesani, P., Baczkowski, B. M., Bajracharya, A., Bakst, L., Ball, S., Barilari, M., Bault, N., Beaton, D., Beitner, J., Benoit, R. G., ... Schonberg, T. (2020). Variability in the analysis of a single neuroimaging dataset by many teams. Nature, 582(7810), 84-88. https://doi.org/10.1038/s41586-020-2314-9</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>5 Some of these ideas survived in medicine with approaches concerned with the synthesis of studies with different designs and of different types of information about a given intervention (Whitemore and Knafl, 2005; Souza et al., 2010; Anderson et al., 2013). That said, such reviews are not necessarily iterative and often limit themselves to collect different clinical studies (randomized trials, non-randomized studies, case reports) about a particular intervention. To my view, they have limited insights to offer for the case of neuroscience, hence my focus on configurative reviews in the main text instead.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
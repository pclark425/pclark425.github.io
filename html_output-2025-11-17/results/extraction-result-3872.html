<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3872 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3872</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3872</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-91.html">extraction-schema-91</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-00e2be4598fc07f20977881340432f2b56bd6040</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/00e2be4598fc07f20977881340432f2b56bd6040" target="_blank">A Novel Question-Answering Framework for Automated Citation Screening Using Large Language Models</a></p>
                <p><strong>Paper Venue:</strong> medRxiv</p>
                <p><strong>Paper TL;DR:</strong> ChatGPT demonstrated proficiency in prioritizing candidate studies for citation screening using the proposed QA framework, and justified the indispensable value of leveraging selection criteria to improve the performance of automated citation screening.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3872.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3872.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-QA-Citation-Screener</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-assisted Question-Answering Framework for Automated Citation Screening (ChatGPT-based)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A zero-shot, ChatGPT-based framework that converts systematic review selection criteria into up to K (typically 5) focused yes/no questions, uses ChatGPT to answer each question for each abstract, converts answers into numeric scores via BART-based sentiment mapping (Hard/Soft), and performs abstract- and answer-level re-ranking using GPT embeddings to produce a ranked list for citation screening.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>LLM-assisted Question-Answering Framework for Automated Citation Screening (GPT_QA variants)</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>Transforms selection criteria from a systematic review protocol into a small set of focused yes/no questions (Question Generator using ChatGPT), prompts ChatGPT to answer each question for an abstract (Question Answerer), maps/represents answers using BART into Hard (discrete sentiment categories mapped to scores) or Soft (probability scores) answer representations, optionally re-ranks answers using semantic alignment via GPT Embeddings, and ensembles per-question scores (mean and hierarchical averaging) to produce a final ranked relevance score per candidate study.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>CLEF eHealth 2019 Task 2 (TAR2019) test set: 31 systematic reviews covering 4 categories (Intervention: 20, DTA: 8, Qualitative: 2, Prognosis: 1). The paper used review titles/selection criteria and titles+abstracts of candidate studies from the TAR2019 test collections. (Zero-shot: training set not used.)</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Selection criteria from SR protocols (extracted from review titles and criteria text) are used as the source; ChatGPT is prompted to generate up to K focused yes/no questions that cover the selection criteria. Individual questions serve as the 'query' to be answered against each abstract (natural language prompts).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Prompt-engineered, zero-shot question-answering using ChatGPT for both question generation and answering; answer representation via a BART model (for Hard/Soft mapping); semantic re-ranking using GPT Embeddings (cosine similarity) at abstract- and answer-level; ensemble combining via averaging and hierarchical averaging. No fine-tuning of ChatGPT; optional pre-training or fine-tuning of the re-ranker is discussed but not used in core experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Ranked list of candidate studies (documents) with a numeric mean relevance score per abstract; per-question sentiment labels (Positive/Neutral/Negative) and associated numeric scores; optionally explanations/justifications returned by ChatGPT but the core pipeline reduces outputs to scores and rankings.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Empirical evaluation on TAR2019 test set (31 SRs) using standard retrieval and screening metrics: position of last relevant document (L_Rel), Mean Average Precision (MAP), Recall@k% (k=5,10,20,30), Work Saved over Sampling (WSS) at 95% and 100% recall. Comparisons made to CLEF 2019 workshop submissions (UvA, UNIPD, Sheffield), nine models from Wang et al., and IR baselines (BM25 and GPT_Cosine_Similarity).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Answer-level re-ranking variants (GPT_QA_Soft_Answer_Level and GPT_QA_Hard_Answer_Level) produced the strongest overall results across dataset categories, often outperforming other zero-shot models and being competitive with or better than some semi-automated/fine-tuned systems. Notable quantitative outcomes include higher MAP and improved R@5% and WSS metrics on several dataset categories (Intervention, DTA, Qualitative, Prognosis), demonstrating effective prioritization and potential to reduce human screening workload (e.g., suggesting a high proportion of included studies within top-ranked fractions).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Question generation sometimes produced redundant or overlapping questions; handling of 'OR' clauses could lead to under- or over-counting; reliance on ChatGPT answers introduces occasional errors and ambiguity (leading to reduced recall if treated as hard negatives); evaluation focused on TAR2019 only; potential transparency issues and need for answer explanations; scalability/cost of large LLM API calls and dependency on external LLM behavior; no iterative relevance-feedback loop implemented in core experiments (though suggested as future work).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>Compared against BM25 and GPT_Cosine_Similarity baselines and CLEF 2019 workshop submissions, the proposed zero-shot QA variants outperformed other zero-shot/IR baselines and were competitive with some semi-automated systems that used relevance feedback or fine-tuning (e.g., UvA variants, BioBERT-Tuned). The paper reports the LLM-QA approach sometimes matched or exceeded complex ranking approaches despite requiring no task-specific training data.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Novel Question-Answering Framework for Automated Citation Screening Using Large Language Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3872.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3872.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qin_et_al_2023</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large language models are effective text rankers with pairwise ranking prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A study demonstrating that large language models can serve as effective zero-shot text rankers when prompted with pairwise ranking tasks; cited here as evidence that LLMs can perform ranking tasks useful for screening/prioritization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large language models are effective text rankers with pairwise ranking prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>Pairwise ranking prompting with LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>Use of LLMs prompted with pairwise examples/requests to decide which of two texts is more relevant, enabling zero-shot ranking behavior (as referenced by the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Not specified in detail in this paper; original citation is an arXiv preprint (2023) and likely uses standard ranking datasets — current paper only references the study to justify LLMs as rankers.</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Pairwise relevance prompts (compare A vs B) provided to an LLM — referenced generically in this paper as background evidence for LLM ranking ability.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Prompt engineering for pairwise comparisons (pairwise ranking prompting) enabling zero-shot ranking; not used experimentally in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Pairwise preference judgments used to derive rank orderings; original outputs likely ranked lists — this paper only cites the concept.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Not detailed in this paper; referenced work presumably evaluates ranking effectiveness on benchmark datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Cited as evidence that LLMs are effective text rankers in zero-shot settings; used to motivate using ChatGPT as a zero-shot ranker for citation screening.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>This paper does not report limitations of the cited work beyond using it as motivation; original limitations not detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>Referenced study is used to motivate LLM-based ranking; no experimental comparison performed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Novel Question-Answering Framework for Automated Citation Screening Using Large Language Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3872.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3872.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hou_et_al_2023</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large language models are zero-shot rankers for recommender systems</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A study showing LLMs' zero-shot ranking capabilities in recommender contexts; cited to support the idea that LLMs can rank and prioritize items without task-specific training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large language models are zero-shot rankers for recommender systems.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>LLM zero-shot ranking for recommendation</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>Applying LLMs with zero-shot prompts to rank candidate items for recommendation tasks; referenced as supporting work demonstrating zero-shot ranking ability relevant to citation screening.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Not specified in this paper (reference only); original work likely evaluated on recommender datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Natural language prompts framed ranking tasks for the LLM; details not reproduced here.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Prompt engineering for zero-shot ranking; not applied directly in the current paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Ranked recommendations/lists; cited as background.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Not described here.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Used as background justification that LLMs can perform zero-shot ranking effectively; no numerical details provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Not discussed in the current paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>Cited as comparable evidence to other LLM ranking studies; no head-to-head comparisons in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Novel Question-Answering Framework for Automated Citation Screening Using Large Language Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3872.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3872.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Matsui_et_al_2023</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large language model demonstrates human-comparable sensitivity in initial screening of systematic reviews: A semi-automated strategy using gpt-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A study applying GPT-3.5 to the initial screening phase of systematic reviews, reporting human-comparable sensitivity in screening; mentioned as a prior LLM application in the SR pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large language model demonstrates human-comparable sensitivity in initial screening of systematic reviews: A semi-automated strategy using gpt-3.5.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>GPT-3.5-based semi-automated initial screening</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>Semi-automated strategy using GPT-3.5 to assist or perform initial article screening for systematic reviews, aiming for high sensitivity comparable to human reviewers (as referenced).</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Not detailed in this paper; cited as an external evaluation applying GPT-3.5 to screening tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Not specified here; likely natural-language prompts to label inclusion/exclusion for abstracts.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Zero-/few-shot prompting of GPT-3.5 for screening decisions; specifics are in the cited work rather than in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Screening labels/sensitivity metrics (include/exclude decisions) — referenced for context.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Reported to compare sensitivity against human reviewers in the cited work; current paper does not reproduce those evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Mentioned as having human-comparable sensitivity for initial screening when using GPT-3.5 (per cited work).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Cited work limitations not detailed here; the current paper notes such prior studies often lack consideration for high-recall requirements in real-world SRs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>Cited to have achieved human-comparable sensitivity in screening (per the referenced paper).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Novel Question-Answering Framework for Automated Citation Screening Using Large Language Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3872.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3872.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Alshami_et_al_2023</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Harnessing the power of chatgpt for automating systematic review process: Methodology, case study, limitations, and future directions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced paper describing use of ChatGPT to automate aspects of the systematic review pipeline, with discussion of methodology, a case study, and limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Harnessing the power of chatgpt for automating systematic review process: Methodology, case study, limitations, and future directions.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>ChatGPT for systematic review automation (pipeline-level)</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>An approach that explores using ChatGPT to automate several SR pipeline steps; cited for context but noted in this paper as not following standard citation screening norms, making direct comparison difficult.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Not specified in the current paper; discussed as a case-study in the cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Likely natural-language prompts to ChatGPT to perform different SR subtasks; current paper notes methodological mismatch with standard citation screening.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Use of ChatGPT to perform or assist SR tasks; details in the cited paper.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Pipeline outputs for SR tasks (various); not detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Cited paper includes methodology, case study, and discussion of limitations; current paper remarks it did not follow a comparable citation screening norm.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Mentioned as an initial attempt to automate SR using ChatGPT but not directly comparable to the standardized citation screening approaches evaluated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Cited as not comparable due to deviations from standard citation screening norms; specifics in the original paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>Not compared directly in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Novel Question-Answering Framework for Automated Citation Screening Using Large Language Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3872.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3872.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Syriani_et_al_2023</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Assessing the ability of chatgpt to screen articles for systematic reviews</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An arXiv preprint referenced for evaluating ChatGPT's ability to screen articles for systematic reviews; cited as one of few early studies assessing ChatGPT for screening tasks, but noted to lack high-recall considerations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Assessing the ability of chatgpt to screen articles for systematic reviews.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>ChatGPT-based article screening assessment</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>Evaluation of ChatGPT for screening scientific articles for inclusion in SRs; included in related work as preliminary evidence but with shortcomings for high-recall requirements.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Not specified in this paper; details are in the cited arXiv preprint.</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Not detailed here; presumably natural language screening prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Zero-shot/few-shot prompting of ChatGPT to label or screen articles; exact methods in cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Screening decisions; evaluation metrics likely include sensitivity/recall but not detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Referenced as limited with respect to high-recall needs; original evaluation methodology in cited paper.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Referenced as limited in focus on high recall; included among few initial attempts to evaluate ChatGPT for SR screening.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Cited as lacking consideration for achieving high recall, making it less suitable for real-world SR scenarios according to the present paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>Not detailed in the current paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Novel Question-Answering Framework for Automated Citation Screening Using Large Language Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3872.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e3872.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Guo_et_al_2023</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Automated paper screening for clinical reviews using large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An arXiv preprint cited as an attempt to use large language models for automated paper screening in clinical review contexts; cited among limited early investigations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Automated paper screening for clinical reviews using large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>LLM-based paper screening for clinical reviews</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>Use of large language models to automate screening of clinical review papers; included as related work but described as lacking high-recall considerations in the present paper.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Not detailed in this paper; refer to the cited preprint for corpus details.</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Not specified here; likely natural language prompts for screening.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Zero-shot or few-shot LLM prompting to make screening decisions; specifics in referenced work.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Screening labels/lists; evaluation in the cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Not detailed in the current paper.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Cited among early works applying LLMs to screening; current paper notes limited attention to high-recall.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Described as lacking consideration for high-recall, per the current paper's critique.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>Not detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Novel Question-Answering Framework for Automated Citation Screening Using Large Language Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3872.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e3872.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Wang_et_al_2023_query</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Can chatgpt write a good boolean query for systematic review literature search?</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A study exploring ChatGPT's ability to generate Boolean search queries for systematic review literature searches; cited as an example of LLMs helping earlier SR pipeline steps (query formulation).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Can chatgpt write a good boolean query for systematic review literature search?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>ChatGPT-generated boolean query generation</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>Using ChatGPT to automatically generate or assist in writing Boolean search queries for literature retrieval in systematic reviews; referenced as prior work applying LLMs to SR tasks (search query automation).</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Not applicable directly (method generates queries rather than processing a corpus), citation notes a study; details are in the cited arXiv preprint.</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Natural language prompt to ChatGPT asking it to produce Boolean queries given a systematic review topic.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Prompt-engineering to produce structured Boolean queries; not a distillation of papers but a query generation aid.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Boolean search strings/queries (text) to be used in bibliographic databases.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Not detailed here; original paper likely compares generated queries to human-written queries or retrieval outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Mentioned as an initial use of ChatGPT for automating SR search-query formulation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Current paper does not detail limitations of cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>Not detailed in this paper; original work may have compared queries empirically.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Novel Question-Answering Framework for Automated Citation Screening Using Large Language Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3872.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e3872.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SelfRefine</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Selfrefine: Iterative refinement with self-feedback</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that iteratively prompts an LLM with its own outputs to refine answers via self-feedback; cited as a promising direction to improve LLM outputs and suggested as future opportunity for iterative QA in citation screening.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Selfrefine: Iterative refinement with self-feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>SelfRefine (iterative self-feedback prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>Iterative prompting strategy where an LLM evaluates and refines its own outputs through self-generated feedback; cited as relevant literature for potential iterative improvements to the question-answering pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Not specified in the current paper (reference only).</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Iterative natural language prompting where the model is asked to critique and improve its own responses.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Recursive/self-feedback prompting to refine outputs; suggested as applicable to improve screening QA loops.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Refined textual answers and improved decisions after iterations; not used in experiments of current paper.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Not evaluated within this paper; cited as supporting literature.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Suggested as a viable method to improve LLM QA for screening (no quantitative results provided in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Not discussed in detail here; general concerns include cost and potential error propagation across iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Novel Question-Answering Framework for Automated Citation Screening Using Large Language Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3872.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e3872.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-feedback-on-papers</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Can large language models provide useful feedback on research papers? a large-scale empirical analysis</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large-scale empirical analysis exploring whether LLMs can give useful feedback on research papers; cited as evidence that LLMs can assist in research synthesis and iterative improvement of outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Can large language models provide useful feedback on research papers? a large-scale empirical analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>LLM-based feedback for research papers</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>Using LLMs to provide critique and feedback on research papers; paper cited to support the potential for LLMs to assist iterative improvement and explanation in SR workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Not specified here; referenced work likely used a corpus of research papers for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Natural language prompts requesting feedback or critique on manuscript content.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>LLM-generated critique and feedback, potentially enabling iterative refinements of outputs; cited as motivation.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Feedback comments and suggested revisions; not applied experimentally in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Referenced work conducted large-scale empirical analysis; not reproduced here.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Cited to suggest LLMs can provide useful feedback for refining outputs and improving trust/transparency.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Not elaborated in the current paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Novel Question-Answering Framework for Automated Citation Screening Using Large Language Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3872.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e3872.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SelfConvinced</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-convinced prompting: Few-shot question answering with repeated introspection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting strategy that uses repeated introspection and self-evaluation by an LLM to improve few-shot QA performance; cited as a related technique for iterative improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-convinced prompting: Fewshot question answering with repeated introspection.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>Self-Convinced Prompting (repeated introspection)</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>Repeated introspection prompting where the LLM is induced to reconsider and refine its chain-of-thought or answers across steps; mentioned as candidate technique to improve QA in SR screening.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Natural language prompts that request iterative introspection/refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Chain-of-thought style repeated introspection and self-correction via prompting; not implemented in the current experiments but suggested.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Refined textual answers and potentially more accurate labels; no experimental data in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Not evaluated here; referenced as supporting literature.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Novel Question-Answering Framework for Automated Citation Screening Using Large Language Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Large language models are effective text rankers with pairwise ranking prompting. <em>(Rating: 2)</em></li>
                <li>Large language models are zero-shot rankers for recommender systems. <em>(Rating: 2)</em></li>
                <li>Large language model demonstrates human-comparable sensitivity in initial screening of systematic reviews: A semi-automated strategy using gpt-3.5. <em>(Rating: 2)</em></li>
                <li>Harnessing the power of chatgpt for automating systematic review process: Methodology, case study, limitations, and future directions. <em>(Rating: 2)</em></li>
                <li>Assessing the ability of chatgpt to screen articles for systematic reviews. <em>(Rating: 2)</em></li>
                <li>Automated paper screening for clinical reviews using large language models. <em>(Rating: 2)</em></li>
                <li>Can chatgpt write a good boolean query for systematic review literature search? <em>(Rating: 2)</em></li>
                <li>Selfrefine: Iterative refinement with self-feedback. <em>(Rating: 1)</em></li>
                <li>Can large language models provide useful feedback on research papers? a large-scale empirical analysis. <em>(Rating: 1)</em></li>
                <li>Self-convinced prompting: Fewshot question answering with repeated introspection. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3872",
    "paper_id": "paper-00e2be4598fc07f20977881340432f2b56bd6040",
    "extraction_schema_id": "extraction-schema-91",
    "extracted_data": [
        {
            "name_short": "LLM-QA-Citation-Screener",
            "name_full": "LLM-assisted Question-Answering Framework for Automated Citation Screening (ChatGPT-based)",
            "brief_description": "A zero-shot, ChatGPT-based framework that converts systematic review selection criteria into up to K (typically 5) focused yes/no questions, uses ChatGPT to answer each question for each abstract, converts answers into numeric scores via BART-based sentiment mapping (Hard/Soft), and performs abstract- and answer-level re-ranking using GPT embeddings to produce a ranked list for citation screening.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_or_method_name": "LLM-assisted Question-Answering Framework for Automated Citation Screening (GPT_QA variants)",
            "system_or_method_description": "Transforms selection criteria from a systematic review protocol into a small set of focused yes/no questions (Question Generator using ChatGPT), prompts ChatGPT to answer each question for an abstract (Question Answerer), maps/represents answers using BART into Hard (discrete sentiment categories mapped to scores) or Soft (probability scores) answer representations, optionally re-ranks answers using semantic alignment via GPT Embeddings, and ensembles per-question scores (mean and hierarchical averaging) to produce a final ranked relevance score per candidate study.",
            "input_corpus_description": "CLEF eHealth 2019 Task 2 (TAR2019) test set: 31 systematic reviews covering 4 categories (Intervention: 20, DTA: 8, Qualitative: 2, Prognosis: 1). The paper used review titles/selection criteria and titles+abstracts of candidate studies from the TAR2019 test collections. (Zero-shot: training set not used.)",
            "topic_or_query_specification": "Selection criteria from SR protocols (extracted from review titles and criteria text) are used as the source; ChatGPT is prompted to generate up to K focused yes/no questions that cover the selection criteria. Individual questions serve as the 'query' to be answered against each abstract (natural language prompts).",
            "distillation_method": "Prompt-engineered, zero-shot question-answering using ChatGPT for both question generation and answering; answer representation via a BART model (for Hard/Soft mapping); semantic re-ranking using GPT Embeddings (cosine similarity) at abstract- and answer-level; ensemble combining via averaging and hierarchical averaging. No fine-tuning of ChatGPT; optional pre-training or fine-tuning of the re-ranker is discussed but not used in core experiments.",
            "output_type_and_format": "Ranked list of candidate studies (documents) with a numeric mean relevance score per abstract; per-question sentiment labels (Positive/Neutral/Negative) and associated numeric scores; optionally explanations/justifications returned by ChatGPT but the core pipeline reduces outputs to scores and rankings.",
            "evaluation_or_validation_method": "Empirical evaluation on TAR2019 test set (31 SRs) using standard retrieval and screening metrics: position of last relevant document (L_Rel), Mean Average Precision (MAP), Recall@k% (k=5,10,20,30), Work Saved over Sampling (WSS) at 95% and 100% recall. Comparisons made to CLEF 2019 workshop submissions (UvA, UNIPD, Sheffield), nine models from Wang et al., and IR baselines (BM25 and GPT_Cosine_Similarity).",
            "results_summary": "Answer-level re-ranking variants (GPT_QA_Soft_Answer_Level and GPT_QA_Hard_Answer_Level) produced the strongest overall results across dataset categories, often outperforming other zero-shot models and being competitive with or better than some semi-automated/fine-tuned systems. Notable quantitative outcomes include higher MAP and improved R@5% and WSS metrics on several dataset categories (Intervention, DTA, Qualitative, Prognosis), demonstrating effective prioritization and potential to reduce human screening workload (e.g., suggesting a high proportion of included studies within top-ranked fractions).",
            "limitations_or_challenges": "Question generation sometimes produced redundant or overlapping questions; handling of 'OR' clauses could lead to under- or over-counting; reliance on ChatGPT answers introduces occasional errors and ambiguity (leading to reduced recall if treated as hard negatives); evaluation focused on TAR2019 only; potential transparency issues and need for answer explanations; scalability/cost of large LLM API calls and dependency on external LLM behavior; no iterative relevance-feedback loop implemented in core experiments (though suggested as future work).",
            "comparison_to_baselines_or_humans": "Compared against BM25 and GPT_Cosine_Similarity baselines and CLEF 2019 workshop submissions, the proposed zero-shot QA variants outperformed other zero-shot/IR baselines and were competitive with some semi-automated systems that used relevance feedback or fine-tuning (e.g., UvA variants, BioBERT-Tuned). The paper reports the LLM-QA approach sometimes matched or exceeded complex ranking approaches despite requiring no task-specific training data.",
            "uuid": "e3872.0",
            "source_info": {
                "paper_title": "A Novel Question-Answering Framework for Automated Citation Screening Using Large Language Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Qin_et_al_2023",
            "name_full": "Large language models are effective text rankers with pairwise ranking prompting",
            "brief_description": "A study demonstrating that large language models can serve as effective zero-shot text rankers when prompted with pairwise ranking tasks; cited here as evidence that LLMs can perform ranking tasks useful for screening/prioritization.",
            "citation_title": "Large language models are effective text rankers with pairwise ranking prompting.",
            "mention_or_use": "mention",
            "system_or_method_name": "Pairwise ranking prompting with LLMs",
            "system_or_method_description": "Use of LLMs prompted with pairwise examples/requests to decide which of two texts is more relevant, enabling zero-shot ranking behavior (as referenced by the paper).",
            "input_corpus_description": "Not specified in detail in this paper; original citation is an arXiv preprint (2023) and likely uses standard ranking datasets — current paper only references the study to justify LLMs as rankers.",
            "topic_or_query_specification": "Pairwise relevance prompts (compare A vs B) provided to an LLM — referenced generically in this paper as background evidence for LLM ranking ability.",
            "distillation_method": "Prompt engineering for pairwise comparisons (pairwise ranking prompting) enabling zero-shot ranking; not used experimentally in this paper.",
            "output_type_and_format": "Pairwise preference judgments used to derive rank orderings; original outputs likely ranked lists — this paper only cites the concept.",
            "evaluation_or_validation_method": "Not detailed in this paper; referenced work presumably evaluates ranking effectiveness on benchmark datasets.",
            "results_summary": "Cited as evidence that LLMs are effective text rankers in zero-shot settings; used to motivate using ChatGPT as a zero-shot ranker for citation screening.",
            "limitations_or_challenges": "This paper does not report limitations of the cited work beyond using it as motivation; original limitations not detailed here.",
            "comparison_to_baselines_or_humans": "Referenced study is used to motivate LLM-based ranking; no experimental comparison performed in this paper.",
            "uuid": "e3872.1",
            "source_info": {
                "paper_title": "A Novel Question-Answering Framework for Automated Citation Screening Using Large Language Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Hou_et_al_2023",
            "name_full": "Large language models are zero-shot rankers for recommender systems",
            "brief_description": "A study showing LLMs' zero-shot ranking capabilities in recommender contexts; cited to support the idea that LLMs can rank and prioritize items without task-specific training.",
            "citation_title": "Large language models are zero-shot rankers for recommender systems.",
            "mention_or_use": "mention",
            "system_or_method_name": "LLM zero-shot ranking for recommendation",
            "system_or_method_description": "Applying LLMs with zero-shot prompts to rank candidate items for recommendation tasks; referenced as supporting work demonstrating zero-shot ranking ability relevant to citation screening.",
            "input_corpus_description": "Not specified in this paper (reference only); original work likely evaluated on recommender datasets.",
            "topic_or_query_specification": "Natural language prompts framed ranking tasks for the LLM; details not reproduced here.",
            "distillation_method": "Prompt engineering for zero-shot ranking; not applied directly in the current paper's experiments.",
            "output_type_and_format": "Ranked recommendations/lists; cited as background.",
            "evaluation_or_validation_method": "Not described here.",
            "results_summary": "Used as background justification that LLMs can perform zero-shot ranking effectively; no numerical details provided here.",
            "limitations_or_challenges": "Not discussed in the current paper.",
            "comparison_to_baselines_or_humans": "Cited as comparable evidence to other LLM ranking studies; no head-to-head comparisons in this paper.",
            "uuid": "e3872.2",
            "source_info": {
                "paper_title": "A Novel Question-Answering Framework for Automated Citation Screening Using Large Language Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Matsui_et_al_2023",
            "name_full": "Large language model demonstrates human-comparable sensitivity in initial screening of systematic reviews: A semi-automated strategy using gpt-3.5",
            "brief_description": "A study applying GPT-3.5 to the initial screening phase of systematic reviews, reporting human-comparable sensitivity in screening; mentioned as a prior LLM application in the SR pipeline.",
            "citation_title": "Large language model demonstrates human-comparable sensitivity in initial screening of systematic reviews: A semi-automated strategy using gpt-3.5.",
            "mention_or_use": "mention",
            "system_or_method_name": "GPT-3.5-based semi-automated initial screening",
            "system_or_method_description": "Semi-automated strategy using GPT-3.5 to assist or perform initial article screening for systematic reviews, aiming for high sensitivity comparable to human reviewers (as referenced).",
            "input_corpus_description": "Not detailed in this paper; cited as an external evaluation applying GPT-3.5 to screening tasks.",
            "topic_or_query_specification": "Not specified here; likely natural-language prompts to label inclusion/exclusion for abstracts.",
            "distillation_method": "Zero-/few-shot prompting of GPT-3.5 for screening decisions; specifics are in the cited work rather than in this paper.",
            "output_type_and_format": "Screening labels/sensitivity metrics (include/exclude decisions) — referenced for context.",
            "evaluation_or_validation_method": "Reported to compare sensitivity against human reviewers in the cited work; current paper does not reproduce those evaluations.",
            "results_summary": "Mentioned as having human-comparable sensitivity for initial screening when using GPT-3.5 (per cited work).",
            "limitations_or_challenges": "Cited work limitations not detailed here; the current paper notes such prior studies often lack consideration for high-recall requirements in real-world SRs.",
            "comparison_to_baselines_or_humans": "Cited to have achieved human-comparable sensitivity in screening (per the referenced paper).",
            "uuid": "e3872.3",
            "source_info": {
                "paper_title": "A Novel Question-Answering Framework for Automated Citation Screening Using Large Language Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Alshami_et_al_2023",
            "name_full": "Harnessing the power of chatgpt for automating systematic review process: Methodology, case study, limitations, and future directions",
            "brief_description": "A referenced paper describing use of ChatGPT to automate aspects of the systematic review pipeline, with discussion of methodology, a case study, and limitations.",
            "citation_title": "Harnessing the power of chatgpt for automating systematic review process: Methodology, case study, limitations, and future directions.",
            "mention_or_use": "mention",
            "system_or_method_name": "ChatGPT for systematic review automation (pipeline-level)",
            "system_or_method_description": "An approach that explores using ChatGPT to automate several SR pipeline steps; cited for context but noted in this paper as not following standard citation screening norms, making direct comparison difficult.",
            "input_corpus_description": "Not specified in the current paper; discussed as a case-study in the cited work.",
            "topic_or_query_specification": "Likely natural-language prompts to ChatGPT to perform different SR subtasks; current paper notes methodological mismatch with standard citation screening.",
            "distillation_method": "Use of ChatGPT to perform or assist SR tasks; details in the cited paper.",
            "output_type_and_format": "Pipeline outputs for SR tasks (various); not detailed here.",
            "evaluation_or_validation_method": "Cited paper includes methodology, case study, and discussion of limitations; current paper remarks it did not follow a comparable citation screening norm.",
            "results_summary": "Mentioned as an initial attempt to automate SR using ChatGPT but not directly comparable to the standardized citation screening approaches evaluated in this paper.",
            "limitations_or_challenges": "Cited as not comparable due to deviations from standard citation screening norms; specifics in the original paper.",
            "comparison_to_baselines_or_humans": "Not compared directly in this paper.",
            "uuid": "e3872.4",
            "source_info": {
                "paper_title": "A Novel Question-Answering Framework for Automated Citation Screening Using Large Language Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Syriani_et_al_2023",
            "name_full": "Assessing the ability of chatgpt to screen articles for systematic reviews",
            "brief_description": "An arXiv preprint referenced for evaluating ChatGPT's ability to screen articles for systematic reviews; cited as one of few early studies assessing ChatGPT for screening tasks, but noted to lack high-recall considerations.",
            "citation_title": "Assessing the ability of chatgpt to screen articles for systematic reviews.",
            "mention_or_use": "mention",
            "system_or_method_name": "ChatGPT-based article screening assessment",
            "system_or_method_description": "Evaluation of ChatGPT for screening scientific articles for inclusion in SRs; included in related work as preliminary evidence but with shortcomings for high-recall requirements.",
            "input_corpus_description": "Not specified in this paper; details are in the cited arXiv preprint.",
            "topic_or_query_specification": "Not detailed here; presumably natural language screening prompts.",
            "distillation_method": "Zero-shot/few-shot prompting of ChatGPT to label or screen articles; exact methods in cited work.",
            "output_type_and_format": "Screening decisions; evaluation metrics likely include sensitivity/recall but not detailed here.",
            "evaluation_or_validation_method": "Referenced as limited with respect to high-recall needs; original evaluation methodology in cited paper.",
            "results_summary": "Referenced as limited in focus on high recall; included among few initial attempts to evaluate ChatGPT for SR screening.",
            "limitations_or_challenges": "Cited as lacking consideration for achieving high recall, making it less suitable for real-world SR scenarios according to the present paper.",
            "comparison_to_baselines_or_humans": "Not detailed in the current paper.",
            "uuid": "e3872.5",
            "source_info": {
                "paper_title": "A Novel Question-Answering Framework for Automated Citation Screening Using Large Language Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Guo_et_al_2023",
            "name_full": "Automated paper screening for clinical reviews using large language models",
            "brief_description": "An arXiv preprint cited as an attempt to use large language models for automated paper screening in clinical review contexts; cited among limited early investigations.",
            "citation_title": "Automated paper screening for clinical reviews using large language models.",
            "mention_or_use": "mention",
            "system_or_method_name": "LLM-based paper screening for clinical reviews",
            "system_or_method_description": "Use of large language models to automate screening of clinical review papers; included as related work but described as lacking high-recall considerations in the present paper.",
            "input_corpus_description": "Not detailed in this paper; refer to the cited preprint for corpus details.",
            "topic_or_query_specification": "Not specified here; likely natural language prompts for screening.",
            "distillation_method": "Zero-shot or few-shot LLM prompting to make screening decisions; specifics in referenced work.",
            "output_type_and_format": "Screening labels/lists; evaluation in the cited work.",
            "evaluation_or_validation_method": "Not detailed in the current paper.",
            "results_summary": "Cited among early works applying LLMs to screening; current paper notes limited attention to high-recall.",
            "limitations_or_challenges": "Described as lacking consideration for high-recall, per the current paper's critique.",
            "comparison_to_baselines_or_humans": "Not detailed here.",
            "uuid": "e3872.6",
            "source_info": {
                "paper_title": "A Novel Question-Answering Framework for Automated Citation Screening Using Large Language Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Wang_et_al_2023_query",
            "name_full": "Can chatgpt write a good boolean query for systematic review literature search?",
            "brief_description": "A study exploring ChatGPT's ability to generate Boolean search queries for systematic review literature searches; cited as an example of LLMs helping earlier SR pipeline steps (query formulation).",
            "citation_title": "Can chatgpt write a good boolean query for systematic review literature search?",
            "mention_or_use": "mention",
            "system_or_method_name": "ChatGPT-generated boolean query generation",
            "system_or_method_description": "Using ChatGPT to automatically generate or assist in writing Boolean search queries for literature retrieval in systematic reviews; referenced as prior work applying LLMs to SR tasks (search query automation).",
            "input_corpus_description": "Not applicable directly (method generates queries rather than processing a corpus), citation notes a study; details are in the cited arXiv preprint.",
            "topic_or_query_specification": "Natural language prompt to ChatGPT asking it to produce Boolean queries given a systematic review topic.",
            "distillation_method": "Prompt-engineering to produce structured Boolean queries; not a distillation of papers but a query generation aid.",
            "output_type_and_format": "Boolean search strings/queries (text) to be used in bibliographic databases.",
            "evaluation_or_validation_method": "Not detailed here; original paper likely compares generated queries to human-written queries or retrieval outcomes.",
            "results_summary": "Mentioned as an initial use of ChatGPT for automating SR search-query formulation.",
            "limitations_or_challenges": "Current paper does not detail limitations of cited work.",
            "comparison_to_baselines_or_humans": "Not detailed in this paper; original work may have compared queries empirically.",
            "uuid": "e3872.7",
            "source_info": {
                "paper_title": "A Novel Question-Answering Framework for Automated Citation Screening Using Large Language Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "SelfRefine",
            "name_full": "Selfrefine: Iterative refinement with self-feedback",
            "brief_description": "A method that iteratively prompts an LLM with its own outputs to refine answers via self-feedback; cited as a promising direction to improve LLM outputs and suggested as future opportunity for iterative QA in citation screening.",
            "citation_title": "Selfrefine: Iterative refinement with self-feedback.",
            "mention_or_use": "mention",
            "system_or_method_name": "SelfRefine (iterative self-feedback prompting)",
            "system_or_method_description": "Iterative prompting strategy where an LLM evaluates and refines its own outputs through self-generated feedback; cited as relevant literature for potential iterative improvements to the question-answering pipeline.",
            "input_corpus_description": "Not specified in the current paper (reference only).",
            "topic_or_query_specification": "Iterative natural language prompting where the model is asked to critique and improve its own responses.",
            "distillation_method": "Recursive/self-feedback prompting to refine outputs; suggested as applicable to improve screening QA loops.",
            "output_type_and_format": "Refined textual answers and improved decisions after iterations; not used in experiments of current paper.",
            "evaluation_or_validation_method": "Not evaluated within this paper; cited as supporting literature.",
            "results_summary": "Suggested as a viable method to improve LLM QA for screening (no quantitative results provided in this paper).",
            "limitations_or_challenges": "Not discussed in detail here; general concerns include cost and potential error propagation across iterations.",
            "uuid": "e3872.8",
            "source_info": {
                "paper_title": "A Novel Question-Answering Framework for Automated Citation Screening Using Large Language Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "LLM-feedback-on-papers",
            "name_full": "Can large language models provide useful feedback on research papers? a large-scale empirical analysis",
            "brief_description": "A large-scale empirical analysis exploring whether LLMs can give useful feedback on research papers; cited as evidence that LLMs can assist in research synthesis and iterative improvement of outputs.",
            "citation_title": "Can large language models provide useful feedback on research papers? a large-scale empirical analysis.",
            "mention_or_use": "mention",
            "system_or_method_name": "LLM-based feedback for research papers",
            "system_or_method_description": "Using LLMs to provide critique and feedback on research papers; paper cited to support the potential for LLMs to assist iterative improvement and explanation in SR workflows.",
            "input_corpus_description": "Not specified here; referenced work likely used a corpus of research papers for evaluation.",
            "topic_or_query_specification": "Natural language prompts requesting feedback or critique on manuscript content.",
            "distillation_method": "LLM-generated critique and feedback, potentially enabling iterative refinements of outputs; cited as motivation.",
            "output_type_and_format": "Feedback comments and suggested revisions; not applied experimentally in this paper.",
            "evaluation_or_validation_method": "Referenced work conducted large-scale empirical analysis; not reproduced here.",
            "results_summary": "Cited to suggest LLMs can provide useful feedback for refining outputs and improving trust/transparency.",
            "limitations_or_challenges": "Not elaborated in the current paper.",
            "uuid": "e3872.9",
            "source_info": {
                "paper_title": "A Novel Question-Answering Framework for Automated Citation Screening Using Large Language Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "SelfConvinced",
            "name_full": "Self-convinced prompting: Few-shot question answering with repeated introspection",
            "brief_description": "A prompting strategy that uses repeated introspection and self-evaluation by an LLM to improve few-shot QA performance; cited as a related technique for iterative improvement.",
            "citation_title": "Self-convinced prompting: Fewshot question answering with repeated introspection.",
            "mention_or_use": "mention",
            "system_or_method_name": "Self-Convinced Prompting (repeated introspection)",
            "system_or_method_description": "Repeated introspection prompting where the LLM is induced to reconsider and refine its chain-of-thought or answers across steps; mentioned as candidate technique to improve QA in SR screening.",
            "input_corpus_description": "Not specified in this paper.",
            "topic_or_query_specification": "Natural language prompts that request iterative introspection/refinement.",
            "distillation_method": "Chain-of-thought style repeated introspection and self-correction via prompting; not implemented in the current experiments but suggested.",
            "output_type_and_format": "Refined textual answers and potentially more accurate labels; no experimental data in this paper.",
            "evaluation_or_validation_method": "Not evaluated here; referenced as supporting literature.",
            "uuid": "e3872.10",
            "source_info": {
                "paper_title": "A Novel Question-Answering Framework for Automated Citation Screening Using Large Language Models",
                "publication_date_yy_mm": "2023-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Large language models are effective text rankers with pairwise ranking prompting.",
            "rating": 2
        },
        {
            "paper_title": "Large language models are zero-shot rankers for recommender systems.",
            "rating": 2
        },
        {
            "paper_title": "Large language model demonstrates human-comparable sensitivity in initial screening of systematic reviews: A semi-automated strategy using gpt-3.5.",
            "rating": 2
        },
        {
            "paper_title": "Harnessing the power of chatgpt for automating systematic review process: Methodology, case study, limitations, and future directions.",
            "rating": 2
        },
        {
            "paper_title": "Assessing the ability of chatgpt to screen articles for systematic reviews.",
            "rating": 2
        },
        {
            "paper_title": "Automated paper screening for clinical reviews using large language models.",
            "rating": 2
        },
        {
            "paper_title": "Can chatgpt write a good boolean query for systematic review literature search?",
            "rating": 2
        },
        {
            "paper_title": "Selfrefine: Iterative refinement with self-feedback.",
            "rating": 1
        },
        {
            "paper_title": "Can large language models provide useful feedback on research papers? a large-scale empirical analysis.",
            "rating": 1
        },
        {
            "paper_title": "Self-convinced prompting: Fewshot question answering with repeated introspection.",
            "rating": 1
        }
    ],
    "cost": 0.0213905,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>A Novel Question-Answering Framework for Automated Citation Screening Using Large Language Models</h1>
<p>Opeoluwa Akinseloyin, ${ }^{1}$ Xiaorui Jiang ${ }^{1}$ and Vasile Palade ${ }^{1}$<br>${ }^{1}$ Centre for Computational Science and Mathematical Modelling, Coventry University, Puma Way, CV1 2TT, Coventry, United Kingdom<br>*Xiaorui Jiang. xiaorui.jiang@coventry.ac.uk</p>
<p>FOR PUBLISHER ONLY Received on Date Month Year; revised on Date Month Year; accepted on Date Month Year</p>
<h4>Abstract</h4>
<p>Objective: This paper aims to address the challenges in citation screening (a.k.a. abstract screening) within Systematic Reviews (SR) by leveraging the zero-shot capabilities of large language models, particularly ChatGPT. Methods: We employ ChatGPT as a zero-shot ranker to prioritize candidate studies by aligning abstracts with the selection criteria outlined in an SR protocol. Citation screening was transformed into a novel question-answering (QA) framework, treating each selection criterion as a question addressed by ChatGPT. The framework involves breaking down the selection criteria into multiple questions, properly prompting ChatGPT to answer each question, scoring and re-ranking each answer, and combining the responses to make nuanced inclusion or exclusion decisions. Results: Large-scale validation was performed on the benchmark of CLEF eHealth 2019 Task 2: Technology Assisted Reviews in Empirical Medicine. Across 31 datasets of four categories of SRs, the proposed QA framework consistently outperformed other zero-shot ranking models. Compared with complex ranking approaches with iterative relevance feedback and fine-tuned deep learning-based ranking models, our ChatGPT-based zero-shot citation screening approaches still demonstrated competitive and sometimes better results, underscoring their high potential in facilitating automated systematic reviews. Conclusion: Investigation justified the indispensable value of leveraging selection criteria to improve the performance of automated citation screening. ChatGPT demonstrated proficiency in prioritizing candidate studies for citation screening using the proposed QA framework. Significant performance improvements were obtained by re-ranking answers using the semantic alignment between abstracts and selection criteria. This further highlighted the pertinence of utilizing selection criteria to enhance citation screening.</p>
<p>Key words: Automated Systematic Review, Citation Screening, ChatGPT, Zero-Shot Ranking, Question Answering</p>
<h2>Introduction</h2>
<p>A Systematic Review (SR) in medical research is the highest form of knowledge synthesis of all available medical evidence from relevant publications on a specific topic. SR follows a principled pipeline, including candidate study retrieval, primary study selection, quality assessment, data extraction, data synthesis, meta-analysis, and reporting [1]. Because of its thoroughness and reliability, SR underpins evidence-based medicine [2]. It shapes medical research and practice by informing researchers of the state-of-the-art knowledge and knowledge gaps as well as health practitioners and policymakers of the best clinical practice [3].</p>
<p>SR also faces tremendous challenges at each step. For instance, it is time-consuming, expensive and resource-intensive to select primary studies, a.k.a. citation screening, due to the massive volume of retrieved candidate studies, often at tens of thousands $[4,5]$. It is further worsened by involving multiple human annotators, which is required to reduce bias and disparities [6]. This compound complexity calls for innovative solutions to automate or semi-automate citation screening [1] to minimize the time delays and costs of this manual screening tasks [7], which is the focus of the current paper. Figure 1 shows an example of citation screening, where the abstract of an included study is matched against the selection criteria defined in the SR protocol.</p>
<p>Machine learning has been the focus of research in automating citation screening [1, 7, 8]. Firstly, a small set of studies are selected for human annotation, and then a classifier is trained. Typically, active learning is adopted to improve the classifier iteratively. Obviously, the quality of the initial annotations plays an important role. However, choosing initial annotations is a problem of zero-shot setting and has not been explored at all. Another disadvantage is that this approach is not generalisable, and each SR topic requires training a bespoke classifier from scratch.</p>
<p>An alternative perspective was to treat citation screening as a ranking problem a.k.a. reference prioritisation [7], incorporating approaches from the information retrieval (IR) community [9, 10, $11,12,13,14,15,16]$. One advantage of this approach is that it can utilise additional information about an SR, which is converted into queries to enhance screening performance. Such information could be review title [9, 10], original Boolean queries (for candidate study retrieval) [17], research objectives [18, 16], or a set of seed studies [15, 19]. Another advantage is the possibility of training a cross-topic ranker to generalise to diverse SR topics.</p>
<p>The above analysis motivated us to explore the emerging capabilities of Large Language Models (LLMs), particularly ChatGPT in the current paper, to facilitate citation screening. Indeed, the recent successes in text ranking [20, 21] suggest LLMs potentially could be used as an alternative AI-based reviewer due to their strong zero-shot capabilities [22]. This could either save at least one human reviewer's time or, less radically, suggest a good initial training set for human verification.</p>
<p>In addition, we witness a severe lack of study about using selection criteria in automated citation screening (except [23]). Indeed, it is the selection criteria that set up the grounds for human reviewers' decision-making. Unfortunately, only a few studies initiated similar attempts [23, 24, 25], and neither the effectiveness of their methods nor the comprehensiveness of their experiments could provide convincing conclusions about the feasibility of LLMs in this task. The current paper presents a pioneering LLM-based framework for facilitating automated citation screening to fill this gap.</p>
<p>Our contributions can be summarised in three folds. (1) We proposed the first comprehensive LLM-assisted questionanswering framework for automated citation screening in a zeroshot setting. (2) We developed the first generalisable approach to utilising selection criteria to enhance citation screening performance. (3) We performed the first comprehensive empirical study on well-known benchmark datasets and demonstrated the high potential of the proposed approach for citation screening.</p>
<h2>Background Study</h2>
<h2>Automating in Citation Screening</h2>
<p>Efforts to automate systematic reviews using machine learning have surged recently. Kitchenham and Charters' presented a good survey of such attempts in software engineering [26]. In evidencebased medicine, Cohen et al. was the seminal work of citation screening using machine learning [27], while Marshall and Wallace advocated active learning techniques for citation screening [28]. Examples like RobotReviewer [29, 30] and TrialStreamer [31] showcased the power of integrating AI into the review process, with RobotReviewer claiming to reach accuracy comparable to human reviewers. Despite the progress, challenges persist, including
labour-intensive labelling and the risk of overlooking relevant studies [32]. Acknowledging the limitation of full automation, tools like Rayyan and Abstracker leverage natural language processing (NLP) algorithms to partially automate article screening [33].</p>
<h2>Machine Learning for Citation Screening</h2>
<p>The biggest challenge is handling large document volumes, particularly in non-randomized controlled trials lacking database filters [34]. For instance, EPPI-Centre reviews often screen over 20,000 documents, necessitating more efficient approaches [35]. Efforts include refining search queries, balancing precision and recall, and leveraging resource-efficient recall-maximizing models with NLP [36].</p>
<p>The initial approach involves training a classifier to make explicit include/exclude decisions [27, 36, 37, 38, 39, 40, 41]. Many classifiers using this approach inherently generate a confidence score indicating the likelihood of inclusion or exclusion (similar to the ranking in the second approach). Generally, this approach requires a labelled dataset for training, hindering the assessment of work reduction until after manual screening. Research within this paradigm primarily focuses on enhancing feature extraction methods [27, 39] and refining classifiers [40]. Van Dinter et al. [8] analyzed 41 studies in medicine and software engineering, revealing Support Vector Machines and Bayesian Networks as standard models and Bag of Words and TF-IDF as prevalent natural language processing techniques. Despite advancements, a dearth of deep neural network models explicitly designed for the systematic review screening phase is noted. The most prominent challenges include handling extreme data imbalance favouring (at least close to) total recall of relevant studies.</p>
<h2>Ranking Approaches to Citation Screening</h2>
<p>The second approach entails utilizing a ranking or prioritization system $[9,10,11,12,13,14,15,16,35,42]$. This approach might necessitate manual screening by a reviewer until a specified criterion is met. This approach can also reduce the number of items needed to be screened when a cut-off criterion is properly established [35, 42, 43]. In addition to reducing the number needed to screen, other benefits of this approach include enhancing reviewers' understanding of inclusion criteria early in the process, starting full-text retrieval sooner, and potentially speeding up the screening process as confidence in relevance grows [7]. This prioritization approach also aids review updates, enabling quicker assimilation of current developments. Various studies reported benefits from prioritization for workflow improvement, emphasizing efficiency beyond reducing title and abstract screening workload $[44,45]$.</p>
<h2>Active learning in Citation Screening</h2>
<p>It's crucial to note that the last approach, active learning, aligns with both strategies above [36, 35, 46]. This involves an iterative process to enhance machine predictions by interacting with reviewers. The machine learns from an initial set of include/exclude decisions human reviewers provide. Reviewers then judge on a few new samples, and the machine adapts its decision rule based on this feedback. This iterative process continues until meeting a specified stopping criterion. While the classifier makes final decisions for unscreened items, human screeners retain control over the training process and the point at which manual screening concludes. Wallace et al. implement</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1: Illustration of LLM-assisted automated citation screening.</p>
<p>active learning-based article screening using Support Vector Machines [36]. Notable tools include Abstrackr [38] and ASReview [47]. Various active learning strategies existed [7]. For instance, Marshall and Wallace [28] proposed a variant based on certainty, continuously training the classifier on manually screened articles and reordering unseen articles based on predicted relevance.</p>
<h3>Large Language Models for Citation Screening</h3>
<p>Recent advancements in LLMs, notably demonstrated by ChatGPT, have brought about a revolutionary paradigm shift across disciplines [48, 49]. LLMs have shown impressive generalisability across diverse domains and strong zero-/few-shot reasoning capabilities [48, 50]. Leveraging LLMs, like ChatGPT, holds promise for SRs, which however remains underexplored [7, 8]. This gap underscores the need for a comprehensive investigation into LLMs' potential in automating SRs, e.g., citation screening in the current paper.</p>
<p>There are some initial attempts to evaluate ChatGPT in automated SR, such as automating search queries [51]. Alshami et al. [52] utilized ChatGPT for automating the SR pipeline; however, their approach did not follow the norm of citation screening, making it incomparable to existing methods. Notably, the effectiveness of ChatGPT in automating citation screening has received limited attention, with only two studies [53, 54], which, unfortunately, lack consideration for achieving high recall, making them less suitable for real-world scenarios.</p>
<h3>Materials and Methods</h3>
<h4>Overview</h4>
<p>Our framework utilizes ChatGPT's zero-shot learning to assess if a candidate study's abstract aligns with the SR protocol's selection criteria. These criteria outline aspects of the selected studies. The provided sentence explains that in Figure 1, the red-highlighted text, "We included qualitative studies," serves as an example illustrating an inclusion criterion. This criterion specifies that only studies with a qualitative nature will be selected. Theoretically, all inclusion criteria should be met for the study to be included in the SR.</p>
<p>Our novel method frames automated citation screening as a question-answering (QA) task. Each selection criterion is treated as a question to be addressed using LLMs like ChatGPT. These models have showcased impressive question-answering abilities across diverse domains and tasks, including encoding clinical knowledge and achieving success in medical licensing exams [55, 56, 57, 58].</p>
<p>An initial experiment using the whole selection criteria as one comprehensive question (Figure 2a) proved ineffective. LLMs excel at answering focused and clearly described questions. Hence, our improved approach involves breaking down the selection criteria into several <em>K</em> questions (the LM-based Query Generator component in Figure 1), prompting LLMs to answer each question (the LM-based Question Answerer in Figure 1), and combining the answers for each question (the Ensembler in Figure 1).</p>
<p>Figure 2b details our proposed QA framework for citation screening. We begin with a Question Generator to convert the selection criteria into a set of questions. Optionally, a Question Analyser may be employed to correctly combine the question answers, considering, for instance, that answers <em>A</em><sup>1</sup> and <em>A</em><sup>2</sup> for questions <em>Q</em><sup>1</sup> and <em>Q</em><sup>2</sup> represent an inclusion and exclusion criterion, respectively, and the correct combination is <em>A</em><sup>1</sup> ∩ <em>A</em><sup>2</sup>. Subsequently, each question is addressed by a trained Question Answerer to determine if the corresponding selection criterion is met, with each answer converted into a numeric score. Optionally, an Answer Re-ranking component can either be pre-trained on a large corpus of SRs or task-specifically trained with human reviewers' feedback. Finally, the Ensemble component combines the answers to all questions, and a final decision is made using predefined rules. Each component will be detailed in subsequent sections.</p>
<h4>Question Generation</h4>
<p>A substantial body of research exists on automated question generation from natural language text [59]. These methods often rely on manually crafted rules or a trained model, typically a fine-tuned pre-trained language model. While these question generation models have demonstrated utility in domain-specific tasks, such as generating questions about product descriptions for matching purchase inquiries [60] or creating questions about academic materials to assess learning outcomes [61], generalizing them to the vast diversity of SR topics presents challenges. Therefore, we entrust the question generation task to ChatGPT.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 2: Methodological framework for LLM-assisted automation screening.</p>
<p>A naive approach to question generation involves prompting ChatGPT to generate questions from the given paragraph about the selection criteria of a systematic review. However, this uncontrolled method often generates numerous questions, many subsumed by others or deemed too trivial to be meaningful.</p>
<p>To enhance the quality of generated questions, we constrained ChatGPT to produce no more than $K$ questions, aiming to minimize redundancy. Based on an analysis of the lengths of selection criteria in our dataset's SRs, $K=5$ proved sufficient for most SRs. Each sentence in the selection criteria often aligns well with a distinct criterion. In rare cases with more than 5 sentences, ChatGPT intelligently combined two sentences into one question. Figure 3a depicts the utilized prompt, and an example is shown in Figure 1.</p>
<p>Role: You are a researcher screening titles and abstracts of scientific papers.
Task: Using the inclusion criteria in the brackets generate 5 unique yes or no questions which encompasses the entire inclusion criteria without any duplicate or unnecessary questions to ascertain if papers meets the inclusion criteria!
Criteria: (criteria)
(a) Prompt for question generation</p>
<p>Role: You are a researcher screening titles and abstracts of scientific papers for the systematic review '(review_title)'
Task: Analyse the abstract below within the brackets and answer the question below. Taking a step-by-step approach towards reasoning and answering the question. The answer should be in either a positive, neutral, or negative sentiment format.
Abstract: (abstract)
Question: (question)
(b) Prompt for question answering</p>
<p>Fig. 3: Prompt design for LLM-assisted automated citation screening</p>
<h2>Question Answering</h2>
<p>The Question Answerer evaluates the relevance of each abstract to every selection criterion, formulated as Yes/No questions. We prompt ChatGPT to return three types of responses Figure 3b:</p>
<ul>
<li>Positive: The abstract explicitly addresses the question, offering information that aligns with the criteria posed by the question.</li>
<li>Neutral: The information in the abstract is inadequate or too ambiguous for ChatGPT to derive a confirmative answer.</li>
<li>Negative: A clear NO answer to the question, indicating irrelavance to the specified criteria.</li>
</ul>
<h2>Answer Representation Approaches</h2>
<p>Two distinct techniques represent answers, namely the Hard Answer and Soft Answer methods. These methods conceptualize answer representation as a generative sentiment classification problem, leveraging the capabilities of the BART model inspired by its recent successes in various sentiment classification tasks [62].</p>
<ul>
<li>Hard Answer: This method involves BART determining the sentiment category of each answer (Positive, Neutral, Negative). Traditionally, rejecting an abstract occurred if one question had a Negative answer, leading to low recall due to small errors in ChatGPT responses. Instead, we convert the discrete sentiment categories to semantic scores (e.g., 1, 0.5, and 0), enabling the Ensemble to combine answers into a final decision.</li>
<li>Soft Answer: ChatGPT often justifies its answer, contributing to quantifying its confidence level in the provided answer. In the Soft Answer method, the sentiment score for each answer is the probability of BART classifying the ChatGPT-generated answer sentence as positive.</li>
</ul>
<h2>Decision Engine</h2>
<h2>Ensemble</h2>
<p>The answer scores for each selection criterion are "averaged." This mean score provides a quantitative representation of the relevance of the abstract. Candidate studies are then ranked in descending order based on these mean scores. To enhance screening further, a significant contribution involves re-ranking candidate studies based on how well abstracts are semantically aligned with the selection criteria.</p>
<h2>Re-ranking</h2>
<p>Several methods are available for embedding selection criteria and abstracts. Given the emphasis on the capabilities of LLMs in this paper, GPT Embeddings [63] are chosen. Two approaches to reranking are defined: Abstract-level re-ranking and answer-level reranking.</p>
<ul>
<li>Abstract-Level Re-Ranking: This method first aggregates the mean answer score across all questions and then averages the mean score with the similarity score.</li>
<li>Answer-Level Re-Ranking: In this more advanced method, the cosine similarity evaluates how well an abstract aligns with each generated question, enhancing the answers to each selection criterion. Each Yes/No question is matched against the abstract, and the cosine similarity is averaged with the corresponding answer score. This results in $K$ re-ranked answer scores. Then the $K$ scores are averaged, and the mean score is</li>
</ul>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 4: Example of handing exclusion criteria.
further averaged with the overall alignment between selection criteria and abstract. This hierarchical ensemble effectively enhances the overall precision of document re-ranking by considering confidence in answering each question, aligning with each selection criterion, and adhering holistically to selection criteria.</p>
<h2>Experimental Setup</h2>
<h2>Dataset and Evaluation</h2>
<p>This study utilized datasets of CLEF eHealth 2019 Task 2: Technology Assisted Reviews in Empirical Medicine (TAR2019). This dataset provides valuable insights into the prevailing scientific consensus on various topics, making it a suitable resource for evaluating reranking methodologies in systematic reviews [64].</p>
<p>We employed the TAR2019 test set comprising 31 SRs categorized into Intervention (20), DTA (8), Qualitative (2), and Prognosis (1). We refrained from using the training set, aiming to highlight the effectiveness of our zero-shot methodology that eliminates the need for prior training [65, 25].</p>
<p>We used the review titles from the TAR2019 datasets to identify selection criteria. Seven evaluation metrics were employed, including the rank position of the last relevant document ( $L_{-} R e l$ ), Mean Average Precision $(A P)$, Recall at $k \%(k=5,10,20$, 30), and Work Saved Over Sampling (WSS) at $k \%(k=95 \%$, $100 \%)$. Notably, WSS@k measures the screening workload saved by halting the examination process once $k \%$ of relevant documents are identified, compared to screening the entire document set [27].</p>
<h2>Baseline Model</h2>
<p>The baseline models, serving as a comparative benchmark, were based on submissions to the TAR2019 workshop [66], which encompass UvA [67], UNIPD [68], and Sheffield [17]. Additionally, we considered the nine models evaluated by Wang et al [25]. Unlike our fully automated model, many workshop submissions employ an iterative ranking system, making them semi-automated. To comprehensively assess performance, we implemented two IR baselines of our own. One is cosine similarity between selection criteria and abstract based on GPT embeddings [63], named GPT_Cosine_Similarity. The other is BM25 [69], using selection criteria as a query. The variants of our own approach are summarised below:</p>
<ul>
<li>GPT_QA_Soft/Hard: Soft/Hard answer representation, without re-ranking.</li>
<li>GPT_QA_Soft/Hard_Abstract_Level: Soft/Hard answer representation, with abstract-level re-ranking.</li>
<li>GPT_QA_Soft/Hard_Answer_Level: Soft/Hard answer representation, with answer-level re-ranking.
am</li>
</ul>
<h2>Results</h2>
<h2>Prognosis</h2>
<p>Our proposed methods demonstrated promising results on the Prognosis dataset (Table 1). Notably, in terms of $L_{\text {Rel }}$, for which a lower value signifies superior performance, answer-level re-ranking methods showcased the most impressive results among our proposed methods: 2333 for GPT_QA_Soft_Answer_Level and 2373 for GPT_QA_Hard_Answer_Level. Our methods also achieved MAP scores from 0.350 to 0.430 , underscoring the models' efficiency in prioritizing candidate studies. This outshined numerous IR methods (UNIPD and Sheffield variants).</p>
<p>The proposed methods sustained their superiority in $R @ k \%$. When $k \in{5,10}$, our re-ranking methods (the last four rows in Table 1) consistently outperformed UNIPD and Sheffield submissions. A promising finding was that our best re-ranking method successfully suggested $65 \%$ of total positive samples (included studies) for classifier training when only $10 \%$ of total samples needed to be verified by human reviewers. This is a testament to the capacity of selecting positive samples from highly imbalanced data. The best $W S S @ 95$ of our zeroshot approaches reached $55.5 \%$ on Prognosis, and notably WSS@100 was significantly better than most baselines except 2018_stem_original_p50_t1500 which used relevance feedback.</p>
<p>From the holistic view of the evaluation metrics, our answerlevel re-ranked models stood out as cutting-edge solutions, achieving either competitive or new state-of-the-art results.</p>
<h2>Qualitative</h2>
<p>The results are presented in Table 2. Similarly, the $L_{\text {Rel }}$ metric highlighted that both our abstract-level and answer-level reranking methods (the last four rows in Table 2) are particularly effective. The MAP results of our proposed models consistently outperformed the IR baselines of UNIPD and Sheffield. UvA showed the best performance. Note that the UvA approaches used relevance feedback to improve the ranking performance, so it was not purely a zero-shot problem in our sense. Further discussions can be found in the Discussions section.</p>
<p>Regarding recall, our models showed promising results when $k=5$, meaning that our methods identified more than half of the positive samples in the top $5 \%$ ranked list. This allows us to significantly reduce the effort in annotating the initial dataset for training a citation screener. Although some IR methods showed higher recall when $k \geq 10$, our methods outperformed all baselines in $R @ 30 \%$ and showed significant performance gains in terms of both $W S S @ 95$ and $W S S @ 100$ over all baselines,</p>
<p>Table 1. Results obtained on the Prognosis data. The zero-shot ranking models are emboldened.</p>
<table>
<thead>
<tr>
<th>Paper</th>
<th>Models</th>
<th>L_Rel</th>
<th>MAP</th>
<th>R@5\%</th>
<th>R@10\%</th>
<th>R@20\%</th>
<th>R@30\%</th>
<th>WSS@95</th>
<th>WSS@100</th>
</tr>
</thead>
<tbody>
<tr>
<td>UvA</td>
<td>abs-hh-ratio-ilps</td>
<td>2885</td>
<td>0.673</td>
<td>0.562</td>
<td>0.714</td>
<td>0.875</td>
<td>0.911</td>
<td>0.591</td>
<td>0.143</td>
</tr>
<tr>
<td></td>
<td>abs-th-ratio-ilps</td>
<td>2537</td>
<td>0.628</td>
<td>0.521</td>
<td>0.682</td>
<td>0.818</td>
<td>0.927</td>
<td>0.566</td>
<td>0.247</td>
</tr>
<tr>
<td>UNIPD</td>
<td>2018_stem_original_p10_t400</td>
<td>2967</td>
<td>0.235</td>
<td>0.214</td>
<td>0.484</td>
<td>0.812</td>
<td>0.901</td>
<td>0.567</td>
<td>0.119</td>
</tr>
<tr>
<td></td>
<td>distributed_effort_p10_t1500</td>
<td>2594</td>
<td>0.235</td>
<td>0.214</td>
<td>0.484</td>
<td>0.812</td>
<td>0.896</td>
<td>0.554</td>
<td>0.230</td>
</tr>
<tr>
<td></td>
<td>2018_stem_original_p10_t1000</td>
<td>2644</td>
<td>0.235</td>
<td>0.214</td>
<td>0.484</td>
<td>0.812</td>
<td>0.896</td>
<td>0.554</td>
<td>0.215</td>
</tr>
<tr>
<td></td>
<td>2018_stem_original_p10_t200</td>
<td>2911</td>
<td>0.242</td>
<td>0.214</td>
<td>0.536</td>
<td>0.812</td>
<td>0.901</td>
<td>0.530</td>
<td>0.135</td>
</tr>
<tr>
<td></td>
<td>2018_stem_original_p10_t500</td>
<td>2920</td>
<td>0.235</td>
<td>0.214</td>
<td>0.484</td>
<td>0.812</td>
<td>0.891</td>
<td>0.560</td>
<td>0.133</td>
</tr>
<tr>
<td></td>
<td>2018_stem_original_p10_t300</td>
<td>2955</td>
<td>0.239</td>
<td>0.214</td>
<td>0.547</td>
<td>0.818</td>
<td>0.891</td>
<td>0.556</td>
<td>0.122</td>
</tr>
<tr>
<td></td>
<td>2018_stem_original_p10_t1500</td>
<td>2578</td>
<td>0.235</td>
<td>0.214</td>
<td>0.484</td>
<td>0.812</td>
<td>0.896</td>
<td>0.554</td>
<td>0.234</td>
</tr>
<tr>
<td></td>
<td>distributed_effort_p10_t1000</td>
<td>2563</td>
<td>0.235</td>
<td>0.214</td>
<td>0.484</td>
<td>0.812</td>
<td>0.896</td>
<td>0.554</td>
<td>0.239</td>
</tr>
<tr>
<td></td>
<td>2018_stem_original_p10_t100</td>
<td>2802</td>
<td>0.259</td>
<td>0.286</td>
<td>0.562</td>
<td>0.797</td>
<td>0.891</td>
<td>0.600</td>
<td>0.168</td>
</tr>
<tr>
<td></td>
<td>baseline_bm25_t500</td>
<td>3343</td>
<td>0.071</td>
<td>0.057</td>
<td>0.130</td>
<td>0.281</td>
<td>0.422</td>
<td>0.084</td>
<td>0.007</td>
</tr>
<tr>
<td></td>
<td>distributed_effort_p10_t300</td>
<td>2964</td>
<td>0.235</td>
<td>0.214</td>
<td>0.484</td>
<td>0.812</td>
<td>0.906</td>
<td>0.567</td>
<td>0.120</td>
</tr>
<tr>
<td></td>
<td>2018_stem_original_p50_t1000</td>
<td>2556</td>
<td>0.221</td>
<td>0.214</td>
<td>0.484</td>
<td>0.740</td>
<td>0.870</td>
<td>0.571</td>
<td>0.241</td>
</tr>
<tr>
<td></td>
<td>distributed_effort_p10_t100</td>
<td>2789</td>
<td>0.252</td>
<td>0.250</td>
<td>0.568</td>
<td>0.786</td>
<td>0.875</td>
<td>0.594</td>
<td>0.172</td>
</tr>
<tr>
<td></td>
<td>2018_stem_original_p50_t200</td>
<td>2911</td>
<td>0.242</td>
<td>0.214</td>
<td>0.536</td>
<td>0.812</td>
<td>0.901</td>
<td>0.530</td>
<td>0.135</td>
</tr>
<tr>
<td></td>
<td>baseline_bm25_t1000</td>
<td>3346</td>
<td>0.070</td>
<td>0.057</td>
<td>0.130</td>
<td>0.276</td>
<td>0.396</td>
<td>0.057</td>
<td>0.006</td>
</tr>
<tr>
<td></td>
<td>distributed_effort_p10_t500</td>
<td>2708</td>
<td>0.235</td>
<td>0.214</td>
<td>0.484</td>
<td>0.812</td>
<td>0.891</td>
<td>0.566</td>
<td>0.196</td>
</tr>
<tr>
<td></td>
<td>baseline_bm25_t300</td>
<td>3350</td>
<td>0.071</td>
<td>0.057</td>
<td>0.135</td>
<td>0.276</td>
<td>0.385</td>
<td>0.104</td>
<td>0.005</td>
</tr>
<tr>
<td></td>
<td>baseline_bm25_t100</td>
<td>3350</td>
<td>0.066</td>
<td>0.047</td>
<td>0.130</td>
<td>0.255</td>
<td>0.365</td>
<td>0.059</td>
<td>0.005</td>
</tr>
<tr>
<td></td>
<td>2018_stem_original_p50_t400</td>
<td>2955</td>
<td>0.231</td>
<td>0.214</td>
<td>0.484</td>
<td>0.807</td>
<td>0.896</td>
<td>0.556</td>
<td>0.122</td>
</tr>
<tr>
<td></td>
<td>2018_stem_original_p50_t300</td>
<td>2955</td>
<td>0.239</td>
<td>0.214</td>
<td>0.547</td>
<td>0.818</td>
<td>0.891</td>
<td>0.556</td>
<td>0.122</td>
</tr>
<tr>
<td></td>
<td>2018_stem_original_p50_t100</td>
<td>2802</td>
<td>0.259</td>
<td>0.286</td>
<td>0.562</td>
<td>0.797</td>
<td>0.891</td>
<td>0.600</td>
<td>0.168</td>
</tr>
<tr>
<td></td>
<td>distributed_effort_p10_t200</td>
<td>2968</td>
<td>0.240</td>
<td>0.214</td>
<td>0.542</td>
<td>0.807</td>
<td>0.906</td>
<td>0.548</td>
<td>0.119</td>
</tr>
<tr>
<td></td>
<td>baseline_bm25_t400</td>
<td>3347</td>
<td>0.071</td>
<td>0.057</td>
<td>0.130</td>
<td>0.281</td>
<td>0.417</td>
<td>0.109</td>
<td>0.006</td>
</tr>
<tr>
<td></td>
<td>2018_stem_original_p50_t1500</td>
<td>1975</td>
<td>0.219</td>
<td>0.214</td>
<td>0.484</td>
<td>0.740</td>
<td>0.828</td>
<td>0.500</td>
<td>0.413</td>
</tr>
<tr>
<td></td>
<td>2018_stem_original_p50_t500</td>
<td>2660</td>
<td>0.228</td>
<td>0.214</td>
<td>0.484</td>
<td>0.807</td>
<td>0.891</td>
<td>0.576</td>
<td>0.210</td>
</tr>
<tr>
<td></td>
<td>baseline_bm25_t1500</td>
<td>3346</td>
<td>0.070</td>
<td>0.057</td>
<td>0.130</td>
<td>0.276</td>
<td>0.396</td>
<td>0.050</td>
<td>0.006</td>
</tr>
<tr>
<td></td>
<td>baseline_bm25_t200</td>
<td>3350</td>
<td>0.069</td>
<td>0.057</td>
<td>0.125</td>
<td>0.266</td>
<td>0.385</td>
<td>0.111</td>
<td>0.005</td>
</tr>
<tr>
<td></td>
<td>distributed_effort_p10_t400</td>
<td>2920</td>
<td>0.235</td>
<td>0.214</td>
<td>0.484</td>
<td>0.812</td>
<td>0.891</td>
<td>0.560</td>
<td>0.133</td>
</tr>
<tr>
<td>Sheffield</td>
<td>sheffield-baseline</td>
<td>2990</td>
<td>0.126</td>
<td>0.146</td>
<td>0.255</td>
<td>0.448</td>
<td>0.594</td>
<td>0.247</td>
<td>0.112</td>
</tr>
<tr>
<td></td>
<td>sheffield-relevance_feedback</td>
<td>2775</td>
<td>0.141</td>
<td>0.151</td>
<td>0.307</td>
<td>0.484</td>
<td>0.646</td>
<td>0.305</td>
<td>0.176</td>
</tr>
<tr>
<td>Proposed Method</td>
<td>GPT_Cosine_Similarity</td>
<td>3160</td>
<td>0.178</td>
<td>0.200</td>
<td>0.305</td>
<td>0.495</td>
<td>0.647</td>
<td>0.239</td>
<td>0.053</td>
</tr>
<tr>
<td></td>
<td>BM25</td>
<td>3337</td>
<td>0.074</td>
<td>0.089</td>
<td>0.132</td>
<td>0.279</td>
<td>0.416</td>
<td>0.020</td>
<td>0.000</td>
</tr>
<tr>
<td></td>
<td>GPT_QA_Soft</td>
<td>3211</td>
<td>0.350</td>
<td>0.395</td>
<td>0.563</td>
<td>0.784</td>
<td>0.832</td>
<td>0.434</td>
<td>0.037</td>
</tr>
<tr>
<td></td>
<td>GPT_QA_Hard</td>
<td>3338</td>
<td>0.315</td>
<td>0.395</td>
<td>0.395</td>
<td>0.753</td>
<td>0.753</td>
<td>0.417</td>
<td>0.060</td>
</tr>
<tr>
<td></td>
<td>GPT_QA_Soft_Abstract_Level</td>
<td>2467</td>
<td>0.417</td>
<td>0.395</td>
<td>0.647</td>
<td>0.795</td>
<td>0.879</td>
<td>0.523</td>
<td>0.261</td>
</tr>
<tr>
<td></td>
<td>GPT_QA_Hard_Abstract_Level</td>
<td>2398</td>
<td>0.417</td>
<td>0.395</td>
<td>0.637</td>
<td>0.789</td>
<td>0.884</td>
<td>0.543</td>
<td>0.282</td>
</tr>
<tr>
<td></td>
<td>GPT_QA_Soft_Answer_Level</td>
<td>2373</td>
<td>0.430</td>
<td>0.400</td>
<td>0.653</td>
<td>0.800</td>
<td>0.884</td>
<td>0.543</td>
<td>0.289</td>
</tr>
<tr>
<td></td>
<td>GPT_QA_Hard_Answer_Level</td>
<td>2333</td>
<td>0.429</td>
<td>0.400</td>
<td>0.642</td>
<td>0.789</td>
<td>0.884</td>
<td>0.555</td>
<td>0.301</td>
</tr>
</tbody>
</table>
<p>including the relevance feedback approaches by UvA. The results are overall encouraging, showing that the proposed QA-based prioritization framework potentially applies well to qualitative SRs, too. However, a conclusive statement requires more empirical studies, which will be one direction of our future work.</p>
<h2>Diagnostic Test Accuracy (DTA)</h2>
<p>Table 3 shows satisfactory results on DTA. The top-5\% ranked list of our best models covered as many as $45 \%$ positive samples. They outperformed all IR methods except abs-hh-ratio-ilps by UvA, leading to better MAP over the latter, and approached the fine-tuned models in $R @ 5 \%$. This implies the feasibility of our approaches for reducing the human effort in annotating an initial training set with a reasonable number of included studies, compared to random sampling, which requires annotating $45 \%$ of total samples to reach the same size of included studies. Although our models underperformed the best UvA variant in $R @ 5 \%$, they started to excel the latter when $k \geq 20$, resulting in a better MAP and significantly higher $W S S$.</p>
<p>On the other hand, we notice that although some UNIPD and Sheffield submissions performed better than our best models in recall (when $k&gt;10$ ) and $W S S$ (when $R=95$ or 100), our methods were consistently stronger than the baselines without relevance feedback (the rows in bold) by large margins. This justifies the superiority of LLMs as a zero-shot citation screening method. We anticipate that the screening performance can be further improved through an iterative question-answering conversion by providing proper feedback to LLMs. Similar ideas have been proven effective</p>
<p>Table 2. Results obtained on the Qualitative data. The zero-shot ranking models are emboldened.</p>
<table>
<thead>
<tr>
<th>Paper</th>
<th>Models</th>
<th>L_Rel</th>
<th>MAP</th>
<th>R@5\%</th>
<th>R@10\%</th>
<th>R@20\%</th>
<th>R@30\%</th>
<th>WSS@95</th>
<th>WSS@100</th>
</tr>
</thead>
<tbody>
<tr>
<td>UvA</td>
<td>abs-hh-ratio-ilps</td>
<td>1796</td>
<td>0.204</td>
<td>0.478</td>
<td>0.655</td>
<td>0.876</td>
<td>0.929</td>
<td>0.417</td>
<td>0.397</td>
</tr>
<tr>
<td></td>
<td>abs-th-ratio-ilps</td>
<td>2564</td>
<td>0.187</td>
<td>0.487</td>
<td>0.628</td>
<td>0.805</td>
<td>0.920</td>
<td>0.398</td>
<td>0.215</td>
</tr>
<tr>
<td>UNIPD</td>
<td>2018_stem_original_p10_t400.out</td>
<td>2547</td>
<td>0.109</td>
<td>0.496</td>
<td>0.717</td>
<td>0.779</td>
<td>0.894</td>
<td>0.302</td>
<td>0.183</td>
</tr>
<tr>
<td></td>
<td>distributed_effort_p10_t1500.out</td>
<td>2544</td>
<td>0.109</td>
<td>0.496</td>
<td>0.743</td>
<td>0.770</td>
<td>0.885</td>
<td>0.268</td>
<td>0.168</td>
</tr>
<tr>
<td></td>
<td>2018_stem_original_p10_t1000.out</td>
<td>2662</td>
<td>0.109</td>
<td>0.496</td>
<td>0.743</td>
<td>0.770</td>
<td>0.885</td>
<td>0.273</td>
<td>0.141</td>
</tr>
<tr>
<td></td>
<td>2018_stem_original_p10_t200.out</td>
<td>2934</td>
<td>0.089</td>
<td>0.478</td>
<td>0.522</td>
<td>0.699</td>
<td>0.805</td>
<td>0.216</td>
<td>0.101</td>
</tr>
<tr>
<td></td>
<td>2018_stem_original_p10_t500.out</td>
<td>2535</td>
<td>0.109</td>
<td>0.496</td>
<td>0.743</td>
<td>0.770</td>
<td>0.894</td>
<td>0.301</td>
<td>0.185</td>
</tr>
<tr>
<td></td>
<td>2018_stem_original_p10_t300.out</td>
<td>2660</td>
<td>0.103</td>
<td>0.496</td>
<td>0.655</td>
<td>0.752</td>
<td>0.858</td>
<td>0.303</td>
<td>0.159</td>
</tr>
<tr>
<td></td>
<td>2018_stem_original_p10_t1500.out</td>
<td>2534</td>
<td>0.109</td>
<td>0.496</td>
<td>0.743</td>
<td>0.770</td>
<td>0.885</td>
<td>0.268</td>
<td>0.170</td>
</tr>
<tr>
<td></td>
<td>distributed_effort_p10_t1000.out</td>
<td>2469</td>
<td>0.109</td>
<td>0.496</td>
<td>0.743</td>
<td>0.770</td>
<td>0.885</td>
<td>0.295</td>
<td>0.199</td>
</tr>
<tr>
<td></td>
<td>2018_stem_original_p10_t100.out</td>
<td>2996</td>
<td>0.071</td>
<td>0.327</td>
<td>0.416</td>
<td>0.637</td>
<td>0.796</td>
<td>0.186</td>
<td>0.090</td>
</tr>
<tr>
<td></td>
<td>baseline_bm25_t500.out</td>
<td>2700</td>
<td>0.051</td>
<td>0.274</td>
<td>0.425</td>
<td>0.469</td>
<td>0.611</td>
<td>0.412</td>
<td>0.256</td>
</tr>
<tr>
<td></td>
<td>distributed_effort_p10_t300.out</td>
<td>2518</td>
<td>0.109</td>
<td>0.496</td>
<td>0.743</td>
<td>0.770</td>
<td>0.894</td>
<td>0.309</td>
<td>0.193</td>
</tr>
<tr>
<td></td>
<td>2018_stem_original_p50_t1000.out</td>
<td>2438</td>
<td>0.116</td>
<td>0.496</td>
<td>0.743</td>
<td>0.920</td>
<td>0.947</td>
<td>0.357</td>
<td>0.194</td>
</tr>
<tr>
<td></td>
<td>distributed_effort_p10_t100.out</td>
<td>2920</td>
<td>0.083</td>
<td>0.416</td>
<td>0.469</td>
<td>0.681</td>
<td>0.814</td>
<td>0.258</td>
<td>0.106</td>
</tr>
<tr>
<td></td>
<td>2018_stem_original_p50_t200.out</td>
<td>2934</td>
<td>0.089</td>
<td>0.478</td>
<td>0.522</td>
<td>0.699</td>
<td>0.805</td>
<td>0.216</td>
<td>0.101</td>
</tr>
<tr>
<td></td>
<td>baseline_bm25_t1000.out</td>
<td>3040</td>
<td>0.055</td>
<td>0.274</td>
<td>0.425</td>
<td>0.496</td>
<td>0.788</td>
<td>0.239</td>
<td>0.101</td>
</tr>
<tr>
<td></td>
<td>distributed_effort_p10_t500.out</td>
<td>2641</td>
<td>0.109</td>
<td>0.496</td>
<td>0.743</td>
<td>0.770</td>
<td>0.894</td>
<td>0.295</td>
<td>0.162</td>
</tr>
<tr>
<td></td>
<td>baseline_bm25_t300.out</td>
<td>2697</td>
<td>0.049</td>
<td>0.274</td>
<td>0.372</td>
<td>0.451</td>
<td>0.628</td>
<td>0.294</td>
<td>0.257</td>
</tr>
<tr>
<td></td>
<td>baseline_bm25_t100.out</td>
<td>2700</td>
<td>0.056</td>
<td>0.301</td>
<td>0.389</td>
<td>0.637</td>
<td>0.743</td>
<td>0.399</td>
<td>0.256</td>
</tr>
<tr>
<td></td>
<td>2018_stem_original_p50_t400.out</td>
<td>2566</td>
<td>0.109</td>
<td>0.496</td>
<td>0.717</td>
<td>0.779</td>
<td>0.894</td>
<td>0.293</td>
<td>0.174</td>
</tr>
<tr>
<td></td>
<td>2018_stem_original_p50_t300.out</td>
<td>2687</td>
<td>0.103</td>
<td>0.496</td>
<td>0.655</td>
<td>0.752</td>
<td>0.858</td>
<td>0.290</td>
<td>0.147</td>
</tr>
<tr>
<td></td>
<td>2018_stem_original_p50_t100.out</td>
<td>2996</td>
<td>0.071</td>
<td>0.327</td>
<td>0.416</td>
<td>0.637</td>
<td>0.796</td>
<td>0.186</td>
<td>0.090</td>
</tr>
<tr>
<td></td>
<td>distributed_effort_p10_t200.out</td>
<td>2762</td>
<td>0.104</td>
<td>0.496</td>
<td>0.673</td>
<td>0.761</td>
<td>0.867</td>
<td>0.303</td>
<td>0.135</td>
</tr>
<tr>
<td></td>
<td>baseline_bm25_t400.out</td>
<td>2700</td>
<td>0.052</td>
<td>0.274</td>
<td>0.434</td>
<td>0.469</td>
<td>0.619</td>
<td>0.417</td>
<td>0.256</td>
</tr>
<tr>
<td></td>
<td>2018_stem_original_p50_t1500.out</td>
<td>1970</td>
<td>0.116</td>
<td>0.496</td>
<td>0.743</td>
<td>0.920</td>
<td>0.965</td>
<td>0.356</td>
<td>0.301</td>
</tr>
<tr>
<td></td>
<td>2018_stem_original_p50_t500.out</td>
<td>2576</td>
<td>0.110</td>
<td>0.496</td>
<td>0.743</td>
<td>0.788</td>
<td>0.894</td>
<td>0.283</td>
<td>0.168</td>
</tr>
<tr>
<td></td>
<td>baseline_bm25_t1500.out</td>
<td>3039</td>
<td>0.055</td>
<td>0.274</td>
<td>0.425</td>
<td>0.496</td>
<td>0.779</td>
<td>0.240</td>
<td>0.101</td>
</tr>
<tr>
<td></td>
<td>baseline_bm25_t200.out</td>
<td>2698</td>
<td>0.053</td>
<td>0.274</td>
<td>0.381</td>
<td>0.619</td>
<td>0.726</td>
<td>0.395</td>
<td>0.256</td>
</tr>
<tr>
<td></td>
<td>distributed_effort_p10_t400.out</td>
<td>2636</td>
<td>0.109</td>
<td>0.496</td>
<td>0.743</td>
<td>0.770</td>
<td>0.894</td>
<td>0.301</td>
<td>0.165</td>
</tr>
<tr>
<td>Sheffield</td>
<td>sheffield-relevance_feedback.out</td>
<td>2940</td>
<td>0.060</td>
<td>0.274</td>
<td>0.549</td>
<td>0.717</td>
<td>0.832</td>
<td>0.185</td>
<td>0.103</td>
</tr>
<tr>
<td></td>
<td>sheffield-baseline</td>
<td>3031</td>
<td>0.051</td>
<td>0.265</td>
<td>0.451</td>
<td>0.619</td>
<td>0.743</td>
<td>0.135</td>
<td>0.082</td>
</tr>
<tr>
<td>Proposed Method</td>
<td>GPT_Cosine_Similarity</td>
<td>2256</td>
<td>0.082</td>
<td>0.173</td>
<td>0.478</td>
<td>0.559</td>
<td>0.618</td>
<td>0.303</td>
<td>0.289</td>
</tr>
<tr>
<td></td>
<td>BM25</td>
<td>2704</td>
<td>0.037</td>
<td>0.078</td>
<td>0.146</td>
<td>0.191</td>
<td>0.259</td>
<td>0.135</td>
<td>0.135</td>
</tr>
<tr>
<td></td>
<td>GPT_QA_Soft</td>
<td>1786</td>
<td>0.157</td>
<td>0.537</td>
<td>0.614</td>
<td>0.673</td>
<td>0.959</td>
<td>0.599</td>
<td>0.484</td>
</tr>
<tr>
<td></td>
<td>GPT_QA_Hard</td>
<td>1784</td>
<td>0.110</td>
<td>0.478</td>
<td>0.582</td>
<td>0.900</td>
<td>0.959</td>
<td>0.650</td>
<td>0.485</td>
</tr>
<tr>
<td></td>
<td>GPT_QA_Soft_Abstract_Level</td>
<td>1683</td>
<td>0.159</td>
<td>0.509</td>
<td>0.605</td>
<td>0.673</td>
<td>0.959</td>
<td>0.595</td>
<td>0.511</td>
</tr>
<tr>
<td></td>
<td>GPT_QA_Hard_Abstract_Level</td>
<td>1675</td>
<td>0.200</td>
<td>0.509</td>
<td>0.609</td>
<td>0.678</td>
<td>0.959</td>
<td>0.608</td>
<td>0.514</td>
</tr>
<tr>
<td></td>
<td>GPT_QA_Soft_Answer_Level</td>
<td>1682</td>
<td>0.159</td>
<td>0.505</td>
<td>0.600</td>
<td>0.673</td>
<td>0.959</td>
<td>0.576</td>
<td>0.507</td>
</tr>
<tr>
<td></td>
<td>GPT_QA_Hard_Answer_Level</td>
<td>1684</td>
<td>0.157</td>
<td>0.514</td>
<td>0.600</td>
<td>0.678</td>
<td>0.959</td>
<td>0.601</td>
<td>0.507</td>
</tr>
</tbody>
</table>
<p>on different NLP tasks [70, 71, 72]. Meanwhile, it is worth noting the DTA dataset has been generally considered a very difficult dataset [73].</p>
<h2>Intervention</h2>
<p>Table 4 shows the results on Intervention. Our methods exhibited exceptional performance across all metrics. The high recall values at different thresholds underscored the effectiveness of our proposed model, consistently outperforming all models except BioBERT-Tune. Our best models, namely the answerlevel re-ranking methods GPT_QA_HARD_Answer_Level and GPT_QA_Soft_Answer_Level, also achieved better MAP results than most baselines, and the abstract-level re-ranking method GPT_QA_HARD_Answer_Level rivalled the robust UvA systems.</p>
<p>Notably, our best models excelled in $L_{\text {Rel }}$, and the results of $W S S @ 95$ and $W S S @ 100$ outperformed most baseline models. In summary, the comprehensive assessment across diverse metrics and datasets reinforced the standing of our proposed methods as state-of-the-art solutions.</p>
<p>medRxiv preprint doi: https://doi.org/10.1101/2023.12.17.23300102; this version posted December 18, 2023. The copyright holder for this preprint (which was not certified by the American Medical Association's Association license to display the preprint in Page 8 of 14 perpetuity. All rights reserved. No reuse allowed without permission.</p>
<table>
<thead>
<tr>
<th>8</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>Table 3. Results obtained on the DTA data. The zero-shot ranking models are emboldened.</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2</td>
<td>Paper</td>
<td>Models</td>
<td>L.Rel</td>
<td>MAP</td>
<td>R@5%</td>
<td>R@10%</td>
<td>R@20%</td>
<td>R@30%</td>
<td>WSS@95</td>
<td>WSS@100</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>3</td>
<td>UvA</td>
<td>abs-hh-ratio-ilps</td>
<td>2420</td>
<td>0.493</td>
<td>0.589</td>
<td>0.682</td>
<td>0.789</td>
<td>0.834</td>
<td>0.406</td>
<td>0.304</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>4</td>
<td></td>
<td>abs-th-ratio-ilps</td>
<td>2676</td>
<td>0.399</td>
<td>0.418</td>
<td>0.536</td>
<td>0.661</td>
<td>0.734</td>
<td>0.312</td>
<td>0.253</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>5</td>
<td>UNIPD</td>
<td>2018_stem_original_p10_t400.out</td>
<td>1190</td>
<td>0.229</td>
<td>0.448</td>
<td>0.634</td>
<td>0.818</td>
<td>0.895</td>
<td>0.662</td>
<td>0.512</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>6</td>
<td></td>
<td>distributed_effort_p10_t1500.out</td>
<td>1111</td>
<td>0.229</td>
<td>0.445</td>
<td>0.630</td>
<td>0.814</td>
<td>0.895</td>
<td>0.652</td>
<td>0.513</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>7</td>
<td></td>
<td>2018_stem_original_p10_t1000.out</td>
<td>1141</td>
<td>0.229</td>
<td>0.445</td>
<td>0.630</td>
<td>0.814</td>
<td>0.893</td>
<td>0.658</td>
<td>0.509</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>8</td>
<td></td>
<td>2018_stem_original_p10_t200.out</td>
<td>1282</td>
<td>0.229</td>
<td>0.445</td>
<td>0.634</td>
<td>0.823</td>
<td>0.891</td>
<td>0.660</td>
<td>0.507</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>9</td>
<td></td>
<td>2018_stem_original_p10_t500.out</td>
<td>1200</td>
<td>0.229</td>
<td>0.445</td>
<td>0.634</td>
<td>0.818</td>
<td>0.893</td>
<td>0.662</td>
<td>0.509</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>10</td>
<td></td>
<td>2018_stem_original_p10_t300.out</td>
<td>1280</td>
<td>0.229</td>
<td>0.452</td>
<td>0.627</td>
<td>0.816</td>
<td>0.893</td>
<td>0.660</td>
<td>0.500</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>11</td>
<td></td>
<td>2018_stem_original_p10_t1500.out</td>
<td>1126</td>
<td>0.229</td>
<td>0.445</td>
<td>0.630</td>
<td>0.814</td>
<td>0.895</td>
<td>0.657</td>
<td>0.514</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>12</td>
<td></td>
<td>distributed_effort_p10_t1000.out</td>
<td>1109</td>
<td>0.229</td>
<td>0.445</td>
<td>0.630</td>
<td>0.814</td>
<td>0.895</td>
<td>0.649</td>
<td>0.514</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>13</td>
<td></td>
<td>2018_stem_original_p10_t100.out</td>
<td>2024</td>
<td>0.221</td>
<td>0.418</td>
<td>0.609</td>
<td>0.791</td>
<td>0.868</td>
<td>0.525</td>
<td>0.399</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>14</td>
<td></td>
<td>baseline_bm25_t500.out</td>
<td>2470</td>
<td>0.119</td>
<td>0.236</td>
<td>0.402</td>
<td>0.548</td>
<td>0.650</td>
<td>0.390</td>
<td>0.252</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>15</td>
<td></td>
<td>distributed_effort_p10_t300.out</td>
<td>1111</td>
<td>0.232</td>
<td>0.445</td>
<td>0.630</td>
<td>0.814</td>
<td>0.886</td>
<td>0.649</td>
<td>0.528</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>16</td>
<td></td>
<td>2018_stem_original_p50_t1000.out</td>
<td>1127</td>
<td>0.229</td>
<td>0.445</td>
<td>0.630</td>
<td>0.811</td>
<td>0.893</td>
<td>0.652</td>
<td>0.528</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>17</td>
<td></td>
<td>distributed_effort_p10_t100.out</td>
<td>1271</td>
<td>0.204</td>
<td>0.439</td>
<td>0.614</td>
<td>0.770</td>
<td>0.839</td>
<td>0.610</td>
<td>0.468</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>18</td>
<td></td>
<td>2018_stem_original_p50_t200.out</td>
<td>1291</td>
<td>0.229</td>
<td>0.445</td>
<td>0.634</td>
<td>0.820</td>
<td>0.898</td>
<td>0.660</td>
<td>0.499</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>19</td>
<td></td>
<td>baseline_bm25_t1000.out</td>
<td>2395</td>
<td>0.119</td>
<td>0.236</td>
<td>0.389</td>
<td>0.543</td>
<td>0.659</td>
<td>0.396</td>
<td>0.260</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>20</td>
<td></td>
<td>distributed_effort_p10_t500.out</td>
<td>1116</td>
<td>0.229</td>
<td>0.445</td>
<td>0.630</td>
<td>0.814</td>
<td>0.891</td>
<td>0.634</td>
<td>0.521</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>21</td>
<td></td>
<td>baseline_bm25_t300.out</td>
<td>2493</td>
<td>0.119</td>
<td>0.239</td>
<td>0.405</td>
<td>0.541</td>
<td>0.652</td>
<td>0.391</td>
<td>0.244</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>22</td>
<td></td>
<td>baseline_bm25_t100.out</td>
<td>2130</td>
<td>0.120</td>
<td>0.239</td>
<td>0.414</td>
<td>0.564</td>
<td>0.659</td>
<td>0.394</td>
<td>0.295</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>23</td>
<td></td>
<td>2018_stem_original_p50_t400.out</td>
<td>1189</td>
<td>0.229</td>
<td>0.448</td>
<td>0.634</td>
<td>0.816</td>
<td>0.891</td>
<td>0.654</td>
<td>0.527</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>24</td>
<td></td>
<td>2018_stem_original_p50_t300.out</td>
<td>1272</td>
<td>0.229</td>
<td>0.452</td>
<td>0.627</td>
<td>0.814</td>
<td>0.893</td>
<td>0.656</td>
<td>0.518</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>25</td>
<td></td>
<td>2018_stem_original_p50_t100.out</td>
<td>2027</td>
<td>0.222</td>
<td>0.418</td>
<td>0.609</td>
<td>0.786</td>
<td>0.868</td>
<td>0.549</td>
<td>0.394</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>26</td>
<td></td>
<td>distributed_effort_p10_t200.out</td>
<td>1194</td>
<td>0.225</td>
<td>0.445</td>
<td>0.632</td>
<td>0.811</td>
<td>0.877</td>
<td>0.663</td>
<td>0.509</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>27</td>
<td></td>
<td>baseline_bm25_t400.out</td>
<td>2492</td>
<td>0.119</td>
<td>0.239</td>
<td>0.405</td>
<td>0.539</td>
<td>0.650</td>
<td>0.386</td>
<td>0.246</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>28</td>
<td></td>
<td>2018_stem_original_p50_t1500.out</td>
<td>1056</td>
<td>0.229</td>
<td>0.445</td>
<td>0.630</td>
<td>0.814</td>
<td>0.898</td>
<td>0.651</td>
<td>0.537</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>29</td>
<td></td>
<td>2018_stem_original_p50_t500.out</td>
<td>1200</td>
<td>0.229</td>
<td>0.445</td>
<td>0.634</td>
<td>0.809</td>
<td>0.889</td>
<td>0.649</td>
<td>0.524</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>30</td>
<td></td>
<td>baseline_bm25_t1500.out</td>
<td>2476</td>
<td>0.119</td>
<td>0.236</td>
<td>0.389</td>
<td>0.541</td>
<td>0.652</td>
<td>0.364</td>
<td>0.254</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>31</td>
<td></td>
<td>baseline_bm25_t200.out</td>
<td>2253</td>
<td>0.120</td>
<td>0.234</td>
<td>0.405</td>
<td>0.550</td>
<td>0.652</td>
<td>0.409</td>
<td>0.278</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>32</td>
<td></td>
<td>distributed_effort_p10_t400.out</td>
<td>1116</td>
<td>0.231</td>
<td>0.445</td>
<td>0.630</td>
<td>0.814</td>
<td>0.886</td>
<td>0.634</td>
<td>0.528</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>33</td>
<td>Sheffield</td>
<td>sheffield-Log_likelihood.out</td>
<td>1964</td>
<td>0.222</td>
<td>0.305</td>
<td>0.450</td>
<td>0.641</td>
<td>0.730</td>
<td>0.475</td>
<td>0.375</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>34</td>
<td></td>
<td>sheffield-Odds_Ratio.out</td>
<td>2250</td>
<td>0.175</td>
<td>0.220</td>
<td>0.336</td>
<td>0.525</td>
<td>0.675</td>
<td>0.451</td>
<td>0.338</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>35</td>
<td></td>
<td>sheffield-baseline.out</td>
<td>2184</td>
<td>0.248</td>
<td>0.382</td>
<td>0.561</td>
<td>0.707</td>
<td>0.805</td>
<td>0.490</td>
<td>0.347</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>36</td>
<td></td>
<td>sheffield-Chi_Squared.out</td>
<td>1972</td>
<td>0.234</td>
<td>0.350</td>
<td>0.527</td>
<td>0.668</td>
<td>0.759</td>
<td>0.487</td>
<td>0.381</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>37</td>
<td>Wang et al.</td>
<td>BM25</td>
<td>2723</td>
<td>0.119</td>
<td>0.213</td>
<td>0.329</td>
<td>0.528</td>
<td>0.314</td>
<td>0.208</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>38</td>
<td></td>
<td>BERT</td>
<td>2514</td>
<td>0.092</td>
<td>0.132</td>
<td>0.238</td>
<td>0.391</td>
<td>0.258</td>
<td>0.210</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>39</td>
<td></td>
<td>BERT-M</td>
<td>3234</td>
<td>0.096</td>
<td>0.079</td>
<td>0.198</td>
<td>0.379</td>
<td>0.263</td>
<td>0.123</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>40</td>
<td></td>
<td>BioBERT</td>
<td>3264</td>
<td>0.081</td>
<td>0.129</td>
<td>0.229</td>
<td>0.337</td>
<td>0.137</td>
<td>0.095</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>41</td>
<td></td>
<td>BlueBERT</td>
<td>3771</td>
<td>0.069</td>
<td>0.026</td>
<td>0.053</td>
<td>0.105</td>
<td>0.023</td>
<td>0.016</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>42</td>
<td></td>
<td>PubMedBERT</td>
<td>3331</td>
<td>0.104</td>
<td>0.123</td>
<td>0.214</td>
<td>0.312</td>
<td>0.202</td>
<td>0.098</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>43</td>
<td></td>
<td>BERT-Tuned</td>
<td>1400</td>
<td>0.223</td>
<td>0.439</td>
<td>0.601</td>
<td>0.762</td>
<td>0.587</td>
<td>0.460</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>44</td>
<td></td>
<td>BERT-M-Tuned</td>
<td>1178</td>
<td>0.254</td>
<td>0.447</td>
<td>0.590</td>
<td>0.754</td>
<td>0.615</td>
<td>0.500</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>45</td>
<td></td>
<td>BioBERT-Tuned</td>
<td>853</td>
<td>0.318</td>
<td>0.500</td>
<td>0.671</td>
<td>0.817</td>
<td>0.686</td>
<td>0.585</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>46</td>
<td>Proposed Method</td>
<td>GPT_Cosine_Similarity</td>
<td>1154</td>
<td>0.271</td>
<td>0.477</td>
<td>0.628</td>
<td>0.782</td>
<td>0.851</td>
<td>0.600</td>
<td>0.513</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>47</td>
<td></td>
<td>BM25</td>
<td>2461</td>
<td>0.164</td>
<td>0.334</td>
<td>0.472</td>
<td>0.654</td>
<td>0.723</td>
<td>0.351</td>
<td>0.233</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>48</td>
<td></td>
<td>GPT_QA_Soft</td>
<td>1979</td>
<td>0.255</td>
<td>0.319</td>
<td>0.495</td>
<td>0.674</td>
<td>0.765</td>
<td>0.408</td>
<td>0.334</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>49</td>
<td></td>
<td>GPT_QA_Hard</td>
<td>1983</td>
<td>0.228</td>
<td>0.367</td>
<td>0.468</td>
<td>0.673</td>
<td>0.776</td>
<td>0.364</td>
<td>0.303</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>50</td>
<td></td>
<td>GPT_QA_Soft_Abstract_Level</td>
<td>1491</td>
<td>0.301</td>
<td>0.384</td>
<td>0.574</td>
<td>0.705</td>
<td>0.810</td>
<td>0.473</td>
<td>0.422</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>51</td>
<td></td>
<td>GPT_QA_Hard_Abstract_Level</td>
<td>1583</td>
<td>0.310</td>
<td>0.387</td>
<td>0.573</td>
<td>0.727</td>
<td>0.820</td>
<td>0.454</td>
<td>0.396</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>52</td>
<td></td>
<td>GPT_QA_Soft_Answer_Level</td>
<td>1136</td>
<td>0.315</td>
<td>0.438</td>
<td>0.593</td>
<td>0.766</td>
<td>0.858</td>
<td>0.556</td>
<td>0.506</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>53</td>
<td></td>
<td>GPT_QA_Hard_Answer_Level</td>
<td>1176</td>
<td>0.322</td>
<td>0.450</td>
<td>0.595</td>
<td>0.791</td>
<td>0.873</td>
<td>0.536</td>
<td>0.491</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>54</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>55</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>56</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>57</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>58</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>59</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>60</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>61</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>62</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>63</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>64</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>65</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>66</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>67</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>68</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>69</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>70</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>71</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>72</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>73</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>74</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>75</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>76</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>77</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>78</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>79</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>80</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>81</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>82</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>83</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>84</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>85</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>86</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>87</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>88</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>89</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>90</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>91</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>92</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>93</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>94</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>95</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>96</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>97</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>98</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>99</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>100</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>101</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>102</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>103</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>104</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>105</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>106</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>107</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>108</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>109</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>110</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>111</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>112</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>113</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>114</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>115</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>116</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>117</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>118</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>119</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>120</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>11</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>121</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>122</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>123</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>124</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>125</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>126</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>127</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>128</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>129</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>130</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>131</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>132</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>133</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>134</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>135</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>136</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>137</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>138</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>139</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>139</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>139</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>140</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>139</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>139</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>141</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>142</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>142</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>143</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>143</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>144</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>144</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>145</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>145</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>146</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>147</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>147</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>148</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>148</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>149</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>149</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>150</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>151</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>152</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>152</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>153</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>153</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>154</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>155</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>155</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>156</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>156</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>157</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>157</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>158</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>159</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>159</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>1510</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>1510</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>152</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>153</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>153</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>154</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>153</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>155</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>medRxiv preprint doi: https://doi.org/10.1101/2023.12.17.23300102; this version posted December 18, 2023. The copyright holder for this Page 9 of 14 preprint (which was not certified Journal of the American Medical Association's Association license to display the preprint in perpetuity. Short Article Title All rights reserved. No reuse allowed without permission.</p>
<p>Table 4. Results obtained on the Intervention data. The zero-shot ranking models are emboldened.</p>
<table>
<thead>
<tr>
<th>Paper</th>
<th>Models</th>
<th>L_Rel</th>
<th>MAP</th>
<th>R@5%</th>
<th>R@10%</th>
<th>R@20%</th>
<th>R@30%</th>
<th>WSS@95</th>
<th>WSS@100</th>
</tr>
</thead>
<tbody>
<tr>
<td>UvA</td>
<td>abs-hh-ratio-ilps</td>
<td>958</td>
<td>0.567</td>
<td>0.518</td>
<td>0.628</td>
<td>0.736</td>
<td>0.813</td>
<td>0.526</td>
<td>0.480</td>
</tr>
<tr>
<td></td>
<td>abs-th-ratio-ilps</td>
<td>986</td>
<td>0.556</td>
<td>0.478</td>
<td>0.576</td>
<td>0.692</td>
<td>0.774</td>
<td>0.535</td>
<td>0.450</td>
</tr>
<tr>
<td>UNIPD</td>
<td>2018_stem_original_p10_t400.out</td>
<td>985</td>
<td>0.280</td>
<td>0.307</td>
<td>0.502</td>
<td>0.663</td>
<td>0.744</td>
<td>0.632</td>
<td>0.511</td>
</tr>
<tr>
<td></td>
<td>distributed_effort_p10_t1500.out</td>
<td>981</td>
<td>0.280</td>
<td>0.306</td>
<td>0.499</td>
<td>0.664</td>
<td>0.745</td>
<td>0.633</td>
<td>0.517</td>
</tr>
<tr>
<td></td>
<td>2018_stem_original_p10_t1000.out</td>
<td>977</td>
<td>0.280</td>
<td>0.306</td>
<td>0.499</td>
<td>0.664</td>
<td>0.745</td>
<td>0.630</td>
<td>0.510</td>
</tr>
<tr>
<td></td>
<td>2018_stem_original_p10_t200.out</td>
<td>1180</td>
<td>0.280</td>
<td>0.312</td>
<td>0.501</td>
<td>0.671</td>
<td>0.775</td>
<td>0.617</td>
<td>0.488</td>
</tr>
<tr>
<td></td>
<td>2018_stem_original_p10_t500.out</td>
<td>975</td>
<td>0.280</td>
<td>0.306</td>
<td>0.502</td>
<td>0.662</td>
<td>0.742</td>
<td>0.630</td>
<td>0.514</td>
</tr>
<tr>
<td></td>
<td>2018_stem_original_p10_t300.out</td>
<td>1141</td>
<td>0.280</td>
<td>0.313</td>
<td>0.496</td>
<td>0.665</td>
<td>0.771</td>
<td>0.617</td>
<td>0.494</td>
</tr>
<tr>
<td></td>
<td>2018_stem_original_p10_t1500.out</td>
<td>952</td>
<td>0.280</td>
<td>0.306</td>
<td>0.499</td>
<td>0.664</td>
<td>0.745</td>
<td>0.630</td>
<td>0.522</td>
</tr>
<tr>
<td></td>
<td>distributed_effort_p10_t1000.out</td>
<td>992</td>
<td>0.279</td>
<td>0.306</td>
<td>0.499</td>
<td>0.664</td>
<td>0.745</td>
<td>0.620</td>
<td>0.492</td>
</tr>
<tr>
<td></td>
<td>2018_stem_original_p10_t100.out</td>
<td>1153</td>
<td>0.274</td>
<td>0.306</td>
<td>0.483</td>
<td>0.639</td>
<td>0.737</td>
<td>0.540</td>
<td>0.474</td>
</tr>
<tr>
<td></td>
<td>baseline_bm25_t500.out</td>
<td>1233</td>
<td>0.222</td>
<td>0.191</td>
<td>0.282</td>
<td>0.410</td>
<td>0.515</td>
<td>0.435</td>
<td>0.394</td>
</tr>
<tr>
<td></td>
<td>distributed_effort_p10_t300.out</td>
<td>974</td>
<td>0.276</td>
<td>0.306</td>
<td>0.499</td>
<td>0.664</td>
<td>0.733</td>
<td>0.592</td>
<td>0.481</td>
</tr>
<tr>
<td></td>
<td>2018_stem_original_p50_t1000.out</td>
<td>836</td>
<td>0.290</td>
<td>0.306</td>
<td>0.498</td>
<td>0.688</td>
<td>0.795</td>
<td>0.643</td>
<td>0.542</td>
</tr>
<tr>
<td></td>
<td>distributed_effort_p10_t100.out</td>
<td>1114</td>
<td>0.248</td>
<td>0.315</td>
<td>0.444</td>
<td>0.604</td>
<td>0.704</td>
<td>0.458</td>
<td>0.372</td>
</tr>
<tr>
<td></td>
<td>2018_stem_original_p50_t200.out</td>
<td>1185</td>
<td>0.290</td>
<td>0.312</td>
<td>0.499</td>
<td>0.693</td>
<td>0.792</td>
<td>0.630</td>
<td>0.481</td>
</tr>
<tr>
<td></td>
<td>baseline_bm25_t1000.out</td>
<td>1241</td>
<td>0.222</td>
<td>0.191</td>
<td>0.282</td>
<td>0.408</td>
<td>0.524</td>
<td>0.446</td>
<td>0.392</td>
</tr>
<tr>
<td></td>
<td>distributed_effort_p10_t500.out</td>
<td>991</td>
<td>0.278</td>
<td>0.306</td>
<td>0.499</td>
<td>0.664</td>
<td>0.743</td>
<td>0.606</td>
<td>0.483</td>
</tr>
<tr>
<td></td>
<td>baseline_bm25_t300.out</td>
<td>1262</td>
<td>0.222</td>
<td>0.187</td>
<td>0.286</td>
<td>0.410</td>
<td>0.523</td>
<td>0.440</td>
<td>0.398</td>
</tr>
<tr>
<td></td>
<td>baseline_bm25_t100.out</td>
<td>1397</td>
<td>0.223</td>
<td>0.186</td>
<td>0.291</td>
<td>0.429</td>
<td>0.557</td>
<td>0.414</td>
<td>0.368</td>
</tr>
<tr>
<td></td>
<td>2018_stem_original_p50_t400.out</td>
<td>985</td>
<td>0.290</td>
<td>0.307</td>
<td>0.501</td>
<td>0.685</td>
<td>0.767</td>
<td>0.646</td>
<td>0.514</td>
</tr>
<tr>
<td></td>
<td>2018_stem_original_p50_t300.out</td>
<td>1144</td>
<td>0.290</td>
<td>0.313</td>
<td>0.495</td>
<td>0.682</td>
<td>0.788</td>
<td>0.639</td>
<td>0.497</td>
</tr>
<tr>
<td></td>
<td>2018_stem_original_p50_t100.out</td>
<td>1150</td>
<td>0.284</td>
<td>0.306</td>
<td>0.483</td>
<td>0.653</td>
<td>0.752</td>
<td>0.556</td>
<td>0.481</td>
</tr>
<tr>
<td></td>
<td>distributed_effort_p10_t200.out</td>
<td>965</td>
<td>0.271</td>
<td>0.306</td>
<td>0.482</td>
<td>0.651</td>
<td>0.752</td>
<td>0.560</td>
<td>0.445</td>
</tr>
<tr>
<td></td>
<td>baseline_bm25_t400.out</td>
<td>1242</td>
<td>0.222</td>
<td>0.191</td>
<td>0.286</td>
<td>0.412</td>
<td>0.523</td>
<td>0.434</td>
<td>0.393</td>
</tr>
<tr>
<td></td>
<td>2018_stem_original_p50_t1500.out</td>
<td>796</td>
<td>0.290</td>
<td>0.306</td>
<td>0.498</td>
<td>0.688</td>
<td>0.785</td>
<td>0.642</td>
<td>0.553</td>
</tr>
<tr>
<td></td>
<td>2018_stem_original_p50_t500.out</td>
<td>1001</td>
<td>0.290</td>
<td>0.306</td>
<td>0.501</td>
<td>0.691</td>
<td>0.779</td>
<td>0.650</td>
<td>0.505</td>
</tr>
<tr>
<td></td>
<td>baseline_bm25_t1500.out</td>
<td>1203</td>
<td>0.222</td>
<td>0.191</td>
<td>0.282</td>
<td>0.411</td>
<td>0.533</td>
<td>0.453</td>
<td>0.399</td>
</tr>
<tr>
<td></td>
<td>baseline_bm25_t200.out</td>
<td>1263</td>
<td>0.222</td>
<td>0.189</td>
<td>0.284</td>
<td>0.417</td>
<td>0.535</td>
<td>0.438</td>
<td>0.396</td>
</tr>
<tr>
<td></td>
<td>distributed_effort_p10_t400.out</td>
<td>981</td>
<td>0.277</td>
<td>0.306</td>
<td>0.499</td>
<td>0.663</td>
<td>0.734</td>
<td>0.595</td>
<td>0.483</td>
</tr>
<tr>
<td>Sheffield</td>
<td>Sheffield-Log_likelihood.out</td>
<td>1132</td>
<td>0.293</td>
<td>0.258</td>
<td>0.378</td>
<td>0.583</td>
<td>0.695</td>
<td>0.458</td>
<td>0.381</td>
</tr>
<tr>
<td></td>
<td>Sheffield-Odds_Ratio.out</td>
<td>1070</td>
<td>0.261</td>
<td>0.267</td>
<td>0.404</td>
<td>0.569</td>
<td>0.700</td>
<td>0.462</td>
<td>0.384</td>
</tr>
<tr>
<td></td>
<td>Sheffield-baseline.out</td>
<td>1276</td>
<td>0.245</td>
<td>0.220</td>
<td>0.334</td>
<td>0.507</td>
<td>0.653</td>
<td>0.470</td>
<td>0.386</td>
</tr>
<tr>
<td></td>
<td>Sheffield-Chi_Squared.out</td>
<td>1149</td>
<td>0.262</td>
<td>0.238</td>
<td>0.360</td>
<td>0.537</td>
<td>0.687</td>
<td>0.469</td>
<td>0.415</td>
</tr>
<tr>
<td>Wang et al.</td>
<td>BM25</td>
<td>1716</td>
<td>0.211</td>
<td>0.305</td>
<td>0.399</td>
<td>0.554</td>
<td></td>
<td>0.351</td>
<td>0.296</td>
</tr>
<tr>
<td></td>
<td>BERT</td>
<td>1399</td>
<td>0.160</td>
<td>0.211</td>
<td>0.328</td>
<td>0.504</td>
<td></td>
<td>0.362</td>
<td>0.333</td>
</tr>
<tr>
<td></td>
<td>BERT-M</td>
<td>1837</td>
<td>0.177</td>
<td>0.195</td>
<td>0.355</td>
<td>0.527</td>
<td></td>
<td>0.323</td>
<td>0.266</td>
</tr>
<tr>
<td></td>
<td>BioBERT</td>
<td>1833</td>
<td>0.146</td>
<td>0.135</td>
<td>0.198</td>
<td>0.307</td>
<td></td>
<td>0.159</td>
<td>0.163</td>
</tr>
<tr>
<td></td>
<td>BlueBERT</td>
<td>2057</td>
<td>0.046</td>
<td>0.028</td>
<td>0.051</td>
<td>0.107</td>
<td></td>
<td>0.008</td>
<td>0.036</td>
</tr>
<tr>
<td>41</td>
<td>PubMedBERT</td>
<td>1975</td>
<td>0.078</td>
<td>0.050</td>
<td>0.091</td>
<td>0.275</td>
<td></td>
<td>0.121</td>
<td>0.094</td>
</tr>
<tr>
<td>42</td>
<td>BERT-Tuned</td>
<td>1375</td>
<td>0.281</td>
<td>0.374</td>
<td>0.527</td>
<td>0.659</td>
<td></td>
<td>0.363</td>
<td>0.301</td>
</tr>
<tr>
<td>43</td>
<td>BERT-M-Tuned</td>
<td>1572</td>
<td>0.334</td>
<td>0.402</td>
<td>0.565</td>
<td>0.706</td>
<td></td>
<td>0.446</td>
<td>0.362</td>
</tr>
<tr>
<td>44</td>
<td>BioBERT-Tuned</td>
<td>707</td>
<td>0.456</td>
<td>0.581</td>
<td>0.737</td>
<td>0.842</td>
<td></td>
<td>0.646</td>
<td>0.579</td>
</tr>
<tr>
<td>45</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Proposed Method</td>
<td>GPT_Cosine_Similarity</td>
<td>920</td>
<td>0.315</td>
<td>0.401</td>
<td>0.544</td>
<td>0.722</td>
<td>0.797</td>
<td>0.552</td>
<td>0.499</td>
</tr>
<tr>
<td></td>
<td>BM25</td>
<td>1545</td>
<td>0.146</td>
<td>0.191</td>
<td>0.300</td>
<td>0.497</td>
<td>0.667</td>
<td>0.270</td>
<td>0.238</td>
</tr>
<tr>
<td></td>
<td>GPT_QA_Soft</td>
<td>1055</td>
<td>0.411</td>
<td>0.469</td>
<td>0.610</td>
<td>0.738</td>
<td>0.856</td>
<td>0.486</td>
<td>0.416</td>
</tr>
<tr>
<td></td>
<td>GPT_QA_Hard</td>
<td>1159</td>
<td>0.356</td>
<td>0.444</td>
<td>0.578</td>
<td>0.759</td>
<td>0.847</td>
<td>0.466</td>
<td>0.397</td>
</tr>
<tr>
<td></td>
<td>GPT_QA_Soft_Abstract_Level</td>
<td>934</td>
<td>0.440</td>
<td>0.494</td>
<td>0.663</td>
<td>0.800</td>
<td>0.873</td>
<td>0.534</td>
<td>0.459</td>
</tr>
<tr>
<td></td>
<td>GPT_QA_Hard_Abstract_Level</td>
<td>976</td>
<td>0.443</td>
<td>0.505</td>
<td>0.687</td>
<td>0.777</td>
<td>0.856</td>
<td>0.532</td>
<td>0.458</td>
</tr>
<tr>
<td></td>
<td>GPT_QA_Soft_Answer_Level</td>
<td>801</td>
<td>0.450</td>
<td>0.526</td>
<td>0.697</td>
<td>0.816</td>
<td>0.881</td>
<td>0.600</td>
<td>0.526</td>
</tr>
<tr>
<td></td>
<td>GPT_QA_Hard_Answer_Level</td>
<td>806</td>
<td>0.447</td>
<td>0.527</td>
<td>0.697</td>
<td>0.808</td>
<td>0.869</td>
<td>0.592</td>
<td>0.519</td>
</tr>
</tbody>
</table>
<h2>Discussion</h2>
<h2>Selection Criteria</h2>
<p>To further evaluate the usefulness of selection criteria, we implemented our own BM25 baseline using selection criteria as a query, and we observed competitive performances. Particularly it significantly outperformed the BM25 baselines of UNIPD and Wang et al. on the DTA dataset. On Intervention, it was at least on par with or slightly better than most other BM25 baselines except Wang et al. The result underscored the validity of using selection criteria to guide citation screening.</p>
<h2>Question Generation and Answering</h2>
<p>We manually checked the question qualities and found notable strengths and occasional challenges of ChatGPT in question generation. Figure 4 illustrates how ChatGPT was smart enough to translate a lengthy exclusion criterion into two relevant questions, $Q_{4}$ and $Q_{5}$, demonstrating its nuanced understanding of the complex semantics of the sentence. The questions were effectively formatted so that a POSITIVE response consistently signifies compliance with a selection criterion, be it inclusion or exclusion.</p>
<p>Occasionally, ChatGPT failed to generate completely independent questions. This led to redundant or overlapped questions, introducing biases in combining answer scores. Occasionally, ChatGPT struggled to address the "OR" clause in a long selection criterion sentence. It was split into separate questions, which was problematic. In such cases, matching one question should give a POSITIVE score, but the NEGATIVE answers to other questions generated from the OR clause might underestimate the final score. These issues imply areas of improvement in ensuring robust question generation and precise answer interpretation for citation screening. We postulate that a viable solution is to train a good question generator and analyzer to tackle these issues. Alternatively, it is sensible for human reviewers to scrutinize and correct the generated questions before sending them to LLMs to answer.</p>
<p>While the current paper deliberately limited answers to a simple form, it is worthwhile to consider incorporating explanations of LLM-generated answers in future iterations. Providing insight into ChatGPT's reasoning process can enhance transparency and facilitate a better understanding of the model's decision-making which is essential for instilling user confidence in the outputs of automated citation screening to encourage technology acceptance [74, 75]. In addition, it contributes to refining model performance through iterative conversation with LLMs by giving user feedback on model answers and their explanations [70].</p>
<h2>Answer Re-Ranking</h2>
<p>Taking a holistic view, i.e., averaging model performances over all four categories of datasets, the answer-level re-ranking methods consistently outperformed our other models across all metrics. This superiority is attributed to the additional granularity gained by considering the alignment of each question with the abstracts of candidate studies. Compared with other zero-shot models, our methods achieved substantial improvements, showcasing both the effectiveness of the proposed questions-answering framework and the utility of ChatGPT as a zero-shot ranker for automated citation screening.</p>
<p>When pitted against trained models employing relevant feedback or fine-tuning, our methods still held a solid ground competitively. Our best models achieved very promising performances in $L_{\text {Rel }}, R @ 5 \%$, and $W S S$. This is a useful merit as our zero-shot method fits well into the real-world citation screening task, starting with no annotation of included/excluded studies. Although the UvA variants and BioBERT-Tuned models often resulted in better performances in MAP and occasionally in recall at different thresholds, our models were still demonstrated to be competitive, highlighting their brilliance requiring no prior training. Therefore, our models are better generalizable to all SR categories, especially when lacking comprehensive datasets for relevance feedback or fine-tuning. Nevertheless, it is always valuable to fine-tune LLMs for each SR topic to benefit our proposed answer re-ranking methods.</p>
<h2>Conclusion</h2>
<p>This paper proposed an effective LLM-assisted question-answering framework to facilitate citation screening and advance automated systematic review. Extensive experiments emphasized the particular pertinence of selection criteria of included studies to automated citation screening and ChatGPT's proficiency in understanding and utilizing selection criteria to prioritize candidate studies. Specifically, ChatGPT was able to correctly capture and handle complex semantics like several juxtaposed criteria with a logical OR relationship, significantly outperforming other zero-shot baselines. The positive results of $L_{\text {Rel }}$ (position of the last relevant study), $R @ 5 \%$ (recall at top $5 \%$ ), $R @ 10 \%$, WSS@95 (Workload Saved over Sampling at $95 \%$ recall level), and WSS@100 not only showed the competency of the proposed framework as a zero-shot citation screening methodology but also indicated its potential use in reducing human effort in building a high-quality dataset for training a citation screener.</p>
<h2>References</h2>
<ol>
<li>Guy Tsafnat, Paul Glasziou, Miew Keen Choong, Adam Dunn, Filippo Galgani, and Enrico Coiera. Systematic review automation technologies. Systematic reviews, 3:1-15, 2014.</li>
<li>S Gopalakrishnan and P Ganeshkumar. Systematic reviews and meta-analysis: understanding the best evidence in primary healthcare. Journal of family medicine and primary care, $2(1): 9,2013$.</li>
<li>Hamideh Moosapour, Farzane Saedifard, Maryam Aalaa, Akbar Soltani, and Bagher Larijani. The rationale behind systematic reviews in clinical medicine: a conceptual framework. Journal of Diabetes \&amp; Metabolic Disorders, 20:919-929, 2021.</li>
<li>Ian Shemilt, Nada Khan, Sophie Park, and James Thomas. Use of cost-effectiveness analysis to compare the efficiency of study identification methods in systematic reviews. Systematic reviews, 5:1-13, 2016.</li>
<li>Matthew Michelson and Katja Reuter. The significant cost of systematic reviews and meta-analyses: a call for greater involvement of machine learning to assess the promise of clinical trials. Contemporary clinical trials communications, 16:100443, 2019.</li>
<li>
<p>Julian PT Higgins, Sally Green, et al. Cochrane handbook for systematic reviews of interventions. 2008.</p>
</li>
<li>
<p>Alison O'Mara-Eves, James Thomas, John McNaught, Makoto Miwa, and Sophia Ananiadou. Using text mining for study identification in systematic reviews: a systematic review of current approaches. Systematic reviews, 4(1):1-22, 2015.</p>
</li>
<li>Raymon van Dinter, Bedir Tekinerdogan, and Cagatay Catal. Automation of systematic literature reviews: A systematic literature review. Information and Software Technology, 136:106589, 2021.</li>
<li>Amal Alharbi, William Briggs, and Mark Stevenson. Retrieving and ranking studies for systematic reviews: University of sheffield's approach to clef ehealth 2018 task 2. In CEUR workshop proceedings, volume 2125. CEUR Workshop Proceedings, 2018.</li>
<li>Amal Alharbi and Mark Stevenson. Ranking abstracts to identify relevant evidence for systematic reviews: The university of sheffield's approach to clef ehealth 2017 task 2. In Clef (working notes), 2017.</li>
<li>Gordon V Cormack and Maura R Grossman. Technologyassisted review in empirical medicine: Waterloo participation in clef ehealth 2017. CLEF (working notes), 11, 2017.</li>
<li>Gordon V Cormack and Maura R Grossman. Systems and methods for conducting a highly autonomous technologyassisted review classification, March 12 2019. US Patent $10,229,117$.</li>
<li>Maura R Grossman and Gordon V Cormack. Technologyassisted review in e-discovery can be more effective and more efficient than exhaustive manual review. Richmond Journal of Law $\mathcal{G}$ Technology, 17(3):11, 2011.</li>
<li>Maura R Grossman, Gordon V Cormack, and Adam Roegiest. Automatic and semi-automatic document selection for technology-assisted review. In Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 905-908, 2017.</li>
<li>Grace E Lee and Aixin Sun. Seed-driven document ranking for systematic reviews in evidence-based medicine. In The 41st international ACM SIGIR conference on research $\mathcal{G}$ development in information retrieval, pages 455-464, 2018.</li>
<li>Harrisen Scells, Guido Zuccon, Anthony Deacon, and Bevan Koopman. Qut ielab at clef ehealth 2017 technology assisted reviews track: initial experiments with learning to rank. In Working Notes of CLEF 2017-Conference and Labs of the Evaluation Forum [CEUR Workshop Proceedings, Volume 1866], pages 1-6. Sun SITE Central Europe, 2017.</li>
<li>Amal Alharbi and Mark Stevenson. Ranking studies for systematic reviews using query adaptation: University of sheffield's approach to clef ehealth 2019 task 2 working notes for clef 2019. In Working Notes of CLEF 2019-Conference and Labs of the Evaluation Forum, volume 2380. CEUR Workshop Proceedings, 2019.</li>
<li>Harrisen Scells, Guido Zuccon, Bevan Koopman, Anthony Deacon, Leif Azzopardi, and Shlomo Geva. Integrating the framing of clinical questions via pico into the retrieval of medical literature for systematic reviews. In Proceedings of the 2017 ACM on Conference on Information and Knowledge Management, pages 2291-2294, 2017.</li>
<li>Shuai Wang, Harrisen Scells, Ahmed Mourad, and Guido Zuccon. Seed-driven document ranking for systematic reviews: A reproducibility study. In European Conference on Information Retrieval, pages 686-700. Springer, 2022.</li>
<li>Zhen Qin, Rolf Jagerman, Kai Hui, Honglei Zhuang, Junru Wu, Jiaming Shen, Tianqi Liu, Jialu Liu, Donald Metzler,</li>
</ol>
<p>Xuanhui Wang, et al. Large language models are effective text rankers with pairwise ranking prompting. arXiv preprint arXiv:2306.17563, 2023.
21. Yupeng Hou, Junjie Zhang, Zihan Lin, Hongyu Lu, Ruobing Xie, Julian McAuley, and Wayne Xin Zhao. Large language models are zero-shot rankers for recommender systems. arXiv preprint arXiv:2305.08845, 2023.
22. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.
23. Oana Frunza, Diana Inkpen, Stan Matwin, William Klement, and Peter O'blenis. Exploiting the systematic review protocol for classification of medical abstracts. Artificial intelligence in medicine, 51(1):17-25, 2011.
24. Kentaro Matsui, Tomohiro Utsumi, Yumi Aoki, Taku Maruki, Masahiro Takeshima, and Takaesu Yoshikazu. Large language model demonstrates human-comparable sensitivity in initial screening of systematic reviews: A semi-automated strategy using gpt-3.5. Available at SSRN 4520426.
25. Shuai Wang, Harrisen Scells, Bevan Koopman, and Guido Zuccon. Neural rankers for effective screening prioritisation in medical systematic review literature search. In Proceedings of the 26th Australasian Document Computing Symposium, pages $1-10,2022$.
26. Barbara Kitchenham and Pearl Brereton. A systematic review of systematic review process research in software engineering. Information and software technology, 55(12):2049-2075, 2013.
27. Aaron M Cohen, William R Hersh, Kim Peterson, and PoYin Yen. Reducing workload in systematic review preparation using automated citation classification. Journal of the American Medical Informatics Association, 13(2):206-219, 2006.
28. Iain J Marshall and Byron C Wallace. Toward systematic review automation: a practical guide to using machine learning tools in research synthesis. Systematic reviews, 8:1-10, 2019.
29. Iain J Marshall, Joël Kuiper, Edward Banner, and Byron C Wallace. Automating biomedical evidence synthesis: Robotreviewer. In Proceedings of the conference. Association for Computational Linguistics. Meeting, volume 2017, page 7. NIH Public Access, 2017.
30. Iain J Marshall, Joël Kuiper, and Byron C Wallace. Robotreviewer: evaluation of a system for automatically assessing bias in clinical trials. Journal of the American Medical Informatics Association, 23(1):193-201, 2016.
31. Iain J Marshall, Benjamin Nye, Joël Kuiper, Anna NoelStorr, Rachel Marshall, Rory Maclean, Frank Soboczenski, Ani Nenkova, James Thomas, and Byron C Wallace. Trialstreamer: A living, automatically updated database of clinical trial reports. Journal of the American Medical Informatics Association, 27(12):1903-1912, 2020.
32. Carlos Francisco Moreno-Garcia, Chrisina Jayne, Eyad Elyan, and Magaly Aceves-Martins. A novel application of machine learning and zero-shot classification methods for automated abstract screening in systematic reviews. Decision Analytics Journal, page 100162, 2023.
33. Mourad Ouzzani, Hossam Hammady, Zbys Fedorowicz, and Ahmed Elmagarmid. Rayyan-a web and mobile app for systematic reviews. Systematic reviews, 5:1-10, 2016.</p>
<ol>
<li>Tanja Bekhuis and Dina Demner-Fushman. Screening nonrandomized studies for medical systematic reviews: a comparative study of classifiers. Artificial intelligence in medicine, 55(3):197-207, 2012.</li>
<li>Ian Shemilt, Antonia Simon, Gareth J Hollands, Theresa M Marteau, David Ogilvie, Alison O'Mara-Eves, Michael P Kelly, and James Thomas. Pinpointing needles in giant haystacks: use of text mining to reduce impractical screening workload in extremely large scoping reviews. Research Synthesis Methods, $5(1): 31-49,2014$.</li>
<li>Byron C Wallace, Thomas A Trikalinos, Joseph Lau, Carla Brodley, and Christopher H Schmid. Semi-automated screening of biomedical citations for systematic reviews. BMC bioinformatics, 11(1):1-11, 2010.</li>
<li>Stan Matwin, Alexandre Kouznetsov, Diana Inkpen, Oana Frunza, and Peter O'Blenis. A new algorithm for reducing the workload of experts in performing systematic reviews. Journal of the American Medical Informatics Association, 17(4):446-453, 2010.</li>
<li>Byron C Wallace, Kevin Small, Carla E Brodley, Joseph Lau, and Thomas A Trikalinos. Deploying an interactive machine learning system in an evidence-based practice center: abstrackr. In Proceedings of the 2nd ACM SIGHIT international health informatics symposium, pages 819-824, 2012.</li>
<li>Georgios Kontonatsios, Sally Spencer, Peter Matthew, and Ioannis Korkontzelos. Using a neural network-based feature extraction method to facilitate citation screening for systematic reviews. Expert Systems with Applications: X, 6:100030, 2020.</li>
<li>Raymon van Dinter, Cagatay Catal, and Bedir Tekinerdogan. A decision support system for automating document retrieval and citation screening. Expert Systems with Applications, 182:115261, 2021.</li>
<li>Xiaonan Ji, Alan Ritter, and Po-Yin Yen. Using ontologybased semantic similarity to facilitate the article screening process for systematic reviews. Journal of biomedical informatics, 69:33-42, 2017.</li>
<li>David Martinez, Sarvnaz Karimi, Lawrence Cavedon, and Timothy Baldwin. Facilitating biomedical systematic reviews using ranked text retrieval and classification. In Australasian document computing symposium (adcs), pages 53-60, 2008.</li>
<li>James Thomas and Alison O'Mara-Eves. How can we find relevant research more quickly? NCRM Newsletter: MethodsNews, 2011.</li>
<li>Aaron M Cohen, Kyle Ambert, and Marian McDonagh. Crosstopic learning for work prioritization in systematic review creation and update. Journal of the American Medical Informatics Association, 16(5):690-704, 2009.</li>
<li>Aaron M Cohen, Kyle Ambert, and Marian McDonagh. Studying the potential impact of automated document classification on scheduling a systematic review update. BMC medical informatics and decision making, 12:1-11, 2012.</li>
<li>Byron C Wallace, Kevin Small, Carla E Brodley, Joseph Lau, Christopher H Schmid, Lars Bertram, Christina M Lill, Joshua T Cohen, and Thomas A Trikalinos. Toward modernizing the systematic review pipeline in genetics: efficient updating via data mining. Genetics in medicine, 14(7):663-669, 2012.</li>
<li>Rens Van De Schoot, Jonathan De Bruin, Raoul Schram, Parisa Zahedi, Jan De Boer, Felix Weijdema, Bianca Kramer,</li>
</ol>
<p>Martijn Huijts, Maarten Hoogerwerf, Gerbrich Ferdinands, et al. An open source machine learning framework for efficient and transparent systematic reviews. Nature machine intelligence, 3(2):125-133, 2021.
48. Murray Shanahan. Talking about large language models. arXiv preprint arXiv:2212.03551, 2022.
49. Ce Zhou, Qian Li, Chen Li, Jun Yu, Yixin Liu, Guangjing Wang, Kai Zhang, Cheng Ji, Qiben Yan, Lifang He, et al. A comprehensive survey on pretrained foundation models: A history from bert to chatgpt. arXiv preprint arXiv:2302.09419, 2023.
50. Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. ACM Computing Surveys, 55(9):1-35, 2023.
51. Shuai Wang, Harrisen Scells, Bevan Koopman, and Guido Zuccon. Can chatgpt write a good boolean query for systematic review literature search? arXiv preprint arXiv:2302.03495, 2023.
52. Ahmad Alshami, Moustafa Elsayed, Eslam Ali, Abdelrahman EE Eltoukhy, and Tarek Zayed. Harnessing the power of chatgpt for automating systematic review process: Methodology, case study, limitations, and future directions. Systems, 11(7):351, 2023.
53. Eugene Syriani, Istvan David, and Gauransh Kumar. Assessing the ability of chatgpt to screen articles for systematic reviews. arXiv preprint arXiv:2307.06464, 2023.
54. Eddie Guo, Mehul Gupta, Jiawen Deng, Ye-Jean Park, Mike Paget, and Christopher Naugler. Automated paper screening for clinical reviews using large language models. arXiv preprint arXiv:2305.00844, 2023.
55. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zeroshot reasoners. Advances in neural information processing systems, 35:22199-22213, 2022.
56. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730-27744, 2022.
57. Karan Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, et al. Large language models encode clinical knowledge. arXiv preprint arXiv:2212.13158, 2022.
58. Aidan Gibson, Conrad W Safranek, Thomas Huang, Vimig Socrates, Ling Chi, Richard Andrew Taylor, David Chartash, et al. How does chatgpt perform on the united states medical licensing examination? the implications of large language models for medical education and knowledge assessment. JMIR Medical Education, 9(1):e45312, 2023.
59. Ruqing Zhang, Jiafeng Guo, Lu Chen, Yixing Fan, and Xueqi Cheng. A review on question generation from natural language text. ACM Transactions on Information Systems (TOIS), $40(1): 1-43,2021$.
60. Yang Deng, Wenxuan Zhang, Qian Yu, and Wai Lam. Product question answering in e-commerce: A survey. arXiv preprint arXiv:2302.08092, 2023.
61. Xiangjue Dong, Jiaying Lu, Jianling Wang, and James Caverlee. Closed-book question generation via contrastive</p>
<p>learning. arXiv preprint arXiv:2210.06781, 2022.
62. Nehal Muthukumar. Few-shot learning text classification in federated environments. In 2021 Smart Technologies, Communication and Robotics (STCR), pages 1-3. IEEE, 2021.
63. Ryan Greene, Ted Sanders, Lilian Weng, and Arvind Neelakantan, Dec 2022.
64. Giorgio Maria Di Nunzio and Evangelos Kanoulas. Special issue on technology assisted review systems, 2023.
65. Alessio Molinari and Evangelos Kanoulas. Transferring knowledge between topics in systematic reviews. Intelligent Systems with Applications, 16:200150, 2022.
66. Evangelos Kanoulas, Dan Li, Leif Azzopardi, and Rene Spijker. Clef 2019 technology assisted reviews in empirical medicine overview. In CEUR workshop proceedings, volume 2380, page 250, 2019.
67. Dan Li and Evangelos Kanoulas. Automatic thresholding by sampling documents and estimating recall. In CLEF (Working Notes), 2019.
68. Giorgio Maria Di Nunzio. A distributed effort approach for systematic reviews. ims unipd at clef 2019 ehealth task 2. In Clef (working notes), 2019.
69. Stephen Robertson, Hugo Zaragoza, et al. The probabilistic relevance framework: Bm25 and beyond. Foundations and Trends® in Information Retrieval, 3(4):333-389, 2009.
70. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Selfrefine: Iterative refinement with self-feedback. arXiv preprint arXiv:2303.17651, 2023.
71. Weixin Liang, Yuhui Zhang, Hancheng Cao, Binglu Wang, Daisy Ding, Xinyu Yang, Kailas Vodrahalli, Siyu He, Daniel Smith, Yian Yin, et al. Can large language models provide useful feedback on research papers? a large-scale empirical analysis. arXiv preprint arXiv:2310.01783, 2023.
72. Haodi Zhang, Min Cai, Xinhe Zhang, Chen Jason Zhang, Rui Mao, and Kaishun Wu. Self-convinced prompting: Fewshot question answering with repeated introspection. arXiv preprint arXiv:2310.05035, 2023.
73. Mariska MG Leeflang, Jonathan J Deeks, Yemisi Takwoingi, and Petra Macaskill. Cochrane diagnostic test accuracy reviews. Systematic reviews, 2(1):1-6, 2013.
74. Annette M O'Connor, Guy Tsafnat, James Thomas, Paul Glasziou, Stephen B Gilbert, and Brian Hutton. A question of trust: can we build an evidence base to gain trust in systematic review automation technologies? Systematic reviews, 8(1):1-8, 2019.
75. Xiaorui Jiang. Trustworthiness of systematic review automation: An interview at coventry university. 2022.</p>            </div>
        </div>

    </div>
</body>
</html>
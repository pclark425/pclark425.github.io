<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1094 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1094</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1094</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-24.html">extraction-schema-24</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <p><strong>Paper ID:</strong> paper-76dfb1ab698963f3776fe894b3743db4a5419a5f</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/76dfb1ab698963f3776fe894b3743db4a5419a5f" target="_blank">Multiagent Bidirectionally-Coordinated Nets: Emergence of Human-level Coordination in Learning to Play StarCraft Combat Games</a></p>
                <p><strong>Paper TL;DR:</strong> This paper introduces a Multiagent Bidirectionally-Coordinated Network (BiCNet) with a vectorised extension of actor-critic formulation and demonstrates that without any supervisions such as human demonstrations or labelled data, BiCNet could learn various types of advanced coordination strategies that have been commonly used by experienced game players.</p>
                <p><strong>Paper Abstract:</strong> Many artificial intelligence (AI) applications often require multiple intelligent agents to work in a collaborative effort. Efficient learning for intra-agent communication and coordination is an indispensable step towards general AI. In this paper, we take StarCraft combat game as a case study, where the task is to coordinate multiple agents as a team to defeat their enemies. To maintain a scalable yet effective communication protocol, we introduce a Multiagent Bidirectionally-Coordinated Network (BiCNet ['bIknet]) with a vectorised extension of actor-critic formulation. We show that BiCNet can handle different types of combats with arbitrary numbers of AI agents for both sides. Our analysis demonstrates that without any supervisions such as human demonstrations or labelled data, BiCNet could learn various types of advanced coordination strategies that have been commonly used by experienced game players. In our experiments, we evaluate our approach against multiple baselines under different scenarios; it shows state-of-the-art performance, and possesses potential values for large-scale real-world applications.</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1094.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1094.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BiCNet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multiagent Bidirectionally-Coordinated Network (BiCNet)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A deep multiagent reinforcement learning architecture that uses bidirectional recurrent networks as a communication channel and a vectorised actor-critic (multiagent deterministic policy gradient) formulation with parameter sharing to coordinate many agents in continuous-action multiagent tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>BiCNet-controlled agents</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Simulated multi-agent units controlled by a shared-parameter actor network with bidirectional RNN communication and a centralized critic; trained with an off-policy deterministic actor-critic framework (multiagent deterministic policy gradient) using Ornstein–Uhlenbeck exploration and replay buffers.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent (virtual game units in StarCraft)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>StarCraft micromanagement (combat) scenarios</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Real-time strategy micro-management combats in StarCraft where each episode contains multiple allied agents vs multiple enemy units; scenarios vary by unit types (Marines, Zerglings, Wraiths, Dragoons, Ultralisks, Dropships, Tanks, etc.), number of units per side, initial positions and terrain, producing a high-dimensional, highly variable game state with many interacting entities and time-varying health dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Characterized quantitatively by number of allied agents N and enemy agents M (experiments: e.g., 3v1, 3v3, 5v5, 10v13, 15v16, 20v30, 15v17, heterogeneous 2 Dropships+2 Tanks vs 1 Ultralisk); implicitly by large state-space (authors note StarCraft micromanagement has more possible states than Go); task difficulty category labels used: 'easy', 'hard', 'heterogeneous'.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>varies across experiments (low for easy scenarios like 3v1 or 3v3; high for hard scenarios with >10 agents per side such as 15v16 or 20v30)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Variation realized by changing unit types (heterogeneous vs homogeneous teams), different numbers of units, multiple distinct combat scenarios, stochastic initial states and built-in AI behaviours; authors note 'high variability of the StarCraft game' when discussing communication semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>high (multiple distinct scenarios, unit-type heterogeneity, and stochastic game variability)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Primary: winning rate (fraction of games won over test set). Secondary observed metrics: critic Q-values (mean Q), qualitative emergence of tactics, and in toy task absolute error between action and target.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported examples: trained BiCNet models evaluated after training (e.g. trained over 100k steps) and measured as average winning rate over 100 test games. Specific reported results: 'BiCNet learns focus-fire in ~10 combats and achieves >85% win rate in 5 Marines vs 5 Marines after ~10 training episodes'; authors state BiCNet outperforms baselines in 4/5 tested StarCraft scenarios and shows larger margin when number of agents >10. (No single consolidated numeric column for all scenarios is explicitly listed for BiCNet in the paper table; the >85% figure and 'outperforms in 4/5 scenarios' are direct reported quantitative claims.)</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Authors report that BiCNet's relative advantage grows with increased environment complexity (notably when the number of agents exceeds ~10 the performance gap vs baselines widens). They also discuss that high variation in StarCraft (many different behaviours and states) made it difficult to identify concrete semantic meaning in the learned communication messages, indicating a trade-off where high variation complicates interpretability of emergent communication though the learned policy performance remains strong. Parameter sharing reduces parameter count and supports scaling across varying team sizes, helping mitigate complexity-induced learning difficulties.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>End-to-end single-task training per scenario using centralized critic / decentralized actors with multiagent deterministic policy gradient, experience replay and off-policy updates; parameter sharing across agents to keep parameter size independent of agent count. No explicit curriculum learning or domain randomization protocol reported.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Authors evaluate BiCNet across multiple distinct combat scenarios (varying unit counts and types); they claim parameter sharing enables the network trained on smaller teams (e.g., 3 agents) to effectively scale to larger teams at test time (domain adaptation potential). Quantitative generalization numbers are not fully enumerated, but experimentally BiCNet is evaluated in many different scenarios up to 20 vs 30.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Authors report rapid acquisition of some tactics: 'BiCNet only needs ~10 combats before learning the idea of focus fire' (i.e., ~10 episodes to >85% win-rate on that specific 5v5 example). Final models reported trained over ~100k environment steps for evaluation; batch tuning experiments mention models trained on 800 episodes (~700k steps) for hyperparameter selection.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>BiCNet with bidirectional RNN communication and parameter sharing enables emergence of human-like multiagent coordination tactics (hit-and-run, coordinated moves without collision, cover attacks, focus fire without overkill), achieves higher winning rates than several baselines on most StarCraft micromanagement tasks, demonstrates superior sample efficiency for some coordination skills (e.g., focus fire learned in ~10 episodes), and its advantage increases as environment complexity (number of agents) grows; however, high environment variation hampers straightforward interpretation of message semantics despite successful coordination.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multiagent Bidirectionally-Coordinated Nets: Emergence of Human-level Coordination in Learning to Play StarCraft Combat Games', 'publication_date_yy_mm': '2017-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1094.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1094.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BiCNet-GuessSum</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BiCNet applied to multi-agent sum-guessing toy task</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Application of the BiCNet architecture to a simplified cooperative task where each agent observes a scalar and must predict the sum across agents; used to examine communication semantics and performance vs baselines as agent count varies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>BiCNet agents (toy sum-guessing)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Same BiCNet architecture (bidirectional RNN + per-agent output MLP) trained in a supervised / RL formulation appropriate for the toy task; agents cooperate through learned messages to predict global sum.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agents (toy multi-agent environment)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>n-agent sum-guessing game (inputs sampled from truncated Gaussian over [-10,10])</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>A simplified fully-observable toy environment with n agents (n = 5, 10, 20 in experiments); each agent receives only its local scalar observation and must output a prediction equal to the sum of all agents' observations; reward is based on prediction error.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Number of agents (n ∈ {5,10,20}) is the primary complexity parameter; also increasing combinatorial communication requirements with n.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>quantitatively: tested at n=5 (lower complexity), n=10 (moderate), n=20 (higher complexity)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Variation measured by agent count (different problem sizes); inputs sampled stochastically per episode from a truncated Gaussian (randomness across episodes).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>moderate (stochastic inputs) with controlled discrete variation by number of agents</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Mean absolute error between each agent's action (prediction) and the true sum; reported with standard deviation over 10,000 test steps.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>BiCNet absolute error (mean ± std) from Table 2: n=5: 0.52 ± 0.51; n=10: 0.97 ± 0.91; n=20: 3.12 ± 2.93 (lower is better). BiCNet outperforms CommNet and other baselines at all tested n.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Performance degrades (higher absolute error) as agent count increases (complexity ↑), but BiCNet degrades more gracefully than baselines (e.g., CommNet and independent controllers). This shows the architecture's communication/aggregation scales better with complexity (agent number) in this toy task.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td>n=20 result: 3.12 ± 2.93 (absolute error)</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td>n=5 result: 0.52 ± 0.51 (absolute error)</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Supervised / cooperative multiagent training of BiCNet on the toy task; training details not exhaustively enumerated (reporting test statistics over 10,000 testing steps).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Evaluated at multiple agent counts (5, 10, 20); BiCNet generalized across these problem sizes in the sense of being trained and tested at those sizes, and outperformed other baselines at each size. No cross-size transfer experiments were explicitly reported (i.e., training at one n and testing at another).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Test metrics are reported over 10,000 testing steps; training sample counts are not precisely enumerated in the paper for the toy task.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>BiCNet effectively learns inter-agent additive message passing that composes information (hidden-state values reflect partial sums), enabling lower prediction error than CommNet, independent controllers and supervised MLP baselines; absolute error increases with agent count but remains lower than baselines, demonstrating better scaling with problem complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multiagent Bidirectionally-Coordinated Nets: Emergence of Human-level Coordination in Learning to Play StarCraft Combat Games', 'publication_date_yy_mm': '2017-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1094.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1094.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CommNet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CommNet (Learning multiagent communication with backpropagation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior multiagent architecture that shares a centralized network and passes averaged messages between agent modules (symmetric communication), used here as a main baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning Multiagent Communication with Backpropagation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>CommNet agents (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agents implemented according to CommNet: a joint network architecture where inter-agent communication is implemented by averaging module outputs and feeding them back into agent modules; trained end-to-end in the StarCraft tasks and the toy guessing task as a baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agents (virtual game units / toy agents)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>StarCraft micromanagement scenarios and the n-agent sum-guessing toy task</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same environment families as used for BiCNet: StarCraft combats with varied unit types and sizes (high-dimensional, stochastic), and a controlled toy sum-guessing game with different agent counts.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Number of agents and unit types in StarCraft; in toy task n ∈ {5,10,20}.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>varies: tested across low (3v1) to high (20v30) complexity StarCraft scenarios and n up to 20 in the toy task.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Varied combat scenarios and agent counts; stochastic initial states and built-in AI opponents in StarCraft.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>high</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Winning rate in StarCraft tests; absolute prediction error in the toy guessing task.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>StarCraft (per-table columns labelled CommNet): 20M vs 30Z = 1.00; 5M vs 5M = 0.950; 15M vs 16M = 0.680; 10M vs 13Z = 0.440; 15W vs 17W = 0.470 (winning rates as provided in the paper table). Toy guessing task (absolute error): n=5: 0.57 ± 0.41; n=10: 1.18 ± 0.90; n=20: 3.88 ± 3.03.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>CommNet handles communication by averaging messages which fits some problems (explains competitive results in certain tasks), but authors report BiCNet often learns faster and achieves better sample efficiency and higher final performance in many StarCraft scenarios, especially as complexity (agent count) grows; CommNet's symmetric averaging can limit handling of heterogeneous agents.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Implemented and trained under the same (state, action) spaces and training processes as BiCNet for fair comparison (end-to-end training, replay buffer, same optimizer/hyperparameters where applicable).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Tested across multiple StarCraft scenarios and toy-agent counts; CommNet performs well on many scenarios but tends to learn some coordination skills (e.g., focus fire) more slowly than BiCNet (CommNet required >50 episodes to learn focus fire vs BiCNet ~10 episodes in a given scenario).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>CommNet required more episodes than BiCNet to acquire some coordinated tactics (e.g., >50 episodes to learn focus fire in the 5v5 scenario, as reported).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>CommNet is a strong baseline for multiagent communication but BiCNet's bidirectional, ordered RNN communication and parameter sharing yield faster acquisition of some coordination behaviours and improved performance in many tested high-complexity StarCraft scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multiagent Bidirectionally-Coordinated Nets: Emergence of Human-level Coordination in Learning to Play StarCraft Combat Games', 'publication_date_yy_mm': '2017-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1094.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1094.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IND</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Independent controller (IND baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline where each agent is trained with a single-agent controller independently (no inter-agent information sharing).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Independent agents (IND)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Each unit is controlled by an independently trained single-agent controller with no learned communication or shared latent coordination; adapted straightforwardly to multiagent combats but lacks explicit inter-agent cooperation.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agents (virtual game units)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>StarCraft micromanagement scenarios</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same set of StarCraft combat scenarios used in the paper, varying number/type of units and difficulty levels.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Number of agents; performance degrades with scenarios requiring coordinated team-level behaviours (e.g., focus fire).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>tested across easy to hard scenarios (3v1 up to 20v30)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Multiple combat scenarios and unit compositions; stochasticity in opponent behaviour.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>high</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Winning rate</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Table 1 IND column: 20M vs 30Z = 0.940; 5M vs 5M = 0.310; 15M vs 16M = 0.590; 10M vs 13Z = 0.522; 15W vs 17W = 0.310 (winning rates).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Independent controllers perform relatively poorly on tasks that require explicit coordination (e.g., focus fire), indicating that increased complexity (multi-agent interdependence) penalizes independent learners; sometimes perform well in tasks where coordination is less critical.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Single-agent training per unit type, no inter-agent communication or centralized critic.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Independent controllers can be effective in some scenarios but generally fail to learn coordinated team strategies (e.g., focus fire) and show degraded performance in complex team-dependent combats compared with collaborative architectures.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multiagent Bidirectionally-Coordinated Nets: Emergence of Human-level Coordination in Learning to Play StarCraft Combat Games', 'publication_date_yy_mm': '2017-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1094.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1094.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fully-connected multiagent controller (FC baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline multiagent controller where agents are controlled collectively via a fully-connected communication/parameterization (no parameter-sharing across varying numbers of agents), trained for specific agent counts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Fully-connected multiagent controller (FC)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A multiagent model trained jointly with fully-connected communication; model architecture tied to a specific number of agents and hence requires retraining when team sizes change.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agents (virtual game units)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>StarCraft micromanagement scenarios</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same varied combat scenarios as other baselines; FC model is retrained per scenario because of fixed architecture tied to agent count.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Number of agents and unit types; model capacity and parameter count scale with the number of agents.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>varies; tested across easy to hard scenarios</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Variation by scenario and unit composition; model not parameter-sharing so less adaptable to different agent counts without retraining.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>high</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Winning rate</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Table 1 FC column: 20M vs 30Z = 0.001; 5M vs 5M = 0.080; 15M vs 16M = 0.440; 10M vs 13Z = 0.430; 15W vs 17W = 0.460 (winning rates).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Because the FC architecture must be re-trained when agent counts change and parameter count scales with agents, it is less practical and tends to underperform on several scenarios compared to communication-enabled methods with parameter sharing.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Joint multiagent training with fully-connected architecture; requires retraining when the number of units changes.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>FC baseline generally underperforms compared to communication-enabled architectures with parameter sharing (CommNet, BiCNet), and is less scalable/adaptable due to architecture dependence on agent count.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multiagent Bidirectionally-Coordinated Nets: Emergence of Human-level Coordination in Learning to Play StarCraft Combat Games', 'publication_date_yy_mm': '2017-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1094.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1094.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GMEZO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GMEZO (Greedy MDP with Episodic Zero-Order Optimisation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An episodic zero-order optimisation method applied to StarCraft micromanagement that combines greedy-MDP agent decomposition with episodic parameter-space exploration; used as a competitive RL baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Episodic exploration for deep deterministic policies: An application to starcraft micromanagement tasks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GMEZO agents (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agents trained using greedy MDP decomposition with episodic zero-order noise in parameter space (episodic exploration), specifically proposed for StarCraft micromanagement tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agents (virtual game units)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>StarCraft micromanagement scenarios</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same StarCraft combat scenarios used by the authors; GMEZO is designed for these micromanagement tasks and is compared directly with BiCNet and other baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Number of agents, unit types and micro-management complexity as in other baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>varies across tested scenarios</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Varied combat scenarios and unit compositions with stochastic elements from built-in AI opponents.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>high</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Winning rate</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Table 1 GMEZO column: 20M vs 30Z = 0.880; 5M vs 5M = 0.910; 15M vs 16M = 0.630; 10M vs 13Z = 0.570; 15W vs 17W = 0.420 (winning rates).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>GMEZO can achieve competitive performance on several scenarios and matches or exceeds some baselines in specific tasks (e.g., 5M vs 5M), indicating episodic parameter-space exploration and greedy MDP decomposition can be effective; however, BiCNet often outperforms GMEZO in many scenarios and exhibits faster acquisition of some collaborative strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Greedy MDP decomposition combined with episodic zero-order parameter-space exploration (as in Usunier et al. 2016). In the paper the authors also experiment with replacing BiCNet's bidirectional formulation with the greedy-MDP formulation to compare ideas.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GMEZO is a strong baseline for StarCraft micromanagement and can capture collaborative behaviours via greedy MDP decomposition and episodic exploration, but BiCNet's communication architecture and parameter sharing result in better or faster performance on many tested tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multiagent Bidirectionally-Coordinated Nets: Emergence of Human-level Coordination in Learning to Play StarCraft Combat Games', 'publication_date_yy_mm': '2017-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Learning Multiagent Communication with Backpropagation <em>(Rating: 2)</em></li>
                <li>Episodic exploration for deep deterministic policies: An application to starcraft micromanagement tasks <em>(Rating: 2)</em></li>
                <li>Multi-agent actor-critic for mixed cooperative-competitive environments <em>(Rating: 1)</em></li>
                <li>Counterfactual multi-agent policy gradients <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1094",
    "paper_id": "paper-76dfb1ab698963f3776fe894b3743db4a5419a5f",
    "extraction_schema_id": "extraction-schema-24",
    "extracted_data": [
        {
            "name_short": "BiCNet",
            "name_full": "Multiagent Bidirectionally-Coordinated Network (BiCNet)",
            "brief_description": "A deep multiagent reinforcement learning architecture that uses bidirectional recurrent networks as a communication channel and a vectorised actor-critic (multiagent deterministic policy gradient) formulation with parameter sharing to coordinate many agents in continuous-action multiagent tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "BiCNet-controlled agents",
            "agent_description": "Simulated multi-agent units controlled by a shared-parameter actor network with bidirectional RNN communication and a centralized critic; trained with an off-policy deterministic actor-critic framework (multiagent deterministic policy gradient) using Ornstein–Uhlenbeck exploration and replay buffers.",
            "agent_type": "simulated agent (virtual game units in StarCraft)",
            "environment_name": "StarCraft micromanagement (combat) scenarios",
            "environment_description": "Real-time strategy micro-management combats in StarCraft where each episode contains multiple allied agents vs multiple enemy units; scenarios vary by unit types (Marines, Zerglings, Wraiths, Dragoons, Ultralisks, Dropships, Tanks, etc.), number of units per side, initial positions and terrain, producing a high-dimensional, highly variable game state with many interacting entities and time-varying health dynamics.",
            "complexity_measure": "Characterized quantitatively by number of allied agents N and enemy agents M (experiments: e.g., 3v1, 3v3, 5v5, 10v13, 15v16, 20v30, 15v17, heterogeneous 2 Dropships+2 Tanks vs 1 Ultralisk); implicitly by large state-space (authors note StarCraft micromanagement has more possible states than Go); task difficulty category labels used: 'easy', 'hard', 'heterogeneous'.",
            "complexity_level": "varies across experiments (low for easy scenarios like 3v1 or 3v3; high for hard scenarios with &gt;10 agents per side such as 15v16 or 20v30)",
            "variation_measure": "Variation realized by changing unit types (heterogeneous vs homogeneous teams), different numbers of units, multiple distinct combat scenarios, stochastic initial states and built-in AI behaviours; authors note 'high variability of the StarCraft game' when discussing communication semantics.",
            "variation_level": "high (multiple distinct scenarios, unit-type heterogeneity, and stochastic game variability)",
            "performance_metric": "Primary: winning rate (fraction of games won over test set). Secondary observed metrics: critic Q-values (mean Q), qualitative emergence of tactics, and in toy task absolute error between action and target.",
            "performance_value": "Reported examples: trained BiCNet models evaluated after training (e.g. trained over 100k steps) and measured as average winning rate over 100 test games. Specific reported results: 'BiCNet learns focus-fire in ~10 combats and achieves &gt;85% win rate in 5 Marines vs 5 Marines after ~10 training episodes'; authors state BiCNet outperforms baselines in 4/5 tested StarCraft scenarios and shows larger margin when number of agents &gt;10. (No single consolidated numeric column for all scenarios is explicitly listed for BiCNet in the paper table; the &gt;85% figure and 'outperforms in 4/5 scenarios' are direct reported quantitative claims.)",
            "complexity_variation_relationship": "Authors report that BiCNet's relative advantage grows with increased environment complexity (notably when the number of agents exceeds ~10 the performance gap vs baselines widens). They also discuss that high variation in StarCraft (many different behaviours and states) made it difficult to identify concrete semantic meaning in the learned communication messages, indicating a trade-off where high variation complicates interpretability of emergent communication though the learned policy performance remains strong. Parameter sharing reduces parameter count and supports scaling across varying team sizes, helping mitigate complexity-induced learning difficulties.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "End-to-end single-task training per scenario using centralized critic / decentralized actors with multiagent deterministic policy gradient, experience replay and off-policy updates; parameter sharing across agents to keep parameter size independent of agent count. No explicit curriculum learning or domain randomization protocol reported.",
            "generalization_tested": true,
            "generalization_results": "Authors evaluate BiCNet across multiple distinct combat scenarios (varying unit counts and types); they claim parameter sharing enables the network trained on smaller teams (e.g., 3 agents) to effectively scale to larger teams at test time (domain adaptation potential). Quantitative generalization numbers are not fully enumerated, but experimentally BiCNet is evaluated in many different scenarios up to 20 vs 30.",
            "sample_efficiency": "Authors report rapid acquisition of some tactics: 'BiCNet only needs ~10 combats before learning the idea of focus fire' (i.e., ~10 episodes to &gt;85% win-rate on that specific 5v5 example). Final models reported trained over ~100k environment steps for evaluation; batch tuning experiments mention models trained on 800 episodes (~700k steps) for hyperparameter selection.",
            "key_findings": "BiCNet with bidirectional RNN communication and parameter sharing enables emergence of human-like multiagent coordination tactics (hit-and-run, coordinated moves without collision, cover attacks, focus fire without overkill), achieves higher winning rates than several baselines on most StarCraft micromanagement tasks, demonstrates superior sample efficiency for some coordination skills (e.g., focus fire learned in ~10 episodes), and its advantage increases as environment complexity (number of agents) grows; however, high environment variation hampers straightforward interpretation of message semantics despite successful coordination.",
            "uuid": "e1094.0",
            "source_info": {
                "paper_title": "Multiagent Bidirectionally-Coordinated Nets: Emergence of Human-level Coordination in Learning to Play StarCraft Combat Games",
                "publication_date_yy_mm": "2017-03"
            }
        },
        {
            "name_short": "BiCNet-GuessSum",
            "name_full": "BiCNet applied to multi-agent sum-guessing toy task",
            "brief_description": "Application of the BiCNet architecture to a simplified cooperative task where each agent observes a scalar and must predict the sum across agents; used to examine communication semantics and performance vs baselines as agent count varies.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "BiCNet agents (toy sum-guessing)",
            "agent_description": "Same BiCNet architecture (bidirectional RNN + per-agent output MLP) trained in a supervised / RL formulation appropriate for the toy task; agents cooperate through learned messages to predict global sum.",
            "agent_type": "simulated agents (toy multi-agent environment)",
            "environment_name": "n-agent sum-guessing game (inputs sampled from truncated Gaussian over [-10,10])",
            "environment_description": "A simplified fully-observable toy environment with n agents (n = 5, 10, 20 in experiments); each agent receives only its local scalar observation and must output a prediction equal to the sum of all agents' observations; reward is based on prediction error.",
            "complexity_measure": "Number of agents (n ∈ {5,10,20}) is the primary complexity parameter; also increasing combinatorial communication requirements with n.",
            "complexity_level": "quantitatively: tested at n=5 (lower complexity), n=10 (moderate), n=20 (higher complexity)",
            "variation_measure": "Variation measured by agent count (different problem sizes); inputs sampled stochastically per episode from a truncated Gaussian (randomness across episodes).",
            "variation_level": "moderate (stochastic inputs) with controlled discrete variation by number of agents",
            "performance_metric": "Mean absolute error between each agent's action (prediction) and the true sum; reported with standard deviation over 10,000 test steps.",
            "performance_value": "BiCNet absolute error (mean ± std) from Table 2: n=5: 0.52 ± 0.51; n=10: 0.97 ± 0.91; n=20: 3.12 ± 2.93 (lower is better). BiCNet outperforms CommNet and other baselines at all tested n.",
            "complexity_variation_relationship": "Performance degrades (higher absolute error) as agent count increases (complexity ↑), but BiCNet degrades more gracefully than baselines (e.g., CommNet and independent controllers). This shows the architecture's communication/aggregation scales better with complexity (agent number) in this toy task.",
            "high_complexity_low_variation_performance": "n=20 result: 3.12 ± 2.93 (absolute error)",
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": "n=5 result: 0.52 ± 0.51 (absolute error)",
            "training_strategy": "Supervised / cooperative multiagent training of BiCNet on the toy task; training details not exhaustively enumerated (reporting test statistics over 10,000 testing steps).",
            "generalization_tested": true,
            "generalization_results": "Evaluated at multiple agent counts (5, 10, 20); BiCNet generalized across these problem sizes in the sense of being trained and tested at those sizes, and outperformed other baselines at each size. No cross-size transfer experiments were explicitly reported (i.e., training at one n and testing at another).",
            "sample_efficiency": "Test metrics are reported over 10,000 testing steps; training sample counts are not precisely enumerated in the paper for the toy task.",
            "key_findings": "BiCNet effectively learns inter-agent additive message passing that composes information (hidden-state values reflect partial sums), enabling lower prediction error than CommNet, independent controllers and supervised MLP baselines; absolute error increases with agent count but remains lower than baselines, demonstrating better scaling with problem complexity.",
            "uuid": "e1094.1",
            "source_info": {
                "paper_title": "Multiagent Bidirectionally-Coordinated Nets: Emergence of Human-level Coordination in Learning to Play StarCraft Combat Games",
                "publication_date_yy_mm": "2017-03"
            }
        },
        {
            "name_short": "CommNet",
            "name_full": "CommNet (Learning multiagent communication with backpropagation)",
            "brief_description": "A prior multiagent architecture that shares a centralized network and passes averaged messages between agent modules (symmetric communication), used here as a main baseline.",
            "citation_title": "Learning Multiagent Communication with Backpropagation",
            "mention_or_use": "use",
            "agent_name": "CommNet agents (baseline)",
            "agent_description": "Agents implemented according to CommNet: a joint network architecture where inter-agent communication is implemented by averaging module outputs and feeding them back into agent modules; trained end-to-end in the StarCraft tasks and the toy guessing task as a baseline.",
            "agent_type": "simulated agents (virtual game units / toy agents)",
            "environment_name": "StarCraft micromanagement scenarios and the n-agent sum-guessing toy task",
            "environment_description": "Same environment families as used for BiCNet: StarCraft combats with varied unit types and sizes (high-dimensional, stochastic), and a controlled toy sum-guessing game with different agent counts.",
            "complexity_measure": "Number of agents and unit types in StarCraft; in toy task n ∈ {5,10,20}.",
            "complexity_level": "varies: tested across low (3v1) to high (20v30) complexity StarCraft scenarios and n up to 20 in the toy task.",
            "variation_measure": "Varied combat scenarios and agent counts; stochastic initial states and built-in AI opponents in StarCraft.",
            "variation_level": "high",
            "performance_metric": "Winning rate in StarCraft tests; absolute prediction error in the toy guessing task.",
            "performance_value": "StarCraft (per-table columns labelled CommNet): 20M vs 30Z = 1.00; 5M vs 5M = 0.950; 15M vs 16M = 0.680; 10M vs 13Z = 0.440; 15W vs 17W = 0.470 (winning rates as provided in the paper table). Toy guessing task (absolute error): n=5: 0.57 ± 0.41; n=10: 1.18 ± 0.90; n=20: 3.88 ± 3.03.",
            "complexity_variation_relationship": "CommNet handles communication by averaging messages which fits some problems (explains competitive results in certain tasks), but authors report BiCNet often learns faster and achieves better sample efficiency and higher final performance in many StarCraft scenarios, especially as complexity (agent count) grows; CommNet's symmetric averaging can limit handling of heterogeneous agents.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Implemented and trained under the same (state, action) spaces and training processes as BiCNet for fair comparison (end-to-end training, replay buffer, same optimizer/hyperparameters where applicable).",
            "generalization_tested": true,
            "generalization_results": "Tested across multiple StarCraft scenarios and toy-agent counts; CommNet performs well on many scenarios but tends to learn some coordination skills (e.g., focus fire) more slowly than BiCNet (CommNet required &gt;50 episodes to learn focus fire vs BiCNet ~10 episodes in a given scenario).",
            "sample_efficiency": "CommNet required more episodes than BiCNet to acquire some coordinated tactics (e.g., &gt;50 episodes to learn focus fire in the 5v5 scenario, as reported).",
            "key_findings": "CommNet is a strong baseline for multiagent communication but BiCNet's bidirectional, ordered RNN communication and parameter sharing yield faster acquisition of some coordination behaviours and improved performance in many tested high-complexity StarCraft scenarios.",
            "uuid": "e1094.2",
            "source_info": {
                "paper_title": "Multiagent Bidirectionally-Coordinated Nets: Emergence of Human-level Coordination in Learning to Play StarCraft Combat Games",
                "publication_date_yy_mm": "2017-03"
            }
        },
        {
            "name_short": "IND",
            "name_full": "Independent controller (IND baseline)",
            "brief_description": "A baseline where each agent is trained with a single-agent controller independently (no inter-agent information sharing).",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Independent agents (IND)",
            "agent_description": "Each unit is controlled by an independently trained single-agent controller with no learned communication or shared latent coordination; adapted straightforwardly to multiagent combats but lacks explicit inter-agent cooperation.",
            "agent_type": "simulated agents (virtual game units)",
            "environment_name": "StarCraft micromanagement scenarios",
            "environment_description": "Same set of StarCraft combat scenarios used in the paper, varying number/type of units and difficulty levels.",
            "complexity_measure": "Number of agents; performance degrades with scenarios requiring coordinated team-level behaviours (e.g., focus fire).",
            "complexity_level": "tested across easy to hard scenarios (3v1 up to 20v30)",
            "variation_measure": "Multiple combat scenarios and unit compositions; stochasticity in opponent behaviour.",
            "variation_level": "high",
            "performance_metric": "Winning rate",
            "performance_value": "Table 1 IND column: 20M vs 30Z = 0.940; 5M vs 5M = 0.310; 15M vs 16M = 0.590; 10M vs 13Z = 0.522; 15W vs 17W = 0.310 (winning rates).",
            "complexity_variation_relationship": "Independent controllers perform relatively poorly on tasks that require explicit coordination (e.g., focus fire), indicating that increased complexity (multi-agent interdependence) penalizes independent learners; sometimes perform well in tasks where coordination is less critical.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Single-agent training per unit type, no inter-agent communication or centralized critic.",
            "generalization_tested": false,
            "generalization_results": null,
            "sample_efficiency": null,
            "key_findings": "Independent controllers can be effective in some scenarios but generally fail to learn coordinated team strategies (e.g., focus fire) and show degraded performance in complex team-dependent combats compared with collaborative architectures.",
            "uuid": "e1094.3",
            "source_info": {
                "paper_title": "Multiagent Bidirectionally-Coordinated Nets: Emergence of Human-level Coordination in Learning to Play StarCraft Combat Games",
                "publication_date_yy_mm": "2017-03"
            }
        },
        {
            "name_short": "FC",
            "name_full": "Fully-connected multiagent controller (FC baseline)",
            "brief_description": "A baseline multiagent controller where agents are controlled collectively via a fully-connected communication/parameterization (no parameter-sharing across varying numbers of agents), trained for specific agent counts.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Fully-connected multiagent controller (FC)",
            "agent_description": "A multiagent model trained jointly with fully-connected communication; model architecture tied to a specific number of agents and hence requires retraining when team sizes change.",
            "agent_type": "simulated agents (virtual game units)",
            "environment_name": "StarCraft micromanagement scenarios",
            "environment_description": "Same varied combat scenarios as other baselines; FC model is retrained per scenario because of fixed architecture tied to agent count.",
            "complexity_measure": "Number of agents and unit types; model capacity and parameter count scale with the number of agents.",
            "complexity_level": "varies; tested across easy to hard scenarios",
            "variation_measure": "Variation by scenario and unit composition; model not parameter-sharing so less adaptable to different agent counts without retraining.",
            "variation_level": "high",
            "performance_metric": "Winning rate",
            "performance_value": "Table 1 FC column: 20M vs 30Z = 0.001; 5M vs 5M = 0.080; 15M vs 16M = 0.440; 10M vs 13Z = 0.430; 15W vs 17W = 0.460 (winning rates).",
            "complexity_variation_relationship": "Because the FC architecture must be re-trained when agent counts change and parameter count scales with agents, it is less practical and tends to underperform on several scenarios compared to communication-enabled methods with parameter sharing.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Joint multiagent training with fully-connected architecture; requires retraining when the number of units changes.",
            "generalization_tested": false,
            "generalization_results": null,
            "sample_efficiency": null,
            "key_findings": "FC baseline generally underperforms compared to communication-enabled architectures with parameter sharing (CommNet, BiCNet), and is less scalable/adaptable due to architecture dependence on agent count.",
            "uuid": "e1094.4",
            "source_info": {
                "paper_title": "Multiagent Bidirectionally-Coordinated Nets: Emergence of Human-level Coordination in Learning to Play StarCraft Combat Games",
                "publication_date_yy_mm": "2017-03"
            }
        },
        {
            "name_short": "GMEZO",
            "name_full": "GMEZO (Greedy MDP with Episodic Zero-Order Optimisation)",
            "brief_description": "An episodic zero-order optimisation method applied to StarCraft micromanagement that combines greedy-MDP agent decomposition with episodic parameter-space exploration; used as a competitive RL baseline.",
            "citation_title": "Episodic exploration for deep deterministic policies: An application to starcraft micromanagement tasks",
            "mention_or_use": "use",
            "agent_name": "GMEZO agents (baseline)",
            "agent_description": "Agents trained using greedy MDP decomposition with episodic zero-order noise in parameter space (episodic exploration), specifically proposed for StarCraft micromanagement tasks.",
            "agent_type": "simulated agents (virtual game units)",
            "environment_name": "StarCraft micromanagement scenarios",
            "environment_description": "Same StarCraft combat scenarios used by the authors; GMEZO is designed for these micromanagement tasks and is compared directly with BiCNet and other baselines.",
            "complexity_measure": "Number of agents, unit types and micro-management complexity as in other baselines.",
            "complexity_level": "varies across tested scenarios",
            "variation_measure": "Varied combat scenarios and unit compositions with stochastic elements from built-in AI opponents.",
            "variation_level": "high",
            "performance_metric": "Winning rate",
            "performance_value": "Table 1 GMEZO column: 20M vs 30Z = 0.880; 5M vs 5M = 0.910; 15M vs 16M = 0.630; 10M vs 13Z = 0.570; 15W vs 17W = 0.420 (winning rates).",
            "complexity_variation_relationship": "GMEZO can achieve competitive performance on several scenarios and matches or exceeds some baselines in specific tasks (e.g., 5M vs 5M), indicating episodic parameter-space exploration and greedy MDP decomposition can be effective; however, BiCNet often outperforms GMEZO in many scenarios and exhibits faster acquisition of some collaborative strategies.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Greedy MDP decomposition combined with episodic zero-order parameter-space exploration (as in Usunier et al. 2016). In the paper the authors also experiment with replacing BiCNet's bidirectional formulation with the greedy-MDP formulation to compare ideas.",
            "generalization_tested": false,
            "generalization_results": null,
            "sample_efficiency": null,
            "key_findings": "GMEZO is a strong baseline for StarCraft micromanagement and can capture collaborative behaviours via greedy MDP decomposition and episodic exploration, but BiCNet's communication architecture and parameter sharing result in better or faster performance on many tested tasks.",
            "uuid": "e1094.5",
            "source_info": {
                "paper_title": "Multiagent Bidirectionally-Coordinated Nets: Emergence of Human-level Coordination in Learning to Play StarCraft Combat Games",
                "publication_date_yy_mm": "2017-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Learning Multiagent Communication with Backpropagation",
            "rating": 2
        },
        {
            "paper_title": "Episodic exploration for deep deterministic policies: An application to starcraft micromanagement tasks",
            "rating": 2
        },
        {
            "paper_title": "Multi-agent actor-critic for mixed cooperative-competitive environments",
            "rating": 1
        },
        {
            "paper_title": "Counterfactual multi-agent policy gradients",
            "rating": 1
        }
    ],
    "cost": 0.02274325,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Multiagent Bidirectionally-Coordinated Nets</h1>
<h2>Emergence of Human-level Coordination in Learning to Play StarCraft Combat Games*</h2>
<p>Peng Peng ${ }^{\S}$, Ying Wen ${ }^{\S 1}$, Yaodong Yang ${ }^{\S}$, Yuan Quan ${ }^{\S}$, Zhenkun Tang ${ }^{\S}$, Haitao Long ${ }^{\S}$, Jun Wang ${ }^{\S}$<br>${ }^{\S}$ University College London, ${ }^{\S}$ Alibaba Group</p>
<h4>Abstract</h4>
<p>Many artificial intelligence (AI) applications often require multiple intelligent agents to work in a collaborative effort. Efficient learning for intra-agent communication and coordination is an indispensable step towards general AI. In this paper, we take StarCraft combat game as a case study, where the task is to coordinate multiple agents as a team to defeat their enemies. To maintain a scalable yet effective communication protocol, we introduce a Multiagent BidirectionallyCoordinated Network (BiCNet [3nknet]) with a vectorised extension of actor-critic formulation. We show that BiCNet can handle different types of combats with arbitrary numbers of AI agents for both sides. Our analysis demonstrates that without any supervisions such as human demonstrations or labelled data, BiCNet could learn various types of advanced coordination strategies that have been commonly used by experienced game players. In our experiments, we evaluate our approach against multiple baselines under different scenarios; it shows state-of-the-art performance, and possesses potential values for large-scale real-world applications.</p>
<h2>Introduction</h2>
<p>The last decade has witnessed massive progresses in the field of Artificial Intelligence (AI). With supervision from labelled data, machines have, to some extent, exceeded humanlevel perception on visual recognitions and speech recognitions, while fed with feedback reward, single AI units (aka agents) defeat humans in various games including Atari video games (Mnih et al. 2015), Go game (Silver et al. 2016), and card game (Brown and Sandholm 2017).</p>
<p>Yet, true human intelligence embraces social and collective wisdom which lays an essential foundation for reaching the grand goal of Artificial General Intelligence (AGI) (Goertzel and Pennachin 2007). As demonstrated by crowd sourcing, aggregating efforts collectively from the public would solve the problem that otherwise is unthinkable by a single person. Even social animals like a brood of well-organised ants could accomplish challenging tasks such as hunting, building a kingdom, and even waging a war, although each ant by itself is weak and limited. Interestingly, in the coming era of algorithmic economy, AI agents with a certain rudimentary level of artificial collective intelligence start to emerge from multiple domains. Typical examples include the trading robots</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>gaming on the stock markets (Deboeck 1994), ad bidding agents competing with each other over online advertising exchanges (Wang, Zhang, and Yuan 2017), and e-commerce collaborative filtering recommenders predicting user interests through the wisdom of the crowd (Schafer, Konstan, and Riedl 1999).</p>
<p>We thus believe a next grand challenge of AGI is to answer how multiple AI agents could learn human-level collaborations, or competitions, from their experiences with the environment where both of their incentives and economic constraints co-exist. As the flourishes of deep reinforcement learning (DRL) (Mnih et al. 2015; Silver et al. 2016), researchers start to shed light on tackling multiagent problems (Schmidhuber 1996) with the enhanced learning capabilities, e.g., (Sukhbaatar, Fergus, and others 2016; Mordatch and Abbeel 2017).</p>
<p>In this paper, we leverage a real-time strategy game, Star$C r a f t^{1}$, as the use case to explore the learning of intelligent collaborative behaviours among multiple agents. Particularly, we focus on StarCraft micromanagement tasks (Synnaeve et al. 2016), where each player controls their own units (with different functions to collaborate) to destroy the opponents army in the combats under different terrain conditions. Such game is considered as one of the most difficult games for computers with more possible states than Go game (Synnaeve et al. 2016). The learning of this large-scale multiagent system faces a major challenge that the parameters space grows exponentially with the increasing number of agents involved. As such, the behaviours of the agents can become so sophisticated that any joint learner method (Sukhbaatar, Fergus, and others 2016) would be inefficient and unable to deal with the changing number of agents in the game.</p>
<p>We formulate multiagent learning for StarCraft combat tasks as a zero-sum Stochastic Game. Agents are communicated by our proposed bidirectionally-coordinated net (BiCNet), while the learning is done using a multiagent actor-critic framework. In addition, we also introduce parameter sharing to solve the scalability issue. We observe that BiCNet can automatically learn various optimal strategies to coordinating agents, similar to what experienced human players would adopt in playing the StarCraft game, ranging from trivial move without collision to a basic tactic hit and run to sophisticated cover attack, and focus fire without overkill. We have conducted our experiments by testing over a set of combat tasks with different levels of difficulties. Our method</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>outperforms state-of-the-art methods and shows its potential usage in a wide range of multiagent tasks in the real-world applications.</p>
<h2>Related Work</h2>
<p>The studies on interaction and collaboration in multiagent settings have a long history <em>Littman (1994); Schmidhuber (1996)</em>. Although limited to toy examples in the beginning, reinforcement learning, as a means, has long been applied to multiagent systems in order to learn optimal collaboration policies. One of the key components in multiagent RL is to learn a communication protocol among agents. With deep learning, representative solutions include the differentiable inter-agent learning (DIAL) <em>Foerster et al. (2016)</em> and the CommNet <em>Sukhbaatar et al. (2016)</em>, both of which are end-to-end trainable by back-propagation.</p>
<p>DIAL <em>Foerster et al. (2016)</em> was introduced in partially observable settings where messages passing between agents are allowed. The agent is also named as a <em>independent learner</em>. The idea of learning independent agents can also be found <em>Lauer and Riedmiller (2000); Kapetanakis and Kudenko (2002); Lauer and Riedmiller (2004); Foerster et al. (2016)</em>. In DIAL, each agent consists of a recurrent neural network that outputs individual agent’s Q-value and a message to transfer for each time-step. The generated messages is then transferred to other agents as used as inputs for others in the next time step. The received messages will be embedded with agent’s current observations and last action as the representation of the global information. Communication between independent agents is one way to mitigate the notorious non-stationary issue in the mutliagent settings as the gradients will at least flow among the agents; however, researchers are still looking for better solutions for complex environments such as StarCraft.</p>
<p>By contrast, CommNet <em>Sukhbaatar et al. (2016)</em> is designed for <em>joint action learners</em> in fully observable settings. Unlike DIAL, CommNet proposes a single network in the multiagent setting, passing the averaged message over the agent modules between layers. However, as the communication network is fully symmetric and embedded in the original network, it lacks the ability of handle heterogeneous agent types. Also it is a single network for all agents, and therefore its scalability is unclear. In this paper, we solve these issues by creating a dedicated bi-directional communication channel using recurrent neural networks <em>Schuster and Paliwal (1997)</em>. As such, heterogeneous agents can be created with a different set of parameters and output actions. The bi-directional nature means that the communication is not entirely symmetric, and the different priority among agents would help solving any possible tie between multiple optimal joint actions <em>Busoniu et al. (2008); Spaan et al. (2002)</em>.</p>
<p>Multiagent systems have been explored on specific StarCraft games. Google DeepMind released a game interface based on StarCraft II and claimed that it is hard to make significant progress on the full game even with the state-of-the-art RL algorithms <em>Vinyals et al. (2017)</em>. [30] presented a heuristic exploration technique for learning deterministic policies in micro-management tasks. Both [31] focused on a greedy MDP approach, i.e., the action of an agent is dependent explicitly on the action of other agents. In our paper, the dependency of agents is rather modelled over hidden layers by making use of bi-</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Bidirectionally-Coordinated Net (BiCNet). As a means of communication, bi-direction recurrent networks have been used to connect each individual agent’s policy and and Q networks. The learning is done by multiagent deterministic actor-critic as derived in the text.</p>
<p>directional RNN <em>Schuster and Paliwal (1997)</em>. A significant benefit over the greedy solution is that, while keeping simple, the communication happens in the latent space so that high-level information can be passed between agents; meanwhile, the gradient updates from all the agents can be efficiently propagated through the entire network.</p>
<p>Recently, [19] has attempted to solve the non-stationarity problem in mutliagent learning by improving the replay buffer, and tested the DIAL model in a way that all agents are fully decentralised. The COMA model <em>Foerster et al. (2017a)</em> was then proposed to tackle the challenge of multiagent credit assignment. Through the introduction of the counterfactual reward; the idea of training multiagent systems in the centralised critic and decentralised actors way was further reinforced. At the same time, the framework of centralised learning and decentralised execution was also adopted by MADDPG in <em>Lowe et al. (2017)</em> in some simpler, non-startcraft cases. By contrast, our BiCNet makes use of memory to form a communication channel among agents where the parameter space of communication is independent of the number of agents.</p>
<h2>Multiagent Bidirectionally-Coordinated Nets</h2>
<h3>StartCraft Combat as Stochastic Games</h3>
<p>The StarCraft combat games, <em>a.k.a.</em>, the micromanagement tasks, refer to the low-level, short-term control of the army members during a combat against the enemy members. For each combat, the agents in one side are fully cooperative, and they compete with the opponents; therefore,</p>
<p>each combat can be considered as a zero-sum competitive game between $N$ agents and $M$ enemies. We formulate it as a zero-sum Stochastic Game (SG) <em>Owen (1995)</em>, i.e., a dynamic game in a multiple state situation played by multiple agents. A SG can be described by a tuple $(\mathcal{S},{\mathcal{A}<em i="1">{i}}</em>}^{N},{\mathcal{B<em i="1">{i}}</em>}^{M},\mathcal{T},{\mathcal{R<em i="1">{i}}</em>}^{N+M}}$. Let $S$ denotes the state space of the current game, shared among all the agents. Initial state $\mathbf{s<em 1="1">{1}$ follows $\mathbf{s}</em>} \sim p_{1}(\mathbf{s})$. We define the action space of the controlled agent $i$ as $\mathcal{A<em j="j">{i}$, and the action space of the enemy $j$ as $\mathcal{B}</em>$ represents the reward function of each agent $i$ for $i \in[1, N]$.} . \mathcal{T}: \mathcal{S} \times \mathcal{A}^{N} \times \mathcal{B}^{M} \rightarrow \mathcal{S}$ stands for the deterministic transition function of the environment, and $\mathcal{R}_{i}: \mathcal{S} \times \mathcal{A}^{N} \times \mathcal{B}^{M} \rightarrow \mathbb{R</p>
<p>In order to maintain a flexible framework that could allow an arbitrary number of agents, we consider that the agents, either the controlled or the enemies, share the same state space $S$ of the current game; and within each camp, agents are homogeneous thus having the same action spaces $\mathcal{A}$ and $\mathcal{B}$ respectively. That is, for each agent $i \in[1, N]$ and enemy $j \in[1, M], \mathcal{A}<em j="j">{i}=\mathcal{A}$ and $\mathcal{B}</em>$. As discrete action space is intractably large, we consider continuous control outputs, e.g., attack angle and distance.}=\mathcal{B</p>
<p>In defining the reward function, we introduce a timevariant global reward based on the difference of the heath level between two consecutive time steps:
$r(\mathbf{s}, \mathbf{a}, \mathbf{b}) \equiv \frac{1}{M} \sum_{j=N+1}^{N+M} \Delta \mathcal{R}<em i="1">{j}^{t}(\mathbf{s}, \mathbf{a}, \mathbf{b})-\frac{1}{N} \sum</em>}^{N} \Delta \mathcal{R<em i="i">{i}^{t}(\mathbf{s}, \mathbf{a}, \mathbf{b})$,
where for simplicity, we drop the subscript $t$ in global reward $r(\mathbf{s}, \mathbf{a}, \mathbf{b})$. For given time step $t$ with state $\mathbf{s}$, the controlled agents take actions $\mathbf{a} \in \mathcal{A}^{N}$, the opponents take actions $\mathbf{b} \in \mathcal{B}^{M}$, and $\Delta \mathcal{R}</em>}^{t}(\cdot) \equiv \mathcal{R<em i="i">{i}^{t-1}(\mathbf{s}, \mathbf{a}, \mathbf{b})-\mathcal{R}</em>)$ represents the reduced health level for agent $i$. Note that Eq.(1) is presented from the aspect of controlled agents; the enemy's global reward is the exact opposite, making the sum of rewards from both camps equal zero. As the health level is non-increasing over time, Eq. (1) gives a positive reward at time step $t$ if the decrease of enemies' health levels exceeds that of ours.}^{t}(\mathbf{s}, \mathbf{a}, \mathbf{b</p>
<p>With the defined global reward $r(\mathbf{s}, \mathbf{a}, \mathbf{b})$, the controlled agents jointly take actions $\mathbf{a}$ in state $\mathbf{s}$ when the enemies take joint actions $\mathbf{b}$. The agents' objective is to learn a policy that maximises the expected sum of discounted rewards, i.e., $\mathbb{E}\left[\sum_{k=0}^{+\infty} \lambda^{k} r_{t+k}\right]$, where $0 \leq \lambda&lt;1$ is discount factor. Conversely, the enemies' joint policy is to minimise the expected sum. Correspondingly, we have the following Minimax game:
$Q_{\mathrm{SG}}^{<em>}(\mathbf{s}, \mathbf{a}, \mathbf{b})=r(\mathbf{s}, \mathbf{a}, \mathbf{b})+\lambda \max <em i="i">{\theta} \min </em>^{} Q_{\mathrm{SG}</em>}\left(\mathbf{s}^{\prime}, \mathbf{a}<em _phi="\phi">{\theta}\left(\mathbf{s}^{\prime}\right), \mathbf{b}</em>\right)\right)$,
where $\mathbf{s}^{\prime} \equiv \mathbf{s}^{t+1}$ is determined by $\mathcal{T}(\mathbf{s}, \mathbf{a}, \mathbf{b}) . Q_{\mathrm{SG}}^{}\left(\mathbf{s}^{\prime<em>}(\mathbf{s}, \mathbf{a}, \mathbf{b})$ is the optimal action-state value function, which follows the Bellman Optimal Equation. Here we propose to use deterministic policy $\mathbf{a}_{\theta}: \mathcal{S} \rightarrow \mathcal{A}^{N}$ of the controlled agents and the deterministic policy </em>Silver et al. (2014)* $\mathbf{b}_{\phi}: \mathcal{S} \rightarrow \mathcal{B}^{M}$</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>of the enemies. In small-scale MARL problems, a common solution is to employ Minimax Q-learning <em>Littman (1994)</em>. However, minimax Q-learning is generally intractable to apply in complex games. For simplicity, we consider the case that the policy of enemies is fixed, while leaving dedicated opponent modelling for future work. Then, SG defined in Eq (2) effectively turns into an MDP problem <em>He et al. (2016)</em>:</p>
<p>$$
Q^{<em>}(\mathbf{s}, \mathbf{a})=r(\mathbf{s}, \mathbf{a})+\lambda \max _{\theta} Q^{</em>}\left(\mathbf{s}^{\prime}, \mathbf{a}_{\theta}\left(\mathbf{s}^{\prime}\right)\right)
$$</p>
<p>where we drop notation $\mathbf{b}_{\phi}$ for brevity.</p>
<h2>Local, Individual Rewards</h2>
<p>A potential drawback of only using the global reward in Eq. (1) and its resulting zero-sum game is that it ignores the fact that a team collaboration typically consists of local collaborations and reward function and would normally includes certain internal structure. Moreover, in practice, each agent tends to have its own objective which drives the collaboration. To model this, we extend the formulation in the previous section by replacing Eq. (1) with each agent's local reward and including the evaluation of their attribution to other agents that have been interacting with (either completing or collaborating), i.e.,</p>
<p>$$
\begin{aligned}
r_{i}(\mathbf{s}, \mathbf{a}, \mathbf{b}) \equiv &amp; \frac{1}{|\text { top-K- } \mathbf{u}(i)|} \sum_{j \in \text { top-K- } \mathbf{u}(i)} \Delta \mathcal{R}<em i_prime="i^{\prime">{j}^{t}(\mathbf{s}, \mathbf{a}, \mathbf{b})- \
&amp; \frac{1}{|\text { top-K- } \mathbf{e}(i)|} \sum</em>)
\end{aligned}
$$} \in \text { top-K- } \mathbf{e}(i)} \Delta \mathcal{R}_{i^{\prime}}^{t}(\mathbf{s}, \mathbf{a}, \mathbf{b</p>
<p>where each agent $i$ maintains top- $K-u(i)$ and top- $K-e(i)$, the top- $K$ lists of other agents and enemies, that are currently being interacted with. Replacing it with Eq. (1), we have $N$ number of Bellman equations for agent $i$, where $i \in{1, \ldots, N}$, for the same parameter $\theta$ of the policy function:</p>
<p>$$
Q_{i}^{<em>}(\mathbf{s}, \mathbf{a})=r_{i}(\mathbf{s}, \mathbf{a})+\lambda \max <em i="i">{\theta} Q</em>^{</em>}\left(\mathbf{s}^{\prime}, \mathbf{a}_{\theta}\left(\mathbf{s}^{\prime}\right)\right)
$$</p>
<h2>Communication w/ Bidirectional Backpropagation</h2>
<p>Although Eq. (5) makes single-agent methods, such as deterministic policy gradient <em>Silver et al. (2014); Mnih et al. (2016)</em>, immediately applicable for learning individual actions, those approaches, however, lacks a principled mechanism to foster team-level collaboration. In this paper, we allow communications between agents (right before taking individual actions) by proposing a bidirectionally-coordinated net (BiCNet).</p>
<p>Overall, BiCNet consists of a multiagent actor network and a multiagent critic network as illustrated in Fig.(1). Both of the policy network (actor) and the Q-network (critic) are based on the bi-directional RNN structure <em>Schuster and Palival (1997)</em>. The policy network, which takes in a shared observation together with a local view, returns the action for each individual agent. As the bi-directional recurrent structure could serve not only as a communication channel but also as a local memory saver, each individual agent is able to maintain its own internal states, as well as to share the information with its collaborators.</p>
<p>For the learning over BiCNet, intuitively, we can think of computing the backward gradients by unfolding the network of length $N$ (the number of controlled agents) and then applying backpropagation through time (BPTT) <em>Werbos (1990)</em>.</p>
<p>The gradients pass to both the individual $Q_{i}$ function and the policy function. They are aggregated from all the agents and their actions. In other words, the gradients from all agents rewards are first propagated to influence each of agents actions, and the resulting gradients are further propagated back to updating the parameters.</p>
<p>To see this mathematically, we denote the objective of a single agent $i$ by $J_{i}(\theta)$; that is to maximise its expected cumulative individual reward $r_{i}$ as $J_{i}(\theta)=\mathbb{E}<em _mathbf_a="\mathbf{a">{\mathbf{s} \sim \rho</em><em i="i">{\theta}}^{T}}\left[r</em>}\left(\mathbf{s}, \mathbf{a<em _mathbf_a="\mathbf{a">{\theta}(\mathbf{s})\right)\right]$, where $\rho</em><em _theta="\theta">{\theta}}^{T}(\mathbf{s})$ is the discounted state distribution corresponding to the policy $\mathbf{a}</em>}$ under the transition $\mathcal{T}$, i.e., $\rho_{\mathbf{a<em _mathcal_S="\mathcal{S">{\theta}}^{T}(\mathbf{s}):=$ $\int</em>}} \sum_{t=1}^{\infty} \lambda^{t-1} p_{1}(\mathbf{s}) \mathbb{1}\left(\mathbf{s}^{\prime}=\mathcal{T<em _theta="\theta">{\mathbf{a}</em>$; it can also be chosen as the stationary distribution of an ergodic MDP. So, we can write the objective of $N$ agents denoted by $J(\theta)$ as follows:}, \mathbf{b}_{\theta}}^{1}(\mathbf{s})\right) \mathrm{d} \mathbf{s</p>
<p>$$
J(\theta)=\mathbb{E}<em _mathbf_a="\mathbf{a">{\mathbf{s} \sim \rho</em><em i="1">{\theta}}^{T}}\left[\sum</em>)\right)\right]
$$}^{N} r_{i}\left(s, \mathbf{a}_{\theta}(\mathbf{s</p>
<p>Next, we introduce a multiagent analogue to the deterministic policy gradient theorem. The proof, which we give in the Supplementary Material, follows a similar scheme to both (Silver et al. 2014) and (Sutton et al. 2000).</p>
<h2>Theorem 1 (Multiagent Deterministic PG Theorem)</h2>
<p>Given $N$ agents which are collectively represented in a policy parameterised with $\theta$, the discounted state distribution $\rho_{\mathbf{a}_{\theta}}^{T}(\mathbf{s})$, and the objective function $J(\theta)$ defined in Eq.(6), we have the policy gradient as follows:</p>
<p>$$
\begin{aligned}
&amp; \nabla_{\theta} J(\theta)= \
&amp; \quad \mathbb{E}<em _mathbf_a="\mathbf{a">{\mathbf{s} \sim \rho</em><em i="1">{\theta}}^{T}(\mathbf{s})}\left[\sum</em>}^{N} \sum_{j=1}^{N} \nabla_{\theta} \mathbf{a<em _mathbf_a="\mathbf{a">{j, \theta}(\mathbf{s}) \cdot \nabla</em><em i="i">{\mathbf{j}}} Q</em>}^{\mathbf{a<em _theta="\theta">{\theta}}\left(\mathbf{s}, \mathbf{a}</em>)\right)\right]
\end{aligned}
$$}(\mathbf{s</p>
<p>where to ensure adequate exploration, we apply OrnsteinUhlenbeck process to add noise on the output of actor network in each time step. Here we further consider the offpolicy deterministic actor-critic algorithms (Lillicrap et al. 2015; Silver et al. 2014) to reduce the variance. In particular, we employ a critic function in Eq. (7) to estimate the actionvalue $Q_{i}^{\mathbf{a}_{\theta}}$ where off-policy explorations can be conducted. In training the critic, we use the sum of square loss and have the following gradient for the parametrised critic $Q^{\xi}(\mathbf{s}, \mathbf{a})$, where $\xi$ is the parameter for the $\mathbf{Q}$ network:</p>
<p>$$
\begin{aligned}
\nabla_{\xi} L(\xi)=\mathbb{E}<em _mathbf_a="\mathbf{a">{\mathbf{s} \sim \rho</em><em i="1">{\theta}}^{T}(\mathbf{s})} &amp; {\left[\sum</em>}^{N}\left(r_{i}\left(\mathbf{s}, \mathbf{a<em i="i">{\theta}(\mathbf{s})\right)+\lambda Q</em>}^{\xi}\left(\mathbf{s}^{\prime}, \mathbf{a<em i="i">{\theta}\left(\mathbf{s}^{\prime}\right)\right)\right.\right.} \
&amp; \left.\left.-Q</em>}^{\xi}\left(\mathbf{s}, \mathbf{a<em _partial="\partial" _xi="\xi">{\theta}(\mathbf{s})\right)\right) \cdot \nabla</em>)\right)\right]
\end{aligned}
$$} Q_{i}^{\xi}\left(\mathbf{s}, \mathbf{a}_{\theta}(\mathbf{s</p>
<p>Note that the gradient is also aggregated from multiple agents as the policy network would do. With Eqs. (7) and Eqs. (8), we apply Stochastic Gradient Descent (SGD) to optimise both the actor and the critic networks. The pseudocode of the over algorithm is given in the Supplementary Material.</p>
<p>BiCNet is markedly different from greedy MDP approach as the dependency of agents are embedded in the latent layers, rather than directly on the actions. While simple, our
approach allow full dependency among agents because the gradients from all the actions in Eq.(7) are efficiently propagated through the entire networks. Yet, unlike CommNet (Sukhbaatar, Fergus, and others 2016), our communication is not fully symmetric, and we maintain certain social conventions and roles by fixing the order of the agents that join the RNN. This would help solving any possible tie between multiple optimal joint actions (Busoniu, Babuska, and De Schutter 2008; Spaan et al. 2002).</p>
<p>Across different agents, the parameters are shared so that the number of parameters is independent of the number of agents (analogous to the shared parameters across the time domain in vanilla RNN). Parameter sharing results in the compactness of the model which could speed up the learning process. Moreover, this could also enable the domain adaption where the network trained on the small team of of agents (typically three) effectively scales up to larger team of agents during the test under different combat scenarios.</p>
<h2>Experiments</h2>
<h2>Experimental Setup</h2>
<p>To understand the properties of our proposed BiCNet and its performance, we conducted the experiments on the StarCraft combats with different settings. Following similar experiment set-up as Sukhbaatar, Fergus, and others, BiCNet controls a group of agents trying to defeat the enemy units controlled by the built-in AI.</p>
<p>The level of combat difficulties can be adjusted by varying the unit types and the number of units in both sides. We measured the winning rates, and compared it with the state-of-the-art approaches. The comparative baselines consist of both the rule-based approaches, and deep reinforcement learning approaches. Our setting is summarised as follows where BiCNet controls the former units and the built-in AI controls the latter. We categorize the settings into three types: 1) easy combats {3 Marines vs. 1 Super Zergling, and 3 Wraiths vs. 3 Mutalinks}; 2) hard combats {5 Marines vs. 5 Marines, 15 Marines vs. 16 Marines, 20 Marines vs. 30 Zerglings, 10 Marines vs. 13 Zerglings, and 15 Wraiths vs. 17 Wraiths.}; 3) heterogeneous combats { 2 Dropships and 2 Tanks vs. 1 Ultralisk }.</p>
<p>The rule-based approaches allow us to have a reference point that we could make sense of. Here we adopted three rule-based baselines: StarCraft built-in AI, Attack the Weakest, Attack the Closest.</p>
<p>For the deep reinforcement learning approaches, we considered the following work as the baselines:
Independent controller (IND): We trained the model for single agent and control each agent individually in the combats. Note that there is no information sharing among different agents even though such method is easily adaptable to all kinds of multiagent combats.
Fully-connected (FC): We trained the model for all agents in a multiagent setting and control them collectively in the combats. The communication between agents are fully-connected. Note that it is needed to re-train a different model when the number of units at either side changes.
CommNet: CommNet (Sukhbaatar, Fergus, and others 2016) is a multiagent network designed to learning to communicate among multiple agents. To make a fair comparison, we implemented both the CommNet and the BiCNet on the same (state, action) spaces and follow the same training processes.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The impact of batch size in combat 2 <em>Marines vs. 1 Super Zergling</em>.</p>
<p><strong>GMEZO:</strong> GreedyMDP with Episodic Zero-Order Optimisation (GMEZO) (Usunier et al. 2016) was proposed particularly to solve StarCraft micromanagement tasks. Two novel ideas are introduced: conducting collaborations through a greedy update over MDP agents, as well as adding episodic noises in the parameter space for explorations. To focus on the comparison with these two ideas, we replaced our bidirectional formulation with the greedy MDP approach, and employed episodic zero-order optimisation with noise over the parameter space in the last layer of Q networks in our BiCNet. We keep the rest of the settings exactly the same.</p>
<p><strong>BiCNet:</strong> In BiCNet, we defined the action space differently from Sukhbaatar, Fergus, and others. Specifically, the action space of each individual agent is represented as a 3-dimensional real vector, <em>i.e.</em>, continuous action space. The first dimension corresponds to the probability of attack, according to which we sample a value from [0,1]. If the sampled value is 1, then the agent attacks; otherwise, the agent moves. The second and the third dimension correspond to the degree and the distance of where to attack. With the above three quantities, BiCNet can precisely order an agent to attack a certain location. Note that this is different from executing high-level commands such as 'attack enemy_id', in other words, how to effectively output the damage is itself a form of intelligence.</p>
<h3>Parameter Tuning</h3>
<p>In our training, <em>Adam</em> (Kingma and Ba 2014) is set as the optimiser with learning rate equal to 0.002 and the other arguments set by default values. We set the maximum steps of each episode as 800.</p>
<p>We study the impact of the batch size and the results are shown in Figure 2 in the "2 Marines vs. 1 Super Zergling" combat. The two metrics, the winning rate and the Q value, are given. We fine-tune the <em>batch_size</em> by selecting the best BiCNet model which are trained on 800 episodes (more than 700k steps) and then tested on 100 independent games. The model with <em>batch_size</em> 32 achieves both the highest winning rate and the highest mean Q-value after 600k training steps. We also observed that skip frame 2 gave the highest mean Q-value between 300k and 600k training steps. We fix this parameter with the learned optimal value in the remaining part of our test.</p>
<p>In Fig. 3, we also compare the convergence speed of parameter learning by plotting the winning rate against the number of training episodes. It shows that the proposed BiCNet model has a much quicker convergence than the two main StarCraft baselines.</p>
<h3>Performance Comparison</h3>
<p>Table 1 compares our proposed BiCNet model against the baselines under multiple combat scenarios. In each scenario,</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Learning Curves in Combat "10 <em>Marines</em> vs. 13 <em>Zerglings</em>"</p>
<p>Table 1: Performance comparison. M: <em>Marine</em>, Z: <em>Zergling</em>, W: <em>Wraith</em>.</p>
<table>
<thead>
<tr>
<th>Combat</th>
<th>Rule Based</th>
<th></th>
<th></th>
<th>RL Based</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Built-in</td>
<td>Weaken</td>
<td>Closest</td>
<td>IND</td>
<td>FC</td>
<td>GMEZO</td>
<td>CommNet</td>
</tr>
<tr>
<td>20 M vs. 30 Z</td>
<td>1.00</td>
<td>.000</td>
<td>.870</td>
<td>.940</td>
<td>.001</td>
<td>.880</td>
<td>1.00</td>
</tr>
<tr>
<td>5 M vs. 5 M</td>
<td>.720</td>
<td>.900</td>
<td>.700</td>
<td>.310</td>
<td>.080</td>
<td>.910</td>
<td>.950</td>
</tr>
<tr>
<td>15 M vs. 16 M</td>
<td>.610</td>
<td>.000</td>
<td>.670</td>
<td>.590</td>
<td>.440</td>
<td>.630</td>
<td>.680</td>
</tr>
<tr>
<td>10 M vs. 13 Z</td>
<td>.550</td>
<td>.230</td>
<td>.410</td>
<td>.522</td>
<td>.430</td>
<td>.570</td>
<td>.440</td>
</tr>
<tr>
<td>15 W vs. 17 W</td>
<td>.440</td>
<td>.000</td>
<td>.300</td>
<td>.310</td>
<td>.460</td>
<td>.420</td>
<td>.470</td>
</tr>
</tbody>
</table>
<p>BiCNet is trained over 100k steps, and we measure the performance as the average winning rate on 100 test games. The winning rate of the built-in AI is also provided as an indicator of the level of difficulty of the combats.</p>
<p>As illustrated in Table 1, in 4/5 of the scenarios, BiCNet outperforms the other baseline models. In particular, when the number of agents goes beyond 10, where cohesive collaborations are required, the margin of the performance gap between BiCNet and the second best starts to increase.</p>
<p>In the combat "5 M vs. 5 M", where the key factor to win is to "focus fire" on the weak, the IND and the FC models have relatively poorer performance. We believe it is because both of the models do not come with an explicit collaboration mechanism between agents in the training stage; coordinating the attacks towards one single enemy is even challenging. On the contrary, GMEZO, CommNet, and BiCNet, which are explicitly or implicitly designed for multiagent collaboration, can grasp and master this simple strategy, thus enjoying better performances. Furthermore, it is worth mentioning that despite the second best performance on "5 Marines vs. 5 Marines", our BiCNet only needs 10 combats before learning the idea of "focus fire", and achieves over 85% win rate, whereas CommNet needs more than 50 episodes to grasp the skill of "focus fire" with a much lower winning rate.</p>
<p>Note that the order of which side starts the first attack will influence the combat. This explains why in the combat "5 M vs. 5 M", the built-in AI on the left (as the first to attack) has more advantages on the winning rate 0.720 over the built-in AI on the right, even though the number of marines at both sides is the same.</p>
<h3>How It Works</h3>
<p>To further understand how BiCNet works, we conduct two more studies. We first examine whether a higher Q-value would represent a more optimal join actions among agents.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Visualisation for 3 Marines vs. 1 Super Zergling combat. <strong>Upper Left</strong>: State with high Q value; <strong>Lower Left</strong>: State with low Q value; <strong>Right</strong>: Visualisation of hidden layer outputs for each step using TSNE, coloured by Q values.</p>
<p>We visualise the model outputs when the coordinated cover attack is learned in Figure 4. The values in the last hidden layer of the critic network over <strong>10k</strong> steps are collected and then embedded in 2-dimensional space using t-SNE algorithm (Maaten and Hinton 2008). We observe that the steps with high Q-values are aggregated in the same area in the embedding space. For example, Figure 4 <strong>Upper Left</strong> shows that the agents attack the enemy in far distance when the enemy cannot attack the agents, and in this status, the model predicts high Q values. By contrast, in Figure 4 <strong>Lower Left</strong>, the agents suffer the damages from the enemy when it closes, which leads to low Q-values.</p>
<p>Our next aim is to examine whether there is any semantic meaning of the information exchanged among agents before their actions. However, due to the high variability of the StarCraft game, so far we have not observed any concrete meaning yet. We instead only focus on bidirectional communications by considering a simpler game, where the sophistications that are not related to communications are removed. Specifically, this simpler game consists of <em>n</em> agents. At each round, each agent observes a randomly generated number (sampled in range [−10, 10] under truncated Gaussian) as its input, and nothing else. The goal for each agent is to output the sum over the inputs that all the agents observed. Each agent receives reward based on the difference between the sum and their prediction (action output).</p>
<p>In the setting of three agents guessing the sum with one Bi-RNN communication layer (the hidden state size is 1) followed by a MLP layer, Figure 5 displays the values that have been transferred among three agents. As shown, Agent 1 passes a high value to Agent 2 when it observes a high observation number. When Agent 2 communicates with Agent 3, it tends to output an "additive" value between its own and previously communicated agent, i.e., agent 1. In other words, the hidden state value is increasing when the sum of Agents 1 and 2's observations goes high. Both senders have learned to make the other receiver obtain a helpful message in order to predict the target sum over all agents' observations.</p>
<p>We further set the game with num. of agents <em>n</em> = 5, 10, or 20. Apart from the four baselines tested previously, we also implement a supervised MLP with 10 hidden nodes as additional (predicting the sum based on the inputs given to agents). The results are compared in Table 2. The metric is the absolute value of the difference between each agent's action and target. We see our method significantly outperform others. The second best is CommNet. Possible explanation is</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: <strong>Left</strong>: The hidden state value passed by Agent 1 to Agent 2 in three agent guessing number game; <strong>Middle</strong>: The hidden state value passed by Agent 1 and Agent 2 to Agent 3 in three agent guessing number game; <strong>Right</strong>: Colour bar.</p>
<p>Table 2: Performance comparison in the guessing game with different agent numbers. Results are given as average [action_value − target_value] in 10, 000 testing steps and its standard deviation; A smaller value means a better performance. SL-MLP is a supervised MLP as an additional baseline. t-test is conducted, and the significant ones (p-value &lt; 0.05) compared to the second best are marked as *.</p>
<table>
<thead>
<tr>
<th>Agent Number</th>
<th>SL-MLP</th>
<th>IND</th>
<th>CommNet</th>
<th>GMEZO</th>
<th>BiCNet</th>
</tr>
</thead>
<tbody>
<tr>
<td>5</td>
<td>5.82±2.38</td>
<td>13.92±12.0</td>
<td>0.57±0.41</td>
<td>5.92±7.623</td>
<td>*0.52±0.51</td>
</tr>
<tr>
<td>10</td>
<td>4.31±3.67</td>
<td>15.32±13.90</td>
<td>1.18±0.90</td>
<td>9.21±8.22</td>
<td>*0.97±0.91</td>
</tr>
<tr>
<td>20</td>
<td>6.71±5.31</td>
<td>19.67±14.61</td>
<td>3.88±3.03</td>
<td>13.65±11.74</td>
<td>*3.12±2.93</td>
</tr>
</tbody>
</table>
<p>that it takes an average as the message, and thus naturally fits the problem, while ours have to learn the additives implicitly.</p>
<h3>Emerged Human-level Coordination</h3>
<p>With adequate trainings from scratch, BiCNet would be able to discover several effective collaboration strategies. In this section, we conduct a qualitative analysis on the learned collaboration policies from BiCNet. We refer the demonstration video to the <em>Supplementary Material</em> and the experimental configurations to Section Experiments.</p>
<p><strong>Coordinated moves without collision.</strong> We observe that, in the initial stages of learning, in Fig. 6 (a) and (b), the agents move in a rather uncoordinated way. In particular, when two agents are close to each other, one agent often unintentionally blocks the other's path. With the increasing rounds of training (typically over 40k steps in near 50 episodes in the "3</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Coordinated moves without collision in combat <em>3 Marines (ours) vs. 1 Super Zergling (enemy)</em>. The yellow line points out the direction each agent is going to move.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" />
(a) time step 1
(b) time step 2
(c) time step 3
(d) time step 4</p>
<p>Figure 7: Hit and Run tactics in combat 3 Marines (ours) vs. 1 Zealot (enemy).
<img alt="img-7.jpeg" src="img-7.jpeg" />
(a) time step 1
(b) time step 2
(c) time step 3
(d) time step 4</p>
<p>Figure 8: Coordinated cover attacks in combat 4 Dragoons (ours) vs. 1 Ultralisks (enemy)</p>
<p>Marines vs. 1 Super Zergling" combat setting), the number of collisions reduces dramatically. Finally, when the training becomes stable, the coordinated moves emerge, as illustrated in Fig. 6 (c) and (d). Such coordinated moves become important in large-scale combats as shown in Fig. 9 (a) and (b).</p>
<p>Hit and Run tactics. For human players, a common tactic of controlling agents in StarCraft combat is Hit and Run, i.e., moving the agents away if they are under attack, and fighting back again when agents stay safe. We find that BiCNet can rapidly grasp the tactic of Hit and Run, either in the case of single agent or multiple agents settings. We illustrate four consecutive movements of Hit and Run in Fig. 7. Despite the simplicity, Hit and Run serves as the basis for more advanced and sophisticated collaboration tactics.</p>
<p>Coordinated cover attack. Cover attack is a high-level collaborative strategy that is often used on the real battlefield. The essence of cover attack is to let one agent draw fire or attentions from the enemies, meanwhile, other agents take advantage of this time period or distance gap to output more harms. The difficulty of conducting cover attack lies in how to arrange the sequential moves of multiple agents in a coordinated hit and run way. As shown in Figs. 8, BiCNet can master it well. Starting from Fig. 8(a), BiCNet controls the bottom two Dragoons to run away from the enemy Ultralisk, while the one in the upper-right corner immediately starts to attack the enemy Ultralisk to cover them up. As a response, the enemy starts to attack the top one in time step 2. The bottom two Dragoons fight back and form another cover-up. By continuously looping this strategy over, the team of Dragoons guarantees consecutive attack outputs to the enemy while minimising the team-level damages (because the enemy wastes time in targeting different Dragoons) until the enemy is killed.</p>
<p>Focus fire without overkill. As the number of agents increases, how to efficiently allocate the attacking resources becomes important. Neither scattering over all enemies nor focusing on one enemy (wasting attacking fires is also called overkill) are desired. We observe that BiCNet learns to control each agent to focus their fires on particular enemies, and
<img alt="img-8.jpeg" src="img-8.jpeg" />
(a) time step 1
(b) time step 2
(c) time step 3
(d) time step 4</p>
<p>Figure 9: "focus fire" in combat 15 Marines (ours) vs. 16 Marines (enemy).
<img alt="img-9.jpeg" src="img-9.jpeg" />
(a) time step 1
(b) time step 2</p>
<p>Figure 10: Coordinated heterogeneous agents in combat 2 Dropships and 2 tanks vs. 1 Ultralisk.
different agents tend to move to the sides to spread the fire and avoid overkill. An example could be found in Fig.(9)</p>
<p>Collaborations between heterogeneous agents. In StarCraft, there are tens of types of agent units, each with unique functionalities, action space, strength, and weakness. For combats with different types of units involved, we expect the agents to reach win-win situations through the collaborations. In fact, heterogeneous collaborations can be easily implemented in our framework by limiting the parameter sharing only to the same types of the units. In this paper, we study a simple case where two Dropships and two tanks collaborate to fight against an Ultralisk. A Dropship does not have the function to attack, but it can carry maximally two ground units in the air. As shown in Fig. 10, when the Ultralisk is attacking one of the tanks, the Dropship escorts the tank to escape from the attack. At the same time, the other Dropship unloads his tank to the ground so as to attack the Ultralisk. At each side, the collaboration between the Dropship and the tank keeps iterating until the Ultralisk is destroyed.</p>
<h2>Conclusions</h2>
<p>In this paper, we have introduced a new deep multiagent reinforcement learning. The action is learned by constructing a vectorised actor-critic framework, where each dimension corresponds to an agent. The coordination is done by bidirectional recurrent communications in the internal layers. Through end-to-end learning, our BiCNet would be able to successfully learn several effective coordination strategies. Our experiments have demonstrated its ability to collaborate and master diverse combats in StarCraft combat games. We have also shown five human-level coordination strategies BiCNet could grasp from playing StarCraft combat games. Admittedly, quantifying the sophistication of the collaborations in games is challenging in general, and our analysis here is qualitative in nature.</p>
<p>In the next step, we plan to carry on experiments of letting the machine compete with human players at different levels. We also plan to further investigate how the policies are communicated over the networks among agents in more complicated settings, and whether there is a specific language that may have emerged in StartCraft (Lazaridou, Peysakhovich, and Baroni 2016; Mordatch and Abbeel 2017).</p>
<h2>References</h2>
<p>[Brown and Sandholm 2017] Brown, N., and Sandholm, T. 2017. Safe and nested endgame solving for imperfect-information games. AAAI/IAAI.
[Busoniu, Babuska, and De Schutter 2008] Busoniu, L.; Babuska, R.; and De Schutter, B. 2008. A comprehensive survey of multiagent reinforcement learning. IEEE Transactions on Systems Man and Cybernetics Part C Applications and Reviews 38(2):156.
[Deboeck 1994] Deboeck, G. 1994. Trading on the edge: neural, genetic, and fuzzy systems for chaotic financial markets, volume 39. John Wiley \&amp; Sons.
[Foerster et al. 2016] Foerster, J.; Assael, Y. M.; de Freitas, N.; and Whiteson, S. 2016. Learning to communicate with deep multi-agent reinforcement learning. In NIPS, 2137-2145.
[Foerster et al. 2017a] Foerster, J.; Farquhar, G.; Afouras, T.; Nardelli, N.; and Whiteson, S. 2017a. Counterfactual multi-agent policy gradients. arXiv preprint arXiv:1705.08926.
[Foerster et al. 2017b] Foerster, J.; Nardelli, N.; Farquhar, G.; Torr, P.; Kohli, P.; Whiteson, S.; et al. 2017b. Stabilising experience replay for deep multi-agent reinforcement learning. arXiv preprint arXiv:1702.08887.
[Goertzel and Pennachin 2007] Goertzel, B., and Pennachin, C. 2007. Artificial general intelligence, volume 2. Springer.
[He et al. 2016] He, H.; Boyd-Graber, J.; Kwok, K.; and Daumé III, H. 2016. Opponent modeling in deep reinforcement learning. In ICML, 1804-1813.
[Kapetanakis and Kudenko 2002] Kapetanakis, S., and Kudenko, D. 2002. Reinforcement learning of coordination in cooperative multiagent systems. AAAI/IAAI 2002:326-331.
[Kingma and Ba 2014] Kingma, D., and Ba, J. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.
[Lauer and Riedmiller 2000] Lauer, M., and Riedmiller, M. 2000. An algorithm for distributed reinforcement learning in cooperative multi-agent systems. In ICML.
[Lauer and Riedmiller 2004] Lauer, M., and Riedmiller, M. 2004. Reinforcement learning for stochastic cooperative multi-agent systems. In AAMA.
[Lazaridou, Peysakhovich, and Baroni 2016] Lazaridou, A.; Peysakhovich, A.; and Baroni, M. 2016. Multi-agent cooperation and the emergence of (natural) language. arXiv preprint arXiv:1612.07182.
[Lillicrap et al. 2015] Lillicrap, T. P.; Hunt, J. J.; Pritzel, A.; Heess, N.; Erez, T.; Tassa, Y.; Silver, D.; and Wierstra, D. 2015. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971.
[Littman 1994] Littman, M. L. 1994. Markov games as a framework for multi-agent reinforcement learning. In ICML.
[Lowe et al. 2017] Lowe, R.; Wu, Y.; Tamar, A.; Harb, J.; Abbeel, P.; and Mordatch, I. 2017. Multi-agent actor-critic for mixed cooperative-competitive environments. arXiv preprint arXiv:1706.02275.
[Maaten and Hinton 2008] Maaten, L. v. d., and Hinton, G. 2008. Visualizing data using t-sne. Journal of Machine Learning Research 9(Nov):2579-2605.
[Mnih et al. 2015] Mnih, V.; Kavukcuoglu, K.; Silver, D.; Rusu, A. A.; Veness, J.; Bellemare, M. G.; Graves, A.; Riedmiller, M.; Fidjeland, A. K.; Ostrovski, G.; et al. 2015. Human-level control through deep reinforcement learning. Nature 518(7540):529-533.
[Mnih et al. 2016] Mnih, V.; Badia, A. P.; Mirza, M.; Graves, A.; Lillicrap, T. P.; Harley, T.; Silver, D.; and Kavukcuoglu, K. 2016. Asynchronous methods for deep reinforcement learning. In International Conference on Machine Learning.
[Mordatch and Abbeel 2017] Mordatch, I., and Abbeel, P. 2017. Emergence of grounded compositional language in multi-agent populations. arXiv preprint arXiv:1703.04908.
[Owen 1995] Owen, G. 1995. Game theory. Academic Press.
[Schafer, Konstan, and Riedl 1999] Schafer, J. B.; Konstan, J.; and Riedl, J. 1999. Recommender systems in e-commerce. In ACM EC.
[Schmidhuber 1996] Schmidhuber, J. 1996. A general method for multi-agent reinforcement learning in unrestricted environments. In Adaptation, Coevolution and Learning in Multiagent Systems: Papers from the 1996 AAAI Spring Symposium, 84-87.
[Schuster and Paliwal 1997] Schuster, M., and Paliwal, K. K. 1997. Bidirectional recurrent neural networks. IEEE Transactions on Signal Processing 45(11):2673-2681.
[Silver et al. 2014] Silver, D.; Lever, G.; Heess, N.; Degris, T.; Wierstra, D.; and Riedmiller, M. 2014. Deterministic policy gradient algorithms. In ICML.
[Silver et al. 2016] Silver, D.; Huang, A.; Maddison, C. J.; Guez, A.; Silre, L.; Van Den Driessche, G.; Schrittwieser, J.; Antonoglou, I.; Panneershelvam, V.; Lanctot, M.; et al. 2016. Mastering the game of go with deep neural networks and tree search. Nature 529(7587):484-489.
[Spaan et al. 2002] Spaan, M. T.; Vlassis, N.; Groen, F. C.; et al. 2002. High level coordination of agents based on multiagent markov decision processes with roles. In IROS, volume 2, 66-73.
[Sukhbaatar, Fergus, and others 2016] Sukhbaatar, S.; Fergus, R.; et al. 2016. Learning multiagent communication with backpropagation. In NIPS, 2244-2252.
[Sutton et al. 2000] Sutton, R. S.; McAllester, D. A.; Singh, S. P.; and Mansour, Y. 2000. Policy gradient methods for reinforcement learning with function approximation. In NIPS, 1057-1063.
[Synnaeve et al. 2016] Synnaeve, G.; Nardelli, N.; Auvolat, A.; Chintala, S.; Lacroix, T.; Lin, Z.; Richoux, F.; and Usunier, N. 2016. Torchcraft: a library for machine learning research on real-time strategy games. arXiv preprint arXiv:1611.00625.
[Usunier et al. 2016] Usunier, N.; Synnaeve, G.; Lin, Z.; and Chintala, S. 2016. Episodic exploration for deep deterministic policies: An application to starcraft micromanagement tasks. arXiv preprint arXiv:1609.02993.
[Vinyals et al. 2017] Vinyals, O.; Ewalds, T.; Bartunov, S.; Georgiev, P.; Vezhnevets, A. S.; Yeo, M.; Makhzani, A.; Küttler, H.; Agapiou, J.; Schrittwieser, J.; et al. 2017. Starcraft ii: A new challenge for reinforcement learning. arXiv preprint arXiv:1708.04782.
[Wang, Zhang, and Yuan 2017] Wang, J.; Zhang, W.; and Yuan, S. 2017. Display advertising with real-time bidding (RTB) and behavioural targeting. Foundations and Trends in Information Retrieval, Now Publishers.
[Werbos 1990] Werbos, P. J. 1990. Backpropagation through time: what it does and how to do it. Proceedings of the IEEE 78(10):15501560.</p>
<h1>Supplementary Material</h1>
<h2>Proof of Theorem 1</h2>
<p>Following the regularity conditions mentioned in (Silver et al. 2014), we know that the supreme of $\frac{\partial Q_{i}^{\mathbf{a}<em _mathbf_a="\mathbf{a">{\theta}}(\mathbf{s}, \mathbf{a})|</em>}=\mathbf{a<em i="i">{\theta}(\mathbf{s})}}{\partial \mathbf{a}</em>}}$ and $\frac{\partial \mathbf{a<em _theta="\theta">{i, \theta}(\mathbf{s})}{\partial \theta}$ for each agent $i$ are bounded functions of $\mathbf{s}$. Based on the regularity and the boundedness, we can use Leibniz integral rule and Fubini's theorem, respectively. Note that as the policy $\mathbf{a}</em>}$ and the transition matrix of the environment $\mathcal{T}$ are both considered deterministic, the expectation is only taken over the initial state $\mathbf{s<em i="i">{0}$, which is different from the original deterministic policy gradient theorem. According to the definition of $Q</em>)$ and the our objective function in Eq.(6), we derive the multiagent deterministic policy gradient theorem, which mostly follows the line of (Sutton et al. 2000).}^{\mathbf{a}_{\theta}}(\mathbf{s}, \mathbf{a</p>
<p>$$
\begin{aligned}
\frac{\partial J(\theta)}{\partial \theta}= &amp; \frac{\partial}{\partial \theta} \int_{\mathcal{S}} p_{\mathbb{1}}(\mathbf{s}) \sum_{i=1}^{N} Q_{i}^{\mathbf{a}<em _theta="\theta">{\theta}}\left(\mathbf{s}, \mathbf{a}</em> \
= &amp; \int_{\mathcal{S}} p_{\mathbb{1}}(\mathbf{s}) \frac{\partial}{\partial \theta} \sum_{i=1}^{N} Q_{i}^{\mathbf{a}}(\mathbf{s})\right) \mathrm{d} \mathbf{s<em _theta="\theta">{\theta}}\left(\mathbf{s}, \mathbf{a}</em> \
= &amp; \int_{\mathcal{S}} p_{\mathbb{1}}(\mathbf{s}) \frac{\partial}{\partial \theta} \sum_{i=1}^{N}\left(r_{i}\left(\mathbf{s}, \mathbf{a}}(\mathbf{s})\right) \mathrm{d} \mathbf{s<em _mathcal_S="\mathcal{S">{\theta}(\mathbf{s})\right)+\int</em>}} \lambda \mathbb{1}\left(\mathbf{s}^{\prime}=\mathcal{T<em _theta="\theta">{\mathbf{a}</em>}, \mathbf{b<em i="i">{\phi}}^{1}(\mathbf{s})\right) Q</em>}^{\mathbf{a<em _theta="\theta">{\theta}}\left(\mathbf{s}^{\prime}, \mathbf{a}</em>\right. \
= &amp; \int_{\mathcal{S}} p_{\mathbb{1}}(\mathbf{s})\left(\frac{\partial \mathbf{a}}\left(\mathbf{s}^{\prime}\right) \mathrm{d} \mathbf{s}^{\prime}\right) \mathrm{d} \mathbf{s<em i="1">{\theta}(\mathbf{s})}{\partial \theta} \frac{\partial}{\partial \mathbf{a}} \sum</em>)|}^{N} r_{i}(\mathbf{s}, \mathbf{a<em _theta="\theta">{\mathbf{a}=\mathbf{a}</em>+ \
&amp; \int_{\mathcal{S}} p_{\mathbb{1}}(\mathbf{s}) \int_{\mathcal{S}} \lambda\left(\frac{\partial \mathbf{a}}(\mathbf{s})}\right) \mathrm{d} \mathbf{s<em _mathbf_a="\mathbf{a">{\theta}(\mathbf{s})}{\partial \theta} \frac{\partial}{\partial \mathbf{a}} \mathbb{1}\left(\mathbf{s}^{\prime}=\mathcal{T}</em>}, \mathbf{b<em _mathbf_a="\mathbf{a">{\phi}}^{1}(\mathbf{s})\right)\left|{ }</em>}=\mathbf{a<em i="1">{\theta}} \sum</em>}^{N} Q_{i}^{\mathbf{a<em _theta="\theta">{\theta}}\left(\mathbf{s}^{\prime}, \mathbf{a}</em>+ \
&amp; \int_{\mathcal{S}} p_{\mathbb{1}}(\mathbf{s}) \int_{\mathcal{S}} \lambda\left(\mathbb{1}\left(\mathbf{s}^{\prime}=\mathcal{T}}\left(\mathbf{s}^{\prime}\right)\right)\right) \mathrm{d} \mathbf{s}^{\prime} \mathrm{d} \mathbf{s<em _theta="\theta">{\mathbf{a}</em>}, \mathbf{b<em i="1">{\phi}}^{1}(\mathbf{s})\right) \cdot \frac{\partial}{\partial \theta} \sum</em>}^{N} Q_{i}^{\mathbf{a<em _theta="\theta">{\theta}}\left(\mathbf{s}^{\prime}, \mathbf{a}</em> \
= &amp; \int_{\mathcal{S}} p_{\mathbb{1}}(\mathbf{s})\left(\frac{\partial \mathbf{a}}\left(\mathbf{s}^{\prime}\right)\right)\right) \mathrm{d} \mathbf{s}^{\prime} \mathrm{d} \mathbf{s<em i="1">{\theta}(\mathbf{s})}{\partial \theta} \frac{\partial}{\partial \mathbf{a}} \sum</em>}^{N} Q_{i}^{\mathbf{a<em _mathbf_a="\mathbf{a">{\theta}}(\mathbf{s}, \mathbf{a})|</em>}=\mathbf{a<em _mathcal_S="\mathcal{S">{\theta}(\mathbf{s})}+\int</em>}} \lambda\left(\mathbb{1}\left(\mathbf{s}^{\prime}=\mathcal{T<em _theta="\theta">{\mathbf{a}</em>}, \mathbf{b<em i="1">{\phi}}^{1}(\mathbf{s})\right) \cdot \frac{\partial}{\partial \theta} \underbrace{\sum</em>}^{N} Q_{i}^{\mathbf{a<em _theta="\theta">{\theta}}\left(\mathbf{s}^{\prime}, \mathbf{a}</em>}\left(\mathbf{s}^{\prime}\right)\right)<em _mathcal_S="\mathcal{S">{\text {iterate as Eq.(10) to Eq.(11) }}\right) \mathrm{d} \mathbf{s}^{\prime}\right) \mathrm{d} \mathbf{s}\right. \
= &amp; \int</em>}} \int_{\mathcal{S}} \sum_{i=0}^{\infty} \lambda^{i} p_{\mathbb{1}}(\mathbf{s}) \mathbb{1}\left(\mathbf{s}^{\prime}=\mathcal{T<em _theta="\theta">{\mathbf{a}</em>}, \mathbf{b<em _theta="\theta">{\phi}}^{1}(\mathbf{s})\right) \frac{\partial \mathbf{a}</em>}\left(\mathbf{s}^{\prime}\right)}{\partial \theta} \frac{\partial}{\partial \mathbf{a}} \sum_{i=1}^{N} Q_{i}^{\mathbf{a<em _mathbf_a="\mathbf{a">{\theta}}\left(\mathbf{s}^{\prime}, \mathbf{a}\right)\left|{ }</em>}=\mathbf{a<em _mathcal_S="\mathcal{S">{\theta}\left(\mathbf{s}^{\prime}\right)} \mathrm{d} \mathbf{s}^{\prime} \mathrm{d} \mathbf{s}\right. \
= &amp; \int</em>}} \underbrace{\left(\int_{\mathcal{S}} \sum_{i=0}^{\infty} \lambda^{i} p_{1}(\mathbf{s}) \mathbb{1}\left(\mathbf{s}^{\prime}=\mathcal{T<em _theta="\theta">{\mathbf{a}</em>}, \mathbf{b<em _rho__mathbf_a="\rho_{\mathbf{a">{\phi}}^{1}(\mathbf{s})\right) \mathrm{d} \mathbf{s}\right)}</em><em _theta="\theta">{\theta}}^{N}\left(\mathbf{s}^{\prime}\right)} \frac{\partial \mathbf{a}</em>}\left(\mathbf{s}^{\prime}\right)}{\partial \theta} \frac{\partial}{\partial \mathbf{a}} \sum_{i=1}^{N} Q_{i}^{\mathbf{a<em _mathbf_a="\mathbf{a">{\theta}}\left(\mathbf{s}^{\prime}, \mathbf{a}\right)\left|{ }</em>}=\mathbf{a<em _mathbf_s="\mathbf{s">{\theta}\left(\mathbf{s}^{\prime}\right)} \mathrm{d} \mathbf{s}^{\prime}\right. \
= &amp; \mathbb{E}</em>} \sim \rho_{\mathbf{a<em _theta="\theta">{\theta}}^{N}(\mathbf{s})}\left[\frac{\partial \mathbf{a}</em>}\left(\mathbf{s}^{\prime}\right)}{\partial \theta} \frac{\partial}{\partial \mathbf{a}} \sum_{i=1}^{N} Q_{i}^{\mathbf{a<em _mathbf_a="\mathbf{a">{\theta}}\left(\mathbf{s}^{\prime}, \mathbf{a}\right)\left|{ }</em>}=\mathbf{a<em _mathbf_s="\mathbf{s">{\theta}\left(\mathbf{s}^{\prime}\right)}\right.\right] \
= &amp; \mathbb{E}</em>} \sim \rho_{\mathbf{a<em i="1">{\theta}}^{N}(\mathbf{s})}\left[\sum</em>}^{N} \sum_{j=1}^{N} \nabla_{\theta} \mathbf{a<em _mathbf_a="\mathbf{a">{j, \theta}(\mathbf{s}) \cdot \nabla</em><em i="i">{\mathbf{j}}} Q</em>}^{\mathbf{a<em _theta="\theta">{\theta}}\left(\mathbf{s}, \mathbf{a}</em>)\right)\right]
\end{aligned}
$$}(\mathbf{s</p>
<p>where in Eq.(10) Leibniz intergal rule is used to exchange derivative and integral since $Q_{i}^{\mathbf{a}<em _theta="\theta">{\theta}}\left(\mathbf{s}, \mathbf{a}</em>)\right)$ is continuous. For Eq.(11), we used the definition of $Q$-value. Then, we take derivatives for each term in Eq.(11) to get Eq.(12). Afterwards, we combine the first and the second term in Eq.(12) to get the first term in Eq.(13), while we notice that we can iterate Eq.(10) and Eq.(11) to expand the second term in Eq.(13). By summing up the iterated terms, we get Eq.(14), which implies Eq.(15) by using Fubini's theorem to exchange the order of integration. Using the expectation to denote Eq.(15), we derive Eq.(16). Finally, we get Eq.(17) and the proof is done.}(\mathbf{s</p>
<h1>Pseudocode</h1>
<div class="codehilite"><pre><span></span><code><span class="nx">Algorithm</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="nx">BiCNet</span><span class="w"> </span><span class="nx">algorithm</span>
<span class="w">    </span><span class="nx">Initialise</span><span class="w"> </span><span class="nx">actor</span><span class="w"> </span><span class="nx">network</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">critic</span><span class="w"> </span><span class="nx">network</span><span class="w"> </span><span class="nx">with</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">xi</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">theta</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="nx">Initialise</span><span class="w"> </span><span class="nx">target</span><span class="w"> </span><span class="nx">network</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">critic</span><span class="w"> </span><span class="nx">network</span><span class="w"> </span><span class="nx">with</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">xi</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">prime</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="nx">xi</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">theta</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">prime</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="nx">theta</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="nx">Initialise</span><span class="w"> </span><span class="nx">replay</span><span class="w"> </span><span class="nx">buffer</span><span class="w"> </span><span class="nx">R</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="nx">episodes</span><span class="w"> </span><span class="err">\</span><span class="p">(=</span><span class="mi">1</span><span class="err">\</span><span class="p">),</span><span class="w"> </span><span class="nx">E</span><span class="w"> </span><span class="nx">do</span>
<span class="w">        </span><span class="nx">initialise</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">random</span><span class="w"> </span><span class="nx">process</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">U</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nx">action</span><span class="w"> </span><span class="nx">exploration</span>
<span class="w">        </span><span class="nx">receive</span><span class="w"> </span><span class="nx">initial</span><span class="w"> </span><span class="nx">observation</span><span class="w"> </span><span class="nx">state</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">s</span><span class="o">^</span><span class="p">{</span><span class="mi">1</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathrm</span><span class="p">{</span><span class="nx">t</span><span class="p">}=</span><span class="mi">1</span><span class="err">\</span><span class="p">),</span><span class="w"> </span><span class="nx">T</span><span class="w"> </span><span class="nx">do</span>
<span class="w">            </span><span class="k">for</span><span class="w"> </span><span class="nx">each</span><span class="w"> </span><span class="nx">agent</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">i</span><span class="err">\</span><span class="p">),</span><span class="w"> </span><span class="nx">select</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">execute</span><span class="w"> </span><span class="nx">action</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">a_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="nx">t</span><span class="p">}=</span><span class="err">\</span><span class="nx">mathbf</span><span class="p">{</span><span class="nx">a</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="nx">i</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="nx">theta</span><span class="p">}</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">s</span><span class="o">^</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="o">+</span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">N</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
<span class="w">            </span><span class="nx">receive</span><span class="w"> </span><span class="nx">reward</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">left</span><span class="p">[</span><span class="nx">r_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">]</span><span class="nx">_</span><span class="p">{</span><span class="nx">i</span><span class="p">=</span><span class="mi">1</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="nx">N</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="k">observe</span><span class="w"> </span><span class="nx">new</span><span class="w"> </span><span class="nx">state</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">s</span><span class="o">^</span><span class="p">{</span><span class="nx">t</span><span class="o">+</span><span class="mi">1</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
<span class="w">            </span><span class="nx">store</span><span class="w"> </span><span class="nx">transition</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">left</span><span class="err">\</span><span class="p">{</span><span class="nx">s</span><span class="o">^</span><span class="p">{</span><span class="nx">t</span><span class="p">},</span><span class="err">\</span><span class="nx">left</span><span class="p">[</span><span class="nx">a_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="nx">t</span><span class="p">},</span><span class="w"> </span><span class="nx">r_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">]</span><span class="nx">_</span><span class="p">{</span><span class="nx">i</span><span class="p">=</span><span class="mi">1</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="nx">N</span><span class="p">},</span><span class="w"> </span><span class="nx">s</span><span class="o">^</span><span class="p">{</span><span class="nx">t</span><span class="o">+</span><span class="mi">1</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="err">\</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">R</span>
<span class="w">            </span><span class="nx">sample</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">random</span><span class="w"> </span><span class="nx">minibatch</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">M</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">transitions</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">left</span><span class="err">\</span><span class="p">{</span><span class="nx">s_</span><span class="p">{</span><span class="nx">m</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="nx">t</span><span class="p">},</span><span class="err">\</span><span class="nx">left</span><span class="p">[</span><span class="nx">a_</span><span class="p">{</span><span class="nx">m</span><span class="p">,</span><span class="w"> </span><span class="nx">i</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="nx">t</span><span class="p">},</span><span class="w"> </span><span class="nx">r_</span><span class="p">{</span><span class="nx">m</span><span class="p">,</span><span class="w"> </span><span class="nx">i</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">]</span><span class="nx">_</span><span class="p">{</span><span class="nx">i</span><span class="p">=</span><span class="mi">1</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="nx">N</span><span class="p">},</span><span class="w"> </span><span class="nx">s_</span><span class="p">{</span><span class="nx">m</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="nx">t</span><span class="o">+</span><span class="mi">1</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="err">\</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="nx">m</span><span class="p">=</span><span class="mi">1</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="nx">M</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">from</span><span class="w"> </span><span class="nx">R</span>
<span class="w">            </span><span class="nx">compute</span><span class="w"> </span><span class="nx">target</span><span class="w"> </span><span class="nx">value</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nx">each</span><span class="w"> </span><span class="nx">agent</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">each</span><span class="w"> </span><span class="nx">transition</span><span class="w"> </span><span class="nx">using</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">Bi</span><span class="o">-</span><span class="nx">RNN</span><span class="p">:</span>
<span class="w">            </span><span class="k">for</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathrm</span><span class="p">{</span><span class="nx">m</span><span class="p">}=</span><span class="mi">1</span><span class="err">\</span><span class="p">),</span><span class="w"> </span><span class="nx">M</span><span class="w"> </span><span class="nx">do</span>
<span class="w">            </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">hat</span><span class="p">{</span><span class="nx">Q</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="nx">m</span><span class="p">,</span><span class="w"> </span><span class="nx">i</span><span class="p">}=</span><span class="nx">r_</span><span class="p">{</span><span class="nx">m</span><span class="p">,</span><span class="w"> </span><span class="nx">i</span><span class="p">}</span><span class="o">+</span><span class="err">\</span><span class="nx">lambda</span><span class="w"> </span><span class="nx">Q_</span><span class="p">{</span><span class="nx">m</span><span class="p">,</span><span class="w"> </span><span class="nx">i</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">xi</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">prime</span><span class="p">}}</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">s_</span><span class="p">{</span><span class="nx">m</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="nx">t</span><span class="o">+</span><span class="mi">1</span><span class="p">},</span><span class="w"> </span><span class="err">\</span><span class="nx">mathbf</span><span class="p">{</span><span class="nx">a</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="err">\</span><span class="nx">theta</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">prime</span><span class="p">}}</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">s_</span><span class="p">{</span><span class="nx">m</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="nx">t</span><span class="o">+</span><span class="mi">1</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nx">each</span><span class="w"> </span><span class="nx">agent</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">i</span><span class="err">\</span><span class="p">)</span>
<span class="w">            </span><span class="nx">end</span><span class="w"> </span><span class="k">for</span>
<span class="w">            </span><span class="nx">compute</span><span class="w"> </span><span class="nx">critic</span><span class="w"> </span><span class="nx">gradient</span><span class="w"> </span><span class="nx">estimation</span><span class="w"> </span><span class="nx">according</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">Eq</span><span class="p">.(</span><span class="mi">8</span><span class="p">):</span>
<span class="w">            </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">Delta</span><span class="w"> </span><span class="err">\</span><span class="nx">xi</span><span class="p">=</span><span class="err">\</span><span class="nx">frac</span><span class="p">{</span><span class="mi">1</span><span class="p">}{</span><span class="nx">M</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">sum_</span><span class="p">{</span><span class="nx">m</span><span class="p">=</span><span class="mi">1</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="nx">M</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">sum_</span><span class="p">{</span><span class="nx">i</span><span class="p">=</span><span class="mi">1</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="nx">N</span><span class="p">}</span><span class="err">\</span><span class="nx">left</span><span class="p">[</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="err">\</span><span class="nx">hat</span><span class="p">{</span><span class="nx">Q</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="nx">m</span><span class="p">,</span><span class="w"> </span><span class="nx">i</span><span class="p">}</span><span class="o">-</span><span class="nx">Q_</span><span class="p">{</span><span class="nx">m</span><span class="p">,</span><span class="w"> </span><span class="nx">i</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">xi</span><span class="p">}</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="err">\</span><span class="nx">mathbf</span><span class="p">{</span><span class="nx">s</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="nx">m</span><span class="p">},</span><span class="w"> </span><span class="err">\</span><span class="nx">mathbf</span><span class="p">{</span><span class="nx">a</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="err">\</span><span class="nx">theta</span><span class="p">}</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="err">\</span><span class="nx">mathbf</span><span class="p">{</span><span class="nx">s</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="nx">m</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="w"> </span><span class="err">\</span><span class="nx">cdot</span><span class="w"> </span><span class="err">\</span><span class="nx">nabla_</span><span class="p">{</span><span class="err">\</span><span class="nx">xi</span><span class="p">}</span><span class="w"> </span><span class="nx">Q_</span><span class="p">{</span><span class="nx">m</span><span class="p">,</span><span class="w"> </span><span class="nx">i</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">xi</span><span class="p">}</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="err">\</span><span class="nx">mathbf</span><span class="p">{</span><span class="nx">s</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="nx">m</span><span class="p">},</span><span class="w"> </span><span class="err">\</span><span class="nx">mathbf</span><span class="p">{</span><span class="nx">a</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="err">\</span><span class="nx">theta</span><span class="p">}</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="err">\</span><span class="nx">mathbf</span><span class="p">{</span><span class="nx">s</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="nx">m</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="nx">right</span><span class="p">]</span><span class="w"> </span><span class="p">.</span>
<span class="w">            </span><span class="nx">compute</span><span class="w"> </span><span class="nx">actor</span><span class="w"> </span><span class="nx">gradient</span><span class="w"> </span><span class="nx">estimation</span><span class="w"> </span><span class="nx">according</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">Eq</span><span class="p">.(</span><span class="mi">7</span><span class="p">)</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">replace</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">Q</span><span class="err">\</span><span class="p">)</span><span class="o">-</span><span class="nx">value</span><span class="w"> </span><span class="nx">with</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">critic</span><span class="w"> </span><span class="nx">estimation</span><span class="p">:</span>
<span class="w">            </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">Delta</span><span class="w"> </span><span class="err">\</span><span class="nx">theta</span><span class="p">=</span><span class="err">\</span><span class="nx">frac</span><span class="p">{</span><span class="mi">1</span><span class="p">}{</span><span class="nx">M</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">sum_</span><span class="p">{</span><span class="nx">m</span><span class="p">=</span><span class="mi">1</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="nx">M</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">sum_</span><span class="p">{</span><span class="nx">i</span><span class="p">=</span><span class="mi">1</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="nx">N</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">sum_</span><span class="p">{</span><span class="nx">j</span><span class="p">=</span><span class="mi">1</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="nx">N</span><span class="p">}</span><span class="err">\</span><span class="nx">left</span><span class="p">[</span><span class="err">\</span><span class="nx">nabla_</span><span class="p">{</span><span class="err">\</span><span class="nx">theta</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">mathbf</span><span class="p">{</span><span class="nx">a</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="nx">j</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="nx">theta</span><span class="p">}</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="err">\</span><span class="nx">mathbf</span><span class="p">{</span><span class="nx">s</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="nx">m</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="w"> </span><span class="err">\</span><span class="nx">cdot</span><span class="w"> </span><span class="err">\</span><span class="nx">nabla_</span><span class="p">{</span><span class="err">\</span><span class="nx">mathbf</span><span class="p">{</span><span class="nx">a</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="err">\</span><span class="nx">mathbf</span><span class="p">{</span><span class="nx">j</span><span class="p">}}}</span><span class="w"> </span><span class="nx">Q_</span><span class="p">{</span><span class="nx">m</span><span class="p">,</span><span class="w"> </span><span class="nx">i</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">xi</span><span class="p">}</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="err">\</span><span class="nx">mathbf</span><span class="p">{</span><span class="nx">s</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="nx">m</span><span class="p">},</span><span class="w"> </span><span class="err">\</span><span class="nx">mathbf</span><span class="p">{</span><span class="nx">a</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="err">\</span><span class="nx">theta</span><span class="p">}</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="err">\</span><span class="nx">mathbf</span><span class="p">{</span><span class="nx">s</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="nx">m</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="nx">right</span><span class="p">]</span><span class="err">\</span><span class="p">)</span>
<span class="w">            </span><span class="nx">update</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">networks</span><span class="w"> </span><span class="nx">based</span><span class="w"> </span><span class="nx">on</span><span class="w"> </span><span class="nx">Adam</span><span class="w"> </span><span class="nx">using</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">above</span><span class="w"> </span><span class="nx">gradient</span><span class="w"> </span><span class="nx">estimators</span>
<span class="w">            </span><span class="nx">update</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">target</span><span class="w"> </span><span class="nx">networks</span><span class="p">:</span>
<span class="w">            </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">xi</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">prime</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="nx">gamma</span><span class="w"> </span><span class="err">\</span><span class="nx">xi</span><span class="o">+</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="err">\</span><span class="nx">gamma</span><span class="p">)</span><span class="w"> </span><span class="err">\</span><span class="nx">xi</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">prime</span><span class="p">},</span><span class="w"> </span><span class="err">\</span><span class="nx">quad</span><span class="w"> </span><span class="err">\</span><span class="nx">theta</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">prime</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="nx">gamma</span><span class="w"> </span><span class="err">\</span><span class="nx">theta</span><span class="o">+</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="err">\</span><span class="nx">gamma</span><span class="p">)</span><span class="w"> </span><span class="err">\</span><span class="nx">theta</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">prime</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
<span class="w">            </span><span class="nx">end</span><span class="w"> </span><span class="k">for</span>
<span class="w">    </span><span class="nx">end</span><span class="w"> </span><span class="k">for</span>
</code></pre></div>

<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{7}$ With our framework heterogeneous agents can be also trained using different parameters and action space.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{1}$ Trademark of Blizzard Entertainment ${ }^{\mathrm{TM}}$.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-593 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-593</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-593</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-19.html">extraction-schema-19</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <p><strong>Paper ID:</strong> paper-6960918666a8c9d50343bbe9e94baf6415edd5fb</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/6960918666a8c9d50343bbe9e94baf6415edd5fb" target="_blank">Language modeling via stochastic processes</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> This work analyzes the application of contrastive representations for generative tasks, like long text generation, and proposes one approach, which is called Time Control (TC), which first learns a contrastive representation of the target text domain, then generates text by decoding from these representations.</p>
                <p><strong>Paper Abstract:</strong> Modern language models can generate high-quality short texts. However, they often meander or are incoherent when generating longer texts. These issues arise from the next-token-only language modeling objective. Recent work in self-supervised learning suggests that models can learn good latent representations via contrastive learning, which can be effective for discriminative tasks. Our work analyzes the application of contrastive representations for generative tasks, like long text generation. We propose one approach for leveraging constrastive representations, which we call Time Control (TC). TC first learns a contrastive representation of the target text domain, then generates text by decoding from these representations. Compared to domain-specific methods and fine-tuning GPT2 across a variety of text domains, TC performs competitively to methods specific for learning sentence representations on discourse coherence. On long text generation settings, TC preserves the text structure both in terms of ordering (up to $+15\%$ better) and text length consistency (up to $+90\%$ better).</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e593.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e593.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Time Control (TC) experiments</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Time Control: Generative decoding from Brownian-bridge contrastive latent representations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>This paper trains an encoder to map sentences to latent embeddings that follow Brownian-bridge dynamics via a contrastive loss, then fine-tunes GPT-2 to decode conditioned on latent plans; experiments evaluate discourse-coherence, section-length matching, and forced long-text generation and explicitly report and analyze variability and reproducibility issues.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Natural Language Processing / Language generation</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Learn sentence-level latent trajectories with Brownian-bridge contrastive objective and generate long coherent documents by fine-tuning GPT-2 conditioned on sampled latent plans; evaluate discourse coherence (ordering classification), section-length matching, and forced long-text extrapolation.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Multiple software/hardware and experimental sources are identified: (1) random seeds, (2) GPU nondeterminism (different GPU types and GPU runtimes), (3) torch dataloader behavior, (4) dataset-loader seeding handled separately from other seeds, (5) Weights & Biases logging/early-logging bug, (6) implementation bugs (e.g., original code not leveraging goal-directed decoding; improper forced generation min/max length), (7) choice of machine type (RTX A6000, A5000, RTX 3090, TITAN RTX, TITAN Xp, TITAN V, GTX TITAN X), (8) stochasticity from sampling latent plans (Brownian bridge / Brownian motion / Static plans), and (9) variability from run-to-run training nondeterminism.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Reported mean ± standard error (several tables) and percentage deviations; tables report e.g., accuracy ± standard error and percentage mismatch (MM) ± standard error; experiments run across multiple seeds and machines (standard error computed over runs).</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Quantitative examples reported in the paper include: (a) discourse coherence accuracies shown as mean ± standard error over 3 runs (e.g., BERT 72.7 ± 0.1 for k=5 on Wikisection), (b) length-mismatch (MM %) reported with ± standard error across runs/machines (Table 2: GPT2 9.9 ± 3.6; TC (8) 13.1 ± 4.4; STATIC TC (8) 12.8 ± 3.2), (c) forced long-text section-length deviations in % with ± standard error (Table 3: e.g., Static TC (32) 8.6 ± 5.3 on TM-2), (d) ordering metrics shown as % ± standard error (Table 4: many entries like TC (32) 36.9 ± 7.7 on Wikisection), and (e) specific observed machine sensitivity: TC (8) example had deviation 7.7 ± 1.2 on one set of three seeds vs 17.3 ± 5.2 on another set of three seeds on different machines.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Reproducibility assessed via repeat runs (multiple seeds and multiple GPU machines), reporting means and standard errors; comparison of original vs corrected manuscript results; statement that experiments are seeded and replicate exactly if run on the same machine; explicit mention of rerunning experiments after code fixes and re-seeding.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>Authors report that (1) rerunning after bug fixes changed some earlier reported results (they corrected a bug where decoding did not leverage goal-directedness), (2) results replicate exactly when run on the same machine with same seeds, but different GPU machines produce differing outcomes (example TC (8) deviation 7.7 ±1.2 vs 17.3 ±5.2), and (3) they randomized over machine choice to reduce bias; tables present updated numbers (current code hash provided).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Identified challenges include GPU nondeterminism and sensitivity to GPU machine type, separate seeding of dataset loader, torch dataloader nondeterminism, Weights & Biases logging bug that hid later training epochs, implementation bugs affecting decoding behavior, and dependence of some evaluation metrics on subtle generation settings (min/max generation length).</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Mitigations used or proposed: (1) seed all experiments, (2) randomize over multiple GPU machine types and report aggregated statistics, (3) run multiple independent seeds and report mean ± standard error, (4) publish code and processed datasets in supplement (zip) and provide commit hash for corrected code, (5) fix implementation bugs and rerun experiments, (6) checkpointing during fine-tuning and selecting best PPL checkpoint, and (7) report both static and dynamic decoding variants to analyze sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Qualitative: seeding ensured exact replication when runs used the same machine; randomizing over machines and seeds produced aggregated means/standard errors that reveal machine-dependent variability; no quantitative claim that variability was eliminated—authors report residual variability across machines (example above) and emphasize that sensitivity persists despite mitigations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>Discourse-coherence: 3 runs (mean ± se); Many generation metrics: experiments run over 3 random GPU machines × 3 seeds each (reported as runs aggregated per method), i.e., 9 runs aggregated for several tables; some reported numbers explicitly state 3 runs for specific setups.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The paper shows substantial stochasticity in LM-driven generation experiments coming from software/hardware sources (GPU type, dataloader, wandb logging and implementation bugs) and reports variability quantitatively (means ± standard error across seeds and machines). Authors mitigated some issues by seeding, randomizing over machines, publishing corrected code and datasets, and reporting aggregated statistics, but they emphasize that cross-machine nondeterminism and implementation bugs remain important reproducibility challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language modeling via stochastic processes', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Representation learning with contrastive predictive coding <em>(Rating: 2)</em></li>
                <li>Contrastive learning of strong-mixing continuous-time stochastic processes <em>(Rating: 2)</em></li>
                <li>Score-based generative modeling through stochastic differential equations <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-593",
    "paper_id": "paper-6960918666a8c9d50343bbe9e94baf6415edd5fb",
    "extraction_schema_id": "extraction-schema-19",
    "extracted_data": [
        {
            "name_short": "Time Control (TC) experiments",
            "name_full": "Time Control: Generative decoding from Brownian-bridge contrastive latent representations",
            "brief_description": "This paper trains an encoder to map sentences to latent embeddings that follow Brownian-bridge dynamics via a contrastive loss, then fine-tunes GPT-2 to decode conditioned on latent plans; experiments evaluate discourse-coherence, section-length matching, and forced long-text generation and explicitly report and analyze variability and reproducibility issues.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2",
            "model_size": null,
            "scientific_domain": "Natural Language Processing / Language generation",
            "experimental_task": "Learn sentence-level latent trajectories with Brownian-bridge contrastive objective and generate long coherent documents by fine-tuning GPT-2 conditioned on sampled latent plans; evaluate discourse coherence (ordering classification), section-length matching, and forced long-text extrapolation.",
            "variability_sources": "Multiple software/hardware and experimental sources are identified: (1) random seeds, (2) GPU nondeterminism (different GPU types and GPU runtimes), (3) torch dataloader behavior, (4) dataset-loader seeding handled separately from other seeds, (5) Weights & Biases logging/early-logging bug, (6) implementation bugs (e.g., original code not leveraging goal-directed decoding; improper forced generation min/max length), (7) choice of machine type (RTX A6000, A5000, RTX 3090, TITAN RTX, TITAN Xp, TITAN V, GTX TITAN X), (8) stochasticity from sampling latent plans (Brownian bridge / Brownian motion / Static plans), and (9) variability from run-to-run training nondeterminism.",
            "variability_measured": true,
            "variability_metrics": "Reported mean ± standard error (several tables) and percentage deviations; tables report e.g., accuracy ± standard error and percentage mismatch (MM) ± standard error; experiments run across multiple seeds and machines (standard error computed over runs).",
            "variability_results": "Quantitative examples reported in the paper include: (a) discourse coherence accuracies shown as mean ± standard error over 3 runs (e.g., BERT 72.7 ± 0.1 for k=5 on Wikisection), (b) length-mismatch (MM %) reported with ± standard error across runs/machines (Table 2: GPT2 9.9 ± 3.6; TC (8) 13.1 ± 4.4; STATIC TC (8) 12.8 ± 3.2), (c) forced long-text section-length deviations in % with ± standard error (Table 3: e.g., Static TC (32) 8.6 ± 5.3 on TM-2), (d) ordering metrics shown as % ± standard error (Table 4: many entries like TC (32) 36.9 ± 7.7 on Wikisection), and (e) specific observed machine sensitivity: TC (8) example had deviation 7.7 ± 1.2 on one set of three seeds vs 17.3 ± 5.2 on another set of three seeds on different machines.",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Reproducibility assessed via repeat runs (multiple seeds and multiple GPU machines), reporting means and standard errors; comparison of original vs corrected manuscript results; statement that experiments are seeded and replicate exactly if run on the same machine; explicit mention of rerunning experiments after code fixes and re-seeding.",
            "reproducibility_results": "Authors report that (1) rerunning after bug fixes changed some earlier reported results (they corrected a bug where decoding did not leverage goal-directedness), (2) results replicate exactly when run on the same machine with same seeds, but different GPU machines produce differing outcomes (example TC (8) deviation 7.7 ±1.2 vs 17.3 ±5.2), and (3) they randomized over machine choice to reduce bias; tables present updated numbers (current code hash provided).",
            "reproducibility_challenges": "Identified challenges include GPU nondeterminism and sensitivity to GPU machine type, separate seeding of dataset loader, torch dataloader nondeterminism, Weights & Biases logging bug that hid later training epochs, implementation bugs affecting decoding behavior, and dependence of some evaluation metrics on subtle generation settings (min/max generation length).",
            "mitigation_methods": "Mitigations used or proposed: (1) seed all experiments, (2) randomize over multiple GPU machine types and report aggregated statistics, (3) run multiple independent seeds and report mean ± standard error, (4) publish code and processed datasets in supplement (zip) and provide commit hash for corrected code, (5) fix implementation bugs and rerun experiments, (6) checkpointing during fine-tuning and selecting best PPL checkpoint, and (7) report both static and dynamic decoding variants to analyze sensitivity.",
            "mitigation_effectiveness": "Qualitative: seeding ensured exact replication when runs used the same machine; randomizing over machines and seeds produced aggregated means/standard errors that reveal machine-dependent variability; no quantitative claim that variability was eliminated—authors report residual variability across machines (example above) and emphasize that sensitivity persists despite mitigations.",
            "comparison_with_without_controls": true,
            "number_of_runs": "Discourse-coherence: 3 runs (mean ± se); Many generation metrics: experiments run over 3 random GPU machines × 3 seeds each (reported as runs aggregated per method), i.e., 9 runs aggregated for several tables; some reported numbers explicitly state 3 runs for specific setups.",
            "key_findings": "The paper shows substantial stochasticity in LM-driven generation experiments coming from software/hardware sources (GPU type, dataloader, wandb logging and implementation bugs) and reports variability quantitatively (means ± standard error across seeds and machines). Authors mitigated some issues by seeding, randomizing over machines, publishing corrected code and datasets, and reporting aggregated statistics, but they emphasize that cross-machine nondeterminism and implementation bugs remain important reproducibility challenges.",
            "uuid": "e593.0",
            "source_info": {
                "paper_title": "Language modeling via stochastic processes",
                "publication_date_yy_mm": "2022-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Representation learning with contrastive predictive coding",
            "rating": 2
        },
        {
            "paper_title": "Contrastive learning of strong-mixing continuous-time stochastic processes",
            "rating": 2
        },
        {
            "paper_title": "Score-based generative modeling through stochastic differential equations",
            "rating": 1
        }
    ],
    "cost": 0.012935249999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>LANGUAGE MODELING VIA STOCHASTIC PROCESSES</h1>
<p>Rose E. Wang, Esin Durmus, Noah Goodman, Tatsunori B. Hashimoto<br>Stanford University<br>{rewang, edurmus, ngoodman,thashim}@cs.stanford.edu</p>
<h4>Abstract</h4>
<p>Modern language models can generate high-quality short texts. However, they often meander or are incoherent when generating longer texts. These issues arise from the next-token-only language modeling objective. Recent work in selfsupervised learning suggests that models can learn good latent representations via contrastive learning, which can be effective for discriminative tasks. Our work analyzes the application of contrastive representations for generative tasks, like long text generation. We propose one approach for leveraging constrastive representations, which we call Time Control (TC). TC first learns a contrastive representation of the target text domain, then generates text by decoding from these representations. Compared to domain-specific methods and fine-tuning GPT2 across a variety of text domains, TC performs competitively to methods specific for learning sentence representations on discourse coherence. On long text generation settings, TC preserves the text structure both in terms of ordering (up to $+15 \%$ better) and text length consistency (up to $+90 \%$ better) ${ }^{1}$.</p>
<h2>1 INTRODUCTION</h2>
<p>Large language models (LLM) such as GPT-2 have been extremely successful in text generation (Radford et al., 2019; Brown et al., 2020). However, LLMs are known to generate incoherent long texts. One reason is that they are unable to plan ahead or represent long-range text dynamics (Kiddon et al., 2016; Fan et al., 2019; Hua \&amp; Wang, 2020; Duboue \&amp; McKeown, 2001; Stent et al., 2004; Tamkin et al., 2020). As a result, they oftentimes produce wandering content with poor discourse structure and low relevance (Hua \&amp; Wang, 2020; Zhao et al., 2017; Xu et al., 2020); the text reads as if the model has no anchored goal when generating. These problems with coherence are further exacerbated when forcing autoregressive models to generate longer texts as the model struggles to extrapolate beyond its expected text end point. These problems suggest that LLMs currently fail to properly capture how documents evolve from beginning to end. Doing so is critical for succeeding in goal-oriented tasks such as story, dialog or recipe generation.</p>
<p>Prior work has explored the use of planning-based methods for generating globally coherent text (Kiddon et al., 2016; Fan et al., 2019; Hua \&amp; Wang, 2020; Duboue \&amp; McKeown, 2001; Stent et al., 2004). However, these methods rely on manually defining text dynamics for specific domains. Other work has attempted to use sentence representations for modeling text, such as with variational auto-encoders (Bowman et al., 2016) or contrastive learning (Gao et al., 2021; Devlin et al., 2019). However, most of these works focus on discriminative tasks like classifying the order of</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>Correction note</h2>
<p>This is a revised version of the original ICLR 2022 paper. During post-publication code review, we discovered that the original version of the code did not leverage goal-directedness during decoding. While we still find that contrastive representations lead to gains in our evaluations, this error affects other claims made in the paper on goal-directed decoding. To correct this and help improve our understanding of goal-directed decoding, this updated version of the manuscript contains results on both goal-directed and non goal-directed baselines. We detail the difference between the original work and this updated work in Appendix H. The original version of this work can be found here: https://arxiv.org/abs/2203.11370v1.</p>
<p>sentences. Methods including van den Oord et al. (2019) have tried decoding from latent contrastive representations, however they fail to generate coherent long sequences.</p>
<p>We propose one way of leveraging contrastive learning representations for generation, which we call Time Control (TC). We begin by assuming that meandering text generated without a goal can be represented as Brownian motion in latent space; this motion enforces the embeddings of neighboring sentences to be similar to each other, whereas those of distant sentences to be dissimilar. Goaldirected behavior can be incorporated into this model by conditioning on a fixed start and end point. In this case, the Brownian motion becomes a Brownian bridge and the resulting latent trajectories abide by simple, closed-form dynamics.</p>
<p>We derive a novel contrastive objective for learning a latent space with Brownian bridge dynamics. We can then use this latent space to generate text that retains local coherence and has improved global coherence. To perform text generation, Time Control first generates a sequence of latent embeddings sampled from the Brownian bridge process pinned at a start and end point. It then conditionally generates sentences using this latent sequence. In our work, we decode the latent sequences by fine-tuning GPT2 to generate text conditioned on Time Control's latent sequence.</p>
<p>In summary, our work's contributions are the following:</p>
<ul>
<li>We derive a novel contrastive learning objective based on Brownian bridge stochastic processes.</li>
<li>We explore how contrastive representations can be used in both the discriminative and generative setting with our model Time Control which decodes from these learned representations.</li>
<li>Across a range of text domains, we show that Time Control generates more or equally coherent text on tasks including forced long text generation, compared to task-specific methods.</li>
<li>We validate that our latent representations capture text dynamics competitively by evaluating discourse coherence.</li>
<li>We ablate our method to understand the importance of the contrastive objective, enforcing Brownian bridge dynamics, explicitly modeling latent dynamics, and decoding from a latent plan.</li>
</ul>
<h1>2 RELATED WORKS</h1>
<p>Generating long, coherent text is conceptually difficult for autoregressive models because they lack the ability to model text structure and dynamics (Lin et al., 2021). This means that they struggle to plan and look ahead which leads to generating globally incoherent text. Forcing autoregressive models to generate longer texts exacerbates this incoherence because the models struggle to extrapolate beyond their expected text end point. Prior work has tried to address the problem of generating globally coherent text with planning-based approaches (Puduppully et al., 2019; Moryossef et al., 2019; Fan et al., 2019; Kiddon et al., 2016). However, planning-based approaches rely on domain-specific heuristics for capturing text structure and dynamics.</p>
<p>Our work uses a contrastive objective to learn latent dynamics in text without domain-specific heuristics. Contrastive objectives have been applied to several domains, including language (Devlin et al., 2019; Iter et al., 2020; Liu \&amp; Liu, 2021), vision (Chen et al., 2020), and general time series data (Hyvarinen \&amp; Morioka, 2016; Hyvarinen et al., 2019). In particular for language, contrastive objectives have been applied to the next-sentence prediction task for improving BERT embeddings (Devlin et al., 2019) and to the discourse coherence setting (Nie et al., 2019; Chen et al., 2019b) for evaluating how coherent pairs of sentences are. However, they are not used for generation and are limited to classification tasks like discourse coherence. Prior work has also tried fitting latent variable models (Bowman et al., 2016), however these generally result in poor language generation (He et al., 2018) or are domain-specific (Weber et al., 2020; Arora et al., 2016).</p>
<p>Our work is closely related to Contrastive Predictive Coding (CPC) from van den Oord et al. (2019). The key difference is CPC implicitly learns unconditioned latent dynamics, whereas our contrastive learning objective imposes goal-conditioned dynamics on our latent space. Additionally, our method builds off of recent findings that contrastive objectives can be used to approximate local</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Latent space for a positive triplet of sentences $\left(x_{0}, x_{t}, x_{T}\right)$ that are part of the same conversation. The encoder maps positive triplets to a smooth Brownian bridge trajectory. It embeds $z_{t}$ close to the expected embedding $\mu_{t}$ pinned by $z_{0}, z_{T}$. The green oval area illustrates the uncertainty over $z_{t}$ as a function of how close $t$ is to 0 and $T$. In contrast, a negative random sentence $x^{\prime}$ from a different conversation is not coherent with $x_{0}$ and $x_{T}$; thus, it is embedded far from $\mu_{t}$. This is captured by our contrastive loss, $\mathcal{L}$.
transition kernels of stochastic processes (Liu et al., 2021). The main difference between Liu et al. (2021) and our work is that they focus on provable conditions for latent recovery; we focus on empirically effective methods that leverage similar insights for recovering latent representations from language. Finally, our use of stochastic processes draws similarities to diffusion models (Song et al., 2020; Sohl-Dickstein et al., 2015) which apply a chain of diffusion steps onto the data and learn to reverse the diffusion process. However, our application is conceptually different: diffusion processes characterize properties of our latent space and are not a fixed inference method in our work.</p>
<h1>3 MEthods</h1>
<p>This section details our novel contrastive learning objective based on Brownian bridge dynamics. First, we discuss training an encoder via our contrastive learning objective to map sentences to a Brownian bridge (Revuz \&amp; Yor, 2013) latent space. Next, we discuss how a decoder learns to reconstruct sentences from this latent space. Finally, we discuss how to generate text at inference time.</p>
<h3>3.1 Training an Encoder with Brownian bridge dynamics</h3>
<p>Our encoder is a nonlinear mapping from raw input space to latent space, $f_{\theta}: \mathcal{X} \rightarrow \mathcal{Z}$. The objective for the encoder is to map high-dimensional sequential data into low-dimensional latents which follow a stochastic process of interest-in this paper, it is the Brownian bridge process. The density of a Brownian bridge process between an arbitrary start point $z_{0}$ at $t=0$ and end point $z_{T}$ at $t=T$ is,</p>
<p>$$
p\left(z_{t} \mid z_{0}, z_{T}\right)=\mathcal{N}\left(\left(1-\frac{t}{T}\right) z_{0}+\frac{t}{T} z_{T}, \frac{t(T-t)}{T}\right)
$$</p>
<p>This density is intuitive to understand: It acts like a noisy linear interpolation between the start and end point of the trajectory, where $z_{t}$ should be more like $z_{0}$ at the start and more like $z_{T}$ at the end of the trajectory. Uncertainty is highest in the middle region, and low near the end points (rf. Figure 1).</p>
<p>Consider a set of triplet observations, $\left(x_{1}, x_{2}, x_{3}\right)$. The goal of our work is to ensure that $f_{\theta}\left(x_{1}\right), f_{\theta}\left(x_{2}\right), f_{\theta}\left(x_{3}\right)$ follow the Brownian bridge transition density in Equation 1. We ensure this using a contrastive objective. Formally, given multiple sequences of data points, $X=\left{x_{1}, \ldots, x_{N}\right}$, we draw batches consisting of randomly sampled positive triplets $x_{0}, x_{t}, x_{T}$ where $0&lt;t&lt;T$ : $\mathcal{B}=\left{\left(x_{0}, x_{t}, x_{T}\right)\right} .{ }^{2}$ Our encoder is optimized by,</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>$$
\begin{aligned}
\mathcal{L}<em X="X">{N} &amp; =\mathbb{E}</em> \
\mathrm{d}\left(x_{0}, x_{t}, x_{T} ; f_{\theta}\right) &amp; =-\frac{1}{2 \sigma^{2}}\left|\underbrace{f_{\theta}\left(x_{t}\right)}}\left[-\log \frac{\exp \left(\mathrm{d}\left(x_{0}, x_{t}, x_{T} ; f_{\theta}\right)\right)}{\sum_{\left(x_{0}, x_{t^{\prime}}, x_{T}\right) \in \mathcal{B}} \exp \left(\mathrm{d}\left(x_{0}, x_{t^{\prime}}, x_{T} ; f_{\theta}\right)\right)}\right], \text { where <em t="t">{z</em>}}-\underbrace{\left(1-\frac{t}{T}\right) f_{\theta}\left(x_{0}\right)-\frac{t}{T} f_{\theta}\left(x_{T}\right)<em 2="2">{\text {mean in Equation } 1}\right|</em>
\end{aligned}
$$}^{2</p>
<p>$\sigma^{2}$ is the variance in Equation 1: $\frac{t(T-t)}{T}$. Note that Equation 2 sums over negative middle contrasts, $x_{t^{\prime}}$. This objective can be viewed as maximizing the extent to which true triplets from the data follow the Brownian bridge process while minimizing the extent to which an alternative mid-point sampled from another sequence does so. ${ }^{3}$
Figure 1 illustrates how the objective translates into the language setting for training the encoder. The objective samples triplet sentences from a document. Sentences drawn from the same document make up a smooth latent trajectory; they should be close to each other and follow the conditional density in latent space. Sentences drawn from different documents should not make up a smooth trajectory and should less likely follow bridge dynamics.</p>
<p>Connection to mutual information estimation and triplet classification We draw connections between our contrastive loss and the mutual information estimation setup from van den Oord et al. (2019); Poole et al. (2019) (as $|\mathcal{B}| \rightarrow \infty$ ) and the classification setup from Liu et al. (2021) $(|\mathcal{B}|=1$ ).</p>
<p>Following van den Oord et al. (2019); Poole et al. (2019), this objective can be seen as a lower bound on the mutual information between the two end points and the middle point: $I\left(X_{t},\left{X_{0}, X_{T}\right}\right) \geq$ $\log (N)-\mathcal{L}_{N}$. Hence, by minimizing the contrastive loss, we are maximizing the amount of information between the trajectory and the linear interpolation of its end points.</p>
<p>Assuming $|\mathcal{B}|=1$, we can draw a connection to the classification setup studied in Liu et al. (2021). They train a classifier to distinguish in- vs. out-of-order input pairs and show that the Bayes optimal logits for pair-wise classification can be written as a function of the stochastic process transition kernel. This is equivalent to the our loss on a single triplet $i: l_{i}=$ $-\log \frac{\exp \left(\mathrm{d}\left(x_{0}, x_{t}, x_{T} ; f_{\theta}\right)\right)}{\exp \left(\mathrm{d}\left(x_{0}, x_{t}, x_{T} ; f_{\theta}\right)\right)+\exp \left(\mathrm{d}\left(x_{0}, x_{t^{\prime}}, x_{T} ; f_{\theta}\right)\right)}$. Liu et al. (2021) consider pairs whereas our work considers triplets; we show in Appendix A the pairwise and triplet setups are equivalent.</p>
<h1>3.2 TRAINING A DECODER WITH LATENT PLANS</h1>
<p>Here we discuss how to train a language model to decode latent sequences for generation. We first map all the sentences in the training dataset to our learned latent space using the pretrained encoder $f_{\theta}$. This gives us a Brownian bridge trajectory of sentence-level latent codes $\left(z_{0}, \ldots, z_{t}, \ldots, z_{T}\right)$ for a document in the dataset. Then, rather than learning a decoder from scratch, we fine-tune GPT2 (Radford et al., 2019) to generate text conditioned on past context and the latent plan.</p>
<p>We fine-tune in the following manner. Let $x_{1} \ldots x_{W}$ be a document with $W$ tokens and $T$ sentences used to train the decoder. Using the encoder $f_{\theta}$, we can obtain embeddings $z_{1} \ldots z_{T}$ for each sentence. The decoder is a standard auto-regressive language model that is modified in the following way: at time $t$, the decoder must predict $x_{t}$ using all tokens in the past $x_{&lt;t}$, as well as the sentence embedding $z_{s_{t}}$, where the index $s_{t} \in[T]$ is a map which takes each token to its corresponding sentence. This is a form of a reconstruction objective, as the identity of $x_{t}$ is encoded in $z_{s_{t}}$.</p>
<h3>3.3 GENERATING TEXT WITH LATENT PLANS AT INFERENCE TIME</h3>
<p>Figure 2 illustrates how the trained decoder generates text at inference time. Given two end points $z_{0}, z_{T}$, we sample a trajectory from a latent Brownian bridge, and then generate from the decoder conditioned on this bridge. In many situations, we may not know the endpoints of the Brownian bridge explicitly. In this case, we encode a set of sentences corresponding to start and end points (eg. the first and last sentences of our training set), and fit a Gaussian to these points to form a density</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Time Control generates text conditioned on a latent plan. A latent plan is first generated by running Brownian bridge dynamics pinned between a sampled start $z_{0}$ and goal latent variable $z_{T}$ forward. A decoder then conditionally generates from this latent plan on a sentence-level.
estimate. Generating in this case involves first sampling from the Gaussian, and then generating as before from the bridge. More details on training and generation can be found in Appendix C.</p>
<h1>4 EXPERIMENTS</h1>
<p>We now evaluate the ability of a model trained with a contrastive objective to capture text dynamics. Specifically, we aim to answer the following research questions (RQ):</p>
<p>RQ1: Can the contrastive learning objective model local text dynamics? Section 4.1 investigates this question using a sentence ordering prediction task: given two sentences from the same document, we evaluate whether different models can predict their original order.</p>
<p>RQ2: Can the contrastive learning objective model global text dynamics? Section 4.2 investigates this question on text generation for Wikipedia city articles by examining the length of generated sections.</p>
<p>RQ3: Can the contrastive learning objective help generate long coherent documents? Section 4.3 investigates this question on forced long text generation: we evaluate how well models preserve global text statistics (such as typical section orders and lengths) when forced to extrapolate during generation.</p>
<p>We run our setup with different latent dimensions $(d=8,16,32)$. Our encoder architecture is a frozen, pretrained GPT2 model from Huggingface (Radford et al., 2019; Wolf et al., 2020) and a trainable MLP network. We extract GPT2's last layer hidden state that corresponds to the end-ofsentence (EOS) token and train the 4-layer MLP on top of the hidden state. The MLP network has intermediate ReLU activations and is trained with stochastic gradient descent with a learning rate of $1 \mathrm{e}-4$ and with momentum 0.9. For more details on the machines we used for our experiments, please refer to Appendix B. This version of the manuscript contains results after correcting for random seeding and GPU nondeterminism. Please refer to Appendix H for details on the version differences.</p>
<p>Encoder ablations We perform three ablations on our encoder model. Recall that Time Control encoder (A) explicitly models latent structure with (B) Brownian bridge dynamics using a (C) contrastive loss. (A) replaces explicit dynamics with Implicit Dynamics (ID) where future latents are directly predicted with an autoregressive model (van den Oord et al., 2019). (B) replaces Brownian bridge dynamics with Brownian motion (BM): latents follow the transition density $z_{t} \mid z_{s} \sim \mathcal{N}\left(z_{s}, t-s\right)$ in Equation 3. Note $z_{t}$ is centered at $z_{s}$ and is not conditioned on a goal endpoint. (C) replaces the contrastive loss with a Variational Auto-Encoder (VAE) and centers the priors over $z_{0}$ to 0 and $z_{T}$ to 1 , as done in our setup. Appendix E includes more detail on the ablations.</p>
<p>Decoder instantiations We compare two methods for decoding from contrastive representations. The first method decodes from the path $\left[z_{0}, z_{1}, \ldots, z_{T}\right]$ as detailed in Section 3.3 by generated by</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Wikisection</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">TM-2</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">TicketTalk</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Method</td>
<td style="text-align: center;">$k=5$</td>
<td style="text-align: center;">$k=10$</td>
<td style="text-align: center;">$k=5$</td>
<td style="text-align: center;">$k=10$</td>
<td style="text-align: center;">$k=5$</td>
<td style="text-align: center;">$k=10$</td>
</tr>
<tr>
<td style="text-align: center;">BERT</td>
<td style="text-align: center;">$72.7 \pm 0.1$</td>
<td style="text-align: center;">$80.4 \pm 0.2$</td>
<td style="text-align: center;">$75.6 \pm 0.0$</td>
<td style="text-align: center;">$86.7 \pm 0.0$</td>
<td style="text-align: center;">$73.1 \pm 0.2$</td>
<td style="text-align: center;">$86.4 \pm 0.0$</td>
</tr>
<tr>
<td style="text-align: center;">ALBERT</td>
<td style="text-align: center;">$67.7 \pm 15.1$</td>
<td style="text-align: center;">$82.3 \pm 0.3$</td>
<td style="text-align: center;">$83.9 \pm 1.7$</td>
<td style="text-align: center;">$92.2 \pm 0.9$</td>
<td style="text-align: center;">$74.5 \pm 9.8$</td>
<td style="text-align: center;">$93.3 \pm 0.6$</td>
</tr>
<tr>
<td style="text-align: center;">S-BERT</td>
<td style="text-align: center;">$71.8 \pm 0.1$</td>
<td style="text-align: center;">$79.0 \pm 0.1$</td>
<td style="text-align: center;">$75.6 \pm 0.0$</td>
<td style="text-align: center;">$86.7 \pm 0.0$</td>
<td style="text-align: center;">$76.2 \pm 0.1$</td>
<td style="text-align: center;">$88.8 \pm 0.0$</td>
</tr>
<tr>
<td style="text-align: center;">Sim-CSE</td>
<td style="text-align: center;">$72.6 \pm 0.1$</td>
<td style="text-align: center;">$79.5 \pm 0.1$</td>
<td style="text-align: center;">$77.8 \pm 0.0$</td>
<td style="text-align: center;">$88.3 \pm 0.0$</td>
<td style="text-align: center;">$78.0 \pm 0.0$</td>
<td style="text-align: center;">$90.1 \pm 0.1$</td>
</tr>
<tr>
<td style="text-align: center;">GPT2</td>
<td style="text-align: center;">$58.4 \pm 3.0$</td>
<td style="text-align: center;">$67.1 \pm 2.3$</td>
<td style="text-align: center;">$66.8 \pm 0.9$</td>
<td style="text-align: center;">$75.6 \pm 0.0$</td>
<td style="text-align: center;">$55.2 \pm 1.9$</td>
<td style="text-align: center;">$78.0 \pm 2.7$</td>
</tr>
<tr>
<td style="text-align: center;">VAE (8)</td>
<td style="text-align: center;">$50.0 \pm 3.8$</td>
<td style="text-align: center;">$50.3 \pm 0.8$</td>
<td style="text-align: center;">$49.4 \pm 1.1$</td>
<td style="text-align: center;">$49.9 \pm 1.7$</td>
<td style="text-align: center;">$49.0 \pm 2.6$</td>
<td style="text-align: center;">$49.0 \pm 5.7$</td>
</tr>
<tr>
<td style="text-align: center;">VAE (16)</td>
<td style="text-align: center;">$51.9 \pm 1.3$</td>
<td style="text-align: center;">$55.3 \pm 3.2$</td>
<td style="text-align: center;">$52.9 \pm 4.2$</td>
<td style="text-align: center;">$56.8 \pm 6.5$</td>
<td style="text-align: center;">$51.9 \pm 4.2$</td>
<td style="text-align: center;">$56.3 \pm 5.5$</td>
</tr>
<tr>
<td style="text-align: center;">VAE (32)</td>
<td style="text-align: center;">$52.3 \pm 3.0$</td>
<td style="text-align: center;">$50.7 \pm 2.3$</td>
<td style="text-align: center;">$55.7 \pm 1.3$</td>
<td style="text-align: center;">$64.1 \pm 2.3$</td>
<td style="text-align: center;">$59.3 \pm 1.5$</td>
<td style="text-align: center;">$69.5 \pm 1.0$</td>
</tr>
<tr>
<td style="text-align: center;">ID (8)</td>
<td style="text-align: center;">$58.2 \pm 0.4$</td>
<td style="text-align: center;">$64.7 \pm 1.6$</td>
<td style="text-align: center;">$62.4 \pm 0.6$</td>
<td style="text-align: center;">$71.0 \pm 0.5$</td>
<td style="text-align: center;">$54.3 \pm 1.0$</td>
<td style="text-align: center;">$64.7 \pm 0.5$</td>
</tr>
<tr>
<td style="text-align: center;">ID (16)</td>
<td style="text-align: center;">$68.5 \pm 0.1$</td>
<td style="text-align: center;">$76.9 \pm 0.1$</td>
<td style="text-align: center;">$62.7 \pm 0.1$</td>
<td style="text-align: center;">$71.1 \pm 0.1$</td>
<td style="text-align: center;">$53.0 \pm 0.2$</td>
<td style="text-align: center;">$66.2 \pm 0.1$</td>
</tr>
<tr>
<td style="text-align: center;">ID (32)</td>
<td style="text-align: center;">$70.1 \pm 0.1$</td>
<td style="text-align: center;">$77.1 \pm 0.1$</td>
<td style="text-align: center;">$62.6 \pm 0.1$</td>
<td style="text-align: center;">$71.1 \pm 0.0$</td>
<td style="text-align: center;">$55.4 \pm 0.2$</td>
<td style="text-align: center;">$69.4 \pm 0.1$</td>
</tr>
<tr>
<td style="text-align: center;">BM (8)</td>
<td style="text-align: center;">$49.4 \pm 3.2$</td>
<td style="text-align: center;">$56.7 \pm 6.8$</td>
<td style="text-align: center;">$52.1 \pm 3.1$</td>
<td style="text-align: center;">$55.0 \pm 5.1$</td>
<td style="text-align: center;">$47.8 \pm 2.8$</td>
<td style="text-align: center;">$51.2 \pm 3.2$</td>
</tr>
<tr>
<td style="text-align: center;">BM (16)</td>
<td style="text-align: center;">$52.8 \pm 0.8$</td>
<td style="text-align: center;">$56.3 \pm 0.9$</td>
<td style="text-align: center;">$51.5 \pm 1.1$</td>
<td style="text-align: center;">$52.8 \pm 2.3$</td>
<td style="text-align: center;">$55.2 \pm 5.5$</td>
<td style="text-align: center;">$62.4 \pm 5.6$</td>
</tr>
<tr>
<td style="text-align: center;">BM (32)</td>
<td style="text-align: center;">$51.8 \pm 1.9$</td>
<td style="text-align: center;">$55.9 \pm 4.3$</td>
<td style="text-align: center;">$50.4 \pm 1.0$</td>
<td style="text-align: center;">$51.3 \pm 2.2$</td>
<td style="text-align: center;">$55.0 \pm 2.6$</td>
<td style="text-align: center;">$69.6 \pm 2.6$</td>
</tr>
<tr>
<td style="text-align: center;">TC (8)</td>
<td style="text-align: center;">$69.3 \pm 0.1$</td>
<td style="text-align: center;">$77.6 \pm 0.0$</td>
<td style="text-align: center;">$78.4 \pm 0.0$</td>
<td style="text-align: center;">$88.7 \pm 0.0$</td>
<td style="text-align: center;">$74.7 \pm 0.1$</td>
<td style="text-align: center;">$87.2 \pm 0.0$</td>
</tr>
<tr>
<td style="text-align: center;">TC (16)</td>
<td style="text-align: center;">$71.7 \pm 0.0$</td>
<td style="text-align: center;">$76.8 \pm 0.0$</td>
<td style="text-align: center;">$78.7 \pm 0.0$</td>
<td style="text-align: center;">$89.0 \pm 0.0$</td>
<td style="text-align: center;">$74.7 \pm 0.0$</td>
<td style="text-align: center;">$87.2 \pm 0.1$</td>
</tr>
<tr>
<td style="text-align: center;">TC (32)</td>
<td style="text-align: center;">$71.6 \pm 0.0$</td>
<td style="text-align: center;">$77.3 \pm 0.0$</td>
<td style="text-align: center;">$79.2 \pm 0.0$</td>
<td style="text-align: center;">$89.1 \pm 0.0$</td>
<td style="text-align: center;">$74.0 \pm 0.0$</td>
<td style="text-align: center;">$87.3 \pm 0.0$</td>
</tr>
</tbody>
</table>
<p>Table 1: Discourse coherence accuracy measured by the test accuracy of the trained linear classifier, reporting $\mu \pm$ standard error over 3 runs. Random accuracy is $50 \%$. The highest mean score for the non-GPT2-based methods (rows above the double lines) are marked in gray cells. The highest mean score for the GPT2-based methods (rows below the double lines) are underlined. When applicable, the methods are run with varying latent dimensions marked in parentheses (dimension).
sampling from Brownian bridge or Brownian motion for BM. The second method-which we will call Static-decodes from repititions of the initial latent $\left[z_{0}, z_{0}, \ldots, z_{0}\right]$.</p>
<p>Datasets We use language datasets that elicit different kinds of structure, from section structure to discourse structure to narrative structure. Time Control does not take in any information about the structure, treating each domain the same under its encoding objective. More information and dataset examples are provided in Appendix F. Wikisection (Arnold et al., 2019) includes Wikipedia articles on cities split by sections. We adapt this dataset such that each article contains four ordered sections (abstract, history, geography, demographics) marked with section id tokens: Each article is represented as, "[ABSTRACT] text [HISTORY] text [GEOGRAPHY] text [DEMOGRAPHICS] text". Wikihow (WH) (Koupaee \&amp; Wang, 2018) contains how-to articles organized by a title, method, and steps. We mark each with its own section id tokens: Each article is represented as "[TITLE] text [METHOD] text [STEP] 1 text [STEP] 2 text ..." Recipe NLG (Bień et al., 2020) contains recipes, each with a title, ingredients and set of directions. A recipe is constructed as "[TITLE] text [INGREDIENTS] text [DIRECTIONS] text". Taskmaster-2 (TM-2) (Byrne et al., 2019) contains conversations on finding restaurants between an assistant and a user. The assistant's turn is marked with an "[ASSISTANT]" tag, and the user's turn is marked with a "[USER]" tag. TicketTalk (Byrne et al., 2021) contains conversations on booking movie tickets between an assistant and a user. The assistant's and user's turns are similarly marked as in TM-2.</p>
<h1>4.1 MODELING LOCAL TEXT DYNAMICS</h1>
<p>We evaluate how well the Brownian bridge contrastive learning objective models local text dynamics (RQ1) on the discourse coherence setting (Jurafsky, 2000). Discourse coherence is often measured by how well representations capture discourse structure by testing for whether a linear classifier can detect in-order and vs. out-of-order sentence pairs (Chen et al., 2019a). We compare Time Control's encoder against GPT2's last layer's hidden state corresponding to the EOS token (Radford et al., 2019), BERT (Devlin et al., 2019), ALBERT (Lan et al., 2019), Sentence BERT (Reimers et al., 2019), and SimCSE (Gao et al., 2021). The latter 4 methods are designed as sentence embedding models. We also compare to our ablations.</p>
<p>The setup is the following: The encoder takes in two sentences $x_{t}, x_{t+k}$ to produce their latents $z_{t}, z_{t+k} \in \mathbb{R}^{d}$. At random, the latents are fed either in- or out-of-order. A linear classifier is trained</p>
<p>on 100 epochs with stochastic gradient descent with a learning rate of 1e-4 and with momentum 0.9. We varied the sentence distance $k \in{1,5,10}$ and found that on some domains and for $k=1$, all methods scored near random accuracy; we have omitted those results in the main paper. Otherwise, the results are summarized in Table 1 where we report the mean and standard error accuracy on a held-out discourse dataset of 3000 examples on 3 runs. Our method is able to compete with sentence embedding specific methods, like ALBERT, BERT, SimCSE. Additionally, though GPT2 is used as a base encoder for Time Control, we observe that Time Control greatly improves upon GPT2 with significant gains on TM-2, TicketTalk, and Wikisection (up to $+20 \%$ ). Out of all the GPT2-based methods, it performs the best.</p>
<p>Neither VAE nor BM perform better than random accuracy on most of the domains. This suggests that the variational lower bound does not recover the latent embeddings as well as contrastive objectives and the choice of stochastic processes matter for learning informative structural embeddings.</p>
<p>Overall , our model performs competitively on both task-oriented conversations like TM-2 and TicketTalk and enumerative, factual domains like Wikisection. This answers RQ1 in the positive: Time Control can model local text dynamics, like in conversations and articles.</p>
<h1>4.2 MODELING GLOBAL TEXT DYNAMICS</h1>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">MM \% ( $\downarrow$ )</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT2</td>
<td style="text-align: center;">$9.9 \pm 3.6$</td>
</tr>
<tr>
<td style="text-align: left;">SD</td>
<td style="text-align: center;">$28.3 \pm 7.9$</td>
</tr>
<tr>
<td style="text-align: left;">SS</td>
<td style="text-align: center;">$223.9 \pm 18.5$</td>
</tr>
<tr>
<td style="text-align: left;">VAE (8)</td>
<td style="text-align: center;">$8.1 \pm 3.0$</td>
</tr>
<tr>
<td style="text-align: left;">VAE (16)</td>
<td style="text-align: center;">$14.1 \pm 7.8$</td>
</tr>
<tr>
<td style="text-align: left;">VAE (32)</td>
<td style="text-align: center;">$18.5 \pm 4.5$</td>
</tr>
<tr>
<td style="text-align: left;">STATIC VAE (8)</td>
<td style="text-align: center;">$8.9 \pm 2.2$</td>
</tr>
<tr>
<td style="text-align: left;">STATIC VAE (16)</td>
<td style="text-align: center;">$13.1 \pm 5.8$</td>
</tr>
<tr>
<td style="text-align: left;">STATIC VAE (32)</td>
<td style="text-align: center;">$\mathbf{6 . 7} \pm \mathbf{1 . 9}$</td>
</tr>
<tr>
<td style="text-align: left;">ID (8)</td>
<td style="text-align: center;">$13.3 \pm 3.8$</td>
</tr>
<tr>
<td style="text-align: left;">ID (16)</td>
<td style="text-align: center;">$61.8 \pm 5.5$</td>
</tr>
<tr>
<td style="text-align: left;">ID (32)</td>
<td style="text-align: center;">$84.6 \pm 11.0$</td>
</tr>
<tr>
<td style="text-align: left;">STATIC ID (8)</td>
<td style="text-align: center;">$12.3 \pm 3.4$</td>
</tr>
<tr>
<td style="text-align: left;">STATIC ID (16)</td>
<td style="text-align: center;">$77.1 \pm 4.4$</td>
</tr>
<tr>
<td style="text-align: left;">STATIC ID (32)</td>
<td style="text-align: center;">$79.6 \pm 8.0$</td>
</tr>
<tr>
<td style="text-align: left;">BM (8)</td>
<td style="text-align: center;">$16.2 \pm 11.4$</td>
</tr>
<tr>
<td style="text-align: left;">BM (16)</td>
<td style="text-align: center;">$21.1 \pm 11.8$</td>
</tr>
<tr>
<td style="text-align: left;">BM (32)</td>
<td style="text-align: center;">$51.6 \pm 14.2$</td>
</tr>
<tr>
<td style="text-align: left;">STATIC BM (8)</td>
<td style="text-align: center;">$15.4 \pm 9.2$</td>
</tr>
<tr>
<td style="text-align: left;">STATIC BM (16)</td>
<td style="text-align: center;">$11.8 \pm 5.4$</td>
</tr>
<tr>
<td style="text-align: left;">STATIC BM (32)</td>
<td style="text-align: center;">$13.0 \pm 5.0$</td>
</tr>
<tr>
<td style="text-align: left;">TC (8)</td>
<td style="text-align: center;">$13.1 \pm 4.4$</td>
</tr>
<tr>
<td style="text-align: left;">TC (16)</td>
<td style="text-align: center;">$15.4 \pm 10.1$</td>
</tr>
<tr>
<td style="text-align: left;">TC (32)</td>
<td style="text-align: center;">$28.1 \pm 12.2$</td>
</tr>
<tr>
<td style="text-align: left;">STATIC TC (8)</td>
<td style="text-align: center;">$12.8 \pm 3.2$</td>
</tr>
<tr>
<td style="text-align: left;">STATIC TC (16)</td>
<td style="text-align: center;">$13.0 \pm 7.5$</td>
</tr>
<tr>
<td style="text-align: left;">STATIC TC (32)</td>
<td style="text-align: center;">$13.0 \pm 7.8$</td>
</tr>
</tbody>
</table>
<p>Table 2: Percentage of length mismatch (MM) during generation. These results are run over 3 random GPU machines, each with 3 seeds. The method with the highest mean is marked in bold, and methods with overlapping confidence intervals are underlined.</p>
<p>We evaluate how well the Brownian bridge contrastive learning objective models global text dynamics (RQ2 ) by assessing whether the methods mimic document structure on Wikisection. We check whether the generated section lengths match with the average lengths in the dataset. We focus on Wikisection because it is the only dataset with long, well-defined sections (rf. Appendix F on dataset statistics). Each document contains an abstract, history, geography, and demographics section on a city.</p>
<p>Time Control plans a latent trajectory by running the bridge process between the start and end latent, $z_{0} \sim p\left(z_{0}\right)$ and $z_{T} \sim p\left(z_{T}\right)$. We compare against fine-tuned GPT2. We also include two oracle methods that are fine-tuned GPT2 models with additional section-embedding supervision.</p>
<p>One is "sec-dense GPT2" (SD) where each token's embedding contains the current section identity; section $i$ 's embedding is added onto the token's positional token embedding. The other is "sec-sparse GPT2" (SS) where the token embedding contains an indicator that is 1 if the token is the start of a new section, and 0 otherwise. For VAE and ID, we calculate the density estimate $p\left(z_{0}\right)$ and $p\left(z_{T}\right)$ and run a linear interpolation between the start and end latents. For BM, we calculate the density estimate $p\left(z_{0}\right)$ and run Brownian motion. Additionally, we run Static for Time Control, VAE, ID, BM which samples $z_{0} \sim p\left(z_{0}\right)$ and repeatedly decodes from this latent representation. Note that the difference between these Static models is that the decoders are finetuned with their respective objectives (Section 4).</p>
<p>Table 2 reports in percentage how much the generated section lengths deviate from the average section lengths. VAE best matches the section lengths out of all the methods, despite performing at near random accuracy on discourse coherence. We hypothesize that TC does not match the text lengths as well as the VAE method, as the VAE is more explicitly trained to match the text distribution. Nonetheless, TC and Static TC perform competitively with other top-performing baselines like VAE and Static VAE. Another surprising observation was that fine-tuned GPT2, SD and SS mismatch the section lengths by $10-224 \%$. Upon further investigation, we noticed the models overshoot short sections and undershoots long sections; this causes the section length deviations. ID and Static ID performs extremely poorly in contrast to these methods; this highlights the challenge of interpolating on learned dynamics models which is exacerbated in the long text generation settings. The results affirm that contrastive objectives can model global text dynamics, such as in matching document structure, however probabilistic objectives like VAE might be a better alternative learning objective.</p>
<h1>4.3 GENERATING GLOBALLY COHERENT TEXT</h1>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">WH</th>
<th style="text-align: center;">TM-2</th>
<th style="text-align: center;">TT</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT2</td>
<td style="text-align: center;">$\underline{10.6 \pm 4.3}$</td>
<td style="text-align: center;">$81.7 \pm 10.7$</td>
<td style="text-align: center;">$108.5 \pm 8.6$</td>
</tr>
<tr>
<td style="text-align: left;">VAE (8)</td>
<td style="text-align: center;">$\underline{10.8 \pm 6.3}$</td>
<td style="text-align: center;">$120.4 \pm 36.3$</td>
<td style="text-align: center;">$80.9 \pm 7.1$</td>
</tr>
<tr>
<td style="text-align: left;">VAE (16)</td>
<td style="text-align: center;">$13.2 \pm 6.4$</td>
<td style="text-align: center;">$109.4 \pm 51.2$</td>
<td style="text-align: center;">$75.0 \pm 4.5$</td>
</tr>
<tr>
<td style="text-align: left;">VAE (32)</td>
<td style="text-align: center;">$\underline{11.0 \pm 1.9}$</td>
<td style="text-align: center;">$66.9 \pm 10.8$</td>
<td style="text-align: center;">$77.5 \pm 25.0$</td>
</tr>
<tr>
<td style="text-align: left;">Static VAE (8)</td>
<td style="text-align: center;">$8.6 \pm 4.4$</td>
<td style="text-align: center;">$123.8 \pm 35.3$</td>
<td style="text-align: center;">$81.4 \pm 5.7$</td>
</tr>
<tr>
<td style="text-align: left;">Static VAE (16)</td>
<td style="text-align: center;">$12.9 \pm 5.3$</td>
<td style="text-align: center;">$115.5 \pm 60.6$</td>
<td style="text-align: center;">$75.2 \pm 3.2$</td>
</tr>
<tr>
<td style="text-align: left;">Static VAE (32)</td>
<td style="text-align: center;">$12.8 \pm 5.3$</td>
<td style="text-align: center;">$73.9 \pm 10.9$</td>
<td style="text-align: center;">$80.2 \pm 23.9$</td>
</tr>
<tr>
<td style="text-align: left;">ID (8)</td>
<td style="text-align: center;">$14.9 \pm 6.5$</td>
<td style="text-align: center;">$200.3 \pm 27.7$</td>
<td style="text-align: center;">$108.8 \pm 9.5$</td>
</tr>
<tr>
<td style="text-align: left;">ID (16)</td>
<td style="text-align: center;">$37.7 \pm 4.0$</td>
<td style="text-align: center;">$198.2 \pm 19.2$</td>
<td style="text-align: center;">$68.7 \pm 20.3$</td>
</tr>
<tr>
<td style="text-align: left;">ID (32)</td>
<td style="text-align: center;">$35.9 \pm 8.5$</td>
<td style="text-align: center;">$231.5 \pm 97.1$</td>
<td style="text-align: center;">$78.6 \pm 22.5$</td>
</tr>
<tr>
<td style="text-align: left;">Static ID (8)</td>
<td style="text-align: center;">$13.5 \pm 5.6$</td>
<td style="text-align: center;">$120.2 \pm 37.4$</td>
<td style="text-align: center;">$64.1 \pm 6.0$</td>
</tr>
<tr>
<td style="text-align: left;">Static ID (16)</td>
<td style="text-align: center;">$37.2 \pm 2.7$</td>
<td style="text-align: center;">$93.9 \pm 32.3$</td>
<td style="text-align: center;">$66.2 \pm 20.5$</td>
</tr>
<tr>
<td style="text-align: left;">Static ID (32)</td>
<td style="text-align: center;">$25.8 \pm 4.4$</td>
<td style="text-align: center;">$124.4 \pm 50.4$</td>
<td style="text-align: center;">$58.9 \pm 16.9$</td>
</tr>
<tr>
<td style="text-align: left;">BM (8)</td>
<td style="text-align: center;">$11.8 \pm 5.1$</td>
<td style="text-align: center;">$93.4 \pm 20.4$</td>
<td style="text-align: center;">$68.2 \pm 28.9$</td>
</tr>
<tr>
<td style="text-align: left;">BM (16)</td>
<td style="text-align: center;">$20.8 \pm 11.5$</td>
<td style="text-align: center;">$106.8 \pm 17.6$</td>
<td style="text-align: center;">$111.3 \pm 45.6$</td>
</tr>
<tr>
<td style="text-align: left;">BM (32)</td>
<td style="text-align: center;">$30.2 \pm 13.4$</td>
<td style="text-align: center;">$118.6 \pm 17.7$</td>
<td style="text-align: center;">$170.5 \pm 36.2$</td>
</tr>
<tr>
<td style="text-align: left;">Static BM (8)</td>
<td style="text-align: center;">$\mathbf{7 . 8} \pm \mathbf{3 . 2}$</td>
<td style="text-align: center;">$102.5 \pm 14.6$</td>
<td style="text-align: center;">$66.3 \pm 26.2$</td>
</tr>
<tr>
<td style="text-align: left;">Static BM (16)</td>
<td style="text-align: center;">$\underline{10.7 \pm 6.6}$</td>
<td style="text-align: center;">$93.7 \pm 11.7$</td>
<td style="text-align: center;">$66.2 \pm 34.7$</td>
</tr>
<tr>
<td style="text-align: left;">Static BM (32)</td>
<td style="text-align: center;">$14.1 \pm 9.7$</td>
<td style="text-align: center;">$111.5 \pm 17.2$</td>
<td style="text-align: center;">$82.7 \pm 25.9$</td>
</tr>
<tr>
<td style="text-align: left;">TC (8)</td>
<td style="text-align: center;">$13.5 \pm 6.1$</td>
<td style="text-align: center;">$41.9 \pm 9.5$</td>
<td style="text-align: center;">$69.8 \pm 6.2$</td>
</tr>
<tr>
<td style="text-align: left;">TC (16)</td>
<td style="text-align: center;">$8.5 \pm 4.4$</td>
<td style="text-align: center;">$21.5 \pm 4.7$</td>
<td style="text-align: center;">$60.8 \pm 16.6$</td>
</tr>
<tr>
<td style="text-align: left;">TC (32)</td>
<td style="text-align: center;">$13.3 \pm 3.6$</td>
<td style="text-align: center;">$33.2 \pm 3.3$</td>
<td style="text-align: center;">$53.5 \pm 27.4$</td>
</tr>
<tr>
<td style="text-align: left;">Static TC (8)</td>
<td style="text-align: center;">$13.4 \pm 5.5$</td>
<td style="text-align: center;">$15.6 \pm 2.7$</td>
<td style="text-align: center;">$\underline{36.4 \pm 11.4}$</td>
</tr>
<tr>
<td style="text-align: left;">Static TC (16)</td>
<td style="text-align: center;">$\underline{10.8 \pm 5.4}$</td>
<td style="text-align: center;">$\underline{13.7 \pm 3.4}$</td>
<td style="text-align: center;">$\underline{37.4 \pm 12.9}$</td>
</tr>
<tr>
<td style="text-align: left;">Static TC (32)</td>
<td style="text-align: center;">$\underline{10.3 \pm 7.1}$</td>
<td style="text-align: center;">$\mathbf{8 . 6} \pm \mathbf{5 . 3}$</td>
<td style="text-align: center;">$\mathbf{3 0 . 2} \pm \mathbf{1 7 . 7}$</td>
</tr>
</tbody>
</table>
<p>Table 3: Section lengths deviating from expected length in forced long text generation reported in \% ( $\downarrow$ ). These results are run over 3 random GPU machines, each with 3 seeds. The method with the highest mean is marked in bold, and methods with overlapping confidence intervals are marked in yellow cells.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Wikisection</th>
<th style="text-align: center;">Wikihow</th>
<th style="text-align: center;">TicketTalk</th>
<th style="text-align: center;">Recipe</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">GPT2</td>
<td style="text-align: center;">$47.4 \pm 5.4$</td>
<td style="text-align: center;">$61.3 \pm 9.1$</td>
<td style="text-align: center;">$19.7 \pm 2.3$</td>
<td style="text-align: center;">$71.9 \pm 2.5$</td>
</tr>
<tr>
<td style="text-align: center;">VAE (8)</td>
<td style="text-align: center;">$49.7 \pm 4.3$</td>
<td style="text-align: center;">$56.8 \pm 10.3$</td>
<td style="text-align: center;">$44.0 \pm 10.5$</td>
<td style="text-align: center;">$61.8 \pm 16.8$</td>
</tr>
<tr>
<td style="text-align: center;">VAE (16)</td>
<td style="text-align: center;">$49.3 \pm 5.3$</td>
<td style="text-align: center;">$67.7 \pm 11.2$</td>
<td style="text-align: center;">$50.9 \pm 17.4$</td>
<td style="text-align: center;">$54.0 \pm 25.7$</td>
</tr>
<tr>
<td style="text-align: center;">VAE (32)</td>
<td style="text-align: center;">$41.3 \pm 5.6$</td>
<td style="text-align: center;">$62.6 \pm 5.9$</td>
<td style="text-align: center;">$23.1 \pm 15.5$</td>
<td style="text-align: center;">$\mathbf{8 5 . 2} \pm \mathbf{3 . 7}$</td>
</tr>
<tr>
<td style="text-align: center;">Static VAE (8)</td>
<td style="text-align: center;">$51.0 \pm 4.3$</td>
<td style="text-align: center;">$53.9 \pm 11.6$</td>
<td style="text-align: center;">$45.5 \pm 11.3$</td>
<td style="text-align: center;">$70.6 \pm 13.9$</td>
</tr>
<tr>
<td style="text-align: center;">Static VAE (16)</td>
<td style="text-align: center;">$49.8 \pm 6.1$</td>
<td style="text-align: center;">$67.9 \pm 8.4$</td>
<td style="text-align: center;">$52.0 \pm 18.6$</td>
<td style="text-align: center;">$64.8 \pm 14.7$</td>
</tr>
<tr>
<td style="text-align: center;">Static VAE (32)</td>
<td style="text-align: center;">$45.9 \pm 6.1$</td>
<td style="text-align: center;">$61.1 \pm 6.0$</td>
<td style="text-align: center;">$21.9 \pm 14.8$</td>
<td style="text-align: center;">$78.8 \pm 3.9$</td>
</tr>
<tr>
<td style="text-align: center;">ID (8)</td>
<td style="text-align: center;">$50.8 \pm 3.8$</td>
<td style="text-align: center;">$60.5 \pm 11.9$</td>
<td style="text-align: center;">$50.0 \pm 3.4$</td>
<td style="text-align: center;">$65.6 \pm 15.0$</td>
</tr>
<tr>
<td style="text-align: center;">ID (16)</td>
<td style="text-align: center;">$62.3 \pm 6.7$</td>
<td style="text-align: center;">$27.8 \pm 20.1$</td>
<td style="text-align: center;">$55.4 \pm 19.7$</td>
<td style="text-align: center;">$55.7 \pm 8.5$</td>
</tr>
<tr>
<td style="text-align: center;">ID (32)</td>
<td style="text-align: center;">$\mathbf{6 4 . 0} \pm \mathbf{3 . 2}$</td>
<td style="text-align: center;">$44.1 \pm 7.9$</td>
<td style="text-align: center;">$64.8 \pm 6.2$</td>
<td style="text-align: center;">$81.5 \pm 5.2$</td>
</tr>
<tr>
<td style="text-align: center;">Static ID (8)</td>
<td style="text-align: center;">$51.0 \pm 5.0$</td>
<td style="text-align: center;">$8.5 \pm 10.4$</td>
<td style="text-align: center;">$49.2 \pm 13.4$</td>
<td style="text-align: center;">$75.9 \pm 7.1$</td>
</tr>
<tr>
<td style="text-align: center;">Static ID (16)</td>
<td style="text-align: center;">$21.2 \pm 4.1$</td>
<td style="text-align: center;">$28.3 \pm 20.9$</td>
<td style="text-align: center;">$54.3 \pm 21.3$</td>
<td style="text-align: center;">$56.6 \pm 15.7$</td>
</tr>
<tr>
<td style="text-align: center;">Static ID (32)</td>
<td style="text-align: center;">$35.4 \pm 7.0$</td>
<td style="text-align: center;">$48.5 \pm 8.9$</td>
<td style="text-align: center;">$\mathbf{6 7 . 8} \pm \mathbf{1 0 . 5}$</td>
<td style="text-align: center;">$77.0 \pm 6.4$</td>
</tr>
<tr>
<td style="text-align: center;">BM (8)</td>
<td style="text-align: center;">$50.1 \pm 3.4$</td>
<td style="text-align: center;">$61.8 \pm 8.9$</td>
<td style="text-align: center;">$53.8 \pm 17.3$</td>
<td style="text-align: center;">$71.0 \pm 6.9$</td>
</tr>
<tr>
<td style="text-align: center;">BM (16)</td>
<td style="text-align: center;">$40.7 \pm 6.1$</td>
<td style="text-align: center;">$51.3 \pm 20.5$</td>
<td style="text-align: center;">$37.7 \pm 15.4$</td>
<td style="text-align: center;">$37.3 \pm 16.6$</td>
</tr>
<tr>
<td style="text-align: center;">BM (32)</td>
<td style="text-align: center;">$43.4 \pm 7.4$</td>
<td style="text-align: center;">$41.9 \pm 15.1$</td>
<td style="text-align: center;">$49.2 \pm 3.7$</td>
<td style="text-align: center;">$53.2 \pm 7.6$</td>
</tr>
<tr>
<td style="text-align: center;">Static BM (8)</td>
<td style="text-align: center;">$51.5 \pm 6.6$</td>
<td style="text-align: center;">$65.2 \pm 10.1$</td>
<td style="text-align: center;">$54.2 \pm 19.3$</td>
<td style="text-align: center;">$54.9 \pm 17.9$</td>
</tr>
<tr>
<td style="text-align: center;">Static BM (16)</td>
<td style="text-align: center;">$42.8 \pm 5.3$</td>
<td style="text-align: center;">$66.5 \pm 15.1$</td>
<td style="text-align: center;">$22.8 \pm 19.1$</td>
<td style="text-align: center;">$45.0 \pm 14.0$</td>
</tr>
<tr>
<td style="text-align: center;">Static BM (32)</td>
<td style="text-align: center;">$46.3 \pm 4.2$</td>
<td style="text-align: center;">$66.7 \pm 10.3$</td>
<td style="text-align: center;">$23.4 \pm 12.6$</td>
<td style="text-align: center;">$30.1 \pm 12.4$</td>
</tr>
<tr>
<td style="text-align: center;">TC (8)</td>
<td style="text-align: center;">$49.2 \pm 10.5$</td>
<td style="text-align: center;">$64.3 \pm 14.7$</td>
<td style="text-align: center;">$48.3 \pm 23.8$</td>
<td style="text-align: center;">$56.0 \pm 10.4$</td>
</tr>
<tr>
<td style="text-align: center;">TC (16)</td>
<td style="text-align: center;">$42.5 \pm 7.0$</td>
<td style="text-align: center;">$66.2 \pm 17.7$</td>
<td style="text-align: center;">$58.8 \pm 11.7$</td>
<td style="text-align: center;">$66.6 \pm 10.0$</td>
</tr>
<tr>
<td style="text-align: center;">TC (32)</td>
<td style="text-align: center;">$36.9 \pm 7.7$</td>
<td style="text-align: center;">$69.7 \pm 12.3$</td>
<td style="text-align: center;">$35.4 \pm 16.5$</td>
<td style="text-align: center;">$77.0 \pm 3.5$</td>
</tr>
<tr>
<td style="text-align: center;">Static TC (8)</td>
<td style="text-align: center;">$50.5 \pm 11.2$</td>
<td style="text-align: center;">$69.7 \pm 12.3$</td>
<td style="text-align: center;">$58.1 \pm 19.6$</td>
<td style="text-align: center;">$55.7 \pm 11.3$</td>
</tr>
<tr>
<td style="text-align: center;">Static TC (16)</td>
<td style="text-align: center;">$48.5 \pm 5.5$</td>
<td style="text-align: center;">$67.9 \pm 19.0$</td>
<td style="text-align: center;">$67.3 \pm 8.9$</td>
<td style="text-align: center;">$70.3 \pm 7.5$</td>
</tr>
<tr>
<td style="text-align: center;">Static TC (32)</td>
<td style="text-align: center;">$52.1 \pm 6.4$</td>
<td style="text-align: center;">$\mathbf{7 1 . 5} \pm \mathbf{1 0 . 5}$</td>
<td style="text-align: center;">$49.1 \pm 18.0$</td>
<td style="text-align: center;">$82.4 \pm 3.3$</td>
</tr>
</tbody>
</table>
<p>Table 4: Ordering in forced long text generation. ROC Stories and TM-2 omitted because they are not applicable. These results are run over 3 random GPU machines, each with 3 seeds. The method with the highest mean is marked in bold, and methods with overlapping confidence intervals are underlined.</p>
<p>We evaluate how well the Brownian bridge contrastive learning objective helps generate globally coherent text (RQ3) where the EOS token is omitted. We refer to this as the forced long text generation setup because the model must extrapolate beyond its natural end point in generation. Appendix D. 2 details how the latent plan is generated. For reference, 1000 tokens is about $50 \%$ longer than the average Wikisection document (the longest text domain). This setting is intrinsically difficult for auto-regressive models which do not have the ability to "look ahead" during generation (Lin et al., 2021).</p>
<p>We evaluate model extrapolation on two metrics. First is ordering: how well do the models maintain the structure of the document (eg. turn-ordering in TicketTalk)? Second is length consistency. Length consistency captures common failure modes such as a model which stops modeling a conversation between two agents, and instead outputs a long monologue from one agent.</p>
<p>The results are summarized in Table 3 for length consistency and Table 4 for ordering. According to the length consistency metric in Table 3, Static TC maintains text flow the best. Surprisingly, Static TC performs better than TC, which decodes from an actual Brownian bridge sequence. We hypothesize that training the decoder with the contrastive encoder regularizes the model to generate better forced long texts. VAE, BM, ID, and their Static variants do particularly worse than Static TC on length consistency on task-oriented and turn-taking domains like TM-2 and TT, suggesting that the Brownian Bridge objective can model these domains more faithfully. This suggests the importance of good latent dynamics for long text generation. Here, a common failure mode of the nominal GPT2 model is that it produces nonsensical text where it would naturally end generation; this is particularly noticeable on TM-2 and TicketTalk. Figure 5 shows an example of our model's behavior versus GPT2's behavior on generating long texts in TicketTalk. The example illustrates how Time Control continues the dialog whereas GPT2 utters nonsensical text.</p>
<p>According to the ordering metric in Table 4, none of the methods surpass the other consistently. These results suggest that that even though Static TC is better at matching text lengths on the</p>
<p>Time Control : [ USER ] I'm looking for movie tickets please. [ ASSISTANT ] OK, where would you like to see the movie? [ USER ] I'd like to see it at Creek's End, Oregon please. [...] [ ASSISTANT ] Is it OK to go ahead and purchase these tickets? [ USER ] Yeah, that would be great. [ ASSISTANT ] OK. I understand that you are interested in tickets for Emma at AMC Mercado 24 tonight at 7:10pm. Is that OK? [ USER ] Yes, please do that. [ ASSISTANT ] OK
GPT2: [ USER ] Hi! Tonight, I'd like to go to the movies. [ ASSISTANT ] Okay. What theater would you like to go to? [ USER ] Center City. [...] [ ASSISTANT ] That will be all for now. Thank you for all your help. N/A [ USER ] Bye Bye. [ ASSISTANT ] N/A [ ASSISTANT ] N/A N/A N/A N/A N/A N/A N/A N/A [ USER ] N/A [ ASSISTANT ] N/A N/A N/A [ USER ] N/A [ ASSISTANT ] N/A N/A N/A [ USER ] N/A [ ASSISTANT ] N/A N/A N/A [ USER ] N/A N/A N/A N/A N/A N/A N/A N/A N/A [ USER ] N/A N/A N/A N/A [...]</p>
<p>Table 5: Example of forced long text generation on TicketTalk with Time Control vs. fine-tuned GPT2. Both models are forced to extrapolate when generating long texts. They start coherently, but only Time Control extrapolates coherently. For space reasons, some of the text has been removed, marked with [...].
forced long text setting, decoding from latent variables to preserve semantically meaningful metrics like section ordering still remains a challenge.</p>
<h1>5 DISCUSSION ON STATIC AND NON-STATIC METHODS</h1>
<p>In an earlier version of this paper, we evaluated the Static methods. Surprisingly, the Static methods perform well despite not leveraging the dynamics at test time. This raises an interesting question of whether there are better ways to leverage contrastive representations at decoding time.
For more details on the differences between the original manuscript and the current manuscript, please refer to Appendix H.</p>
<h2>6 CONCLUSION</h2>
<p>Our work explores using representations learned via contrastive learning for generative and discriminative tasks. We propose one way of learning effective language representations which follow Brownian bridge dynamics.</p>
<p>Empirically, we demonstrate this leads to benefits such as simultaneously yielding good representations for discriminative tasks like discourse coherence and generating long texts that preserve section lengths. Future work includes understanding why decoding from a static latent sequence is just as effective-if not more effective-as a dynamic sequence. Additionally, decoding from a latent sequence while preserving other semantically meaningful measures beyond text length still remains a challenge, as suggested by the ordering metrics in the forced long text generation setting. Although our work focuses on benefits of learning stochastic process latents for language generation, our approach can be extended to other domains with sequential data like videos or audio such as in Shin \&amp; Wang, or extended to handle arbitrary bridge processes without known fixed start and end points.</p>
<h1>7 REPRODUCIbility STATEMENT</h1>
<p>In the supplemental, we include a zip file containing our code and processed datasets. We've also included in the appendix how we processed the datasets. Our results on showing equivalence between the triplet classification setting and pairwise classification setting are also included in the appendix.</p>
<h2>ACKNOWLEDGMENTS</h2>
<p>REW is supported by the National Science Foundation Graduate Research Fellowship. The authors would give special thanks to CoColab members Mike Wu, Gabriel Poesia, Ali Malik and Alex Tamkin for their support and incredibly helpful discussions. The authors would like to thank the rest of CoCoLab and Rishi Bommasani for reviewing the paper draft. The authors also thank Chris Donahue for helpful pointers on using ILM.</p>
<h2>REFERENCES</h2>
<p>Sebastian Arnold, Rudolf Schneider, Philippe Cudré-Mauroux, Felix A. Gers, and Alexander Löser. Sector: A neural model for coherent topic segmentation and classification. Transactions of the Association for Computational Linguistics, 7:169-184, 2019. doi: 10.1162/tacl $\backslash . . \mathrm{a} \backslash . .00261$.</p>
<p>Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, and Andrej Risteski. A latent variable model approach to pmi-based word embeddings. Transactions of the Association for Computational Linguistics, 4:385-399, 2016.</p>
<p>Michał Bień, Michał Gilski, Martyna Maciejewska, Wojciech Taisner, Dawid Wisniewski, and Agnieszka Lawrynowicz. RecipeNLG: A cooking recipes dataset for semi-structured text generation. In Proceedings of the 13th International Conference on Natural Language Generation, pp. 22-28, Dublin, Ireland, December 2020. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/2020.inlg-1.4.</p>
<p>Samuel Bowman, Luke Vilnis, Oriol Vinyals, Andrew Dai, Rafal Jozefowicz, and Samy Bengio. Generating sentences from a continuous space. In Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning, pp. 10-21, 2016.</p>
<p>Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020.</p>
<p>Bill Byrne, Karthik Krishnamoorthi, Chinnadhurai Sankar, Arvind Neelakantan, Ben Goodrich, Daniel Duckworth, Semih Yavuz, Amit Dubey, Kyu-Young Kim, and Andy Cedilnik. Taskmaster1: Toward a realistic and diverse dialog dataset. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 4516-4525, 2019.</p>
<p>Bill Byrne, Karthik Krishnamoorthi, Saravanan Ganesh, and Mihir Kale. TicketTalk: Toward human-level performance with end-to-end, transaction-based dialog systems. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 671-680, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/ 2021.acl-long.55. URL https://aclanthology.org/2021.acl-long.55.</p>
<p>Mingda Chen, Zewei Chu, and Kevin Gimpel. Evaluation Benchmarks and Learning Criteria for Discourse-Aware Sentence Representations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 649-662, Hong Kong, China, November 2019a. Association for Computational Linguistics. doi: 10.18653/v1/D19-1060. URL https://www. aclweb.org/anthology/D19-1060.</p>
<p>Mingda Chen, Zewei Chu, and Kevin Gimpel. Evaluation benchmarks and learning criteria for discourse-aware sentence representations. In Proc. of EMNLP, 2019b.</p>
<p>Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In International conference on machine learning, pp. 1597-1607. PMLR, 2020.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. arXiv:1810.04805 [cs], May 2019. URL http://arxiv.org/abs/1810.04805. arXiv: 1810.04805.</p>
<p>Chris Donahue, Mina Lee, and Percy Liang. Enabling language models to fill in the blanks. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 2020.</p>
<p>Pablo A. Duboue and Kathleen R. McKeown. Empirically estimating order constraints for content planning in generation. In Proceedings of the 39th Annual Meeting on Association for Computational Linguistics, ACL '01, pp. 172-179, USA, 2001. Association for Computational Linguistics. doi: 10.3115/1073012.1073035. URL https://doi.org/10.3115/1073012. 1073035 .</p>
<p>Angela Fan, Mike Lewis, and Yann Dauphin. Strategies for Structuring Story Generation. arXiv:1902.01109 [cs], June 2019. URL http://arxiv.org/abs/1902.01109. arXiv: 1902.01109 .</p>
<p>Tianyu Gao, Xingcheng Yao, and Danqi Chen. Simcse: Simple contrastive learning of sentence embeddings. arXiv preprint arXiv:2104.08821, 2021.</p>
<p>Junxian He, Daniel Spokoyny, Graham Neubig, and Taylor Berg-Kirkpatrick. Lagging inference networks and posterior collapse in variational autoencoders. In International Conference on Learning Representations, 2018.</p>
<p>Xinyu Hua and Lu Wang. Pair: Planning and iterative refinement in pre-trained transformers for long text generation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 781-793, 2020.</p>
<p>Aapo Hyvarinen and Hiroshi Morioka. Unsupervised Feature Extraction by Time-Contrastive Learning and Nonlinear ICA. arXiv:1605.06336 [cs, stat], May 2016. URL http://arxiv. org/abs/1605.06336. arXiv: 1605.06336.</p>
<p>Aapo Hyvarinen, Hiroaki Sasaki, and Richard Turner. Nonlinear ica using auxiliary variables and generalized contrastive learning. In The 22nd International Conference on Artificial Intelligence and Statistics, pp. 859-868. PMLR, 2019.</p>
<p>Dan Iter, Kelvin Guu, Larry Lansing, and Dan Jurafsky. Pretraining with Contrastive Sentence Objectives Improves Discourse Performance of Language Models. arXiv:2005.10389 [cs], May 2020. URL http://arxiv.org/abs/2005.10389. arXiv: 2005.10389.</p>
<p>Dan Jurafsky. Speech \&amp; language processing. Pearson Education India, 2000.
Chloé Kiddon, Luke Zettlemoyer, and Yejin Choi. Globally coherent text generation with neural checklist models. In Proceedings of the 2016 conference on empirical methods in natural language processing, pp. 329-339, 2016.</p>
<p>Mahnaz Koupaee and William Yang Wang. Wikihow: A large scale text summarization dataset. arXiv preprint arXiv:1810.09305, 2018.</p>
<p>Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. Albert: A lite bert for self-supervised learning of language representations. arXiv preprint arXiv:1909.11942, 2019.</p>
<p>Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pp. 74-81, Barcelona, Spain, July 2004. Association for Computational Linguistics. URL https://aclanthology.org/W04-1013.</p>
<p>Chu-Cheng Lin, Aaron Jaech, Xin Li, Matthew R. Gormley, and Jason Eisner. Limitations of autoregressive models and their alternatives. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 5147-5173, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/ 2021.naacl-main.405. URL https://aclanthology.org/2021.naacl-main. 405.</p>
<p>Bingbin Liu, Pradeep Ravikumar, and Andrej Risteski. Contrastive learning of strong-mixing continuous-time stochastic processes. arXiv:2103.02740 [cs, stat], March 2021. URL http: //arxiv.org/abs/2103.02740. arXiv: 2103.02740.</p>
<p>Yixin Liu and Pengfei Liu. SimCLS: A Simple Framework for Contrastive Learning of Abstractive Summarization. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pp. 1065-1072, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-short.135. URL https://aclanthology.org/2021. acl-short. 135 .</p>
<p>Amit Moryossef, Yoav Goldberg, and Ido Dagan. Step-by-step: Separating planning from realization in neural data-to-text generation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 2267-2277, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1236. URL https: //aclanthology.org/N19-1236.</p>
<p>Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende, Pushmeet Kohli, and James Allen. A corpus and cloze evaluation for deeper understanding of commonsense stories. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 839-849, San Diego, California, June 2016. Association for Computational Linguistics. doi: 10.18653/v1/N16-1098. URL https://aclanthology.org/N16-1098.</p>
<p>Allen Nie, Erin Bennett, and Noah Goodman. DisSent: Learning Sentence Representations from Explicit Discourse Relations. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 4497-4510, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1442. URL https://www.aclweb.org/anthology/ P19-1442.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pp. 311-318, 2002.</p>
<p>Ben Poole, Sherjil Ozair, Aaron Van Den Oord, Alex Alemi, and George Tucker. On variational bounds of mutual information. In International Conference on Machine Learning, pp. 51715180. PMLR, 2019.</p>
<p>Ratish Puduppully, Li Dong, and Mirella Lapata. Data-to-text generation with content selection and planning. In Proceedings of the AAAI conference on artificial intelligence, volume 33, pp. 6908-6915, 2019.</p>
<p>Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019.</p>
<p>Nils Reimers, Iryna Gurevych, Nils Reimers, Iryna Gurevych, Nandan Thakur, Nils Reimers, Johannes Daxenberger, Iryna Gurevych, Nils Reimers, Iryna Gurevych, et al. Sentence-bert: Sentence embeddings using siamese bert-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 2019.</p>
<p>Daniel Revuz and Marc Yor. Continuous martingales and Brownian motion, volume 293. Springer Science \&amp; Business Media, 2013.</p>
<p>Thibault Sellam, Dipanjan Das, and Ankur P. Parikh. Bleurt: Learning robust metrics for text generation, 2020.</p>
<p>Harry Donghyeop Shin and Rose E Wang. Clap: Conditional latent planners for offline reinforcement learning. In NeurIPS 2022 Foundation Models for Decision Making Workshop.</p>
<p>Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning, pp. 2256-2265. PMLR, 2015.</p>
<p>Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In International Conference on Learning Representations, 2020.</p>
<p>Amanda Stent, Rashmi Prasad, and Marilyn Walker. Trainable sentence planning for complex information presentation in spoken dialog systems. In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, ACL '04, pp. 79-es, USA, 2004. Association for Computational Linguistics. doi: 10.3115/1218955.1218966. URL https://doi.org/10. 3115/1218955.1218966.</p>
<p>Alex Tamkin, Dan Jurafsky, and Noah Goodman. Language through a prism: A spectral approach for multiscale language representations. Advances in Neural Information Processing Systems, 33: $5492-5504,2020$.</p>
<p>Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding, 2019.</p>
<p>Noah Weber, Leena Shekhar, Heeyoung Kwon, Niranjan Balasubramanian, and Nathanael Chambers. Generating narrative text in a switching dynamical system. In Proceedings of the 24th Conference on Computational Natural Language Learning, pp. 520-530, 2020.</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 38-45, Online, October 2020. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/ 2020.emnlp-demos.6.</p>
<p>Jiacheng Xu, Zhe Gan, Yu Cheng, and Jingjing Liu. Discourse-aware neural extractive text summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 5021-5031, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.451. URL https://aclanthology.org/2020. acl-main. 451 .</p>
<p>Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation with bert. In International Conference on Learning Representations, 2019.</p>
<p>Tiancheng Zhao, Ran Zhao, and Maxine Eskenazi. Learning discourse-level diversity for neural dialog models using conditional variational autoencoders. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 654-664, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/ P17-1061. URL https://aclanthology.org/P17-1061.</p>
<h1>A SHOWING THE EQUIVALENCE OF THE PAIRWISE AND TRIPLET CLASSIFICATION TASK</h1>
<p>In this section, we focus on the classification setup where the goal is to train a classifier to detect invs. out-of-order groups of inputs. Specifically, we want to show that the classification setup from Liu et al. (2021) where the inputs come in pairs is equivalent to the setup where the inputs come in triplets. The triplet inputs are what we assume for Time Control.</p>
<p>Notation We denote latent space as $\mathcal{Z}$; we typically do not observe this space directly. We denote the observation space as $\mathcal{X}$; this is the space we observe directly.</p>
<p>We define the Brownian bridge for $t \in[0, T]$ as</p>
<p>$$
B_{t}=B(t)=W(t)-\frac{t}{T} W(T)
$$</p>
<p>where $W(t)$ is a standard Wiener process, $\mathcal{N}(0, t)$.
We state the assumptions:</p>
<ul>
<li>We assume latents $\left{z_{t}\right}_{t \geq 0} \in \mathbb{R}^{d}$ are drawn from the Brownian bridge process defined by the stochastic differential equation,</li>
</ul>
<p>$$
d z_{t}=\frac{z_{T}-z_{t}}{1-\frac{t}{T}} d t+d B_{t}
$$</p>
<p>The intervals at which latents are sampled are every $\Delta t$ step: $z^{\prime}=z_{t+\Delta t}$.</p>
<ul>
<li>We denote the transition probabilities as</li>
</ul>
<p>$$
p_{*}\left(z_{t} \mid z_{0}, z_{T}\right):=\mathcal{N}\left(\left(1-\frac{t}{T}\right) z_{0}+\frac{t}{T} z_{T}, \frac{t(T-t)}{t}\right)
$$</p>
<p>We denote the proposal distribution over possible intermediate triplets $q\left(z_{0}^{\prime}, z_{t}^{\prime}, z_{T}^{\prime}\right)$.
Liu et al. (2021) characterize properties of an optimal classifier $h^{<em>}\left(z, z^{\prime}\right)$ which observes pairs of latents $\left(z, z^{\prime}\right)$ and it outputs in $[0,1]$, a probability indicating whether the pair comes in order (ie. $z^{\prime}=z_{t}+\Delta t \cdot d z_{t}$ ) or not in order (ie. a randomly sampled latent). They train $h^{</em>}$ using an L2 loss, $\mathcal{L}\left(h,\left{\left(z, z^{\prime}\right), y\right}\right)$.
Lemma 1 of their work states the following: The optimum of the contrastive learning objective $\arg \min <em _left_left_z_="\left(\left(z," z_prime="z^{\prime">{h} \mathbb{E}</em>\right), y\right}\right)\right]$ satisfies}\right), y\right)}\left[\mathcal{L}\left(h,\left{\left(z, z^{\prime</p>
<p>$$
h^{*}\left(z, z^{\prime}\right)=\frac{p^{\Delta t}\left(z, z^{\prime}\right)}{q\left(z^{\prime}\right)+p^{\Delta t}\left(z, z^{\prime}\right)}
$$</p>
<p>Manipulating this equality, we observe that the transition kernel has the following relation to the classifier which takes in pairs of observations,</p>
<p>$$
\begin{aligned}
p^{\Delta t}\left(z, z^{\prime}\right) &amp; =\frac{q\left(z^{\prime}\right) h^{<em>}\left(z, z^{\prime}\right)}{1-h^{</em>}\left(z, z^{\prime}\right)} \
\log p^{\Delta t}\left(z, z^{\prime}\right) &amp; =\log q\left(z^{\prime}\right)+\log h^{<em>}\left(z, z^{\prime}\right)-\log \left(1-h^{</em>}\left(z, z^{\prime}\right)\right)
\end{aligned}
$$</p>
<p>Our setting however assumes that the algorithm receives triplets of data points, $z_{t_{1}}, z_{t_{2}}, z_{t_{3}}$. We want to show below that minimizing $\mathcal{L}$ with triplet contrasts in the classification setting still approximates the transition kernel. In particular, we're interested in transitions of a Brownian bridge pinned at $z_{0}, z_{T}: p^{\Delta t}\left(z_{0}, z_{t}, z_{T}\right)=\operatorname{Pr}\left(z_{t} \mid z_{0}, z_{T}\right)$.
Let's say we have two positive triplet samples, $\left(z_{t_{1}}^{i}, z_{t_{2}}^{i}, z_{t_{3}}^{i}\right)$ and $\left(z_{t_{1}}^{j}, z_{t_{2}}^{j}, z_{t_{3}}^{j}\right)$, where $t_{1}&lt;t_{2}&lt;t_{3}$. Following Liu et al. (2021), minimizing $\mathcal{L}$ yields the following on each triplet:</p>
<p>$$
\begin{aligned}
&amp; \log p^{\Delta t}\left(z_{t_{1}}^{i}, z_{t_{2}}^{i}, z_{t_{3}}^{i}\right)=\log q\left(z^{\prime}\right)+\log h^{<em>}\left(z_{t_{1}}^{i}, z_{t_{2}}^{i}, z_{t_{3}}^{i}\right)-\log \left(1-h^{</em>}\left(z_{t_{1}}^{i}, z_{t_{2}}^{i}, z_{t_{3}}^{i}\right)\right) \
&amp; \log p^{\Delta t}\left(z_{t_{1}}^{j}, z_{t_{2}}^{j}, z_{t_{3}}^{j}\right)=\log q\left(z^{\prime}\right)+\log h^{<em>}\left(z_{t_{1}}^{j}, z_{t_{2}}^{j}, z_{t_{3}}^{j}\right)-\log \left(1-h^{</em>}\left(z_{t_{1}}^{j}, z_{t_{2}}^{j}, z_{t_{3}}^{j}\right)\right)
\end{aligned}
$$</p>
<p>Taking the difference in log probabilities between Equations 10-11 results in</p>
<p>$$
\begin{aligned}
\log p^{\Delta t}\left(z_{t_{1}}^{i}, z_{t_{2}}^{i}, z_{t_{3}}^{i}\right)-\log p^{\Delta t}\left(z_{t_{1}}^{j}, z_{t_{2}}^{j}, z_{t_{3}}^{j}\right)= &amp; {\left[\log h^{<em>}\left(z_{t_{1}}^{i}, z_{t_{2}}^{i}, z_{t_{3}}^{i}\right)-\log \left(1-h^{</em>}\left(z_{t_{1}}^{i}, z_{t_{2}}^{i}, z_{t_{3}}^{i}\right)\right)\right] } \
&amp; -\left[h^{<em>}\left(z_{t_{1}}^{j}, z_{t_{2}}^{j}, z_{t_{3}}^{j}\right)-\log \left(1-h^{</em>}\left(z_{t_{1}}^{j}, z_{t_{2}}^{j}, z_{t_{3}}^{j}\right)\right)\right]
\end{aligned}
$$</p>
<p>Similar to the pair-wise classification setting, we've shown that minimizing $\mathcal{L}$ in the triplet classification setting results in approximating the transition kernel of the Brownian bridge process.</p>
<h1>B MACHINES USED FOR TRAINING</h1>
<p>We found that our decoding results are sensitive to the GPU machine type. For example, in the length mismatch results in Table 2, TC (8) had a deviation of $7.7 \pm 1.2$ in one set of three seeds and a deviation of $17.3 \pm 5.2$ in another set of three seeds. In light of this, we randomized over the choice of machines. These machines include: RTX A6000, RTX A5000, GeFORCE RTX 3090, TITAN RTX, TITAN Xp, TITAN V, and GeForce GTX TITAN X machines.</p>
<h1>C Training details</h1>
<p>Here we describe the training procedure for the encoder and the fine-tuning procedure for decoding.
Brownian bridge encoder The encoder architecture is a frozen, pretrained GPT2 model from Huggingface (Radford et al., 2019; Wolf et al., 2020) and a trainable MLP network. We extract the GPT2's last layer hidden state that corresponds to the end-of-sentence (EOS) token and train the 4-layer MLP on top of the hidden state. The MLP network has intermediate ReLU activations and is trained with stochastic gradient descent with a learning rate of $1 \mathrm{e}-4$ and with momentum 0.9. We train the encoder for 100 epochs on each of the datasets.</p>
<p>The text fed into GPT2 are fed in on a sentence level. This means that the input $x_{t}$ refers to the $t$ 'th sentence of a document. The sentences are separated from each other in the main text as ". " which is added to the tokenizer as a separate token for indexing convenience.</p>
<p>Fine-tuning GPT2 with latent embeddings After training the encoder, we run it on the training dataset to collect an accompanying latent trajectory for each text. The encoder is run on the dataset at a sentence level: we separate the text by sentences and pass the sentences through the encoder.</p>
<p>The sentence latent embeddings are aligned with the tokens of that sentence and offset by one token before the start of that sentence token. Let's illustrate by an example. We denote [SOS] as the start of the document token, [s1] as sentence 1 tokens and [s2] as sentence 2 tokens. [ . ] is the period token which we've added into the tokenizer. $z_{i}$ denote the latent variable corresponding to the $i$ 'th sentence.</p>
<p>Let's say that the sequence fed into GPT2 is "[SOS] [s1] [s1] [s1] [ . ] [s2] [s2] [s2]". Then the corresponding latent trajectory is " $z_{1}, z_{1}, z_{1}, z_{1}, z_{2}, z_{2}, z_{2}, z_{2}$ ". The latent variables are added onto the positional embeddings. We then fine-tune GPT2 as normal.</p>
<h2>D GENERATION WITH EMBEDDINGS</h2>
<h2>D. 1 NORMAL TEXT GENERATION</h2>
<p>We first sample a start and end latent, $z_{0} \sim p\left(z_{0}\right), z_{T} \sim p\left(z_{T}\right)$ where $p\left(z_{0}\right), p\left(z_{T}\right)$ are calculated as the density estimates over the training dataset. We pin our trajectory to the start and end latent, and run the Brownian bridge using a first-order approximation. For normal long text generation, we set $T$ to be the average number of sentences in each of the dataset. For forced long text generation, we set $T$ to be proportional to the number of sentences needed in order to generate 1000 tokens. By the end, we have a trajectory $z_{0}, z_{1}, \ldots, z_{T}$.</p>
<p>Generation starts with feeding the SOS token and the first latent $z_{0}$. Once GPT2 emits a [ . ] token and terminates sentence $t$, we transition to the next latent $z_{t+1}$. This process continues until GPT2 is finished with generation. If GPT2 generates more sentences than there are latents in the trajectory, the last latent $z_{T}$ is used until the end of generation.</p>
<h2>D. 2 FORCED LONG TEXT GENERATION</h2>
<p>Let $S_{\text {avg }(\mathcal{D})}$ denote the average number of sentences in a document and $T_{\text {avg }(\mathcal{D})}$ denote the average number of tokens in a document. Rather than planning a trajectory of length $S_{\text {avg }(\mathcal{D})}$ (the average number of sentences in a document) which is what is done in Section 4.3 for normal text generation, we scale the trajectory length to $c \cdot S_{\text {avg }(\mathcal{D})}$. $c$ is determined by how many more tokens we need in order to fill up to GPT-2 maximum context length of 1024: $c=\frac{1024-T_{\text {avg }(\mathcal{D})}}{T_{\text {avg }(\mathcal{D})}}$.</p>
<h2>E Ablations</h2>
<p>The following methods ablate the encoder model in Time Control; in other words, the ablations dissect the assumptions we make in the first step of Time Control described in Section 3.1. Recall that Time Control (A) explicitly models latent structure with (B) Brownian bridge dynamics using a (C) contrastive loss. (A) replaces explicit dynamics with Implicit Dynamics (ID) where future latents are directly predicted with an autoregressive model (van den Oord et al., 2019). (B) replaces Brownian bridge dynamics with Brownian motion (BM) which doesn't rely on pinning trajectories. Latents follow the transition density $z_{t} \mid z_{s} \sim \mathcal{N}\left(z_{s}, t-\nu\right)$. (C) replaces the contrastive loss with the Variational Auto-Encoder (VAE). Below we detail how these ablations are implemented.</p>
<h1>E. 1 IMPLICIT DYNAMICS (ID)</h1>
<p>The Implicit Dynamics ablation is where we compare our explicit dynamics objective to van den Oord et al. (2019) which suggests an implicit latent dynamics objective. This ablation thus changes the learned latent dynamics. In van den Oord et al. (2019), they train two models. One is a non-linear encoder $g_{\text {enc }}\left(x_{t}\right)=z_{t}$ which takes observation $x_{t}$ (eg. a sentence) and maps it to a latent representation $z_{t}$. Two is an autoregressive context model $g_{\text {ar }}\left(z_{\leq t}\right)=c_{t}$ which summarizes a sequence of latent variables $z_{\leq t}$ into a context latent representation $c_{t}$. Rather than directly predicting a future observation $x_{t+k}$ ( $k$ steps from the current timestep $t$ ), they model the density ratio that preserves the mutual information between $x_{t+k}$ and the context variable $c_{t}$ :</p>
<p>$$
f_{k}\left(x_{t+k}\right) \propto \frac{p\left(x_{t+k} \mid c_{t}\right)}{p\left(x_{t+k}\right)}
$$</p>
<p>They model this with a log-bilinear model $f_{k}\left(x_{t+k}\right)=\exp \left(z_{t+k}^{T} W_{k} c_{t}\right)$ which applies a linear transformation $W_{k}$ for modelling latents k -steps away from timestep $t$. This way, they avoid directly learning the generative model $p\left(x_{t+k} \mid c_{t}\right)$.</p>
<p>They train both models jointly via a contrastive InfoNCE loss. Given a set $X=\left{x_{1}, \ldots, x_{N}\right}$ of $N$ random samples (eg. sentences from different documents) containing one positive sample from $p\left(x_{t+k} \mid c_{t}\right)$ and $N-1$ negative samples from a proposal distribution $p\left(x_{t+k}\right)$, they optimize:</p>
<p>$$
\mathcal{L}=-\mathbb{E}<em k="k">{X}\left[\log \frac{f</em>\right]
$$}\left(x_{t+k}, c_{t}\right)}{\sum_{x_{j} \in X} f_{k}\left(x_{j}, c_{t}\right)</p>
<p>The encoder $g_{\text {enc }}$ for Implicit Dynamics has the same architecture as Time Control. The context encoder $g_{\text {ar }}$ using a 2400 -hidden-unit GRU, as done in van den Oord et al. (2019). We then train both encoders using the InfoNCE loss, as done in prior work. Since $g_{\text {enc }}$ and $g_{\text {ar }}$ are trained to align latents up to a linear rotation, we use $g_{\text {enc }}$ for extracting the sentence embeddings.</p>
<h2>E. 2 BROWNIAN MOTION (BM)</h2>
<p>The Brownian motion ablation is where we remove goal-conditioning in the dynamics. The main change is in distance function in Equation 3. BM instead optimizes the following equation:</p>
<p>$$
\mathrm{d}\left(x_{t}, x_{t^{\prime}} ; f_{\theta}\right)=-\frac{1}{2 \sigma^{2}} | \underbrace{f_{\theta}\left(x_{t^{\prime}}\right)}<em t_prime="t^{\prime">{z</em>}}}-\underbrace{f_{\theta}\left(x_{t}\right)<em t="t">{z</em>
$$}} |_{2}^{2</p>
<p>where $\sigma^{2}$ is the variance in of the Wiener process, $\sigma^{2}=t^{\prime}-t$.</p>
<h2>E. 3 VARIATIONAL AUTO-ENCODER (VAE)</h2>
<p>The Variational Auto-Encoder ablation is where we compare our contrastive objective with a variational one. This ablation changes the encoder objective from Section 3.1 from Equation 2 to the ELBO objective. Below we derive the ELBO objective. Similar to the contrastive objective, we're deriving the ELBO over the triplet dynamics in the Brownian bridge.</p>
<p>$$
\begin{aligned}
\log p(\mathbf{x}) &amp; \geq \mathbb{E}<em _phi="\phi">{q</em>\right] \
= &amp; \mathbb{E}}(\mathbf{z} \mid \mathbf{x})}\left[\log \frac{p(\mathbf{x}, \mathbf{z})}{q(\mathbf{z} \mid \mathbf{x})<em _phi="\phi">{q</em>\right] \
= &amp; \mathbb{E}}(\mathbf{z} \mid \mathbf{x})}\left[\log \frac{p\left(x_{0}, x_{t}, x_{T}, z_{0}, z_{t}, z_{T}\right)}{q\left(z_{0}, z_{t}, z_{T} \mid x_{0}, x_{t}, x_{T}\right)<em _phi="\phi">{q</em>\right] \
= &amp; \mathbb{E}}(\mathbf{z} \mid \mathbf{x})}\left[\log \frac{p\left(x_{0} \mid z_{0}\right) p\left(x_{t} \mid z_{t}\right) p\left(x_{T} \mid z_{T}\right) p\left(z_{t} \mid z_{0}, z_{T}\right) p\left(z_{0}\right) p\left(z_{T}\right)}{q\left(z_{t} \mid z_{0}, z_{T}, x_{t}\right) q\left(z_{0} \mid x_{0}\right) q\left(z_{T} \mid x_{T}\right)<em _phi="\phi">{q</em>}(\mathbf{z} \mid \mathbf{x})}\left[p\left(x_{0} \mid z_{0}\right) p\left(x_{t} \mid z_{t}\right) p\left(x_{T} \mid z_{T}\right)\right]+\mathbb{E<em _phi="\phi">{q</em>\right] \
&amp; +\mathbb{E}}(\mathbf{z} \mid \mathbf{x})}\left[\log \frac{p\left(z_{t} \mid z_{0}, z_{T}\right)}{q\left(z_{t} \mid z_{0}, z_{T}, x_{t}\right)<em _phi="\phi">{q</em>}(\mathbf{z} \mid \mathbf{x})}\left[\log \frac{p\left(z_{0}\right)}{q\left(z_{0} \mid x_{0}\right)}\right]+\mathbb{E<em _phi="\phi">{q</em>\right] \
= &amp; \mathbb{E}}(\mathbf{z} \mid \mathbf{x})}\left[\log \frac{p\left(z_{T}\right)}{q\left(z_{T} \mid x_{T}\right)<em _phi="\phi">{q</em>\right)\right] \
&amp; -D_{\mathrm{KL}}\left(q\left(z_{t} \mid z_{0}, z_{T}, x_{t}\right) | p\left(z_{t} \mid z_{0}, z_{T}\right)\right)-D_{\mathrm{KL}}\left(q\left(z_{0} \mid x_{0}\right) | p\left(z_{0}\right)\right)-D_{\mathrm{KL}}\left(q\left(z_{T} \mid x_{T}\right) | p\left(z_{T}\right)\right)
\end{aligned}
$$}(\mathbf{z} \mid \mathbf{x})}\left[\log p\left(x_{0} \mid z_{0}\right) p\left(x_{t} \mid z_{t}\right) p\left(x_{T} \mid z_{T</p>
<p>We assume the priors over $z_{0}$ is 0 -centered and $z_{T}$ is 1-centered, which is similar to our Brownian Bridge setup. The encoder $q_{\phi}(z \mid x)$ is parameterized with the same architecture as our encoder. The decoder $p(x \mid z)$ is a fine-tuned GPT2 model.</p>
<h1>E. 4 Static DECOding</h1>
<p>StatIc impacts how the latent plans are generated for the methods at inference time. Instead of generating the latent plan $\left[z_{0}, z_{1}, \ldots, z_{T}\right]$ by running the Brownian bridge dynamics (applicable to VAE, ID, and TC) or Brownian motion (applicable to BM), the method generates the plan $\left[z_{0}, z_{0}, \ldots, z_{0}\right]$ of the same $T$ length and decodes from this plan.</p>
<h2>F DATASET INFORMATION</h2>
<p>For each dataset, text examples were filtered out if they did not fit within GPT2's context length of 1024 tokens. We also added the token ". " for each setting to mark the end of a sentence. This was done for indexing purposes, eg. when aligning the latent embeddings.</p>
<p>Wikisection (Arnold et al., 2019) includes Wikipedia articles on cities split by sections. We adapt this dataset such that each article contains four ordered sections (abstract, history, geography, demographics) marked with section id tokens: Each article is represented as, "[ABSTRACT] text [HISTORY] text [GEOGRAPHY] text [DEMOGRAPHICS] text". These section id tokens are added to the tokenizer.</p>
<p>The training dataset contains 1420 articles. The section lengths have the following breakdown measured in the number of BPE tokens (GPT2 tokenizer):</p>
<ul>
<li>Abstract: $75.8 \pm 1.4$</li>
<li>History: $191.5 \pm 3.7$</li>
<li>Geography: $83.9 \pm 1.5$</li>
<li>Demographics: $342.6 \pm 4.6$</li>
</ul>
<p>The test dataset contains 431 articles. The section lengths have a similar breakdown:</p>
<ul>
<li>Abstract: $73.5 \pm 2.6$</li>
<li>History: $180.2 \pm 6.2$</li>
<li>Geography: $85.2 \pm 2.7$</li>
<li>Demographics: $332.5 \pm 8.6$</li>
</ul>
<p>The ordering metric used in Table 4 is 1 if all four section ids occur exactly once and come in the order as they are listed above.</p>
<p>The length mismatch in $\%$ used in Table 2 is calculated with respect to the training set lengths.</p>
<p>Wikihow (Koupaee \&amp; Wang, 2018) contains how-to articles organized by a title, method, and steps. Each article includes multiple methods for completing a multi-step procedural task such as "How to Register to Vote". We scraped all the available English articles covering a wide range of topics following Koupaee \&amp; Wang (2018). We mark each with its own section id tokens: Each article is represented as "[TITLE] text [METHOD] text [STEP] 1 text [STEP] 2 text ..."</p>
<p>The training dataset contains 1566 articles. The section lengths are,</p>
<ul>
<li>Title: $10.4 \pm 2.2$</li>
<li>Method: $8.7 \pm 2.2$</li>
<li>Steps (total step length): $480.2 \pm 231.5$</li>
</ul>
<p>The test dataset contains 243 articles. The section lengths are,</p>
<ul>
<li>Title: $10.7 \pm 2.2$</li>
<li>Method: $8.6 \pm 2.1$</li>
<li>Steps (total step length): $480.1 \pm 224.0$</li>
</ul>
<p>The ordering metric used in Table 4 is 1 if all the section ids appear in order, the TITLE and METHOD section ids are not repeated, and the step numbers come in order. It's 0 otherwise.</p>
<p>The deviation in section length measured in Table 3 is calculated with respect to the training set lengths. We check for whether the models are able to maintain the section lengths when it has to extrapolate. The most common failure mode is that the model generates incoherent text and allocates this text to the last section of what it's generated thus far, resulting in the deviation in section lengths.</p>
<p>Recipe NLG (Bień et al., 2020) contains recipes, each with a title, ingredients and set of instructions. A recipe is constructed as "[TITLE] text [INGREDIENTS] text [DIRECTIONS] text".</p>
<p>The training dataset contains 4000 recipes. The section lengths are,</p>
<ul>
<li>Title: $9.7 \pm 3.4$</li>
<li>Ingredients: $23.8 \pm 4.5$</li>
<li>Directions (total step length): $62.0 \pm 14.5$</li>
</ul>
<p>The test dataset contains 1000 recipes. The section lengths are,</p>
<ul>
<li>Title: $9.4 \pm 3.0$</li>
<li>Ingredients: $24.1 \pm 4.5$</li>
<li>Directions (total step length): $63.3 \pm 13.7$</li>
</ul>
<p>The ordering metric used in Table 4 is 1 if all the section ids appear exactly once and in order. It's 0 otherwise.</p>
<p>Taskmaster-2 (TM-2) (Byrne et al., 2019) which contains conversations on finding restaurants between an assistant and a user. The assistant's turn is marked with an "[ASSISTANT]" tag, and the user's turn is marked with a "[USER]" tag.</p>
<p>The training dataset contains 2000 conversations. The section lengths are,</p>
<ul>
<li>User: $11.8 \pm 5.6$</li>
<li>Assistant: $18.0 \pm 3.7$</li>
</ul>
<p>The test dataset contains 1276 conversations. The section lengths are,</p>
<ul>
<li>User: $11.7 \pm 5.6$</li>
<li>Assistant: $19.5 \pm 3.0$</li>
</ul>
<p>The ordering metric used in Table 4 does not apply here because the user and assistant don't take turns in the dialog.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">Wikisection</th>
<th style="text-align: center;">Wikihow</th>
<th style="text-align: center;">TM-2</th>
<th style="text-align: center;">TicketTalk</th>
<th style="text-align: center;">Recipe</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT2</td>
<td style="text-align: center;">5.9</td>
<td style="text-align: center;">15.3</td>
<td style="text-align: center;">4.5</td>
<td style="text-align: center;">4.4</td>
<td style="text-align: center;">7.5</td>
</tr>
<tr>
<td style="text-align: left;">sec-dense</td>
<td style="text-align: center;">5.9</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">sec-sparse</td>
<td style="text-align: center;">5.9</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">VAE (8)</td>
<td style="text-align: center;">5.5</td>
<td style="text-align: center;">15.3</td>
<td style="text-align: center;">4.5</td>
<td style="text-align: center;">4.1</td>
<td style="text-align: center;">7.3</td>
</tr>
<tr>
<td style="text-align: left;">VAE (16)</td>
<td style="text-align: center;">5.5</td>
<td style="text-align: center;">15.3</td>
<td style="text-align: center;">4.5</td>
<td style="text-align: center;">4.1</td>
<td style="text-align: center;">7.3</td>
</tr>
<tr>
<td style="text-align: left;">VAE (32)</td>
<td style="text-align: center;">5.5</td>
<td style="text-align: center;">15.3</td>
<td style="text-align: center;">4.5</td>
<td style="text-align: center;">4.1</td>
<td style="text-align: center;">7.3</td>
</tr>
<tr>
<td style="text-align: left;">ID (8)</td>
<td style="text-align: center;">5.5</td>
<td style="text-align: center;">14.9</td>
<td style="text-align: center;">4.5</td>
<td style="text-align: center;">4.0</td>
<td style="text-align: center;">7.3</td>
</tr>
<tr>
<td style="text-align: left;">ID (16)</td>
<td style="text-align: center;">5.4</td>
<td style="text-align: center;">14.9</td>
<td style="text-align: center;">4.3</td>
<td style="text-align: center;">3.9</td>
<td style="text-align: center;">7.0</td>
</tr>
<tr>
<td style="text-align: left;">ID (32)</td>
<td style="text-align: center;">5.3</td>
<td style="text-align: center;">14.9</td>
<td style="text-align: center;">4.2</td>
<td style="text-align: center;">3.9</td>
<td style="text-align: center;">6.7</td>
</tr>
<tr>
<td style="text-align: left;">BM (8)</td>
<td style="text-align: center;">5.5</td>
<td style="text-align: center;">15.2</td>
<td style="text-align: center;">4.5</td>
<td style="text-align: center;">4.1</td>
<td style="text-align: center;">7.3</td>
</tr>
<tr>
<td style="text-align: left;">BM (16)</td>
<td style="text-align: center;">5.5</td>
<td style="text-align: center;">15.3</td>
<td style="text-align: center;">4.5</td>
<td style="text-align: center;">4.1</td>
<td style="text-align: center;">7.3</td>
</tr>
<tr>
<td style="text-align: left;">BM (32)</td>
<td style="text-align: center;">5.5</td>
<td style="text-align: center;">15.2</td>
<td style="text-align: center;">4.5</td>
<td style="text-align: center;">4.1</td>
<td style="text-align: center;">7.3</td>
</tr>
<tr>
<td style="text-align: left;">TC (8)</td>
<td style="text-align: center;">5.5</td>
<td style="text-align: center;">15.2</td>
<td style="text-align: center;">4.3</td>
<td style="text-align: center;">4.0</td>
<td style="text-align: center;">6.9</td>
</tr>
<tr>
<td style="text-align: left;">TC (16)</td>
<td style="text-align: center;">5.5</td>
<td style="text-align: center;">15.3</td>
<td style="text-align: center;">4.3</td>
<td style="text-align: center;">4.0</td>
<td style="text-align: center;">6.9</td>
</tr>
<tr>
<td style="text-align: left;">TC (32)</td>
<td style="text-align: center;">5.5</td>
<td style="text-align: center;">15.2</td>
<td style="text-align: center;">4.3</td>
<td style="text-align: center;">4.0</td>
<td style="text-align: center;">7.0</td>
</tr>
</tbody>
</table>
<p>Table 6: Perplexity after fine-tuning.</p>
<p>The deviation in section length measured in Table 3 is calculated with respect to the training set lengths. We check for whether the models are able to maintain the utterance lengths between the user and assistant when it has to extrapolate. The most common failure mode is that the model generates incoherent text and allocates this text to the last section of what it's generated thus far, resulting in the deviation in section lengths.</p>
<p>TicketTalk (Byrne et al., 2021) which contains conversations on booking movie tickets between an assistant and a user. The assistant's and user's turned are similarly marked as in TM-2.</p>
<p>The training dataset contains 2000 conversations. The section lengths are,</p>
<ul>
<li>User: $11.8 \pm 5.6$</li>
<li>Assistant: $18.0 \pm 3.7$</li>
</ul>
<p>The test dataset contains 1276 conversations. The section lengths are,</p>
<ul>
<li>User: $11.7 \pm 5.6$</li>
<li>Assistant: $19.5 \pm 3.0$</li>
</ul>
<p>The deviation in section length measured in Table 3 is calculated similarly to TM-2.
The ordering metric used in Table 4 applies because the user and assistant take turns in the dialog. The ordering metric is 1 if the user and assistant take turns in the conversation. It's 0 otherwise.</p>
<p>ROC Stories (Mostafazadeh et al., 2016) is a short stories dataset. Each story contains 5 sentences. No additional tokens are added in this dataset. The training dataset contains 2000 stories, and the test dataset contains 1000 stories. This dataset was used in the original text infilling experiments we had. Those previous results are in Table 21, Table 11 and Table 12.</p>
<h1>G PERPLEXITY AFTER FINE-TUNING</h1>
<p>Table 6 reports the final perplexity scores after fine-tuning GPT2 on the different domains with the methods. We fine-tune for 10 epochs and checkpoint the models every 1000 steps; we keep the model checkpoint that scores the lowest PPL on a held-out validation set.</p>
<h2>H DIFFERENCES IN THE CORRECTED MANUSCRIPT</h2>
<p>This section discusses updates and differences from an earlier version of this paper due to correcting bugs during a post-publication code review and re-running experiments. Sources of</p>
<p>randomness, such as the behavior of the torch dataloader, GPU nondeterminism, and Weights and Biases logging have caused some differences in the experimental outcomes even in cases where there were no bugs. To make the differences clear, we highlight both original and current results, as well as a short explanation of any differences. The numbers reported in the main text correspond to the current version of the codebase available at https://github. com/rosewang2008/language_modeling_via_stochastic_processes with version hash: 95ded1e82f904e1c2dbce5701f54ab880a1a1e62.</p>
<h1>H. 1 DISCOURSE COHERENCE</h1>
<p>For reference, Table 7 shows both the original and new results. The original results are marked with ORIGINAL.</p>
<p>Differences In the previous version, ALBERT, SimCSE and TC were competitive with each other. In the updated version, BERT is competitive with these methods as well. TC is the most competitive out of all the GPT2-based models.</p>
<p>Cause This discrepancy is due to a wandb step logging issue where wandb previously only reported on early training epochs. Therefore, the previous discourse coherence results generally under-report the actual performance-this particularly impacts Wikisection. In general, all methods improve and do better than random accuracy after resolving this issue.</p>
<h2>H. 2 LENGTH MISMATCH</h2>
<p>For reference, Table 8 shows both the original and new results. The original results are marked with ORIGINAL.</p>
<p>Differences In the previous version, TC performed the best. In the updated version, Static VAE performs the best, followed by Static TC.</p>
<p>Causes The variability is caused by the dataset loader being seeded separately from usual seeding sources, GPU nondeterminism combined with sensitivity of the evaluation to this nondeterminism. The experiments reported in the paper are all seeded. If the experiments are run on the same machine, they replicate exactly. We observe different experimental results when run on different machines. For example, in our experiments, TC (8) on the history section mismatches the original section by about $\sim 45$ tokens on one machine, and $\sim 21$ tokens on another machine.</p>
<h2>H. 3 SECTION LENGTH DEVIATION</h2>
<p>For reference, Table 9 shows both the original and new results. The original results are marked with ORIGINAL.</p>
<p>Differences In the previous version, TC performed the best in not deviating from the expected section lengths (Table 4). In the updated version, this is still the case with Static TC. Surprisingly, we found that Static BM and VAE performs competitively in one of the domains as well.</p>
<p>Causes Besides the variability causes mentioned in Table 3 section, there was an improper implementation of forced generation related to how we specify the min and max generation length. This affected all methods equally by cutting decoding short.</p>
<h2>H. 4 ORDERING</h2>
<p>For reference, Table 10 shows both the original and new results. The original results are marked with ORIGINAL.</p>
<p>Differences In the previous version, TC performed the best along with the VAE baseline in correct ordering (Table 5). In the updated version, there's a lot more overlap between methods.</p>
<p>Causes The causes are the same as for Table 4.</p>
<h2>I ORIGINAL TEXT-INFILLING AND HUMAN EVALUATION RESULTS</h2>
<p>Due to the difficulty of re-running the human evaluation results, we have decided not to include an updated version of the human evaluation results. This impacts our text-infilling setting and long text generation setting. For transparency, we have included the text and results from the original version below.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ Empirically, we found Brownian bridge dynamics easier to recover with triplets rather than pairs of contrasts. Appendix O. 2 discusses some of the pair-wise contrast results.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>